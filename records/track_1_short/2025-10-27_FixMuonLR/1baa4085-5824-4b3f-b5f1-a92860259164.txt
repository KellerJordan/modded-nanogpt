import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from collections import defaultdict
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class NorMuon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).

    Differences from standard Muon:
    - Newton-Shulz is replaced with Polar Express for the orthogonalization step
    - NorMuon adds a low-rank variance estimator similar to Adafactor.
    - small 1D parameters handled via magnitude normalization of the grad (faster execution than Adam)
    - Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as
    needed on the forward pass. This enables attn and mlp weights to be contained within the same
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn.
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, beta2=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, beta2=beta2)
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def reset(self):
        # expose a reset for clearing buffers
        for group in self.param_groups:
            group["momentum_buffer"].zero_()
            group["second_momentum_buffer"].zero_()

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per module.
        """
        groups = defaultdict(list)
        for param in params:
            groups[param.label].append(param)

        param_groups = []
        for module_name, group_params in groups.items():
            chunk_size = (len(group_params) + self.world_size - 1) // self.world_size
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))

        return param_groups

    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn
        and mlp params when a param group is split across GPUs.
        """
        module_group_order = ['smear_gate', 'attn_gate', 'attn', 'mlp']
        params_list = list(params)
        params_list.sort(key=lambda x: module_group_order.index(x.label))

        idx = 0
        group_sizes = [1, 10, 16, 16]
        assert len(params_list) == sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            chunk_size = (size + self.world_size - 1) // self.world_size
            group_params = params_list[idx: idx + size]
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))
            idx += size

        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, @vagrawal, and @varunneal.
        rank = dist.get_rank()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            stacked_grads = torch.empty(
                (padded_num_params, *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device
            )
            for i, p in enumerate(params):
                stacked_grads[i].copy_(p.grad, non_blocking=True)
            if len(params) < padded_num_params:
                stacked_grads[len(params):].zero_()

            grad_chunk = torch.empty_like(stacked_grads[:chunk_size])

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(dict(grad_chunk=grad_chunk, reduce_future=reduce_future))

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = group["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            start_idx = rank * chunk_size
            module_idx = start_idx if start_idx < len(params) else 0

            num_params = min(chunk_size, max(0, len(params) - start_idx))  # num params for this rank

            if "momentum_buffer" not in group:
                group["momentum_buffer"]  = torch.zeros_like(grad_chunk[:num_params])
            momentum_buffer = group["momentum_buffer"]
            # Apply momentum update to the persistent momentum buffer in-place
            momentum_buffer.lerp_(grad_chunk[:num_params], 1 - group["momentum"])
            updated_grads = grad_chunk[:num_params].lerp_(momentum_buffer, group["momentum"])

            grad_shape = updated_grads.shape
            if params[module_idx].label == 'attn':
                # Reshape attn params from [hdim, dim*4] to [4,hdim,dim]
                for p in params[module_idx:module_idx + num_params]:
                    assert p.label == 'attn'
                updated_grads = updated_grads.view(4 * grad_shape[0], grad_shape[1], grad_shape[2] // 4)
            ref_param = params[module_idx]
            param_shape = ref_param.shape

            if "second_momentum_buffer" not in group:
                group["second_momentum_buffer"] = (torch.zeros_like(updated_grads[..., :, :1])
                    if param_shape[-2] >= param_shape[-1] else torch.zeros_like(updated_grads[..., :1, :])
                )
            second_momentum_buffer = group["second_momentum_buffer"]

            if "param_lr" not in group:
                group["param_lr"] = (
                    max(1., param_shape[-2] / param_shape[-1]) ** 0.5
                    * ref_param.new_tensor(
                        [getattr(param, "lr_mul", 1.0) for param in params[module_idx:module_idx + num_params]]
                    ).view(-1, 1, 1)
                )

                group["param_wd"] = ref_param.new_tensor(
                    [getattr(param, "wd_mul", 1.0) for param in params[module_idx:module_idx + num_params]]
                ).view(-1, 1, 1)

            # Determine LR and WR
            eff_lr = group["lr"] * group["param_lr"]
            eff_wd = group["weight_decay"] * group["param_wd"]

            # Compute zeropower for the entire chunk in a single, batched call.
            if num_params == 0:
                v_chunk = updated_grads
            else:
                v_chunk = polar_express(updated_grads)

            # NorMuon: second_momentum_buffer tracks squared magnitude of gradients along one dim (https://arxiv.org/pdf/2510.05491)
            v_norm = v_chunk.norm(dim=(-2, -1), keepdim=True)
            v_mean = v_chunk.square().mean(dim=-1 if param_shape[-2] >= param_shape[-1] else -2, keepdim=True)
            second_momentum_buffer.lerp_(v_mean.to(dtype=ref_param.dtype), 1 - group["beta2"])
            step_size = second_momentum_buffer.clamp_min(1e-10).rsqrt_()
            v_chunk.mul_(step_size)
            v_norm_new = v_chunk.norm(dim=(-2, -1), keepdim=True)
            v_chunk.mul_(v_norm / v_norm_new.clamp_min_(1e-10))

            v_chunk = v_chunk.view(grad_shape)

            updated_params = torch.empty_like(grad_chunk)
            param_chunk = torch.stack(params[module_idx:module_idx + num_params]) if num_params > 0 else torch.zeros_like(v_chunk)
            # Apply weight decay directly to the buffer.
            param_chunk.mul_(1 - eff_wd)

            param_chunk.add_(-eff_lr * v_chunk)

            updated_params[:num_params].copy_(param_chunk)
            if num_params < chunk_size:
                updated_params[num_params:].zero_()

            stacked_params = torch.empty(
                (padded_num_params, *param_shape),
                dtype=updated_params.dtype,
                device=updated_params.device,
            )

            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_params, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # init state
        for p in params:
            chunk_size = p.size(0) // self.world_size
            exp_avg = torch.zeros_like(p[:chunk_size], dtype=torch.bfloat16, device=p[0].device)
            exp_avg_sq = torch.zeros_like(exp_avg)
            self.state[p] = dict(step=0, exp_avg=exp_avg, exp_avg_sq=exp_avg_sq)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        reduce_scatter_futures: list[torch.Future] = []
        all_gather_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                grad = param.grad
                rank_size = grad.shape[0] // self.world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for param in params:
                reduce_scatter_futures[idx].wait()
                rank_size = param.shape[0] // self.world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]
                g_slice = grad_slices[idx]

                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (bias2 ** 0.5 / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        with torch.no_grad():
            self.weight.zero_()  # @Grad62304977 and others

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.label='attn'

        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.label = 'attn_gate'

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4, self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens,
                                                        max_seqlen_q=max_len, max_seqlen_k=max_len,
                                                        causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4, self.hdim, self.dim)[3].type_as(y))
        return y


class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.label = 'mlp'
        self.c_proj.label = 'mlp'
        # corrective factor to account for transpose
        self.c_fc.lr_mul = 2.

        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.label = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        smear_lambda = self.scalars[5 * len(self.blocks)]
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        # smear token embed forward 1 position @classiclarryd
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        n = len(self.blocks) // 2

        x_backout = None
        backout_layer = 8
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            # since layer 0 is skipped, layer 11 does not have skip_connection
            if i >= n and i<11:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()

    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 16 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2245  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: float = 0.50  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_final: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizer2 = NorMuon(hidden_matrix_params + gate_params, lr=0.03, momentum=0.95, beta2=0.95, weight_decay=0.0)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: flat, then linear decay, then flat
def get_lr(step: int):
    x = min(0.9999, step / args.num_scheduled_iterations)
    assert 0 <= x < 1
    lr = 1.0
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = w * 1.0 + (1 - w) * 0.1
    return lr

def get_ws(step: int):
    # set short window size to half of long window size
    # Higher ws on "extension" steps
    if step >= args.num_scheduled_iterations:
        return args.ws_final // 2, args.ws_final
    x = step / args.num_scheduled_iterations
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
ws_schedule = list(args.ws_schedule) + [args.ws_final]
ws_long = ws_schedule[0]
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    # each window size is a new graph, need to warm up each with Yarn.attn_scale
    ws_idx = step % len(ws_schedule)
    if ws_idx==0:
        model.yarn.reset()
        ws_long = ws_schedule[0]
    else:
        new_ws_long = ws_schedule[ws_idx]
        model.yarn.apply(ws_long, new_ws_long)
        ws_long = new_ws_long
    model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset() # rotary buffer is not stored in state_dict
model.load_state_dict(initial_state["model"])
optimizer2.reset() # muon momentum buffers not in state dict
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for _ in range(grad_accum_steps):
        inputs, targets, cum_seqlens = next(train_loader)
        model(inputs, targets, cum_seqlens, ws_short, ws_long).backward()
    step_optimizers(step, optimizers, model)

    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.10.12 (main, Aug 15 2025, 14:32:43) [GCC 11.4.0]
Running PyTorch 2.10.0.dev20250926+cu126 compiled for CUDA 12.6
Running Triton version 3.5.0
Thu Nov  6 05:13:36 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:19:00.0 Off |                    0 |
| N/A   38C    P0            126W /  700W |    5858MiB /  81559MiB |      1%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:3B:00.0 Off |                    0 |
| N/A   31C    P0            123W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:4C:00.0 Off |                    0 |
| N/A   31C    P0            118W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:5D:00.0 Off |                    0 |
| N/A   38C    P0            129W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:9B:00.0 Off |                    0 |
| N/A   35C    P0            120W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:BB:00.0 Off |                    0 |
| N/A   28C    P0            118W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:CB:00.0 Off |                    0 |
| N/A   36C    P0            122W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:DB:00.0 Off |                    0 |
| N/A   28C    P0            118W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A     93958      C   /root/.venv/bin/python3                         0MiB |
|    0   N/A  N/A     93959      C   /root/.venv/bin/python3                         0MiB |
|    0   N/A  N/A     93960      C   /root/.venv/bin/python3                         0MiB |
|    0   N/A  N/A     93961      C   /root/.venv/bin/python3                         0MiB |
|    0   N/A  N/A     93962      C   /root/.venv/bin/python3                         0MiB |
|    0   N/A  N/A     93963      C   /root/.venv/bin/python3                         0MiB |
|    0   N/A  N/A     93964      C   /root/.venv/bin/python3                         0MiB |
|    0   N/A  N/A     93965      C   /root/.venv/bin/python3                         0MiB |
|    1   N/A  N/A     93959      C   /root/.venv/bin/python3                         0MiB |
|    2   N/A  N/A     93960      C   /root/.venv/bin/python3                         0MiB |
|    3   N/A  N/A     93961      C   /root/.venv/bin/python3                         0MiB |
|    4   N/A  N/A     93962      C   /root/.venv/bin/python3                         0MiB |
|    5   N/A  N/A     93963      C   /root/.venv/bin/python3                         0MiB |
|    6   N/A  N/A     93964      C   /root/.venv/bin/python3                         0MiB |
|    7   N/A  N/A     93965      C   /root/.venv/bin/python3                         0MiB |
+-----------------------------------------------------------------------------------------+

====================================================================================================
step:0/2285 val_loss:10.8258 train_time:0ms step_avg:0.02ms
step:1/2285 train_time:109ms step_avg:109.40ms
step:2/2285 train_time:131ms step_avg:65.42ms
step:3/2285 train_time:169ms step_avg:56.25ms
step:4/2285 train_time:225ms step_avg:56.31ms
step:5/2285 train_time:285ms step_avg:56.93ms
step:6/2285 train_time:343ms step_avg:57.18ms
step:7/2285 train_time:404ms step_avg:57.72ms
step:8/2285 train_time:462ms step_avg:57.81ms
step:9/2285 train_time:523ms step_avg:58.15ms
step:10/2285 train_time:582ms step_avg:58.21ms
step:11/2285 train_time:644ms step_avg:58.53ms
step:12/2285 train_time:702ms step_avg:58.52ms
step:13/2285 train_time:764ms step_avg:58.75ms
step:14/2285 train_time:823ms step_avg:58.79ms
step:15/2285 train_time:884ms step_avg:58.93ms
step:16/2285 train_time:943ms step_avg:58.96ms
step:17/2285 train_time:1008ms step_avg:59.31ms
step:18/2285 train_time:1070ms step_avg:59.45ms
step:19/2285 train_time:1135ms step_avg:59.74ms
step:20/2285 train_time:1195ms step_avg:59.74ms
step:21/2285 train_time:1257ms step_avg:59.88ms
step:22/2285 train_time:1317ms step_avg:59.88ms
step:23/2285 train_time:1380ms step_avg:59.99ms
step:24/2285 train_time:1439ms step_avg:59.97ms
step:25/2285 train_time:1500ms step_avg:60.02ms
step:26/2285 train_time:1560ms step_avg:59.98ms
step:27/2285 train_time:1621ms step_avg:60.03ms
step:28/2285 train_time:1680ms step_avg:59.99ms
step:29/2285 train_time:1741ms step_avg:60.04ms
step:30/2285 train_time:1800ms step_avg:60.01ms
step:31/2285 train_time:1862ms step_avg:60.06ms
step:32/2285 train_time:1921ms step_avg:60.04ms
step:33/2285 train_time:1985ms step_avg:60.16ms
step:34/2285 train_time:2046ms step_avg:60.19ms
step:35/2285 train_time:2110ms step_avg:60.27ms
step:36/2285 train_time:2169ms step_avg:60.25ms
step:37/2285 train_time:2232ms step_avg:60.32ms
step:38/2285 train_time:2291ms step_avg:60.30ms
step:39/2285 train_time:2353ms step_avg:60.34ms
step:40/2285 train_time:2411ms step_avg:60.28ms
step:41/2285 train_time:2473ms step_avg:60.32ms
step:42/2285 train_time:2532ms step_avg:60.28ms
step:43/2285 train_time:2593ms step_avg:60.31ms
step:44/2285 train_time:2652ms step_avg:60.28ms
step:45/2285 train_time:2713ms step_avg:60.29ms
step:46/2285 train_time:2772ms step_avg:60.25ms
step:47/2285 train_time:2833ms step_avg:60.27ms
step:48/2285 train_time:2892ms step_avg:60.24ms
step:49/2285 train_time:2954ms step_avg:60.28ms
step:50/2285 train_time:3013ms step_avg:60.26ms
step:51/2285 train_time:3075ms step_avg:60.30ms
step:52/2285 train_time:3136ms step_avg:60.30ms
step:53/2285 train_time:3198ms step_avg:60.35ms
step:54/2285 train_time:3259ms step_avg:60.35ms
step:55/2285 train_time:3321ms step_avg:60.38ms
step:56/2285 train_time:3380ms step_avg:60.36ms
step:57/2285 train_time:3442ms step_avg:60.39ms
step:58/2285 train_time:3501ms step_avg:60.37ms
step:59/2285 train_time:3563ms step_avg:60.39ms
step:60/2285 train_time:3623ms step_avg:60.38ms
step:61/2285 train_time:3685ms step_avg:60.40ms
step:62/2285 train_time:3744ms step_avg:60.38ms
step:63/2285 train_time:3807ms step_avg:60.42ms
step:64/2285 train_time:3866ms step_avg:60.40ms
step:65/2285 train_time:3928ms step_avg:60.43ms
step:66/2285 train_time:3987ms step_avg:60.41ms
step:67/2285 train_time:4049ms step_avg:60.44ms
step:68/2285 train_time:4109ms step_avg:60.43ms
step:69/2285 train_time:4171ms step_avg:60.46ms
step:70/2285 train_time:4231ms step_avg:60.44ms
step:71/2285 train_time:4293ms step_avg:60.46ms
step:72/2285 train_time:4352ms step_avg:60.44ms
step:73/2285 train_time:4413ms step_avg:60.45ms
step:74/2285 train_time:4472ms step_avg:60.43ms
step:75/2285 train_time:4534ms step_avg:60.45ms
step:76/2285 train_time:4593ms step_avg:60.43ms
step:77/2285 train_time:4654ms step_avg:60.44ms
step:78/2285 train_time:4713ms step_avg:60.42ms
step:79/2285 train_time:4774ms step_avg:60.44ms
step:80/2285 train_time:4834ms step_avg:60.42ms
step:81/2285 train_time:4896ms step_avg:60.44ms
step:82/2285 train_time:4955ms step_avg:60.43ms
step:83/2285 train_time:5018ms step_avg:60.45ms
step:84/2285 train_time:5077ms step_avg:60.44ms
step:85/2285 train_time:5139ms step_avg:60.46ms
step:86/2285 train_time:5198ms step_avg:60.45ms
step:87/2285 train_time:5260ms step_avg:60.46ms
step:88/2285 train_time:5319ms step_avg:60.44ms
step:89/2285 train_time:5381ms step_avg:60.46ms
step:90/2285 train_time:5440ms step_avg:60.45ms
step:91/2285 train_time:5502ms step_avg:60.46ms
step:92/2285 train_time:5562ms step_avg:60.45ms
step:93/2285 train_time:5623ms step_avg:60.46ms
step:94/2285 train_time:5682ms step_avg:60.45ms
step:95/2285 train_time:5744ms step_avg:60.46ms
step:96/2285 train_time:5803ms step_avg:60.45ms
step:97/2285 train_time:5866ms step_avg:60.47ms
step:98/2285 train_time:5925ms step_avg:60.46ms
step:99/2285 train_time:5988ms step_avg:60.48ms
step:100/2285 train_time:6047ms step_avg:60.47ms
step:101/2285 train_time:6109ms step_avg:60.48ms
step:102/2285 train_time:6168ms step_avg:60.47ms
step:103/2285 train_time:6230ms step_avg:60.49ms
step:104/2285 train_time:6289ms step_avg:60.47ms
step:105/2285 train_time:6351ms step_avg:60.49ms
step:106/2285 train_time:6410ms step_avg:60.47ms
step:107/2285 train_time:6472ms step_avg:60.48ms
step:108/2285 train_time:6530ms step_avg:60.47ms
step:109/2285 train_time:6591ms step_avg:60.47ms
step:110/2285 train_time:6650ms step_avg:60.45ms
step:111/2285 train_time:6711ms step_avg:60.46ms
step:112/2285 train_time:6770ms step_avg:60.44ms
step:113/2285 train_time:6831ms step_avg:60.45ms
step:114/2285 train_time:6890ms step_avg:60.43ms
step:115/2285 train_time:6951ms step_avg:60.44ms
step:116/2285 train_time:7009ms step_avg:60.43ms
step:117/2285 train_time:7071ms step_avg:60.43ms
step:118/2285 train_time:7130ms step_avg:60.42ms
step:119/2285 train_time:7191ms step_avg:60.43ms
step:120/2285 train_time:7251ms step_avg:60.42ms
step:121/2285 train_time:7312ms step_avg:60.43ms
step:122/2285 train_time:7371ms step_avg:60.42ms
step:123/2285 train_time:7432ms step_avg:60.43ms
step:124/2285 train_time:7491ms step_avg:60.41ms
step:125/2285 train_time:7552ms step_avg:60.42ms
step:126/2285 train_time:7611ms step_avg:60.40ms
step:127/2285 train_time:7672ms step_avg:60.41ms
step:128/2285 train_time:7730ms step_avg:60.39ms
step:129/2285 train_time:7791ms step_avg:60.40ms
step:130/2285 train_time:7851ms step_avg:60.39ms
step:131/2285 train_time:7912ms step_avg:60.39ms
step:132/2285 train_time:7971ms step_avg:60.38ms
step:133/2285 train_time:8032ms step_avg:60.39ms
step:134/2285 train_time:8090ms step_avg:60.38ms
step:135/2285 train_time:8152ms step_avg:60.38ms
step:136/2285 train_time:8210ms step_avg:60.37ms
step:137/2285 train_time:8272ms step_avg:60.38ms
step:138/2285 train_time:8331ms step_avg:60.37ms
step:139/2285 train_time:8393ms step_avg:60.38ms
step:140/2285 train_time:8452ms step_avg:60.37ms
step:141/2285 train_time:8513ms step_avg:60.38ms
step:142/2285 train_time:8572ms step_avg:60.37ms
step:143/2285 train_time:8634ms step_avg:60.38ms
step:144/2285 train_time:8692ms step_avg:60.36ms
step:145/2285 train_time:8754ms step_avg:60.37ms
step:146/2285 train_time:8813ms step_avg:60.36ms
step:147/2285 train_time:8875ms step_avg:60.37ms
step:148/2285 train_time:8934ms step_avg:60.37ms
step:149/2285 train_time:8996ms step_avg:60.37ms
step:150/2285 train_time:9055ms step_avg:60.37ms
step:151/2285 train_time:9116ms step_avg:60.37ms
step:152/2285 train_time:9175ms step_avg:60.36ms
step:153/2285 train_time:9237ms step_avg:60.37ms
step:154/2285 train_time:9296ms step_avg:60.37ms
step:155/2285 train_time:9358ms step_avg:60.37ms
step:156/2285 train_time:9417ms step_avg:60.37ms
step:157/2285 train_time:9478ms step_avg:60.37ms
step:158/2285 train_time:9537ms step_avg:60.36ms
step:159/2285 train_time:9599ms step_avg:60.37ms
step:160/2285 train_time:9658ms step_avg:60.36ms
step:161/2285 train_time:9719ms step_avg:60.37ms
step:162/2285 train_time:9779ms step_avg:60.36ms
step:163/2285 train_time:9841ms step_avg:60.38ms
step:164/2285 train_time:9900ms step_avg:60.37ms
step:165/2285 train_time:9963ms step_avg:60.38ms
step:166/2285 train_time:10022ms step_avg:60.38ms
step:167/2285 train_time:10084ms step_avg:60.38ms
step:168/2285 train_time:10144ms step_avg:60.38ms
step:169/2285 train_time:10206ms step_avg:60.39ms
step:170/2285 train_time:10264ms step_avg:60.38ms
step:171/2285 train_time:10326ms step_avg:60.39ms
step:172/2285 train_time:10385ms step_avg:60.38ms
step:173/2285 train_time:10447ms step_avg:60.39ms
step:174/2285 train_time:10505ms step_avg:60.37ms
step:175/2285 train_time:10567ms step_avg:60.38ms
step:176/2285 train_time:10626ms step_avg:60.37ms
step:177/2285 train_time:10688ms step_avg:60.38ms
step:178/2285 train_time:10747ms step_avg:60.38ms
step:179/2285 train_time:10809ms step_avg:60.39ms
step:180/2285 train_time:10869ms step_avg:60.38ms
step:181/2285 train_time:10930ms step_avg:60.39ms
step:182/2285 train_time:10989ms step_avg:60.38ms
step:183/2285 train_time:11050ms step_avg:60.38ms
step:184/2285 train_time:11109ms step_avg:60.37ms
step:185/2285 train_time:11170ms step_avg:60.38ms
step:186/2285 train_time:11228ms step_avg:60.36ms
step:187/2285 train_time:11289ms step_avg:60.37ms
step:188/2285 train_time:11348ms step_avg:60.36ms
step:189/2285 train_time:11409ms step_avg:60.36ms
step:190/2285 train_time:11467ms step_avg:60.35ms
step:191/2285 train_time:11529ms step_avg:60.36ms
step:192/2285 train_time:11587ms step_avg:60.35ms
step:193/2285 train_time:11649ms step_avg:60.36ms
step:194/2285 train_time:11707ms step_avg:60.35ms
step:195/2285 train_time:11769ms step_avg:60.36ms
step:196/2285 train_time:11828ms step_avg:60.35ms
step:197/2285 train_time:11890ms step_avg:60.36ms
step:198/2285 train_time:11949ms step_avg:60.35ms
step:199/2285 train_time:12011ms step_avg:60.36ms
step:200/2285 train_time:12069ms step_avg:60.35ms
step:201/2285 train_time:12130ms step_avg:60.35ms
step:202/2285 train_time:12189ms step_avg:60.34ms
step:203/2285 train_time:12250ms step_avg:60.34ms
step:204/2285 train_time:12308ms step_avg:60.33ms
step:205/2285 train_time:12369ms step_avg:60.34ms
step:206/2285 train_time:12427ms step_avg:60.33ms
step:207/2285 train_time:12489ms step_avg:60.33ms
step:208/2285 train_time:12548ms step_avg:60.33ms
step:209/2285 train_time:12609ms step_avg:60.33ms
step:210/2285 train_time:12668ms step_avg:60.32ms
step:211/2285 train_time:12730ms step_avg:60.33ms
step:212/2285 train_time:12788ms step_avg:60.32ms
step:213/2285 train_time:12850ms step_avg:60.33ms
step:214/2285 train_time:12909ms step_avg:60.32ms
step:215/2285 train_time:12970ms step_avg:60.33ms
step:216/2285 train_time:13029ms step_avg:60.32ms
step:217/2285 train_time:13090ms step_avg:60.32ms
step:218/2285 train_time:13149ms step_avg:60.31ms
step:219/2285 train_time:13210ms step_avg:60.32ms
step:220/2285 train_time:13268ms step_avg:60.31ms
step:221/2285 train_time:13329ms step_avg:60.31ms
step:222/2285 train_time:13388ms step_avg:60.31ms
step:223/2285 train_time:13449ms step_avg:60.31ms
step:224/2285 train_time:13507ms step_avg:60.30ms
step:225/2285 train_time:13569ms step_avg:60.31ms
step:226/2285 train_time:13628ms step_avg:60.30ms
step:227/2285 train_time:13689ms step_avg:60.30ms
step:228/2285 train_time:13748ms step_avg:60.30ms
step:229/2285 train_time:13810ms step_avg:60.31ms
step:230/2285 train_time:13869ms step_avg:60.30ms
step:231/2285 train_time:13931ms step_avg:60.31ms
step:232/2285 train_time:13989ms step_avg:60.30ms
step:233/2285 train_time:14050ms step_avg:60.30ms
step:234/2285 train_time:14109ms step_avg:60.30ms
step:235/2285 train_time:14170ms step_avg:60.30ms
step:236/2285 train_time:14228ms step_avg:60.29ms
step:237/2285 train_time:14290ms step_avg:60.30ms
step:238/2285 train_time:14349ms step_avg:60.29ms
step:239/2285 train_time:14410ms step_avg:60.29ms
step:240/2285 train_time:14468ms step_avg:60.28ms
step:241/2285 train_time:14529ms step_avg:60.29ms
step:242/2285 train_time:14588ms step_avg:60.28ms
step:243/2285 train_time:14649ms step_avg:60.28ms
step:244/2285 train_time:14708ms step_avg:60.28ms
step:245/2285 train_time:14770ms step_avg:60.29ms
step:246/2285 train_time:14829ms step_avg:60.28ms
step:247/2285 train_time:14890ms step_avg:60.29ms
step:248/2285 train_time:14949ms step_avg:60.28ms
step:249/2285 train_time:15011ms step_avg:60.28ms
step:250/2285 train_time:15069ms step_avg:60.28ms
step:250/2285 val_loss:4.1789 train_time:15132ms step_avg:60.53ms
step:251/2285 train_time:15150ms step_avg:60.36ms
step:252/2285 train_time:15193ms step_avg:60.29ms
step:253/2285 train_time:15262ms step_avg:60.32ms
step:254/2285 train_time:15323ms step_avg:60.33ms
step:255/2285 train_time:15386ms step_avg:60.34ms
step:256/2285 train_time:15445ms step_avg:60.33ms
step:257/2285 train_time:15505ms step_avg:60.33ms
step:258/2285 train_time:15563ms step_avg:60.32ms
step:259/2285 train_time:15624ms step_avg:60.32ms
step:260/2285 train_time:15682ms step_avg:60.32ms
step:261/2285 train_time:15743ms step_avg:60.32ms
step:262/2285 train_time:15802ms step_avg:60.31ms
step:263/2285 train_time:15862ms step_avg:60.31ms
step:264/2285 train_time:15920ms step_avg:60.30ms
step:265/2285 train_time:15981ms step_avg:60.30ms
step:266/2285 train_time:16039ms step_avg:60.30ms
step:267/2285 train_time:16101ms step_avg:60.30ms
step:268/2285 train_time:16161ms step_avg:60.30ms
step:269/2285 train_time:16224ms step_avg:60.31ms
step:270/2285 train_time:16285ms step_avg:60.32ms
step:271/2285 train_time:16347ms step_avg:60.32ms
step:272/2285 train_time:16406ms step_avg:60.32ms
step:273/2285 train_time:16468ms step_avg:60.32ms
step:274/2285 train_time:16527ms step_avg:60.32ms
step:275/2285 train_time:16588ms step_avg:60.32ms
step:276/2285 train_time:16646ms step_avg:60.31ms
step:277/2285 train_time:16707ms step_avg:60.31ms
step:278/2285 train_time:16766ms step_avg:60.31ms
step:279/2285 train_time:16827ms step_avg:60.31ms
step:280/2285 train_time:16886ms step_avg:60.31ms
step:281/2285 train_time:16947ms step_avg:60.31ms
step:282/2285 train_time:17006ms step_avg:60.30ms
step:283/2285 train_time:17068ms step_avg:60.31ms
step:284/2285 train_time:17128ms step_avg:60.31ms
step:285/2285 train_time:17191ms step_avg:60.32ms
step:286/2285 train_time:17250ms step_avg:60.31ms
step:287/2285 train_time:17312ms step_avg:60.32ms
step:288/2285 train_time:17372ms step_avg:60.32ms
step:289/2285 train_time:17434ms step_avg:60.32ms
step:290/2285 train_time:17493ms step_avg:60.32ms
step:291/2285 train_time:17554ms step_avg:60.32ms
step:292/2285 train_time:17612ms step_avg:60.32ms
step:293/2285 train_time:17674ms step_avg:60.32ms
step:294/2285 train_time:17733ms step_avg:60.32ms
step:295/2285 train_time:17794ms step_avg:60.32ms
step:296/2285 train_time:17853ms step_avg:60.31ms
step:297/2285 train_time:17915ms step_avg:60.32ms
step:298/2285 train_time:17973ms step_avg:60.31ms
step:299/2285 train_time:18035ms step_avg:60.32ms
step:300/2285 train_time:18094ms step_avg:60.31ms
step:301/2285 train_time:18156ms step_avg:60.32ms
step:302/2285 train_time:18215ms step_avg:60.31ms
step:303/2285 train_time:18277ms step_avg:60.32ms
step:304/2285 train_time:18335ms step_avg:60.31ms
step:305/2285 train_time:18397ms step_avg:60.32ms
step:306/2285 train_time:18455ms step_avg:60.31ms
step:307/2285 train_time:18517ms step_avg:60.31ms
step:308/2285 train_time:18575ms step_avg:60.31ms
step:309/2285 train_time:18637ms step_avg:60.31ms
step:310/2285 train_time:18695ms step_avg:60.31ms
step:311/2285 train_time:18756ms step_avg:60.31ms
step:312/2285 train_time:18814ms step_avg:60.30ms
step:313/2285 train_time:18876ms step_avg:60.31ms
step:314/2285 train_time:18935ms step_avg:60.30ms
step:315/2285 train_time:18996ms step_avg:60.31ms
step:316/2285 train_time:19054ms step_avg:60.30ms
step:317/2285 train_time:19117ms step_avg:60.30ms
step:318/2285 train_time:19176ms step_avg:60.30ms
step:319/2285 train_time:19237ms step_avg:60.31ms
step:320/2285 train_time:19296ms step_avg:60.30ms
step:321/2285 train_time:19357ms step_avg:60.30ms
step:322/2285 train_time:19415ms step_avg:60.30ms
step:323/2285 train_time:19477ms step_avg:60.30ms
step:324/2285 train_time:19535ms step_avg:60.29ms
step:325/2285 train_time:19597ms step_avg:60.30ms
step:326/2285 train_time:19655ms step_avg:60.29ms
step:327/2285 train_time:19716ms step_avg:60.29ms
step:328/2285 train_time:19775ms step_avg:60.29ms
step:329/2285 train_time:19835ms step_avg:60.29ms
step:330/2285 train_time:19894ms step_avg:60.28ms
step:331/2285 train_time:19955ms step_avg:60.29ms
step:332/2285 train_time:20013ms step_avg:60.28ms
step:333/2285 train_time:20076ms step_avg:60.29ms
step:334/2285 train_time:20135ms step_avg:60.28ms
step:335/2285 train_time:20197ms step_avg:60.29ms
step:336/2285 train_time:20255ms step_avg:60.28ms
step:337/2285 train_time:20317ms step_avg:60.29ms
step:338/2285 train_time:20376ms step_avg:60.28ms
step:339/2285 train_time:20437ms step_avg:60.29ms
step:340/2285 train_time:20496ms step_avg:60.28ms
step:341/2285 train_time:20557ms step_avg:60.28ms
step:342/2285 train_time:20615ms step_avg:60.28ms
step:343/2285 train_time:20676ms step_avg:60.28ms
step:344/2285 train_time:20735ms step_avg:60.28ms
step:345/2285 train_time:20795ms step_avg:60.28ms
step:346/2285 train_time:20854ms step_avg:60.27ms
step:347/2285 train_time:20915ms step_avg:60.27ms
step:348/2285 train_time:20974ms step_avg:60.27ms
step:349/2285 train_time:21036ms step_avg:60.27ms
step:350/2285 train_time:21094ms step_avg:60.27ms
step:351/2285 train_time:21156ms step_avg:60.27ms
step:352/2285 train_time:21214ms step_avg:60.27ms
step:353/2285 train_time:21276ms step_avg:60.27ms
step:354/2285 train_time:21335ms step_avg:60.27ms
step:355/2285 train_time:21397ms step_avg:60.27ms
step:356/2285 train_time:21455ms step_avg:60.27ms
step:357/2285 train_time:21517ms step_avg:60.27ms
step:358/2285 train_time:21576ms step_avg:60.27ms
step:359/2285 train_time:21637ms step_avg:60.27ms
step:360/2285 train_time:21696ms step_avg:60.27ms
step:361/2285 train_time:21757ms step_avg:60.27ms
step:362/2285 train_time:21815ms step_avg:60.26ms
step:363/2285 train_time:21876ms step_avg:60.27ms
step:364/2285 train_time:21935ms step_avg:60.26ms
step:365/2285 train_time:21996ms step_avg:60.26ms
step:366/2285 train_time:22055ms step_avg:60.26ms
step:367/2285 train_time:22116ms step_avg:60.26ms
step:368/2285 train_time:22174ms step_avg:60.26ms
step:369/2285 train_time:22236ms step_avg:60.26ms
step:370/2285 train_time:22295ms step_avg:60.26ms
step:371/2285 train_time:22356ms step_avg:60.26ms
step:372/2285 train_time:22414ms step_avg:60.25ms
step:373/2285 train_time:22476ms step_avg:60.26ms
step:374/2285 train_time:22534ms step_avg:60.25ms
step:375/2285 train_time:22597ms step_avg:60.26ms
step:376/2285 train_time:22655ms step_avg:60.25ms
step:377/2285 train_time:22716ms step_avg:60.26ms
step:378/2285 train_time:22775ms step_avg:60.25ms
step:379/2285 train_time:22836ms step_avg:60.25ms
step:380/2285 train_time:22894ms step_avg:60.25ms
step:381/2285 train_time:22956ms step_avg:60.25ms
step:382/2285 train_time:23014ms step_avg:60.25ms
step:383/2285 train_time:23076ms step_avg:60.25ms
step:384/2285 train_time:23134ms step_avg:60.25ms
step:385/2285 train_time:23195ms step_avg:60.25ms
step:386/2285 train_time:23254ms step_avg:60.24ms
step:387/2285 train_time:23315ms step_avg:60.25ms
step:388/2285 train_time:23374ms step_avg:60.24ms
step:389/2285 train_time:23436ms step_avg:60.25ms
step:390/2285 train_time:23495ms step_avg:60.24ms
step:391/2285 train_time:23555ms step_avg:60.24ms
step:392/2285 train_time:23614ms step_avg:60.24ms
step:393/2285 train_time:23676ms step_avg:60.24ms
step:394/2285 train_time:23734ms step_avg:60.24ms
step:395/2285 train_time:23796ms step_avg:60.24ms
step:396/2285 train_time:23854ms step_avg:60.24ms
step:397/2285 train_time:23915ms step_avg:60.24ms
step:398/2285 train_time:23974ms step_avg:60.24ms
step:399/2285 train_time:24035ms step_avg:60.24ms
step:400/2285 train_time:24094ms step_avg:60.24ms
step:401/2285 train_time:24155ms step_avg:60.24ms
step:402/2285 train_time:24214ms step_avg:60.23ms
step:403/2285 train_time:24276ms step_avg:60.24ms
step:404/2285 train_time:24334ms step_avg:60.23ms
step:405/2285 train_time:24396ms step_avg:60.24ms
step:406/2285 train_time:24455ms step_avg:60.23ms
step:407/2285 train_time:24516ms step_avg:60.24ms
step:408/2285 train_time:24575ms step_avg:60.23ms
step:409/2285 train_time:24637ms step_avg:60.24ms
step:410/2285 train_time:24696ms step_avg:60.23ms
step:411/2285 train_time:24757ms step_avg:60.24ms
step:412/2285 train_time:24815ms step_avg:60.23ms
step:413/2285 train_time:24876ms step_avg:60.23ms
step:414/2285 train_time:24934ms step_avg:60.23ms
step:415/2285 train_time:24995ms step_avg:60.23ms
step:416/2285 train_time:25054ms step_avg:60.23ms
step:417/2285 train_time:25115ms step_avg:60.23ms
step:418/2285 train_time:25174ms step_avg:60.22ms
step:419/2285 train_time:25235ms step_avg:60.23ms
step:420/2285 train_time:25294ms step_avg:60.22ms
step:421/2285 train_time:25356ms step_avg:60.23ms
step:422/2285 train_time:25415ms step_avg:60.22ms
step:423/2285 train_time:25476ms step_avg:60.23ms
step:424/2285 train_time:25536ms step_avg:60.23ms
step:425/2285 train_time:25598ms step_avg:60.23ms
step:426/2285 train_time:25658ms step_avg:60.23ms
step:427/2285 train_time:25718ms step_avg:60.23ms
step:428/2285 train_time:25776ms step_avg:60.22ms
step:429/2285 train_time:25837ms step_avg:60.23ms
step:430/2285 train_time:25896ms step_avg:60.22ms
step:431/2285 train_time:25957ms step_avg:60.23ms
step:432/2285 train_time:26016ms step_avg:60.22ms
step:433/2285 train_time:26077ms step_avg:60.22ms
step:434/2285 train_time:26135ms step_avg:60.22ms
step:435/2285 train_time:26197ms step_avg:60.22ms
step:436/2285 train_time:26255ms step_avg:60.22ms
step:437/2285 train_time:26316ms step_avg:60.22ms
step:438/2285 train_time:26375ms step_avg:60.22ms
step:439/2285 train_time:26436ms step_avg:60.22ms
step:440/2285 train_time:26495ms step_avg:60.22ms
step:441/2285 train_time:26557ms step_avg:60.22ms
step:442/2285 train_time:26616ms step_avg:60.22ms
step:443/2285 train_time:26677ms step_avg:60.22ms
step:444/2285 train_time:26736ms step_avg:60.22ms
step:445/2285 train_time:26797ms step_avg:60.22ms
step:446/2285 train_time:26856ms step_avg:60.21ms
step:447/2285 train_time:26917ms step_avg:60.22ms
step:448/2285 train_time:26975ms step_avg:60.21ms
step:449/2285 train_time:27036ms step_avg:60.21ms
step:450/2285 train_time:27094ms step_avg:60.21ms
step:451/2285 train_time:27156ms step_avg:60.21ms
step:452/2285 train_time:27214ms step_avg:60.21ms
step:453/2285 train_time:27276ms step_avg:60.21ms
step:454/2285 train_time:27334ms step_avg:60.21ms
step:455/2285 train_time:27396ms step_avg:60.21ms
step:456/2285 train_time:27455ms step_avg:60.21ms
step:457/2285 train_time:27517ms step_avg:60.21ms
step:458/2285 train_time:27575ms step_avg:60.21ms
step:459/2285 train_time:27637ms step_avg:60.21ms
step:460/2285 train_time:27696ms step_avg:60.21ms
step:461/2285 train_time:27757ms step_avg:60.21ms
step:462/2285 train_time:27816ms step_avg:60.21ms
step:463/2285 train_time:27877ms step_avg:60.21ms
step:464/2285 train_time:27935ms step_avg:60.21ms
step:465/2285 train_time:27997ms step_avg:60.21ms
step:466/2285 train_time:28056ms step_avg:60.21ms
step:467/2285 train_time:28117ms step_avg:60.21ms
step:468/2285 train_time:28175ms step_avg:60.20ms
step:469/2285 train_time:28237ms step_avg:60.21ms
step:470/2285 train_time:28295ms step_avg:60.20ms
step:471/2285 train_time:28356ms step_avg:60.20ms
step:472/2285 train_time:28415ms step_avg:60.20ms
step:473/2285 train_time:28476ms step_avg:60.20ms
step:474/2285 train_time:28535ms step_avg:60.20ms
step:475/2285 train_time:28596ms step_avg:60.20ms
step:476/2285 train_time:28655ms step_avg:60.20ms
step:477/2285 train_time:28716ms step_avg:60.20ms
step:478/2285 train_time:28774ms step_avg:60.20ms
step:479/2285 train_time:28836ms step_avg:60.20ms
step:480/2285 train_time:28895ms step_avg:60.20ms
step:481/2285 train_time:28957ms step_avg:60.20ms
step:482/2285 train_time:29016ms step_avg:60.20ms
step:483/2285 train_time:29077ms step_avg:60.20ms
step:484/2285 train_time:29136ms step_avg:60.20ms
step:485/2285 train_time:29197ms step_avg:60.20ms
step:486/2285 train_time:29256ms step_avg:60.20ms
step:487/2285 train_time:29317ms step_avg:60.20ms
step:488/2285 train_time:29375ms step_avg:60.20ms
step:489/2285 train_time:29437ms step_avg:60.20ms
step:490/2285 train_time:29496ms step_avg:60.20ms
step:491/2285 train_time:29557ms step_avg:60.20ms
step:492/2285 train_time:29616ms step_avg:60.19ms
step:493/2285 train_time:29677ms step_avg:60.20ms
step:494/2285 train_time:29736ms step_avg:60.19ms
step:495/2285 train_time:29797ms step_avg:60.20ms
step:496/2285 train_time:29855ms step_avg:60.19ms
step:497/2285 train_time:29916ms step_avg:60.19ms
step:498/2285 train_time:29975ms step_avg:60.19ms
step:499/2285 train_time:30037ms step_avg:60.19ms
step:500/2285 train_time:30096ms step_avg:60.19ms
step:500/2285 val_loss:3.8117 train_time:30158ms step_avg:60.32ms
step:501/2285 train_time:30177ms step_avg:60.23ms
step:502/2285 train_time:30219ms step_avg:60.20ms
step:503/2285 train_time:30282ms step_avg:60.20ms
step:504/2285 train_time:30343ms step_avg:60.21ms
step:505/2285 train_time:30406ms step_avg:60.21ms
step:506/2285 train_time:30465ms step_avg:60.21ms
step:507/2285 train_time:30526ms step_avg:60.21ms
step:508/2285 train_time:30585ms step_avg:60.21ms
step:509/2285 train_time:30646ms step_avg:60.21ms
step:510/2285 train_time:30704ms step_avg:60.20ms
step:511/2285 train_time:30765ms step_avg:60.20ms
step:512/2285 train_time:30823ms step_avg:60.20ms
step:513/2285 train_time:30884ms step_avg:60.20ms
step:514/2285 train_time:30943ms step_avg:60.20ms
step:515/2285 train_time:31004ms step_avg:60.20ms
step:516/2285 train_time:31064ms step_avg:60.20ms
step:517/2285 train_time:31126ms step_avg:60.21ms
step:518/2285 train_time:31187ms step_avg:60.21ms
step:519/2285 train_time:31250ms step_avg:60.21ms
step:520/2285 train_time:31310ms step_avg:60.21ms
step:521/2285 train_time:31371ms step_avg:60.21ms
step:522/2285 train_time:31430ms step_avg:60.21ms
step:523/2285 train_time:31491ms step_avg:60.21ms
step:524/2285 train_time:31550ms step_avg:60.21ms
step:525/2285 train_time:31611ms step_avg:60.21ms
step:526/2285 train_time:31669ms step_avg:60.21ms
step:527/2285 train_time:31730ms step_avg:60.21ms
step:528/2285 train_time:31789ms step_avg:60.21ms
step:529/2285 train_time:31850ms step_avg:60.21ms
step:530/2285 train_time:31908ms step_avg:60.20ms
step:531/2285 train_time:31969ms step_avg:60.21ms
step:532/2285 train_time:32028ms step_avg:60.20ms
step:533/2285 train_time:32091ms step_avg:60.21ms
step:534/2285 train_time:32151ms step_avg:60.21ms
step:535/2285 train_time:32212ms step_avg:60.21ms
step:536/2285 train_time:32271ms step_avg:60.21ms
step:537/2285 train_time:32333ms step_avg:60.21ms
step:538/2285 train_time:32392ms step_avg:60.21ms
step:539/2285 train_time:32453ms step_avg:60.21ms
step:540/2285 train_time:32512ms step_avg:60.21ms
step:541/2285 train_time:32573ms step_avg:60.21ms
step:542/2285 train_time:32631ms step_avg:60.20ms
step:543/2285 train_time:32692ms step_avg:60.21ms
step:544/2285 train_time:32750ms step_avg:60.20ms
step:545/2285 train_time:32811ms step_avg:60.20ms
step:546/2285 train_time:32870ms step_avg:60.20ms
step:547/2285 train_time:32930ms step_avg:60.20ms
step:548/2285 train_time:32989ms step_avg:60.20ms
step:549/2285 train_time:33050ms step_avg:60.20ms
step:550/2285 train_time:33109ms step_avg:60.20ms
step:551/2285 train_time:33171ms step_avg:60.20ms
step:552/2285 train_time:33230ms step_avg:60.20ms
step:553/2285 train_time:33292ms step_avg:60.20ms
step:554/2285 train_time:33352ms step_avg:60.20ms
step:555/2285 train_time:33414ms step_avg:60.21ms
step:556/2285 train_time:33473ms step_avg:60.20ms
step:557/2285 train_time:33534ms step_avg:60.20ms
step:558/2285 train_time:33592ms step_avg:60.20ms
step:559/2285 train_time:33653ms step_avg:60.20ms
step:560/2285 train_time:33711ms step_avg:60.20ms
step:561/2285 train_time:33772ms step_avg:60.20ms
step:562/2285 train_time:33830ms step_avg:60.20ms
step:563/2285 train_time:33891ms step_avg:60.20ms
step:564/2285 train_time:33950ms step_avg:60.19ms
step:565/2285 train_time:34011ms step_avg:60.20ms
step:566/2285 train_time:34069ms step_avg:60.19ms
step:567/2285 train_time:34131ms step_avg:60.20ms
step:568/2285 train_time:34190ms step_avg:60.19ms
step:569/2285 train_time:34252ms step_avg:60.20ms
step:570/2285 train_time:34310ms step_avg:60.19ms
step:571/2285 train_time:34372ms step_avg:60.20ms
step:572/2285 train_time:34431ms step_avg:60.19ms
step:573/2285 train_time:34493ms step_avg:60.20ms
step:574/2285 train_time:34553ms step_avg:60.20ms
step:575/2285 train_time:34613ms step_avg:60.20ms
step:576/2285 train_time:34672ms step_avg:60.19ms
step:577/2285 train_time:34733ms step_avg:60.20ms
step:578/2285 train_time:34791ms step_avg:60.19ms
step:579/2285 train_time:34852ms step_avg:60.19ms
step:580/2285 train_time:34910ms step_avg:60.19ms
step:581/2285 train_time:34971ms step_avg:60.19ms
step:582/2285 train_time:35030ms step_avg:60.19ms
step:583/2285 train_time:35091ms step_avg:60.19ms
step:584/2285 train_time:35150ms step_avg:60.19ms
step:585/2285 train_time:35211ms step_avg:60.19ms
step:586/2285 train_time:35270ms step_avg:60.19ms
step:587/2285 train_time:35332ms step_avg:60.19ms
step:588/2285 train_time:35391ms step_avg:60.19ms
step:589/2285 train_time:35452ms step_avg:60.19ms
step:590/2285 train_time:35511ms step_avg:60.19ms
step:591/2285 train_time:35572ms step_avg:60.19ms
step:592/2285 train_time:35631ms step_avg:60.19ms
step:593/2285 train_time:35692ms step_avg:60.19ms
step:594/2285 train_time:35751ms step_avg:60.19ms
step:595/2285 train_time:35812ms step_avg:60.19ms
step:596/2285 train_time:35870ms step_avg:60.18ms
step:597/2285 train_time:35931ms step_avg:60.19ms
step:598/2285 train_time:35989ms step_avg:60.18ms
step:599/2285 train_time:36051ms step_avg:60.18ms
step:600/2285 train_time:36109ms step_avg:60.18ms
step:601/2285 train_time:36171ms step_avg:60.18ms
step:602/2285 train_time:36229ms step_avg:60.18ms
step:603/2285 train_time:36292ms step_avg:60.18ms
step:604/2285 train_time:36351ms step_avg:60.18ms
step:605/2285 train_time:36412ms step_avg:60.19ms
step:606/2285 train_time:36471ms step_avg:60.18ms
step:607/2285 train_time:36533ms step_avg:60.19ms
step:608/2285 train_time:36591ms step_avg:60.18ms
step:609/2285 train_time:36653ms step_avg:60.19ms
step:610/2285 train_time:36711ms step_avg:60.18ms
step:611/2285 train_time:36772ms step_avg:60.18ms
step:612/2285 train_time:36831ms step_avg:60.18ms
step:613/2285 train_time:36892ms step_avg:60.18ms
step:614/2285 train_time:36950ms step_avg:60.18ms
step:615/2285 train_time:37012ms step_avg:60.18ms
step:616/2285 train_time:37071ms step_avg:60.18ms
step:617/2285 train_time:37132ms step_avg:60.18ms
step:618/2285 train_time:37191ms step_avg:60.18ms
step:619/2285 train_time:37252ms step_avg:60.18ms
step:620/2285 train_time:37311ms step_avg:60.18ms
step:621/2285 train_time:37372ms step_avg:60.18ms
step:622/2285 train_time:37431ms step_avg:60.18ms
step:623/2285 train_time:37492ms step_avg:60.18ms
step:624/2285 train_time:37551ms step_avg:60.18ms
step:625/2285 train_time:37612ms step_avg:60.18ms
step:626/2285 train_time:37671ms step_avg:60.18ms
step:627/2285 train_time:37733ms step_avg:60.18ms
step:628/2285 train_time:37792ms step_avg:60.18ms
step:629/2285 train_time:37853ms step_avg:60.18ms
step:630/2285 train_time:37911ms step_avg:60.18ms
step:631/2285 train_time:37972ms step_avg:60.18ms
step:632/2285 train_time:38030ms step_avg:60.17ms
step:633/2285 train_time:38092ms step_avg:60.18ms
step:634/2285 train_time:38150ms step_avg:60.17ms
step:635/2285 train_time:38212ms step_avg:60.18ms
step:636/2285 train_time:38271ms step_avg:60.17ms
step:637/2285 train_time:38332ms step_avg:60.18ms
step:638/2285 train_time:38391ms step_avg:60.17ms
step:639/2285 train_time:38452ms step_avg:60.18ms
step:640/2285 train_time:38512ms step_avg:60.17ms
step:641/2285 train_time:38573ms step_avg:60.18ms
step:642/2285 train_time:38632ms step_avg:60.17ms
step:643/2285 train_time:38693ms step_avg:60.18ms
step:644/2285 train_time:38752ms step_avg:60.17ms
step:645/2285 train_time:38813ms step_avg:60.18ms
step:646/2285 train_time:38871ms step_avg:60.17ms
step:647/2285 train_time:38932ms step_avg:60.17ms
step:648/2285 train_time:38991ms step_avg:60.17ms
step:649/2285 train_time:39052ms step_avg:60.17ms
step:650/2285 train_time:39111ms step_avg:60.17ms
step:651/2285 train_time:39173ms step_avg:60.17ms
step:652/2285 train_time:39232ms step_avg:60.17ms
step:653/2285 train_time:39293ms step_avg:60.17ms
step:654/2285 train_time:39352ms step_avg:60.17ms
step:655/2285 train_time:39413ms step_avg:60.17ms
step:656/2285 train_time:39472ms step_avg:60.17ms
step:657/2285 train_time:39533ms step_avg:60.17ms
step:658/2285 train_time:39591ms step_avg:60.17ms
step:659/2285 train_time:39653ms step_avg:60.17ms
step:660/2285 train_time:39711ms step_avg:60.17ms
step:661/2285 train_time:39772ms step_avg:60.17ms
step:662/2285 train_time:39831ms step_avg:60.17ms
step:663/2285 train_time:39892ms step_avg:60.17ms
step:664/2285 train_time:39950ms step_avg:60.17ms
step:665/2285 train_time:40011ms step_avg:60.17ms
step:666/2285 train_time:40070ms step_avg:60.17ms
step:667/2285 train_time:40131ms step_avg:60.17ms
step:668/2285 train_time:40190ms step_avg:60.16ms
step:669/2285 train_time:40251ms step_avg:60.17ms
step:670/2285 train_time:40310ms step_avg:60.16ms
step:671/2285 train_time:40371ms step_avg:60.17ms
step:672/2285 train_time:40430ms step_avg:60.16ms
step:673/2285 train_time:40492ms step_avg:60.17ms
step:674/2285 train_time:40551ms step_avg:60.16ms
step:675/2285 train_time:40612ms step_avg:60.17ms
step:676/2285 train_time:40671ms step_avg:60.16ms
step:677/2285 train_time:40732ms step_avg:60.17ms
step:678/2285 train_time:40791ms step_avg:60.16ms
step:679/2285 train_time:40852ms step_avg:60.17ms
step:680/2285 train_time:40911ms step_avg:60.16ms
step:681/2285 train_time:40972ms step_avg:60.16ms
step:682/2285 train_time:41030ms step_avg:60.16ms
step:683/2285 train_time:41092ms step_avg:60.16ms
step:684/2285 train_time:41151ms step_avg:60.16ms
step:685/2285 train_time:41213ms step_avg:60.16ms
step:686/2285 train_time:41271ms step_avg:60.16ms
step:687/2285 train_time:41332ms step_avg:60.16ms
step:688/2285 train_time:41391ms step_avg:60.16ms
step:689/2285 train_time:41452ms step_avg:60.16ms
step:690/2285 train_time:41511ms step_avg:60.16ms
step:691/2285 train_time:41572ms step_avg:60.16ms
step:692/2285 train_time:41630ms step_avg:60.16ms
step:693/2285 train_time:41693ms step_avg:60.16ms
step:694/2285 train_time:41752ms step_avg:60.16ms
step:695/2285 train_time:41814ms step_avg:60.16ms
step:696/2285 train_time:41872ms step_avg:60.16ms
step:697/2285 train_time:41933ms step_avg:60.16ms
step:698/2285 train_time:41992ms step_avg:60.16ms
step:699/2285 train_time:42053ms step_avg:60.16ms
step:700/2285 train_time:42112ms step_avg:60.16ms
step:701/2285 train_time:42173ms step_avg:60.16ms
step:702/2285 train_time:42232ms step_avg:60.16ms
step:703/2285 train_time:42293ms step_avg:60.16ms
step:704/2285 train_time:42353ms step_avg:60.16ms
step:705/2285 train_time:42414ms step_avg:60.16ms
step:706/2285 train_time:42472ms step_avg:60.16ms
step:707/2285 train_time:42533ms step_avg:60.16ms
step:708/2285 train_time:42592ms step_avg:60.16ms
step:709/2285 train_time:42653ms step_avg:60.16ms
step:710/2285 train_time:42712ms step_avg:60.16ms
step:711/2285 train_time:42773ms step_avg:60.16ms
step:712/2285 train_time:42832ms step_avg:60.16ms
step:713/2285 train_time:42893ms step_avg:60.16ms
step:714/2285 train_time:42952ms step_avg:60.16ms
step:715/2285 train_time:43013ms step_avg:60.16ms
step:716/2285 train_time:43071ms step_avg:60.16ms
step:717/2285 train_time:43132ms step_avg:60.16ms
step:718/2285 train_time:43191ms step_avg:60.15ms
step:719/2285 train_time:43252ms step_avg:60.16ms
step:720/2285 train_time:43311ms step_avg:60.15ms
step:721/2285 train_time:43373ms step_avg:60.16ms
step:722/2285 train_time:43431ms step_avg:60.15ms
step:723/2285 train_time:43493ms step_avg:60.16ms
step:724/2285 train_time:43552ms step_avg:60.15ms
step:725/2285 train_time:43613ms step_avg:60.16ms
step:726/2285 train_time:43672ms step_avg:60.15ms
step:727/2285 train_time:43732ms step_avg:60.15ms
step:728/2285 train_time:43791ms step_avg:60.15ms
step:729/2285 train_time:43853ms step_avg:60.15ms
step:730/2285 train_time:43911ms step_avg:60.15ms
step:731/2285 train_time:43972ms step_avg:60.15ms
step:732/2285 train_time:44031ms step_avg:60.15ms
step:733/2285 train_time:44092ms step_avg:60.15ms
step:734/2285 train_time:44152ms step_avg:60.15ms
step:735/2285 train_time:44212ms step_avg:60.15ms
step:736/2285 train_time:44271ms step_avg:60.15ms
step:737/2285 train_time:44332ms step_avg:60.15ms
step:738/2285 train_time:44391ms step_avg:60.15ms
step:739/2285 train_time:44452ms step_avg:60.15ms
step:740/2285 train_time:44512ms step_avg:60.15ms
step:741/2285 train_time:44573ms step_avg:60.15ms
step:742/2285 train_time:44632ms step_avg:60.15ms
step:743/2285 train_time:44693ms step_avg:60.15ms
step:744/2285 train_time:44752ms step_avg:60.15ms
step:745/2285 train_time:44813ms step_avg:60.15ms
step:746/2285 train_time:44872ms step_avg:60.15ms
step:747/2285 train_time:44933ms step_avg:60.15ms
step:748/2285 train_time:44991ms step_avg:60.15ms
step:749/2285 train_time:45052ms step_avg:60.15ms
step:750/2285 train_time:45111ms step_avg:60.15ms
step:750/2285 val_loss:3.6712 train_time:45174ms step_avg:60.23ms
step:751/2285 train_time:45193ms step_avg:60.18ms
step:752/2285 train_time:45235ms step_avg:60.15ms
step:753/2285 train_time:45297ms step_avg:60.16ms
step:754/2285 train_time:45357ms step_avg:60.15ms
step:755/2285 train_time:45420ms step_avg:60.16ms
step:756/2285 train_time:45479ms step_avg:60.16ms
step:757/2285 train_time:45540ms step_avg:60.16ms
step:758/2285 train_time:45599ms step_avg:60.16ms
step:759/2285 train_time:45660ms step_avg:60.16ms
step:760/2285 train_time:45719ms step_avg:60.16ms
step:761/2285 train_time:45780ms step_avg:60.16ms
step:762/2285 train_time:45839ms step_avg:60.16ms
step:763/2285 train_time:45901ms step_avg:60.16ms
step:764/2285 train_time:45960ms step_avg:60.16ms
step:765/2285 train_time:46021ms step_avg:60.16ms
step:766/2285 train_time:46085ms step_avg:60.16ms
step:767/2285 train_time:46153ms step_avg:60.17ms
step:768/2285 train_time:46214ms step_avg:60.17ms
step:769/2285 train_time:46277ms step_avg:60.18ms
step:770/2285 train_time:46337ms step_avg:60.18ms
step:771/2285 train_time:46399ms step_avg:60.18ms
step:772/2285 train_time:46458ms step_avg:60.18ms
step:773/2285 train_time:46520ms step_avg:60.18ms
step:774/2285 train_time:46579ms step_avg:60.18ms
step:775/2285 train_time:46641ms step_avg:60.18ms
step:776/2285 train_time:46700ms step_avg:60.18ms
step:777/2285 train_time:46761ms step_avg:60.18ms
step:778/2285 train_time:46820ms step_avg:60.18ms
step:779/2285 train_time:46881ms step_avg:60.18ms
step:780/2285 train_time:46940ms step_avg:60.18ms
step:781/2285 train_time:47003ms step_avg:60.18ms
step:782/2285 train_time:47065ms step_avg:60.19ms
step:783/2285 train_time:47129ms step_avg:60.19ms
step:784/2285 train_time:47190ms step_avg:60.19ms
step:785/2285 train_time:47253ms step_avg:60.19ms
step:786/2285 train_time:47313ms step_avg:60.20ms
step:787/2285 train_time:47376ms step_avg:60.20ms
step:788/2285 train_time:47437ms step_avg:60.20ms
step:789/2285 train_time:47499ms step_avg:60.20ms
step:790/2285 train_time:47558ms step_avg:60.20ms
step:791/2285 train_time:47620ms step_avg:60.20ms
step:792/2285 train_time:47680ms step_avg:60.20ms
step:793/2285 train_time:47741ms step_avg:60.20ms
step:794/2285 train_time:47800ms step_avg:60.20ms
step:795/2285 train_time:47862ms step_avg:60.20ms
step:796/2285 train_time:47921ms step_avg:60.20ms
step:797/2285 train_time:47984ms step_avg:60.21ms
step:798/2285 train_time:48044ms step_avg:60.21ms
step:799/2285 train_time:48107ms step_avg:60.21ms
step:800/2285 train_time:48168ms step_avg:60.21ms
step:801/2285 train_time:48231ms step_avg:60.21ms
step:802/2285 train_time:48290ms step_avg:60.21ms
step:803/2285 train_time:48352ms step_avg:60.21ms
step:804/2285 train_time:48412ms step_avg:60.21ms
step:805/2285 train_time:48475ms step_avg:60.22ms
step:806/2285 train_time:48535ms step_avg:60.22ms
step:807/2285 train_time:48596ms step_avg:60.22ms
step:808/2285 train_time:48655ms step_avg:60.22ms
step:809/2285 train_time:48718ms step_avg:60.22ms
step:810/2285 train_time:48778ms step_avg:60.22ms
step:811/2285 train_time:48839ms step_avg:60.22ms
step:812/2285 train_time:48899ms step_avg:60.22ms
step:813/2285 train_time:48961ms step_avg:60.22ms
step:814/2285 train_time:49022ms step_avg:60.22ms
step:815/2285 train_time:49085ms step_avg:60.23ms
step:816/2285 train_time:49146ms step_avg:60.23ms
step:817/2285 train_time:49208ms step_avg:60.23ms
step:818/2285 train_time:49269ms step_avg:60.23ms
step:819/2285 train_time:49331ms step_avg:60.23ms
step:820/2285 train_time:49391ms step_avg:60.23ms
step:821/2285 train_time:49453ms step_avg:60.24ms
step:822/2285 train_time:49513ms step_avg:60.23ms
step:823/2285 train_time:49574ms step_avg:60.24ms
step:824/2285 train_time:49634ms step_avg:60.24ms
step:825/2285 train_time:49696ms step_avg:60.24ms
step:826/2285 train_time:49756ms step_avg:60.24ms
step:827/2285 train_time:49818ms step_avg:60.24ms
step:828/2285 train_time:49879ms step_avg:60.24ms
step:829/2285 train_time:49940ms step_avg:60.24ms
step:830/2285 train_time:50000ms step_avg:60.24ms
step:831/2285 train_time:50064ms step_avg:60.25ms
step:832/2285 train_time:50124ms step_avg:60.25ms
step:833/2285 train_time:50187ms step_avg:60.25ms
step:834/2285 train_time:50246ms step_avg:60.25ms
step:835/2285 train_time:50308ms step_avg:60.25ms
step:836/2285 train_time:50368ms step_avg:60.25ms
step:837/2285 train_time:50429ms step_avg:60.25ms
step:838/2285 train_time:50489ms step_avg:60.25ms
step:839/2285 train_time:50551ms step_avg:60.25ms
step:840/2285 train_time:50610ms step_avg:60.25ms
step:841/2285 train_time:50672ms step_avg:60.25ms
step:842/2285 train_time:50732ms step_avg:60.25ms
step:843/2285 train_time:50794ms step_avg:60.25ms
step:844/2285 train_time:50854ms step_avg:60.25ms
step:845/2285 train_time:50916ms step_avg:60.26ms
step:846/2285 train_time:50977ms step_avg:60.26ms
step:847/2285 train_time:51040ms step_avg:60.26ms
step:848/2285 train_time:51100ms step_avg:60.26ms
step:849/2285 train_time:51162ms step_avg:60.26ms
step:850/2285 train_time:51223ms step_avg:60.26ms
step:851/2285 train_time:51286ms step_avg:60.27ms
step:852/2285 train_time:51345ms step_avg:60.26ms
step:853/2285 train_time:51407ms step_avg:60.27ms
step:854/2285 train_time:51467ms step_avg:60.27ms
step:855/2285 train_time:51528ms step_avg:60.27ms
step:856/2285 train_time:51588ms step_avg:60.27ms
step:857/2285 train_time:51650ms step_avg:60.27ms
step:858/2285 train_time:51709ms step_avg:60.27ms
step:859/2285 train_time:51771ms step_avg:60.27ms
step:860/2285 train_time:51830ms step_avg:60.27ms
step:861/2285 train_time:51893ms step_avg:60.27ms
step:862/2285 train_time:51952ms step_avg:60.27ms
step:863/2285 train_time:52015ms step_avg:60.27ms
step:864/2285 train_time:52076ms step_avg:60.27ms
step:865/2285 train_time:52140ms step_avg:60.28ms
step:866/2285 train_time:52200ms step_avg:60.28ms
step:867/2285 train_time:52262ms step_avg:60.28ms
step:868/2285 train_time:52322ms step_avg:60.28ms
step:869/2285 train_time:52385ms step_avg:60.28ms
step:870/2285 train_time:52444ms step_avg:60.28ms
step:871/2285 train_time:52507ms step_avg:60.28ms
step:872/2285 train_time:52566ms step_avg:60.28ms
step:873/2285 train_time:52629ms step_avg:60.28ms
step:874/2285 train_time:52688ms step_avg:60.28ms
step:875/2285 train_time:52750ms step_avg:60.29ms
step:876/2285 train_time:52809ms step_avg:60.28ms
step:877/2285 train_time:52871ms step_avg:60.29ms
step:878/2285 train_time:52931ms step_avg:60.29ms
step:879/2285 train_time:52993ms step_avg:60.29ms
step:880/2285 train_time:53053ms step_avg:60.29ms
step:881/2285 train_time:53116ms step_avg:60.29ms
step:882/2285 train_time:53177ms step_avg:60.29ms
step:883/2285 train_time:53240ms step_avg:60.29ms
step:884/2285 train_time:53300ms step_avg:60.29ms
step:885/2285 train_time:53362ms step_avg:60.30ms
step:886/2285 train_time:53422ms step_avg:60.30ms
step:887/2285 train_time:53484ms step_avg:60.30ms
step:888/2285 train_time:53544ms step_avg:60.30ms
step:889/2285 train_time:53606ms step_avg:60.30ms
step:890/2285 train_time:53666ms step_avg:60.30ms
step:891/2285 train_time:53729ms step_avg:60.30ms
step:892/2285 train_time:53788ms step_avg:60.30ms
step:893/2285 train_time:53850ms step_avg:60.30ms
step:894/2285 train_time:53909ms step_avg:60.30ms
step:895/2285 train_time:53971ms step_avg:60.30ms
step:896/2285 train_time:54031ms step_avg:60.30ms
step:897/2285 train_time:54094ms step_avg:60.31ms
step:898/2285 train_time:54154ms step_avg:60.31ms
step:899/2285 train_time:54217ms step_avg:60.31ms
step:900/2285 train_time:54279ms step_avg:60.31ms
step:901/2285 train_time:54341ms step_avg:60.31ms
step:902/2285 train_time:54400ms step_avg:60.31ms
step:903/2285 train_time:54462ms step_avg:60.31ms
step:904/2285 train_time:54522ms step_avg:60.31ms
step:905/2285 train_time:54584ms step_avg:60.31ms
step:906/2285 train_time:54644ms step_avg:60.31ms
step:907/2285 train_time:54706ms step_avg:60.32ms
step:908/2285 train_time:54765ms step_avg:60.31ms
step:909/2285 train_time:54827ms step_avg:60.32ms
step:910/2285 train_time:54887ms step_avg:60.32ms
step:911/2285 train_time:54949ms step_avg:60.32ms
step:912/2285 train_time:55008ms step_avg:60.32ms
step:913/2285 train_time:55070ms step_avg:60.32ms
step:914/2285 train_time:55130ms step_avg:60.32ms
step:915/2285 train_time:55192ms step_avg:60.32ms
step:916/2285 train_time:55252ms step_avg:60.32ms
step:917/2285 train_time:55314ms step_avg:60.32ms
step:918/2285 train_time:55375ms step_avg:60.32ms
step:919/2285 train_time:55438ms step_avg:60.32ms
step:920/2285 train_time:55499ms step_avg:60.32ms
step:921/2285 train_time:55560ms step_avg:60.33ms
step:922/2285 train_time:55620ms step_avg:60.33ms
step:923/2285 train_time:55683ms step_avg:60.33ms
step:924/2285 train_time:55742ms step_avg:60.33ms
step:925/2285 train_time:55805ms step_avg:60.33ms
step:926/2285 train_time:55865ms step_avg:60.33ms
step:927/2285 train_time:55927ms step_avg:60.33ms
step:928/2285 train_time:55987ms step_avg:60.33ms
step:929/2285 train_time:56048ms step_avg:60.33ms
step:930/2285 train_time:56108ms step_avg:60.33ms
step:931/2285 train_time:56170ms step_avg:60.33ms
step:932/2285 train_time:56229ms step_avg:60.33ms
step:933/2285 train_time:56292ms step_avg:60.33ms
step:934/2285 train_time:56352ms step_avg:60.33ms
step:935/2285 train_time:56414ms step_avg:60.34ms
step:936/2285 train_time:56474ms step_avg:60.34ms
step:937/2285 train_time:56537ms step_avg:60.34ms
step:938/2285 train_time:56597ms step_avg:60.34ms
step:939/2285 train_time:56660ms step_avg:60.34ms
step:940/2285 train_time:56719ms step_avg:60.34ms
step:941/2285 train_time:56781ms step_avg:60.34ms
step:942/2285 train_time:56841ms step_avg:60.34ms
step:943/2285 train_time:56904ms step_avg:60.34ms
step:944/2285 train_time:56964ms step_avg:60.34ms
step:945/2285 train_time:57026ms step_avg:60.35ms
step:946/2285 train_time:57086ms step_avg:60.35ms
step:947/2285 train_time:57149ms step_avg:60.35ms
step:948/2285 train_time:57208ms step_avg:60.35ms
step:949/2285 train_time:57270ms step_avg:60.35ms
step:950/2285 train_time:57330ms step_avg:60.35ms
step:951/2285 train_time:57392ms step_avg:60.35ms
step:952/2285 train_time:57452ms step_avg:60.35ms
step:953/2285 train_time:57515ms step_avg:60.35ms
step:954/2285 train_time:57576ms step_avg:60.35ms
step:955/2285 train_time:57638ms step_avg:60.35ms
step:956/2285 train_time:57699ms step_avg:60.35ms
step:957/2285 train_time:57761ms step_avg:60.36ms
step:958/2285 train_time:57821ms step_avg:60.36ms
step:959/2285 train_time:57883ms step_avg:60.36ms
step:960/2285 train_time:57943ms step_avg:60.36ms
step:961/2285 train_time:58006ms step_avg:60.36ms
step:962/2285 train_time:58065ms step_avg:60.36ms
step:963/2285 train_time:58128ms step_avg:60.36ms
step:964/2285 train_time:58187ms step_avg:60.36ms
step:965/2285 train_time:58249ms step_avg:60.36ms
step:966/2285 train_time:58309ms step_avg:60.36ms
step:967/2285 train_time:58370ms step_avg:60.36ms
step:968/2285 train_time:58430ms step_avg:60.36ms
step:969/2285 train_time:58492ms step_avg:60.36ms
step:970/2285 train_time:58552ms step_avg:60.36ms
step:971/2285 train_time:58614ms step_avg:60.36ms
step:972/2285 train_time:58674ms step_avg:60.36ms
step:973/2285 train_time:58737ms step_avg:60.37ms
step:974/2285 train_time:58798ms step_avg:60.37ms
step:975/2285 train_time:58860ms step_avg:60.37ms
step:976/2285 train_time:58920ms step_avg:60.37ms
step:977/2285 train_time:58983ms step_avg:60.37ms
step:978/2285 train_time:59042ms step_avg:60.37ms
step:979/2285 train_time:59105ms step_avg:60.37ms
step:980/2285 train_time:59165ms step_avg:60.37ms
step:981/2285 train_time:59227ms step_avg:60.37ms
step:982/2285 train_time:59287ms step_avg:60.37ms
step:983/2285 train_time:59349ms step_avg:60.38ms
step:984/2285 train_time:59409ms step_avg:60.37ms
step:985/2285 train_time:59471ms step_avg:60.38ms
step:986/2285 train_time:59530ms step_avg:60.38ms
step:987/2285 train_time:59592ms step_avg:60.38ms
step:988/2285 train_time:59652ms step_avg:60.38ms
step:989/2285 train_time:59715ms step_avg:60.38ms
step:990/2285 train_time:59776ms step_avg:60.38ms
step:991/2285 train_time:59838ms step_avg:60.38ms
step:992/2285 train_time:59899ms step_avg:60.38ms
step:993/2285 train_time:59961ms step_avg:60.38ms
step:994/2285 train_time:60020ms step_avg:60.38ms
step:995/2285 train_time:60083ms step_avg:60.39ms
step:996/2285 train_time:60143ms step_avg:60.38ms
step:997/2285 train_time:60206ms step_avg:60.39ms
step:998/2285 train_time:60265ms step_avg:60.39ms
step:999/2285 train_time:60328ms step_avg:60.39ms
step:1000/2285 train_time:60387ms step_avg:60.39ms
step:1000/2285 val_loss:3.5701 train_time:60451ms step_avg:60.45ms
step:1001/2285 train_time:60469ms step_avg:60.41ms
step:1002/2285 train_time:60514ms step_avg:60.39ms
step:1003/2285 train_time:60579ms step_avg:60.40ms
step:1004/2285 train_time:60640ms step_avg:60.40ms
step:1005/2285 train_time:60702ms step_avg:60.40ms
step:1006/2285 train_time:60763ms step_avg:60.40ms
step:1007/2285 train_time:60824ms step_avg:60.40ms
step:1008/2285 train_time:60884ms step_avg:60.40ms
step:1009/2285 train_time:60945ms step_avg:60.40ms
step:1010/2285 train_time:61004ms step_avg:60.40ms
step:1011/2285 train_time:61067ms step_avg:60.40ms
step:1012/2285 train_time:61126ms step_avg:60.40ms
step:1013/2285 train_time:61188ms step_avg:60.40ms
step:1014/2285 train_time:61247ms step_avg:60.40ms
step:1015/2285 train_time:61310ms step_avg:60.40ms
step:1016/2285 train_time:61370ms step_avg:60.40ms
step:1017/2285 train_time:61433ms step_avg:60.41ms
step:1018/2285 train_time:61494ms step_avg:60.41ms
step:1019/2285 train_time:61558ms step_avg:60.41ms
step:1020/2285 train_time:61619ms step_avg:60.41ms
step:1021/2285 train_time:61681ms step_avg:60.41ms
step:1022/2285 train_time:61741ms step_avg:60.41ms
step:1023/2285 train_time:61803ms step_avg:60.41ms
step:1024/2285 train_time:61863ms step_avg:60.41ms
step:1025/2285 train_time:61925ms step_avg:60.41ms
step:1026/2285 train_time:61984ms step_avg:60.41ms
step:1027/2285 train_time:62047ms step_avg:60.42ms
step:1028/2285 train_time:62106ms step_avg:60.41ms
step:1029/2285 train_time:62168ms step_avg:60.42ms
step:1030/2285 train_time:62227ms step_avg:60.42ms
step:1031/2285 train_time:62289ms step_avg:60.42ms
step:1032/2285 train_time:62350ms step_avg:60.42ms
step:1033/2285 train_time:62414ms step_avg:60.42ms
step:1034/2285 train_time:62474ms step_avg:60.42ms
step:1035/2285 train_time:62537ms step_avg:60.42ms
step:1036/2285 train_time:62597ms step_avg:60.42ms
step:1037/2285 train_time:62660ms step_avg:60.42ms
step:1038/2285 train_time:62720ms step_avg:60.42ms
step:1039/2285 train_time:62783ms step_avg:60.43ms
step:1040/2285 train_time:62843ms step_avg:60.43ms
step:1041/2285 train_time:62905ms step_avg:60.43ms
step:1042/2285 train_time:62965ms step_avg:60.43ms
step:1043/2285 train_time:63027ms step_avg:60.43ms
step:1044/2285 train_time:63087ms step_avg:60.43ms
step:1045/2285 train_time:63149ms step_avg:60.43ms
step:1046/2285 train_time:63209ms step_avg:60.43ms
step:1047/2285 train_time:63271ms step_avg:60.43ms
step:1048/2285 train_time:63331ms step_avg:60.43ms
step:1049/2285 train_time:63393ms step_avg:60.43ms
step:1050/2285 train_time:63453ms step_avg:60.43ms
step:1051/2285 train_time:63516ms step_avg:60.43ms
step:1052/2285 train_time:63576ms step_avg:60.43ms
step:1053/2285 train_time:63638ms step_avg:60.44ms
step:1054/2285 train_time:63698ms step_avg:60.43ms
step:1055/2285 train_time:63760ms step_avg:60.44ms
step:1056/2285 train_time:63820ms step_avg:60.44ms
step:1057/2285 train_time:63883ms step_avg:60.44ms
step:1058/2285 train_time:63943ms step_avg:60.44ms
step:1059/2285 train_time:64005ms step_avg:60.44ms
step:1060/2285 train_time:64065ms step_avg:60.44ms
step:1061/2285 train_time:64127ms step_avg:60.44ms
step:1062/2285 train_time:64187ms step_avg:60.44ms
step:1063/2285 train_time:64249ms step_avg:60.44ms
step:1064/2285 train_time:64309ms step_avg:60.44ms
step:1065/2285 train_time:64372ms step_avg:60.44ms
step:1066/2285 train_time:64432ms step_avg:60.44ms
step:1067/2285 train_time:64495ms step_avg:60.44ms
step:1068/2285 train_time:64554ms step_avg:60.44ms
step:1069/2285 train_time:64617ms step_avg:60.45ms
step:1070/2285 train_time:64677ms step_avg:60.45ms
step:1071/2285 train_time:64739ms step_avg:60.45ms
step:1072/2285 train_time:64798ms step_avg:60.45ms
step:1073/2285 train_time:64860ms step_avg:60.45ms
step:1074/2285 train_time:64921ms step_avg:60.45ms
step:1075/2285 train_time:64983ms step_avg:60.45ms
step:1076/2285 train_time:65043ms step_avg:60.45ms
step:1077/2285 train_time:65105ms step_avg:60.45ms
step:1078/2285 train_time:65165ms step_avg:60.45ms
step:1079/2285 train_time:65228ms step_avg:60.45ms
step:1080/2285 train_time:65288ms step_avg:60.45ms
step:1081/2285 train_time:65351ms step_avg:60.45ms
step:1082/2285 train_time:65411ms step_avg:60.45ms
step:1083/2285 train_time:65474ms step_avg:60.46ms
step:1084/2285 train_time:65534ms step_avg:60.46ms
step:1085/2285 train_time:65596ms step_avg:60.46ms
step:1086/2285 train_time:65655ms step_avg:60.46ms
step:1087/2285 train_time:65718ms step_avg:60.46ms
step:1088/2285 train_time:65777ms step_avg:60.46ms
step:1089/2285 train_time:65839ms step_avg:60.46ms
step:1090/2285 train_time:65899ms step_avg:60.46ms
step:1091/2285 train_time:65961ms step_avg:60.46ms
step:1092/2285 train_time:66021ms step_avg:60.46ms
step:1093/2285 train_time:66083ms step_avg:60.46ms
step:1094/2285 train_time:66143ms step_avg:60.46ms
step:1095/2285 train_time:66206ms step_avg:60.46ms
step:1096/2285 train_time:66266ms step_avg:60.46ms
step:1097/2285 train_time:66329ms step_avg:60.46ms
step:1098/2285 train_time:66389ms step_avg:60.46ms
step:1099/2285 train_time:66452ms step_avg:60.47ms
step:1100/2285 train_time:66512ms step_avg:60.47ms
step:1101/2285 train_time:66574ms step_avg:60.47ms
step:1102/2285 train_time:66635ms step_avg:60.47ms
step:1103/2285 train_time:66697ms step_avg:60.47ms
step:1104/2285 train_time:66757ms step_avg:60.47ms
step:1105/2285 train_time:66819ms step_avg:60.47ms
step:1106/2285 train_time:66878ms step_avg:60.47ms
step:1107/2285 train_time:66940ms step_avg:60.47ms
step:1108/2285 train_time:67000ms step_avg:60.47ms
step:1109/2285 train_time:67062ms step_avg:60.47ms
step:1110/2285 train_time:67123ms step_avg:60.47ms
step:1111/2285 train_time:67185ms step_avg:60.47ms
step:1112/2285 train_time:67245ms step_avg:60.47ms
step:1113/2285 train_time:67308ms step_avg:60.47ms
step:1114/2285 train_time:67368ms step_avg:60.47ms
step:1115/2285 train_time:67431ms step_avg:60.48ms
step:1116/2285 train_time:67491ms step_avg:60.48ms
step:1117/2285 train_time:67553ms step_avg:60.48ms
step:1118/2285 train_time:67613ms step_avg:60.48ms
step:1119/2285 train_time:67676ms step_avg:60.48ms
step:1120/2285 train_time:67735ms step_avg:60.48ms
step:1121/2285 train_time:67797ms step_avg:60.48ms
step:1122/2285 train_time:67857ms step_avg:60.48ms
step:1123/2285 train_time:67919ms step_avg:60.48ms
step:1124/2285 train_time:67978ms step_avg:60.48ms
step:1125/2285 train_time:68040ms step_avg:60.48ms
step:1126/2285 train_time:68101ms step_avg:60.48ms
step:1127/2285 train_time:68164ms step_avg:60.48ms
step:1128/2285 train_time:68224ms step_avg:60.48ms
step:1129/2285 train_time:68287ms step_avg:60.48ms
step:1130/2285 train_time:68347ms step_avg:60.48ms
step:1131/2285 train_time:68410ms step_avg:60.49ms
step:1132/2285 train_time:68470ms step_avg:60.49ms
step:1133/2285 train_time:68532ms step_avg:60.49ms
step:1134/2285 train_time:68592ms step_avg:60.49ms
step:1135/2285 train_time:68654ms step_avg:60.49ms
step:1136/2285 train_time:68714ms step_avg:60.49ms
step:1137/2285 train_time:68777ms step_avg:60.49ms
step:1138/2285 train_time:68836ms step_avg:60.49ms
step:1139/2285 train_time:68898ms step_avg:60.49ms
step:1140/2285 train_time:68958ms step_avg:60.49ms
step:1141/2285 train_time:69020ms step_avg:60.49ms
step:1142/2285 train_time:69080ms step_avg:60.49ms
step:1143/2285 train_time:69142ms step_avg:60.49ms
step:1144/2285 train_time:69202ms step_avg:60.49ms
step:1145/2285 train_time:69265ms step_avg:60.49ms
step:1146/2285 train_time:69325ms step_avg:60.49ms
step:1147/2285 train_time:69388ms step_avg:60.49ms
step:1148/2285 train_time:69448ms step_avg:60.49ms
step:1149/2285 train_time:69511ms step_avg:60.50ms
step:1150/2285 train_time:69571ms step_avg:60.50ms
step:1151/2285 train_time:69633ms step_avg:60.50ms
step:1152/2285 train_time:69693ms step_avg:60.50ms
step:1153/2285 train_time:69755ms step_avg:60.50ms
step:1154/2285 train_time:69815ms step_avg:60.50ms
step:1155/2285 train_time:69878ms step_avg:60.50ms
step:1156/2285 train_time:69937ms step_avg:60.50ms
step:1157/2285 train_time:69998ms step_avg:60.50ms
step:1158/2285 train_time:70058ms step_avg:60.50ms
step:1159/2285 train_time:70121ms step_avg:60.50ms
step:1160/2285 train_time:70182ms step_avg:60.50ms
step:1161/2285 train_time:70245ms step_avg:60.50ms
step:1162/2285 train_time:70305ms step_avg:60.50ms
step:1163/2285 train_time:70368ms step_avg:60.51ms
step:1164/2285 train_time:70428ms step_avg:60.51ms
step:1165/2285 train_time:70491ms step_avg:60.51ms
step:1166/2285 train_time:70551ms step_avg:60.51ms
step:1167/2285 train_time:70614ms step_avg:60.51ms
step:1168/2285 train_time:70674ms step_avg:60.51ms
step:1169/2285 train_time:70736ms step_avg:60.51ms
step:1170/2285 train_time:70795ms step_avg:60.51ms
step:1171/2285 train_time:70858ms step_avg:60.51ms
step:1172/2285 train_time:70917ms step_avg:60.51ms
step:1173/2285 train_time:70979ms step_avg:60.51ms
step:1174/2285 train_time:71038ms step_avg:60.51ms
step:1175/2285 train_time:71101ms step_avg:60.51ms
step:1176/2285 train_time:71161ms step_avg:60.51ms
step:1177/2285 train_time:71224ms step_avg:60.51ms
step:1178/2285 train_time:71285ms step_avg:60.51ms
step:1179/2285 train_time:71347ms step_avg:60.52ms
step:1180/2285 train_time:71407ms step_avg:60.51ms
step:1181/2285 train_time:71470ms step_avg:60.52ms
step:1182/2285 train_time:71530ms step_avg:60.52ms
step:1183/2285 train_time:71593ms step_avg:60.52ms
step:1184/2285 train_time:71652ms step_avg:60.52ms
step:1185/2285 train_time:71714ms step_avg:60.52ms
step:1186/2285 train_time:71774ms step_avg:60.52ms
step:1187/2285 train_time:71836ms step_avg:60.52ms
step:1188/2285 train_time:71896ms step_avg:60.52ms
step:1189/2285 train_time:71957ms step_avg:60.52ms
step:1190/2285 train_time:72017ms step_avg:60.52ms
step:1191/2285 train_time:72079ms step_avg:60.52ms
step:1192/2285 train_time:72139ms step_avg:60.52ms
step:1193/2285 train_time:72202ms step_avg:60.52ms
step:1194/2285 train_time:72263ms step_avg:60.52ms
step:1195/2285 train_time:72327ms step_avg:60.52ms
step:1196/2285 train_time:72387ms step_avg:60.52ms
step:1197/2285 train_time:72449ms step_avg:60.53ms
step:1198/2285 train_time:72509ms step_avg:60.53ms
step:1199/2285 train_time:72572ms step_avg:60.53ms
step:1200/2285 train_time:72632ms step_avg:60.53ms
step:1201/2285 train_time:72694ms step_avg:60.53ms
step:1202/2285 train_time:72753ms step_avg:60.53ms
step:1203/2285 train_time:72816ms step_avg:60.53ms
step:1204/2285 train_time:72875ms step_avg:60.53ms
step:1205/2285 train_time:72937ms step_avg:60.53ms
step:1206/2285 train_time:72997ms step_avg:60.53ms
step:1207/2285 train_time:73059ms step_avg:60.53ms
step:1208/2285 train_time:73118ms step_avg:60.53ms
step:1209/2285 train_time:73181ms step_avg:60.53ms
step:1210/2285 train_time:73242ms step_avg:60.53ms
step:1211/2285 train_time:73305ms step_avg:60.53ms
step:1212/2285 train_time:73365ms step_avg:60.53ms
step:1213/2285 train_time:73427ms step_avg:60.53ms
step:1214/2285 train_time:73487ms step_avg:60.53ms
step:1215/2285 train_time:73551ms step_avg:60.54ms
step:1216/2285 train_time:73610ms step_avg:60.53ms
step:1217/2285 train_time:73673ms step_avg:60.54ms
step:1218/2285 train_time:73732ms step_avg:60.54ms
step:1219/2285 train_time:73795ms step_avg:60.54ms
step:1220/2285 train_time:73854ms step_avg:60.54ms
step:1221/2285 train_time:73916ms step_avg:60.54ms
step:1222/2285 train_time:73975ms step_avg:60.54ms
step:1223/2285 train_time:74037ms step_avg:60.54ms
step:1224/2285 train_time:74096ms step_avg:60.54ms
step:1225/2285 train_time:74158ms step_avg:60.54ms
step:1226/2285 train_time:74219ms step_avg:60.54ms
step:1227/2285 train_time:74282ms step_avg:60.54ms
step:1228/2285 train_time:74343ms step_avg:60.54ms
step:1229/2285 train_time:74405ms step_avg:60.54ms
step:1230/2285 train_time:74465ms step_avg:60.54ms
step:1231/2285 train_time:74529ms step_avg:60.54ms
step:1232/2285 train_time:74588ms step_avg:60.54ms
step:1233/2285 train_time:74651ms step_avg:60.54ms
step:1234/2285 train_time:74711ms step_avg:60.54ms
step:1235/2285 train_time:74773ms step_avg:60.54ms
step:1236/2285 train_time:74832ms step_avg:60.54ms
step:1237/2285 train_time:74894ms step_avg:60.54ms
step:1238/2285 train_time:74954ms step_avg:60.54ms
step:1239/2285 train_time:75016ms step_avg:60.55ms
step:1240/2285 train_time:75075ms step_avg:60.54ms
step:1241/2285 train_time:75138ms step_avg:60.55ms
step:1242/2285 train_time:75197ms step_avg:60.55ms
step:1243/2285 train_time:75260ms step_avg:60.55ms
step:1244/2285 train_time:75321ms step_avg:60.55ms
step:1245/2285 train_time:75384ms step_avg:60.55ms
step:1246/2285 train_time:75444ms step_avg:60.55ms
step:1247/2285 train_time:75507ms step_avg:60.55ms
step:1248/2285 train_time:75566ms step_avg:60.55ms
step:1249/2285 train_time:75629ms step_avg:60.55ms
step:1250/2285 train_time:75689ms step_avg:60.55ms
step:1250/2285 val_loss:3.5020 train_time:75752ms step_avg:60.60ms
step:1251/2285 train_time:75777ms step_avg:60.57ms
step:1252/2285 train_time:75816ms step_avg:60.56ms
step:1253/2285 train_time:75880ms step_avg:60.56ms
step:1254/2285 train_time:75942ms step_avg:60.56ms
step:1255/2285 train_time:76005ms step_avg:60.56ms
step:1256/2285 train_time:76065ms step_avg:60.56ms
step:1257/2285 train_time:76127ms step_avg:60.56ms
step:1258/2285 train_time:76187ms step_avg:60.56ms
step:1259/2285 train_time:76249ms step_avg:60.56ms
step:1260/2285 train_time:76309ms step_avg:60.56ms
step:1261/2285 train_time:76371ms step_avg:60.56ms
step:1262/2285 train_time:76430ms step_avg:60.56ms
step:1263/2285 train_time:76492ms step_avg:60.56ms
step:1264/2285 train_time:76552ms step_avg:60.56ms
step:1265/2285 train_time:76613ms step_avg:60.56ms
step:1266/2285 train_time:76673ms step_avg:60.56ms
step:1267/2285 train_time:76735ms step_avg:60.56ms
step:1268/2285 train_time:76796ms step_avg:60.56ms
step:1269/2285 train_time:76860ms step_avg:60.57ms
step:1270/2285 train_time:76920ms step_avg:60.57ms
step:1271/2285 train_time:76983ms step_avg:60.57ms
step:1272/2285 train_time:77043ms step_avg:60.57ms
step:1273/2285 train_time:77105ms step_avg:60.57ms
step:1274/2285 train_time:77165ms step_avg:60.57ms
step:1275/2285 train_time:77227ms step_avg:60.57ms
step:1276/2285 train_time:77288ms step_avg:60.57ms
step:1277/2285 train_time:77350ms step_avg:60.57ms
step:1278/2285 train_time:77410ms step_avg:60.57ms
step:1279/2285 train_time:77472ms step_avg:60.57ms
step:1280/2285 train_time:77531ms step_avg:60.57ms
step:1281/2285 train_time:77593ms step_avg:60.57ms
step:1282/2285 train_time:77653ms step_avg:60.57ms
step:1283/2285 train_time:77715ms step_avg:60.57ms
step:1284/2285 train_time:77775ms step_avg:60.57ms
step:1285/2285 train_time:77838ms step_avg:60.57ms
step:1286/2285 train_time:77898ms step_avg:60.57ms
step:1287/2285 train_time:77961ms step_avg:60.58ms
step:1288/2285 train_time:78021ms step_avg:60.58ms
step:1289/2285 train_time:78084ms step_avg:60.58ms
step:1290/2285 train_time:78144ms step_avg:60.58ms
step:1291/2285 train_time:78207ms step_avg:60.58ms
step:1292/2285 train_time:78267ms step_avg:60.58ms
step:1293/2285 train_time:78329ms step_avg:60.58ms
step:1294/2285 train_time:78389ms step_avg:60.58ms
step:1295/2285 train_time:78450ms step_avg:60.58ms
step:1296/2285 train_time:78510ms step_avg:60.58ms
step:1297/2285 train_time:78572ms step_avg:60.58ms
step:1298/2285 train_time:78632ms step_avg:60.58ms
step:1299/2285 train_time:78694ms step_avg:60.58ms
step:1300/2285 train_time:78754ms step_avg:60.58ms
step:1301/2285 train_time:78816ms step_avg:60.58ms
step:1302/2285 train_time:78877ms step_avg:60.58ms
step:1303/2285 train_time:78939ms step_avg:60.58ms
step:1304/2285 train_time:78998ms step_avg:60.58ms
step:1305/2285 train_time:79060ms step_avg:60.58ms
step:1306/2285 train_time:79121ms step_avg:60.58ms
step:1307/2285 train_time:79183ms step_avg:60.58ms
step:1308/2285 train_time:79243ms step_avg:60.58ms
step:1309/2285 train_time:79306ms step_avg:60.59ms
step:1310/2285 train_time:79366ms step_avg:60.59ms
step:1311/2285 train_time:79429ms step_avg:60.59ms
step:1312/2285 train_time:79488ms step_avg:60.59ms
step:1313/2285 train_time:79550ms step_avg:60.59ms
step:1314/2285 train_time:79610ms step_avg:60.59ms
step:1315/2285 train_time:79673ms step_avg:60.59ms
step:1316/2285 train_time:79733ms step_avg:60.59ms
step:1317/2285 train_time:79796ms step_avg:60.59ms
step:1318/2285 train_time:79855ms step_avg:60.59ms
step:1319/2285 train_time:79918ms step_avg:60.59ms
step:1320/2285 train_time:79977ms step_avg:60.59ms
step:1321/2285 train_time:80040ms step_avg:60.59ms
step:1322/2285 train_time:80100ms step_avg:60.59ms
step:1323/2285 train_time:80163ms step_avg:60.59ms
step:1324/2285 train_time:80223ms step_avg:60.59ms
step:1325/2285 train_time:80285ms step_avg:60.59ms
step:1326/2285 train_time:80345ms step_avg:60.59ms
step:1327/2285 train_time:80407ms step_avg:60.59ms
step:1328/2285 train_time:80468ms step_avg:60.59ms
step:1329/2285 train_time:80530ms step_avg:60.59ms
step:1330/2285 train_time:80590ms step_avg:60.59ms
step:1331/2285 train_time:80653ms step_avg:60.60ms
step:1332/2285 train_time:80712ms step_avg:60.59ms
step:1333/2285 train_time:80774ms step_avg:60.60ms
step:1334/2285 train_time:80834ms step_avg:60.60ms
step:1335/2285 train_time:80897ms step_avg:60.60ms
step:1336/2285 train_time:80958ms step_avg:60.60ms
step:1337/2285 train_time:81020ms step_avg:60.60ms
step:1338/2285 train_time:81079ms step_avg:60.60ms
step:1339/2285 train_time:81141ms step_avg:60.60ms
step:1340/2285 train_time:81201ms step_avg:60.60ms
step:1341/2285 train_time:81263ms step_avg:60.60ms
step:1342/2285 train_time:81324ms step_avg:60.60ms
step:1343/2285 train_time:81386ms step_avg:60.60ms
step:1344/2285 train_time:81446ms step_avg:60.60ms
step:1345/2285 train_time:81508ms step_avg:60.60ms
step:1346/2285 train_time:81568ms step_avg:60.60ms
step:1347/2285 train_time:81630ms step_avg:60.60ms
step:1348/2285 train_time:81691ms step_avg:60.60ms
step:1349/2285 train_time:81753ms step_avg:60.60ms
step:1350/2285 train_time:81813ms step_avg:60.60ms
step:1351/2285 train_time:81875ms step_avg:60.60ms
step:1352/2285 train_time:81934ms step_avg:60.60ms
step:1353/2285 train_time:81997ms step_avg:60.60ms
step:1354/2285 train_time:82056ms step_avg:60.60ms
step:1355/2285 train_time:82118ms step_avg:60.60ms
step:1356/2285 train_time:82177ms step_avg:60.60ms
step:1357/2285 train_time:82240ms step_avg:60.60ms
step:1358/2285 train_time:82300ms step_avg:60.60ms
step:1359/2285 train_time:82363ms step_avg:60.61ms
step:1360/2285 train_time:82424ms step_avg:60.61ms
step:1361/2285 train_time:82486ms step_avg:60.61ms
step:1362/2285 train_time:82547ms step_avg:60.61ms
step:1363/2285 train_time:82609ms step_avg:60.61ms
step:1364/2285 train_time:82669ms step_avg:60.61ms
step:1365/2285 train_time:82731ms step_avg:60.61ms
step:1366/2285 train_time:82791ms step_avg:60.61ms
step:1367/2285 train_time:82855ms step_avg:60.61ms
step:1368/2285 train_time:82914ms step_avg:60.61ms
step:1369/2285 train_time:82977ms step_avg:60.61ms
step:1370/2285 train_time:83036ms step_avg:60.61ms
step:1371/2285 train_time:83098ms step_avg:60.61ms
step:1372/2285 train_time:83158ms step_avg:60.61ms
step:1373/2285 train_time:83220ms step_avg:60.61ms
step:1374/2285 train_time:83279ms step_avg:60.61ms
step:1375/2285 train_time:83341ms step_avg:60.61ms
step:1376/2285 train_time:83402ms step_avg:60.61ms
step:1377/2285 train_time:83465ms step_avg:60.61ms
step:1378/2285 train_time:83525ms step_avg:60.61ms
step:1379/2285 train_time:83587ms step_avg:60.61ms
step:1380/2285 train_time:83647ms step_avg:60.61ms
step:1381/2285 train_time:83710ms step_avg:60.62ms
step:1382/2285 train_time:83770ms step_avg:60.62ms
step:1383/2285 train_time:83833ms step_avg:60.62ms
step:1384/2285 train_time:83893ms step_avg:60.62ms
step:1385/2285 train_time:83955ms step_avg:60.62ms
step:1386/2285 train_time:84015ms step_avg:60.62ms
step:1387/2285 train_time:84077ms step_avg:60.62ms
step:1388/2285 train_time:84136ms step_avg:60.62ms
step:1389/2285 train_time:84198ms step_avg:60.62ms
step:1390/2285 train_time:84258ms step_avg:60.62ms
step:1391/2285 train_time:84320ms step_avg:60.62ms
step:1392/2285 train_time:84380ms step_avg:60.62ms
step:1393/2285 train_time:84442ms step_avg:60.62ms
step:1394/2285 train_time:84502ms step_avg:60.62ms
step:1395/2285 train_time:84565ms step_avg:60.62ms
step:1396/2285 train_time:84625ms step_avg:60.62ms
step:1397/2285 train_time:84687ms step_avg:60.62ms
step:1398/2285 train_time:84747ms step_avg:60.62ms
step:1399/2285 train_time:84810ms step_avg:60.62ms
step:1400/2285 train_time:84871ms step_avg:60.62ms
step:1401/2285 train_time:84933ms step_avg:60.62ms
step:1402/2285 train_time:84993ms step_avg:60.62ms
step:1403/2285 train_time:85056ms step_avg:60.62ms
step:1404/2285 train_time:85115ms step_avg:60.62ms
step:1405/2285 train_time:85178ms step_avg:60.62ms
step:1406/2285 train_time:85237ms step_avg:60.62ms
step:1407/2285 train_time:85299ms step_avg:60.62ms
step:1408/2285 train_time:85359ms step_avg:60.62ms
step:1409/2285 train_time:85421ms step_avg:60.63ms
step:1410/2285 train_time:85481ms step_avg:60.63ms
step:1411/2285 train_time:85544ms step_avg:60.63ms
step:1412/2285 train_time:85604ms step_avg:60.63ms
step:1413/2285 train_time:85667ms step_avg:60.63ms
step:1414/2285 train_time:85727ms step_avg:60.63ms
step:1415/2285 train_time:85790ms step_avg:60.63ms
step:1416/2285 train_time:85849ms step_avg:60.63ms
step:1417/2285 train_time:85913ms step_avg:60.63ms
step:1418/2285 train_time:85973ms step_avg:60.63ms
step:1419/2285 train_time:86035ms step_avg:60.63ms
step:1420/2285 train_time:86094ms step_avg:60.63ms
step:1421/2285 train_time:86158ms step_avg:60.63ms
step:1422/2285 train_time:86217ms step_avg:60.63ms
step:1423/2285 train_time:86279ms step_avg:60.63ms
step:1424/2285 train_time:86338ms step_avg:60.63ms
step:1425/2285 train_time:86400ms step_avg:60.63ms
step:1426/2285 train_time:86460ms step_avg:60.63ms
step:1427/2285 train_time:86523ms step_avg:60.63ms
step:1428/2285 train_time:86583ms step_avg:60.63ms
step:1429/2285 train_time:86645ms step_avg:60.63ms
step:1430/2285 train_time:86706ms step_avg:60.63ms
step:1431/2285 train_time:86769ms step_avg:60.64ms
step:1432/2285 train_time:86829ms step_avg:60.63ms
step:1433/2285 train_time:86892ms step_avg:60.64ms
step:1434/2285 train_time:86952ms step_avg:60.64ms
step:1435/2285 train_time:87014ms step_avg:60.64ms
step:1436/2285 train_time:87073ms step_avg:60.64ms
step:1437/2285 train_time:87135ms step_avg:60.64ms
step:1438/2285 train_time:87195ms step_avg:60.64ms
step:1439/2285 train_time:87258ms step_avg:60.64ms
step:1440/2285 train_time:87318ms step_avg:60.64ms
step:1441/2285 train_time:87380ms step_avg:60.64ms
step:1442/2285 train_time:87439ms step_avg:60.64ms
step:1443/2285 train_time:87501ms step_avg:60.64ms
step:1444/2285 train_time:87561ms step_avg:60.64ms
step:1445/2285 train_time:87624ms step_avg:60.64ms
step:1446/2285 train_time:87685ms step_avg:60.64ms
step:1447/2285 train_time:87748ms step_avg:60.64ms
step:1448/2285 train_time:87808ms step_avg:60.64ms
step:1449/2285 train_time:87871ms step_avg:60.64ms
step:1450/2285 train_time:87931ms step_avg:60.64ms
step:1451/2285 train_time:87993ms step_avg:60.64ms
step:1452/2285 train_time:88053ms step_avg:60.64ms
step:1453/2285 train_time:88115ms step_avg:60.64ms
step:1454/2285 train_time:88175ms step_avg:60.64ms
step:1455/2285 train_time:88238ms step_avg:60.64ms
step:1456/2285 train_time:88297ms step_avg:60.64ms
step:1457/2285 train_time:88360ms step_avg:60.65ms
step:1458/2285 train_time:88419ms step_avg:60.64ms
step:1459/2285 train_time:88482ms step_avg:60.65ms
step:1460/2285 train_time:88542ms step_avg:60.64ms
step:1461/2285 train_time:88604ms step_avg:60.65ms
step:1462/2285 train_time:88665ms step_avg:60.65ms
step:1463/2285 train_time:88728ms step_avg:60.65ms
step:1464/2285 train_time:88788ms step_avg:60.65ms
step:1465/2285 train_time:88850ms step_avg:60.65ms
step:1466/2285 train_time:88912ms step_avg:60.65ms
step:1467/2285 train_time:88973ms step_avg:60.65ms
step:1468/2285 train_time:89033ms step_avg:60.65ms
step:1469/2285 train_time:89095ms step_avg:60.65ms
step:1470/2285 train_time:89155ms step_avg:60.65ms
step:1471/2285 train_time:89217ms step_avg:60.65ms
step:1472/2285 train_time:89277ms step_avg:60.65ms
step:1473/2285 train_time:89339ms step_avg:60.65ms
step:1474/2285 train_time:89398ms step_avg:60.65ms
step:1475/2285 train_time:89460ms step_avg:60.65ms
step:1476/2285 train_time:89519ms step_avg:60.65ms
step:1477/2285 train_time:89581ms step_avg:60.65ms
step:1478/2285 train_time:89642ms step_avg:60.65ms
step:1479/2285 train_time:89705ms step_avg:60.65ms
step:1480/2285 train_time:89765ms step_avg:60.65ms
step:1481/2285 train_time:89828ms step_avg:60.65ms
step:1482/2285 train_time:89888ms step_avg:60.65ms
step:1483/2285 train_time:89951ms step_avg:60.65ms
step:1484/2285 train_time:90011ms step_avg:60.65ms
step:1485/2285 train_time:90074ms step_avg:60.66ms
step:1486/2285 train_time:90133ms step_avg:60.66ms
step:1487/2285 train_time:90195ms step_avg:60.66ms
step:1488/2285 train_time:90255ms step_avg:60.66ms
step:1489/2285 train_time:90318ms step_avg:60.66ms
step:1490/2285 train_time:90377ms step_avg:60.66ms
step:1491/2285 train_time:90438ms step_avg:60.66ms
step:1492/2285 train_time:90498ms step_avg:60.66ms
step:1493/2285 train_time:90560ms step_avg:60.66ms
step:1494/2285 train_time:90619ms step_avg:60.66ms
step:1495/2285 train_time:90682ms step_avg:60.66ms
step:1496/2285 train_time:90743ms step_avg:60.66ms
step:1497/2285 train_time:90806ms step_avg:60.66ms
step:1498/2285 train_time:90868ms step_avg:60.66ms
step:1499/2285 train_time:90930ms step_avg:60.66ms
step:1500/2285 train_time:90991ms step_avg:60.66ms
step:1500/2285 val_loss:3.4283 train_time:91055ms step_avg:60.70ms
step:1501/2285 train_time:91078ms step_avg:60.68ms
step:1502/2285 train_time:91118ms step_avg:60.66ms
step:1503/2285 train_time:91180ms step_avg:60.67ms
step:1504/2285 train_time:91243ms step_avg:60.67ms
step:1505/2285 train_time:91306ms step_avg:60.67ms
step:1506/2285 train_time:91367ms step_avg:60.67ms
step:1507/2285 train_time:91429ms step_avg:60.67ms
step:1508/2285 train_time:91488ms step_avg:60.67ms
step:1509/2285 train_time:91550ms step_avg:60.67ms
step:1510/2285 train_time:91609ms step_avg:60.67ms
step:1511/2285 train_time:91671ms step_avg:60.67ms
step:1512/2285 train_time:91731ms step_avg:60.67ms
step:1513/2285 train_time:91793ms step_avg:60.67ms
step:1514/2285 train_time:91852ms step_avg:60.67ms
step:1515/2285 train_time:91914ms step_avg:60.67ms
step:1516/2285 train_time:91976ms step_avg:60.67ms
step:1517/2285 train_time:92042ms step_avg:60.67ms
step:1518/2285 train_time:92103ms step_avg:60.67ms
step:1519/2285 train_time:92167ms step_avg:60.68ms
step:1520/2285 train_time:92228ms step_avg:60.68ms
step:1521/2285 train_time:92291ms step_avg:60.68ms
step:1522/2285 train_time:92350ms step_avg:60.68ms
step:1523/2285 train_time:92413ms step_avg:60.68ms
step:1524/2285 train_time:92473ms step_avg:60.68ms
step:1525/2285 train_time:92535ms step_avg:60.68ms
step:1526/2285 train_time:92596ms step_avg:60.68ms
step:1527/2285 train_time:92658ms step_avg:60.68ms
step:1528/2285 train_time:92719ms step_avg:60.68ms
step:1529/2285 train_time:92782ms step_avg:60.68ms
step:1530/2285 train_time:92842ms step_avg:60.68ms
step:1531/2285 train_time:92906ms step_avg:60.68ms
step:1532/2285 train_time:92967ms step_avg:60.68ms
step:1533/2285 train_time:93031ms step_avg:60.69ms
step:1534/2285 train_time:93092ms step_avg:60.69ms
step:1535/2285 train_time:93155ms step_avg:60.69ms
step:1536/2285 train_time:93215ms step_avg:60.69ms
step:1537/2285 train_time:93278ms step_avg:60.69ms
step:1538/2285 train_time:93338ms step_avg:60.69ms
step:1539/2285 train_time:93401ms step_avg:60.69ms
step:1540/2285 train_time:93461ms step_avg:60.69ms
step:1541/2285 train_time:93524ms step_avg:60.69ms
step:1542/2285 train_time:93584ms step_avg:60.69ms
step:1543/2285 train_time:93647ms step_avg:60.69ms
step:1544/2285 train_time:93707ms step_avg:60.69ms
step:1545/2285 train_time:93770ms step_avg:60.69ms
step:1546/2285 train_time:93830ms step_avg:60.69ms
step:1547/2285 train_time:93892ms step_avg:60.69ms
step:1548/2285 train_time:93952ms step_avg:60.69ms
step:1549/2285 train_time:94015ms step_avg:60.69ms
step:1550/2285 train_time:94075ms step_avg:60.69ms
step:1551/2285 train_time:94138ms step_avg:60.69ms
step:1552/2285 train_time:94200ms step_avg:60.70ms
step:1553/2285 train_time:94261ms step_avg:60.70ms
step:1554/2285 train_time:94322ms step_avg:60.70ms
step:1555/2285 train_time:94385ms step_avg:60.70ms
step:1556/2285 train_time:94445ms step_avg:60.70ms
step:1557/2285 train_time:94508ms step_avg:60.70ms
step:1558/2285 train_time:94568ms step_avg:60.70ms
step:1559/2285 train_time:94630ms step_avg:60.70ms
step:1560/2285 train_time:94691ms step_avg:60.70ms
step:1561/2285 train_time:94753ms step_avg:60.70ms
step:1562/2285 train_time:94813ms step_avg:60.70ms
step:1563/2285 train_time:94875ms step_avg:60.70ms
step:1564/2285 train_time:94935ms step_avg:60.70ms
step:1565/2285 train_time:94998ms step_avg:60.70ms
step:1566/2285 train_time:95058ms step_avg:60.70ms
step:1567/2285 train_time:95121ms step_avg:60.70ms
step:1568/2285 train_time:95182ms step_avg:60.70ms
step:1569/2285 train_time:95244ms step_avg:60.70ms
step:1570/2285 train_time:95305ms step_avg:60.70ms
step:1571/2285 train_time:95368ms step_avg:60.71ms
step:1572/2285 train_time:95429ms step_avg:60.71ms
step:1573/2285 train_time:95491ms step_avg:60.71ms
step:1574/2285 train_time:95551ms step_avg:60.71ms
step:1575/2285 train_time:95614ms step_avg:60.71ms
step:1576/2285 train_time:95674ms step_avg:60.71ms
step:1577/2285 train_time:95737ms step_avg:60.71ms
step:1578/2285 train_time:95797ms step_avg:60.71ms
step:1579/2285 train_time:95861ms step_avg:60.71ms
step:1580/2285 train_time:95920ms step_avg:60.71ms
step:1581/2285 train_time:95983ms step_avg:60.71ms
step:1582/2285 train_time:96043ms step_avg:60.71ms
step:1583/2285 train_time:96107ms step_avg:60.71ms
step:1584/2285 train_time:96167ms step_avg:60.71ms
step:1585/2285 train_time:96231ms step_avg:60.71ms
step:1586/2285 train_time:96290ms step_avg:60.71ms
step:1587/2285 train_time:96353ms step_avg:60.71ms
step:1588/2285 train_time:96413ms step_avg:60.71ms
step:1589/2285 train_time:96476ms step_avg:60.71ms
step:1590/2285 train_time:96536ms step_avg:60.71ms
step:1591/2285 train_time:96598ms step_avg:60.72ms
step:1592/2285 train_time:96659ms step_avg:60.72ms
step:1593/2285 train_time:96722ms step_avg:60.72ms
step:1594/2285 train_time:96783ms step_avg:60.72ms
step:1595/2285 train_time:96846ms step_avg:60.72ms
step:1596/2285 train_time:96906ms step_avg:60.72ms
step:1597/2285 train_time:96969ms step_avg:60.72ms
step:1598/2285 train_time:97029ms step_avg:60.72ms
step:1599/2285 train_time:97092ms step_avg:60.72ms
step:1600/2285 train_time:97152ms step_avg:60.72ms
step:1601/2285 train_time:97215ms step_avg:60.72ms
step:1602/2285 train_time:97275ms step_avg:60.72ms
step:1603/2285 train_time:97337ms step_avg:60.72ms
step:1604/2285 train_time:97397ms step_avg:60.72ms
step:1605/2285 train_time:97460ms step_avg:60.72ms
step:1606/2285 train_time:97520ms step_avg:60.72ms
step:1607/2285 train_time:97583ms step_avg:60.72ms
step:1608/2285 train_time:97644ms step_avg:60.72ms
step:1609/2285 train_time:97707ms step_avg:60.73ms
step:1610/2285 train_time:97767ms step_avg:60.73ms
step:1611/2285 train_time:97830ms step_avg:60.73ms
step:1612/2285 train_time:97890ms step_avg:60.73ms
step:1613/2285 train_time:97953ms step_avg:60.73ms
step:1614/2285 train_time:98013ms step_avg:60.73ms
step:1615/2285 train_time:98075ms step_avg:60.73ms
step:1616/2285 train_time:98135ms step_avg:60.73ms
step:1617/2285 train_time:98198ms step_avg:60.73ms
step:1618/2285 train_time:98259ms step_avg:60.73ms
step:1619/2285 train_time:98321ms step_avg:60.73ms
step:1620/2285 train_time:98381ms step_avg:60.73ms
step:1621/2285 train_time:98444ms step_avg:60.73ms
step:1622/2285 train_time:98505ms step_avg:60.73ms
step:1623/2285 train_time:98567ms step_avg:60.73ms
step:1624/2285 train_time:98628ms step_avg:60.73ms
step:1625/2285 train_time:98691ms step_avg:60.73ms
step:1626/2285 train_time:98751ms step_avg:60.73ms
step:1627/2285 train_time:98813ms step_avg:60.73ms
step:1628/2285 train_time:98873ms step_avg:60.73ms
step:1629/2285 train_time:98935ms step_avg:60.73ms
step:1630/2285 train_time:98995ms step_avg:60.73ms
step:1631/2285 train_time:99058ms step_avg:60.73ms
step:1632/2285 train_time:99119ms step_avg:60.73ms
step:1633/2285 train_time:99182ms step_avg:60.74ms
step:1634/2285 train_time:99243ms step_avg:60.74ms
step:1635/2285 train_time:99305ms step_avg:60.74ms
step:1636/2285 train_time:99367ms step_avg:60.74ms
step:1637/2285 train_time:99430ms step_avg:60.74ms
step:1638/2285 train_time:99490ms step_avg:60.74ms
step:1639/2285 train_time:99551ms step_avg:60.74ms
step:1640/2285 train_time:99611ms step_avg:60.74ms
step:1641/2285 train_time:99674ms step_avg:60.74ms
step:1642/2285 train_time:99734ms step_avg:60.74ms
step:1643/2285 train_time:99797ms step_avg:60.74ms
step:1644/2285 train_time:99857ms step_avg:60.74ms
step:1645/2285 train_time:99919ms step_avg:60.74ms
step:1646/2285 train_time:99980ms step_avg:60.74ms
step:1647/2285 train_time:100043ms step_avg:60.74ms
step:1648/2285 train_time:100103ms step_avg:60.74ms
step:1649/2285 train_time:100167ms step_avg:60.74ms
step:1650/2285 train_time:100227ms step_avg:60.74ms
step:1651/2285 train_time:100290ms step_avg:60.75ms
step:1652/2285 train_time:100350ms step_avg:60.74ms
step:1653/2285 train_time:100413ms step_avg:60.75ms
step:1654/2285 train_time:100474ms step_avg:60.75ms
step:1655/2285 train_time:100536ms step_avg:60.75ms
step:1656/2285 train_time:100597ms step_avg:60.75ms
step:1657/2285 train_time:100659ms step_avg:60.75ms
step:1658/2285 train_time:100720ms step_avg:60.75ms
step:1659/2285 train_time:100783ms step_avg:60.75ms
step:1660/2285 train_time:100843ms step_avg:60.75ms
step:1661/2285 train_time:100907ms step_avg:60.75ms
step:1662/2285 train_time:100967ms step_avg:60.75ms
step:1663/2285 train_time:101031ms step_avg:60.75ms
step:1664/2285 train_time:101091ms step_avg:60.75ms
step:1665/2285 train_time:101153ms step_avg:60.75ms
step:1666/2285 train_time:101213ms step_avg:60.75ms
step:1667/2285 train_time:101276ms step_avg:60.75ms
step:1668/2285 train_time:101337ms step_avg:60.75ms
step:1669/2285 train_time:101399ms step_avg:60.75ms
step:1670/2285 train_time:101460ms step_avg:60.75ms
step:1671/2285 train_time:101522ms step_avg:60.76ms
step:1672/2285 train_time:101584ms step_avg:60.76ms
step:1673/2285 train_time:101646ms step_avg:60.76ms
step:1674/2285 train_time:101706ms step_avg:60.76ms
step:1675/2285 train_time:101770ms step_avg:60.76ms
step:1676/2285 train_time:101830ms step_avg:60.76ms
step:1677/2285 train_time:101892ms step_avg:60.76ms
step:1678/2285 train_time:101952ms step_avg:60.76ms
step:1679/2285 train_time:102014ms step_avg:60.76ms
step:1680/2285 train_time:102074ms step_avg:60.76ms
step:1681/2285 train_time:102137ms step_avg:60.76ms
step:1682/2285 train_time:102197ms step_avg:60.76ms
step:1683/2285 train_time:102260ms step_avg:60.76ms
step:1684/2285 train_time:102321ms step_avg:60.76ms
step:1685/2285 train_time:102385ms step_avg:60.76ms
step:1686/2285 train_time:102445ms step_avg:60.76ms
step:1687/2285 train_time:102508ms step_avg:60.76ms
step:1688/2285 train_time:102568ms step_avg:60.76ms
step:1689/2285 train_time:102631ms step_avg:60.76ms
step:1690/2285 train_time:102691ms step_avg:60.76ms
step:1691/2285 train_time:102754ms step_avg:60.77ms
step:1692/2285 train_time:102814ms step_avg:60.77ms
step:1693/2285 train_time:102877ms step_avg:60.77ms
step:1694/2285 train_time:102938ms step_avg:60.77ms
step:1695/2285 train_time:103000ms step_avg:60.77ms
step:1696/2285 train_time:103060ms step_avg:60.77ms
step:1697/2285 train_time:103124ms step_avg:60.77ms
step:1698/2285 train_time:103184ms step_avg:60.77ms
step:1699/2285 train_time:103249ms step_avg:60.77ms
step:1700/2285 train_time:103308ms step_avg:60.77ms
step:1701/2285 train_time:103370ms step_avg:60.77ms
step:1702/2285 train_time:103430ms step_avg:60.77ms
step:1703/2285 train_time:103492ms step_avg:60.77ms
step:1704/2285 train_time:103552ms step_avg:60.77ms
step:1705/2285 train_time:103615ms step_avg:60.77ms
step:1706/2285 train_time:103675ms step_avg:60.77ms
step:1707/2285 train_time:103738ms step_avg:60.77ms
step:1708/2285 train_time:103799ms step_avg:60.77ms
step:1709/2285 train_time:103862ms step_avg:60.77ms
step:1710/2285 train_time:103923ms step_avg:60.77ms
step:1711/2285 train_time:103986ms step_avg:60.78ms
step:1712/2285 train_time:104046ms step_avg:60.77ms
step:1713/2285 train_time:104109ms step_avg:60.78ms
step:1714/2285 train_time:104170ms step_avg:60.78ms
step:1715/2285 train_time:104233ms step_avg:60.78ms
step:1716/2285 train_time:104293ms step_avg:60.78ms
step:1717/2285 train_time:104355ms step_avg:60.78ms
step:1718/2285 train_time:104415ms step_avg:60.78ms
step:1719/2285 train_time:104477ms step_avg:60.78ms
step:1720/2285 train_time:104538ms step_avg:60.78ms
step:1721/2285 train_time:104601ms step_avg:60.78ms
step:1722/2285 train_time:104661ms step_avg:60.78ms
step:1723/2285 train_time:104724ms step_avg:60.78ms
step:1724/2285 train_time:104786ms step_avg:60.78ms
step:1725/2285 train_time:104849ms step_avg:60.78ms
step:1726/2285 train_time:104909ms step_avg:60.78ms
step:1727/2285 train_time:104972ms step_avg:60.78ms
step:1728/2285 train_time:105033ms step_avg:60.78ms
step:1729/2285 train_time:105095ms step_avg:60.78ms
step:1730/2285 train_time:105154ms step_avg:60.78ms
step:1731/2285 train_time:105217ms step_avg:60.78ms
step:1732/2285 train_time:105278ms step_avg:60.78ms
step:1733/2285 train_time:105341ms step_avg:60.79ms
step:1734/2285 train_time:105402ms step_avg:60.79ms
step:1735/2285 train_time:105463ms step_avg:60.79ms
step:1736/2285 train_time:105524ms step_avg:60.79ms
step:1737/2285 train_time:105587ms step_avg:60.79ms
step:1738/2285 train_time:105648ms step_avg:60.79ms
step:1739/2285 train_time:105711ms step_avg:60.79ms
step:1740/2285 train_time:105771ms step_avg:60.79ms
step:1741/2285 train_time:105834ms step_avg:60.79ms
step:1742/2285 train_time:105894ms step_avg:60.79ms
step:1743/2285 train_time:105957ms step_avg:60.79ms
step:1744/2285 train_time:106017ms step_avg:60.79ms
step:1745/2285 train_time:106080ms step_avg:60.79ms
step:1746/2285 train_time:106141ms step_avg:60.79ms
step:1747/2285 train_time:106204ms step_avg:60.79ms
step:1748/2285 train_time:106265ms step_avg:60.79ms
step:1749/2285 train_time:106328ms step_avg:60.79ms
step:1750/2285 train_time:106389ms step_avg:60.79ms
step:1750/2285 val_loss:3.3700 train_time:106452ms step_avg:60.83ms
step:1751/2285 train_time:106474ms step_avg:60.81ms
step:1752/2285 train_time:106518ms step_avg:60.80ms
step:1753/2285 train_time:106580ms step_avg:60.80ms
step:1754/2285 train_time:106641ms step_avg:60.80ms
step:1755/2285 train_time:106703ms step_avg:60.80ms
step:1756/2285 train_time:106763ms step_avg:60.80ms
step:1757/2285 train_time:106826ms step_avg:60.80ms
step:1758/2285 train_time:106885ms step_avg:60.80ms
step:1759/2285 train_time:106947ms step_avg:60.80ms
step:1760/2285 train_time:107006ms step_avg:60.80ms
step:1761/2285 train_time:107068ms step_avg:60.80ms
step:1762/2285 train_time:107127ms step_avg:60.80ms
step:1763/2285 train_time:107188ms step_avg:60.80ms
step:1764/2285 train_time:107248ms step_avg:60.80ms
step:1765/2285 train_time:107311ms step_avg:60.80ms
step:1766/2285 train_time:107372ms step_avg:60.80ms
step:1767/2285 train_time:107436ms step_avg:60.80ms
step:1768/2285 train_time:107498ms step_avg:60.80ms
step:1769/2285 train_time:107561ms step_avg:60.80ms
step:1770/2285 train_time:107622ms step_avg:60.80ms
step:1771/2285 train_time:107684ms step_avg:60.80ms
step:1772/2285 train_time:107743ms step_avg:60.80ms
step:1773/2285 train_time:107806ms step_avg:60.80ms
step:1774/2285 train_time:107866ms step_avg:60.80ms
step:1775/2285 train_time:107927ms step_avg:60.80ms
step:1776/2285 train_time:107987ms step_avg:60.80ms
step:1777/2285 train_time:108050ms step_avg:60.80ms
step:1778/2285 train_time:108110ms step_avg:60.80ms
step:1779/2285 train_time:108172ms step_avg:60.81ms
step:1780/2285 train_time:108232ms step_avg:60.80ms
step:1781/2285 train_time:108295ms step_avg:60.81ms
step:1782/2285 train_time:108357ms step_avg:60.81ms
step:1783/2285 train_time:108420ms step_avg:60.81ms
step:1784/2285 train_time:108481ms step_avg:60.81ms
step:1785/2285 train_time:108544ms step_avg:60.81ms
step:1786/2285 train_time:108605ms step_avg:60.81ms
step:1787/2285 train_time:108667ms step_avg:60.81ms
step:1788/2285 train_time:108728ms step_avg:60.81ms
step:1789/2285 train_time:108790ms step_avg:60.81ms
step:1790/2285 train_time:108851ms step_avg:60.81ms
step:1791/2285 train_time:108913ms step_avg:60.81ms
step:1792/2285 train_time:108974ms step_avg:60.81ms
step:1793/2285 train_time:109036ms step_avg:60.81ms
step:1794/2285 train_time:109096ms step_avg:60.81ms
step:1795/2285 train_time:109158ms step_avg:60.81ms
step:1796/2285 train_time:109218ms step_avg:60.81ms
step:1797/2285 train_time:109280ms step_avg:60.81ms
step:1798/2285 train_time:109340ms step_avg:60.81ms
step:1799/2285 train_time:109403ms step_avg:60.81ms
step:1800/2285 train_time:109464ms step_avg:60.81ms
step:1801/2285 train_time:109528ms step_avg:60.81ms
step:1802/2285 train_time:109589ms step_avg:60.81ms
step:1803/2285 train_time:109652ms step_avg:60.82ms
step:1804/2285 train_time:109712ms step_avg:60.82ms
step:1805/2285 train_time:109775ms step_avg:60.82ms
step:1806/2285 train_time:109835ms step_avg:60.82ms
step:1807/2285 train_time:109898ms step_avg:60.82ms
step:1808/2285 train_time:109958ms step_avg:60.82ms
step:1809/2285 train_time:110021ms step_avg:60.82ms
step:1810/2285 train_time:110081ms step_avg:60.82ms
step:1811/2285 train_time:110143ms step_avg:60.82ms
step:1812/2285 train_time:110203ms step_avg:60.82ms
step:1813/2285 train_time:110266ms step_avg:60.82ms
step:1814/2285 train_time:110326ms step_avg:60.82ms
step:1815/2285 train_time:110390ms step_avg:60.82ms
step:1816/2285 train_time:110451ms step_avg:60.82ms
step:1817/2285 train_time:110513ms step_avg:60.82ms
step:1818/2285 train_time:110574ms step_avg:60.82ms
step:1819/2285 train_time:110637ms step_avg:60.82ms
step:1820/2285 train_time:110698ms step_avg:60.82ms
step:1821/2285 train_time:110761ms step_avg:60.82ms
step:1822/2285 train_time:110821ms step_avg:60.82ms
step:1823/2285 train_time:110884ms step_avg:60.82ms
step:1824/2285 train_time:110943ms step_avg:60.82ms
step:1825/2285 train_time:111005ms step_avg:60.82ms
step:1826/2285 train_time:111064ms step_avg:60.82ms
step:1827/2285 train_time:111127ms step_avg:60.82ms
step:1828/2285 train_time:111187ms step_avg:60.82ms
step:1829/2285 train_time:111249ms step_avg:60.82ms
step:1830/2285 train_time:111309ms step_avg:60.82ms
step:1831/2285 train_time:111372ms step_avg:60.83ms
step:1832/2285 train_time:111433ms step_avg:60.83ms
step:1833/2285 train_time:111495ms step_avg:60.83ms
step:1834/2285 train_time:111556ms step_avg:60.83ms
step:1835/2285 train_time:111619ms step_avg:60.83ms
step:1836/2285 train_time:111680ms step_avg:60.83ms
step:1837/2285 train_time:111742ms step_avg:60.83ms
step:1838/2285 train_time:111802ms step_avg:60.83ms
step:1839/2285 train_time:111865ms step_avg:60.83ms
step:1840/2285 train_time:111926ms step_avg:60.83ms
step:1841/2285 train_time:111988ms step_avg:60.83ms
step:1842/2285 train_time:112048ms step_avg:60.83ms
step:1843/2285 train_time:112110ms step_avg:60.83ms
step:1844/2285 train_time:112171ms step_avg:60.83ms
step:1845/2285 train_time:112233ms step_avg:60.83ms
step:1846/2285 train_time:112294ms step_avg:60.83ms
step:1847/2285 train_time:112357ms step_avg:60.83ms
step:1848/2285 train_time:112417ms step_avg:60.83ms
step:1849/2285 train_time:112480ms step_avg:60.83ms
step:1850/2285 train_time:112540ms step_avg:60.83ms
step:1851/2285 train_time:112603ms step_avg:60.83ms
step:1852/2285 train_time:112663ms step_avg:60.83ms
step:1853/2285 train_time:112726ms step_avg:60.83ms
step:1854/2285 train_time:112786ms step_avg:60.83ms
step:1855/2285 train_time:112849ms step_avg:60.83ms
step:1856/2285 train_time:112910ms step_avg:60.83ms
step:1857/2285 train_time:112973ms step_avg:60.84ms
step:1858/2285 train_time:113032ms step_avg:60.84ms
step:1859/2285 train_time:113095ms step_avg:60.84ms
step:1860/2285 train_time:113155ms step_avg:60.84ms
step:1861/2285 train_time:113218ms step_avg:60.84ms
step:1862/2285 train_time:113280ms step_avg:60.84ms
step:1863/2285 train_time:113342ms step_avg:60.84ms
step:1864/2285 train_time:113402ms step_avg:60.84ms
step:1865/2285 train_time:113464ms step_avg:60.84ms
step:1866/2285 train_time:113524ms step_avg:60.84ms
step:1867/2285 train_time:113586ms step_avg:60.84ms
step:1868/2285 train_time:113646ms step_avg:60.84ms
step:1869/2285 train_time:113709ms step_avg:60.84ms
step:1870/2285 train_time:113770ms step_avg:60.84ms
step:1871/2285 train_time:113834ms step_avg:60.84ms
step:1872/2285 train_time:113894ms step_avg:60.84ms
step:1873/2285 train_time:113957ms step_avg:60.84ms
step:1874/2285 train_time:114018ms step_avg:60.84ms
step:1875/2285 train_time:114080ms step_avg:60.84ms
step:1876/2285 train_time:114140ms step_avg:60.84ms
step:1877/2285 train_time:114203ms step_avg:60.84ms
step:1878/2285 train_time:114263ms step_avg:60.84ms
step:1879/2285 train_time:114326ms step_avg:60.84ms
step:1880/2285 train_time:114386ms step_avg:60.84ms
step:1881/2285 train_time:114448ms step_avg:60.84ms
step:1882/2285 train_time:114509ms step_avg:60.84ms
step:1883/2285 train_time:114571ms step_avg:60.84ms
step:1884/2285 train_time:114631ms step_avg:60.84ms
step:1885/2285 train_time:114693ms step_avg:60.85ms
step:1886/2285 train_time:114754ms step_avg:60.85ms
step:1887/2285 train_time:114817ms step_avg:60.85ms
step:1888/2285 train_time:114877ms step_avg:60.85ms
step:1889/2285 train_time:114940ms step_avg:60.85ms
step:1890/2285 train_time:115001ms step_avg:60.85ms
step:1891/2285 train_time:115063ms step_avg:60.85ms
step:1892/2285 train_time:115124ms step_avg:60.85ms
step:1893/2285 train_time:115186ms step_avg:60.85ms
step:1894/2285 train_time:115246ms step_avg:60.85ms
step:1895/2285 train_time:115309ms step_avg:60.85ms
step:1896/2285 train_time:115369ms step_avg:60.85ms
step:1897/2285 train_time:115432ms step_avg:60.85ms
step:1898/2285 train_time:115492ms step_avg:60.85ms
step:1899/2285 train_time:115555ms step_avg:60.85ms
step:1900/2285 train_time:115615ms step_avg:60.85ms
step:1901/2285 train_time:115678ms step_avg:60.85ms
step:1902/2285 train_time:115739ms step_avg:60.85ms
step:1903/2285 train_time:115802ms step_avg:60.85ms
step:1904/2285 train_time:115862ms step_avg:60.85ms
step:1905/2285 train_time:115924ms step_avg:60.85ms
step:1906/2285 train_time:115984ms step_avg:60.85ms
step:1907/2285 train_time:116047ms step_avg:60.85ms
step:1908/2285 train_time:116107ms step_avg:60.85ms
step:1909/2285 train_time:116169ms step_avg:60.85ms
step:1910/2285 train_time:116230ms step_avg:60.85ms
step:1911/2285 train_time:116293ms step_avg:60.85ms
step:1912/2285 train_time:116353ms step_avg:60.85ms
step:1913/2285 train_time:116416ms step_avg:60.86ms
step:1914/2285 train_time:116477ms step_avg:60.86ms
step:1915/2285 train_time:116540ms step_avg:60.86ms
step:1916/2285 train_time:116601ms step_avg:60.86ms
step:1917/2285 train_time:116663ms step_avg:60.86ms
step:1918/2285 train_time:116724ms step_avg:60.86ms
step:1919/2285 train_time:116786ms step_avg:60.86ms
step:1920/2285 train_time:116847ms step_avg:60.86ms
step:1921/2285 train_time:116909ms step_avg:60.86ms
step:1922/2285 train_time:116970ms step_avg:60.86ms
step:1923/2285 train_time:117033ms step_avg:60.86ms
step:1924/2285 train_time:117094ms step_avg:60.86ms
step:1925/2285 train_time:117157ms step_avg:60.86ms
step:1926/2285 train_time:117217ms step_avg:60.86ms
step:1927/2285 train_time:117280ms step_avg:60.86ms
step:1928/2285 train_time:117341ms step_avg:60.86ms
step:1929/2285 train_time:117403ms step_avg:60.86ms
step:1930/2285 train_time:117463ms step_avg:60.86ms
step:1931/2285 train_time:117526ms step_avg:60.86ms
step:1932/2285 train_time:117586ms step_avg:60.86ms
step:1933/2285 train_time:117649ms step_avg:60.86ms
step:1934/2285 train_time:117710ms step_avg:60.86ms
step:1935/2285 train_time:117773ms step_avg:60.86ms
step:1936/2285 train_time:117834ms step_avg:60.86ms
step:1937/2285 train_time:117896ms step_avg:60.87ms
step:1938/2285 train_time:117957ms step_avg:60.87ms
step:1939/2285 train_time:118020ms step_avg:60.87ms
step:1940/2285 train_time:118081ms step_avg:60.87ms
step:1941/2285 train_time:118143ms step_avg:60.87ms
step:1942/2285 train_time:118203ms step_avg:60.87ms
step:1943/2285 train_time:118266ms step_avg:60.87ms
step:1944/2285 train_time:118326ms step_avg:60.87ms
step:1945/2285 train_time:118388ms step_avg:60.87ms
step:1946/2285 train_time:118449ms step_avg:60.87ms
step:1947/2285 train_time:118511ms step_avg:60.87ms
step:1948/2285 train_time:118572ms step_avg:60.87ms
step:1949/2285 train_time:118635ms step_avg:60.87ms
step:1950/2285 train_time:118695ms step_avg:60.87ms
step:1951/2285 train_time:118758ms step_avg:60.87ms
step:1952/2285 train_time:118818ms step_avg:60.87ms
step:1953/2285 train_time:118881ms step_avg:60.87ms
step:1954/2285 train_time:118941ms step_avg:60.87ms
step:1955/2285 train_time:119004ms step_avg:60.87ms
step:1956/2285 train_time:119065ms step_avg:60.87ms
step:1957/2285 train_time:119127ms step_avg:60.87ms
step:1958/2285 train_time:119187ms step_avg:60.87ms
step:1959/2285 train_time:119250ms step_avg:60.87ms
step:1960/2285 train_time:119311ms step_avg:60.87ms
step:1961/2285 train_time:119374ms step_avg:60.87ms
step:1962/2285 train_time:119434ms step_avg:60.87ms
step:1963/2285 train_time:119496ms step_avg:60.87ms
step:1964/2285 train_time:119557ms step_avg:60.87ms
step:1965/2285 train_time:119620ms step_avg:60.88ms
step:1966/2285 train_time:119680ms step_avg:60.87ms
step:1967/2285 train_time:119743ms step_avg:60.88ms
step:1968/2285 train_time:119803ms step_avg:60.88ms
step:1969/2285 train_time:119865ms step_avg:60.88ms
step:1970/2285 train_time:119925ms step_avg:60.88ms
step:1971/2285 train_time:119988ms step_avg:60.88ms
step:1972/2285 train_time:120048ms step_avg:60.88ms
step:1973/2285 train_time:120111ms step_avg:60.88ms
step:1974/2285 train_time:120172ms step_avg:60.88ms
step:1975/2285 train_time:120235ms step_avg:60.88ms
step:1976/2285 train_time:120295ms step_avg:60.88ms
step:1977/2285 train_time:120358ms step_avg:60.88ms
step:1978/2285 train_time:120418ms step_avg:60.88ms
step:1979/2285 train_time:120481ms step_avg:60.88ms
step:1980/2285 train_time:120541ms step_avg:60.88ms
step:1981/2285 train_time:120604ms step_avg:60.88ms
step:1982/2285 train_time:120664ms step_avg:60.88ms
step:1983/2285 train_time:120726ms step_avg:60.88ms
step:1984/2285 train_time:120786ms step_avg:60.88ms
step:1985/2285 train_time:120848ms step_avg:60.88ms
step:1986/2285 train_time:120909ms step_avg:60.88ms
step:1987/2285 train_time:120971ms step_avg:60.88ms
step:1988/2285 train_time:121031ms step_avg:60.88ms
step:1989/2285 train_time:121093ms step_avg:60.88ms
step:1990/2285 train_time:121154ms step_avg:60.88ms
step:1991/2285 train_time:121217ms step_avg:60.88ms
step:1992/2285 train_time:121277ms step_avg:60.88ms
step:1993/2285 train_time:121340ms step_avg:60.88ms
step:1994/2285 train_time:121400ms step_avg:60.88ms
step:1995/2285 train_time:121463ms step_avg:60.88ms
step:1996/2285 train_time:121524ms step_avg:60.88ms
step:1997/2285 train_time:121586ms step_avg:60.88ms
step:1998/2285 train_time:121646ms step_avg:60.88ms
step:1999/2285 train_time:121708ms step_avg:60.88ms
step:2000/2285 train_time:121768ms step_avg:60.88ms
step:2000/2285 val_loss:3.3233 train_time:121832ms step_avg:60.92ms
step:2001/2285 train_time:121850ms step_avg:60.89ms
step:2002/2285 train_time:121896ms step_avg:60.89ms
step:2003/2285 train_time:121960ms step_avg:60.89ms
step:2004/2285 train_time:122020ms step_avg:60.89ms
step:2005/2285 train_time:122083ms step_avg:60.89ms
step:2006/2285 train_time:122143ms step_avg:60.89ms
step:2007/2285 train_time:122205ms step_avg:60.89ms
step:2008/2285 train_time:122266ms step_avg:60.89ms
step:2009/2285 train_time:122328ms step_avg:60.89ms
step:2010/2285 train_time:122387ms step_avg:60.89ms
step:2011/2285 train_time:122451ms step_avg:60.89ms
step:2012/2285 train_time:122510ms step_avg:60.89ms
step:2013/2285 train_time:122573ms step_avg:60.89ms
step:2014/2285 train_time:122634ms step_avg:60.89ms
step:2015/2285 train_time:122697ms step_avg:60.89ms
step:2016/2285 train_time:122757ms step_avg:60.89ms
step:2017/2285 train_time:122821ms step_avg:60.89ms
step:2018/2285 train_time:122883ms step_avg:60.89ms
step:2019/2285 train_time:122947ms step_avg:60.89ms
step:2020/2285 train_time:123008ms step_avg:60.90ms
step:2021/2285 train_time:123072ms step_avg:60.90ms
step:2022/2285 train_time:123132ms step_avg:60.90ms
step:2023/2285 train_time:123194ms step_avg:60.90ms
step:2024/2285 train_time:123254ms step_avg:60.90ms
step:2025/2285 train_time:123316ms step_avg:60.90ms
step:2026/2285 train_time:123376ms step_avg:60.90ms
step:2027/2285 train_time:123438ms step_avg:60.90ms
step:2028/2285 train_time:123498ms step_avg:60.90ms
step:2029/2285 train_time:123561ms step_avg:60.90ms
step:2030/2285 train_time:123622ms step_avg:60.90ms
step:2031/2285 train_time:123685ms step_avg:60.90ms
step:2032/2285 train_time:123746ms step_avg:60.90ms
step:2033/2285 train_time:123810ms step_avg:60.90ms
step:2034/2285 train_time:123871ms step_avg:60.90ms
step:2035/2285 train_time:123935ms step_avg:60.90ms
step:2036/2285 train_time:123995ms step_avg:60.90ms
step:2037/2285 train_time:124058ms step_avg:60.90ms
step:2038/2285 train_time:124118ms step_avg:60.90ms
step:2039/2285 train_time:124181ms step_avg:60.90ms
step:2040/2285 train_time:124242ms step_avg:60.90ms
step:2041/2285 train_time:124304ms step_avg:60.90ms
step:2042/2285 train_time:124365ms step_avg:60.90ms
step:2043/2285 train_time:124427ms step_avg:60.90ms
step:2044/2285 train_time:124488ms step_avg:60.90ms
step:2045/2285 train_time:124551ms step_avg:60.91ms
step:2046/2285 train_time:124611ms step_avg:60.90ms
step:2047/2285 train_time:124674ms step_avg:60.91ms
step:2048/2285 train_time:124734ms step_avg:60.91ms
step:2049/2285 train_time:124797ms step_avg:60.91ms
step:2050/2285 train_time:124858ms step_avg:60.91ms
step:2051/2285 train_time:124921ms step_avg:60.91ms
step:2052/2285 train_time:124983ms step_avg:60.91ms
step:2053/2285 train_time:125046ms step_avg:60.91ms
step:2054/2285 train_time:125107ms step_avg:60.91ms
step:2055/2285 train_time:125170ms step_avg:60.91ms
step:2056/2285 train_time:125231ms step_avg:60.91ms
step:2057/2285 train_time:125294ms step_avg:60.91ms
step:2058/2285 train_time:125354ms step_avg:60.91ms
step:2059/2285 train_time:125416ms step_avg:60.91ms
step:2060/2285 train_time:125476ms step_avg:60.91ms
step:2061/2285 train_time:125539ms step_avg:60.91ms
step:2062/2285 train_time:125599ms step_avg:60.91ms
step:2063/2285 train_time:125662ms step_avg:60.91ms
step:2064/2285 train_time:125723ms step_avg:60.91ms
step:2065/2285 train_time:125786ms step_avg:60.91ms
step:2066/2285 train_time:125847ms step_avg:60.91ms
step:2067/2285 train_time:125911ms step_avg:60.91ms
step:2068/2285 train_time:125971ms step_avg:60.91ms
step:2069/2285 train_time:126034ms step_avg:60.92ms
step:2070/2285 train_time:126095ms step_avg:60.92ms
step:2071/2285 train_time:126158ms step_avg:60.92ms
step:2072/2285 train_time:126218ms step_avg:60.92ms
step:2073/2285 train_time:126281ms step_avg:60.92ms
step:2074/2285 train_time:126342ms step_avg:60.92ms
step:2075/2285 train_time:126405ms step_avg:60.92ms
step:2076/2285 train_time:126465ms step_avg:60.92ms
step:2077/2285 train_time:126529ms step_avg:60.92ms
step:2078/2285 train_time:126590ms step_avg:60.92ms
step:2079/2285 train_time:126653ms step_avg:60.92ms
step:2080/2285 train_time:126713ms step_avg:60.92ms
step:2081/2285 train_time:126776ms step_avg:60.92ms
step:2082/2285 train_time:126836ms step_avg:60.92ms
step:2083/2285 train_time:126899ms step_avg:60.92ms
step:2084/2285 train_time:126960ms step_avg:60.92ms
step:2085/2285 train_time:127023ms step_avg:60.92ms
step:2086/2285 train_time:127084ms step_avg:60.92ms
step:2087/2285 train_time:127149ms step_avg:60.92ms
step:2088/2285 train_time:127209ms step_avg:60.92ms
step:2089/2285 train_time:127273ms step_avg:60.93ms
step:2090/2285 train_time:127334ms step_avg:60.93ms
step:2091/2285 train_time:127396ms step_avg:60.93ms
step:2092/2285 train_time:127456ms step_avg:60.93ms
step:2093/2285 train_time:127518ms step_avg:60.93ms
step:2094/2285 train_time:127578ms step_avg:60.93ms
step:2095/2285 train_time:127641ms step_avg:60.93ms
step:2096/2285 train_time:127701ms step_avg:60.93ms
step:2097/2285 train_time:127764ms step_avg:60.93ms
step:2098/2285 train_time:127824ms step_avg:60.93ms
step:2099/2285 train_time:127887ms step_avg:60.93ms
step:2100/2285 train_time:127947ms step_avg:60.93ms
step:2101/2285 train_time:128010ms step_avg:60.93ms
step:2102/2285 train_time:128071ms step_avg:60.93ms
step:2103/2285 train_time:128134ms step_avg:60.93ms
step:2104/2285 train_time:128194ms step_avg:60.93ms
step:2105/2285 train_time:128257ms step_avg:60.93ms
step:2106/2285 train_time:128317ms step_avg:60.93ms
step:2107/2285 train_time:128380ms step_avg:60.93ms
step:2108/2285 train_time:128442ms step_avg:60.93ms
step:2109/2285 train_time:128504ms step_avg:60.93ms
step:2110/2285 train_time:128565ms step_avg:60.93ms
step:2111/2285 train_time:128628ms step_avg:60.93ms
step:2112/2285 train_time:128688ms step_avg:60.93ms
step:2113/2285 train_time:128751ms step_avg:60.93ms
step:2114/2285 train_time:128811ms step_avg:60.93ms
step:2115/2285 train_time:128874ms step_avg:60.93ms
step:2116/2285 train_time:128934ms step_avg:60.93ms
step:2117/2285 train_time:128997ms step_avg:60.93ms
step:2118/2285 train_time:129058ms step_avg:60.93ms
step:2119/2285 train_time:129121ms step_avg:60.93ms
step:2120/2285 train_time:129181ms step_avg:60.93ms
step:2121/2285 train_time:129243ms step_avg:60.94ms
step:2122/2285 train_time:129304ms step_avg:60.93ms
step:2123/2285 train_time:129367ms step_avg:60.94ms
step:2124/2285 train_time:129429ms step_avg:60.94ms
step:2125/2285 train_time:129493ms step_avg:60.94ms
step:2126/2285 train_time:129553ms step_avg:60.94ms
step:2127/2285 train_time:129615ms step_avg:60.94ms
step:2128/2285 train_time:129675ms step_avg:60.94ms
step:2129/2285 train_time:129738ms step_avg:60.94ms
step:2130/2285 train_time:129797ms step_avg:60.94ms
step:2131/2285 train_time:129860ms step_avg:60.94ms
step:2132/2285 train_time:129921ms step_avg:60.94ms
step:2133/2285 train_time:129984ms step_avg:60.94ms
step:2134/2285 train_time:130045ms step_avg:60.94ms
step:2135/2285 train_time:130108ms step_avg:60.94ms
step:2136/2285 train_time:130168ms step_avg:60.94ms
step:2137/2285 train_time:130231ms step_avg:60.94ms
step:2138/2285 train_time:130292ms step_avg:60.94ms
step:2139/2285 train_time:130355ms step_avg:60.94ms
step:2140/2285 train_time:130415ms step_avg:60.94ms
step:2141/2285 train_time:130478ms step_avg:60.94ms
step:2142/2285 train_time:130538ms step_avg:60.94ms
step:2143/2285 train_time:130602ms step_avg:60.94ms
step:2144/2285 train_time:130662ms step_avg:60.94ms
step:2145/2285 train_time:130725ms step_avg:60.94ms
step:2146/2285 train_time:130785ms step_avg:60.94ms
step:2147/2285 train_time:130849ms step_avg:60.95ms
step:2148/2285 train_time:130910ms step_avg:60.95ms
step:2149/2285 train_time:130973ms step_avg:60.95ms
step:2150/2285 train_time:131033ms step_avg:60.95ms
step:2151/2285 train_time:131096ms step_avg:60.95ms
step:2152/2285 train_time:131156ms step_avg:60.95ms
step:2153/2285 train_time:131219ms step_avg:60.95ms
step:2154/2285 train_time:131279ms step_avg:60.95ms
step:2155/2285 train_time:131342ms step_avg:60.95ms
step:2156/2285 train_time:131403ms step_avg:60.95ms
step:2157/2285 train_time:131467ms step_avg:60.95ms
step:2158/2285 train_time:131528ms step_avg:60.95ms
step:2159/2285 train_time:131591ms step_avg:60.95ms
step:2160/2285 train_time:131651ms step_avg:60.95ms
step:2161/2285 train_time:131714ms step_avg:60.95ms
step:2162/2285 train_time:131774ms step_avg:60.95ms
step:2163/2285 train_time:131836ms step_avg:60.95ms
step:2164/2285 train_time:131897ms step_avg:60.95ms
step:2165/2285 train_time:131960ms step_avg:60.95ms
step:2166/2285 train_time:132021ms step_avg:60.95ms
step:2167/2285 train_time:132084ms step_avg:60.95ms
step:2168/2285 train_time:132145ms step_avg:60.95ms
step:2169/2285 train_time:132207ms step_avg:60.95ms
step:2170/2285 train_time:132268ms step_avg:60.95ms
step:2171/2285 train_time:132331ms step_avg:60.95ms
step:2172/2285 train_time:132392ms step_avg:60.95ms
step:2173/2285 train_time:132455ms step_avg:60.95ms
step:2174/2285 train_time:132515ms step_avg:60.95ms
step:2175/2285 train_time:132578ms step_avg:60.96ms
step:2176/2285 train_time:132637ms step_avg:60.95ms
step:2177/2285 train_time:132700ms step_avg:60.96ms
step:2178/2285 train_time:132761ms step_avg:60.96ms
step:2179/2285 train_time:132824ms step_avg:60.96ms
step:2180/2285 train_time:132885ms step_avg:60.96ms
step:2181/2285 train_time:132949ms step_avg:60.96ms
step:2182/2285 train_time:133010ms step_avg:60.96ms
step:2183/2285 train_time:133073ms step_avg:60.96ms
step:2184/2285 train_time:133134ms step_avg:60.96ms
step:2185/2285 train_time:133196ms step_avg:60.96ms
step:2186/2285 train_time:133256ms step_avg:60.96ms
step:2187/2285 train_time:133319ms step_avg:60.96ms
step:2188/2285 train_time:133379ms step_avg:60.96ms
step:2189/2285 train_time:133442ms step_avg:60.96ms
step:2190/2285 train_time:133503ms step_avg:60.96ms
step:2191/2285 train_time:133566ms step_avg:60.96ms
step:2192/2285 train_time:133626ms step_avg:60.96ms
step:2193/2285 train_time:133690ms step_avg:60.96ms
step:2194/2285 train_time:133751ms step_avg:60.96ms
step:2195/2285 train_time:133814ms step_avg:60.96ms
step:2196/2285 train_time:133873ms step_avg:60.96ms
step:2197/2285 train_time:133936ms step_avg:60.96ms
step:2198/2285 train_time:133996ms step_avg:60.96ms
step:2199/2285 train_time:134059ms step_avg:60.96ms
step:2200/2285 train_time:134120ms step_avg:60.96ms
step:2201/2285 train_time:134183ms step_avg:60.96ms
step:2202/2285 train_time:134243ms step_avg:60.96ms
step:2203/2285 train_time:134306ms step_avg:60.96ms
step:2204/2285 train_time:134366ms step_avg:60.96ms
step:2205/2285 train_time:134429ms step_avg:60.97ms
step:2206/2285 train_time:134490ms step_avg:60.97ms
step:2207/2285 train_time:134554ms step_avg:60.97ms
step:2208/2285 train_time:134614ms step_avg:60.97ms
step:2209/2285 train_time:134676ms step_avg:60.97ms
step:2210/2285 train_time:134737ms step_avg:60.97ms
step:2211/2285 train_time:134799ms step_avg:60.97ms
step:2212/2285 train_time:134860ms step_avg:60.97ms
step:2213/2285 train_time:134923ms step_avg:60.97ms
step:2214/2285 train_time:134984ms step_avg:60.97ms
step:2215/2285 train_time:135047ms step_avg:60.97ms
step:2216/2285 train_time:135108ms step_avg:60.97ms
step:2217/2285 train_time:135171ms step_avg:60.97ms
step:2218/2285 train_time:135232ms step_avg:60.97ms
step:2219/2285 train_time:135295ms step_avg:60.97ms
step:2220/2285 train_time:135355ms step_avg:60.97ms
step:2221/2285 train_time:135418ms step_avg:60.97ms
step:2222/2285 train_time:135478ms step_avg:60.97ms
step:2223/2285 train_time:135541ms step_avg:60.97ms
step:2224/2285 train_time:135601ms step_avg:60.97ms
step:2225/2285 train_time:135664ms step_avg:60.97ms
step:2226/2285 train_time:135724ms step_avg:60.97ms
step:2227/2285 train_time:135787ms step_avg:60.97ms
step:2228/2285 train_time:135848ms step_avg:60.97ms
step:2229/2285 train_time:135910ms step_avg:60.97ms
step:2230/2285 train_time:135971ms step_avg:60.97ms
step:2231/2285 train_time:136035ms step_avg:60.97ms
step:2232/2285 train_time:136095ms step_avg:60.97ms
step:2233/2285 train_time:136158ms step_avg:60.98ms
step:2234/2285 train_time:136218ms step_avg:60.97ms
step:2235/2285 train_time:136281ms step_avg:60.98ms
step:2236/2285 train_time:136343ms step_avg:60.98ms
step:2237/2285 train_time:136405ms step_avg:60.98ms
step:2238/2285 train_time:136466ms step_avg:60.98ms
step:2239/2285 train_time:136529ms step_avg:60.98ms
step:2240/2285 train_time:136590ms step_avg:60.98ms
step:2241/2285 train_time:136652ms step_avg:60.98ms
step:2242/2285 train_time:136712ms step_avg:60.98ms
step:2243/2285 train_time:136775ms step_avg:60.98ms
step:2244/2285 train_time:136835ms step_avg:60.98ms
step:2245/2285 train_time:136898ms step_avg:60.98ms
step:2246/2285 train_time:136957ms step_avg:60.98ms
step:2247/2285 train_time:137021ms step_avg:60.98ms
step:2248/2285 train_time:137082ms step_avg:60.98ms
step:2249/2285 train_time:137145ms step_avg:60.98ms
step:2250/2285 train_time:137206ms step_avg:60.98ms
step:2250/2285 val_loss:3.2866 train_time:137271ms step_avg:61.01ms
step:2251/2285 train_time:137289ms step_avg:60.99ms
step:2252/2285 train_time:137334ms step_avg:60.98ms
step:2253/2285 train_time:137398ms step_avg:60.98ms
step:2254/2285 train_time:137461ms step_avg:60.99ms
step:2255/2285 train_time:137525ms step_avg:60.99ms
step:2256/2285 train_time:137585ms step_avg:60.99ms
step:2257/2285 train_time:137647ms step_avg:60.99ms
step:2258/2285 train_time:137706ms step_avg:60.99ms
step:2259/2285 train_time:137768ms step_avg:60.99ms
step:2260/2285 train_time:137828ms step_avg:60.99ms
step:2261/2285 train_time:137890ms step_avg:60.99ms
step:2262/2285 train_time:137950ms step_avg:60.99ms
step:2263/2285 train_time:138012ms step_avg:60.99ms
step:2264/2285 train_time:138072ms step_avg:60.99ms
step:2265/2285 train_time:138134ms step_avg:60.99ms
step:2266/2285 train_time:138197ms step_avg:60.99ms
step:2267/2285 train_time:138264ms step_avg:60.99ms
step:2268/2285 train_time:138327ms step_avg:60.99ms
step:2269/2285 train_time:138392ms step_avg:60.99ms
step:2270/2285 train_time:138453ms step_avg:60.99ms
step:2271/2285 train_time:138516ms step_avg:60.99ms
step:2272/2285 train_time:138576ms step_avg:60.99ms
step:2273/2285 train_time:138639ms step_avg:60.99ms
step:2274/2285 train_time:138700ms step_avg:60.99ms
step:2275/2285 train_time:138763ms step_avg:60.99ms
step:2276/2285 train_time:138823ms step_avg:60.99ms
step:2277/2285 train_time:138886ms step_avg:60.99ms
step:2278/2285 train_time:138946ms step_avg:60.99ms
step:2279/2285 train_time:139008ms step_avg:61.00ms
step:2280/2285 train_time:139067ms step_avg:60.99ms
step:2281/2285 train_time:139130ms step_avg:61.00ms
step:2282/2285 train_time:139191ms step_avg:61.00ms
step:2283/2285 train_time:139254ms step_avg:61.00ms
step:2284/2285 train_time:139317ms step_avg:61.00ms
step:2285/2285 train_time:139380ms step_avg:61.00ms
step:2285/2285 val_loss:3.2804 train_time:139441ms step_avg:61.02ms
peak memory allocated: 29248 MiB reserved: 50528 MiB

import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from collections import defaultdict
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class NorMuon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).

    Differences from standard Muon:
    - Newton-Shulz is replaced with Polar Express for the orthogonalization step
    - NorMuon adds a low-rank variance estimator similar to Adafactor.
    - small 1D parameters handled via magnitude normalization of the grad (faster execution than Adam)
    - Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as
    needed on the forward pass. This enables attn and mlp weights to be contained within the same
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn.
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, beta2=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, beta2=beta2)
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def reset(self):
        # expose a reset for clearing buffers
        for group in self.param_groups:
            group["momentum_buffer"].zero_()
            group["second_momentum_buffer"].zero_()

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per module.
        """
        groups = defaultdict(list)
        for param in params:
            groups[param.label].append(param)

        param_groups = []
        for module_name, group_params in groups.items():
            chunk_size = (len(group_params) + self.world_size - 1) // self.world_size
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))

        return param_groups

    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn
        and mlp params when a param group is split across GPUs.
        """
        module_group_order = ['smear_gate', 'attn_gate', 'attn', 'mlp']
        params_list = list(params)
        params_list.sort(key=lambda x: module_group_order.index(x.label))

        idx = 0
        group_sizes = [1, 10, 16, 16]
        assert len(params_list) == sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            chunk_size = (size + self.world_size - 1) // self.world_size
            group_params = params_list[idx: idx + size]
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))
            idx += size

        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, @vagrawal, and @varunneal.
        rank = dist.get_rank()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            stacked_grads = torch.empty(
                (padded_num_params, *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device
            )
            for i, p in enumerate(params):
                stacked_grads[i].copy_(p.grad, non_blocking=True)
            if len(params) < padded_num_params:
                stacked_grads[len(params):].zero_()

            grad_chunk = torch.empty_like(stacked_grads[:chunk_size])

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(dict(grad_chunk=grad_chunk, reduce_future=reduce_future))

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = group["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            start_idx = rank * chunk_size
            module_idx = start_idx if start_idx < len(params) else 0

            num_params = min(chunk_size, max(0, len(params) - start_idx))  # num params for this rank

            if "momentum_buffer" not in group:
                group["momentum_buffer"]  = torch.zeros_like(grad_chunk[:num_params])
            momentum_buffer = group["momentum_buffer"]
            # Apply momentum update to the persistent momentum buffer in-place
            momentum_buffer.lerp_(grad_chunk[:num_params], 1 - group["momentum"])
            updated_grads = grad_chunk[:num_params].lerp_(momentum_buffer, group["momentum"])

            grad_shape = updated_grads.shape
            if params[module_idx].label == 'attn':
                # Reshape attn params from [hdim, dim*4] to [4,hdim,dim]
                for p in params[module_idx:module_idx + num_params]:
                    assert p.label == 'attn'
                updated_grads = updated_grads.view(4 * grad_shape[0], grad_shape[1], grad_shape[2] // 4)
            ref_param = params[module_idx]
            param_shape = ref_param.shape

            if "second_momentum_buffer" not in group:
                group["second_momentum_buffer"] = (torch.zeros_like(updated_grads[..., :, :1])
                    if param_shape[-2] >= param_shape[-1] else torch.zeros_like(updated_grads[..., :1, :])
                )
            second_momentum_buffer = group["second_momentum_buffer"]

            if "param_lr" not in group:
                group["param_lr"] = (
                    max(1., param_shape[-2] / param_shape[-1]) ** 0.5
                    * ref_param.new_tensor(
                        [getattr(param, "lr_mul", 1.0) for param in params[module_idx:module_idx + num_params]]
                    ).view(-1, 1, 1)
                )

                group["param_wd"] = ref_param.new_tensor(
                    [getattr(param, "wd_mul", 1.0) for param in params[module_idx:module_idx + num_params]]
                ).view(-1, 1, 1)

            # Determine LR and WR
            eff_lr = group["lr"] * group["param_lr"]
            eff_wd = group["weight_decay"] * group["param_wd"]

            # Compute zeropower for the entire chunk in a single, batched call.
            if num_params == 0:
                v_chunk = updated_grads
            else:
                v_chunk = polar_express(updated_grads)

            # NorMuon: second_momentum_buffer tracks squared magnitude of gradients along one dim (https://arxiv.org/pdf/2510.05491)
            v_norm = v_chunk.norm(dim=(-2, -1), keepdim=True)
            v_mean = v_chunk.square().mean(dim=-1 if param_shape[-2] >= param_shape[-1] else -2, keepdim=True)
            second_momentum_buffer.lerp_(v_mean.to(dtype=ref_param.dtype), 1 - group["beta2"])
            step_size = second_momentum_buffer.clamp_min(1e-10).rsqrt_()
            v_chunk.mul_(step_size)
            v_norm_new = v_chunk.norm(dim=(-2, -1), keepdim=True)
            v_chunk.mul_(v_norm / v_norm_new.clamp_min_(1e-10))

            v_chunk = v_chunk.view(grad_shape)

            updated_params = torch.empty_like(grad_chunk)
            param_chunk = torch.stack(params[module_idx:module_idx + num_params]) if num_params > 0 else torch.zeros_like(v_chunk)
            # Apply weight decay directly to the buffer.
            param_chunk.mul_(1 - eff_wd)

            param_chunk.add_(-eff_lr * v_chunk)

            updated_params[:num_params].copy_(param_chunk)
            if num_params < chunk_size:
                updated_params[num_params:].zero_()

            stacked_params = torch.empty(
                (padded_num_params, *param_shape),
                dtype=updated_params.dtype,
                device=updated_params.device,
            )

            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_params, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # init state
        for p in params:
            chunk_size = p.size(0) // self.world_size
            exp_avg = torch.zeros_like(p[:chunk_size], dtype=torch.bfloat16, device=p[0].device)
            exp_avg_sq = torch.zeros_like(exp_avg)
            self.state[p] = dict(step=0, exp_avg=exp_avg, exp_avg_sq=exp_avg_sq)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        reduce_scatter_futures: list[torch.Future] = []
        all_gather_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                grad = param.grad
                rank_size = grad.shape[0] // self.world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for param in params:
                reduce_scatter_futures[idx].wait()
                rank_size = param.shape[0] // self.world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]
                g_slice = grad_slices[idx]

                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (bias2 ** 0.5 / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        with torch.no_grad():
            self.weight.zero_()  # @Grad62304977 and others

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.label='attn'

        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.label = 'attn_gate'

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4, self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens,
                                                        max_seqlen_q=max_len, max_seqlen_k=max_len,
                                                        causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4, self.hdim, self.dim)[3].type_as(y))
        return y


class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.label = 'mlp'
        self.c_proj.label = 'mlp'
        # corrective factor to account for transpose
        self.c_fc.lr_mul = 2.

        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.label = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        smear_lambda = self.scalars[5 * len(self.blocks)]
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        # smear token embed forward 1 position @classiclarryd
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        n = len(self.blocks) // 2

        x_backout = None
        backout_layer = 8
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            # since layer 0 is skipped, layer 11 does not have skip_connection
            if i >= n and i<11:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()

    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 16 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2245  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: float = 0.50  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_final: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizer2 = NorMuon(hidden_matrix_params + gate_params, lr=0.03, momentum=0.95, beta2=0.95, weight_decay=0.0)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: flat, then linear decay, then flat
def get_lr(step: int):
    x = min(0.9999, step / args.num_scheduled_iterations)
    assert 0 <= x < 1
    lr = 1.0
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = w * 1.0 + (1 - w) * 0.1
    return lr

def get_ws(step: int):
    # set short window size to half of long window size
    # Higher ws on "extension" steps
    if step >= args.num_scheduled_iterations:
        return args.ws_final // 2, args.ws_final
    x = step / args.num_scheduled_iterations
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
ws_schedule = list(args.ws_schedule) + [args.ws_final]
ws_long = ws_schedule[0]
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    # each window size is a new graph, need to warm up each with Yarn.attn_scale
    ws_idx = step % len(ws_schedule)
    if ws_idx==0:
        model.yarn.reset()
        ws_long = ws_schedule[0]
    else:
        new_ws_long = ws_schedule[ws_idx]
        model.yarn.apply(ws_long, new_ws_long)
        ws_long = new_ws_long
    model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset() # rotary buffer is not stored in state_dict
model.load_state_dict(initial_state["model"])
optimizer2.reset() # muon momentum buffers not in state dict
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for _ in range(grad_accum_steps):
        inputs, targets, cum_seqlens = next(train_loader)
        model(inputs, targets, cum_seqlens, ws_short, ws_long).backward()
    step_optimizers(step, optimizers, model)

    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.10.12 (main, Aug 15 2025, 14:32:43) [GCC 11.4.0]
Running PyTorch 2.10.0.dev20250926+cu126 compiled for CUDA 12.6
Running Triton version 3.5.0
Thu Nov  6 04:33:58 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:19:00.0 Off |                    0 |
| N/A   39C    P0            126W /  700W |    5858MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:3B:00.0 Off |                    0 |
| N/A   32C    P0            123W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:4C:00.0 Off |                    0 |
| N/A   31C    P0            119W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:5D:00.0 Off |                    0 |
| N/A   39C    P0            129W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:9B:00.0 Off |                    0 |
| N/A   36C    P0            121W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:BB:00.0 Off |                    0 |
| N/A   29C    P0            119W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:CB:00.0 Off |                    0 |
| N/A   36C    P0            123W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:DB:00.0 Off |                    0 |
| N/A   29C    P0            118W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A     80859      C   /root/.venv/bin/python3                         0MiB |
|    0   N/A  N/A     80860      C   /root/.venv/bin/python3                         0MiB |
|    0   N/A  N/A     80861      C   /root/.venv/bin/python3                         0MiB |
|    0   N/A  N/A     80862      C   /root/.venv/bin/python3                         0MiB |
|    0   N/A  N/A     80863      C   /root/.venv/bin/python3                         0MiB |
|    0   N/A  N/A     80864      C   /root/.venv/bin/python3                         0MiB |
|    0   N/A  N/A     80865      C   /root/.venv/bin/python3                         0MiB |
|    0   N/A  N/A     80866      C   /root/.venv/bin/python3                         0MiB |
|    1   N/A  N/A     80860      C   /root/.venv/bin/python3                         0MiB |
|    2   N/A  N/A     80861      C   /root/.venv/bin/python3                         0MiB |
|    3   N/A  N/A     80862      C   /root/.venv/bin/python3                         0MiB |
|    4   N/A  N/A     80863      C   /root/.venv/bin/python3                         0MiB |
|    5   N/A  N/A     80864      C   /root/.venv/bin/python3                         0MiB |
|    6   N/A  N/A     80865      C   /root/.venv/bin/python3                         0MiB |
|    7   N/A  N/A     80866      C   /root/.venv/bin/python3                         0MiB |
+-----------------------------------------------------------------------------------------+

====================================================================================================
step:0/2285 val_loss:10.8258 train_time:0ms step_avg:0.02ms
step:1/2285 train_time:127ms step_avg:127.33ms
step:2/2285 train_time:149ms step_avg:74.40ms
step:3/2285 train_time:187ms step_avg:62.30ms
step:4/2285 train_time:243ms step_avg:60.80ms
step:5/2285 train_time:303ms step_avg:60.56ms
step:6/2285 train_time:361ms step_avg:60.17ms
step:7/2285 train_time:422ms step_avg:60.31ms
step:8/2285 train_time:481ms step_avg:60.11ms
step:9/2285 train_time:542ms step_avg:60.23ms
step:10/2285 train_time:601ms step_avg:60.09ms
step:11/2285 train_time:662ms step_avg:60.18ms
step:12/2285 train_time:721ms step_avg:60.08ms
step:13/2285 train_time:782ms step_avg:60.19ms
step:14/2285 train_time:842ms step_avg:60.11ms
step:15/2285 train_time:903ms step_avg:60.19ms
step:16/2285 train_time:962ms step_avg:60.12ms
step:17/2285 train_time:1026ms step_avg:60.34ms
step:18/2285 train_time:1089ms step_avg:60.52ms
step:19/2285 train_time:1154ms step_avg:60.76ms
step:20/2285 train_time:1215ms step_avg:60.77ms
step:21/2285 train_time:1278ms step_avg:60.87ms
step:22/2285 train_time:1338ms step_avg:60.81ms
step:23/2285 train_time:1400ms step_avg:60.86ms
step:24/2285 train_time:1459ms step_avg:60.78ms
step:25/2285 train_time:1520ms step_avg:60.80ms
step:26/2285 train_time:1580ms step_avg:60.77ms
step:27/2285 train_time:1642ms step_avg:60.83ms
step:28/2285 train_time:1701ms step_avg:60.75ms
step:29/2285 train_time:1762ms step_avg:60.77ms
step:30/2285 train_time:1821ms step_avg:60.71ms
step:31/2285 train_time:1883ms step_avg:60.74ms
step:32/2285 train_time:1942ms step_avg:60.69ms
step:33/2285 train_time:2004ms step_avg:60.73ms
step:34/2285 train_time:2066ms step_avg:60.75ms
step:35/2285 train_time:2131ms step_avg:60.89ms
step:36/2285 train_time:2191ms step_avg:60.87ms
step:37/2285 train_time:2253ms step_avg:60.90ms
step:38/2285 train_time:2313ms step_avg:60.87ms
step:39/2285 train_time:2375ms step_avg:60.90ms
step:40/2285 train_time:2435ms step_avg:60.87ms
step:41/2285 train_time:2497ms step_avg:60.89ms
step:42/2285 train_time:2556ms step_avg:60.86ms
step:43/2285 train_time:2618ms step_avg:60.88ms
step:44/2285 train_time:2677ms step_avg:60.84ms
step:45/2285 train_time:2739ms step_avg:60.86ms
step:46/2285 train_time:2798ms step_avg:60.83ms
step:47/2285 train_time:2860ms step_avg:60.86ms
step:48/2285 train_time:2920ms step_avg:60.83ms
step:49/2285 train_time:2983ms step_avg:60.87ms
step:50/2285 train_time:3043ms step_avg:60.85ms
step:51/2285 train_time:3106ms step_avg:60.90ms
step:52/2285 train_time:3166ms step_avg:60.89ms
step:53/2285 train_time:3229ms step_avg:60.93ms
step:54/2285 train_time:3289ms step_avg:60.90ms
step:55/2285 train_time:3351ms step_avg:60.93ms
step:56/2285 train_time:3411ms step_avg:60.91ms
step:57/2285 train_time:3474ms step_avg:60.94ms
step:58/2285 train_time:3533ms step_avg:60.92ms
step:59/2285 train_time:3595ms step_avg:60.94ms
step:60/2285 train_time:3655ms step_avg:60.91ms
step:61/2285 train_time:3716ms step_avg:60.92ms
step:62/2285 train_time:3775ms step_avg:60.89ms
step:63/2285 train_time:3837ms step_avg:60.91ms
step:64/2285 train_time:3896ms step_avg:60.88ms
step:65/2285 train_time:3958ms step_avg:60.90ms
step:66/2285 train_time:4018ms step_avg:60.87ms
step:67/2285 train_time:4081ms step_avg:60.91ms
step:68/2285 train_time:4141ms step_avg:60.89ms
step:69/2285 train_time:4203ms step_avg:60.92ms
step:70/2285 train_time:4263ms step_avg:60.90ms
step:71/2285 train_time:4326ms step_avg:60.93ms
step:72/2285 train_time:4385ms step_avg:60.90ms
step:73/2285 train_time:4447ms step_avg:60.92ms
step:74/2285 train_time:4507ms step_avg:60.91ms
step:75/2285 train_time:4570ms step_avg:60.93ms
step:76/2285 train_time:4630ms step_avg:60.92ms
step:77/2285 train_time:4692ms step_avg:60.93ms
step:78/2285 train_time:4752ms step_avg:60.92ms
step:79/2285 train_time:4814ms step_avg:60.93ms
step:80/2285 train_time:4872ms step_avg:60.91ms
step:81/2285 train_time:4934ms step_avg:60.91ms
step:82/2285 train_time:4993ms step_avg:60.89ms
step:83/2285 train_time:5055ms step_avg:60.90ms
step:84/2285 train_time:5114ms step_avg:60.88ms
step:85/2285 train_time:5176ms step_avg:60.90ms
step:86/2285 train_time:5236ms step_avg:60.88ms
step:87/2285 train_time:5299ms step_avg:60.90ms
step:88/2285 train_time:5358ms step_avg:60.89ms
step:89/2285 train_time:5421ms step_avg:60.91ms
step:90/2285 train_time:5481ms step_avg:60.90ms
step:91/2285 train_time:5543ms step_avg:60.91ms
step:92/2285 train_time:5602ms step_avg:60.89ms
step:93/2285 train_time:5664ms step_avg:60.90ms
step:94/2285 train_time:5724ms step_avg:60.89ms
step:95/2285 train_time:5786ms step_avg:60.91ms
step:96/2285 train_time:5846ms step_avg:60.89ms
step:97/2285 train_time:5908ms step_avg:60.90ms
step:98/2285 train_time:5968ms step_avg:60.90ms
step:99/2285 train_time:6030ms step_avg:60.91ms
step:100/2285 train_time:6090ms step_avg:60.90ms
step:101/2285 train_time:6151ms step_avg:60.90ms
step:102/2285 train_time:6211ms step_avg:60.89ms
step:103/2285 train_time:6273ms step_avg:60.90ms
step:104/2285 train_time:6332ms step_avg:60.88ms
step:105/2285 train_time:6394ms step_avg:60.89ms
step:106/2285 train_time:6452ms step_avg:60.87ms
step:107/2285 train_time:6514ms step_avg:60.87ms
step:108/2285 train_time:6573ms step_avg:60.86ms
step:109/2285 train_time:6634ms step_avg:60.86ms
step:110/2285 train_time:6693ms step_avg:60.85ms
step:111/2285 train_time:6755ms step_avg:60.86ms
step:112/2285 train_time:6815ms step_avg:60.85ms
step:113/2285 train_time:6877ms step_avg:60.85ms
step:114/2285 train_time:6936ms step_avg:60.84ms
step:115/2285 train_time:6998ms step_avg:60.85ms
step:116/2285 train_time:7057ms step_avg:60.84ms
step:117/2285 train_time:7119ms step_avg:60.85ms
step:118/2285 train_time:7179ms step_avg:60.83ms
step:119/2285 train_time:7240ms step_avg:60.84ms
step:120/2285 train_time:7299ms step_avg:60.83ms
step:121/2285 train_time:7360ms step_avg:60.83ms
step:122/2285 train_time:7419ms step_avg:60.81ms
step:123/2285 train_time:7480ms step_avg:60.81ms
step:124/2285 train_time:7539ms step_avg:60.80ms
step:125/2285 train_time:7601ms step_avg:60.80ms
step:126/2285 train_time:7660ms step_avg:60.80ms
step:127/2285 train_time:7722ms step_avg:60.80ms
step:128/2285 train_time:7781ms step_avg:60.79ms
step:129/2285 train_time:7843ms step_avg:60.80ms
step:130/2285 train_time:7902ms step_avg:60.79ms
step:131/2285 train_time:7964ms step_avg:60.79ms
step:132/2285 train_time:8023ms step_avg:60.78ms
step:133/2285 train_time:8085ms step_avg:60.79ms
step:134/2285 train_time:8145ms step_avg:60.78ms
step:135/2285 train_time:8206ms step_avg:60.79ms
step:136/2285 train_time:8266ms step_avg:60.78ms
step:137/2285 train_time:8328ms step_avg:60.79ms
step:138/2285 train_time:8387ms step_avg:60.78ms
step:139/2285 train_time:8449ms step_avg:60.78ms
step:140/2285 train_time:8509ms step_avg:60.78ms
step:141/2285 train_time:8571ms step_avg:60.79ms
step:142/2285 train_time:8630ms step_avg:60.77ms
step:143/2285 train_time:8691ms step_avg:60.78ms
step:144/2285 train_time:8750ms step_avg:60.77ms
step:145/2285 train_time:8813ms step_avg:60.78ms
step:146/2285 train_time:8872ms step_avg:60.76ms
step:147/2285 train_time:8934ms step_avg:60.77ms
step:148/2285 train_time:8993ms step_avg:60.76ms
step:149/2285 train_time:9054ms step_avg:60.77ms
step:150/2285 train_time:9113ms step_avg:60.75ms
step:151/2285 train_time:9175ms step_avg:60.76ms
step:152/2285 train_time:9233ms step_avg:60.75ms
step:153/2285 train_time:9295ms step_avg:60.75ms
step:154/2285 train_time:9354ms step_avg:60.74ms
step:155/2285 train_time:9416ms step_avg:60.75ms
step:156/2285 train_time:9476ms step_avg:60.75ms
step:157/2285 train_time:9538ms step_avg:60.75ms
step:158/2285 train_time:9597ms step_avg:60.74ms
step:159/2285 train_time:9659ms step_avg:60.75ms
step:160/2285 train_time:9718ms step_avg:60.74ms
step:161/2285 train_time:9780ms step_avg:60.75ms
step:162/2285 train_time:9839ms step_avg:60.74ms
step:163/2285 train_time:9901ms step_avg:60.74ms
step:164/2285 train_time:9960ms step_avg:60.73ms
step:165/2285 train_time:10021ms step_avg:60.73ms
step:166/2285 train_time:10080ms step_avg:60.72ms
step:167/2285 train_time:10141ms step_avg:60.73ms
step:168/2285 train_time:10200ms step_avg:60.72ms
step:169/2285 train_time:10261ms step_avg:60.72ms
step:170/2285 train_time:10321ms step_avg:60.71ms
step:171/2285 train_time:10383ms step_avg:60.72ms
step:172/2285 train_time:10443ms step_avg:60.71ms
step:173/2285 train_time:10504ms step_avg:60.72ms
step:174/2285 train_time:10564ms step_avg:60.71ms
step:175/2285 train_time:10625ms step_avg:60.72ms
step:176/2285 train_time:10685ms step_avg:60.71ms
step:177/2285 train_time:10747ms step_avg:60.71ms
step:178/2285 train_time:10806ms step_avg:60.71ms
step:179/2285 train_time:10868ms step_avg:60.72ms
step:180/2285 train_time:10927ms step_avg:60.71ms
step:181/2285 train_time:10989ms step_avg:60.71ms
step:182/2285 train_time:11048ms step_avg:60.70ms
step:183/2285 train_time:11110ms step_avg:60.71ms
step:184/2285 train_time:11170ms step_avg:60.71ms
step:185/2285 train_time:11232ms step_avg:60.71ms
step:186/2285 train_time:11291ms step_avg:60.70ms
step:187/2285 train_time:11353ms step_avg:60.71ms
step:188/2285 train_time:11412ms step_avg:60.70ms
step:189/2285 train_time:11473ms step_avg:60.70ms
step:190/2285 train_time:11533ms step_avg:60.70ms
step:191/2285 train_time:11594ms step_avg:60.70ms
step:192/2285 train_time:11653ms step_avg:60.69ms
step:193/2285 train_time:11715ms step_avg:60.70ms
step:194/2285 train_time:11774ms step_avg:60.69ms
step:195/2285 train_time:11836ms step_avg:60.70ms
step:196/2285 train_time:11896ms step_avg:60.69ms
step:197/2285 train_time:11958ms step_avg:60.70ms
step:198/2285 train_time:12017ms step_avg:60.69ms
step:199/2285 train_time:12079ms step_avg:60.70ms
step:200/2285 train_time:12138ms step_avg:60.69ms
step:201/2285 train_time:12200ms step_avg:60.70ms
step:202/2285 train_time:12259ms step_avg:60.69ms
step:203/2285 train_time:12320ms step_avg:60.69ms
step:204/2285 train_time:12379ms step_avg:60.68ms
step:205/2285 train_time:12441ms step_avg:60.69ms
step:206/2285 train_time:12500ms step_avg:60.68ms
step:207/2285 train_time:12561ms step_avg:60.68ms
step:208/2285 train_time:12620ms step_avg:60.67ms
step:209/2285 train_time:12682ms step_avg:60.68ms
step:210/2285 train_time:12741ms step_avg:60.67ms
step:211/2285 train_time:12802ms step_avg:60.67ms
step:212/2285 train_time:12861ms step_avg:60.67ms
step:213/2285 train_time:12923ms step_avg:60.67ms
step:214/2285 train_time:12982ms step_avg:60.66ms
step:215/2285 train_time:13044ms step_avg:60.67ms
step:216/2285 train_time:13104ms step_avg:60.67ms
step:217/2285 train_time:13165ms step_avg:60.67ms
step:218/2285 train_time:13225ms step_avg:60.67ms
step:219/2285 train_time:13287ms step_avg:60.67ms
step:220/2285 train_time:13347ms step_avg:60.67ms
step:221/2285 train_time:13410ms step_avg:60.68ms
step:222/2285 train_time:13470ms step_avg:60.68ms
step:223/2285 train_time:13532ms step_avg:60.68ms
step:224/2285 train_time:13590ms step_avg:60.67ms
step:225/2285 train_time:13652ms step_avg:60.67ms
step:226/2285 train_time:13711ms step_avg:60.67ms
step:227/2285 train_time:13772ms step_avg:60.67ms
step:228/2285 train_time:13830ms step_avg:60.66ms
step:229/2285 train_time:13892ms step_avg:60.66ms
step:230/2285 train_time:13951ms step_avg:60.65ms
step:231/2285 train_time:14013ms step_avg:60.66ms
step:232/2285 train_time:14072ms step_avg:60.65ms
step:233/2285 train_time:14133ms step_avg:60.66ms
step:234/2285 train_time:14192ms step_avg:60.65ms
step:235/2285 train_time:14254ms step_avg:60.65ms
step:236/2285 train_time:14313ms step_avg:60.65ms
step:237/2285 train_time:14375ms step_avg:60.65ms
step:238/2285 train_time:14433ms step_avg:60.64ms
step:239/2285 train_time:14495ms step_avg:60.65ms
step:240/2285 train_time:14554ms step_avg:60.64ms
step:241/2285 train_time:14615ms step_avg:60.64ms
step:242/2285 train_time:14674ms step_avg:60.64ms
step:243/2285 train_time:14736ms step_avg:60.64ms
step:244/2285 train_time:14795ms step_avg:60.63ms
step:245/2285 train_time:14856ms step_avg:60.64ms
step:246/2285 train_time:14915ms step_avg:60.63ms
step:247/2285 train_time:14977ms step_avg:60.64ms
step:248/2285 train_time:15036ms step_avg:60.63ms
step:249/2285 train_time:15098ms step_avg:60.63ms
step:250/2285 train_time:15157ms step_avg:60.63ms
step:250/2285 val_loss:4.0770 train_time:15220ms step_avg:60.88ms
step:251/2285 train_time:15239ms step_avg:60.71ms
step:252/2285 train_time:15282ms step_avg:60.64ms
step:253/2285 train_time:15349ms step_avg:60.67ms
step:254/2285 train_time:15417ms step_avg:60.70ms
step:255/2285 train_time:15479ms step_avg:60.70ms
step:256/2285 train_time:15538ms step_avg:60.69ms
step:257/2285 train_time:15599ms step_avg:60.70ms
step:258/2285 train_time:15657ms step_avg:60.69ms
step:259/2285 train_time:15718ms step_avg:60.69ms
step:260/2285 train_time:15776ms step_avg:60.68ms
step:261/2285 train_time:15836ms step_avg:60.68ms
step:262/2285 train_time:15894ms step_avg:60.67ms
step:263/2285 train_time:15955ms step_avg:60.67ms
step:264/2285 train_time:16013ms step_avg:60.66ms
step:265/2285 train_time:16074ms step_avg:60.66ms
step:266/2285 train_time:16132ms step_avg:60.65ms
step:267/2285 train_time:16194ms step_avg:60.65ms
step:268/2285 train_time:16253ms step_avg:60.65ms
step:269/2285 train_time:16318ms step_avg:60.66ms
step:270/2285 train_time:16380ms step_avg:60.67ms
step:271/2285 train_time:16443ms step_avg:60.68ms
step:272/2285 train_time:16502ms step_avg:60.67ms
step:273/2285 train_time:16564ms step_avg:60.67ms
step:274/2285 train_time:16623ms step_avg:60.67ms
step:275/2285 train_time:16684ms step_avg:60.67ms
step:276/2285 train_time:16742ms step_avg:60.66ms
step:277/2285 train_time:16803ms step_avg:60.66ms
step:278/2285 train_time:16862ms step_avg:60.65ms
step:279/2285 train_time:16923ms step_avg:60.66ms
step:280/2285 train_time:16982ms step_avg:60.65ms
step:281/2285 train_time:17043ms step_avg:60.65ms
step:282/2285 train_time:17102ms step_avg:60.64ms
step:283/2285 train_time:17163ms step_avg:60.65ms
step:284/2285 train_time:17222ms step_avg:60.64ms
step:285/2285 train_time:17285ms step_avg:60.65ms
step:286/2285 train_time:17345ms step_avg:60.65ms
step:287/2285 train_time:17407ms step_avg:60.65ms
step:288/2285 train_time:17467ms step_avg:60.65ms
step:289/2285 train_time:17529ms step_avg:60.65ms
step:290/2285 train_time:17588ms step_avg:60.65ms
step:291/2285 train_time:17652ms step_avg:60.66ms
step:292/2285 train_time:17711ms step_avg:60.65ms
step:293/2285 train_time:17773ms step_avg:60.66ms
step:294/2285 train_time:17831ms step_avg:60.65ms
step:295/2285 train_time:17892ms step_avg:60.65ms
step:296/2285 train_time:17951ms step_avg:60.64ms
step:297/2285 train_time:18012ms step_avg:60.65ms
step:298/2285 train_time:18071ms step_avg:60.64ms
step:299/2285 train_time:18132ms step_avg:60.64ms
step:300/2285 train_time:18192ms step_avg:60.64ms
step:301/2285 train_time:18254ms step_avg:60.64ms
step:302/2285 train_time:18312ms step_avg:60.64ms
step:303/2285 train_time:18374ms step_avg:60.64ms
step:304/2285 train_time:18433ms step_avg:60.63ms
step:305/2285 train_time:18494ms step_avg:60.64ms
step:306/2285 train_time:18553ms step_avg:60.63ms
step:307/2285 train_time:18615ms step_avg:60.63ms
step:308/2285 train_time:18674ms step_avg:60.63ms
step:309/2285 train_time:18735ms step_avg:60.63ms
step:310/2285 train_time:18794ms step_avg:60.63ms
step:311/2285 train_time:18856ms step_avg:60.63ms
step:312/2285 train_time:18915ms step_avg:60.62ms
step:313/2285 train_time:18976ms step_avg:60.63ms
step:314/2285 train_time:19035ms step_avg:60.62ms
step:315/2285 train_time:19096ms step_avg:60.62ms
step:316/2285 train_time:19155ms step_avg:60.62ms
step:317/2285 train_time:19216ms step_avg:60.62ms
step:318/2285 train_time:19275ms step_avg:60.61ms
step:319/2285 train_time:19336ms step_avg:60.62ms
step:320/2285 train_time:19395ms step_avg:60.61ms
step:321/2285 train_time:19457ms step_avg:60.61ms
step:322/2285 train_time:19516ms step_avg:60.61ms
step:323/2285 train_time:19577ms step_avg:60.61ms
step:324/2285 train_time:19636ms step_avg:60.61ms
step:325/2285 train_time:19698ms step_avg:60.61ms
step:326/2285 train_time:19757ms step_avg:60.61ms
step:327/2285 train_time:19820ms step_avg:60.61ms
step:328/2285 train_time:19879ms step_avg:60.61ms
step:329/2285 train_time:19941ms step_avg:60.61ms
step:330/2285 train_time:20000ms step_avg:60.60ms
step:331/2285 train_time:20062ms step_avg:60.61ms
step:332/2285 train_time:20121ms step_avg:60.60ms
step:333/2285 train_time:20182ms step_avg:60.61ms
step:334/2285 train_time:20241ms step_avg:60.60ms
step:335/2285 train_time:20302ms step_avg:60.60ms
step:336/2285 train_time:20361ms step_avg:60.60ms
step:337/2285 train_time:20423ms step_avg:60.60ms
step:338/2285 train_time:20482ms step_avg:60.60ms
step:339/2285 train_time:20544ms step_avg:60.60ms
step:340/2285 train_time:20603ms step_avg:60.60ms
step:341/2285 train_time:20664ms step_avg:60.60ms
step:342/2285 train_time:20724ms step_avg:60.60ms
step:343/2285 train_time:20786ms step_avg:60.60ms
step:344/2285 train_time:20845ms step_avg:60.60ms
step:345/2285 train_time:20907ms step_avg:60.60ms
step:346/2285 train_time:20967ms step_avg:60.60ms
step:347/2285 train_time:21029ms step_avg:60.60ms
step:348/2285 train_time:21088ms step_avg:60.60ms
step:349/2285 train_time:21150ms step_avg:60.60ms
step:350/2285 train_time:21209ms step_avg:60.60ms
step:351/2285 train_time:21270ms step_avg:60.60ms
step:352/2285 train_time:21329ms step_avg:60.59ms
step:353/2285 train_time:21391ms step_avg:60.60ms
step:354/2285 train_time:21449ms step_avg:60.59ms
step:355/2285 train_time:21511ms step_avg:60.59ms
step:356/2285 train_time:21570ms step_avg:60.59ms
step:357/2285 train_time:21632ms step_avg:60.59ms
step:358/2285 train_time:21691ms step_avg:60.59ms
step:359/2285 train_time:21753ms step_avg:60.59ms
step:360/2285 train_time:21812ms step_avg:60.59ms
step:361/2285 train_time:21874ms step_avg:60.59ms
step:362/2285 train_time:21932ms step_avg:60.59ms
step:363/2285 train_time:21994ms step_avg:60.59ms
step:364/2285 train_time:22053ms step_avg:60.59ms
step:365/2285 train_time:22114ms step_avg:60.59ms
step:366/2285 train_time:22173ms step_avg:60.58ms
step:367/2285 train_time:22234ms step_avg:60.58ms
step:368/2285 train_time:22293ms step_avg:60.58ms
step:369/2285 train_time:22354ms step_avg:60.58ms
step:370/2285 train_time:22413ms step_avg:60.57ms
step:371/2285 train_time:22474ms step_avg:60.58ms
step:372/2285 train_time:22533ms step_avg:60.57ms
step:373/2285 train_time:22594ms step_avg:60.57ms
step:374/2285 train_time:22653ms step_avg:60.57ms
step:375/2285 train_time:22715ms step_avg:60.57ms
step:376/2285 train_time:22773ms step_avg:60.57ms
step:377/2285 train_time:22835ms step_avg:60.57ms
step:378/2285 train_time:22894ms step_avg:60.57ms
step:379/2285 train_time:22956ms step_avg:60.57ms
step:380/2285 train_time:23015ms step_avg:60.56ms
step:381/2285 train_time:23076ms step_avg:60.57ms
step:382/2285 train_time:23135ms step_avg:60.56ms
step:383/2285 train_time:23197ms step_avg:60.57ms
step:384/2285 train_time:23256ms step_avg:60.56ms
step:385/2285 train_time:23318ms step_avg:60.57ms
step:386/2285 train_time:23377ms step_avg:60.56ms
step:387/2285 train_time:23438ms step_avg:60.56ms
step:388/2285 train_time:23497ms step_avg:60.56ms
step:389/2285 train_time:23559ms step_avg:60.56ms
step:390/2285 train_time:23618ms step_avg:60.56ms
step:391/2285 train_time:23680ms step_avg:60.56ms
step:392/2285 train_time:23739ms step_avg:60.56ms
step:393/2285 train_time:23800ms step_avg:60.56ms
step:394/2285 train_time:23859ms step_avg:60.55ms
step:395/2285 train_time:23921ms step_avg:60.56ms
step:396/2285 train_time:23980ms step_avg:60.55ms
step:397/2285 train_time:24041ms step_avg:60.56ms
step:398/2285 train_time:24100ms step_avg:60.55ms
step:399/2285 train_time:24162ms step_avg:60.56ms
step:400/2285 train_time:24222ms step_avg:60.55ms
step:401/2285 train_time:24283ms step_avg:60.56ms
step:402/2285 train_time:24341ms step_avg:60.55ms
step:403/2285 train_time:24403ms step_avg:60.55ms
step:404/2285 train_time:24462ms step_avg:60.55ms
step:405/2285 train_time:24523ms step_avg:60.55ms
step:406/2285 train_time:24583ms step_avg:60.55ms
step:407/2285 train_time:24644ms step_avg:60.55ms
step:408/2285 train_time:24703ms step_avg:60.55ms
step:409/2285 train_time:24765ms step_avg:60.55ms
step:410/2285 train_time:24824ms step_avg:60.55ms
step:411/2285 train_time:24885ms step_avg:60.55ms
step:412/2285 train_time:24945ms step_avg:60.55ms
step:413/2285 train_time:25006ms step_avg:60.55ms
step:414/2285 train_time:25066ms step_avg:60.55ms
step:415/2285 train_time:25127ms step_avg:60.55ms
step:416/2285 train_time:25187ms step_avg:60.54ms
step:417/2285 train_time:25248ms step_avg:60.55ms
step:418/2285 train_time:25307ms step_avg:60.54ms
step:419/2285 train_time:25368ms step_avg:60.54ms
step:420/2285 train_time:25427ms step_avg:60.54ms
step:421/2285 train_time:25489ms step_avg:60.54ms
step:422/2285 train_time:25547ms step_avg:60.54ms
step:423/2285 train_time:25609ms step_avg:60.54ms
step:424/2285 train_time:25668ms step_avg:60.54ms
step:425/2285 train_time:25730ms step_avg:60.54ms
step:426/2285 train_time:25789ms step_avg:60.54ms
step:427/2285 train_time:25851ms step_avg:60.54ms
step:428/2285 train_time:25910ms step_avg:60.54ms
step:429/2285 train_time:25971ms step_avg:60.54ms
step:430/2285 train_time:26030ms step_avg:60.54ms
step:431/2285 train_time:26092ms step_avg:60.54ms
step:432/2285 train_time:26151ms step_avg:60.54ms
step:433/2285 train_time:26213ms step_avg:60.54ms
step:434/2285 train_time:26272ms step_avg:60.53ms
step:435/2285 train_time:26333ms step_avg:60.54ms
step:436/2285 train_time:26392ms step_avg:60.53ms
step:437/2285 train_time:26453ms step_avg:60.53ms
step:438/2285 train_time:26512ms step_avg:60.53ms
step:439/2285 train_time:26573ms step_avg:60.53ms
step:440/2285 train_time:26632ms step_avg:60.53ms
step:441/2285 train_time:26694ms step_avg:60.53ms
step:442/2285 train_time:26753ms step_avg:60.53ms
step:443/2285 train_time:26814ms step_avg:60.53ms
step:444/2285 train_time:26872ms step_avg:60.52ms
step:445/2285 train_time:26933ms step_avg:60.52ms
step:446/2285 train_time:26992ms step_avg:60.52ms
step:447/2285 train_time:27054ms step_avg:60.52ms
step:448/2285 train_time:27113ms step_avg:60.52ms
step:449/2285 train_time:27174ms step_avg:60.52ms
step:450/2285 train_time:27233ms step_avg:60.52ms
step:451/2285 train_time:27294ms step_avg:60.52ms
step:452/2285 train_time:27353ms step_avg:60.52ms
step:453/2285 train_time:27414ms step_avg:60.52ms
step:454/2285 train_time:27473ms step_avg:60.51ms
step:455/2285 train_time:27534ms step_avg:60.51ms
step:456/2285 train_time:27593ms step_avg:60.51ms
step:457/2285 train_time:27654ms step_avg:60.51ms
step:458/2285 train_time:27713ms step_avg:60.51ms
step:459/2285 train_time:27774ms step_avg:60.51ms
step:460/2285 train_time:27832ms step_avg:60.50ms
step:461/2285 train_time:27893ms step_avg:60.51ms
step:462/2285 train_time:27952ms step_avg:60.50ms
step:463/2285 train_time:28013ms step_avg:60.50ms
step:464/2285 train_time:28072ms step_avg:60.50ms
step:465/2285 train_time:28134ms step_avg:60.50ms
step:466/2285 train_time:28193ms step_avg:60.50ms
step:467/2285 train_time:28255ms step_avg:60.50ms
step:468/2285 train_time:28313ms step_avg:60.50ms
step:469/2285 train_time:28374ms step_avg:60.50ms
step:470/2285 train_time:28433ms step_avg:60.50ms
step:471/2285 train_time:28494ms step_avg:60.50ms
step:472/2285 train_time:28553ms step_avg:60.49ms
step:473/2285 train_time:28614ms step_avg:60.49ms
step:474/2285 train_time:28672ms step_avg:60.49ms
step:475/2285 train_time:28734ms step_avg:60.49ms
step:476/2285 train_time:28793ms step_avg:60.49ms
step:477/2285 train_time:28855ms step_avg:60.49ms
step:478/2285 train_time:28913ms step_avg:60.49ms
step:479/2285 train_time:28975ms step_avg:60.49ms
step:480/2285 train_time:29034ms step_avg:60.49ms
step:481/2285 train_time:29096ms step_avg:60.49ms
step:482/2285 train_time:29153ms step_avg:60.48ms
step:483/2285 train_time:29215ms step_avg:60.49ms
step:484/2285 train_time:29274ms step_avg:60.48ms
step:485/2285 train_time:29335ms step_avg:60.48ms
step:486/2285 train_time:29394ms step_avg:60.48ms
step:487/2285 train_time:29455ms step_avg:60.48ms
step:488/2285 train_time:29514ms step_avg:60.48ms
step:489/2285 train_time:29575ms step_avg:60.48ms
step:490/2285 train_time:29633ms step_avg:60.48ms
step:491/2285 train_time:29695ms step_avg:60.48ms
step:492/2285 train_time:29753ms step_avg:60.47ms
step:493/2285 train_time:29815ms step_avg:60.48ms
step:494/2285 train_time:29873ms step_avg:60.47ms
step:495/2285 train_time:29934ms step_avg:60.47ms
step:496/2285 train_time:29993ms step_avg:60.47ms
step:497/2285 train_time:30055ms step_avg:60.47ms
step:498/2285 train_time:30113ms step_avg:60.47ms
step:499/2285 train_time:30175ms step_avg:60.47ms
step:500/2285 train_time:30233ms step_avg:60.47ms
step:500/2285 val_loss:3.8092 train_time:30296ms step_avg:60.59ms
step:501/2285 train_time:30315ms step_avg:60.51ms
step:502/2285 train_time:30356ms step_avg:60.47ms
step:503/2285 train_time:30421ms step_avg:60.48ms
step:504/2285 train_time:30483ms step_avg:60.48ms
step:505/2285 train_time:30545ms step_avg:60.48ms
step:506/2285 train_time:30604ms step_avg:60.48ms
step:507/2285 train_time:30665ms step_avg:60.48ms
step:508/2285 train_time:30723ms step_avg:60.48ms
step:509/2285 train_time:30784ms step_avg:60.48ms
step:510/2285 train_time:30842ms step_avg:60.47ms
step:511/2285 train_time:30902ms step_avg:60.47ms
step:512/2285 train_time:30960ms step_avg:60.47ms
step:513/2285 train_time:31021ms step_avg:60.47ms
step:514/2285 train_time:31079ms step_avg:60.46ms
step:515/2285 train_time:31140ms step_avg:60.47ms
step:516/2285 train_time:31198ms step_avg:60.46ms
step:517/2285 train_time:31260ms step_avg:60.46ms
step:518/2285 train_time:31320ms step_avg:60.46ms
step:519/2285 train_time:31383ms step_avg:60.47ms
step:520/2285 train_time:31444ms step_avg:60.47ms
step:521/2285 train_time:31506ms step_avg:60.47ms
step:522/2285 train_time:31566ms step_avg:60.47ms
step:523/2285 train_time:31627ms step_avg:60.47ms
step:524/2285 train_time:31686ms step_avg:60.47ms
step:525/2285 train_time:31747ms step_avg:60.47ms
step:526/2285 train_time:31806ms step_avg:60.47ms
step:527/2285 train_time:31867ms step_avg:60.47ms
step:528/2285 train_time:31926ms step_avg:60.47ms
step:529/2285 train_time:31987ms step_avg:60.47ms
step:530/2285 train_time:32046ms step_avg:60.46ms
step:531/2285 train_time:32108ms step_avg:60.47ms
step:532/2285 train_time:32167ms step_avg:60.46ms
step:533/2285 train_time:32229ms step_avg:60.47ms
step:534/2285 train_time:32289ms step_avg:60.47ms
step:535/2285 train_time:32351ms step_avg:60.47ms
step:536/2285 train_time:32412ms step_avg:60.47ms
step:537/2285 train_time:32473ms step_avg:60.47ms
step:538/2285 train_time:32533ms step_avg:60.47ms
step:539/2285 train_time:32595ms step_avg:60.47ms
step:540/2285 train_time:32654ms step_avg:60.47ms
step:541/2285 train_time:32716ms step_avg:60.47ms
step:542/2285 train_time:32775ms step_avg:60.47ms
step:543/2285 train_time:32837ms step_avg:60.47ms
step:544/2285 train_time:32896ms step_avg:60.47ms
step:545/2285 train_time:32958ms step_avg:60.47ms
step:546/2285 train_time:33016ms step_avg:60.47ms
step:547/2285 train_time:33078ms step_avg:60.47ms
step:548/2285 train_time:33137ms step_avg:60.47ms
step:549/2285 train_time:33198ms step_avg:60.47ms
step:550/2285 train_time:33258ms step_avg:60.47ms
step:551/2285 train_time:33320ms step_avg:60.47ms
step:552/2285 train_time:33378ms step_avg:60.47ms
step:553/2285 train_time:33440ms step_avg:60.47ms
step:554/2285 train_time:33498ms step_avg:60.47ms
step:555/2285 train_time:33560ms step_avg:60.47ms
step:556/2285 train_time:33619ms step_avg:60.47ms
step:557/2285 train_time:33681ms step_avg:60.47ms
step:558/2285 train_time:33740ms step_avg:60.47ms
step:559/2285 train_time:33801ms step_avg:60.47ms
step:560/2285 train_time:33860ms step_avg:60.46ms
step:561/2285 train_time:33922ms step_avg:60.47ms
step:562/2285 train_time:33981ms step_avg:60.47ms
step:563/2285 train_time:34043ms step_avg:60.47ms
step:564/2285 train_time:34101ms step_avg:60.46ms
step:565/2285 train_time:34162ms step_avg:60.46ms
step:566/2285 train_time:34221ms step_avg:60.46ms
step:567/2285 train_time:34283ms step_avg:60.46ms
step:568/2285 train_time:34342ms step_avg:60.46ms
step:569/2285 train_time:34403ms step_avg:60.46ms
step:570/2285 train_time:34462ms step_avg:60.46ms
step:571/2285 train_time:34524ms step_avg:60.46ms
step:572/2285 train_time:34583ms step_avg:60.46ms
step:573/2285 train_time:34645ms step_avg:60.46ms
step:574/2285 train_time:34704ms step_avg:60.46ms
step:575/2285 train_time:34766ms step_avg:60.46ms
step:576/2285 train_time:34825ms step_avg:60.46ms
step:577/2285 train_time:34886ms step_avg:60.46ms
step:578/2285 train_time:34946ms step_avg:60.46ms
step:579/2285 train_time:35007ms step_avg:60.46ms
step:580/2285 train_time:35066ms step_avg:60.46ms
step:581/2285 train_time:35128ms step_avg:60.46ms
step:582/2285 train_time:35187ms step_avg:60.46ms
step:583/2285 train_time:35248ms step_avg:60.46ms
step:584/2285 train_time:35308ms step_avg:60.46ms
step:585/2285 train_time:35370ms step_avg:60.46ms
step:586/2285 train_time:35430ms step_avg:60.46ms
step:587/2285 train_time:35491ms step_avg:60.46ms
step:588/2285 train_time:35551ms step_avg:60.46ms
step:589/2285 train_time:35613ms step_avg:60.46ms
step:590/2285 train_time:35672ms step_avg:60.46ms
step:591/2285 train_time:35734ms step_avg:60.46ms
step:592/2285 train_time:35794ms step_avg:60.46ms
step:593/2285 train_time:35856ms step_avg:60.47ms
step:594/2285 train_time:35915ms step_avg:60.46ms
step:595/2285 train_time:35976ms step_avg:60.46ms
step:596/2285 train_time:36035ms step_avg:60.46ms
step:597/2285 train_time:36096ms step_avg:60.46ms
step:598/2285 train_time:36155ms step_avg:60.46ms
step:599/2285 train_time:36217ms step_avg:60.46ms
step:600/2285 train_time:36276ms step_avg:60.46ms
step:601/2285 train_time:36338ms step_avg:60.46ms
step:602/2285 train_time:36398ms step_avg:60.46ms
step:603/2285 train_time:36459ms step_avg:60.46ms
step:604/2285 train_time:36518ms step_avg:60.46ms
step:605/2285 train_time:36579ms step_avg:60.46ms
step:606/2285 train_time:36638ms step_avg:60.46ms
step:607/2285 train_time:36699ms step_avg:60.46ms
step:608/2285 train_time:36758ms step_avg:60.46ms
step:609/2285 train_time:36820ms step_avg:60.46ms
step:610/2285 train_time:36879ms step_avg:60.46ms
step:611/2285 train_time:36940ms step_avg:60.46ms
step:612/2285 train_time:36999ms step_avg:60.46ms
step:613/2285 train_time:37061ms step_avg:60.46ms
step:614/2285 train_time:37120ms step_avg:60.46ms
step:615/2285 train_time:37181ms step_avg:60.46ms
step:616/2285 train_time:37240ms step_avg:60.45ms
step:617/2285 train_time:37302ms step_avg:60.46ms
step:618/2285 train_time:37361ms step_avg:60.45ms
step:619/2285 train_time:37422ms step_avg:60.46ms
step:620/2285 train_time:37480ms step_avg:60.45ms
step:621/2285 train_time:37542ms step_avg:60.45ms
step:622/2285 train_time:37600ms step_avg:60.45ms
step:623/2285 train_time:37662ms step_avg:60.45ms
step:624/2285 train_time:37721ms step_avg:60.45ms
step:625/2285 train_time:37783ms step_avg:60.45ms
step:626/2285 train_time:37841ms step_avg:60.45ms
step:627/2285 train_time:37903ms step_avg:60.45ms
step:628/2285 train_time:37963ms step_avg:60.45ms
step:629/2285 train_time:38025ms step_avg:60.45ms
step:630/2285 train_time:38084ms step_avg:60.45ms
step:631/2285 train_time:38146ms step_avg:60.45ms
step:632/2285 train_time:38204ms step_avg:60.45ms
step:633/2285 train_time:38266ms step_avg:60.45ms
step:634/2285 train_time:38326ms step_avg:60.45ms
step:635/2285 train_time:38387ms step_avg:60.45ms
step:636/2285 train_time:38446ms step_avg:60.45ms
step:637/2285 train_time:38508ms step_avg:60.45ms
step:638/2285 train_time:38567ms step_avg:60.45ms
step:639/2285 train_time:38628ms step_avg:60.45ms
step:640/2285 train_time:38687ms step_avg:60.45ms
step:641/2285 train_time:38749ms step_avg:60.45ms
step:642/2285 train_time:38808ms step_avg:60.45ms
step:643/2285 train_time:38870ms step_avg:60.45ms
step:644/2285 train_time:38929ms step_avg:60.45ms
step:645/2285 train_time:38991ms step_avg:60.45ms
step:646/2285 train_time:39050ms step_avg:60.45ms
step:647/2285 train_time:39113ms step_avg:60.45ms
step:648/2285 train_time:39172ms step_avg:60.45ms
step:649/2285 train_time:39234ms step_avg:60.45ms
step:650/2285 train_time:39294ms step_avg:60.45ms
step:651/2285 train_time:39355ms step_avg:60.45ms
step:652/2285 train_time:39414ms step_avg:60.45ms
step:653/2285 train_time:39476ms step_avg:60.45ms
step:654/2285 train_time:39535ms step_avg:60.45ms
step:655/2285 train_time:39596ms step_avg:60.45ms
step:656/2285 train_time:39655ms step_avg:60.45ms
step:657/2285 train_time:39717ms step_avg:60.45ms
step:658/2285 train_time:39775ms step_avg:60.45ms
step:659/2285 train_time:39837ms step_avg:60.45ms
step:660/2285 train_time:39895ms step_avg:60.45ms
step:661/2285 train_time:39957ms step_avg:60.45ms
step:662/2285 train_time:40016ms step_avg:60.45ms
step:663/2285 train_time:40078ms step_avg:60.45ms
step:664/2285 train_time:40137ms step_avg:60.45ms
step:665/2285 train_time:40198ms step_avg:60.45ms
step:666/2285 train_time:40257ms step_avg:60.45ms
step:667/2285 train_time:40319ms step_avg:60.45ms
step:668/2285 train_time:40377ms step_avg:60.45ms
step:669/2285 train_time:40439ms step_avg:60.45ms
step:670/2285 train_time:40498ms step_avg:60.44ms
step:671/2285 train_time:40559ms step_avg:60.45ms
step:672/2285 train_time:40618ms step_avg:60.44ms
step:673/2285 train_time:40679ms step_avg:60.44ms
step:674/2285 train_time:40737ms step_avg:60.44ms
step:675/2285 train_time:40799ms step_avg:60.44ms
step:676/2285 train_time:40857ms step_avg:60.44ms
step:677/2285 train_time:40919ms step_avg:60.44ms
step:678/2285 train_time:40977ms step_avg:60.44ms
step:679/2285 train_time:41039ms step_avg:60.44ms
step:680/2285 train_time:41098ms step_avg:60.44ms
step:681/2285 train_time:41159ms step_avg:60.44ms
step:682/2285 train_time:41218ms step_avg:60.44ms
step:683/2285 train_time:41279ms step_avg:60.44ms
step:684/2285 train_time:41338ms step_avg:60.44ms
step:685/2285 train_time:41399ms step_avg:60.44ms
step:686/2285 train_time:41458ms step_avg:60.43ms
step:687/2285 train_time:41519ms step_avg:60.44ms
step:688/2285 train_time:41578ms step_avg:60.43ms
step:689/2285 train_time:41640ms step_avg:60.43ms
step:690/2285 train_time:41698ms step_avg:60.43ms
step:691/2285 train_time:41759ms step_avg:60.43ms
step:692/2285 train_time:41817ms step_avg:60.43ms
step:693/2285 train_time:41879ms step_avg:60.43ms
step:694/2285 train_time:41938ms step_avg:60.43ms
step:695/2285 train_time:42000ms step_avg:60.43ms
step:696/2285 train_time:42059ms step_avg:60.43ms
step:697/2285 train_time:42121ms step_avg:60.43ms
step:698/2285 train_time:42181ms step_avg:60.43ms
step:699/2285 train_time:42242ms step_avg:60.43ms
step:700/2285 train_time:42300ms step_avg:60.43ms
step:701/2285 train_time:42361ms step_avg:60.43ms
step:702/2285 train_time:42420ms step_avg:60.43ms
step:703/2285 train_time:42482ms step_avg:60.43ms
step:704/2285 train_time:42541ms step_avg:60.43ms
step:705/2285 train_time:42602ms step_avg:60.43ms
step:706/2285 train_time:42660ms step_avg:60.43ms
step:707/2285 train_time:42722ms step_avg:60.43ms
step:708/2285 train_time:42780ms step_avg:60.42ms
step:709/2285 train_time:42842ms step_avg:60.43ms
step:710/2285 train_time:42900ms step_avg:60.42ms
step:711/2285 train_time:42962ms step_avg:60.42ms
step:712/2285 train_time:43021ms step_avg:60.42ms
step:713/2285 train_time:43083ms step_avg:60.42ms
step:714/2285 train_time:43142ms step_avg:60.42ms
step:715/2285 train_time:43204ms step_avg:60.43ms
step:716/2285 train_time:43263ms step_avg:60.42ms
step:717/2285 train_time:43325ms step_avg:60.43ms
step:718/2285 train_time:43384ms step_avg:60.42ms
step:719/2285 train_time:43446ms step_avg:60.43ms
step:720/2285 train_time:43505ms step_avg:60.42ms
step:721/2285 train_time:43566ms step_avg:60.42ms
step:722/2285 train_time:43625ms step_avg:60.42ms
step:723/2285 train_time:43686ms step_avg:60.42ms
step:724/2285 train_time:43746ms step_avg:60.42ms
step:725/2285 train_time:43807ms step_avg:60.42ms
step:726/2285 train_time:43866ms step_avg:60.42ms
step:727/2285 train_time:43928ms step_avg:60.42ms
step:728/2285 train_time:43987ms step_avg:60.42ms
step:729/2285 train_time:44049ms step_avg:60.42ms
step:730/2285 train_time:44109ms step_avg:60.42ms
step:731/2285 train_time:44171ms step_avg:60.43ms
step:732/2285 train_time:44230ms step_avg:60.42ms
step:733/2285 train_time:44291ms step_avg:60.42ms
step:734/2285 train_time:44351ms step_avg:60.42ms
step:735/2285 train_time:44413ms step_avg:60.43ms
step:736/2285 train_time:44472ms step_avg:60.42ms
step:737/2285 train_time:44534ms step_avg:60.43ms
step:738/2285 train_time:44593ms step_avg:60.42ms
step:739/2285 train_time:44654ms step_avg:60.43ms
step:740/2285 train_time:44714ms step_avg:60.42ms
step:741/2285 train_time:44775ms step_avg:60.42ms
step:742/2285 train_time:44834ms step_avg:60.42ms
step:743/2285 train_time:44896ms step_avg:60.43ms
step:744/2285 train_time:44955ms step_avg:60.42ms
step:745/2285 train_time:45016ms step_avg:60.42ms
step:746/2285 train_time:45075ms step_avg:60.42ms
step:747/2285 train_time:45136ms step_avg:60.42ms
step:748/2285 train_time:45195ms step_avg:60.42ms
step:749/2285 train_time:45257ms step_avg:60.42ms
step:750/2285 train_time:45317ms step_avg:60.42ms
step:750/2285 val_loss:3.6737 train_time:45380ms step_avg:60.51ms
step:751/2285 train_time:45400ms step_avg:60.45ms
step:752/2285 train_time:45444ms step_avg:60.43ms
step:753/2285 train_time:45505ms step_avg:60.43ms
step:754/2285 train_time:45565ms step_avg:60.43ms
step:755/2285 train_time:45629ms step_avg:60.44ms
step:756/2285 train_time:45689ms step_avg:60.44ms
step:757/2285 train_time:45751ms step_avg:60.44ms
step:758/2285 train_time:45810ms step_avg:60.44ms
step:759/2285 train_time:45871ms step_avg:60.44ms
step:760/2285 train_time:45930ms step_avg:60.43ms
step:761/2285 train_time:45991ms step_avg:60.44ms
step:762/2285 train_time:46051ms step_avg:60.43ms
step:763/2285 train_time:46113ms step_avg:60.44ms
step:764/2285 train_time:46175ms step_avg:60.44ms
step:765/2285 train_time:46237ms step_avg:60.44ms
step:766/2285 train_time:46297ms step_avg:60.44ms
step:767/2285 train_time:46364ms step_avg:60.45ms
step:768/2285 train_time:46426ms step_avg:60.45ms
step:769/2285 train_time:46491ms step_avg:60.46ms
step:770/2285 train_time:46551ms step_avg:60.46ms
step:771/2285 train_time:46613ms step_avg:60.46ms
step:772/2285 train_time:46674ms step_avg:60.46ms
step:773/2285 train_time:46736ms step_avg:60.46ms
step:774/2285 train_time:46795ms step_avg:60.46ms
step:775/2285 train_time:46856ms step_avg:60.46ms
step:776/2285 train_time:46915ms step_avg:60.46ms
step:777/2285 train_time:46977ms step_avg:60.46ms
step:778/2285 train_time:47036ms step_avg:60.46ms
step:779/2285 train_time:47098ms step_avg:60.46ms
step:780/2285 train_time:47157ms step_avg:60.46ms
step:781/2285 train_time:47220ms step_avg:60.46ms
step:782/2285 train_time:47280ms step_avg:60.46ms
step:783/2285 train_time:47343ms step_avg:60.46ms
step:784/2285 train_time:47404ms step_avg:60.46ms
step:785/2285 train_time:47467ms step_avg:60.47ms
step:786/2285 train_time:47527ms step_avg:60.47ms
step:787/2285 train_time:47590ms step_avg:60.47ms
step:788/2285 train_time:47650ms step_avg:60.47ms
step:789/2285 train_time:47712ms step_avg:60.47ms
step:790/2285 train_time:47772ms step_avg:60.47ms
step:791/2285 train_time:47834ms step_avg:60.47ms
step:792/2285 train_time:47893ms step_avg:60.47ms
step:793/2285 train_time:47954ms step_avg:60.47ms
step:794/2285 train_time:48014ms step_avg:60.47ms
step:795/2285 train_time:48076ms step_avg:60.47ms
step:796/2285 train_time:48136ms step_avg:60.47ms
step:797/2285 train_time:48199ms step_avg:60.47ms
step:798/2285 train_time:48258ms step_avg:60.47ms
step:799/2285 train_time:48321ms step_avg:60.48ms
step:800/2285 train_time:48381ms step_avg:60.48ms
step:801/2285 train_time:48444ms step_avg:60.48ms
step:802/2285 train_time:48503ms step_avg:60.48ms
step:803/2285 train_time:48566ms step_avg:60.48ms
step:804/2285 train_time:48626ms step_avg:60.48ms
step:805/2285 train_time:48689ms step_avg:60.48ms
step:806/2285 train_time:48749ms step_avg:60.48ms
step:807/2285 train_time:48811ms step_avg:60.48ms
step:808/2285 train_time:48870ms step_avg:60.48ms
step:809/2285 train_time:48932ms step_avg:60.48ms
step:810/2285 train_time:48992ms step_avg:60.48ms
step:811/2285 train_time:49054ms step_avg:60.49ms
step:812/2285 train_time:49115ms step_avg:60.49ms
step:813/2285 train_time:49178ms step_avg:60.49ms
step:814/2285 train_time:49238ms step_avg:60.49ms
step:815/2285 train_time:49301ms step_avg:60.49ms
step:816/2285 train_time:49361ms step_avg:60.49ms
step:817/2285 train_time:49423ms step_avg:60.49ms
step:818/2285 train_time:49483ms step_avg:60.49ms
step:819/2285 train_time:49545ms step_avg:60.49ms
step:820/2285 train_time:49605ms step_avg:60.49ms
step:821/2285 train_time:49668ms step_avg:60.50ms
step:822/2285 train_time:49727ms step_avg:60.50ms
step:823/2285 train_time:49790ms step_avg:60.50ms
step:824/2285 train_time:49850ms step_avg:60.50ms
step:825/2285 train_time:49912ms step_avg:60.50ms
step:826/2285 train_time:49972ms step_avg:60.50ms
step:827/2285 train_time:50034ms step_avg:60.50ms
step:828/2285 train_time:50094ms step_avg:60.50ms
step:829/2285 train_time:50157ms step_avg:60.50ms
step:830/2285 train_time:50218ms step_avg:60.50ms
step:831/2285 train_time:50280ms step_avg:60.51ms
step:832/2285 train_time:50340ms step_avg:60.50ms
step:833/2285 train_time:50402ms step_avg:60.51ms
step:834/2285 train_time:50462ms step_avg:60.51ms
step:835/2285 train_time:50524ms step_avg:60.51ms
step:836/2285 train_time:50583ms step_avg:60.51ms
step:837/2285 train_time:50646ms step_avg:60.51ms
step:838/2285 train_time:50705ms step_avg:60.51ms
step:839/2285 train_time:50768ms step_avg:60.51ms
step:840/2285 train_time:50828ms step_avg:60.51ms
step:841/2285 train_time:50891ms step_avg:60.51ms
step:842/2285 train_time:50950ms step_avg:60.51ms
step:843/2285 train_time:51013ms step_avg:60.51ms
step:844/2285 train_time:51073ms step_avg:60.51ms
step:845/2285 train_time:51135ms step_avg:60.51ms
step:846/2285 train_time:51196ms step_avg:60.52ms
step:847/2285 train_time:51259ms step_avg:60.52ms
step:848/2285 train_time:51318ms step_avg:60.52ms
step:849/2285 train_time:51380ms step_avg:60.52ms
step:850/2285 train_time:51440ms step_avg:60.52ms
step:851/2285 train_time:51502ms step_avg:60.52ms
step:852/2285 train_time:51562ms step_avg:60.52ms
step:853/2285 train_time:51623ms step_avg:60.52ms
step:854/2285 train_time:51683ms step_avg:60.52ms
step:855/2285 train_time:51745ms step_avg:60.52ms
step:856/2285 train_time:51805ms step_avg:60.52ms
step:857/2285 train_time:51868ms step_avg:60.52ms
step:858/2285 train_time:51928ms step_avg:60.52ms
step:859/2285 train_time:51990ms step_avg:60.52ms
step:860/2285 train_time:52050ms step_avg:60.52ms
step:861/2285 train_time:52113ms step_avg:60.53ms
step:862/2285 train_time:52174ms step_avg:60.53ms
step:863/2285 train_time:52236ms step_avg:60.53ms
step:864/2285 train_time:52297ms step_avg:60.53ms
step:865/2285 train_time:52358ms step_avg:60.53ms
step:866/2285 train_time:52418ms step_avg:60.53ms
step:867/2285 train_time:52480ms step_avg:60.53ms
step:868/2285 train_time:52540ms step_avg:60.53ms
step:869/2285 train_time:52601ms step_avg:60.53ms
step:870/2285 train_time:52661ms step_avg:60.53ms
step:871/2285 train_time:52723ms step_avg:60.53ms
step:872/2285 train_time:52784ms step_avg:60.53ms
step:873/2285 train_time:52847ms step_avg:60.53ms
step:874/2285 train_time:52907ms step_avg:60.53ms
step:875/2285 train_time:52970ms step_avg:60.54ms
step:876/2285 train_time:53029ms step_avg:60.54ms
step:877/2285 train_time:53091ms step_avg:60.54ms
step:878/2285 train_time:53151ms step_avg:60.54ms
step:879/2285 train_time:53214ms step_avg:60.54ms
step:880/2285 train_time:53274ms step_avg:60.54ms
step:881/2285 train_time:53337ms step_avg:60.54ms
step:882/2285 train_time:53397ms step_avg:60.54ms
step:883/2285 train_time:53460ms step_avg:60.54ms
step:884/2285 train_time:53519ms step_avg:60.54ms
step:885/2285 train_time:53581ms step_avg:60.54ms
step:886/2285 train_time:53640ms step_avg:60.54ms
step:887/2285 train_time:53703ms step_avg:60.54ms
step:888/2285 train_time:53762ms step_avg:60.54ms
step:889/2285 train_time:53825ms step_avg:60.55ms
step:890/2285 train_time:53884ms step_avg:60.54ms
step:891/2285 train_time:53947ms step_avg:60.55ms
step:892/2285 train_time:54007ms step_avg:60.55ms
step:893/2285 train_time:54069ms step_avg:60.55ms
step:894/2285 train_time:54129ms step_avg:60.55ms
step:895/2285 train_time:54191ms step_avg:60.55ms
step:896/2285 train_time:54252ms step_avg:60.55ms
step:897/2285 train_time:54314ms step_avg:60.55ms
step:898/2285 train_time:54375ms step_avg:60.55ms
step:899/2285 train_time:54438ms step_avg:60.55ms
step:900/2285 train_time:54498ms step_avg:60.55ms
step:901/2285 train_time:54560ms step_avg:60.55ms
step:902/2285 train_time:54619ms step_avg:60.55ms
step:903/2285 train_time:54682ms step_avg:60.56ms
step:904/2285 train_time:54741ms step_avg:60.55ms
step:905/2285 train_time:54803ms step_avg:60.56ms
step:906/2285 train_time:54863ms step_avg:60.55ms
step:907/2285 train_time:54924ms step_avg:60.56ms
step:908/2285 train_time:54984ms step_avg:60.55ms
step:909/2285 train_time:55047ms step_avg:60.56ms
step:910/2285 train_time:55107ms step_avg:60.56ms
step:911/2285 train_time:55170ms step_avg:60.56ms
step:912/2285 train_time:55230ms step_avg:60.56ms
step:913/2285 train_time:55293ms step_avg:60.56ms
step:914/2285 train_time:55353ms step_avg:60.56ms
step:915/2285 train_time:55416ms step_avg:60.56ms
step:916/2285 train_time:55476ms step_avg:60.56ms
step:917/2285 train_time:55539ms step_avg:60.57ms
step:918/2285 train_time:55599ms step_avg:60.57ms
step:919/2285 train_time:55661ms step_avg:60.57ms
step:920/2285 train_time:55721ms step_avg:60.57ms
step:921/2285 train_time:55783ms step_avg:60.57ms
step:922/2285 train_time:55842ms step_avg:60.57ms
step:923/2285 train_time:55905ms step_avg:60.57ms
step:924/2285 train_time:55964ms step_avg:60.57ms
step:925/2285 train_time:56027ms step_avg:60.57ms
step:926/2285 train_time:56088ms step_avg:60.57ms
step:927/2285 train_time:56151ms step_avg:60.57ms
step:928/2285 train_time:56210ms step_avg:60.57ms
step:929/2285 train_time:56272ms step_avg:60.57ms
step:930/2285 train_time:56332ms step_avg:60.57ms
step:931/2285 train_time:56394ms step_avg:60.57ms
step:932/2285 train_time:56455ms step_avg:60.57ms
step:933/2285 train_time:56518ms step_avg:60.58ms
step:934/2285 train_time:56578ms step_avg:60.58ms
step:935/2285 train_time:56640ms step_avg:60.58ms
step:936/2285 train_time:56700ms step_avg:60.58ms
step:937/2285 train_time:56762ms step_avg:60.58ms
step:938/2285 train_time:56821ms step_avg:60.58ms
step:939/2285 train_time:56883ms step_avg:60.58ms
step:940/2285 train_time:56943ms step_avg:60.58ms
step:941/2285 train_time:57006ms step_avg:60.58ms
step:942/2285 train_time:57066ms step_avg:60.58ms
step:943/2285 train_time:57129ms step_avg:60.58ms
step:944/2285 train_time:57189ms step_avg:60.58ms
step:945/2285 train_time:57252ms step_avg:60.58ms
step:946/2285 train_time:57311ms step_avg:60.58ms
step:947/2285 train_time:57374ms step_avg:60.58ms
step:948/2285 train_time:57434ms step_avg:60.58ms
step:949/2285 train_time:57497ms step_avg:60.59ms
step:950/2285 train_time:57556ms step_avg:60.59ms
step:951/2285 train_time:57619ms step_avg:60.59ms
step:952/2285 train_time:57679ms step_avg:60.59ms
step:953/2285 train_time:57742ms step_avg:60.59ms
step:954/2285 train_time:57802ms step_avg:60.59ms
step:955/2285 train_time:57863ms step_avg:60.59ms
step:956/2285 train_time:57923ms step_avg:60.59ms
step:957/2285 train_time:57985ms step_avg:60.59ms
step:958/2285 train_time:58045ms step_avg:60.59ms
step:959/2285 train_time:58107ms step_avg:60.59ms
step:960/2285 train_time:58167ms step_avg:60.59ms
step:961/2285 train_time:58230ms step_avg:60.59ms
step:962/2285 train_time:58289ms step_avg:60.59ms
step:963/2285 train_time:58352ms step_avg:60.59ms
step:964/2285 train_time:58412ms step_avg:60.59ms
step:965/2285 train_time:58475ms step_avg:60.60ms
step:966/2285 train_time:58535ms step_avg:60.60ms
step:967/2285 train_time:58598ms step_avg:60.60ms
step:968/2285 train_time:58658ms step_avg:60.60ms
step:969/2285 train_time:58720ms step_avg:60.60ms
step:970/2285 train_time:58780ms step_avg:60.60ms
step:971/2285 train_time:58842ms step_avg:60.60ms
step:972/2285 train_time:58902ms step_avg:60.60ms
step:973/2285 train_time:58964ms step_avg:60.60ms
step:974/2285 train_time:59024ms step_avg:60.60ms
step:975/2285 train_time:59086ms step_avg:60.60ms
step:976/2285 train_time:59146ms step_avg:60.60ms
step:977/2285 train_time:59208ms step_avg:60.60ms
step:978/2285 train_time:59268ms step_avg:60.60ms
step:979/2285 train_time:59331ms step_avg:60.60ms
step:980/2285 train_time:59391ms step_avg:60.60ms
step:981/2285 train_time:59454ms step_avg:60.61ms
step:982/2285 train_time:59515ms step_avg:60.61ms
step:983/2285 train_time:59577ms step_avg:60.61ms
step:984/2285 train_time:59637ms step_avg:60.61ms
step:985/2285 train_time:59699ms step_avg:60.61ms
step:986/2285 train_time:59759ms step_avg:60.61ms
step:987/2285 train_time:59821ms step_avg:60.61ms
step:988/2285 train_time:59882ms step_avg:60.61ms
step:989/2285 train_time:59944ms step_avg:60.61ms
step:990/2285 train_time:60004ms step_avg:60.61ms
step:991/2285 train_time:60065ms step_avg:60.61ms
step:992/2285 train_time:60124ms step_avg:60.61ms
step:993/2285 train_time:60188ms step_avg:60.61ms
step:994/2285 train_time:60248ms step_avg:60.61ms
step:995/2285 train_time:60311ms step_avg:60.61ms
step:996/2285 train_time:60370ms step_avg:60.61ms
step:997/2285 train_time:60433ms step_avg:60.61ms
step:998/2285 train_time:60493ms step_avg:60.61ms
step:999/2285 train_time:60556ms step_avg:60.62ms
step:1000/2285 train_time:60617ms step_avg:60.62ms
step:1000/2285 val_loss:3.5673 train_time:60680ms step_avg:60.68ms
step:1001/2285 train_time:60700ms step_avg:60.64ms
step:1002/2285 train_time:60743ms step_avg:60.62ms
step:1003/2285 train_time:60808ms step_avg:60.63ms
step:1004/2285 train_time:60870ms step_avg:60.63ms
step:1005/2285 train_time:60933ms step_avg:60.63ms
step:1006/2285 train_time:60993ms step_avg:60.63ms
step:1007/2285 train_time:61056ms step_avg:60.63ms
step:1008/2285 train_time:61116ms step_avg:60.63ms
step:1009/2285 train_time:61179ms step_avg:60.63ms
step:1010/2285 train_time:61238ms step_avg:60.63ms
step:1011/2285 train_time:61300ms step_avg:60.63ms
step:1012/2285 train_time:61359ms step_avg:60.63ms
step:1013/2285 train_time:61421ms step_avg:60.63ms
step:1014/2285 train_time:61481ms step_avg:60.63ms
step:1015/2285 train_time:61542ms step_avg:60.63ms
step:1016/2285 train_time:61602ms step_avg:60.63ms
step:1017/2285 train_time:61665ms step_avg:60.63ms
step:1018/2285 train_time:61725ms step_avg:60.63ms
step:1019/2285 train_time:61789ms step_avg:60.64ms
step:1020/2285 train_time:61849ms step_avg:60.64ms
step:1021/2285 train_time:61912ms step_avg:60.64ms
step:1022/2285 train_time:61972ms step_avg:60.64ms
step:1023/2285 train_time:62035ms step_avg:60.64ms
step:1024/2285 train_time:62095ms step_avg:60.64ms
step:1025/2285 train_time:62157ms step_avg:60.64ms
step:1026/2285 train_time:62217ms step_avg:60.64ms
step:1027/2285 train_time:62279ms step_avg:60.64ms
step:1028/2285 train_time:62338ms step_avg:60.64ms
step:1029/2285 train_time:62400ms step_avg:60.64ms
step:1030/2285 train_time:62459ms step_avg:60.64ms
step:1031/2285 train_time:62521ms step_avg:60.64ms
step:1032/2285 train_time:62581ms step_avg:60.64ms
step:1033/2285 train_time:62643ms step_avg:60.64ms
step:1034/2285 train_time:62703ms step_avg:60.64ms
step:1035/2285 train_time:62766ms step_avg:60.64ms
step:1036/2285 train_time:62826ms step_avg:60.64ms
step:1037/2285 train_time:62889ms step_avg:60.64ms
step:1038/2285 train_time:62949ms step_avg:60.64ms
step:1039/2285 train_time:63012ms step_avg:60.65ms
step:1040/2285 train_time:63072ms step_avg:60.65ms
step:1041/2285 train_time:63136ms step_avg:60.65ms
step:1042/2285 train_time:63196ms step_avg:60.65ms
step:1043/2285 train_time:63258ms step_avg:60.65ms
step:1044/2285 train_time:63318ms step_avg:60.65ms
step:1045/2285 train_time:63380ms step_avg:60.65ms
step:1046/2285 train_time:63440ms step_avg:60.65ms
step:1047/2285 train_time:63502ms step_avg:60.65ms
step:1048/2285 train_time:63561ms step_avg:60.65ms
step:1049/2285 train_time:63624ms step_avg:60.65ms
step:1050/2285 train_time:63684ms step_avg:60.65ms
step:1051/2285 train_time:63746ms step_avg:60.65ms
step:1052/2285 train_time:63806ms step_avg:60.65ms
step:1053/2285 train_time:63869ms step_avg:60.65ms
step:1054/2285 train_time:63929ms step_avg:60.65ms
step:1055/2285 train_time:63992ms step_avg:60.66ms
step:1056/2285 train_time:64052ms step_avg:60.66ms
step:1057/2285 train_time:64114ms step_avg:60.66ms
step:1058/2285 train_time:64174ms step_avg:60.66ms
step:1059/2285 train_time:64237ms step_avg:60.66ms
step:1060/2285 train_time:64297ms step_avg:60.66ms
step:1061/2285 train_time:64359ms step_avg:60.66ms
step:1062/2285 train_time:64419ms step_avg:60.66ms
step:1063/2285 train_time:64481ms step_avg:60.66ms
step:1064/2285 train_time:64541ms step_avg:60.66ms
step:1065/2285 train_time:64603ms step_avg:60.66ms
step:1066/2285 train_time:64663ms step_avg:60.66ms
step:1067/2285 train_time:64725ms step_avg:60.66ms
step:1068/2285 train_time:64784ms step_avg:60.66ms
step:1069/2285 train_time:64848ms step_avg:60.66ms
step:1070/2285 train_time:64908ms step_avg:60.66ms
step:1071/2285 train_time:64970ms step_avg:60.66ms
step:1072/2285 train_time:65030ms step_avg:60.66ms
step:1073/2285 train_time:65093ms step_avg:60.66ms
step:1074/2285 train_time:65153ms step_avg:60.66ms
step:1075/2285 train_time:65216ms step_avg:60.67ms
step:1076/2285 train_time:65276ms step_avg:60.67ms
step:1077/2285 train_time:65339ms step_avg:60.67ms
step:1078/2285 train_time:65399ms step_avg:60.67ms
step:1079/2285 train_time:65461ms step_avg:60.67ms
step:1080/2285 train_time:65521ms step_avg:60.67ms
step:1081/2285 train_time:65583ms step_avg:60.67ms
step:1082/2285 train_time:65643ms step_avg:60.67ms
step:1083/2285 train_time:65706ms step_avg:60.67ms
step:1084/2285 train_time:65766ms step_avg:60.67ms
step:1085/2285 train_time:65828ms step_avg:60.67ms
step:1086/2285 train_time:65888ms step_avg:60.67ms
step:1087/2285 train_time:65950ms step_avg:60.67ms
step:1088/2285 train_time:66010ms step_avg:60.67ms
step:1089/2285 train_time:66073ms step_avg:60.67ms
step:1090/2285 train_time:66133ms step_avg:60.67ms
step:1091/2285 train_time:66196ms step_avg:60.67ms
step:1092/2285 train_time:66256ms step_avg:60.67ms
step:1093/2285 train_time:66319ms step_avg:60.68ms
step:1094/2285 train_time:66379ms step_avg:60.68ms
step:1095/2285 train_time:66442ms step_avg:60.68ms
step:1096/2285 train_time:66501ms step_avg:60.68ms
step:1097/2285 train_time:66563ms step_avg:60.68ms
step:1098/2285 train_time:66623ms step_avg:60.68ms
step:1099/2285 train_time:66685ms step_avg:60.68ms
step:1100/2285 train_time:66745ms step_avg:60.68ms
step:1101/2285 train_time:66808ms step_avg:60.68ms
step:1102/2285 train_time:66867ms step_avg:60.68ms
step:1103/2285 train_time:66930ms step_avg:60.68ms
step:1104/2285 train_time:66990ms step_avg:60.68ms
step:1105/2285 train_time:67053ms step_avg:60.68ms
step:1106/2285 train_time:67113ms step_avg:60.68ms
step:1107/2285 train_time:67176ms step_avg:60.68ms
step:1108/2285 train_time:67236ms step_avg:60.68ms
step:1109/2285 train_time:67298ms step_avg:60.68ms
step:1110/2285 train_time:67358ms step_avg:60.68ms
step:1111/2285 train_time:67420ms step_avg:60.68ms
step:1112/2285 train_time:67480ms step_avg:60.68ms
step:1113/2285 train_time:67542ms step_avg:60.69ms
step:1114/2285 train_time:67602ms step_avg:60.68ms
step:1115/2285 train_time:67664ms step_avg:60.69ms
step:1116/2285 train_time:67724ms step_avg:60.68ms
step:1117/2285 train_time:67786ms step_avg:60.69ms
step:1118/2285 train_time:67846ms step_avg:60.69ms
step:1119/2285 train_time:67910ms step_avg:60.69ms
step:1120/2285 train_time:67969ms step_avg:60.69ms
step:1121/2285 train_time:68031ms step_avg:60.69ms
step:1122/2285 train_time:68091ms step_avg:60.69ms
step:1123/2285 train_time:68154ms step_avg:60.69ms
step:1124/2285 train_time:68214ms step_avg:60.69ms
step:1125/2285 train_time:68277ms step_avg:60.69ms
step:1126/2285 train_time:68337ms step_avg:60.69ms
step:1127/2285 train_time:68399ms step_avg:60.69ms
step:1128/2285 train_time:68459ms step_avg:60.69ms
step:1129/2285 train_time:68522ms step_avg:60.69ms
step:1130/2285 train_time:68581ms step_avg:60.69ms
step:1131/2285 train_time:68644ms step_avg:60.69ms
step:1132/2285 train_time:68703ms step_avg:60.69ms
step:1133/2285 train_time:68766ms step_avg:60.69ms
step:1134/2285 train_time:68825ms step_avg:60.69ms
step:1135/2285 train_time:68888ms step_avg:60.69ms
step:1136/2285 train_time:68948ms step_avg:60.69ms
step:1137/2285 train_time:69011ms step_avg:60.70ms
step:1138/2285 train_time:69071ms step_avg:60.69ms
step:1139/2285 train_time:69133ms step_avg:60.70ms
step:1140/2285 train_time:69193ms step_avg:60.70ms
step:1141/2285 train_time:69256ms step_avg:60.70ms
step:1142/2285 train_time:69316ms step_avg:60.70ms
step:1143/2285 train_time:69380ms step_avg:60.70ms
step:1144/2285 train_time:69439ms step_avg:60.70ms
step:1145/2285 train_time:69501ms step_avg:60.70ms
step:1146/2285 train_time:69561ms step_avg:60.70ms
step:1147/2285 train_time:69623ms step_avg:60.70ms
step:1148/2285 train_time:69683ms step_avg:60.70ms
step:1149/2285 train_time:69746ms step_avg:60.70ms
step:1150/2285 train_time:69806ms step_avg:60.70ms
step:1151/2285 train_time:69868ms step_avg:60.70ms
step:1152/2285 train_time:69928ms step_avg:60.70ms
step:1153/2285 train_time:69991ms step_avg:60.70ms
step:1154/2285 train_time:70050ms step_avg:60.70ms
step:1155/2285 train_time:70113ms step_avg:60.70ms
step:1156/2285 train_time:70172ms step_avg:60.70ms
step:1157/2285 train_time:70235ms step_avg:60.70ms
step:1158/2285 train_time:70295ms step_avg:60.70ms
step:1159/2285 train_time:70358ms step_avg:60.71ms
step:1160/2285 train_time:70418ms step_avg:60.71ms
step:1161/2285 train_time:70480ms step_avg:60.71ms
step:1162/2285 train_time:70540ms step_avg:60.71ms
step:1163/2285 train_time:70603ms step_avg:60.71ms
step:1164/2285 train_time:70663ms step_avg:60.71ms
step:1165/2285 train_time:70725ms step_avg:60.71ms
step:1166/2285 train_time:70784ms step_avg:60.71ms
step:1167/2285 train_time:70847ms step_avg:60.71ms
step:1168/2285 train_time:70906ms step_avg:60.71ms
step:1169/2285 train_time:70969ms step_avg:60.71ms
step:1170/2285 train_time:71028ms step_avg:60.71ms
step:1171/2285 train_time:71091ms step_avg:60.71ms
step:1172/2285 train_time:71151ms step_avg:60.71ms
step:1173/2285 train_time:71214ms step_avg:60.71ms
step:1174/2285 train_time:71274ms step_avg:60.71ms
step:1175/2285 train_time:71338ms step_avg:60.71ms
step:1176/2285 train_time:71398ms step_avg:60.71ms
step:1177/2285 train_time:71460ms step_avg:60.71ms
step:1178/2285 train_time:71520ms step_avg:60.71ms
step:1179/2285 train_time:71582ms step_avg:60.71ms
step:1180/2285 train_time:71641ms step_avg:60.71ms
step:1181/2285 train_time:71703ms step_avg:60.71ms
step:1182/2285 train_time:71763ms step_avg:60.71ms
step:1183/2285 train_time:71826ms step_avg:60.72ms
step:1184/2285 train_time:71886ms step_avg:60.71ms
step:1185/2285 train_time:71948ms step_avg:60.72ms
step:1186/2285 train_time:72008ms step_avg:60.71ms
step:1187/2285 train_time:72071ms step_avg:60.72ms
step:1188/2285 train_time:72130ms step_avg:60.72ms
step:1189/2285 train_time:72194ms step_avg:60.72ms
step:1190/2285 train_time:72253ms step_avg:60.72ms
step:1191/2285 train_time:72316ms step_avg:60.72ms
step:1192/2285 train_time:72376ms step_avg:60.72ms
step:1193/2285 train_time:72439ms step_avg:60.72ms
step:1194/2285 train_time:72499ms step_avg:60.72ms
step:1195/2285 train_time:72561ms step_avg:60.72ms
step:1196/2285 train_time:72621ms step_avg:60.72ms
step:1197/2285 train_time:72683ms step_avg:60.72ms
step:1198/2285 train_time:72743ms step_avg:60.72ms
step:1199/2285 train_time:72806ms step_avg:60.72ms
step:1200/2285 train_time:72865ms step_avg:60.72ms
step:1201/2285 train_time:72928ms step_avg:60.72ms
step:1202/2285 train_time:72987ms step_avg:60.72ms
step:1203/2285 train_time:73049ms step_avg:60.72ms
step:1204/2285 train_time:73110ms step_avg:60.72ms
step:1205/2285 train_time:73173ms step_avg:60.72ms
step:1206/2285 train_time:73233ms step_avg:60.72ms
step:1207/2285 train_time:73296ms step_avg:60.73ms
step:1208/2285 train_time:73356ms step_avg:60.73ms
step:1209/2285 train_time:73419ms step_avg:60.73ms
step:1210/2285 train_time:73479ms step_avg:60.73ms
step:1211/2285 train_time:73542ms step_avg:60.73ms
step:1212/2285 train_time:73601ms step_avg:60.73ms
step:1213/2285 train_time:73663ms step_avg:60.73ms
step:1214/2285 train_time:73724ms step_avg:60.73ms
step:1215/2285 train_time:73786ms step_avg:60.73ms
step:1216/2285 train_time:73846ms step_avg:60.73ms
step:1217/2285 train_time:73908ms step_avg:60.73ms
step:1218/2285 train_time:73967ms step_avg:60.73ms
step:1219/2285 train_time:74030ms step_avg:60.73ms
step:1220/2285 train_time:74090ms step_avg:60.73ms
step:1221/2285 train_time:74152ms step_avg:60.73ms
step:1222/2285 train_time:74212ms step_avg:60.73ms
step:1223/2285 train_time:74274ms step_avg:60.73ms
step:1224/2285 train_time:74335ms step_avg:60.73ms
step:1225/2285 train_time:74398ms step_avg:60.73ms
step:1226/2285 train_time:74458ms step_avg:60.73ms
step:1227/2285 train_time:74521ms step_avg:60.73ms
step:1228/2285 train_time:74581ms step_avg:60.73ms
step:1229/2285 train_time:74644ms step_avg:60.74ms
step:1230/2285 train_time:74703ms step_avg:60.73ms
step:1231/2285 train_time:74766ms step_avg:60.74ms
step:1232/2285 train_time:74826ms step_avg:60.74ms
step:1233/2285 train_time:74887ms step_avg:60.74ms
step:1234/2285 train_time:74947ms step_avg:60.74ms
step:1235/2285 train_time:75009ms step_avg:60.74ms
step:1236/2285 train_time:75069ms step_avg:60.74ms
step:1237/2285 train_time:75131ms step_avg:60.74ms
step:1238/2285 train_time:75191ms step_avg:60.74ms
step:1239/2285 train_time:75254ms step_avg:60.74ms
step:1240/2285 train_time:75314ms step_avg:60.74ms
step:1241/2285 train_time:75377ms step_avg:60.74ms
step:1242/2285 train_time:75438ms step_avg:60.74ms
step:1243/2285 train_time:75501ms step_avg:60.74ms
step:1244/2285 train_time:75560ms step_avg:60.74ms
step:1245/2285 train_time:75623ms step_avg:60.74ms
step:1246/2285 train_time:75682ms step_avg:60.74ms
step:1247/2285 train_time:75745ms step_avg:60.74ms
step:1248/2285 train_time:75805ms step_avg:60.74ms
step:1249/2285 train_time:75866ms step_avg:60.74ms
step:1250/2285 train_time:75926ms step_avg:60.74ms
step:1250/2285 val_loss:3.4997 train_time:75989ms step_avg:60.79ms
step:1251/2285 train_time:76007ms step_avg:60.76ms
step:1252/2285 train_time:76053ms step_avg:60.74ms
step:1253/2285 train_time:76119ms step_avg:60.75ms
step:1254/2285 train_time:76179ms step_avg:60.75ms
step:1255/2285 train_time:76241ms step_avg:60.75ms
step:1256/2285 train_time:76301ms step_avg:60.75ms
step:1257/2285 train_time:76362ms step_avg:60.75ms
step:1258/2285 train_time:76422ms step_avg:60.75ms
step:1259/2285 train_time:76484ms step_avg:60.75ms
step:1260/2285 train_time:76542ms step_avg:60.75ms
step:1261/2285 train_time:76605ms step_avg:60.75ms
step:1262/2285 train_time:76665ms step_avg:60.75ms
step:1263/2285 train_time:76726ms step_avg:60.75ms
step:1264/2285 train_time:76785ms step_avg:60.75ms
step:1265/2285 train_time:76847ms step_avg:60.75ms
step:1266/2285 train_time:76907ms step_avg:60.75ms
step:1267/2285 train_time:76971ms step_avg:60.75ms
step:1268/2285 train_time:77033ms step_avg:60.75ms
step:1269/2285 train_time:77097ms step_avg:60.75ms
step:1270/2285 train_time:77158ms step_avg:60.75ms
step:1271/2285 train_time:77221ms step_avg:60.76ms
step:1272/2285 train_time:77281ms step_avg:60.76ms
step:1273/2285 train_time:77343ms step_avg:60.76ms
step:1274/2285 train_time:77402ms step_avg:60.76ms
step:1275/2285 train_time:77464ms step_avg:60.76ms
step:1276/2285 train_time:77524ms step_avg:60.76ms
step:1277/2285 train_time:77586ms step_avg:60.76ms
step:1278/2285 train_time:77645ms step_avg:60.76ms
step:1279/2285 train_time:77707ms step_avg:60.76ms
step:1280/2285 train_time:77767ms step_avg:60.76ms
step:1281/2285 train_time:77828ms step_avg:60.76ms
step:1282/2285 train_time:77889ms step_avg:60.76ms
step:1283/2285 train_time:77951ms step_avg:60.76ms
step:1284/2285 train_time:78012ms step_avg:60.76ms
step:1285/2285 train_time:78075ms step_avg:60.76ms
step:1286/2285 train_time:78136ms step_avg:60.76ms
step:1287/2285 train_time:78199ms step_avg:60.76ms
step:1288/2285 train_time:78259ms step_avg:60.76ms
step:1289/2285 train_time:78321ms step_avg:60.76ms
step:1290/2285 train_time:78381ms step_avg:60.76ms
step:1291/2285 train_time:78443ms step_avg:60.76ms
step:1292/2285 train_time:78503ms step_avg:60.76ms
step:1293/2285 train_time:78564ms step_avg:60.76ms
step:1294/2285 train_time:78624ms step_avg:60.76ms
step:1295/2285 train_time:78686ms step_avg:60.76ms
step:1296/2285 train_time:78745ms step_avg:60.76ms
step:1297/2285 train_time:78808ms step_avg:60.76ms
step:1298/2285 train_time:78867ms step_avg:60.76ms
step:1299/2285 train_time:78931ms step_avg:60.76ms
step:1300/2285 train_time:78991ms step_avg:60.76ms
step:1301/2285 train_time:79054ms step_avg:60.76ms
step:1302/2285 train_time:79115ms step_avg:60.76ms
step:1303/2285 train_time:79178ms step_avg:60.77ms
step:1304/2285 train_time:79238ms step_avg:60.77ms
step:1305/2285 train_time:79301ms step_avg:60.77ms
step:1306/2285 train_time:79360ms step_avg:60.77ms
step:1307/2285 train_time:79423ms step_avg:60.77ms
step:1308/2285 train_time:79482ms step_avg:60.77ms
step:1309/2285 train_time:79544ms step_avg:60.77ms
step:1310/2285 train_time:79604ms step_avg:60.77ms
step:1311/2285 train_time:79666ms step_avg:60.77ms
step:1312/2285 train_time:79725ms step_avg:60.77ms
step:1313/2285 train_time:79788ms step_avg:60.77ms
step:1314/2285 train_time:79848ms step_avg:60.77ms
step:1315/2285 train_time:79910ms step_avg:60.77ms
step:1316/2285 train_time:79971ms step_avg:60.77ms
step:1317/2285 train_time:80033ms step_avg:60.77ms
step:1318/2285 train_time:80094ms step_avg:60.77ms
step:1319/2285 train_time:80157ms step_avg:60.77ms
step:1320/2285 train_time:80217ms step_avg:60.77ms
step:1321/2285 train_time:80280ms step_avg:60.77ms
step:1322/2285 train_time:80340ms step_avg:60.77ms
step:1323/2285 train_time:80402ms step_avg:60.77ms
step:1324/2285 train_time:80462ms step_avg:60.77ms
step:1325/2285 train_time:80524ms step_avg:60.77ms
step:1326/2285 train_time:80583ms step_avg:60.77ms
step:1327/2285 train_time:80645ms step_avg:60.77ms
step:1328/2285 train_time:80705ms step_avg:60.77ms
step:1329/2285 train_time:80767ms step_avg:60.77ms
step:1330/2285 train_time:80828ms step_avg:60.77ms
step:1331/2285 train_time:80890ms step_avg:60.77ms
step:1332/2285 train_time:80950ms step_avg:60.77ms
step:1333/2285 train_time:81012ms step_avg:60.77ms
step:1334/2285 train_time:81073ms step_avg:60.77ms
step:1335/2285 train_time:81136ms step_avg:60.78ms
step:1336/2285 train_time:81196ms step_avg:60.78ms
step:1337/2285 train_time:81259ms step_avg:60.78ms
step:1338/2285 train_time:81319ms step_avg:60.78ms
step:1339/2285 train_time:81381ms step_avg:60.78ms
step:1340/2285 train_time:81440ms step_avg:60.78ms
step:1341/2285 train_time:81502ms step_avg:60.78ms
step:1342/2285 train_time:81562ms step_avg:60.78ms
step:1343/2285 train_time:81624ms step_avg:60.78ms
step:1344/2285 train_time:81683ms step_avg:60.78ms
step:1345/2285 train_time:81745ms step_avg:60.78ms
step:1346/2285 train_time:81804ms step_avg:60.78ms
step:1347/2285 train_time:81868ms step_avg:60.78ms
step:1348/2285 train_time:81928ms step_avg:60.78ms
step:1349/2285 train_time:81991ms step_avg:60.78ms
step:1350/2285 train_time:82051ms step_avg:60.78ms
step:1351/2285 train_time:82114ms step_avg:60.78ms
step:1352/2285 train_time:82176ms step_avg:60.78ms
step:1353/2285 train_time:82238ms step_avg:60.78ms
step:1354/2285 train_time:82297ms step_avg:60.78ms
step:1355/2285 train_time:82360ms step_avg:60.78ms
step:1356/2285 train_time:82420ms step_avg:60.78ms
step:1357/2285 train_time:82482ms step_avg:60.78ms
step:1358/2285 train_time:82541ms step_avg:60.78ms
step:1359/2285 train_time:82603ms step_avg:60.78ms
step:1360/2285 train_time:82663ms step_avg:60.78ms
step:1361/2285 train_time:82725ms step_avg:60.78ms
step:1362/2285 train_time:82785ms step_avg:60.78ms
step:1363/2285 train_time:82847ms step_avg:60.78ms
step:1364/2285 train_time:82907ms step_avg:60.78ms
step:1365/2285 train_time:82970ms step_avg:60.78ms
step:1366/2285 train_time:83030ms step_avg:60.78ms
step:1367/2285 train_time:83093ms step_avg:60.79ms
step:1368/2285 train_time:83154ms step_avg:60.78ms
step:1369/2285 train_time:83217ms step_avg:60.79ms
step:1370/2285 train_time:83277ms step_avg:60.79ms
step:1371/2285 train_time:83339ms step_avg:60.79ms
step:1372/2285 train_time:83399ms step_avg:60.79ms
step:1373/2285 train_time:83461ms step_avg:60.79ms
step:1374/2285 train_time:83520ms step_avg:60.79ms
step:1375/2285 train_time:83582ms step_avg:60.79ms
step:1376/2285 train_time:83642ms step_avg:60.79ms
step:1377/2285 train_time:83704ms step_avg:60.79ms
step:1378/2285 train_time:83763ms step_avg:60.79ms
step:1379/2285 train_time:83826ms step_avg:60.79ms
step:1380/2285 train_time:83886ms step_avg:60.79ms
step:1381/2285 train_time:83949ms step_avg:60.79ms
step:1382/2285 train_time:84008ms step_avg:60.79ms
step:1383/2285 train_time:84071ms step_avg:60.79ms
step:1384/2285 train_time:84131ms step_avg:60.79ms
step:1385/2285 train_time:84194ms step_avg:60.79ms
step:1386/2285 train_time:84254ms step_avg:60.79ms
step:1387/2285 train_time:84317ms step_avg:60.79ms
step:1388/2285 train_time:84377ms step_avg:60.79ms
step:1389/2285 train_time:84439ms step_avg:60.79ms
step:1390/2285 train_time:84499ms step_avg:60.79ms
step:1391/2285 train_time:84562ms step_avg:60.79ms
step:1392/2285 train_time:84621ms step_avg:60.79ms
step:1393/2285 train_time:84684ms step_avg:60.79ms
step:1394/2285 train_time:84743ms step_avg:60.79ms
step:1395/2285 train_time:84805ms step_avg:60.79ms
step:1396/2285 train_time:84865ms step_avg:60.79ms
step:1397/2285 train_time:84928ms step_avg:60.79ms
step:1398/2285 train_time:84987ms step_avg:60.79ms
step:1399/2285 train_time:85050ms step_avg:60.79ms
step:1400/2285 train_time:85110ms step_avg:60.79ms
step:1401/2285 train_time:85173ms step_avg:60.79ms
step:1402/2285 train_time:85234ms step_avg:60.79ms
step:1403/2285 train_time:85297ms step_avg:60.80ms
step:1404/2285 train_time:85357ms step_avg:60.80ms
step:1405/2285 train_time:85420ms step_avg:60.80ms
step:1406/2285 train_time:85480ms step_avg:60.80ms
step:1407/2285 train_time:85542ms step_avg:60.80ms
step:1408/2285 train_time:85602ms step_avg:60.80ms
step:1409/2285 train_time:85664ms step_avg:60.80ms
step:1410/2285 train_time:85724ms step_avg:60.80ms
step:1411/2285 train_time:85787ms step_avg:60.80ms
step:1412/2285 train_time:85847ms step_avg:60.80ms
step:1413/2285 train_time:85909ms step_avg:60.80ms
step:1414/2285 train_time:85969ms step_avg:60.80ms
step:1415/2285 train_time:86031ms step_avg:60.80ms
step:1416/2285 train_time:86091ms step_avg:60.80ms
step:1417/2285 train_time:86153ms step_avg:60.80ms
step:1418/2285 train_time:86213ms step_avg:60.80ms
step:1419/2285 train_time:86276ms step_avg:60.80ms
step:1420/2285 train_time:86336ms step_avg:60.80ms
step:1421/2285 train_time:86399ms step_avg:60.80ms
step:1422/2285 train_time:86459ms step_avg:60.80ms
step:1423/2285 train_time:86522ms step_avg:60.80ms
step:1424/2285 train_time:86582ms step_avg:60.80ms
step:1425/2285 train_time:86643ms step_avg:60.80ms
step:1426/2285 train_time:86703ms step_avg:60.80ms
step:1427/2285 train_time:86765ms step_avg:60.80ms
step:1428/2285 train_time:86824ms step_avg:60.80ms
step:1429/2285 train_time:86887ms step_avg:60.80ms
step:1430/2285 train_time:86947ms step_avg:60.80ms
step:1431/2285 train_time:87009ms step_avg:60.80ms
step:1432/2285 train_time:87070ms step_avg:60.80ms
step:1433/2285 train_time:87133ms step_avg:60.80ms
step:1434/2285 train_time:87193ms step_avg:60.80ms
step:1435/2285 train_time:87256ms step_avg:60.81ms
step:1436/2285 train_time:87316ms step_avg:60.81ms
step:1437/2285 train_time:87379ms step_avg:60.81ms
step:1438/2285 train_time:87439ms step_avg:60.81ms
step:1439/2285 train_time:87502ms step_avg:60.81ms
step:1440/2285 train_time:87561ms step_avg:60.81ms
step:1441/2285 train_time:87623ms step_avg:60.81ms
step:1442/2285 train_time:87683ms step_avg:60.81ms
step:1443/2285 train_time:87745ms step_avg:60.81ms
step:1444/2285 train_time:87805ms step_avg:60.81ms
step:1445/2285 train_time:87867ms step_avg:60.81ms
step:1446/2285 train_time:87927ms step_avg:60.81ms
step:1447/2285 train_time:87989ms step_avg:60.81ms
step:1448/2285 train_time:88049ms step_avg:60.81ms
step:1449/2285 train_time:88112ms step_avg:60.81ms
step:1450/2285 train_time:88172ms step_avg:60.81ms
step:1451/2285 train_time:88235ms step_avg:60.81ms
step:1452/2285 train_time:88295ms step_avg:60.81ms
step:1453/2285 train_time:88358ms step_avg:60.81ms
step:1454/2285 train_time:88418ms step_avg:60.81ms
step:1455/2285 train_time:88481ms step_avg:60.81ms
step:1456/2285 train_time:88541ms step_avg:60.81ms
step:1457/2285 train_time:88603ms step_avg:60.81ms
step:1458/2285 train_time:88663ms step_avg:60.81ms
step:1459/2285 train_time:88725ms step_avg:60.81ms
step:1460/2285 train_time:88785ms step_avg:60.81ms
step:1461/2285 train_time:88846ms step_avg:60.81ms
step:1462/2285 train_time:88905ms step_avg:60.81ms
step:1463/2285 train_time:88968ms step_avg:60.81ms
step:1464/2285 train_time:89028ms step_avg:60.81ms
step:1465/2285 train_time:89091ms step_avg:60.81ms
step:1466/2285 train_time:89151ms step_avg:60.81ms
step:1467/2285 train_time:89214ms step_avg:60.81ms
step:1468/2285 train_time:89275ms step_avg:60.81ms
step:1469/2285 train_time:89337ms step_avg:60.82ms
step:1470/2285 train_time:89398ms step_avg:60.81ms
step:1471/2285 train_time:89461ms step_avg:60.82ms
step:1472/2285 train_time:89520ms step_avg:60.82ms
step:1473/2285 train_time:89582ms step_avg:60.82ms
step:1474/2285 train_time:89642ms step_avg:60.82ms
step:1475/2285 train_time:89704ms step_avg:60.82ms
step:1476/2285 train_time:89764ms step_avg:60.82ms
step:1477/2285 train_time:89826ms step_avg:60.82ms
step:1478/2285 train_time:89886ms step_avg:60.82ms
step:1479/2285 train_time:89948ms step_avg:60.82ms
step:1480/2285 train_time:90007ms step_avg:60.82ms
step:1481/2285 train_time:90070ms step_avg:60.82ms
step:1482/2285 train_time:90130ms step_avg:60.82ms
step:1483/2285 train_time:90192ms step_avg:60.82ms
step:1484/2285 train_time:90252ms step_avg:60.82ms
step:1485/2285 train_time:90315ms step_avg:60.82ms
step:1486/2285 train_time:90376ms step_avg:60.82ms
step:1487/2285 train_time:90438ms step_avg:60.82ms
step:1488/2285 train_time:90498ms step_avg:60.82ms
step:1489/2285 train_time:90561ms step_avg:60.82ms
step:1490/2285 train_time:90620ms step_avg:60.82ms
step:1491/2285 train_time:90683ms step_avg:60.82ms
step:1492/2285 train_time:90743ms step_avg:60.82ms
step:1493/2285 train_time:90804ms step_avg:60.82ms
step:1494/2285 train_time:90864ms step_avg:60.82ms
step:1495/2285 train_time:90926ms step_avg:60.82ms
step:1496/2285 train_time:90986ms step_avg:60.82ms
step:1497/2285 train_time:91049ms step_avg:60.82ms
step:1498/2285 train_time:91109ms step_avg:60.82ms
step:1499/2285 train_time:91172ms step_avg:60.82ms
step:1500/2285 train_time:91233ms step_avg:60.82ms
step:1500/2285 val_loss:3.4258 train_time:91297ms step_avg:60.86ms
step:1501/2285 train_time:91319ms step_avg:60.84ms
step:1502/2285 train_time:91359ms step_avg:60.82ms
step:1503/2285 train_time:91421ms step_avg:60.83ms
step:1504/2285 train_time:91483ms step_avg:60.83ms
step:1505/2285 train_time:91547ms step_avg:60.83ms
step:1506/2285 train_time:91607ms step_avg:60.83ms
step:1507/2285 train_time:91669ms step_avg:60.83ms
step:1508/2285 train_time:91728ms step_avg:60.83ms
step:1509/2285 train_time:91790ms step_avg:60.83ms
step:1510/2285 train_time:91850ms step_avg:60.83ms
step:1511/2285 train_time:91911ms step_avg:60.83ms
step:1512/2285 train_time:91971ms step_avg:60.83ms
step:1513/2285 train_time:92033ms step_avg:60.83ms
step:1514/2285 train_time:92094ms step_avg:60.83ms
step:1515/2285 train_time:92157ms step_avg:60.83ms
step:1516/2285 train_time:92219ms step_avg:60.83ms
step:1517/2285 train_time:92285ms step_avg:60.83ms
step:1518/2285 train_time:92346ms step_avg:60.83ms
step:1519/2285 train_time:92409ms step_avg:60.84ms
step:1520/2285 train_time:92470ms step_avg:60.84ms
step:1521/2285 train_time:92533ms step_avg:60.84ms
step:1522/2285 train_time:92594ms step_avg:60.84ms
step:1523/2285 train_time:92657ms step_avg:60.84ms
step:1524/2285 train_time:92717ms step_avg:60.84ms
step:1525/2285 train_time:92779ms step_avg:60.84ms
step:1526/2285 train_time:92840ms step_avg:60.84ms
step:1527/2285 train_time:92902ms step_avg:60.84ms
step:1528/2285 train_time:92963ms step_avg:60.84ms
step:1529/2285 train_time:93026ms step_avg:60.84ms
step:1530/2285 train_time:93086ms step_avg:60.84ms
step:1531/2285 train_time:93148ms step_avg:60.84ms
step:1532/2285 train_time:93209ms step_avg:60.84ms
step:1533/2285 train_time:93273ms step_avg:60.84ms
step:1534/2285 train_time:93334ms step_avg:60.84ms
step:1535/2285 train_time:93398ms step_avg:60.85ms
step:1536/2285 train_time:93458ms step_avg:60.85ms
step:1537/2285 train_time:93522ms step_avg:60.85ms
step:1538/2285 train_time:93582ms step_avg:60.85ms
step:1539/2285 train_time:93646ms step_avg:60.85ms
step:1540/2285 train_time:93706ms step_avg:60.85ms
step:1541/2285 train_time:93768ms step_avg:60.85ms
step:1542/2285 train_time:93828ms step_avg:60.85ms
step:1543/2285 train_time:93891ms step_avg:60.85ms
step:1544/2285 train_time:93950ms step_avg:60.85ms
step:1545/2285 train_time:94013ms step_avg:60.85ms
step:1546/2285 train_time:94073ms step_avg:60.85ms
step:1547/2285 train_time:94136ms step_avg:60.85ms
step:1548/2285 train_time:94197ms step_avg:60.85ms
step:1549/2285 train_time:94260ms step_avg:60.85ms
step:1550/2285 train_time:94322ms step_avg:60.85ms
step:1551/2285 train_time:94385ms step_avg:60.85ms
step:1552/2285 train_time:94445ms step_avg:60.85ms
step:1553/2285 train_time:94509ms step_avg:60.86ms
step:1554/2285 train_time:94569ms step_avg:60.86ms
step:1555/2285 train_time:94632ms step_avg:60.86ms
step:1556/2285 train_time:94692ms step_avg:60.86ms
step:1557/2285 train_time:94755ms step_avg:60.86ms
step:1558/2285 train_time:94817ms step_avg:60.86ms
step:1559/2285 train_time:94879ms step_avg:60.86ms
step:1560/2285 train_time:94940ms step_avg:60.86ms
step:1561/2285 train_time:95003ms step_avg:60.86ms
step:1562/2285 train_time:95063ms step_avg:60.86ms
step:1563/2285 train_time:95126ms step_avg:60.86ms
step:1564/2285 train_time:95186ms step_avg:60.86ms
step:1565/2285 train_time:95250ms step_avg:60.86ms
step:1566/2285 train_time:95310ms step_avg:60.86ms
step:1567/2285 train_time:95373ms step_avg:60.86ms
step:1568/2285 train_time:95433ms step_avg:60.86ms
step:1569/2285 train_time:95497ms step_avg:60.86ms
step:1570/2285 train_time:95557ms step_avg:60.86ms
step:1571/2285 train_time:95621ms step_avg:60.87ms
step:1572/2285 train_time:95681ms step_avg:60.87ms
step:1573/2285 train_time:95745ms step_avg:60.87ms
step:1574/2285 train_time:95805ms step_avg:60.87ms
step:1575/2285 train_time:95868ms step_avg:60.87ms
step:1576/2285 train_time:95928ms step_avg:60.87ms
step:1577/2285 train_time:95990ms step_avg:60.87ms
step:1578/2285 train_time:96050ms step_avg:60.87ms
step:1579/2285 train_time:96112ms step_avg:60.87ms
step:1580/2285 train_time:96173ms step_avg:60.87ms
step:1581/2285 train_time:96235ms step_avg:60.87ms
step:1582/2285 train_time:96295ms step_avg:60.87ms
step:1583/2285 train_time:96358ms step_avg:60.87ms
step:1584/2285 train_time:96419ms step_avg:60.87ms
step:1585/2285 train_time:96482ms step_avg:60.87ms
step:1586/2285 train_time:96543ms step_avg:60.87ms
step:1587/2285 train_time:96607ms step_avg:60.87ms
step:1588/2285 train_time:96666ms step_avg:60.87ms
step:1589/2285 train_time:96729ms step_avg:60.87ms
step:1590/2285 train_time:96789ms step_avg:60.87ms
step:1591/2285 train_time:96852ms step_avg:60.87ms
step:1592/2285 train_time:96912ms step_avg:60.87ms
step:1593/2285 train_time:96975ms step_avg:60.88ms
step:1594/2285 train_time:97035ms step_avg:60.88ms
step:1595/2285 train_time:97099ms step_avg:60.88ms
step:1596/2285 train_time:97159ms step_avg:60.88ms
step:1597/2285 train_time:97222ms step_avg:60.88ms
step:1598/2285 train_time:97283ms step_avg:60.88ms
step:1599/2285 train_time:97346ms step_avg:60.88ms
step:1600/2285 train_time:97407ms step_avg:60.88ms
step:1601/2285 train_time:97469ms step_avg:60.88ms
step:1602/2285 train_time:97530ms step_avg:60.88ms
step:1603/2285 train_time:97593ms step_avg:60.88ms
step:1604/2285 train_time:97654ms step_avg:60.88ms
step:1605/2285 train_time:97716ms step_avg:60.88ms
step:1606/2285 train_time:97776ms step_avg:60.88ms
step:1607/2285 train_time:97840ms step_avg:60.88ms
step:1608/2285 train_time:97900ms step_avg:60.88ms
step:1609/2285 train_time:97963ms step_avg:60.88ms
step:1610/2285 train_time:98024ms step_avg:60.88ms
step:1611/2285 train_time:98086ms step_avg:60.89ms
step:1612/2285 train_time:98146ms step_avg:60.88ms
step:1613/2285 train_time:98209ms step_avg:60.89ms
step:1614/2285 train_time:98269ms step_avg:60.89ms
step:1615/2285 train_time:98332ms step_avg:60.89ms
step:1616/2285 train_time:98392ms step_avg:60.89ms
step:1617/2285 train_time:98455ms step_avg:60.89ms
step:1618/2285 train_time:98515ms step_avg:60.89ms
step:1619/2285 train_time:98578ms step_avg:60.89ms
step:1620/2285 train_time:98638ms step_avg:60.89ms
step:1621/2285 train_time:98701ms step_avg:60.89ms
step:1622/2285 train_time:98762ms step_avg:60.89ms
step:1623/2285 train_time:98825ms step_avg:60.89ms
step:1624/2285 train_time:98886ms step_avg:60.89ms
step:1625/2285 train_time:98949ms step_avg:60.89ms
step:1626/2285 train_time:99009ms step_avg:60.89ms
step:1627/2285 train_time:99072ms step_avg:60.89ms
step:1628/2285 train_time:99132ms step_avg:60.89ms
step:1629/2285 train_time:99195ms step_avg:60.89ms
step:1630/2285 train_time:99255ms step_avg:60.89ms
step:1631/2285 train_time:99317ms step_avg:60.89ms
step:1632/2285 train_time:99377ms step_avg:60.89ms
step:1633/2285 train_time:99440ms step_avg:60.89ms
step:1634/2285 train_time:99501ms step_avg:60.89ms
step:1635/2285 train_time:99564ms step_avg:60.90ms
step:1636/2285 train_time:99625ms step_avg:60.90ms
step:1637/2285 train_time:99688ms step_avg:60.90ms
step:1638/2285 train_time:99748ms step_avg:60.90ms
step:1639/2285 train_time:99811ms step_avg:60.90ms
step:1640/2285 train_time:99871ms step_avg:60.90ms
step:1641/2285 train_time:99933ms step_avg:60.90ms
step:1642/2285 train_time:99993ms step_avg:60.90ms
step:1643/2285 train_time:100056ms step_avg:60.90ms
step:1644/2285 train_time:100117ms step_avg:60.90ms
step:1645/2285 train_time:100179ms step_avg:60.90ms
step:1646/2285 train_time:100240ms step_avg:60.90ms
step:1647/2285 train_time:100304ms step_avg:60.90ms
step:1648/2285 train_time:100364ms step_avg:60.90ms
step:1649/2285 train_time:100428ms step_avg:60.90ms
step:1650/2285 train_time:100489ms step_avg:60.90ms
step:1651/2285 train_time:100551ms step_avg:60.90ms
step:1652/2285 train_time:100611ms step_avg:60.90ms
step:1653/2285 train_time:100674ms step_avg:60.90ms
step:1654/2285 train_time:100734ms step_avg:60.90ms
step:1655/2285 train_time:100798ms step_avg:60.90ms
step:1656/2285 train_time:100858ms step_avg:60.90ms
step:1657/2285 train_time:100921ms step_avg:60.91ms
step:1658/2285 train_time:100982ms step_avg:60.91ms
step:1659/2285 train_time:101045ms step_avg:60.91ms
step:1660/2285 train_time:101106ms step_avg:60.91ms
step:1661/2285 train_time:101168ms step_avg:60.91ms
step:1662/2285 train_time:101228ms step_avg:60.91ms
step:1663/2285 train_time:101291ms step_avg:60.91ms
step:1664/2285 train_time:101351ms step_avg:60.91ms
step:1665/2285 train_time:101414ms step_avg:60.91ms
step:1666/2285 train_time:101474ms step_avg:60.91ms
step:1667/2285 train_time:101537ms step_avg:60.91ms
step:1668/2285 train_time:101598ms step_avg:60.91ms
step:1669/2285 train_time:101662ms step_avg:60.91ms
step:1670/2285 train_time:101723ms step_avg:60.91ms
step:1671/2285 train_time:101786ms step_avg:60.91ms
step:1672/2285 train_time:101847ms step_avg:60.91ms
step:1673/2285 train_time:101909ms step_avg:60.91ms
step:1674/2285 train_time:101969ms step_avg:60.91ms
step:1675/2285 train_time:102033ms step_avg:60.92ms
step:1676/2285 train_time:102093ms step_avg:60.91ms
step:1677/2285 train_time:102156ms step_avg:60.92ms
step:1678/2285 train_time:102216ms step_avg:60.92ms
step:1679/2285 train_time:102279ms step_avg:60.92ms
step:1680/2285 train_time:102340ms step_avg:60.92ms
step:1681/2285 train_time:102404ms step_avg:60.92ms
step:1682/2285 train_time:102464ms step_avg:60.92ms
step:1683/2285 train_time:102527ms step_avg:60.92ms
step:1684/2285 train_time:102587ms step_avg:60.92ms
step:1685/2285 train_time:102650ms step_avg:60.92ms
step:1686/2285 train_time:102710ms step_avg:60.92ms
step:1687/2285 train_time:102773ms step_avg:60.92ms
step:1688/2285 train_time:102833ms step_avg:60.92ms
step:1689/2285 train_time:102896ms step_avg:60.92ms
step:1690/2285 train_time:102957ms step_avg:60.92ms
step:1691/2285 train_time:103020ms step_avg:60.92ms
step:1692/2285 train_time:103081ms step_avg:60.92ms
step:1693/2285 train_time:103144ms step_avg:60.92ms
step:1694/2285 train_time:103205ms step_avg:60.92ms
step:1695/2285 train_time:103268ms step_avg:60.92ms
step:1696/2285 train_time:103328ms step_avg:60.92ms
step:1697/2285 train_time:103391ms step_avg:60.93ms
step:1698/2285 train_time:103451ms step_avg:60.93ms
step:1699/2285 train_time:103514ms step_avg:60.93ms
step:1700/2285 train_time:103575ms step_avg:60.93ms
step:1701/2285 train_time:103638ms step_avg:60.93ms
step:1702/2285 train_time:103698ms step_avg:60.93ms
step:1703/2285 train_time:103761ms step_avg:60.93ms
step:1704/2285 train_time:103822ms step_avg:60.93ms
step:1705/2285 train_time:103886ms step_avg:60.93ms
step:1706/2285 train_time:103946ms step_avg:60.93ms
step:1707/2285 train_time:104009ms step_avg:60.93ms
step:1708/2285 train_time:104069ms step_avg:60.93ms
step:1709/2285 train_time:104132ms step_avg:60.93ms
step:1710/2285 train_time:104192ms step_avg:60.93ms
step:1711/2285 train_time:104254ms step_avg:60.93ms
step:1712/2285 train_time:104315ms step_avg:60.93ms
step:1713/2285 train_time:104378ms step_avg:60.93ms
step:1714/2285 train_time:104439ms step_avg:60.93ms
step:1715/2285 train_time:104502ms step_avg:60.93ms
step:1716/2285 train_time:104562ms step_avg:60.93ms
step:1717/2285 train_time:104626ms step_avg:60.94ms
step:1718/2285 train_time:104686ms step_avg:60.94ms
step:1719/2285 train_time:104749ms step_avg:60.94ms
step:1720/2285 train_time:104809ms step_avg:60.94ms
step:1721/2285 train_time:104872ms step_avg:60.94ms
step:1722/2285 train_time:104933ms step_avg:60.94ms
step:1723/2285 train_time:104996ms step_avg:60.94ms
step:1724/2285 train_time:105056ms step_avg:60.94ms
step:1725/2285 train_time:105119ms step_avg:60.94ms
step:1726/2285 train_time:105179ms step_avg:60.94ms
step:1727/2285 train_time:105243ms step_avg:60.94ms
step:1728/2285 train_time:105304ms step_avg:60.94ms
step:1729/2285 train_time:105366ms step_avg:60.94ms
step:1730/2285 train_time:105426ms step_avg:60.94ms
step:1731/2285 train_time:105490ms step_avg:60.94ms
step:1732/2285 train_time:105549ms step_avg:60.94ms
step:1733/2285 train_time:105612ms step_avg:60.94ms
step:1734/2285 train_time:105672ms step_avg:60.94ms
step:1735/2285 train_time:105734ms step_avg:60.94ms
step:1736/2285 train_time:105794ms step_avg:60.94ms
step:1737/2285 train_time:105858ms step_avg:60.94ms
step:1738/2285 train_time:105918ms step_avg:60.94ms
step:1739/2285 train_time:105981ms step_avg:60.94ms
step:1740/2285 train_time:106041ms step_avg:60.94ms
step:1741/2285 train_time:106104ms step_avg:60.94ms
step:1742/2285 train_time:106164ms step_avg:60.94ms
step:1743/2285 train_time:106228ms step_avg:60.95ms
step:1744/2285 train_time:106288ms step_avg:60.94ms
step:1745/2285 train_time:106350ms step_avg:60.95ms
step:1746/2285 train_time:106410ms step_avg:60.95ms
step:1747/2285 train_time:106473ms step_avg:60.95ms
step:1748/2285 train_time:106533ms step_avg:60.95ms
step:1749/2285 train_time:106596ms step_avg:60.95ms
step:1750/2285 train_time:106656ms step_avg:60.95ms
step:1750/2285 val_loss:3.3669 train_time:106721ms step_avg:60.98ms
step:1751/2285 train_time:106740ms step_avg:60.96ms
step:1752/2285 train_time:106783ms step_avg:60.95ms
step:1753/2285 train_time:106849ms step_avg:60.95ms
step:1754/2285 train_time:106910ms step_avg:60.95ms
step:1755/2285 train_time:106973ms step_avg:60.95ms
step:1756/2285 train_time:107034ms step_avg:60.95ms
step:1757/2285 train_time:107095ms step_avg:60.95ms
step:1758/2285 train_time:107156ms step_avg:60.95ms
step:1759/2285 train_time:107218ms step_avg:60.95ms
step:1760/2285 train_time:107278ms step_avg:60.95ms
step:1761/2285 train_time:107340ms step_avg:60.95ms
step:1762/2285 train_time:107400ms step_avg:60.95ms
step:1763/2285 train_time:107462ms step_avg:60.95ms
step:1764/2285 train_time:107522ms step_avg:60.95ms
step:1765/2285 train_time:107584ms step_avg:60.95ms
step:1766/2285 train_time:107644ms step_avg:60.95ms
step:1767/2285 train_time:107708ms step_avg:60.96ms
step:1768/2285 train_time:107769ms step_avg:60.96ms
step:1769/2285 train_time:107833ms step_avg:60.96ms
step:1770/2285 train_time:107894ms step_avg:60.96ms
step:1771/2285 train_time:107957ms step_avg:60.96ms
step:1772/2285 train_time:108018ms step_avg:60.96ms
step:1773/2285 train_time:108081ms step_avg:60.96ms
step:1774/2285 train_time:108142ms step_avg:60.96ms
step:1775/2285 train_time:108204ms step_avg:60.96ms
step:1776/2285 train_time:108264ms step_avg:60.96ms
step:1777/2285 train_time:108326ms step_avg:60.96ms
step:1778/2285 train_time:108387ms step_avg:60.96ms
step:1779/2285 train_time:108449ms step_avg:60.96ms
step:1780/2285 train_time:108510ms step_avg:60.96ms
step:1781/2285 train_time:108572ms step_avg:60.96ms
step:1782/2285 train_time:108633ms step_avg:60.96ms
step:1783/2285 train_time:108696ms step_avg:60.96ms
step:1784/2285 train_time:108758ms step_avg:60.96ms
step:1785/2285 train_time:108821ms step_avg:60.96ms
step:1786/2285 train_time:108882ms step_avg:60.96ms
step:1787/2285 train_time:108945ms step_avg:60.97ms
step:1788/2285 train_time:109005ms step_avg:60.96ms
step:1789/2285 train_time:109068ms step_avg:60.97ms
step:1790/2285 train_time:109129ms step_avg:60.97ms
step:1791/2285 train_time:109191ms step_avg:60.97ms
step:1792/2285 train_time:109252ms step_avg:60.97ms
step:1793/2285 train_time:109316ms step_avg:60.97ms
step:1794/2285 train_time:109376ms step_avg:60.97ms
step:1795/2285 train_time:109438ms step_avg:60.97ms
step:1796/2285 train_time:109498ms step_avg:60.97ms
step:1797/2285 train_time:109560ms step_avg:60.97ms
step:1798/2285 train_time:109620ms step_avg:60.97ms
step:1799/2285 train_time:109683ms step_avg:60.97ms
step:1800/2285 train_time:109744ms step_avg:60.97ms
step:1801/2285 train_time:109807ms step_avg:60.97ms
step:1802/2285 train_time:109868ms step_avg:60.97ms
step:1803/2285 train_time:109932ms step_avg:60.97ms
step:1804/2285 train_time:109992ms step_avg:60.97ms
step:1805/2285 train_time:110055ms step_avg:60.97ms
step:1806/2285 train_time:110116ms step_avg:60.97ms
step:1807/2285 train_time:110179ms step_avg:60.97ms
step:1808/2285 train_time:110239ms step_avg:60.97ms
step:1809/2285 train_time:110301ms step_avg:60.97ms
step:1810/2285 train_time:110361ms step_avg:60.97ms
step:1811/2285 train_time:110424ms step_avg:60.97ms
step:1812/2285 train_time:110485ms step_avg:60.97ms
step:1813/2285 train_time:110547ms step_avg:60.97ms
step:1814/2285 train_time:110608ms step_avg:60.97ms
step:1815/2285 train_time:110671ms step_avg:60.98ms
step:1816/2285 train_time:110732ms step_avg:60.98ms
step:1817/2285 train_time:110796ms step_avg:60.98ms
step:1818/2285 train_time:110857ms step_avg:60.98ms
step:1819/2285 train_time:110920ms step_avg:60.98ms
step:1820/2285 train_time:110980ms step_avg:60.98ms
step:1821/2285 train_time:111042ms step_avg:60.98ms
step:1822/2285 train_time:111102ms step_avg:60.98ms
step:1823/2285 train_time:111165ms step_avg:60.98ms
step:1824/2285 train_time:111225ms step_avg:60.98ms
step:1825/2285 train_time:111289ms step_avg:60.98ms
step:1826/2285 train_time:111349ms step_avg:60.98ms
step:1827/2285 train_time:111411ms step_avg:60.98ms
step:1828/2285 train_time:111472ms step_avg:60.98ms
step:1829/2285 train_time:111535ms step_avg:60.98ms
step:1830/2285 train_time:111595ms step_avg:60.98ms
step:1831/2285 train_time:111658ms step_avg:60.98ms
step:1832/2285 train_time:111718ms step_avg:60.98ms
step:1833/2285 train_time:111781ms step_avg:60.98ms
step:1834/2285 train_time:111842ms step_avg:60.98ms
step:1835/2285 train_time:111905ms step_avg:60.98ms
step:1836/2285 train_time:111965ms step_avg:60.98ms
step:1837/2285 train_time:112029ms step_avg:60.98ms
step:1838/2285 train_time:112090ms step_avg:60.98ms
step:1839/2285 train_time:112152ms step_avg:60.99ms
step:1840/2285 train_time:112213ms step_avg:60.99ms
step:1841/2285 train_time:112276ms step_avg:60.99ms
step:1842/2285 train_time:112337ms step_avg:60.99ms
step:1843/2285 train_time:112399ms step_avg:60.99ms
step:1844/2285 train_time:112459ms step_avg:60.99ms
step:1845/2285 train_time:112522ms step_avg:60.99ms
step:1846/2285 train_time:112582ms step_avg:60.99ms
step:1847/2285 train_time:112645ms step_avg:60.99ms
step:1848/2285 train_time:112705ms step_avg:60.99ms
step:1849/2285 train_time:112769ms step_avg:60.99ms
step:1850/2285 train_time:112831ms step_avg:60.99ms
step:1851/2285 train_time:112893ms step_avg:60.99ms
step:1852/2285 train_time:112954ms step_avg:60.99ms
step:1853/2285 train_time:113018ms step_avg:60.99ms
step:1854/2285 train_time:113077ms step_avg:60.99ms
step:1855/2285 train_time:113141ms step_avg:60.99ms
step:1856/2285 train_time:113202ms step_avg:60.99ms
step:1857/2285 train_time:113265ms step_avg:60.99ms
step:1858/2285 train_time:113325ms step_avg:60.99ms
step:1859/2285 train_time:113387ms step_avg:60.99ms
step:1860/2285 train_time:113448ms step_avg:60.99ms
step:1861/2285 train_time:113511ms step_avg:60.99ms
step:1862/2285 train_time:113572ms step_avg:60.99ms
step:1863/2285 train_time:113635ms step_avg:61.00ms
step:1864/2285 train_time:113695ms step_avg:61.00ms
step:1865/2285 train_time:113758ms step_avg:61.00ms
step:1866/2285 train_time:113818ms step_avg:61.00ms
step:1867/2285 train_time:113881ms step_avg:61.00ms
step:1868/2285 train_time:113942ms step_avg:61.00ms
step:1869/2285 train_time:114004ms step_avg:61.00ms
step:1870/2285 train_time:114065ms step_avg:61.00ms
step:1871/2285 train_time:114129ms step_avg:61.00ms
step:1872/2285 train_time:114189ms step_avg:61.00ms
step:1873/2285 train_time:114252ms step_avg:61.00ms
step:1874/2285 train_time:114313ms step_avg:61.00ms
step:1875/2285 train_time:114376ms step_avg:61.00ms
step:1876/2285 train_time:114436ms step_avg:61.00ms
step:1877/2285 train_time:114499ms step_avg:61.00ms
step:1878/2285 train_time:114559ms step_avg:61.00ms
step:1879/2285 train_time:114622ms step_avg:61.00ms
step:1880/2285 train_time:114682ms step_avg:61.00ms
step:1881/2285 train_time:114745ms step_avg:61.00ms
step:1882/2285 train_time:114805ms step_avg:61.00ms
step:1883/2285 train_time:114868ms step_avg:61.00ms
step:1884/2285 train_time:114929ms step_avg:61.00ms
step:1885/2285 train_time:114991ms step_avg:61.00ms
step:1886/2285 train_time:115052ms step_avg:61.00ms
step:1887/2285 train_time:115115ms step_avg:61.00ms
step:1888/2285 train_time:115176ms step_avg:61.00ms
step:1889/2285 train_time:115239ms step_avg:61.01ms
step:1890/2285 train_time:115299ms step_avg:61.00ms
step:1891/2285 train_time:115361ms step_avg:61.01ms
step:1892/2285 train_time:115422ms step_avg:61.01ms
step:1893/2285 train_time:115484ms step_avg:61.01ms
step:1894/2285 train_time:115544ms step_avg:61.01ms
step:1895/2285 train_time:115607ms step_avg:61.01ms
step:1896/2285 train_time:115668ms step_avg:61.01ms
step:1897/2285 train_time:115731ms step_avg:61.01ms
step:1898/2285 train_time:115792ms step_avg:61.01ms
step:1899/2285 train_time:115855ms step_avg:61.01ms
step:1900/2285 train_time:115916ms step_avg:61.01ms
step:1901/2285 train_time:115978ms step_avg:61.01ms
step:1902/2285 train_time:116038ms step_avg:61.01ms
step:1903/2285 train_time:116101ms step_avg:61.01ms
step:1904/2285 train_time:116161ms step_avg:61.01ms
step:1905/2285 train_time:116224ms step_avg:61.01ms
step:1906/2285 train_time:116284ms step_avg:61.01ms
step:1907/2285 train_time:116347ms step_avg:61.01ms
step:1908/2285 train_time:116408ms step_avg:61.01ms
step:1909/2285 train_time:116471ms step_avg:61.01ms
step:1910/2285 train_time:116531ms step_avg:61.01ms
step:1911/2285 train_time:116594ms step_avg:61.01ms
step:1912/2285 train_time:116655ms step_avg:61.01ms
step:1913/2285 train_time:116718ms step_avg:61.01ms
step:1914/2285 train_time:116777ms step_avg:61.01ms
step:1915/2285 train_time:116840ms step_avg:61.01ms
step:1916/2285 train_time:116900ms step_avg:61.01ms
step:1917/2285 train_time:116963ms step_avg:61.01ms
step:1918/2285 train_time:117023ms step_avg:61.01ms
step:1919/2285 train_time:117086ms step_avg:61.01ms
step:1920/2285 train_time:117146ms step_avg:61.01ms
step:1921/2285 train_time:117209ms step_avg:61.01ms
step:1922/2285 train_time:117270ms step_avg:61.01ms
step:1923/2285 train_time:117332ms step_avg:61.02ms
step:1924/2285 train_time:117393ms step_avg:61.01ms
step:1925/2285 train_time:117456ms step_avg:61.02ms
step:1926/2285 train_time:117516ms step_avg:61.02ms
step:1927/2285 train_time:117579ms step_avg:61.02ms
step:1928/2285 train_time:117640ms step_avg:61.02ms
step:1929/2285 train_time:117702ms step_avg:61.02ms
step:1930/2285 train_time:117762ms step_avg:61.02ms
step:1931/2285 train_time:117826ms step_avg:61.02ms
step:1932/2285 train_time:117886ms step_avg:61.02ms
step:1933/2285 train_time:117949ms step_avg:61.02ms
step:1934/2285 train_time:118010ms step_avg:61.02ms
step:1935/2285 train_time:118074ms step_avg:61.02ms
step:1936/2285 train_time:118134ms step_avg:61.02ms
step:1937/2285 train_time:118197ms step_avg:61.02ms
step:1938/2285 train_time:118257ms step_avg:61.02ms
step:1939/2285 train_time:118320ms step_avg:61.02ms
step:1940/2285 train_time:118380ms step_avg:61.02ms
step:1941/2285 train_time:118443ms step_avg:61.02ms
step:1942/2285 train_time:118504ms step_avg:61.02ms
step:1943/2285 train_time:118567ms step_avg:61.02ms
step:1944/2285 train_time:118627ms step_avg:61.02ms
step:1945/2285 train_time:118690ms step_avg:61.02ms
step:1946/2285 train_time:118751ms step_avg:61.02ms
step:1947/2285 train_time:118815ms step_avg:61.02ms
step:1948/2285 train_time:118875ms step_avg:61.02ms
step:1949/2285 train_time:118938ms step_avg:61.03ms
step:1950/2285 train_time:118998ms step_avg:61.02ms
step:1951/2285 train_time:119061ms step_avg:61.03ms
step:1952/2285 train_time:119122ms step_avg:61.03ms
step:1953/2285 train_time:119185ms step_avg:61.03ms
step:1954/2285 train_time:119246ms step_avg:61.03ms
step:1955/2285 train_time:119309ms step_avg:61.03ms
step:1956/2285 train_time:119369ms step_avg:61.03ms
step:1957/2285 train_time:119432ms step_avg:61.03ms
step:1958/2285 train_time:119493ms step_avg:61.03ms
step:1959/2285 train_time:119556ms step_avg:61.03ms
step:1960/2285 train_time:119616ms step_avg:61.03ms
step:1961/2285 train_time:119679ms step_avg:61.03ms
step:1962/2285 train_time:119739ms step_avg:61.03ms
step:1963/2285 train_time:119801ms step_avg:61.03ms
step:1964/2285 train_time:119861ms step_avg:61.03ms
step:1965/2285 train_time:119924ms step_avg:61.03ms
step:1966/2285 train_time:119986ms step_avg:61.03ms
step:1967/2285 train_time:120049ms step_avg:61.03ms
step:1968/2285 train_time:120110ms step_avg:61.03ms
step:1969/2285 train_time:120172ms step_avg:61.03ms
step:1970/2285 train_time:120233ms step_avg:61.03ms
step:1971/2285 train_time:120295ms step_avg:61.03ms
step:1972/2285 train_time:120356ms step_avg:61.03ms
step:1973/2285 train_time:120420ms step_avg:61.03ms
step:1974/2285 train_time:120480ms step_avg:61.03ms
step:1975/2285 train_time:120542ms step_avg:61.03ms
step:1976/2285 train_time:120603ms step_avg:61.03ms
step:1977/2285 train_time:120666ms step_avg:61.03ms
step:1978/2285 train_time:120726ms step_avg:61.03ms
step:1979/2285 train_time:120789ms step_avg:61.04ms
step:1980/2285 train_time:120850ms step_avg:61.04ms
step:1981/2285 train_time:120912ms step_avg:61.04ms
step:1982/2285 train_time:120973ms step_avg:61.04ms
step:1983/2285 train_time:121036ms step_avg:61.04ms
step:1984/2285 train_time:121096ms step_avg:61.04ms
step:1985/2285 train_time:121160ms step_avg:61.04ms
step:1986/2285 train_time:121220ms step_avg:61.04ms
step:1987/2285 train_time:121282ms step_avg:61.04ms
step:1988/2285 train_time:121343ms step_avg:61.04ms
step:1989/2285 train_time:121406ms step_avg:61.04ms
step:1990/2285 train_time:121466ms step_avg:61.04ms
step:1991/2285 train_time:121529ms step_avg:61.04ms
step:1992/2285 train_time:121590ms step_avg:61.04ms
step:1993/2285 train_time:121653ms step_avg:61.04ms
step:1994/2285 train_time:121715ms step_avg:61.04ms
step:1995/2285 train_time:121778ms step_avg:61.04ms
step:1996/2285 train_time:121838ms step_avg:61.04ms
step:1997/2285 train_time:121900ms step_avg:61.04ms
step:1998/2285 train_time:121960ms step_avg:61.04ms
step:1999/2285 train_time:122023ms step_avg:61.04ms
step:2000/2285 train_time:122084ms step_avg:61.04ms
step:2000/2285 val_loss:3.3212 train_time:122148ms step_avg:61.07ms
step:2001/2285 train_time:122166ms step_avg:61.05ms
step:2002/2285 train_time:122209ms step_avg:61.04ms
step:2003/2285 train_time:122275ms step_avg:61.05ms
step:2004/2285 train_time:122336ms step_avg:61.05ms
step:2005/2285 train_time:122399ms step_avg:61.05ms
step:2006/2285 train_time:122460ms step_avg:61.05ms
step:2007/2285 train_time:122522ms step_avg:61.05ms
step:2008/2285 train_time:122582ms step_avg:61.05ms
step:2009/2285 train_time:122645ms step_avg:61.05ms
step:2010/2285 train_time:122705ms step_avg:61.05ms
step:2011/2285 train_time:122768ms step_avg:61.05ms
step:2012/2285 train_time:122827ms step_avg:61.05ms
step:2013/2285 train_time:122891ms step_avg:61.05ms
step:2014/2285 train_time:122951ms step_avg:61.05ms
step:2015/2285 train_time:123013ms step_avg:61.05ms
step:2016/2285 train_time:123073ms step_avg:61.05ms
step:2017/2285 train_time:123137ms step_avg:61.05ms
step:2018/2285 train_time:123198ms step_avg:61.05ms
step:2019/2285 train_time:123262ms step_avg:61.05ms
step:2020/2285 train_time:123323ms step_avg:61.05ms
step:2021/2285 train_time:123387ms step_avg:61.05ms
step:2022/2285 train_time:123447ms step_avg:61.05ms
step:2023/2285 train_time:123510ms step_avg:61.05ms
step:2024/2285 train_time:123570ms step_avg:61.05ms
step:2025/2285 train_time:123632ms step_avg:61.05ms
step:2026/2285 train_time:123692ms step_avg:61.05ms
step:2027/2285 train_time:123754ms step_avg:61.05ms
step:2028/2285 train_time:123814ms step_avg:61.05ms
step:2029/2285 train_time:123877ms step_avg:61.05ms
step:2030/2285 train_time:123936ms step_avg:61.05ms
step:2031/2285 train_time:123999ms step_avg:61.05ms
step:2032/2285 train_time:124059ms step_avg:61.05ms
step:2033/2285 train_time:124123ms step_avg:61.05ms
step:2034/2285 train_time:124184ms step_avg:61.05ms
step:2035/2285 train_time:124248ms step_avg:61.06ms
step:2036/2285 train_time:124309ms step_avg:61.06ms
step:2037/2285 train_time:124371ms step_avg:61.06ms
step:2038/2285 train_time:124432ms step_avg:61.06ms
step:2039/2285 train_time:124495ms step_avg:61.06ms
step:2040/2285 train_time:124556ms step_avg:61.06ms
step:2041/2285 train_time:124619ms step_avg:61.06ms
step:2042/2285 train_time:124679ms step_avg:61.06ms
step:2043/2285 train_time:124742ms step_avg:61.06ms
step:2044/2285 train_time:124803ms step_avg:61.06ms
step:2045/2285 train_time:124866ms step_avg:61.06ms
step:2046/2285 train_time:124927ms step_avg:61.06ms
step:2047/2285 train_time:124989ms step_avg:61.06ms
step:2048/2285 train_time:125050ms step_avg:61.06ms
step:2049/2285 train_time:125112ms step_avg:61.06ms
step:2050/2285 train_time:125174ms step_avg:61.06ms
step:2051/2285 train_time:125236ms step_avg:61.06ms
step:2052/2285 train_time:125297ms step_avg:61.06ms
step:2053/2285 train_time:125361ms step_avg:61.06ms
step:2054/2285 train_time:125421ms step_avg:61.06ms
step:2055/2285 train_time:125484ms step_avg:61.06ms
step:2056/2285 train_time:125545ms step_avg:61.06ms
step:2057/2285 train_time:125608ms step_avg:61.06ms
step:2058/2285 train_time:125668ms step_avg:61.06ms
step:2059/2285 train_time:125731ms step_avg:61.06ms
step:2060/2285 train_time:125791ms step_avg:61.06ms
step:2061/2285 train_time:125853ms step_avg:61.06ms
step:2062/2285 train_time:125913ms step_avg:61.06ms
step:2063/2285 train_time:125976ms step_avg:61.06ms
step:2064/2285 train_time:126037ms step_avg:61.06ms
step:2065/2285 train_time:126100ms step_avg:61.07ms
step:2066/2285 train_time:126162ms step_avg:61.07ms
step:2067/2285 train_time:126225ms step_avg:61.07ms
step:2068/2285 train_time:126286ms step_avg:61.07ms
step:2069/2285 train_time:126349ms step_avg:61.07ms
step:2070/2285 train_time:126409ms step_avg:61.07ms
step:2071/2285 train_time:126472ms step_avg:61.07ms
step:2072/2285 train_time:126533ms step_avg:61.07ms
step:2073/2285 train_time:126595ms step_avg:61.07ms
step:2074/2285 train_time:126655ms step_avg:61.07ms
step:2075/2285 train_time:126718ms step_avg:61.07ms
step:2076/2285 train_time:126779ms step_avg:61.07ms
step:2077/2285 train_time:126842ms step_avg:61.07ms
step:2078/2285 train_time:126903ms step_avg:61.07ms
step:2079/2285 train_time:126965ms step_avg:61.07ms
step:2080/2285 train_time:127026ms step_avg:61.07ms
step:2081/2285 train_time:127089ms step_avg:61.07ms
step:2082/2285 train_time:127149ms step_avg:61.07ms
step:2083/2285 train_time:127213ms step_avg:61.07ms
step:2084/2285 train_time:127273ms step_avg:61.07ms
step:2085/2285 train_time:127335ms step_avg:61.07ms
step:2086/2285 train_time:127395ms step_avg:61.07ms
step:2087/2285 train_time:127459ms step_avg:61.07ms
step:2088/2285 train_time:127519ms step_avg:61.07ms
step:2089/2285 train_time:127583ms step_avg:61.07ms
step:2090/2285 train_time:127644ms step_avg:61.07ms
step:2091/2285 train_time:127707ms step_avg:61.07ms
step:2092/2285 train_time:127767ms step_avg:61.07ms
step:2093/2285 train_time:127829ms step_avg:61.07ms
step:2094/2285 train_time:127889ms step_avg:61.07ms
step:2095/2285 train_time:127952ms step_avg:61.08ms
step:2096/2285 train_time:128013ms step_avg:61.08ms
step:2097/2285 train_time:128076ms step_avg:61.08ms
step:2098/2285 train_time:128136ms step_avg:61.08ms
step:2099/2285 train_time:128199ms step_avg:61.08ms
step:2100/2285 train_time:128260ms step_avg:61.08ms
step:2101/2285 train_time:128324ms step_avg:61.08ms
step:2102/2285 train_time:128385ms step_avg:61.08ms
step:2103/2285 train_time:128448ms step_avg:61.08ms
step:2104/2285 train_time:128508ms step_avg:61.08ms
step:2105/2285 train_time:128571ms step_avg:61.08ms
step:2106/2285 train_time:128632ms step_avg:61.08ms
step:2107/2285 train_time:128695ms step_avg:61.08ms
step:2108/2285 train_time:128755ms step_avg:61.08ms
step:2109/2285 train_time:128818ms step_avg:61.08ms
step:2110/2285 train_time:128879ms step_avg:61.08ms
step:2111/2285 train_time:128943ms step_avg:61.08ms
step:2112/2285 train_time:129004ms step_avg:61.08ms
step:2113/2285 train_time:129067ms step_avg:61.08ms
step:2114/2285 train_time:129127ms step_avg:61.08ms
step:2115/2285 train_time:129190ms step_avg:61.08ms
step:2116/2285 train_time:129251ms step_avg:61.08ms
step:2117/2285 train_time:129314ms step_avg:61.08ms
step:2118/2285 train_time:129374ms step_avg:61.08ms
step:2119/2285 train_time:129438ms step_avg:61.08ms
step:2120/2285 train_time:129498ms step_avg:61.08ms
step:2121/2285 train_time:129561ms step_avg:61.08ms
step:2122/2285 train_time:129622ms step_avg:61.08ms
step:2123/2285 train_time:129685ms step_avg:61.09ms
step:2124/2285 train_time:129747ms step_avg:61.09ms
step:2125/2285 train_time:129810ms step_avg:61.09ms
step:2126/2285 train_time:129870ms step_avg:61.09ms
step:2127/2285 train_time:129933ms step_avg:61.09ms
step:2128/2285 train_time:129994ms step_avg:61.09ms
step:2129/2285 train_time:130057ms step_avg:61.09ms
step:2130/2285 train_time:130117ms step_avg:61.09ms
step:2131/2285 train_time:130180ms step_avg:61.09ms
step:2132/2285 train_time:130241ms step_avg:61.09ms
step:2133/2285 train_time:130305ms step_avg:61.09ms
step:2134/2285 train_time:130366ms step_avg:61.09ms
step:2135/2285 train_time:130428ms step_avg:61.09ms
step:2136/2285 train_time:130488ms step_avg:61.09ms
step:2137/2285 train_time:130551ms step_avg:61.09ms
step:2138/2285 train_time:130611ms step_avg:61.09ms
step:2139/2285 train_time:130674ms step_avg:61.09ms
step:2140/2285 train_time:130734ms step_avg:61.09ms
step:2141/2285 train_time:130796ms step_avg:61.09ms
step:2142/2285 train_time:130856ms step_avg:61.09ms
step:2143/2285 train_time:130920ms step_avg:61.09ms
step:2144/2285 train_time:130980ms step_avg:61.09ms
step:2145/2285 train_time:131043ms step_avg:61.09ms
step:2146/2285 train_time:131104ms step_avg:61.09ms
step:2147/2285 train_time:131168ms step_avg:61.09ms
step:2148/2285 train_time:131228ms step_avg:61.09ms
step:2149/2285 train_time:131292ms step_avg:61.09ms
step:2150/2285 train_time:131352ms step_avg:61.09ms
step:2151/2285 train_time:131415ms step_avg:61.10ms
step:2152/2285 train_time:131475ms step_avg:61.09ms
step:2153/2285 train_time:131538ms step_avg:61.10ms
step:2154/2285 train_time:131598ms step_avg:61.09ms
step:2155/2285 train_time:131662ms step_avg:61.10ms
step:2156/2285 train_time:131723ms step_avg:61.10ms
step:2157/2285 train_time:131785ms step_avg:61.10ms
step:2158/2285 train_time:131846ms step_avg:61.10ms
step:2159/2285 train_time:131910ms step_avg:61.10ms
step:2160/2285 train_time:131970ms step_avg:61.10ms
step:2161/2285 train_time:132032ms step_avg:61.10ms
step:2162/2285 train_time:132092ms step_avg:61.10ms
step:2163/2285 train_time:132156ms step_avg:61.10ms
step:2164/2285 train_time:132216ms step_avg:61.10ms
step:2165/2285 train_time:132279ms step_avg:61.10ms
step:2166/2285 train_time:132341ms step_avg:61.10ms
step:2167/2285 train_time:132404ms step_avg:61.10ms
step:2168/2285 train_time:132464ms step_avg:61.10ms
step:2169/2285 train_time:132527ms step_avg:61.10ms
step:2170/2285 train_time:132588ms step_avg:61.10ms
step:2171/2285 train_time:132651ms step_avg:61.10ms
step:2172/2285 train_time:132711ms step_avg:61.10ms
step:2173/2285 train_time:132774ms step_avg:61.10ms
step:2174/2285 train_time:132835ms step_avg:61.10ms
step:2175/2285 train_time:132898ms step_avg:61.10ms
step:2176/2285 train_time:132959ms step_avg:61.10ms
step:2177/2285 train_time:133022ms step_avg:61.10ms
step:2178/2285 train_time:133083ms step_avg:61.10ms
step:2179/2285 train_time:133146ms step_avg:61.10ms
step:2180/2285 train_time:133207ms step_avg:61.10ms
step:2181/2285 train_time:133271ms step_avg:61.11ms
step:2182/2285 train_time:133331ms step_avg:61.10ms
step:2183/2285 train_time:133394ms step_avg:61.11ms
step:2184/2285 train_time:133454ms step_avg:61.11ms
step:2185/2285 train_time:133517ms step_avg:61.11ms
step:2186/2285 train_time:133577ms step_avg:61.11ms
step:2187/2285 train_time:133640ms step_avg:61.11ms
step:2188/2285 train_time:133701ms step_avg:61.11ms
step:2189/2285 train_time:133765ms step_avg:61.11ms
step:2190/2285 train_time:133825ms step_avg:61.11ms
step:2191/2285 train_time:133888ms step_avg:61.11ms
step:2192/2285 train_time:133948ms step_avg:61.11ms
step:2193/2285 train_time:134011ms step_avg:61.11ms
step:2194/2285 train_time:134072ms step_avg:61.11ms
step:2195/2285 train_time:134134ms step_avg:61.11ms
step:2196/2285 train_time:134195ms step_avg:61.11ms
step:2197/2285 train_time:134258ms step_avg:61.11ms
step:2198/2285 train_time:134318ms step_avg:61.11ms
step:2199/2285 train_time:134382ms step_avg:61.11ms
step:2200/2285 train_time:134443ms step_avg:61.11ms
step:2201/2285 train_time:134506ms step_avg:61.11ms
step:2202/2285 train_time:134567ms step_avg:61.11ms
step:2203/2285 train_time:134630ms step_avg:61.11ms
step:2204/2285 train_time:134691ms step_avg:61.11ms
step:2205/2285 train_time:134754ms step_avg:61.11ms
step:2206/2285 train_time:134814ms step_avg:61.11ms
step:2207/2285 train_time:134877ms step_avg:61.11ms
step:2208/2285 train_time:134937ms step_avg:61.11ms
step:2209/2285 train_time:135000ms step_avg:61.11ms
step:2210/2285 train_time:135061ms step_avg:61.11ms
step:2211/2285 train_time:135125ms step_avg:61.11ms
step:2212/2285 train_time:135186ms step_avg:61.11ms
step:2213/2285 train_time:135249ms step_avg:61.12ms
step:2214/2285 train_time:135309ms step_avg:61.12ms
step:2215/2285 train_time:135372ms step_avg:61.12ms
step:2216/2285 train_time:135433ms step_avg:61.12ms
step:2217/2285 train_time:135496ms step_avg:61.12ms
step:2218/2285 train_time:135556ms step_avg:61.12ms
step:2219/2285 train_time:135619ms step_avg:61.12ms
step:2220/2285 train_time:135680ms step_avg:61.12ms
step:2221/2285 train_time:135743ms step_avg:61.12ms
step:2222/2285 train_time:135804ms step_avg:61.12ms
step:2223/2285 train_time:135867ms step_avg:61.12ms
step:2224/2285 train_time:135927ms step_avg:61.12ms
step:2225/2285 train_time:135990ms step_avg:61.12ms
step:2226/2285 train_time:136050ms step_avg:61.12ms
step:2227/2285 train_time:136113ms step_avg:61.12ms
step:2228/2285 train_time:136174ms step_avg:61.12ms
step:2229/2285 train_time:136236ms step_avg:61.12ms
step:2230/2285 train_time:136297ms step_avg:61.12ms
step:2231/2285 train_time:136360ms step_avg:61.12ms
step:2232/2285 train_time:136420ms step_avg:61.12ms
step:2233/2285 train_time:136483ms step_avg:61.12ms
step:2234/2285 train_time:136544ms step_avg:61.12ms
step:2235/2285 train_time:136608ms step_avg:61.12ms
step:2236/2285 train_time:136668ms step_avg:61.12ms
step:2237/2285 train_time:136731ms step_avg:61.12ms
step:2238/2285 train_time:136791ms step_avg:61.12ms
step:2239/2285 train_time:136854ms step_avg:61.12ms
step:2240/2285 train_time:136914ms step_avg:61.12ms
step:2241/2285 train_time:136977ms step_avg:61.12ms
step:2242/2285 train_time:137037ms step_avg:61.12ms
step:2243/2285 train_time:137099ms step_avg:61.12ms
step:2244/2285 train_time:137160ms step_avg:61.12ms
step:2245/2285 train_time:137223ms step_avg:61.12ms
step:2246/2285 train_time:137285ms step_avg:61.12ms
step:2247/2285 train_time:137349ms step_avg:61.13ms
step:2248/2285 train_time:137409ms step_avg:61.12ms
step:2249/2285 train_time:137472ms step_avg:61.13ms
step:2250/2285 train_time:137532ms step_avg:61.13ms
step:2250/2285 val_loss:3.2841 train_time:137596ms step_avg:61.15ms
step:2251/2285 train_time:137615ms step_avg:61.14ms
step:2252/2285 train_time:137658ms step_avg:61.13ms
step:2253/2285 train_time:137721ms step_avg:61.13ms
step:2254/2285 train_time:137781ms step_avg:61.13ms
step:2255/2285 train_time:137844ms step_avg:61.13ms
step:2256/2285 train_time:137905ms step_avg:61.13ms
step:2257/2285 train_time:137967ms step_avg:61.13ms
step:2258/2285 train_time:138026ms step_avg:61.13ms
step:2259/2285 train_time:138088ms step_avg:61.13ms
step:2260/2285 train_time:138148ms step_avg:61.13ms
step:2261/2285 train_time:138210ms step_avg:61.13ms
step:2262/2285 train_time:138269ms step_avg:61.13ms
step:2263/2285 train_time:138331ms step_avg:61.13ms
step:2264/2285 train_time:138391ms step_avg:61.13ms
step:2265/2285 train_time:138454ms step_avg:61.13ms
step:2266/2285 train_time:138520ms step_avg:61.13ms
step:2267/2285 train_time:138588ms step_avg:61.13ms
step:2268/2285 train_time:138650ms step_avg:61.13ms
step:2269/2285 train_time:138713ms step_avg:61.13ms
step:2270/2285 train_time:138774ms step_avg:61.13ms
step:2271/2285 train_time:138837ms step_avg:61.13ms
step:2272/2285 train_time:138898ms step_avg:61.13ms
step:2273/2285 train_time:138960ms step_avg:61.13ms
step:2274/2285 train_time:139020ms step_avg:61.13ms
step:2275/2285 train_time:139081ms step_avg:61.13ms
step:2276/2285 train_time:139141ms step_avg:61.13ms
step:2277/2285 train_time:139203ms step_avg:61.13ms
step:2278/2285 train_time:139263ms step_avg:61.13ms
step:2279/2285 train_time:139325ms step_avg:61.13ms
step:2280/2285 train_time:139385ms step_avg:61.13ms
step:2281/2285 train_time:139449ms step_avg:61.14ms
step:2282/2285 train_time:139510ms step_avg:61.14ms
step:2283/2285 train_time:139575ms step_avg:61.14ms
step:2284/2285 train_time:139636ms step_avg:61.14ms
step:2285/2285 train_time:139699ms step_avg:61.14ms
step:2285/2285 val_loss:3.2780 train_time:139761ms step_avg:61.16ms
peak memory allocated: 29248 MiB reserved: 50528 MiB

import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from collections import defaultdict
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class NorMuon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).

    Differences from standard Muon:
    - Newton-Shulz is replaced with Polar Express for the orthogonalization step
    - NorMuon adds a low-rank variance estimator similar to Adafactor.
    - small 1D parameters handled via magnitude normalization of the grad (faster execution than Adam)
    - Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as
    needed on the forward pass. This enables attn and mlp weights to be contained within the same
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn.
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, beta2=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, beta2=beta2)
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def reset(self):
        # expose a reset for clearing buffers
        for group in self.param_groups:
            group["momentum_buffer"].zero_()
            group["second_momentum_buffer"].zero_()

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per module.
        """
        groups = defaultdict(list)
        for param in params:
            groups[param.label].append(param)

        param_groups = []
        for module_name, group_params in groups.items():
            chunk_size = (len(group_params) + self.world_size - 1) // self.world_size
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))

        return param_groups

    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn
        and mlp params when a param group is split across GPUs.
        """
        module_group_order = ['smear_gate', 'attn_gate', 'attn', 'mlp']
        params_list = list(params)
        params_list.sort(key=lambda x: module_group_order.index(x.label))

        idx = 0
        group_sizes = [1, 10, 16, 16]
        assert len(params_list) == sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            chunk_size = (size + self.world_size - 1) // self.world_size
            group_params = params_list[idx: idx + size]
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))
            idx += size

        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, @vagrawal, and @varunneal.
        rank = dist.get_rank()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            stacked_grads = torch.empty(
                (padded_num_params, *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device
            )
            for i, p in enumerate(params):
                stacked_grads[i].copy_(p.grad, non_blocking=True)
            if len(params) < padded_num_params:
                stacked_grads[len(params):].zero_()

            grad_chunk = torch.empty_like(stacked_grads[:chunk_size])

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(dict(grad_chunk=grad_chunk, reduce_future=reduce_future))

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = group["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            start_idx = rank * chunk_size
            module_idx = start_idx if start_idx < len(params) else 0

            num_params = min(chunk_size, max(0, len(params) - start_idx))  # num params for this rank

            if "momentum_buffer" not in group:
                group["momentum_buffer"]  = torch.zeros_like(grad_chunk[:num_params])
            momentum_buffer = group["momentum_buffer"]
            # Apply momentum update to the persistent momentum buffer in-place
            momentum_buffer.lerp_(grad_chunk[:num_params], 1 - group["momentum"])
            updated_grads = grad_chunk[:num_params].lerp_(momentum_buffer, group["momentum"])

            grad_shape = updated_grads.shape
            if params[module_idx].label == 'attn':
                # Reshape attn params from [hdim, dim*4] to [4,hdim,dim]
                for p in params[module_idx:module_idx + num_params]:
                    assert p.label == 'attn'
                updated_grads = updated_grads.view(4 * grad_shape[0], grad_shape[1], grad_shape[2] // 4)
            ref_param = params[module_idx]
            param_shape = ref_param.shape

            if "second_momentum_buffer" not in group:
                group["second_momentum_buffer"] = (torch.zeros_like(updated_grads[..., :, :1])
                    if param_shape[-2] >= param_shape[-1] else torch.zeros_like(updated_grads[..., :1, :])
                )
            second_momentum_buffer = group["second_momentum_buffer"]

            if "param_lr" not in group:
                group["param_lr"] = (
                    max(1., param_shape[-2] / param_shape[-1]) ** 0.5
                    * ref_param.new_tensor(
                        [getattr(param, "lr_mul", 1.0) for param in params[module_idx:module_idx + num_params]]
                    ).view(-1, 1, 1)
                )

                group["param_wd"] = ref_param.new_tensor(
                    [getattr(param, "wd_mul", 1.0) for param in params[module_idx:module_idx + num_params]]
                ).view(-1, 1, 1)

            # Determine LR and WR
            eff_lr = group["lr"] * group["param_lr"]
            eff_wd = group["weight_decay"] * group["param_wd"]

            # Compute zeropower for the entire chunk in a single, batched call.
            if num_params == 0:
                v_chunk = updated_grads
            else:
                v_chunk = polar_express(updated_grads)

            # NorMuon: second_momentum_buffer tracks squared magnitude of gradients along one dim (https://arxiv.org/pdf/2510.05491)
            v_norm = v_chunk.norm(dim=(-2, -1), keepdim=True)
            v_mean = v_chunk.square().mean(dim=-1 if param_shape[-2] >= param_shape[-1] else -2, keepdim=True)
            second_momentum_buffer.lerp_(v_mean.to(dtype=ref_param.dtype), 1 - group["beta2"])
            step_size = second_momentum_buffer.clamp_min(1e-10).rsqrt_()
            v_chunk.mul_(step_size)
            v_norm_new = v_chunk.norm(dim=(-2, -1), keepdim=True)
            v_chunk.mul_(v_norm / v_norm_new.clamp_min_(1e-10))

            v_chunk = v_chunk.view(grad_shape)

            updated_params = torch.empty_like(grad_chunk)
            param_chunk = torch.stack(params[module_idx:module_idx + num_params]) if num_params > 0 else torch.zeros_like(v_chunk)
            # Apply weight decay directly to the buffer.
            param_chunk.mul_(1 - eff_wd)

            param_chunk.add_(-eff_lr * v_chunk)

            updated_params[:num_params].copy_(param_chunk)
            if num_params < chunk_size:
                updated_params[num_params:].zero_()

            stacked_params = torch.empty(
                (padded_num_params, *param_shape),
                dtype=updated_params.dtype,
                device=updated_params.device,
            )

            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_params, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # init state
        for p in params:
            chunk_size = p.size(0) // self.world_size
            exp_avg = torch.zeros_like(p[:chunk_size], dtype=torch.bfloat16, device=p[0].device)
            exp_avg_sq = torch.zeros_like(exp_avg)
            self.state[p] = dict(step=0, exp_avg=exp_avg, exp_avg_sq=exp_avg_sq)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        reduce_scatter_futures: list[torch.Future] = []
        all_gather_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                grad = param.grad
                rank_size = grad.shape[0] // self.world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for param in params:
                reduce_scatter_futures[idx].wait()
                rank_size = param.shape[0] // self.world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]
                g_slice = grad_slices[idx]

                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (bias2 ** 0.5 / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        with torch.no_grad():
            self.weight.zero_()  # @Grad62304977 and others

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.label='attn'

        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.label = 'attn_gate'

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4, self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens,
                                                        max_seqlen_q=max_len, max_seqlen_k=max_len,
                                                        causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4, self.hdim, self.dim)[3].type_as(y))
        return y


class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.label = 'mlp'
        self.c_proj.label = 'mlp'
        # corrective factor to account for transpose
        self.c_fc.lr_mul = 2.

        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.label = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        smear_lambda = self.scalars[5 * len(self.blocks)]
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        # smear token embed forward 1 position @classiclarryd
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        n = len(self.blocks) // 2

        x_backout = None
        backout_layer = 8
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            # since layer 0 is skipped, layer 11 does not have skip_connection
            if i >= n and i<11:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()

    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 16 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2245  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: float = 0.50  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_final: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizer2 = NorMuon(hidden_matrix_params + gate_params, lr=0.03, momentum=0.95, beta2=0.95, weight_decay=0.0)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: flat, then linear decay, then flat
def get_lr(step: int):
    x = min(0.9999, step / args.num_scheduled_iterations)
    assert 0 <= x < 1
    lr = 1.0
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = w * 1.0 + (1 - w) * 0.1
    return lr

def get_ws(step: int):
    # set short window size to half of long window size
    # Higher ws on "extension" steps
    if step >= args.num_scheduled_iterations:
        return args.ws_final // 2, args.ws_final
    x = step / args.num_scheduled_iterations
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
ws_schedule = list(args.ws_schedule) + [args.ws_final]
ws_long = ws_schedule[0]
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    # each window size is a new graph, need to warm up each with Yarn.attn_scale
    ws_idx = step % len(ws_schedule)
    if ws_idx==0:
        model.yarn.reset()
        ws_long = ws_schedule[0]
    else:
        new_ws_long = ws_schedule[ws_idx]
        model.yarn.apply(ws_long, new_ws_long)
        ws_long = new_ws_long
    model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset() # rotary buffer is not stored in state_dict
model.load_state_dict(initial_state["model"])
optimizer2.reset() # muon momentum buffers not in state dict
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for _ in range(grad_accum_steps):
        inputs, targets, cum_seqlens = next(train_loader)
        model(inputs, targets, cum_seqlens, ws_short, ws_long).backward()
    step_optimizers(step, optimizers, model)

    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.10.12 (main, Aug 15 2025, 14:32:43) [GCC 11.4.0]
Running PyTorch 2.10.0.dev20250926+cu126 compiled for CUDA 12.6
Running Triton version 3.5.0
Thu Nov  6 05:51:20 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:19:00.0 Off |                    0 |
| N/A   39C    P0            126W /  700W |    5858MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:3B:00.0 Off |                    0 |
| N/A   32C    P0            125W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:4C:00.0 Off |                    0 |
| N/A   32C    P0            119W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:5D:00.0 Off |                    0 |
| N/A   39C    P0            130W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:9B:00.0 Off |                    0 |
| N/A   36C    P0            122W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:BB:00.0 Off |                    0 |
| N/A   30C    P0            119W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:CB:00.0 Off |                    0 |
| N/A   37C    P0            124W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:DB:00.0 Off |                    0 |
| N/A   30C    P0            118W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A    108297      C   /root/.venv/bin/python3                         0MiB |
|    0   N/A  N/A    108298      C   /root/.venv/bin/python3                         0MiB |
|    0   N/A  N/A    108299      C   /root/.venv/bin/python3                         0MiB |
|    0   N/A  N/A    108300      C   /root/.venv/bin/python3                         0MiB |
|    0   N/A  N/A    108301      C   /root/.venv/bin/python3                         0MiB |
|    0   N/A  N/A    108302      C   /root/.venv/bin/python3                         0MiB |
|    0   N/A  N/A    108303      C   /root/.venv/bin/python3                         0MiB |
|    0   N/A  N/A    108304      C   /root/.venv/bin/python3                         0MiB |
|    1   N/A  N/A    108298      C   /root/.venv/bin/python3                         0MiB |
|    2   N/A  N/A    108299      C   /root/.venv/bin/python3                         0MiB |
|    3   N/A  N/A    108300      C   /root/.venv/bin/python3                         0MiB |
|    4   N/A  N/A    108301      C   /root/.venv/bin/python3                         0MiB |
|    5   N/A  N/A    108302      C   /root/.venv/bin/python3                         0MiB |
|    6   N/A  N/A    108303      C   /root/.venv/bin/python3                         0MiB |
|    7   N/A  N/A    108304      C   /root/.venv/bin/python3                         0MiB |
+-----------------------------------------------------------------------------------------+

====================================================================================================
step:0/2285 val_loss:10.8258 train_time:0ms step_avg:0.02ms
step:1/2285 train_time:122ms step_avg:121.76ms
step:2/2285 train_time:142ms step_avg:71.08ms
step:3/2285 train_time:181ms step_avg:60.40ms
step:4/2285 train_time:238ms step_avg:59.41ms
step:5/2285 train_time:297ms step_avg:59.44ms
step:6/2285 train_time:356ms step_avg:59.34ms
step:7/2285 train_time:417ms step_avg:59.56ms
step:8/2285 train_time:476ms step_avg:59.47ms
step:9/2285 train_time:536ms step_avg:59.60ms
step:10/2285 train_time:595ms step_avg:59.54ms
step:11/2285 train_time:656ms step_avg:59.67ms
step:12/2285 train_time:715ms step_avg:59.60ms
step:13/2285 train_time:776ms step_avg:59.72ms
step:14/2285 train_time:835ms step_avg:59.65ms
step:15/2285 train_time:896ms step_avg:59.73ms
step:16/2285 train_time:955ms step_avg:59.66ms
step:17/2285 train_time:1019ms step_avg:59.95ms
step:18/2285 train_time:1081ms step_avg:60.06ms
step:19/2285 train_time:1146ms step_avg:60.32ms
step:20/2285 train_time:1207ms step_avg:60.37ms
step:21/2285 train_time:1270ms step_avg:60.48ms
step:22/2285 train_time:1329ms step_avg:60.43ms
step:23/2285 train_time:1392ms step_avg:60.51ms
step:24/2285 train_time:1452ms step_avg:60.49ms
step:25/2285 train_time:1514ms step_avg:60.55ms
step:26/2285 train_time:1573ms step_avg:60.50ms
step:27/2285 train_time:1634ms step_avg:60.53ms
step:28/2285 train_time:1693ms step_avg:60.46ms
step:29/2285 train_time:1754ms step_avg:60.49ms
step:30/2285 train_time:1814ms step_avg:60.46ms
step:31/2285 train_time:1875ms step_avg:60.47ms
step:32/2285 train_time:1933ms step_avg:60.42ms
step:33/2285 train_time:1996ms step_avg:60.47ms
step:34/2285 train_time:2057ms step_avg:60.49ms
step:35/2285 train_time:2121ms step_avg:60.60ms
step:36/2285 train_time:2181ms step_avg:60.60ms
step:37/2285 train_time:2243ms step_avg:60.63ms
step:38/2285 train_time:2303ms step_avg:60.61ms
step:39/2285 train_time:2367ms step_avg:60.69ms
step:40/2285 train_time:2426ms step_avg:60.66ms
step:41/2285 train_time:2488ms step_avg:60.69ms
step:42/2285 train_time:2548ms step_avg:60.66ms
step:43/2285 train_time:2609ms step_avg:60.68ms
step:44/2285 train_time:2668ms step_avg:60.65ms
step:45/2285 train_time:2731ms step_avg:60.68ms
step:46/2285 train_time:2790ms step_avg:60.65ms
step:47/2285 train_time:2851ms step_avg:60.67ms
step:48/2285 train_time:2910ms step_avg:60.63ms
step:49/2285 train_time:2974ms step_avg:60.69ms
step:50/2285 train_time:3034ms step_avg:60.67ms
step:51/2285 train_time:3097ms step_avg:60.73ms
step:52/2285 train_time:3157ms step_avg:60.71ms
step:53/2285 train_time:3219ms step_avg:60.74ms
step:54/2285 train_time:3278ms step_avg:60.71ms
step:55/2285 train_time:3340ms step_avg:60.73ms
step:56/2285 train_time:3399ms step_avg:60.70ms
step:57/2285 train_time:3462ms step_avg:60.74ms
step:58/2285 train_time:3522ms step_avg:60.72ms
step:59/2285 train_time:3584ms step_avg:60.75ms
step:60/2285 train_time:3644ms step_avg:60.73ms
step:61/2285 train_time:3705ms step_avg:60.75ms
step:62/2285 train_time:3765ms step_avg:60.73ms
step:63/2285 train_time:3827ms step_avg:60.75ms
step:64/2285 train_time:3886ms step_avg:60.72ms
step:65/2285 train_time:3948ms step_avg:60.74ms
step:66/2285 train_time:4008ms step_avg:60.73ms
step:67/2285 train_time:4071ms step_avg:60.77ms
step:68/2285 train_time:4131ms step_avg:60.76ms
step:69/2285 train_time:4194ms step_avg:60.79ms
step:70/2285 train_time:4255ms step_avg:60.78ms
step:71/2285 train_time:4316ms step_avg:60.79ms
step:72/2285 train_time:4375ms step_avg:60.76ms
step:73/2285 train_time:4437ms step_avg:60.78ms
step:74/2285 train_time:4496ms step_avg:60.75ms
step:75/2285 train_time:4557ms step_avg:60.77ms
step:76/2285 train_time:4617ms step_avg:60.75ms
step:77/2285 train_time:4679ms step_avg:60.76ms
step:78/2285 train_time:4738ms step_avg:60.74ms
step:79/2285 train_time:4800ms step_avg:60.76ms
step:80/2285 train_time:4859ms step_avg:60.74ms
step:81/2285 train_time:4921ms step_avg:60.76ms
step:82/2285 train_time:4981ms step_avg:60.75ms
step:83/2285 train_time:5044ms step_avg:60.77ms
step:84/2285 train_time:5103ms step_avg:60.75ms
step:85/2285 train_time:5166ms step_avg:60.77ms
step:86/2285 train_time:5225ms step_avg:60.76ms
step:87/2285 train_time:5287ms step_avg:60.77ms
step:88/2285 train_time:5346ms step_avg:60.76ms
step:89/2285 train_time:5408ms step_avg:60.76ms
step:90/2285 train_time:5467ms step_avg:60.74ms
step:91/2285 train_time:5529ms step_avg:60.75ms
step:92/2285 train_time:5588ms step_avg:60.74ms
step:93/2285 train_time:5649ms step_avg:60.74ms
step:94/2285 train_time:5708ms step_avg:60.73ms
step:95/2285 train_time:5770ms step_avg:60.73ms
step:96/2285 train_time:5828ms step_avg:60.71ms
step:97/2285 train_time:5891ms step_avg:60.73ms
step:98/2285 train_time:5951ms step_avg:60.72ms
step:99/2285 train_time:6013ms step_avg:60.74ms
step:100/2285 train_time:6072ms step_avg:60.72ms
step:101/2285 train_time:6134ms step_avg:60.73ms
step:102/2285 train_time:6194ms step_avg:60.73ms
step:103/2285 train_time:6256ms step_avg:60.74ms
step:104/2285 train_time:6315ms step_avg:60.72ms
step:105/2285 train_time:6376ms step_avg:60.73ms
step:106/2285 train_time:6435ms step_avg:60.71ms
step:107/2285 train_time:6497ms step_avg:60.72ms
step:108/2285 train_time:6555ms step_avg:60.70ms
step:109/2285 train_time:6617ms step_avg:60.71ms
step:110/2285 train_time:6676ms step_avg:60.69ms
step:111/2285 train_time:6738ms step_avg:60.70ms
step:112/2285 train_time:6796ms step_avg:60.68ms
step:113/2285 train_time:6858ms step_avg:60.69ms
step:114/2285 train_time:6918ms step_avg:60.68ms
step:115/2285 train_time:6980ms step_avg:60.70ms
step:116/2285 train_time:7040ms step_avg:60.69ms
step:117/2285 train_time:7103ms step_avg:60.71ms
step:118/2285 train_time:7162ms step_avg:60.70ms
step:119/2285 train_time:7225ms step_avg:60.71ms
step:120/2285 train_time:7284ms step_avg:60.70ms
step:121/2285 train_time:7346ms step_avg:60.71ms
step:122/2285 train_time:7404ms step_avg:60.69ms
step:123/2285 train_time:7466ms step_avg:60.70ms
step:124/2285 train_time:7525ms step_avg:60.68ms
step:125/2285 train_time:7587ms step_avg:60.69ms
step:126/2285 train_time:7646ms step_avg:60.68ms
step:127/2285 train_time:7708ms step_avg:60.69ms
step:128/2285 train_time:7767ms step_avg:60.68ms
step:129/2285 train_time:7829ms step_avg:60.69ms
step:130/2285 train_time:7888ms step_avg:60.68ms
step:131/2285 train_time:7950ms step_avg:60.69ms
step:132/2285 train_time:8009ms step_avg:60.68ms
step:133/2285 train_time:8071ms step_avg:60.68ms
step:134/2285 train_time:8130ms step_avg:60.67ms
step:135/2285 train_time:8192ms step_avg:60.68ms
step:136/2285 train_time:8251ms step_avg:60.67ms
step:137/2285 train_time:8313ms step_avg:60.68ms
step:138/2285 train_time:8372ms step_avg:60.67ms
step:139/2285 train_time:8434ms step_avg:60.67ms
step:140/2285 train_time:8493ms step_avg:60.66ms
step:141/2285 train_time:8555ms step_avg:60.68ms
step:142/2285 train_time:8614ms step_avg:60.66ms
step:143/2285 train_time:8675ms step_avg:60.67ms
step:144/2285 train_time:8734ms step_avg:60.65ms
step:145/2285 train_time:8795ms step_avg:60.66ms
step:146/2285 train_time:8854ms step_avg:60.64ms
step:147/2285 train_time:8915ms step_avg:60.65ms
step:148/2285 train_time:8974ms step_avg:60.64ms
step:149/2285 train_time:9035ms step_avg:60.64ms
step:150/2285 train_time:9094ms step_avg:60.63ms
step:151/2285 train_time:9156ms step_avg:60.63ms
step:152/2285 train_time:9215ms step_avg:60.63ms
step:153/2285 train_time:9277ms step_avg:60.64ms
step:154/2285 train_time:9336ms step_avg:60.62ms
step:155/2285 train_time:9397ms step_avg:60.62ms
step:156/2285 train_time:9456ms step_avg:60.62ms
step:157/2285 train_time:9518ms step_avg:60.62ms
step:158/2285 train_time:9577ms step_avg:60.61ms
step:159/2285 train_time:9638ms step_avg:60.62ms
step:160/2285 train_time:9697ms step_avg:60.61ms
step:161/2285 train_time:9759ms step_avg:60.62ms
step:162/2285 train_time:9819ms step_avg:60.61ms
step:163/2285 train_time:9880ms step_avg:60.62ms
step:164/2285 train_time:9939ms step_avg:60.61ms
step:165/2285 train_time:10001ms step_avg:60.61ms
step:166/2285 train_time:10061ms step_avg:60.61ms
step:167/2285 train_time:10123ms step_avg:60.62ms
step:168/2285 train_time:10183ms step_avg:60.61ms
step:169/2285 train_time:10245ms step_avg:60.62ms
step:170/2285 train_time:10304ms step_avg:60.61ms
step:171/2285 train_time:10366ms step_avg:60.62ms
step:172/2285 train_time:10427ms step_avg:60.62ms
step:173/2285 train_time:10488ms step_avg:60.63ms
step:174/2285 train_time:10548ms step_avg:60.62ms
step:175/2285 train_time:10610ms step_avg:60.63ms
step:176/2285 train_time:10669ms step_avg:60.62ms
step:177/2285 train_time:10732ms step_avg:60.63ms
step:178/2285 train_time:10791ms step_avg:60.62ms
step:179/2285 train_time:10853ms step_avg:60.63ms
step:180/2285 train_time:10912ms step_avg:60.62ms
step:181/2285 train_time:10974ms step_avg:60.63ms
step:182/2285 train_time:11032ms step_avg:60.62ms
step:183/2285 train_time:11094ms step_avg:60.62ms
step:184/2285 train_time:11153ms step_avg:60.62ms
step:185/2285 train_time:11215ms step_avg:60.62ms
step:186/2285 train_time:11274ms step_avg:60.61ms
step:187/2285 train_time:11336ms step_avg:60.62ms
step:188/2285 train_time:11394ms step_avg:60.61ms
step:189/2285 train_time:11456ms step_avg:60.61ms
step:190/2285 train_time:11514ms step_avg:60.60ms
step:191/2285 train_time:11577ms step_avg:60.61ms
step:192/2285 train_time:11636ms step_avg:60.60ms
step:193/2285 train_time:11698ms step_avg:60.61ms
step:194/2285 train_time:11756ms step_avg:60.60ms
step:195/2285 train_time:11818ms step_avg:60.61ms
step:196/2285 train_time:11877ms step_avg:60.60ms
step:197/2285 train_time:11939ms step_avg:60.60ms
step:198/2285 train_time:11998ms step_avg:60.60ms
step:199/2285 train_time:12060ms step_avg:60.60ms
step:200/2285 train_time:12119ms step_avg:60.60ms
step:201/2285 train_time:12181ms step_avg:60.60ms
step:202/2285 train_time:12240ms step_avg:60.60ms
step:203/2285 train_time:12302ms step_avg:60.60ms
step:204/2285 train_time:12362ms step_avg:60.60ms
step:205/2285 train_time:12424ms step_avg:60.60ms
step:206/2285 train_time:12484ms step_avg:60.60ms
step:207/2285 train_time:12545ms step_avg:60.61ms
step:208/2285 train_time:12605ms step_avg:60.60ms
step:209/2285 train_time:12667ms step_avg:60.61ms
step:210/2285 train_time:12726ms step_avg:60.60ms
step:211/2285 train_time:12787ms step_avg:60.60ms
step:212/2285 train_time:12846ms step_avg:60.59ms
step:213/2285 train_time:12908ms step_avg:60.60ms
step:214/2285 train_time:12967ms step_avg:60.59ms
step:215/2285 train_time:13029ms step_avg:60.60ms
step:216/2285 train_time:13088ms step_avg:60.59ms
step:217/2285 train_time:13150ms step_avg:60.60ms
step:218/2285 train_time:13210ms step_avg:60.59ms
step:219/2285 train_time:13272ms step_avg:60.60ms
step:220/2285 train_time:13331ms step_avg:60.59ms
step:221/2285 train_time:13392ms step_avg:60.60ms
step:222/2285 train_time:13451ms step_avg:60.59ms
step:223/2285 train_time:13513ms step_avg:60.59ms
step:224/2285 train_time:13572ms step_avg:60.59ms
step:225/2285 train_time:13633ms step_avg:60.59ms
step:226/2285 train_time:13692ms step_avg:60.58ms
step:227/2285 train_time:13753ms step_avg:60.58ms
step:228/2285 train_time:13812ms step_avg:60.58ms
step:229/2285 train_time:13873ms step_avg:60.58ms
step:230/2285 train_time:13932ms step_avg:60.57ms
step:231/2285 train_time:13994ms step_avg:60.58ms
step:232/2285 train_time:14054ms step_avg:60.58ms
step:233/2285 train_time:14116ms step_avg:60.58ms
step:234/2285 train_time:14175ms step_avg:60.57ms
step:235/2285 train_time:14236ms step_avg:60.58ms
step:236/2285 train_time:14294ms step_avg:60.57ms
step:237/2285 train_time:14356ms step_avg:60.57ms
step:238/2285 train_time:14415ms step_avg:60.57ms
step:239/2285 train_time:14476ms step_avg:60.57ms
step:240/2285 train_time:14535ms step_avg:60.56ms
step:241/2285 train_time:14596ms step_avg:60.56ms
step:242/2285 train_time:14655ms step_avg:60.56ms
step:243/2285 train_time:14716ms step_avg:60.56ms
step:244/2285 train_time:14775ms step_avg:60.55ms
step:245/2285 train_time:14836ms step_avg:60.55ms
step:246/2285 train_time:14894ms step_avg:60.55ms
step:247/2285 train_time:14957ms step_avg:60.55ms
step:248/2285 train_time:15016ms step_avg:60.55ms
step:249/2285 train_time:15077ms step_avg:60.55ms
step:250/2285 train_time:15137ms step_avg:60.55ms
step:250/2285 val_loss:4.0743 train_time:15199ms step_avg:60.80ms
step:251/2285 train_time:15218ms step_avg:60.63ms
step:252/2285 train_time:15260ms step_avg:60.56ms
step:253/2285 train_time:15325ms step_avg:60.57ms
step:254/2285 train_time:15391ms step_avg:60.60ms
step:255/2285 train_time:15454ms step_avg:60.61ms
step:256/2285 train_time:15514ms step_avg:60.60ms
step:257/2285 train_time:15575ms step_avg:60.60ms
step:258/2285 train_time:15634ms step_avg:60.60ms
step:259/2285 train_time:15695ms step_avg:60.60ms
step:260/2285 train_time:15754ms step_avg:60.59ms
step:261/2285 train_time:15814ms step_avg:60.59ms
step:262/2285 train_time:15873ms step_avg:60.58ms
step:263/2285 train_time:15934ms step_avg:60.58ms
step:264/2285 train_time:15992ms step_avg:60.58ms
step:265/2285 train_time:16053ms step_avg:60.58ms
step:266/2285 train_time:16111ms step_avg:60.57ms
step:267/2285 train_time:16172ms step_avg:60.57ms
step:268/2285 train_time:16232ms step_avg:60.57ms
step:269/2285 train_time:16296ms step_avg:60.58ms
step:270/2285 train_time:16357ms step_avg:60.58ms
step:271/2285 train_time:16419ms step_avg:60.59ms
step:272/2285 train_time:16479ms step_avg:60.58ms
step:273/2285 train_time:16541ms step_avg:60.59ms
step:274/2285 train_time:16601ms step_avg:60.59ms
step:275/2285 train_time:16662ms step_avg:60.59ms
step:276/2285 train_time:16721ms step_avg:60.58ms
step:277/2285 train_time:16782ms step_avg:60.58ms
step:278/2285 train_time:16840ms step_avg:60.58ms
step:279/2285 train_time:16901ms step_avg:60.58ms
step:280/2285 train_time:16960ms step_avg:60.57ms
step:281/2285 train_time:17022ms step_avg:60.58ms
step:282/2285 train_time:17082ms step_avg:60.57ms
step:283/2285 train_time:17143ms step_avg:60.58ms
step:284/2285 train_time:17202ms step_avg:60.57ms
step:285/2285 train_time:17264ms step_avg:60.58ms
step:286/2285 train_time:17323ms step_avg:60.57ms
step:287/2285 train_time:17386ms step_avg:60.58ms
step:288/2285 train_time:17445ms step_avg:60.57ms
step:289/2285 train_time:17506ms step_avg:60.58ms
step:290/2285 train_time:17565ms step_avg:60.57ms
step:291/2285 train_time:17627ms step_avg:60.58ms
step:292/2285 train_time:17686ms step_avg:60.57ms
step:293/2285 train_time:17747ms step_avg:60.57ms
step:294/2285 train_time:17806ms step_avg:60.57ms
step:295/2285 train_time:17867ms step_avg:60.57ms
step:296/2285 train_time:17926ms step_avg:60.56ms
step:297/2285 train_time:17988ms step_avg:60.56ms
step:298/2285 train_time:18047ms step_avg:60.56ms
step:299/2285 train_time:18108ms step_avg:60.56ms
step:300/2285 train_time:18167ms step_avg:60.56ms
step:301/2285 train_time:18228ms step_avg:60.56ms
step:302/2285 train_time:18288ms step_avg:60.56ms
step:303/2285 train_time:18350ms step_avg:60.56ms
step:304/2285 train_time:18409ms step_avg:60.56ms
step:305/2285 train_time:18471ms step_avg:60.56ms
step:306/2285 train_time:18532ms step_avg:60.56ms
step:307/2285 train_time:18594ms step_avg:60.57ms
step:308/2285 train_time:18653ms step_avg:60.56ms
step:309/2285 train_time:18715ms step_avg:60.57ms
step:310/2285 train_time:18774ms step_avg:60.56ms
step:311/2285 train_time:18836ms step_avg:60.56ms
step:312/2285 train_time:18895ms step_avg:60.56ms
step:313/2285 train_time:18956ms step_avg:60.56ms
step:314/2285 train_time:19015ms step_avg:60.56ms
step:315/2285 train_time:19076ms step_avg:60.56ms
step:316/2285 train_time:19135ms step_avg:60.55ms
step:317/2285 train_time:19197ms step_avg:60.56ms
step:318/2285 train_time:19256ms step_avg:60.55ms
step:319/2285 train_time:19318ms step_avg:60.56ms
step:320/2285 train_time:19377ms step_avg:60.55ms
step:321/2285 train_time:19439ms step_avg:60.56ms
step:322/2285 train_time:19498ms step_avg:60.55ms
step:323/2285 train_time:19561ms step_avg:60.56ms
step:324/2285 train_time:19620ms step_avg:60.56ms
step:325/2285 train_time:19681ms step_avg:60.56ms
step:326/2285 train_time:19740ms step_avg:60.55ms
step:327/2285 train_time:19801ms step_avg:60.55ms
step:328/2285 train_time:19860ms step_avg:60.55ms
step:329/2285 train_time:19922ms step_avg:60.55ms
step:330/2285 train_time:19981ms step_avg:60.55ms
step:331/2285 train_time:20042ms step_avg:60.55ms
step:332/2285 train_time:20101ms step_avg:60.54ms
step:333/2285 train_time:20162ms step_avg:60.55ms
step:334/2285 train_time:20221ms step_avg:60.54ms
step:335/2285 train_time:20282ms step_avg:60.54ms
step:336/2285 train_time:20341ms step_avg:60.54ms
step:337/2285 train_time:20402ms step_avg:60.54ms
step:338/2285 train_time:20462ms step_avg:60.54ms
step:339/2285 train_time:20523ms step_avg:60.54ms
step:340/2285 train_time:20582ms step_avg:60.54ms
step:341/2285 train_time:20645ms step_avg:60.54ms
step:342/2285 train_time:20704ms step_avg:60.54ms
step:343/2285 train_time:20765ms step_avg:60.54ms
step:344/2285 train_time:20824ms step_avg:60.53ms
step:345/2285 train_time:20885ms step_avg:60.54ms
step:346/2285 train_time:20944ms step_avg:60.53ms
step:347/2285 train_time:21005ms step_avg:60.53ms
step:348/2285 train_time:21064ms step_avg:60.53ms
step:349/2285 train_time:21125ms step_avg:60.53ms
step:350/2285 train_time:21184ms step_avg:60.53ms
step:351/2285 train_time:21245ms step_avg:60.53ms
step:352/2285 train_time:21304ms step_avg:60.52ms
step:353/2285 train_time:21365ms step_avg:60.52ms
step:354/2285 train_time:21424ms step_avg:60.52ms
step:355/2285 train_time:21486ms step_avg:60.53ms
step:356/2285 train_time:21546ms step_avg:60.52ms
step:357/2285 train_time:21607ms step_avg:60.52ms
step:358/2285 train_time:21666ms step_avg:60.52ms
step:359/2285 train_time:21727ms step_avg:60.52ms
step:360/2285 train_time:21786ms step_avg:60.52ms
step:361/2285 train_time:21848ms step_avg:60.52ms
step:362/2285 train_time:21906ms step_avg:60.52ms
step:363/2285 train_time:21968ms step_avg:60.52ms
step:364/2285 train_time:22027ms step_avg:60.51ms
step:365/2285 train_time:22088ms step_avg:60.52ms
step:366/2285 train_time:22147ms step_avg:60.51ms
step:367/2285 train_time:22209ms step_avg:60.51ms
step:368/2285 train_time:22268ms step_avg:60.51ms
step:369/2285 train_time:22329ms step_avg:60.51ms
step:370/2285 train_time:22388ms step_avg:60.51ms
step:371/2285 train_time:22450ms step_avg:60.51ms
step:372/2285 train_time:22509ms step_avg:60.51ms
step:373/2285 train_time:22571ms step_avg:60.51ms
step:374/2285 train_time:22630ms step_avg:60.51ms
step:375/2285 train_time:22692ms step_avg:60.51ms
step:376/2285 train_time:22751ms step_avg:60.51ms
step:377/2285 train_time:22813ms step_avg:60.51ms
step:378/2285 train_time:22872ms step_avg:60.51ms
step:379/2285 train_time:22934ms step_avg:60.51ms
step:380/2285 train_time:22993ms step_avg:60.51ms
step:381/2285 train_time:23055ms step_avg:60.51ms
step:382/2285 train_time:23114ms step_avg:60.51ms
step:383/2285 train_time:23176ms step_avg:60.51ms
step:384/2285 train_time:23235ms step_avg:60.51ms
step:385/2285 train_time:23297ms step_avg:60.51ms
step:386/2285 train_time:23356ms step_avg:60.51ms
step:387/2285 train_time:23417ms step_avg:60.51ms
step:388/2285 train_time:23476ms step_avg:60.51ms
step:389/2285 train_time:23538ms step_avg:60.51ms
step:390/2285 train_time:23597ms step_avg:60.51ms
step:391/2285 train_time:23659ms step_avg:60.51ms
step:392/2285 train_time:23718ms step_avg:60.51ms
step:393/2285 train_time:23780ms step_avg:60.51ms
step:394/2285 train_time:23839ms step_avg:60.51ms
step:395/2285 train_time:23902ms step_avg:60.51ms
step:396/2285 train_time:23961ms step_avg:60.51ms
step:397/2285 train_time:24023ms step_avg:60.51ms
step:398/2285 train_time:24082ms step_avg:60.51ms
step:399/2285 train_time:24144ms step_avg:60.51ms
step:400/2285 train_time:24203ms step_avg:60.51ms
step:401/2285 train_time:24264ms step_avg:60.51ms
step:402/2285 train_time:24323ms step_avg:60.50ms
step:403/2285 train_time:24384ms step_avg:60.51ms
step:404/2285 train_time:24443ms step_avg:60.50ms
step:405/2285 train_time:24505ms step_avg:60.51ms
step:406/2285 train_time:24564ms step_avg:60.50ms
step:407/2285 train_time:24626ms step_avg:60.51ms
step:408/2285 train_time:24685ms step_avg:60.50ms
step:409/2285 train_time:24746ms step_avg:60.50ms
step:410/2285 train_time:24805ms step_avg:60.50ms
step:411/2285 train_time:24866ms step_avg:60.50ms
step:412/2285 train_time:24925ms step_avg:60.50ms
step:413/2285 train_time:24986ms step_avg:60.50ms
step:414/2285 train_time:25044ms step_avg:60.49ms
step:415/2285 train_time:25106ms step_avg:60.50ms
step:416/2285 train_time:25165ms step_avg:60.49ms
step:417/2285 train_time:25226ms step_avg:60.49ms
step:418/2285 train_time:25285ms step_avg:60.49ms
step:419/2285 train_time:25347ms step_avg:60.49ms
step:420/2285 train_time:25405ms step_avg:60.49ms
step:421/2285 train_time:25466ms step_avg:60.49ms
step:422/2285 train_time:25525ms step_avg:60.49ms
step:423/2285 train_time:25586ms step_avg:60.49ms
step:424/2285 train_time:25646ms step_avg:60.48ms
step:425/2285 train_time:25707ms step_avg:60.49ms
step:426/2285 train_time:25767ms step_avg:60.49ms
step:427/2285 train_time:25828ms step_avg:60.49ms
step:428/2285 train_time:25887ms step_avg:60.48ms
step:429/2285 train_time:25949ms step_avg:60.49ms
step:430/2285 train_time:26008ms step_avg:60.48ms
step:431/2285 train_time:26069ms step_avg:60.49ms
step:432/2285 train_time:26128ms step_avg:60.48ms
step:433/2285 train_time:26190ms step_avg:60.48ms
step:434/2285 train_time:26249ms step_avg:60.48ms
step:435/2285 train_time:26311ms step_avg:60.48ms
step:436/2285 train_time:26370ms step_avg:60.48ms
step:437/2285 train_time:26432ms step_avg:60.49ms
step:438/2285 train_time:26492ms step_avg:60.48ms
step:439/2285 train_time:26553ms step_avg:60.49ms
step:440/2285 train_time:26613ms step_avg:60.48ms
step:441/2285 train_time:26674ms step_avg:60.49ms
step:442/2285 train_time:26733ms step_avg:60.48ms
step:443/2285 train_time:26795ms step_avg:60.49ms
step:444/2285 train_time:26854ms step_avg:60.48ms
step:445/2285 train_time:26916ms step_avg:60.49ms
step:446/2285 train_time:26975ms step_avg:60.48ms
step:447/2285 train_time:27037ms step_avg:60.49ms
step:448/2285 train_time:27096ms step_avg:60.48ms
step:449/2285 train_time:27157ms step_avg:60.48ms
step:450/2285 train_time:27216ms step_avg:60.48ms
step:451/2285 train_time:27278ms step_avg:60.48ms
step:452/2285 train_time:27337ms step_avg:60.48ms
step:453/2285 train_time:27399ms step_avg:60.48ms
step:454/2285 train_time:27458ms step_avg:60.48ms
step:455/2285 train_time:27520ms step_avg:60.48ms
step:456/2285 train_time:27580ms step_avg:60.48ms
step:457/2285 train_time:27642ms step_avg:60.49ms
step:458/2285 train_time:27701ms step_avg:60.48ms
step:459/2285 train_time:27762ms step_avg:60.48ms
step:460/2285 train_time:27821ms step_avg:60.48ms
step:461/2285 train_time:27882ms step_avg:60.48ms
step:462/2285 train_time:27942ms step_avg:60.48ms
step:463/2285 train_time:28003ms step_avg:60.48ms
step:464/2285 train_time:28062ms step_avg:60.48ms
step:465/2285 train_time:28123ms step_avg:60.48ms
step:466/2285 train_time:28183ms step_avg:60.48ms
step:467/2285 train_time:28244ms step_avg:60.48ms
step:468/2285 train_time:28303ms step_avg:60.48ms
step:469/2285 train_time:28364ms step_avg:60.48ms
step:470/2285 train_time:28423ms step_avg:60.48ms
step:471/2285 train_time:28485ms step_avg:60.48ms
step:472/2285 train_time:28544ms step_avg:60.47ms
step:473/2285 train_time:28606ms step_avg:60.48ms
step:474/2285 train_time:28664ms step_avg:60.47ms
step:475/2285 train_time:28726ms step_avg:60.48ms
step:476/2285 train_time:28784ms step_avg:60.47ms
step:477/2285 train_time:28846ms step_avg:60.47ms
step:478/2285 train_time:28905ms step_avg:60.47ms
step:479/2285 train_time:28966ms step_avg:60.47ms
step:480/2285 train_time:29025ms step_avg:60.47ms
step:481/2285 train_time:29087ms step_avg:60.47ms
step:482/2285 train_time:29146ms step_avg:60.47ms
step:483/2285 train_time:29207ms step_avg:60.47ms
step:484/2285 train_time:29266ms step_avg:60.47ms
step:485/2285 train_time:29327ms step_avg:60.47ms
step:486/2285 train_time:29386ms step_avg:60.47ms
step:487/2285 train_time:29448ms step_avg:60.47ms
step:488/2285 train_time:29507ms step_avg:60.47ms
step:489/2285 train_time:29569ms step_avg:60.47ms
step:490/2285 train_time:29627ms step_avg:60.46ms
step:491/2285 train_time:29689ms step_avg:60.47ms
step:492/2285 train_time:29748ms step_avg:60.46ms
step:493/2285 train_time:29810ms step_avg:60.47ms
step:494/2285 train_time:29869ms step_avg:60.46ms
step:495/2285 train_time:29930ms step_avg:60.47ms
step:496/2285 train_time:29989ms step_avg:60.46ms
step:497/2285 train_time:30051ms step_avg:60.46ms
step:498/2285 train_time:30110ms step_avg:60.46ms
step:499/2285 train_time:30172ms step_avg:60.46ms
step:500/2285 train_time:30231ms step_avg:60.46ms
step:500/2285 val_loss:3.8103 train_time:30294ms step_avg:60.59ms
step:501/2285 train_time:30314ms step_avg:60.51ms
step:502/2285 train_time:30357ms step_avg:60.47ms
step:503/2285 train_time:30422ms step_avg:60.48ms
step:504/2285 train_time:30484ms step_avg:60.48ms
step:505/2285 train_time:30547ms step_avg:60.49ms
step:506/2285 train_time:30606ms step_avg:60.49ms
step:507/2285 train_time:30667ms step_avg:60.49ms
step:508/2285 train_time:30725ms step_avg:60.48ms
step:509/2285 train_time:30786ms step_avg:60.48ms
step:510/2285 train_time:30844ms step_avg:60.48ms
step:511/2285 train_time:30905ms step_avg:60.48ms
step:512/2285 train_time:30963ms step_avg:60.47ms
step:513/2285 train_time:31024ms step_avg:60.47ms
step:514/2285 train_time:31082ms step_avg:60.47ms
step:515/2285 train_time:31143ms step_avg:60.47ms
step:516/2285 train_time:31201ms step_avg:60.47ms
step:517/2285 train_time:31265ms step_avg:60.47ms
step:518/2285 train_time:31327ms step_avg:60.48ms
step:519/2285 train_time:31391ms step_avg:60.48ms
step:520/2285 train_time:31451ms step_avg:60.48ms
step:521/2285 train_time:31512ms step_avg:60.48ms
step:522/2285 train_time:31571ms step_avg:60.48ms
step:523/2285 train_time:31632ms step_avg:60.48ms
step:524/2285 train_time:31691ms step_avg:60.48ms
step:525/2285 train_time:31752ms step_avg:60.48ms
step:526/2285 train_time:31810ms step_avg:60.48ms
step:527/2285 train_time:31871ms step_avg:60.48ms
step:528/2285 train_time:31930ms step_avg:60.47ms
step:529/2285 train_time:31991ms step_avg:60.47ms
step:530/2285 train_time:32050ms step_avg:60.47ms
step:531/2285 train_time:32111ms step_avg:60.47ms
step:532/2285 train_time:32170ms step_avg:60.47ms
step:533/2285 train_time:32231ms step_avg:60.47ms
step:534/2285 train_time:32290ms step_avg:60.47ms
step:535/2285 train_time:32352ms step_avg:60.47ms
step:536/2285 train_time:32412ms step_avg:60.47ms
step:537/2285 train_time:32473ms step_avg:60.47ms
step:538/2285 train_time:32532ms step_avg:60.47ms
step:539/2285 train_time:32594ms step_avg:60.47ms
step:540/2285 train_time:32654ms step_avg:60.47ms
step:541/2285 train_time:32715ms step_avg:60.47ms
step:542/2285 train_time:32775ms step_avg:60.47ms
step:543/2285 train_time:32837ms step_avg:60.47ms
step:544/2285 train_time:32896ms step_avg:60.47ms
step:545/2285 train_time:32958ms step_avg:60.47ms
step:546/2285 train_time:33017ms step_avg:60.47ms
step:547/2285 train_time:33078ms step_avg:60.47ms
step:548/2285 train_time:33137ms step_avg:60.47ms
step:549/2285 train_time:33199ms step_avg:60.47ms
step:550/2285 train_time:33258ms step_avg:60.47ms
step:551/2285 train_time:33320ms step_avg:60.47ms
step:552/2285 train_time:33379ms step_avg:60.47ms
step:553/2285 train_time:33441ms step_avg:60.47ms
step:554/2285 train_time:33500ms step_avg:60.47ms
step:555/2285 train_time:33562ms step_avg:60.47ms
step:556/2285 train_time:33621ms step_avg:60.47ms
step:557/2285 train_time:33683ms step_avg:60.47ms
step:558/2285 train_time:33742ms step_avg:60.47ms
step:559/2285 train_time:33804ms step_avg:60.47ms
step:560/2285 train_time:33863ms step_avg:60.47ms
step:561/2285 train_time:33925ms step_avg:60.47ms
step:562/2285 train_time:33983ms step_avg:60.47ms
step:563/2285 train_time:34045ms step_avg:60.47ms
step:564/2285 train_time:34104ms step_avg:60.47ms
step:565/2285 train_time:34165ms step_avg:60.47ms
step:566/2285 train_time:34224ms step_avg:60.47ms
step:567/2285 train_time:34286ms step_avg:60.47ms
step:568/2285 train_time:34345ms step_avg:60.47ms
step:569/2285 train_time:34406ms step_avg:60.47ms
step:570/2285 train_time:34465ms step_avg:60.46ms
step:571/2285 train_time:34527ms step_avg:60.47ms
step:572/2285 train_time:34586ms step_avg:60.46ms
step:573/2285 train_time:34647ms step_avg:60.47ms
step:574/2285 train_time:34706ms step_avg:60.46ms
step:575/2285 train_time:34767ms step_avg:60.46ms
step:576/2285 train_time:34826ms step_avg:60.46ms
step:577/2285 train_time:34887ms step_avg:60.46ms
step:578/2285 train_time:34946ms step_avg:60.46ms
step:579/2285 train_time:35007ms step_avg:60.46ms
step:580/2285 train_time:35066ms step_avg:60.46ms
step:581/2285 train_time:35128ms step_avg:60.46ms
step:582/2285 train_time:35188ms step_avg:60.46ms
step:583/2285 train_time:35250ms step_avg:60.46ms
step:584/2285 train_time:35309ms step_avg:60.46ms
step:585/2285 train_time:35371ms step_avg:60.46ms
step:586/2285 train_time:35429ms step_avg:60.46ms
step:587/2285 train_time:35491ms step_avg:60.46ms
step:588/2285 train_time:35549ms step_avg:60.46ms
step:589/2285 train_time:35611ms step_avg:60.46ms
step:590/2285 train_time:35669ms step_avg:60.46ms
step:591/2285 train_time:35731ms step_avg:60.46ms
step:592/2285 train_time:35790ms step_avg:60.46ms
step:593/2285 train_time:35852ms step_avg:60.46ms
step:594/2285 train_time:35910ms step_avg:60.45ms
step:595/2285 train_time:35972ms step_avg:60.46ms
step:596/2285 train_time:36030ms step_avg:60.45ms
step:597/2285 train_time:36092ms step_avg:60.46ms
step:598/2285 train_time:36151ms step_avg:60.45ms
step:599/2285 train_time:36213ms step_avg:60.46ms
step:600/2285 train_time:36271ms step_avg:60.45ms
step:601/2285 train_time:36333ms step_avg:60.45ms
step:602/2285 train_time:36392ms step_avg:60.45ms
step:603/2285 train_time:36454ms step_avg:60.45ms
step:604/2285 train_time:36513ms step_avg:60.45ms
step:605/2285 train_time:36575ms step_avg:60.45ms
step:606/2285 train_time:36634ms step_avg:60.45ms
step:607/2285 train_time:36696ms step_avg:60.45ms
step:608/2285 train_time:36755ms step_avg:60.45ms
step:609/2285 train_time:36817ms step_avg:60.45ms
step:610/2285 train_time:36876ms step_avg:60.45ms
step:611/2285 train_time:36938ms step_avg:60.45ms
step:612/2285 train_time:36996ms step_avg:60.45ms
step:613/2285 train_time:37058ms step_avg:60.45ms
step:614/2285 train_time:37117ms step_avg:60.45ms
step:615/2285 train_time:37179ms step_avg:60.45ms
step:616/2285 train_time:37238ms step_avg:60.45ms
step:617/2285 train_time:37299ms step_avg:60.45ms
step:618/2285 train_time:37358ms step_avg:60.45ms
step:619/2285 train_time:37420ms step_avg:60.45ms
step:620/2285 train_time:37480ms step_avg:60.45ms
step:621/2285 train_time:37542ms step_avg:60.45ms
step:622/2285 train_time:37602ms step_avg:60.45ms
step:623/2285 train_time:37663ms step_avg:60.45ms
step:624/2285 train_time:37722ms step_avg:60.45ms
step:625/2285 train_time:37784ms step_avg:60.45ms
step:626/2285 train_time:37844ms step_avg:60.45ms
step:627/2285 train_time:37906ms step_avg:60.46ms
step:628/2285 train_time:37965ms step_avg:60.45ms
step:629/2285 train_time:38026ms step_avg:60.45ms
step:630/2285 train_time:38085ms step_avg:60.45ms
step:631/2285 train_time:38146ms step_avg:60.45ms
step:632/2285 train_time:38205ms step_avg:60.45ms
step:633/2285 train_time:38267ms step_avg:60.45ms
step:634/2285 train_time:38327ms step_avg:60.45ms
step:635/2285 train_time:38389ms step_avg:60.46ms
step:636/2285 train_time:38448ms step_avg:60.45ms
step:637/2285 train_time:38509ms step_avg:60.45ms
step:638/2285 train_time:38568ms step_avg:60.45ms
step:639/2285 train_time:38630ms step_avg:60.45ms
step:640/2285 train_time:38689ms step_avg:60.45ms
step:641/2285 train_time:38751ms step_avg:60.45ms
step:642/2285 train_time:38809ms step_avg:60.45ms
step:643/2285 train_time:38871ms step_avg:60.45ms
step:644/2285 train_time:38930ms step_avg:60.45ms
step:645/2285 train_time:38991ms step_avg:60.45ms
step:646/2285 train_time:39050ms step_avg:60.45ms
step:647/2285 train_time:39112ms step_avg:60.45ms
step:648/2285 train_time:39170ms step_avg:60.45ms
step:649/2285 train_time:39232ms step_avg:60.45ms
step:650/2285 train_time:39290ms step_avg:60.45ms
step:651/2285 train_time:39352ms step_avg:60.45ms
step:652/2285 train_time:39410ms step_avg:60.45ms
step:653/2285 train_time:39472ms step_avg:60.45ms
step:654/2285 train_time:39531ms step_avg:60.44ms
step:655/2285 train_time:39592ms step_avg:60.45ms
step:656/2285 train_time:39651ms step_avg:60.44ms
step:657/2285 train_time:39713ms step_avg:60.45ms
step:658/2285 train_time:39771ms step_avg:60.44ms
step:659/2285 train_time:39834ms step_avg:60.45ms
step:660/2285 train_time:39892ms step_avg:60.44ms
step:661/2285 train_time:39954ms step_avg:60.45ms
step:662/2285 train_time:40013ms step_avg:60.44ms
step:663/2285 train_time:40075ms step_avg:60.45ms
step:664/2285 train_time:40135ms step_avg:60.44ms
step:665/2285 train_time:40197ms step_avg:60.45ms
step:666/2285 train_time:40257ms step_avg:60.45ms
step:667/2285 train_time:40318ms step_avg:60.45ms
step:668/2285 train_time:40377ms step_avg:60.45ms
step:669/2285 train_time:40439ms step_avg:60.45ms
step:670/2285 train_time:40498ms step_avg:60.44ms
step:671/2285 train_time:40559ms step_avg:60.45ms
step:672/2285 train_time:40619ms step_avg:60.44ms
step:673/2285 train_time:40680ms step_avg:60.45ms
step:674/2285 train_time:40740ms step_avg:60.44ms
step:675/2285 train_time:40801ms step_avg:60.45ms
step:676/2285 train_time:40860ms step_avg:60.44ms
step:677/2285 train_time:40922ms step_avg:60.45ms
step:678/2285 train_time:40981ms step_avg:60.44ms
step:679/2285 train_time:41043ms step_avg:60.45ms
step:680/2285 train_time:41102ms step_avg:60.44ms
step:681/2285 train_time:41164ms step_avg:60.45ms
step:682/2285 train_time:41223ms step_avg:60.44ms
step:683/2285 train_time:41285ms step_avg:60.45ms
step:684/2285 train_time:41345ms step_avg:60.45ms
step:685/2285 train_time:41406ms step_avg:60.45ms
step:686/2285 train_time:41466ms step_avg:60.45ms
step:687/2285 train_time:41527ms step_avg:60.45ms
step:688/2285 train_time:41586ms step_avg:60.45ms
step:689/2285 train_time:41649ms step_avg:60.45ms
step:690/2285 train_time:41707ms step_avg:60.45ms
step:691/2285 train_time:41769ms step_avg:60.45ms
step:692/2285 train_time:41829ms step_avg:60.45ms
step:693/2285 train_time:41890ms step_avg:60.45ms
step:694/2285 train_time:41949ms step_avg:60.44ms
step:695/2285 train_time:42010ms step_avg:60.45ms
step:696/2285 train_time:42069ms step_avg:60.44ms
step:697/2285 train_time:42131ms step_avg:60.45ms
step:698/2285 train_time:42190ms step_avg:60.44ms
step:699/2285 train_time:42252ms step_avg:60.45ms
step:700/2285 train_time:42311ms step_avg:60.44ms
step:701/2285 train_time:42372ms step_avg:60.45ms
step:702/2285 train_time:42431ms step_avg:60.44ms
step:703/2285 train_time:42493ms step_avg:60.44ms
step:704/2285 train_time:42552ms step_avg:60.44ms
step:705/2285 train_time:42613ms step_avg:60.44ms
step:706/2285 train_time:42672ms step_avg:60.44ms
step:707/2285 train_time:42734ms step_avg:60.44ms
step:708/2285 train_time:42793ms step_avg:60.44ms
step:709/2285 train_time:42855ms step_avg:60.44ms
step:710/2285 train_time:42914ms step_avg:60.44ms
step:711/2285 train_time:42976ms step_avg:60.44ms
step:712/2285 train_time:43035ms step_avg:60.44ms
step:713/2285 train_time:43096ms step_avg:60.44ms
step:714/2285 train_time:43156ms step_avg:60.44ms
step:715/2285 train_time:43218ms step_avg:60.44ms
step:716/2285 train_time:43278ms step_avg:60.44ms
step:717/2285 train_time:43340ms step_avg:60.45ms
step:718/2285 train_time:43399ms step_avg:60.44ms
step:719/2285 train_time:43461ms step_avg:60.45ms
step:720/2285 train_time:43520ms step_avg:60.44ms
step:721/2285 train_time:43582ms step_avg:60.45ms
step:722/2285 train_time:43642ms step_avg:60.45ms
step:723/2285 train_time:43704ms step_avg:60.45ms
step:724/2285 train_time:43763ms step_avg:60.45ms
step:725/2285 train_time:43825ms step_avg:60.45ms
step:726/2285 train_time:43884ms step_avg:60.45ms
step:727/2285 train_time:43945ms step_avg:60.45ms
step:728/2285 train_time:44004ms step_avg:60.44ms
step:729/2285 train_time:44066ms step_avg:60.45ms
step:730/2285 train_time:44126ms step_avg:60.45ms
step:731/2285 train_time:44188ms step_avg:60.45ms
step:732/2285 train_time:44248ms step_avg:60.45ms
step:733/2285 train_time:44310ms step_avg:60.45ms
step:734/2285 train_time:44369ms step_avg:60.45ms
step:735/2285 train_time:44430ms step_avg:60.45ms
step:736/2285 train_time:44489ms step_avg:60.45ms
step:737/2285 train_time:44550ms step_avg:60.45ms
step:738/2285 train_time:44609ms step_avg:60.45ms
step:739/2285 train_time:44671ms step_avg:60.45ms
step:740/2285 train_time:44730ms step_avg:60.45ms
step:741/2285 train_time:44792ms step_avg:60.45ms
step:742/2285 train_time:44851ms step_avg:60.45ms
step:743/2285 train_time:44913ms step_avg:60.45ms
step:744/2285 train_time:44972ms step_avg:60.45ms
step:745/2285 train_time:45033ms step_avg:60.45ms
step:746/2285 train_time:45092ms step_avg:60.45ms
step:747/2285 train_time:45154ms step_avg:60.45ms
step:748/2285 train_time:45214ms step_avg:60.45ms
step:749/2285 train_time:45276ms step_avg:60.45ms
step:750/2285 train_time:45335ms step_avg:60.45ms
step:750/2285 val_loss:3.6705 train_time:45399ms step_avg:60.53ms
step:751/2285 train_time:45419ms step_avg:60.48ms
step:752/2285 train_time:45460ms step_avg:60.45ms
step:753/2285 train_time:45521ms step_avg:60.45ms
step:754/2285 train_time:45581ms step_avg:60.45ms
step:755/2285 train_time:45643ms step_avg:60.45ms
step:756/2285 train_time:45702ms step_avg:60.45ms
step:757/2285 train_time:45763ms step_avg:60.45ms
step:758/2285 train_time:45823ms step_avg:60.45ms
step:759/2285 train_time:45884ms step_avg:60.45ms
step:760/2285 train_time:45943ms step_avg:60.45ms
step:761/2285 train_time:46004ms step_avg:60.45ms
step:762/2285 train_time:46064ms step_avg:60.45ms
step:763/2285 train_time:46125ms step_avg:60.45ms
step:764/2285 train_time:46184ms step_avg:60.45ms
step:765/2285 train_time:46246ms step_avg:60.45ms
step:766/2285 train_time:46313ms step_avg:60.46ms
step:767/2285 train_time:46380ms step_avg:60.47ms
step:768/2285 train_time:46442ms step_avg:60.47ms
step:769/2285 train_time:46504ms step_avg:60.47ms
step:770/2285 train_time:46565ms step_avg:60.47ms
step:771/2285 train_time:46627ms step_avg:60.48ms
step:772/2285 train_time:46686ms step_avg:60.47ms
step:773/2285 train_time:46748ms step_avg:60.48ms
step:774/2285 train_time:46807ms step_avg:60.47ms
step:775/2285 train_time:46868ms step_avg:60.48ms
step:776/2285 train_time:46928ms step_avg:60.47ms
step:777/2285 train_time:46989ms step_avg:60.48ms
step:778/2285 train_time:47048ms step_avg:60.47ms
step:779/2285 train_time:47110ms step_avg:60.48ms
step:780/2285 train_time:47169ms step_avg:60.47ms
step:781/2285 train_time:47232ms step_avg:60.48ms
step:782/2285 train_time:47292ms step_avg:60.48ms
step:783/2285 train_time:47356ms step_avg:60.48ms
step:784/2285 train_time:47417ms step_avg:60.48ms
step:785/2285 train_time:47480ms step_avg:60.48ms
step:786/2285 train_time:47539ms step_avg:60.48ms
step:787/2285 train_time:47602ms step_avg:60.48ms
step:788/2285 train_time:47661ms step_avg:60.48ms
step:789/2285 train_time:47724ms step_avg:60.49ms
step:790/2285 train_time:47783ms step_avg:60.48ms
step:791/2285 train_time:47845ms step_avg:60.49ms
step:792/2285 train_time:47905ms step_avg:60.49ms
step:793/2285 train_time:47967ms step_avg:60.49ms
step:794/2285 train_time:48027ms step_avg:60.49ms
step:795/2285 train_time:48089ms step_avg:60.49ms
step:796/2285 train_time:48149ms step_avg:60.49ms
step:797/2285 train_time:48211ms step_avg:60.49ms
step:798/2285 train_time:48271ms step_avg:60.49ms
step:799/2285 train_time:48333ms step_avg:60.49ms
step:800/2285 train_time:48393ms step_avg:60.49ms
step:801/2285 train_time:48456ms step_avg:60.49ms
step:802/2285 train_time:48517ms step_avg:60.49ms
step:803/2285 train_time:48579ms step_avg:60.50ms
step:804/2285 train_time:48639ms step_avg:60.50ms
step:805/2285 train_time:48701ms step_avg:60.50ms
step:806/2285 train_time:48760ms step_avg:60.50ms
step:807/2285 train_time:48823ms step_avg:60.50ms
step:808/2285 train_time:48882ms step_avg:60.50ms
step:809/2285 train_time:48944ms step_avg:60.50ms
step:810/2285 train_time:49004ms step_avg:60.50ms
step:811/2285 train_time:49067ms step_avg:60.50ms
step:812/2285 train_time:49126ms step_avg:60.50ms
step:813/2285 train_time:49189ms step_avg:60.50ms
step:814/2285 train_time:49249ms step_avg:60.50ms
step:815/2285 train_time:49312ms step_avg:60.51ms
step:816/2285 train_time:49371ms step_avg:60.50ms
step:817/2285 train_time:49434ms step_avg:60.51ms
step:818/2285 train_time:49494ms step_avg:60.51ms
step:819/2285 train_time:49558ms step_avg:60.51ms
step:820/2285 train_time:49617ms step_avg:60.51ms
step:821/2285 train_time:49679ms step_avg:60.51ms
step:822/2285 train_time:49739ms step_avg:60.51ms
step:823/2285 train_time:49800ms step_avg:60.51ms
step:824/2285 train_time:49860ms step_avg:60.51ms
step:825/2285 train_time:49922ms step_avg:60.51ms
step:826/2285 train_time:49982ms step_avg:60.51ms
step:827/2285 train_time:50045ms step_avg:60.51ms
step:828/2285 train_time:50105ms step_avg:60.51ms
step:829/2285 train_time:50168ms step_avg:60.52ms
step:830/2285 train_time:50229ms step_avg:60.52ms
step:831/2285 train_time:50292ms step_avg:60.52ms
step:832/2285 train_time:50352ms step_avg:60.52ms
step:833/2285 train_time:50414ms step_avg:60.52ms
step:834/2285 train_time:50474ms step_avg:60.52ms
step:835/2285 train_time:50536ms step_avg:60.52ms
step:836/2285 train_time:50596ms step_avg:60.52ms
step:837/2285 train_time:50659ms step_avg:60.52ms
step:838/2285 train_time:50718ms step_avg:60.52ms
step:839/2285 train_time:50781ms step_avg:60.53ms
step:840/2285 train_time:50840ms step_avg:60.52ms
step:841/2285 train_time:50902ms step_avg:60.53ms
step:842/2285 train_time:50961ms step_avg:60.52ms
step:843/2285 train_time:51025ms step_avg:60.53ms
step:844/2285 train_time:51084ms step_avg:60.53ms
step:845/2285 train_time:51147ms step_avg:60.53ms
step:846/2285 train_time:51208ms step_avg:60.53ms
step:847/2285 train_time:51271ms step_avg:60.53ms
step:848/2285 train_time:51331ms step_avg:60.53ms
step:849/2285 train_time:51393ms step_avg:60.53ms
step:850/2285 train_time:51453ms step_avg:60.53ms
step:851/2285 train_time:51515ms step_avg:60.53ms
step:852/2285 train_time:51575ms step_avg:60.53ms
step:853/2285 train_time:51638ms step_avg:60.54ms
step:854/2285 train_time:51697ms step_avg:60.54ms
step:855/2285 train_time:51759ms step_avg:60.54ms
step:856/2285 train_time:51818ms step_avg:60.54ms
step:857/2285 train_time:51881ms step_avg:60.54ms
step:858/2285 train_time:51940ms step_avg:60.54ms
step:859/2285 train_time:52003ms step_avg:60.54ms
step:860/2285 train_time:52063ms step_avg:60.54ms
step:861/2285 train_time:52125ms step_avg:60.54ms
step:862/2285 train_time:52185ms step_avg:60.54ms
step:863/2285 train_time:52248ms step_avg:60.54ms
step:864/2285 train_time:52309ms step_avg:60.54ms
step:865/2285 train_time:52371ms step_avg:60.54ms
step:866/2285 train_time:52431ms step_avg:60.54ms
step:867/2285 train_time:52493ms step_avg:60.55ms
step:868/2285 train_time:52553ms step_avg:60.54ms
step:869/2285 train_time:52615ms step_avg:60.55ms
step:870/2285 train_time:52676ms step_avg:60.55ms
step:871/2285 train_time:52738ms step_avg:60.55ms
step:872/2285 train_time:52798ms step_avg:60.55ms
step:873/2285 train_time:52861ms step_avg:60.55ms
step:874/2285 train_time:52920ms step_avg:60.55ms
step:875/2285 train_time:52982ms step_avg:60.55ms
step:876/2285 train_time:53042ms step_avg:60.55ms
step:877/2285 train_time:53104ms step_avg:60.55ms
step:878/2285 train_time:53165ms step_avg:60.55ms
step:879/2285 train_time:53228ms step_avg:60.55ms
step:880/2285 train_time:53288ms step_avg:60.55ms
step:881/2285 train_time:53350ms step_avg:60.56ms
step:882/2285 train_time:53411ms step_avg:60.56ms
step:883/2285 train_time:53473ms step_avg:60.56ms
step:884/2285 train_time:53533ms step_avg:60.56ms
step:885/2285 train_time:53596ms step_avg:60.56ms
step:886/2285 train_time:53656ms step_avg:60.56ms
step:887/2285 train_time:53719ms step_avg:60.56ms
step:888/2285 train_time:53778ms step_avg:60.56ms
step:889/2285 train_time:53840ms step_avg:60.56ms
step:890/2285 train_time:53899ms step_avg:60.56ms
step:891/2285 train_time:53962ms step_avg:60.56ms
step:892/2285 train_time:54021ms step_avg:60.56ms
step:893/2285 train_time:54083ms step_avg:60.56ms
step:894/2285 train_time:54143ms step_avg:60.56ms
step:895/2285 train_time:54205ms step_avg:60.56ms
step:896/2285 train_time:54266ms step_avg:60.56ms
step:897/2285 train_time:54329ms step_avg:60.57ms
step:898/2285 train_time:54389ms step_avg:60.57ms
step:899/2285 train_time:54452ms step_avg:60.57ms
step:900/2285 train_time:54512ms step_avg:60.57ms
step:901/2285 train_time:54574ms step_avg:60.57ms
step:902/2285 train_time:54634ms step_avg:60.57ms
step:903/2285 train_time:54698ms step_avg:60.57ms
step:904/2285 train_time:54758ms step_avg:60.57ms
step:905/2285 train_time:54820ms step_avg:60.57ms
step:906/2285 train_time:54879ms step_avg:60.57ms
step:907/2285 train_time:54941ms step_avg:60.57ms
step:908/2285 train_time:55000ms step_avg:60.57ms
step:909/2285 train_time:55062ms step_avg:60.57ms
step:910/2285 train_time:55121ms step_avg:60.57ms
step:911/2285 train_time:55184ms step_avg:60.58ms
step:912/2285 train_time:55244ms step_avg:60.57ms
step:913/2285 train_time:55308ms step_avg:60.58ms
step:914/2285 train_time:55369ms step_avg:60.58ms
step:915/2285 train_time:55431ms step_avg:60.58ms
step:916/2285 train_time:55491ms step_avg:60.58ms
step:917/2285 train_time:55553ms step_avg:60.58ms
step:918/2285 train_time:55613ms step_avg:60.58ms
step:919/2285 train_time:55676ms step_avg:60.58ms
step:920/2285 train_time:55735ms step_avg:60.58ms
step:921/2285 train_time:55798ms step_avg:60.58ms
step:922/2285 train_time:55858ms step_avg:60.58ms
step:923/2285 train_time:55920ms step_avg:60.59ms
step:924/2285 train_time:55980ms step_avg:60.58ms
step:925/2285 train_time:56043ms step_avg:60.59ms
step:926/2285 train_time:56102ms step_avg:60.59ms
step:927/2285 train_time:56164ms step_avg:60.59ms
step:928/2285 train_time:56224ms step_avg:60.59ms
step:929/2285 train_time:56287ms step_avg:60.59ms
step:930/2285 train_time:56348ms step_avg:60.59ms
step:931/2285 train_time:56412ms step_avg:60.59ms
step:932/2285 train_time:56471ms step_avg:60.59ms
step:933/2285 train_time:56534ms step_avg:60.59ms
step:934/2285 train_time:56594ms step_avg:60.59ms
step:935/2285 train_time:56656ms step_avg:60.59ms
step:936/2285 train_time:56716ms step_avg:60.59ms
step:937/2285 train_time:56779ms step_avg:60.60ms
step:938/2285 train_time:56838ms step_avg:60.59ms
step:939/2285 train_time:56900ms step_avg:60.60ms
step:940/2285 train_time:56959ms step_avg:60.59ms
step:941/2285 train_time:57021ms step_avg:60.60ms
step:942/2285 train_time:57081ms step_avg:60.60ms
step:943/2285 train_time:57143ms step_avg:60.60ms
step:944/2285 train_time:57202ms step_avg:60.60ms
step:945/2285 train_time:57265ms step_avg:60.60ms
step:946/2285 train_time:57326ms step_avg:60.60ms
step:947/2285 train_time:57390ms step_avg:60.60ms
step:948/2285 train_time:57450ms step_avg:60.60ms
step:949/2285 train_time:57513ms step_avg:60.60ms
step:950/2285 train_time:57573ms step_avg:60.60ms
step:951/2285 train_time:57635ms step_avg:60.60ms
step:952/2285 train_time:57696ms step_avg:60.60ms
step:953/2285 train_time:57758ms step_avg:60.61ms
step:954/2285 train_time:57818ms step_avg:60.61ms
step:955/2285 train_time:57879ms step_avg:60.61ms
step:956/2285 train_time:57939ms step_avg:60.61ms
step:957/2285 train_time:58001ms step_avg:60.61ms
step:958/2285 train_time:58060ms step_avg:60.61ms
step:959/2285 train_time:58123ms step_avg:60.61ms
step:960/2285 train_time:58183ms step_avg:60.61ms
step:961/2285 train_time:58246ms step_avg:60.61ms
step:962/2285 train_time:58306ms step_avg:60.61ms
step:963/2285 train_time:58369ms step_avg:60.61ms
step:964/2285 train_time:58429ms step_avg:60.61ms
step:965/2285 train_time:58492ms step_avg:60.61ms
step:966/2285 train_time:58552ms step_avg:60.61ms
step:967/2285 train_time:58614ms step_avg:60.61ms
step:968/2285 train_time:58674ms step_avg:60.61ms
step:969/2285 train_time:58737ms step_avg:60.62ms
step:970/2285 train_time:58796ms step_avg:60.61ms
step:971/2285 train_time:58859ms step_avg:60.62ms
step:972/2285 train_time:58918ms step_avg:60.62ms
step:973/2285 train_time:58980ms step_avg:60.62ms
step:974/2285 train_time:59039ms step_avg:60.61ms
step:975/2285 train_time:59101ms step_avg:60.62ms
step:976/2285 train_time:59161ms step_avg:60.62ms
step:977/2285 train_time:59223ms step_avg:60.62ms
step:978/2285 train_time:59283ms step_avg:60.62ms
step:979/2285 train_time:59346ms step_avg:60.62ms
step:980/2285 train_time:59407ms step_avg:60.62ms
step:981/2285 train_time:59470ms step_avg:60.62ms
step:982/2285 train_time:59530ms step_avg:60.62ms
step:983/2285 train_time:59593ms step_avg:60.62ms
step:984/2285 train_time:59653ms step_avg:60.62ms
step:985/2285 train_time:59715ms step_avg:60.62ms
step:986/2285 train_time:59775ms step_avg:60.62ms
step:987/2285 train_time:59838ms step_avg:60.63ms
step:988/2285 train_time:59897ms step_avg:60.62ms
step:989/2285 train_time:59960ms step_avg:60.63ms
step:990/2285 train_time:60019ms step_avg:60.63ms
step:991/2285 train_time:60082ms step_avg:60.63ms
step:992/2285 train_time:60141ms step_avg:60.63ms
step:993/2285 train_time:60204ms step_avg:60.63ms
step:994/2285 train_time:60263ms step_avg:60.63ms
step:995/2285 train_time:60326ms step_avg:60.63ms
step:996/2285 train_time:60387ms step_avg:60.63ms
step:997/2285 train_time:60449ms step_avg:60.63ms
step:998/2285 train_time:60510ms step_avg:60.63ms
step:999/2285 train_time:60572ms step_avg:60.63ms
step:1000/2285 train_time:60633ms step_avg:60.63ms
step:1000/2285 val_loss:3.5703 train_time:60696ms step_avg:60.70ms
step:1001/2285 train_time:60714ms step_avg:60.65ms
step:1002/2285 train_time:60756ms step_avg:60.63ms
step:1003/2285 train_time:60824ms step_avg:60.64ms
step:1004/2285 train_time:60886ms step_avg:60.64ms
step:1005/2285 train_time:60949ms step_avg:60.65ms
step:1006/2285 train_time:61010ms step_avg:60.65ms
step:1007/2285 train_time:61072ms step_avg:60.65ms
step:1008/2285 train_time:61131ms step_avg:60.65ms
step:1009/2285 train_time:61194ms step_avg:60.65ms
step:1010/2285 train_time:61253ms step_avg:60.65ms
step:1011/2285 train_time:61315ms step_avg:60.65ms
step:1012/2285 train_time:61374ms step_avg:60.65ms
step:1013/2285 train_time:61436ms step_avg:60.65ms
step:1014/2285 train_time:61495ms step_avg:60.65ms
step:1015/2285 train_time:61556ms step_avg:60.65ms
step:1016/2285 train_time:61616ms step_avg:60.65ms
step:1017/2285 train_time:61679ms step_avg:60.65ms
step:1018/2285 train_time:61740ms step_avg:60.65ms
step:1019/2285 train_time:61804ms step_avg:60.65ms
step:1020/2285 train_time:61865ms step_avg:60.65ms
step:1021/2285 train_time:61928ms step_avg:60.65ms
step:1022/2285 train_time:61989ms step_avg:60.65ms
step:1023/2285 train_time:62051ms step_avg:60.66ms
step:1024/2285 train_time:62111ms step_avg:60.66ms
step:1025/2285 train_time:62173ms step_avg:60.66ms
step:1026/2285 train_time:62233ms step_avg:60.66ms
step:1027/2285 train_time:62295ms step_avg:60.66ms
step:1028/2285 train_time:62355ms step_avg:60.66ms
step:1029/2285 train_time:62416ms step_avg:60.66ms
step:1030/2285 train_time:62476ms step_avg:60.66ms
step:1031/2285 train_time:62538ms step_avg:60.66ms
step:1032/2285 train_time:62598ms step_avg:60.66ms
step:1033/2285 train_time:62660ms step_avg:60.66ms
step:1034/2285 train_time:62721ms step_avg:60.66ms
step:1035/2285 train_time:62784ms step_avg:60.66ms
step:1036/2285 train_time:62845ms step_avg:60.66ms
step:1037/2285 train_time:62908ms step_avg:60.66ms
step:1038/2285 train_time:62968ms step_avg:60.66ms
step:1039/2285 train_time:63031ms step_avg:60.67ms
step:1040/2285 train_time:63092ms step_avg:60.67ms
step:1041/2285 train_time:63155ms step_avg:60.67ms
step:1042/2285 train_time:63214ms step_avg:60.67ms
step:1043/2285 train_time:63277ms step_avg:60.67ms
step:1044/2285 train_time:63336ms step_avg:60.67ms
step:1045/2285 train_time:63398ms step_avg:60.67ms
step:1046/2285 train_time:63458ms step_avg:60.67ms
step:1047/2285 train_time:63520ms step_avg:60.67ms
step:1048/2285 train_time:63581ms step_avg:60.67ms
step:1049/2285 train_time:63643ms step_avg:60.67ms
step:1050/2285 train_time:63703ms step_avg:60.67ms
step:1051/2285 train_time:63766ms step_avg:60.67ms
step:1052/2285 train_time:63826ms step_avg:60.67ms
step:1053/2285 train_time:63889ms step_avg:60.67ms
step:1054/2285 train_time:63949ms step_avg:60.67ms
step:1055/2285 train_time:64012ms step_avg:60.67ms
step:1056/2285 train_time:64072ms step_avg:60.67ms
step:1057/2285 train_time:64135ms step_avg:60.68ms
step:1058/2285 train_time:64195ms step_avg:60.68ms
step:1059/2285 train_time:64257ms step_avg:60.68ms
step:1060/2285 train_time:64317ms step_avg:60.68ms
step:1061/2285 train_time:64378ms step_avg:60.68ms
step:1062/2285 train_time:64438ms step_avg:60.68ms
step:1063/2285 train_time:64499ms step_avg:60.68ms
step:1064/2285 train_time:64559ms step_avg:60.68ms
step:1065/2285 train_time:64621ms step_avg:60.68ms
step:1066/2285 train_time:64682ms step_avg:60.68ms
step:1067/2285 train_time:64745ms step_avg:60.68ms
step:1068/2285 train_time:64805ms step_avg:60.68ms
step:1069/2285 train_time:64868ms step_avg:60.68ms
step:1070/2285 train_time:64927ms step_avg:60.68ms
step:1071/2285 train_time:64990ms step_avg:60.68ms
step:1072/2285 train_time:65050ms step_avg:60.68ms
step:1073/2285 train_time:65113ms step_avg:60.68ms
step:1074/2285 train_time:65173ms step_avg:60.68ms
step:1075/2285 train_time:65235ms step_avg:60.68ms
step:1076/2285 train_time:65296ms step_avg:60.68ms
step:1077/2285 train_time:65358ms step_avg:60.69ms
step:1078/2285 train_time:65417ms step_avg:60.68ms
step:1079/2285 train_time:65480ms step_avg:60.69ms
step:1080/2285 train_time:65539ms step_avg:60.68ms
step:1081/2285 train_time:65601ms step_avg:60.69ms
step:1082/2285 train_time:65661ms step_avg:60.69ms
step:1083/2285 train_time:65724ms step_avg:60.69ms
step:1084/2285 train_time:65784ms step_avg:60.69ms
step:1085/2285 train_time:65847ms step_avg:60.69ms
step:1086/2285 train_time:65906ms step_avg:60.69ms
step:1087/2285 train_time:65969ms step_avg:60.69ms
step:1088/2285 train_time:66029ms step_avg:60.69ms
step:1089/2285 train_time:66091ms step_avg:60.69ms
step:1090/2285 train_time:66151ms step_avg:60.69ms
step:1091/2285 train_time:66214ms step_avg:60.69ms
step:1092/2285 train_time:66274ms step_avg:60.69ms
step:1093/2285 train_time:66337ms step_avg:60.69ms
step:1094/2285 train_time:66398ms step_avg:60.69ms
step:1095/2285 train_time:66460ms step_avg:60.69ms
step:1096/2285 train_time:66519ms step_avg:60.69ms
step:1097/2285 train_time:66582ms step_avg:60.69ms
step:1098/2285 train_time:66642ms step_avg:60.69ms
step:1099/2285 train_time:66704ms step_avg:60.70ms
step:1100/2285 train_time:66764ms step_avg:60.69ms
step:1101/2285 train_time:66826ms step_avg:60.70ms
step:1102/2285 train_time:66886ms step_avg:60.70ms
step:1103/2285 train_time:66948ms step_avg:60.70ms
step:1104/2285 train_time:67008ms step_avg:60.70ms
step:1105/2285 train_time:67071ms step_avg:60.70ms
step:1106/2285 train_time:67131ms step_avg:60.70ms
step:1107/2285 train_time:67193ms step_avg:60.70ms
step:1108/2285 train_time:67253ms step_avg:60.70ms
step:1109/2285 train_time:67316ms step_avg:60.70ms
step:1110/2285 train_time:67376ms step_avg:60.70ms
step:1111/2285 train_time:67439ms step_avg:60.70ms
step:1112/2285 train_time:67499ms step_avg:60.70ms
step:1113/2285 train_time:67561ms step_avg:60.70ms
step:1114/2285 train_time:67621ms step_avg:60.70ms
step:1115/2285 train_time:67684ms step_avg:60.70ms
step:1116/2285 train_time:67744ms step_avg:60.70ms
step:1117/2285 train_time:67806ms step_avg:60.70ms
step:1118/2285 train_time:67866ms step_avg:60.70ms
step:1119/2285 train_time:67928ms step_avg:60.70ms
step:1120/2285 train_time:67989ms step_avg:60.70ms
step:1121/2285 train_time:68051ms step_avg:60.71ms
step:1122/2285 train_time:68111ms step_avg:60.70ms
step:1123/2285 train_time:68173ms step_avg:60.71ms
step:1124/2285 train_time:68233ms step_avg:60.71ms
step:1125/2285 train_time:68296ms step_avg:60.71ms
step:1126/2285 train_time:68356ms step_avg:60.71ms
step:1127/2285 train_time:68419ms step_avg:60.71ms
step:1128/2285 train_time:68478ms step_avg:60.71ms
step:1129/2285 train_time:68542ms step_avg:60.71ms
step:1130/2285 train_time:68602ms step_avg:60.71ms
step:1131/2285 train_time:68664ms step_avg:60.71ms
step:1132/2285 train_time:68724ms step_avg:60.71ms
step:1133/2285 train_time:68787ms step_avg:60.71ms
step:1134/2285 train_time:68846ms step_avg:60.71ms
step:1135/2285 train_time:68908ms step_avg:60.71ms
step:1136/2285 train_time:68968ms step_avg:60.71ms
step:1137/2285 train_time:69031ms step_avg:60.71ms
step:1138/2285 train_time:69090ms step_avg:60.71ms
step:1139/2285 train_time:69153ms step_avg:60.71ms
step:1140/2285 train_time:69214ms step_avg:60.71ms
step:1141/2285 train_time:69276ms step_avg:60.72ms
step:1142/2285 train_time:69337ms step_avg:60.71ms
step:1143/2285 train_time:69399ms step_avg:60.72ms
step:1144/2285 train_time:69459ms step_avg:60.72ms
step:1145/2285 train_time:69521ms step_avg:60.72ms
step:1146/2285 train_time:69581ms step_avg:60.72ms
step:1147/2285 train_time:69644ms step_avg:60.72ms
step:1148/2285 train_time:69704ms step_avg:60.72ms
step:1149/2285 train_time:69766ms step_avg:60.72ms
step:1150/2285 train_time:69826ms step_avg:60.72ms
step:1151/2285 train_time:69888ms step_avg:60.72ms
step:1152/2285 train_time:69947ms step_avg:60.72ms
step:1153/2285 train_time:70010ms step_avg:60.72ms
step:1154/2285 train_time:70069ms step_avg:60.72ms
step:1155/2285 train_time:70132ms step_avg:60.72ms
step:1156/2285 train_time:70192ms step_avg:60.72ms
step:1157/2285 train_time:70255ms step_avg:60.72ms
step:1158/2285 train_time:70315ms step_avg:60.72ms
step:1159/2285 train_time:70377ms step_avg:60.72ms
step:1160/2285 train_time:70438ms step_avg:60.72ms
step:1161/2285 train_time:70500ms step_avg:60.72ms
step:1162/2285 train_time:70560ms step_avg:60.72ms
step:1163/2285 train_time:70623ms step_avg:60.72ms
step:1164/2285 train_time:70683ms step_avg:60.72ms
step:1165/2285 train_time:70745ms step_avg:60.73ms
step:1166/2285 train_time:70804ms step_avg:60.72ms
step:1167/2285 train_time:70867ms step_avg:60.73ms
step:1168/2285 train_time:70926ms step_avg:60.72ms
step:1169/2285 train_time:70989ms step_avg:60.73ms
step:1170/2285 train_time:71049ms step_avg:60.73ms
step:1171/2285 train_time:71111ms step_avg:60.73ms
step:1172/2285 train_time:71171ms step_avg:60.73ms
step:1173/2285 train_time:71233ms step_avg:60.73ms
step:1174/2285 train_time:71294ms step_avg:60.73ms
step:1175/2285 train_time:71357ms step_avg:60.73ms
step:1176/2285 train_time:71417ms step_avg:60.73ms
step:1177/2285 train_time:71480ms step_avg:60.73ms
step:1178/2285 train_time:71540ms step_avg:60.73ms
step:1179/2285 train_time:71603ms step_avg:60.73ms
step:1180/2285 train_time:71662ms step_avg:60.73ms
step:1181/2285 train_time:71725ms step_avg:60.73ms
step:1182/2285 train_time:71785ms step_avg:60.73ms
step:1183/2285 train_time:71847ms step_avg:60.73ms
step:1184/2285 train_time:71908ms step_avg:60.73ms
step:1185/2285 train_time:71969ms step_avg:60.73ms
step:1186/2285 train_time:72028ms step_avg:60.73ms
step:1187/2285 train_time:72091ms step_avg:60.73ms
step:1188/2285 train_time:72151ms step_avg:60.73ms
step:1189/2285 train_time:72213ms step_avg:60.73ms
step:1190/2285 train_time:72273ms step_avg:60.73ms
step:1191/2285 train_time:72336ms step_avg:60.74ms
step:1192/2285 train_time:72396ms step_avg:60.73ms
step:1193/2285 train_time:72458ms step_avg:60.74ms
step:1194/2285 train_time:72518ms step_avg:60.74ms
step:1195/2285 train_time:72581ms step_avg:60.74ms
step:1196/2285 train_time:72641ms step_avg:60.74ms
step:1197/2285 train_time:72703ms step_avg:60.74ms
step:1198/2285 train_time:72763ms step_avg:60.74ms
step:1199/2285 train_time:72826ms step_avg:60.74ms
step:1200/2285 train_time:72886ms step_avg:60.74ms
step:1201/2285 train_time:72947ms step_avg:60.74ms
step:1202/2285 train_time:73007ms step_avg:60.74ms
step:1203/2285 train_time:73070ms step_avg:60.74ms
step:1204/2285 train_time:73129ms step_avg:60.74ms
step:1205/2285 train_time:73191ms step_avg:60.74ms
step:1206/2285 train_time:73251ms step_avg:60.74ms
step:1207/2285 train_time:73314ms step_avg:60.74ms
step:1208/2285 train_time:73375ms step_avg:60.74ms
step:1209/2285 train_time:73438ms step_avg:60.74ms
step:1210/2285 train_time:73498ms step_avg:60.74ms
step:1211/2285 train_time:73561ms step_avg:60.74ms
step:1212/2285 train_time:73621ms step_avg:60.74ms
step:1213/2285 train_time:73684ms step_avg:60.75ms
step:1214/2285 train_time:73744ms step_avg:60.74ms
step:1215/2285 train_time:73807ms step_avg:60.75ms
step:1216/2285 train_time:73866ms step_avg:60.75ms
step:1217/2285 train_time:73929ms step_avg:60.75ms
step:1218/2285 train_time:73989ms step_avg:60.75ms
step:1219/2285 train_time:74051ms step_avg:60.75ms
step:1220/2285 train_time:74111ms step_avg:60.75ms
step:1221/2285 train_time:74173ms step_avg:60.75ms
step:1222/2285 train_time:74233ms step_avg:60.75ms
step:1223/2285 train_time:74296ms step_avg:60.75ms
step:1224/2285 train_time:74356ms step_avg:60.75ms
step:1225/2285 train_time:74418ms step_avg:60.75ms
step:1226/2285 train_time:74478ms step_avg:60.75ms
step:1227/2285 train_time:74541ms step_avg:60.75ms
step:1228/2285 train_time:74601ms step_avg:60.75ms
step:1229/2285 train_time:74664ms step_avg:60.75ms
step:1230/2285 train_time:74724ms step_avg:60.75ms
step:1231/2285 train_time:74786ms step_avg:60.75ms
step:1232/2285 train_time:74846ms step_avg:60.75ms
step:1233/2285 train_time:74907ms step_avg:60.75ms
step:1234/2285 train_time:74966ms step_avg:60.75ms
step:1235/2285 train_time:75028ms step_avg:60.75ms
step:1236/2285 train_time:75089ms step_avg:60.75ms
step:1237/2285 train_time:75151ms step_avg:60.75ms
step:1238/2285 train_time:75211ms step_avg:60.75ms
step:1239/2285 train_time:75273ms step_avg:60.75ms
step:1240/2285 train_time:75333ms step_avg:60.75ms
step:1241/2285 train_time:75396ms step_avg:60.75ms
step:1242/2285 train_time:75457ms step_avg:60.75ms
step:1243/2285 train_time:75519ms step_avg:60.76ms
step:1244/2285 train_time:75579ms step_avg:60.76ms
step:1245/2285 train_time:75642ms step_avg:60.76ms
step:1246/2285 train_time:75703ms step_avg:60.76ms
step:1247/2285 train_time:75765ms step_avg:60.76ms
step:1248/2285 train_time:75825ms step_avg:60.76ms
step:1249/2285 train_time:75888ms step_avg:60.76ms
step:1250/2285 train_time:75947ms step_avg:60.76ms
step:1250/2285 val_loss:3.5015 train_time:76011ms step_avg:60.81ms
step:1251/2285 train_time:76029ms step_avg:60.77ms
step:1252/2285 train_time:76072ms step_avg:60.76ms
step:1253/2285 train_time:76137ms step_avg:60.76ms
step:1254/2285 train_time:76199ms step_avg:60.76ms
step:1255/2285 train_time:76261ms step_avg:60.77ms
step:1256/2285 train_time:76321ms step_avg:60.77ms
step:1257/2285 train_time:76383ms step_avg:60.77ms
step:1258/2285 train_time:76442ms step_avg:60.76ms
step:1259/2285 train_time:76503ms step_avg:60.77ms
step:1260/2285 train_time:76562ms step_avg:60.76ms
step:1261/2285 train_time:76624ms step_avg:60.76ms
step:1262/2285 train_time:76683ms step_avg:60.76ms
step:1263/2285 train_time:76744ms step_avg:60.76ms
step:1264/2285 train_time:76804ms step_avg:60.76ms
step:1265/2285 train_time:76866ms step_avg:60.76ms
step:1266/2285 train_time:76926ms step_avg:60.76ms
step:1267/2285 train_time:76991ms step_avg:60.77ms
step:1268/2285 train_time:77052ms step_avg:60.77ms
step:1269/2285 train_time:77116ms step_avg:60.77ms
step:1270/2285 train_time:77176ms step_avg:60.77ms
step:1271/2285 train_time:77239ms step_avg:60.77ms
step:1272/2285 train_time:77299ms step_avg:60.77ms
step:1273/2285 train_time:77361ms step_avg:60.77ms
step:1274/2285 train_time:77421ms step_avg:60.77ms
step:1275/2285 train_time:77482ms step_avg:60.77ms
step:1276/2285 train_time:77542ms step_avg:60.77ms
step:1277/2285 train_time:77604ms step_avg:60.77ms
step:1278/2285 train_time:77663ms step_avg:60.77ms
step:1279/2285 train_time:77725ms step_avg:60.77ms
step:1280/2285 train_time:77784ms step_avg:60.77ms
step:1281/2285 train_time:77846ms step_avg:60.77ms
step:1282/2285 train_time:77906ms step_avg:60.77ms
step:1283/2285 train_time:77969ms step_avg:60.77ms
step:1284/2285 train_time:78029ms step_avg:60.77ms
step:1285/2285 train_time:78093ms step_avg:60.77ms
step:1286/2285 train_time:78153ms step_avg:60.77ms
step:1287/2285 train_time:78216ms step_avg:60.77ms
step:1288/2285 train_time:78276ms step_avg:60.77ms
step:1289/2285 train_time:78338ms step_avg:60.77ms
step:1290/2285 train_time:78398ms step_avg:60.77ms
step:1291/2285 train_time:78461ms step_avg:60.78ms
step:1292/2285 train_time:78521ms step_avg:60.77ms
step:1293/2285 train_time:78583ms step_avg:60.78ms
step:1294/2285 train_time:78642ms step_avg:60.77ms
step:1295/2285 train_time:78704ms step_avg:60.77ms
step:1296/2285 train_time:78763ms step_avg:60.77ms
step:1297/2285 train_time:78826ms step_avg:60.78ms
step:1298/2285 train_time:78886ms step_avg:60.77ms
step:1299/2285 train_time:78949ms step_avg:60.78ms
step:1300/2285 train_time:79010ms step_avg:60.78ms
step:1301/2285 train_time:79072ms step_avg:60.78ms
step:1302/2285 train_time:79132ms step_avg:60.78ms
step:1303/2285 train_time:79195ms step_avg:60.78ms
step:1304/2285 train_time:79255ms step_avg:60.78ms
step:1305/2285 train_time:79317ms step_avg:60.78ms
step:1306/2285 train_time:79377ms step_avg:60.78ms
step:1307/2285 train_time:79440ms step_avg:60.78ms
step:1308/2285 train_time:79500ms step_avg:60.78ms
step:1309/2285 train_time:79562ms step_avg:60.78ms
step:1310/2285 train_time:79622ms step_avg:60.78ms
step:1311/2285 train_time:79684ms step_avg:60.78ms
step:1312/2285 train_time:79743ms step_avg:60.78ms
step:1313/2285 train_time:79805ms step_avg:60.78ms
step:1314/2285 train_time:79865ms step_avg:60.78ms
step:1315/2285 train_time:79928ms step_avg:60.78ms
step:1316/2285 train_time:79989ms step_avg:60.78ms
step:1317/2285 train_time:80052ms step_avg:60.78ms
step:1318/2285 train_time:80112ms step_avg:60.78ms
step:1319/2285 train_time:80175ms step_avg:60.78ms
step:1320/2285 train_time:80234ms step_avg:60.78ms
step:1321/2285 train_time:80297ms step_avg:60.78ms
step:1322/2285 train_time:80357ms step_avg:60.78ms
step:1323/2285 train_time:80420ms step_avg:60.79ms
step:1324/2285 train_time:80480ms step_avg:60.79ms
step:1325/2285 train_time:80542ms step_avg:60.79ms
step:1326/2285 train_time:80601ms step_avg:60.79ms
step:1327/2285 train_time:80664ms step_avg:60.79ms
step:1328/2285 train_time:80724ms step_avg:60.79ms
step:1329/2285 train_time:80786ms step_avg:60.79ms
step:1330/2285 train_time:80846ms step_avg:60.79ms
step:1331/2285 train_time:80909ms step_avg:60.79ms
step:1332/2285 train_time:80969ms step_avg:60.79ms
step:1333/2285 train_time:81032ms step_avg:60.79ms
step:1334/2285 train_time:81092ms step_avg:60.79ms
step:1335/2285 train_time:81154ms step_avg:60.79ms
step:1336/2285 train_time:81214ms step_avg:60.79ms
step:1337/2285 train_time:81276ms step_avg:60.79ms
step:1338/2285 train_time:81336ms step_avg:60.79ms
step:1339/2285 train_time:81398ms step_avg:60.79ms
step:1340/2285 train_time:81459ms step_avg:60.79ms
step:1341/2285 train_time:81522ms step_avg:60.79ms
step:1342/2285 train_time:81581ms step_avg:60.79ms
step:1343/2285 train_time:81644ms step_avg:60.79ms
step:1344/2285 train_time:81703ms step_avg:60.79ms
step:1345/2285 train_time:81765ms step_avg:60.79ms
step:1346/2285 train_time:81825ms step_avg:60.79ms
step:1347/2285 train_time:81888ms step_avg:60.79ms
step:1348/2285 train_time:81948ms step_avg:60.79ms
step:1349/2285 train_time:82011ms step_avg:60.79ms
step:1350/2285 train_time:82070ms step_avg:60.79ms
step:1351/2285 train_time:82133ms step_avg:60.79ms
step:1352/2285 train_time:82193ms step_avg:60.79ms
step:1353/2285 train_time:82255ms step_avg:60.79ms
step:1354/2285 train_time:82315ms step_avg:60.79ms
step:1355/2285 train_time:82378ms step_avg:60.80ms
step:1356/2285 train_time:82438ms step_avg:60.80ms
step:1357/2285 train_time:82501ms step_avg:60.80ms
step:1358/2285 train_time:82561ms step_avg:60.80ms
step:1359/2285 train_time:82624ms step_avg:60.80ms
step:1360/2285 train_time:82683ms step_avg:60.80ms
step:1361/2285 train_time:82746ms step_avg:60.80ms
step:1362/2285 train_time:82806ms step_avg:60.80ms
step:1363/2285 train_time:82868ms step_avg:60.80ms
step:1364/2285 train_time:82929ms step_avg:60.80ms
step:1365/2285 train_time:82991ms step_avg:60.80ms
step:1366/2285 train_time:83050ms step_avg:60.80ms
step:1367/2285 train_time:83113ms step_avg:60.80ms
step:1368/2285 train_time:83172ms step_avg:60.80ms
step:1369/2285 train_time:83235ms step_avg:60.80ms
step:1370/2285 train_time:83295ms step_avg:60.80ms
step:1371/2285 train_time:83358ms step_avg:60.80ms
step:1372/2285 train_time:83419ms step_avg:60.80ms
step:1373/2285 train_time:83481ms step_avg:60.80ms
step:1374/2285 train_time:83541ms step_avg:60.80ms
step:1375/2285 train_time:83603ms step_avg:60.80ms
step:1376/2285 train_time:83663ms step_avg:60.80ms
step:1377/2285 train_time:83727ms step_avg:60.80ms
step:1378/2285 train_time:83787ms step_avg:60.80ms
step:1379/2285 train_time:83849ms step_avg:60.80ms
step:1380/2285 train_time:83909ms step_avg:60.80ms
step:1381/2285 train_time:83971ms step_avg:60.80ms
step:1382/2285 train_time:84031ms step_avg:60.80ms
step:1383/2285 train_time:84093ms step_avg:60.80ms
step:1384/2285 train_time:84153ms step_avg:60.80ms
step:1385/2285 train_time:84216ms step_avg:60.81ms
step:1386/2285 train_time:84275ms step_avg:60.80ms
step:1387/2285 train_time:84338ms step_avg:60.81ms
step:1388/2285 train_time:84397ms step_avg:60.81ms
step:1389/2285 train_time:84461ms step_avg:60.81ms
step:1390/2285 train_time:84521ms step_avg:60.81ms
step:1391/2285 train_time:84583ms step_avg:60.81ms
step:1392/2285 train_time:84643ms step_avg:60.81ms
step:1393/2285 train_time:84705ms step_avg:60.81ms
step:1394/2285 train_time:84765ms step_avg:60.81ms
step:1395/2285 train_time:84827ms step_avg:60.81ms
step:1396/2285 train_time:84888ms step_avg:60.81ms
step:1397/2285 train_time:84951ms step_avg:60.81ms
step:1398/2285 train_time:85010ms step_avg:60.81ms
step:1399/2285 train_time:85072ms step_avg:60.81ms
step:1400/2285 train_time:85132ms step_avg:60.81ms
step:1401/2285 train_time:85194ms step_avg:60.81ms
step:1402/2285 train_time:85254ms step_avg:60.81ms
step:1403/2285 train_time:85317ms step_avg:60.81ms
step:1404/2285 train_time:85377ms step_avg:60.81ms
step:1405/2285 train_time:85439ms step_avg:60.81ms
step:1406/2285 train_time:85500ms step_avg:60.81ms
step:1407/2285 train_time:85562ms step_avg:60.81ms
step:1408/2285 train_time:85622ms step_avg:60.81ms
step:1409/2285 train_time:85684ms step_avg:60.81ms
step:1410/2285 train_time:85744ms step_avg:60.81ms
step:1411/2285 train_time:85807ms step_avg:60.81ms
step:1412/2285 train_time:85867ms step_avg:60.81ms
step:1413/2285 train_time:85930ms step_avg:60.81ms
step:1414/2285 train_time:85990ms step_avg:60.81ms
step:1415/2285 train_time:86052ms step_avg:60.81ms
step:1416/2285 train_time:86112ms step_avg:60.81ms
step:1417/2285 train_time:86174ms step_avg:60.81ms
step:1418/2285 train_time:86234ms step_avg:60.81ms
step:1419/2285 train_time:86296ms step_avg:60.81ms
step:1420/2285 train_time:86356ms step_avg:60.81ms
step:1421/2285 train_time:86419ms step_avg:60.82ms
step:1422/2285 train_time:86480ms step_avg:60.82ms
step:1423/2285 train_time:86542ms step_avg:60.82ms
step:1424/2285 train_time:86602ms step_avg:60.82ms
step:1425/2285 train_time:86664ms step_avg:60.82ms
step:1426/2285 train_time:86724ms step_avg:60.82ms
step:1427/2285 train_time:86786ms step_avg:60.82ms
step:1428/2285 train_time:86846ms step_avg:60.82ms
step:1429/2285 train_time:86909ms step_avg:60.82ms
step:1430/2285 train_time:86969ms step_avg:60.82ms
step:1431/2285 train_time:87031ms step_avg:60.82ms
step:1432/2285 train_time:87091ms step_avg:60.82ms
step:1433/2285 train_time:87153ms step_avg:60.82ms
step:1434/2285 train_time:87212ms step_avg:60.82ms
step:1435/2285 train_time:87275ms step_avg:60.82ms
step:1436/2285 train_time:87334ms step_avg:60.82ms
step:1437/2285 train_time:87397ms step_avg:60.82ms
step:1438/2285 train_time:87458ms step_avg:60.82ms
step:1439/2285 train_time:87520ms step_avg:60.82ms
step:1440/2285 train_time:87581ms step_avg:60.82ms
step:1441/2285 train_time:87643ms step_avg:60.82ms
step:1442/2285 train_time:87703ms step_avg:60.82ms
step:1443/2285 train_time:87766ms step_avg:60.82ms
step:1444/2285 train_time:87826ms step_avg:60.82ms
step:1445/2285 train_time:87889ms step_avg:60.82ms
step:1446/2285 train_time:87949ms step_avg:60.82ms
step:1447/2285 train_time:88012ms step_avg:60.82ms
step:1448/2285 train_time:88071ms step_avg:60.82ms
step:1449/2285 train_time:88133ms step_avg:60.82ms
step:1450/2285 train_time:88193ms step_avg:60.82ms
step:1451/2285 train_time:88256ms step_avg:60.82ms
step:1452/2285 train_time:88316ms step_avg:60.82ms
step:1453/2285 train_time:88378ms step_avg:60.82ms
step:1454/2285 train_time:88438ms step_avg:60.82ms
step:1455/2285 train_time:88500ms step_avg:60.82ms
step:1456/2285 train_time:88560ms step_avg:60.82ms
step:1457/2285 train_time:88624ms step_avg:60.83ms
step:1458/2285 train_time:88684ms step_avg:60.83ms
step:1459/2285 train_time:88746ms step_avg:60.83ms
step:1460/2285 train_time:88807ms step_avg:60.83ms
step:1461/2285 train_time:88869ms step_avg:60.83ms
step:1462/2285 train_time:88929ms step_avg:60.83ms
step:1463/2285 train_time:88991ms step_avg:60.83ms
step:1464/2285 train_time:89051ms step_avg:60.83ms
step:1465/2285 train_time:89113ms step_avg:60.83ms
step:1466/2285 train_time:89173ms step_avg:60.83ms
step:1467/2285 train_time:89235ms step_avg:60.83ms
step:1468/2285 train_time:89294ms step_avg:60.83ms
step:1469/2285 train_time:89357ms step_avg:60.83ms
step:1470/2285 train_time:89417ms step_avg:60.83ms
step:1471/2285 train_time:89480ms step_avg:60.83ms
step:1472/2285 train_time:89540ms step_avg:60.83ms
step:1473/2285 train_time:89602ms step_avg:60.83ms
step:1474/2285 train_time:89662ms step_avg:60.83ms
step:1475/2285 train_time:89726ms step_avg:60.83ms
step:1476/2285 train_time:89785ms step_avg:60.83ms
step:1477/2285 train_time:89848ms step_avg:60.83ms
step:1478/2285 train_time:89908ms step_avg:60.83ms
step:1479/2285 train_time:89971ms step_avg:60.83ms
step:1480/2285 train_time:90030ms step_avg:60.83ms
step:1481/2285 train_time:90093ms step_avg:60.83ms
step:1482/2285 train_time:90152ms step_avg:60.83ms
step:1483/2285 train_time:90215ms step_avg:60.83ms
step:1484/2285 train_time:90274ms step_avg:60.83ms
step:1485/2285 train_time:90337ms step_avg:60.83ms
step:1486/2285 train_time:90397ms step_avg:60.83ms
step:1487/2285 train_time:90460ms step_avg:60.83ms
step:1488/2285 train_time:90520ms step_avg:60.83ms
step:1489/2285 train_time:90583ms step_avg:60.83ms
step:1490/2285 train_time:90643ms step_avg:60.83ms
step:1491/2285 train_time:90705ms step_avg:60.84ms
step:1492/2285 train_time:90766ms step_avg:60.84ms
step:1493/2285 train_time:90828ms step_avg:60.84ms
step:1494/2285 train_time:90888ms step_avg:60.84ms
step:1495/2285 train_time:90951ms step_avg:60.84ms
step:1496/2285 train_time:91010ms step_avg:60.84ms
step:1497/2285 train_time:91072ms step_avg:60.84ms
step:1498/2285 train_time:91132ms step_avg:60.84ms
step:1499/2285 train_time:91195ms step_avg:60.84ms
step:1500/2285 train_time:91255ms step_avg:60.84ms
step:1500/2285 val_loss:3.4276 train_time:91318ms step_avg:60.88ms
step:1501/2285 train_time:91337ms step_avg:60.85ms
step:1502/2285 train_time:91382ms step_avg:60.84ms
step:1503/2285 train_time:91445ms step_avg:60.84ms
step:1504/2285 train_time:91506ms step_avg:60.84ms
step:1505/2285 train_time:91570ms step_avg:60.84ms
step:1506/2285 train_time:91631ms step_avg:60.84ms
step:1507/2285 train_time:91693ms step_avg:60.84ms
step:1508/2285 train_time:91753ms step_avg:60.84ms
step:1509/2285 train_time:91815ms step_avg:60.84ms
step:1510/2285 train_time:91874ms step_avg:60.84ms
step:1511/2285 train_time:91936ms step_avg:60.84ms
step:1512/2285 train_time:91996ms step_avg:60.84ms
step:1513/2285 train_time:92059ms step_avg:60.85ms
step:1514/2285 train_time:92119ms step_avg:60.84ms
step:1515/2285 train_time:92182ms step_avg:60.85ms
step:1516/2285 train_time:92246ms step_avg:60.85ms
step:1517/2285 train_time:92311ms step_avg:60.85ms
step:1518/2285 train_time:92373ms step_avg:60.85ms
step:1519/2285 train_time:92438ms step_avg:60.85ms
step:1520/2285 train_time:92498ms step_avg:60.85ms
step:1521/2285 train_time:92562ms step_avg:60.86ms
step:1522/2285 train_time:92622ms step_avg:60.86ms
step:1523/2285 train_time:92685ms step_avg:60.86ms
step:1524/2285 train_time:92746ms step_avg:60.86ms
step:1525/2285 train_time:92808ms step_avg:60.86ms
step:1526/2285 train_time:92868ms step_avg:60.86ms
step:1527/2285 train_time:92931ms step_avg:60.86ms
step:1528/2285 train_time:92991ms step_avg:60.86ms
step:1529/2285 train_time:93053ms step_avg:60.86ms
step:1530/2285 train_time:93113ms step_avg:60.86ms
step:1531/2285 train_time:93176ms step_avg:60.86ms
step:1532/2285 train_time:93236ms step_avg:60.86ms
step:1533/2285 train_time:93300ms step_avg:60.86ms
step:1534/2285 train_time:93361ms step_avg:60.86ms
step:1535/2285 train_time:93425ms step_avg:60.86ms
step:1536/2285 train_time:93485ms step_avg:60.86ms
step:1537/2285 train_time:93549ms step_avg:60.86ms
step:1538/2285 train_time:93610ms step_avg:60.86ms
step:1539/2285 train_time:93674ms step_avg:60.87ms
step:1540/2285 train_time:93734ms step_avg:60.87ms
step:1541/2285 train_time:93797ms step_avg:60.87ms
step:1542/2285 train_time:93856ms step_avg:60.87ms
step:1543/2285 train_time:93919ms step_avg:60.87ms
step:1544/2285 train_time:93980ms step_avg:60.87ms
step:1545/2285 train_time:94043ms step_avg:60.87ms
step:1546/2285 train_time:94104ms step_avg:60.87ms
step:1547/2285 train_time:94167ms step_avg:60.87ms
step:1548/2285 train_time:94227ms step_avg:60.87ms
step:1549/2285 train_time:94291ms step_avg:60.87ms
step:1550/2285 train_time:94352ms step_avg:60.87ms
step:1551/2285 train_time:94415ms step_avg:60.87ms
step:1552/2285 train_time:94475ms step_avg:60.87ms
step:1553/2285 train_time:94538ms step_avg:60.87ms
step:1554/2285 train_time:94599ms step_avg:60.87ms
step:1555/2285 train_time:94662ms step_avg:60.88ms
step:1556/2285 train_time:94722ms step_avg:60.88ms
step:1557/2285 train_time:94786ms step_avg:60.88ms
step:1558/2285 train_time:94846ms step_avg:60.88ms
step:1559/2285 train_time:94909ms step_avg:60.88ms
step:1560/2285 train_time:94969ms step_avg:60.88ms
step:1561/2285 train_time:95032ms step_avg:60.88ms
step:1562/2285 train_time:95093ms step_avg:60.88ms
step:1563/2285 train_time:95155ms step_avg:60.88ms
step:1564/2285 train_time:95215ms step_avg:60.88ms
step:1565/2285 train_time:95278ms step_avg:60.88ms
step:1566/2285 train_time:95339ms step_avg:60.88ms
step:1567/2285 train_time:95402ms step_avg:60.88ms
step:1568/2285 train_time:95462ms step_avg:60.88ms
step:1569/2285 train_time:95526ms step_avg:60.88ms
step:1570/2285 train_time:95586ms step_avg:60.88ms
step:1571/2285 train_time:95649ms step_avg:60.88ms
step:1572/2285 train_time:95709ms step_avg:60.88ms
step:1573/2285 train_time:95772ms step_avg:60.89ms
step:1574/2285 train_time:95833ms step_avg:60.88ms
step:1575/2285 train_time:95896ms step_avg:60.89ms
step:1576/2285 train_time:95957ms step_avg:60.89ms
step:1577/2285 train_time:96019ms step_avg:60.89ms
step:1578/2285 train_time:96080ms step_avg:60.89ms
step:1579/2285 train_time:96144ms step_avg:60.89ms
step:1580/2285 train_time:96205ms step_avg:60.89ms
step:1581/2285 train_time:96268ms step_avg:60.89ms
step:1582/2285 train_time:96328ms step_avg:60.89ms
step:1583/2285 train_time:96391ms step_avg:60.89ms
step:1584/2285 train_time:96452ms step_avg:60.89ms
step:1585/2285 train_time:96515ms step_avg:60.89ms
step:1586/2285 train_time:96576ms step_avg:60.89ms
step:1587/2285 train_time:96638ms step_avg:60.89ms
step:1588/2285 train_time:96699ms step_avg:60.89ms
step:1589/2285 train_time:96761ms step_avg:60.89ms
step:1590/2285 train_time:96822ms step_avg:60.89ms
step:1591/2285 train_time:96885ms step_avg:60.90ms
step:1592/2285 train_time:96946ms step_avg:60.90ms
step:1593/2285 train_time:97009ms step_avg:60.90ms
step:1594/2285 train_time:97070ms step_avg:60.90ms
step:1595/2285 train_time:97134ms step_avg:60.90ms
step:1596/2285 train_time:97193ms step_avg:60.90ms
step:1597/2285 train_time:97257ms step_avg:60.90ms
step:1598/2285 train_time:97317ms step_avg:60.90ms
step:1599/2285 train_time:97380ms step_avg:60.90ms
step:1600/2285 train_time:97441ms step_avg:60.90ms
step:1601/2285 train_time:97505ms step_avg:60.90ms
step:1602/2285 train_time:97565ms step_avg:60.90ms
step:1603/2285 train_time:97628ms step_avg:60.90ms
step:1604/2285 train_time:97688ms step_avg:60.90ms
step:1605/2285 train_time:97752ms step_avg:60.90ms
step:1606/2285 train_time:97812ms step_avg:60.90ms
step:1607/2285 train_time:97875ms step_avg:60.91ms
step:1608/2285 train_time:97935ms step_avg:60.91ms
step:1609/2285 train_time:97998ms step_avg:60.91ms
step:1610/2285 train_time:98058ms step_avg:60.91ms
step:1611/2285 train_time:98121ms step_avg:60.91ms
step:1612/2285 train_time:98181ms step_avg:60.91ms
step:1613/2285 train_time:98244ms step_avg:60.91ms
step:1614/2285 train_time:98305ms step_avg:60.91ms
step:1615/2285 train_time:98368ms step_avg:60.91ms
step:1616/2285 train_time:98429ms step_avg:60.91ms
step:1617/2285 train_time:98492ms step_avg:60.91ms
step:1618/2285 train_time:98552ms step_avg:60.91ms
step:1619/2285 train_time:98615ms step_avg:60.91ms
step:1620/2285 train_time:98675ms step_avg:60.91ms
step:1621/2285 train_time:98737ms step_avg:60.91ms
step:1622/2285 train_time:98798ms step_avg:60.91ms
step:1623/2285 train_time:98860ms step_avg:60.91ms
step:1624/2285 train_time:98921ms step_avg:60.91ms
step:1625/2285 train_time:98984ms step_avg:60.91ms
step:1626/2285 train_time:99045ms step_avg:60.91ms
step:1627/2285 train_time:99108ms step_avg:60.91ms
step:1628/2285 train_time:99168ms step_avg:60.91ms
step:1629/2285 train_time:99231ms step_avg:60.92ms
step:1630/2285 train_time:99291ms step_avg:60.91ms
step:1631/2285 train_time:99354ms step_avg:60.92ms
step:1632/2285 train_time:99415ms step_avg:60.92ms
step:1633/2285 train_time:99477ms step_avg:60.92ms
step:1634/2285 train_time:99537ms step_avg:60.92ms
step:1635/2285 train_time:99600ms step_avg:60.92ms
step:1636/2285 train_time:99661ms step_avg:60.92ms
step:1637/2285 train_time:99724ms step_avg:60.92ms
step:1638/2285 train_time:99784ms step_avg:60.92ms
step:1639/2285 train_time:99847ms step_avg:60.92ms
step:1640/2285 train_time:99907ms step_avg:60.92ms
step:1641/2285 train_time:99970ms step_avg:60.92ms
step:1642/2285 train_time:100031ms step_avg:60.92ms
step:1643/2285 train_time:100094ms step_avg:60.92ms
step:1644/2285 train_time:100154ms step_avg:60.92ms
step:1645/2285 train_time:100217ms step_avg:60.92ms
step:1646/2285 train_time:100277ms step_avg:60.92ms
step:1647/2285 train_time:100340ms step_avg:60.92ms
step:1648/2285 train_time:100401ms step_avg:60.92ms
step:1649/2285 train_time:100464ms step_avg:60.92ms
step:1650/2285 train_time:100524ms step_avg:60.92ms
step:1651/2285 train_time:100588ms step_avg:60.93ms
step:1652/2285 train_time:100649ms step_avg:60.93ms
step:1653/2285 train_time:100712ms step_avg:60.93ms
step:1654/2285 train_time:100773ms step_avg:60.93ms
step:1655/2285 train_time:100836ms step_avg:60.93ms
step:1656/2285 train_time:100896ms step_avg:60.93ms
step:1657/2285 train_time:100959ms step_avg:60.93ms
step:1658/2285 train_time:101019ms step_avg:60.93ms
step:1659/2285 train_time:101082ms step_avg:60.93ms
step:1660/2285 train_time:101143ms step_avg:60.93ms
step:1661/2285 train_time:101206ms step_avg:60.93ms
step:1662/2285 train_time:101266ms step_avg:60.93ms
step:1663/2285 train_time:101329ms step_avg:60.93ms
step:1664/2285 train_time:101389ms step_avg:60.93ms
step:1665/2285 train_time:101452ms step_avg:60.93ms
step:1666/2285 train_time:101513ms step_avg:60.93ms
step:1667/2285 train_time:101576ms step_avg:60.93ms
step:1668/2285 train_time:101636ms step_avg:60.93ms
step:1669/2285 train_time:101699ms step_avg:60.93ms
step:1670/2285 train_time:101760ms step_avg:60.93ms
step:1671/2285 train_time:101823ms step_avg:60.94ms
step:1672/2285 train_time:101884ms step_avg:60.94ms
step:1673/2285 train_time:101948ms step_avg:60.94ms
step:1674/2285 train_time:102008ms step_avg:60.94ms
step:1675/2285 train_time:102072ms step_avg:60.94ms
step:1676/2285 train_time:102133ms step_avg:60.94ms
step:1677/2285 train_time:102197ms step_avg:60.94ms
step:1678/2285 train_time:102257ms step_avg:60.94ms
step:1679/2285 train_time:102319ms step_avg:60.94ms
step:1680/2285 train_time:102380ms step_avg:60.94ms
step:1681/2285 train_time:102444ms step_avg:60.94ms
step:1682/2285 train_time:102504ms step_avg:60.94ms
step:1683/2285 train_time:102567ms step_avg:60.94ms
step:1684/2285 train_time:102627ms step_avg:60.94ms
step:1685/2285 train_time:102690ms step_avg:60.94ms
step:1686/2285 train_time:102751ms step_avg:60.94ms
step:1687/2285 train_time:102814ms step_avg:60.94ms
step:1688/2285 train_time:102874ms step_avg:60.94ms
step:1689/2285 train_time:102938ms step_avg:60.95ms
step:1690/2285 train_time:102998ms step_avg:60.95ms
step:1691/2285 train_time:103061ms step_avg:60.95ms
step:1692/2285 train_time:103122ms step_avg:60.95ms
step:1693/2285 train_time:103185ms step_avg:60.95ms
step:1694/2285 train_time:103246ms step_avg:60.95ms
step:1695/2285 train_time:103309ms step_avg:60.95ms
step:1696/2285 train_time:103369ms step_avg:60.95ms
step:1697/2285 train_time:103433ms step_avg:60.95ms
step:1698/2285 train_time:103495ms step_avg:60.95ms
step:1699/2285 train_time:103558ms step_avg:60.95ms
step:1700/2285 train_time:103618ms step_avg:60.95ms
step:1701/2285 train_time:103681ms step_avg:60.95ms
step:1702/2285 train_time:103742ms step_avg:60.95ms
step:1703/2285 train_time:103805ms step_avg:60.95ms
step:1704/2285 train_time:103865ms step_avg:60.95ms
step:1705/2285 train_time:103928ms step_avg:60.96ms
step:1706/2285 train_time:103989ms step_avg:60.95ms
step:1707/2285 train_time:104052ms step_avg:60.96ms
step:1708/2285 train_time:104113ms step_avg:60.96ms
step:1709/2285 train_time:104177ms step_avg:60.96ms
step:1710/2285 train_time:104237ms step_avg:60.96ms
step:1711/2285 train_time:104300ms step_avg:60.96ms
step:1712/2285 train_time:104360ms step_avg:60.96ms
step:1713/2285 train_time:104422ms step_avg:60.96ms
step:1714/2285 train_time:104483ms step_avg:60.96ms
step:1715/2285 train_time:104546ms step_avg:60.96ms
step:1716/2285 train_time:104607ms step_avg:60.96ms
step:1717/2285 train_time:104670ms step_avg:60.96ms
step:1718/2285 train_time:104730ms step_avg:60.96ms
step:1719/2285 train_time:104793ms step_avg:60.96ms
step:1720/2285 train_time:104854ms step_avg:60.96ms
step:1721/2285 train_time:104917ms step_avg:60.96ms
step:1722/2285 train_time:104977ms step_avg:60.96ms
step:1723/2285 train_time:105040ms step_avg:60.96ms
step:1724/2285 train_time:105100ms step_avg:60.96ms
step:1725/2285 train_time:105163ms step_avg:60.96ms
step:1726/2285 train_time:105224ms step_avg:60.96ms
step:1727/2285 train_time:105288ms step_avg:60.97ms
step:1728/2285 train_time:105348ms step_avg:60.97ms
step:1729/2285 train_time:105412ms step_avg:60.97ms
step:1730/2285 train_time:105472ms step_avg:60.97ms
step:1731/2285 train_time:105535ms step_avg:60.97ms
step:1732/2285 train_time:105596ms step_avg:60.97ms
step:1733/2285 train_time:105658ms step_avg:60.97ms
step:1734/2285 train_time:105718ms step_avg:60.97ms
step:1735/2285 train_time:105782ms step_avg:60.97ms
step:1736/2285 train_time:105843ms step_avg:60.97ms
step:1737/2285 train_time:105906ms step_avg:60.97ms
step:1738/2285 train_time:105966ms step_avg:60.97ms
step:1739/2285 train_time:106029ms step_avg:60.97ms
step:1740/2285 train_time:106090ms step_avg:60.97ms
step:1741/2285 train_time:106153ms step_avg:60.97ms
step:1742/2285 train_time:106213ms step_avg:60.97ms
step:1743/2285 train_time:106277ms step_avg:60.97ms
step:1744/2285 train_time:106337ms step_avg:60.97ms
step:1745/2285 train_time:106400ms step_avg:60.97ms
step:1746/2285 train_time:106460ms step_avg:60.97ms
step:1747/2285 train_time:106524ms step_avg:60.98ms
step:1748/2285 train_time:106585ms step_avg:60.98ms
step:1749/2285 train_time:106647ms step_avg:60.98ms
step:1750/2285 train_time:106708ms step_avg:60.98ms
step:1750/2285 val_loss:3.3675 train_time:106772ms step_avg:61.01ms
step:1751/2285 train_time:106791ms step_avg:60.99ms
step:1752/2285 train_time:106836ms step_avg:60.98ms
step:1753/2285 train_time:106903ms step_avg:60.98ms
step:1754/2285 train_time:106966ms step_avg:60.98ms
step:1755/2285 train_time:107029ms step_avg:60.98ms
step:1756/2285 train_time:107089ms step_avg:60.98ms
step:1757/2285 train_time:107151ms step_avg:60.99ms
step:1758/2285 train_time:107211ms step_avg:60.98ms
step:1759/2285 train_time:107273ms step_avg:60.99ms
step:1760/2285 train_time:107332ms step_avg:60.98ms
step:1761/2285 train_time:107395ms step_avg:60.99ms
step:1762/2285 train_time:107455ms step_avg:60.98ms
step:1763/2285 train_time:107518ms step_avg:60.99ms
step:1764/2285 train_time:107578ms step_avg:60.99ms
step:1765/2285 train_time:107640ms step_avg:60.99ms
step:1766/2285 train_time:107700ms step_avg:60.99ms
step:1767/2285 train_time:107764ms step_avg:60.99ms
step:1768/2285 train_time:107826ms step_avg:60.99ms
step:1769/2285 train_time:107891ms step_avg:60.99ms
step:1770/2285 train_time:107952ms step_avg:60.99ms
step:1771/2285 train_time:108015ms step_avg:60.99ms
step:1772/2285 train_time:108075ms step_avg:60.99ms
step:1773/2285 train_time:108138ms step_avg:60.99ms
step:1774/2285 train_time:108198ms step_avg:60.99ms
step:1775/2285 train_time:108260ms step_avg:60.99ms
step:1776/2285 train_time:108320ms step_avg:60.99ms
step:1777/2285 train_time:108383ms step_avg:60.99ms
step:1778/2285 train_time:108443ms step_avg:60.99ms
step:1779/2285 train_time:108506ms step_avg:60.99ms
step:1780/2285 train_time:108567ms step_avg:60.99ms
step:1781/2285 train_time:108629ms step_avg:60.99ms
step:1782/2285 train_time:108689ms step_avg:60.99ms
step:1783/2285 train_time:108752ms step_avg:60.99ms
step:1784/2285 train_time:108813ms step_avg:60.99ms
step:1785/2285 train_time:108877ms step_avg:61.00ms
step:1786/2285 train_time:108938ms step_avg:61.00ms
step:1787/2285 train_time:109001ms step_avg:61.00ms
step:1788/2285 train_time:109062ms step_avg:61.00ms
step:1789/2285 train_time:109125ms step_avg:61.00ms
step:1790/2285 train_time:109185ms step_avg:61.00ms
step:1791/2285 train_time:109247ms step_avg:61.00ms
step:1792/2285 train_time:109308ms step_avg:61.00ms
step:1793/2285 train_time:109371ms step_avg:61.00ms
step:1794/2285 train_time:109431ms step_avg:61.00ms
step:1795/2285 train_time:109493ms step_avg:61.00ms
step:1796/2285 train_time:109553ms step_avg:61.00ms
step:1797/2285 train_time:109615ms step_avg:61.00ms
step:1798/2285 train_time:109676ms step_avg:61.00ms
step:1799/2285 train_time:109739ms step_avg:61.00ms
step:1800/2285 train_time:109800ms step_avg:61.00ms
step:1801/2285 train_time:109864ms step_avg:61.00ms
step:1802/2285 train_time:109926ms step_avg:61.00ms
step:1803/2285 train_time:109989ms step_avg:61.00ms
step:1804/2285 train_time:110050ms step_avg:61.00ms
step:1805/2285 train_time:110112ms step_avg:61.00ms
step:1806/2285 train_time:110172ms step_avg:61.00ms
step:1807/2285 train_time:110235ms step_avg:61.00ms
step:1808/2285 train_time:110295ms step_avg:61.00ms
step:1809/2285 train_time:110359ms step_avg:61.01ms
step:1810/2285 train_time:110419ms step_avg:61.00ms
step:1811/2285 train_time:110481ms step_avg:61.01ms
step:1812/2285 train_time:110541ms step_avg:61.00ms
step:1813/2285 train_time:110604ms step_avg:61.01ms
step:1814/2285 train_time:110665ms step_avg:61.01ms
step:1815/2285 train_time:110728ms step_avg:61.01ms
step:1816/2285 train_time:110790ms step_avg:61.01ms
step:1817/2285 train_time:110853ms step_avg:61.01ms
step:1818/2285 train_time:110914ms step_avg:61.01ms
step:1819/2285 train_time:110978ms step_avg:61.01ms
step:1820/2285 train_time:111038ms step_avg:61.01ms
step:1821/2285 train_time:111101ms step_avg:61.01ms
step:1822/2285 train_time:111161ms step_avg:61.01ms
step:1823/2285 train_time:111224ms step_avg:61.01ms
step:1824/2285 train_time:111284ms step_avg:61.01ms
step:1825/2285 train_time:111347ms step_avg:61.01ms
step:1826/2285 train_time:111408ms step_avg:61.01ms
step:1827/2285 train_time:111471ms step_avg:61.01ms
step:1828/2285 train_time:111531ms step_avg:61.01ms
step:1829/2285 train_time:111594ms step_avg:61.01ms
step:1830/2285 train_time:111655ms step_avg:61.01ms
step:1831/2285 train_time:111718ms step_avg:61.01ms
step:1832/2285 train_time:111779ms step_avg:61.01ms
step:1833/2285 train_time:111841ms step_avg:61.02ms
step:1834/2285 train_time:111901ms step_avg:61.01ms
step:1835/2285 train_time:111964ms step_avg:61.02ms
step:1836/2285 train_time:112025ms step_avg:61.02ms
step:1837/2285 train_time:112089ms step_avg:61.02ms
step:1838/2285 train_time:112150ms step_avg:61.02ms
step:1839/2285 train_time:112212ms step_avg:61.02ms
step:1840/2285 train_time:112273ms step_avg:61.02ms
step:1841/2285 train_time:112335ms step_avg:61.02ms
step:1842/2285 train_time:112396ms step_avg:61.02ms
step:1843/2285 train_time:112459ms step_avg:61.02ms
step:1844/2285 train_time:112519ms step_avg:61.02ms
step:1845/2285 train_time:112581ms step_avg:61.02ms
step:1846/2285 train_time:112642ms step_avg:61.02ms
step:1847/2285 train_time:112705ms step_avg:61.02ms
step:1848/2285 train_time:112766ms step_avg:61.02ms
step:1849/2285 train_time:112830ms step_avg:61.02ms
step:1850/2285 train_time:112890ms step_avg:61.02ms
step:1851/2285 train_time:112953ms step_avg:61.02ms
step:1852/2285 train_time:113013ms step_avg:61.02ms
step:1853/2285 train_time:113077ms step_avg:61.02ms
step:1854/2285 train_time:113137ms step_avg:61.02ms
step:1855/2285 train_time:113200ms step_avg:61.02ms
step:1856/2285 train_time:113260ms step_avg:61.02ms
step:1857/2285 train_time:113324ms step_avg:61.03ms
step:1858/2285 train_time:113384ms step_avg:61.02ms
step:1859/2285 train_time:113447ms step_avg:61.03ms
step:1860/2285 train_time:113507ms step_avg:61.03ms
step:1861/2285 train_time:113570ms step_avg:61.03ms
step:1862/2285 train_time:113631ms step_avg:61.03ms
step:1863/2285 train_time:113693ms step_avg:61.03ms
step:1864/2285 train_time:113754ms step_avg:61.03ms
step:1865/2285 train_time:113817ms step_avg:61.03ms
step:1866/2285 train_time:113877ms step_avg:61.03ms
step:1867/2285 train_time:113940ms step_avg:61.03ms
step:1868/2285 train_time:114000ms step_avg:61.03ms
step:1869/2285 train_time:114063ms step_avg:61.03ms
step:1870/2285 train_time:114124ms step_avg:61.03ms
step:1871/2285 train_time:114186ms step_avg:61.03ms
step:1872/2285 train_time:114247ms step_avg:61.03ms
step:1873/2285 train_time:114310ms step_avg:61.03ms
step:1874/2285 train_time:114371ms step_avg:61.03ms
step:1875/2285 train_time:114433ms step_avg:61.03ms
step:1876/2285 train_time:114494ms step_avg:61.03ms
step:1877/2285 train_time:114556ms step_avg:61.03ms
step:1878/2285 train_time:114617ms step_avg:61.03ms
step:1879/2285 train_time:114680ms step_avg:61.03ms
step:1880/2285 train_time:114740ms step_avg:61.03ms
step:1881/2285 train_time:114804ms step_avg:61.03ms
step:1882/2285 train_time:114864ms step_avg:61.03ms
step:1883/2285 train_time:114927ms step_avg:61.03ms
step:1884/2285 train_time:114987ms step_avg:61.03ms
step:1885/2285 train_time:115051ms step_avg:61.03ms
step:1886/2285 train_time:115111ms step_avg:61.03ms
step:1887/2285 train_time:115174ms step_avg:61.04ms
step:1888/2285 train_time:115234ms step_avg:61.03ms
step:1889/2285 train_time:115297ms step_avg:61.04ms
step:1890/2285 train_time:115357ms step_avg:61.04ms
step:1891/2285 train_time:115421ms step_avg:61.04ms
step:1892/2285 train_time:115481ms step_avg:61.04ms
step:1893/2285 train_time:115544ms step_avg:61.04ms
step:1894/2285 train_time:115604ms step_avg:61.04ms
step:1895/2285 train_time:115667ms step_avg:61.04ms
step:1896/2285 train_time:115728ms step_avg:61.04ms
step:1897/2285 train_time:115792ms step_avg:61.04ms
step:1898/2285 train_time:115852ms step_avg:61.04ms
step:1899/2285 train_time:115916ms step_avg:61.04ms
step:1900/2285 train_time:115976ms step_avg:61.04ms
step:1901/2285 train_time:116039ms step_avg:61.04ms
step:1902/2285 train_time:116099ms step_avg:61.04ms
step:1903/2285 train_time:116162ms step_avg:61.04ms
step:1904/2285 train_time:116223ms step_avg:61.04ms
step:1905/2285 train_time:116286ms step_avg:61.04ms
step:1906/2285 train_time:116347ms step_avg:61.04ms
step:1907/2285 train_time:116410ms step_avg:61.04ms
step:1908/2285 train_time:116471ms step_avg:61.04ms
step:1909/2285 train_time:116533ms step_avg:61.04ms
step:1910/2285 train_time:116594ms step_avg:61.04ms
step:1911/2285 train_time:116657ms step_avg:61.05ms
step:1912/2285 train_time:116718ms step_avg:61.04ms
step:1913/2285 train_time:116780ms step_avg:61.05ms
step:1914/2285 train_time:116841ms step_avg:61.05ms
step:1915/2285 train_time:116904ms step_avg:61.05ms
step:1916/2285 train_time:116965ms step_avg:61.05ms
step:1917/2285 train_time:117028ms step_avg:61.05ms
step:1918/2285 train_time:117089ms step_avg:61.05ms
step:1919/2285 train_time:117151ms step_avg:61.05ms
step:1920/2285 train_time:117212ms step_avg:61.05ms
step:1921/2285 train_time:117276ms step_avg:61.05ms
step:1922/2285 train_time:117337ms step_avg:61.05ms
step:1923/2285 train_time:117399ms step_avg:61.05ms
step:1924/2285 train_time:117459ms step_avg:61.05ms
step:1925/2285 train_time:117522ms step_avg:61.05ms
step:1926/2285 train_time:117582ms step_avg:61.05ms
step:1927/2285 train_time:117645ms step_avg:61.05ms
step:1928/2285 train_time:117706ms step_avg:61.05ms
step:1929/2285 train_time:117771ms step_avg:61.05ms
step:1930/2285 train_time:117831ms step_avg:61.05ms
step:1931/2285 train_time:117894ms step_avg:61.05ms
step:1932/2285 train_time:117955ms step_avg:61.05ms
step:1933/2285 train_time:118017ms step_avg:61.05ms
step:1934/2285 train_time:118077ms step_avg:61.05ms
step:1935/2285 train_time:118141ms step_avg:61.05ms
step:1936/2285 train_time:118201ms step_avg:61.05ms
step:1937/2285 train_time:118264ms step_avg:61.06ms
step:1938/2285 train_time:118325ms step_avg:61.06ms
step:1939/2285 train_time:118388ms step_avg:61.06ms
step:1940/2285 train_time:118448ms step_avg:61.06ms
step:1941/2285 train_time:118511ms step_avg:61.06ms
step:1942/2285 train_time:118571ms step_avg:61.06ms
step:1943/2285 train_time:118635ms step_avg:61.06ms
step:1944/2285 train_time:118695ms step_avg:61.06ms
step:1945/2285 train_time:118757ms step_avg:61.06ms
step:1946/2285 train_time:118818ms step_avg:61.06ms
step:1947/2285 train_time:118880ms step_avg:61.06ms
step:1948/2285 train_time:118940ms step_avg:61.06ms
step:1949/2285 train_time:119004ms step_avg:61.06ms
step:1950/2285 train_time:119065ms step_avg:61.06ms
step:1951/2285 train_time:119127ms step_avg:61.06ms
step:1952/2285 train_time:119188ms step_avg:61.06ms
step:1953/2285 train_time:119251ms step_avg:61.06ms
step:1954/2285 train_time:119311ms step_avg:61.06ms
step:1955/2285 train_time:119374ms step_avg:61.06ms
step:1956/2285 train_time:119434ms step_avg:61.06ms
step:1957/2285 train_time:119497ms step_avg:61.06ms
step:1958/2285 train_time:119557ms step_avg:61.06ms
step:1959/2285 train_time:119620ms step_avg:61.06ms
step:1960/2285 train_time:119680ms step_avg:61.06ms
step:1961/2285 train_time:119743ms step_avg:61.06ms
step:1962/2285 train_time:119803ms step_avg:61.06ms
step:1963/2285 train_time:119866ms step_avg:61.06ms
step:1964/2285 train_time:119926ms step_avg:61.06ms
step:1965/2285 train_time:119989ms step_avg:61.06ms
step:1966/2285 train_time:120050ms step_avg:61.06ms
step:1967/2285 train_time:120112ms step_avg:61.06ms
step:1968/2285 train_time:120172ms step_avg:61.06ms
step:1969/2285 train_time:120235ms step_avg:61.06ms
step:1970/2285 train_time:120295ms step_avg:61.06ms
step:1971/2285 train_time:120359ms step_avg:61.06ms
step:1972/2285 train_time:120419ms step_avg:61.06ms
step:1973/2285 train_time:120482ms step_avg:61.07ms
step:1974/2285 train_time:120543ms step_avg:61.07ms
step:1975/2285 train_time:120606ms step_avg:61.07ms
step:1976/2285 train_time:120666ms step_avg:61.07ms
step:1977/2285 train_time:120729ms step_avg:61.07ms
step:1978/2285 train_time:120789ms step_avg:61.07ms
step:1979/2285 train_time:120853ms step_avg:61.07ms
step:1980/2285 train_time:120913ms step_avg:61.07ms
step:1981/2285 train_time:120977ms step_avg:61.07ms
step:1982/2285 train_time:121037ms step_avg:61.07ms
step:1983/2285 train_time:121100ms step_avg:61.07ms
step:1984/2285 train_time:121160ms step_avg:61.07ms
step:1985/2285 train_time:121224ms step_avg:61.07ms
step:1986/2285 train_time:121285ms step_avg:61.07ms
step:1987/2285 train_time:121347ms step_avg:61.07ms
step:1988/2285 train_time:121408ms step_avg:61.07ms
step:1989/2285 train_time:121472ms step_avg:61.07ms
step:1990/2285 train_time:121532ms step_avg:61.07ms
step:1991/2285 train_time:121595ms step_avg:61.07ms
step:1992/2285 train_time:121656ms step_avg:61.07ms
step:1993/2285 train_time:121719ms step_avg:61.07ms
step:1994/2285 train_time:121779ms step_avg:61.07ms
step:1995/2285 train_time:121842ms step_avg:61.07ms
step:1996/2285 train_time:121902ms step_avg:61.07ms
step:1997/2285 train_time:121965ms step_avg:61.07ms
step:1998/2285 train_time:122026ms step_avg:61.07ms
step:1999/2285 train_time:122089ms step_avg:61.07ms
step:2000/2285 train_time:122149ms step_avg:61.07ms
step:2000/2285 val_loss:3.3217 train_time:122213ms step_avg:61.11ms
step:2001/2285 train_time:122231ms step_avg:61.09ms
step:2002/2285 train_time:122277ms step_avg:61.08ms
step:2003/2285 train_time:122343ms step_avg:61.08ms
step:2004/2285 train_time:122405ms step_avg:61.08ms
step:2005/2285 train_time:122468ms step_avg:61.08ms
step:2006/2285 train_time:122528ms step_avg:61.08ms
step:2007/2285 train_time:122591ms step_avg:61.08ms
step:2008/2285 train_time:122650ms step_avg:61.08ms
step:2009/2285 train_time:122712ms step_avg:61.08ms
step:2010/2285 train_time:122772ms step_avg:61.08ms
step:2011/2285 train_time:122834ms step_avg:61.08ms
step:2012/2285 train_time:122894ms step_avg:61.08ms
step:2013/2285 train_time:122956ms step_avg:61.08ms
step:2014/2285 train_time:123016ms step_avg:61.08ms
step:2015/2285 train_time:123079ms step_avg:61.08ms
step:2016/2285 train_time:123139ms step_avg:61.08ms
step:2017/2285 train_time:123202ms step_avg:61.08ms
step:2018/2285 train_time:123264ms step_avg:61.08ms
step:2019/2285 train_time:123329ms step_avg:61.08ms
step:2020/2285 train_time:123390ms step_avg:61.08ms
step:2021/2285 train_time:123454ms step_avg:61.09ms
step:2022/2285 train_time:123514ms step_avg:61.09ms
step:2023/2285 train_time:123577ms step_avg:61.09ms
step:2024/2285 train_time:123637ms step_avg:61.09ms
step:2025/2285 train_time:123699ms step_avg:61.09ms
step:2026/2285 train_time:123759ms step_avg:61.09ms
step:2027/2285 train_time:123822ms step_avg:61.09ms
step:2028/2285 train_time:123882ms step_avg:61.09ms
step:2029/2285 train_time:123945ms step_avg:61.09ms
step:2030/2285 train_time:124005ms step_avg:61.09ms
step:2031/2285 train_time:124068ms step_avg:61.09ms
step:2032/2285 train_time:124128ms step_avg:61.09ms
step:2033/2285 train_time:124192ms step_avg:61.09ms
step:2034/2285 train_time:124253ms step_avg:61.09ms
step:2035/2285 train_time:124317ms step_avg:61.09ms
step:2036/2285 train_time:124378ms step_avg:61.09ms
step:2037/2285 train_time:124441ms step_avg:61.09ms
step:2038/2285 train_time:124501ms step_avg:61.09ms
step:2039/2285 train_time:124565ms step_avg:61.09ms
step:2040/2285 train_time:124625ms step_avg:61.09ms
step:2041/2285 train_time:124688ms step_avg:61.09ms
step:2042/2285 train_time:124749ms step_avg:61.09ms
step:2043/2285 train_time:124811ms step_avg:61.09ms
step:2044/2285 train_time:124872ms step_avg:61.09ms
step:2045/2285 train_time:124935ms step_avg:61.09ms
step:2046/2285 train_time:124995ms step_avg:61.09ms
step:2047/2285 train_time:125057ms step_avg:61.09ms
step:2048/2285 train_time:125117ms step_avg:61.09ms
step:2049/2285 train_time:125180ms step_avg:61.09ms
step:2050/2285 train_time:125241ms step_avg:61.09ms
step:2051/2285 train_time:125305ms step_avg:61.09ms
step:2052/2285 train_time:125366ms step_avg:61.09ms
step:2053/2285 train_time:125430ms step_avg:61.10ms
step:2054/2285 train_time:125491ms step_avg:61.10ms
step:2055/2285 train_time:125555ms step_avg:61.10ms
step:2056/2285 train_time:125616ms step_avg:61.10ms
step:2057/2285 train_time:125679ms step_avg:61.10ms
step:2058/2285 train_time:125739ms step_avg:61.10ms
step:2059/2285 train_time:125801ms step_avg:61.10ms
step:2060/2285 train_time:125862ms step_avg:61.10ms
step:2061/2285 train_time:125925ms step_avg:61.10ms
step:2062/2285 train_time:125986ms step_avg:61.10ms
step:2063/2285 train_time:126048ms step_avg:61.10ms
step:2064/2285 train_time:126108ms step_avg:61.10ms
step:2065/2285 train_time:126171ms step_avg:61.10ms
step:2066/2285 train_time:126231ms step_avg:61.10ms
step:2067/2285 train_time:126295ms step_avg:61.10ms
step:2068/2285 train_time:126356ms step_avg:61.10ms
step:2069/2285 train_time:126419ms step_avg:61.10ms
step:2070/2285 train_time:126480ms step_avg:61.10ms
step:2071/2285 train_time:126543ms step_avg:61.10ms
step:2072/2285 train_time:126604ms step_avg:61.10ms
step:2073/2285 train_time:126667ms step_avg:61.10ms
step:2074/2285 train_time:126728ms step_avg:61.10ms
step:2075/2285 train_time:126791ms step_avg:61.10ms
step:2076/2285 train_time:126850ms step_avg:61.10ms
step:2077/2285 train_time:126914ms step_avg:61.10ms
step:2078/2285 train_time:126975ms step_avg:61.10ms
step:2079/2285 train_time:127038ms step_avg:61.11ms
step:2080/2285 train_time:127098ms step_avg:61.10ms
step:2081/2285 train_time:127160ms step_avg:61.11ms
step:2082/2285 train_time:127221ms step_avg:61.10ms
step:2083/2285 train_time:127284ms step_avg:61.11ms
step:2084/2285 train_time:127345ms step_avg:61.11ms
step:2085/2285 train_time:127408ms step_avg:61.11ms
step:2086/2285 train_time:127470ms step_avg:61.11ms
step:2087/2285 train_time:127533ms step_avg:61.11ms
step:2088/2285 train_time:127593ms step_avg:61.11ms
step:2089/2285 train_time:127656ms step_avg:61.11ms
step:2090/2285 train_time:127716ms step_avg:61.11ms
step:2091/2285 train_time:127779ms step_avg:61.11ms
step:2092/2285 train_time:127839ms step_avg:61.11ms
step:2093/2285 train_time:127902ms step_avg:61.11ms
step:2094/2285 train_time:127963ms step_avg:61.11ms
step:2095/2285 train_time:128027ms step_avg:61.11ms
step:2096/2285 train_time:128087ms step_avg:61.11ms
step:2097/2285 train_time:128150ms step_avg:61.11ms
step:2098/2285 train_time:128210ms step_avg:61.11ms
step:2099/2285 train_time:128274ms step_avg:61.11ms
step:2100/2285 train_time:128334ms step_avg:61.11ms
step:2101/2285 train_time:128398ms step_avg:61.11ms
step:2102/2285 train_time:128458ms step_avg:61.11ms
step:2103/2285 train_time:128521ms step_avg:61.11ms
step:2104/2285 train_time:128582ms step_avg:61.11ms
step:2105/2285 train_time:128645ms step_avg:61.11ms
step:2106/2285 train_time:128706ms step_avg:61.11ms
step:2107/2285 train_time:128770ms step_avg:61.12ms
step:2108/2285 train_time:128830ms step_avg:61.11ms
step:2109/2285 train_time:128893ms step_avg:61.12ms
step:2110/2285 train_time:128954ms step_avg:61.12ms
step:2111/2285 train_time:129016ms step_avg:61.12ms
step:2112/2285 train_time:129077ms step_avg:61.12ms
step:2113/2285 train_time:129140ms step_avg:61.12ms
step:2114/2285 train_time:129200ms step_avg:61.12ms
step:2115/2285 train_time:129263ms step_avg:61.12ms
step:2116/2285 train_time:129325ms step_avg:61.12ms
step:2117/2285 train_time:129388ms step_avg:61.12ms
step:2118/2285 train_time:129448ms step_avg:61.12ms
step:2119/2285 train_time:129512ms step_avg:61.12ms
step:2120/2285 train_time:129573ms step_avg:61.12ms
step:2121/2285 train_time:129636ms step_avg:61.12ms
step:2122/2285 train_time:129696ms step_avg:61.12ms
step:2123/2285 train_time:129759ms step_avg:61.12ms
step:2124/2285 train_time:129819ms step_avg:61.12ms
step:2125/2285 train_time:129882ms step_avg:61.12ms
step:2126/2285 train_time:129942ms step_avg:61.12ms
step:2127/2285 train_time:130005ms step_avg:61.12ms
step:2128/2285 train_time:130065ms step_avg:61.12ms
step:2129/2285 train_time:130128ms step_avg:61.12ms
step:2130/2285 train_time:130189ms step_avg:61.12ms
step:2131/2285 train_time:130252ms step_avg:61.12ms
step:2132/2285 train_time:130313ms step_avg:61.12ms
step:2133/2285 train_time:130377ms step_avg:61.12ms
step:2134/2285 train_time:130437ms step_avg:61.12ms
step:2135/2285 train_time:130500ms step_avg:61.12ms
step:2136/2285 train_time:130560ms step_avg:61.12ms
step:2137/2285 train_time:130624ms step_avg:61.12ms
step:2138/2285 train_time:130685ms step_avg:61.12ms
step:2139/2285 train_time:130748ms step_avg:61.13ms
step:2140/2285 train_time:130808ms step_avg:61.13ms
step:2141/2285 train_time:130871ms step_avg:61.13ms
step:2142/2285 train_time:130932ms step_avg:61.13ms
step:2143/2285 train_time:130995ms step_avg:61.13ms
step:2144/2285 train_time:131055ms step_avg:61.13ms
step:2145/2285 train_time:131118ms step_avg:61.13ms
step:2146/2285 train_time:131178ms step_avg:61.13ms
step:2147/2285 train_time:131242ms step_avg:61.13ms
step:2148/2285 train_time:131303ms step_avg:61.13ms
step:2149/2285 train_time:131366ms step_avg:61.13ms
step:2150/2285 train_time:131427ms step_avg:61.13ms
step:2151/2285 train_time:131491ms step_avg:61.13ms
step:2152/2285 train_time:131552ms step_avg:61.13ms
step:2153/2285 train_time:131615ms step_avg:61.13ms
step:2154/2285 train_time:131676ms step_avg:61.13ms
step:2155/2285 train_time:131739ms step_avg:61.13ms
step:2156/2285 train_time:131799ms step_avg:61.13ms
step:2157/2285 train_time:131861ms step_avg:61.13ms
step:2158/2285 train_time:131922ms step_avg:61.13ms
step:2159/2285 train_time:131986ms step_avg:61.13ms
step:2160/2285 train_time:132047ms step_avg:61.13ms
step:2161/2285 train_time:132110ms step_avg:61.13ms
step:2162/2285 train_time:132170ms step_avg:61.13ms
step:2163/2285 train_time:132233ms step_avg:61.13ms
step:2164/2285 train_time:132294ms step_avg:61.13ms
step:2165/2285 train_time:132356ms step_avg:61.13ms
step:2166/2285 train_time:132416ms step_avg:61.13ms
step:2167/2285 train_time:132479ms step_avg:61.13ms
step:2168/2285 train_time:132539ms step_avg:61.13ms
step:2169/2285 train_time:132603ms step_avg:61.14ms
step:2170/2285 train_time:132663ms step_avg:61.13ms
step:2171/2285 train_time:132726ms step_avg:61.14ms
step:2172/2285 train_time:132786ms step_avg:61.14ms
step:2173/2285 train_time:132849ms step_avg:61.14ms
step:2174/2285 train_time:132910ms step_avg:61.14ms
step:2175/2285 train_time:132973ms step_avg:61.14ms
step:2176/2285 train_time:133033ms step_avg:61.14ms
step:2177/2285 train_time:133096ms step_avg:61.14ms
step:2178/2285 train_time:133157ms step_avg:61.14ms
step:2179/2285 train_time:133219ms step_avg:61.14ms
step:2180/2285 train_time:133279ms step_avg:61.14ms
step:2181/2285 train_time:133342ms step_avg:61.14ms
step:2182/2285 train_time:133403ms step_avg:61.14ms
step:2183/2285 train_time:133465ms step_avg:61.14ms
step:2184/2285 train_time:133526ms step_avg:61.14ms
step:2185/2285 train_time:133590ms step_avg:61.14ms
step:2186/2285 train_time:133650ms step_avg:61.14ms
step:2187/2285 train_time:133713ms step_avg:61.14ms
step:2188/2285 train_time:133774ms step_avg:61.14ms
step:2189/2285 train_time:133837ms step_avg:61.14ms
step:2190/2285 train_time:133898ms step_avg:61.14ms
step:2191/2285 train_time:133960ms step_avg:61.14ms
step:2192/2285 train_time:134020ms step_avg:61.14ms
step:2193/2285 train_time:134083ms step_avg:61.14ms
step:2194/2285 train_time:134144ms step_avg:61.14ms
step:2195/2285 train_time:134207ms step_avg:61.14ms
step:2196/2285 train_time:134268ms step_avg:61.14ms
step:2197/2285 train_time:134331ms step_avg:61.14ms
step:2198/2285 train_time:134392ms step_avg:61.14ms
step:2199/2285 train_time:134455ms step_avg:61.14ms
step:2200/2285 train_time:134515ms step_avg:61.14ms
step:2201/2285 train_time:134579ms step_avg:61.14ms
step:2202/2285 train_time:134639ms step_avg:61.14ms
step:2203/2285 train_time:134702ms step_avg:61.14ms
step:2204/2285 train_time:134763ms step_avg:61.14ms
step:2205/2285 train_time:134826ms step_avg:61.15ms
step:2206/2285 train_time:134887ms step_avg:61.15ms
step:2207/2285 train_time:134950ms step_avg:61.15ms
step:2208/2285 train_time:135011ms step_avg:61.15ms
step:2209/2285 train_time:135074ms step_avg:61.15ms
step:2210/2285 train_time:135135ms step_avg:61.15ms
step:2211/2285 train_time:135199ms step_avg:61.15ms
step:2212/2285 train_time:135259ms step_avg:61.15ms
step:2213/2285 train_time:135321ms step_avg:61.15ms
step:2214/2285 train_time:135383ms step_avg:61.15ms
step:2215/2285 train_time:135446ms step_avg:61.15ms
step:2216/2285 train_time:135507ms step_avg:61.15ms
step:2217/2285 train_time:135570ms step_avg:61.15ms
step:2218/2285 train_time:135631ms step_avg:61.15ms
step:2219/2285 train_time:135694ms step_avg:61.15ms
step:2220/2285 train_time:135755ms step_avg:61.15ms
step:2221/2285 train_time:135818ms step_avg:61.15ms
step:2222/2285 train_time:135878ms step_avg:61.15ms
step:2223/2285 train_time:135942ms step_avg:61.15ms
step:2224/2285 train_time:136002ms step_avg:61.15ms
step:2225/2285 train_time:136066ms step_avg:61.15ms
step:2226/2285 train_time:136127ms step_avg:61.15ms
step:2227/2285 train_time:136190ms step_avg:61.15ms
step:2228/2285 train_time:136250ms step_avg:61.15ms
step:2229/2285 train_time:136314ms step_avg:61.15ms
step:2230/2285 train_time:136375ms step_avg:61.15ms
step:2231/2285 train_time:136438ms step_avg:61.16ms
step:2232/2285 train_time:136498ms step_avg:61.15ms
step:2233/2285 train_time:136560ms step_avg:61.16ms
step:2234/2285 train_time:136621ms step_avg:61.16ms
step:2235/2285 train_time:136685ms step_avg:61.16ms
step:2236/2285 train_time:136745ms step_avg:61.16ms
step:2237/2285 train_time:136808ms step_avg:61.16ms
step:2238/2285 train_time:136868ms step_avg:61.16ms
step:2239/2285 train_time:136931ms step_avg:61.16ms
step:2240/2285 train_time:136991ms step_avg:61.16ms
step:2241/2285 train_time:137054ms step_avg:61.16ms
step:2242/2285 train_time:137115ms step_avg:61.16ms
step:2243/2285 train_time:137178ms step_avg:61.16ms
step:2244/2285 train_time:137238ms step_avg:61.16ms
step:2245/2285 train_time:137301ms step_avg:61.16ms
step:2246/2285 train_time:137361ms step_avg:61.16ms
step:2247/2285 train_time:137425ms step_avg:61.16ms
step:2248/2285 train_time:137486ms step_avg:61.16ms
step:2249/2285 train_time:137550ms step_avg:61.16ms
step:2250/2285 train_time:137611ms step_avg:61.16ms
step:2250/2285 val_loss:3.2846 train_time:137675ms step_avg:61.19ms
step:2251/2285 train_time:137695ms step_avg:61.17ms
step:2252/2285 train_time:137736ms step_avg:61.16ms
step:2253/2285 train_time:137799ms step_avg:61.16ms
step:2254/2285 train_time:137860ms step_avg:61.16ms
step:2255/2285 train_time:137924ms step_avg:61.16ms
step:2256/2285 train_time:137985ms step_avg:61.16ms
step:2257/2285 train_time:138047ms step_avg:61.16ms
step:2258/2285 train_time:138107ms step_avg:61.16ms
step:2259/2285 train_time:138169ms step_avg:61.16ms
step:2260/2285 train_time:138229ms step_avg:61.16ms
step:2261/2285 train_time:138291ms step_avg:61.16ms
step:2262/2285 train_time:138351ms step_avg:61.16ms
step:2263/2285 train_time:138414ms step_avg:61.16ms
step:2264/2285 train_time:138474ms step_avg:61.16ms
step:2265/2285 train_time:138537ms step_avg:61.16ms
step:2266/2285 train_time:138602ms step_avg:61.17ms
step:2267/2285 train_time:138669ms step_avg:61.17ms
step:2268/2285 train_time:138731ms step_avg:61.17ms
step:2269/2285 train_time:138795ms step_avg:61.17ms
step:2270/2285 train_time:138856ms step_avg:61.17ms
step:2271/2285 train_time:138918ms step_avg:61.17ms
step:2272/2285 train_time:138978ms step_avg:61.17ms
step:2273/2285 train_time:139041ms step_avg:61.17ms
step:2274/2285 train_time:139102ms step_avg:61.17ms
step:2275/2285 train_time:139164ms step_avg:61.17ms
step:2276/2285 train_time:139223ms step_avg:61.17ms
step:2277/2285 train_time:139286ms step_avg:61.17ms
step:2278/2285 train_time:139346ms step_avg:61.17ms
step:2279/2285 train_time:139408ms step_avg:61.17ms
step:2280/2285 train_time:139469ms step_avg:61.17ms
step:2281/2285 train_time:139533ms step_avg:61.17ms
step:2282/2285 train_time:139595ms step_avg:61.17ms
step:2283/2285 train_time:139660ms step_avg:61.17ms
step:2284/2285 train_time:139721ms step_avg:61.17ms
step:2285/2285 train_time:139785ms step_avg:61.18ms
step:2285/2285 val_loss:3.2783 train_time:139847ms step_avg:61.20ms
peak memory allocated: 29626 MiB reserved: 50528 MiB

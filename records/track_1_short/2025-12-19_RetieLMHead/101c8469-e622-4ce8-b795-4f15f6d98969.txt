import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from collections import defaultdict
from itertools import accumulate
from pathlib import Path
import gc

os.environ["PYTORCH_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64


# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng

@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor, split_baddbmm: bool = False):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    # Select batched vs unbatched
    if split_baddbmm:
        BX_matmul = torch.bmm if X.ndim > 2 else torch.mm
    else:
        aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A

        # Referencing X twice causes pytorch to make a defensive copy,
        # resulting in a cudaMemcpyAsync in baddbmm.
        # For large matrices (i.e., the mlp weights), it's faster to split
        # the operation into two kernels to avoid this.
        if split_baddbmm:
            BX_matmul(B, X, out=C)  # C = B @ X
            C.add_(X, alpha=a)      # C = C + a*X  (in-place, X only read)
        else:
            aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X

        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X


# -----------------------------------------------------------------------------
# Compiled helpers for NorMuon by @chrisjmccormick

@torch.compile(dynamic=False, fullgraph=True)
def cautious_wd_and_update_inplace(p, v, wd_tensor, lr_tensor):
    """Cautious weight decay + parameter update. wd_tensor and lr_tensor are 0-D CPU tensors."""
    mask = (v * p) >= 0
    wd_factor = wd_tensor.to(p.dtype)
    lr_factor = lr_tensor.to(p.dtype)
    p.copy_(p - (p * mask * wd_factor * lr_factor) - (v * lr_factor))


@torch.compile(dynamic=False, fullgraph=True)
def apply_normuon_variance_reduction(v_chunk, second_momentum_buffer, beta2, red_dim):
    """NorMuon variance reduction. Algebraically fuses the normalization steps to minimize memory ops."""
    v_mean = v_chunk.float().square().mean(dim=red_dim, keepdim=True)
    red_dim_size = v_chunk.size(red_dim)
    v_norm_sq = v_mean.sum(dim=(-2, -1), keepdim=True).mul_(red_dim_size)
    v_norm = v_norm_sq.sqrt_()
    second_momentum_buffer.lerp_(v_mean.to(dtype=second_momentum_buffer.dtype), 1 - beta2)
    step_size = second_momentum_buffer.clamp_min(1e-10).rsqrt_()
    scaled_sq_sum = (v_mean * red_dim_size) * step_size.float().square()
    v_norm_new = scaled_sq_sum.sum(dim=(-2, -1), keepdim=True).sqrt_()
    final_scale = step_size * (v_norm / v_norm_new.clamp_min_(1e-10))
    return v_chunk.mul_(final_scale.type_as(v_chunk))


# -----------------------------------------------------------------------------
# NorMuon optimizer

class NorMuon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).

    Differences from standard Muon:
    - Newton-Shulz is replaced with Polar Express for the orthogonalization step
    - NorMuon adds a low-rank variance estimator similar to Adafactor. https://arxiv.org/pdf/2510.05491
    - small 1D parameters handled here instead of in Adam
    - Cautious weight decay, a gated version of decoupled weight decay
    - Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as
    needed on the forward pass. This enables attn and mlp weights to be contained within the same
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn.
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, beta2=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, beta2=beta2)
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def reset(self):
        # expose a reset for clearing buffers
        for group in self.param_groups:
            group["momentum_buffer"].zero_()
            group["second_momentum_buffer"].zero_()

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per module.
        """
        groups = defaultdict(list)
        for param in params:
            groups[param.label].append(param)

        param_groups = []
        for module_name, group_params in groups.items():
            chunk_size = (len(group_params) + self.world_size - 1) // self.world_size
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))

        return param_groups

    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn
        and mlp params when a param group is split across GPUs.
        """
        module_group_order = ['smear_gate', 'attn_gate', 'attn', 'mlp']
        params_list = list(params)
        params_list.sort(key=lambda x: module_group_order.index(x.label))

        idx = 0
        group_sizes = [1, 10, 16, 16]
        assert len(params_list) == sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            chunk_size = (size + self.world_size - 1) // self.world_size
            group_params = params_list[idx: idx + size]
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))
            idx += size

        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient distributed step by @YouJiacheng, @KonstantinWilleke, @alexrgilbert,
        # @adricarda, @tuttyfrutyee, @vdlad, @ryanyang0, @vagrawal, @varunneal, @chrisjmccormick
        rank = dist.get_rank()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            stacked_grads = torch.empty(
                (padded_num_params, *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device
            )
            for i, p in enumerate(params):
                stacked_grads[i].copy_(p.grad, non_blocking=True)
            if len(params) < padded_num_params:
                stacked_grads[len(params):].zero_()

            grad_chunk = torch.empty_like(stacked_grads[:chunk_size])

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(dict(grad_chunk=grad_chunk, reduce_future=reduce_future))

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = group["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            start_idx = rank * chunk_size
            module_idx = start_idx if start_idx < len(params) else 0

            num_params = min(chunk_size, max(0, len(params) - start_idx))  # num params for this rank

            if "momentum_buffer" not in group:
                group["momentum_buffer"]  = torch.zeros_like(grad_chunk[:num_params])
            momentum_buffer = group["momentum_buffer"]
            # Apply momentum update to the persistent momentum buffer in-place
            momentum_buffer.lerp_(grad_chunk[:num_params], 1 - group["momentum"])
            updated_grads = grad_chunk[:num_params].lerp_(momentum_buffer, group["momentum"])

            grad_shape = updated_grads.shape
            if params[module_idx].label == 'attn':

                for p in params[module_idx:module_idx + num_params]:
                    assert p.label == 'attn'

                updated_grads = updated_grads.view(4 * grad_shape[0], grad_shape[1] // 4, grad_shape[2])

            ref_param = params[module_idx]
            param_shape = ref_param.shape

            # The below shape-based heuristic assumes that matrices have their input along the
            # row dimension and their output along the columns. Gates are an exception.
            is_gate = ref_param.label in ['smear_gate', 'attn_gate']

            if "second_momentum_buffer" not in group:
                if is_gate:
                    group["second_momentum_buffer"] = torch.zeros_like(updated_grads[..., :, :1])
                else:
                    group["second_momentum_buffer"] = (torch.zeros_like(updated_grads[..., :, :1])
                        if param_shape[-2] >= param_shape[-1] else torch.zeros_like(updated_grads[..., :1, :])
                    )
            second_momentum_buffer = group["second_momentum_buffer"]

            if "param_lr_cpu" not in group:
                # Define multipliers for ALL params in this group (global, not per-shard)
                lr_mults = []
                wd_mults = []
                for p in params:
                    # Increase learning rate for modules with larger inputs than outputs.
                    # This shape check also assumes rows=input, columns=output, so take care
                    # when changing memory layouts. @chrisjmccormick
                    shape = p.shape
                    if len(shape) >= 2:
                        shape_mult = max(1.0, shape[-2] / shape[-1]) ** 0.5
                    else:
                        shape_mult = 1.0
                    lr_mults.append(shape_mult * getattr(p, "lr_mul", 1.0))
                    wd_mults.append(getattr(p, "wd_mul", 1.0))
                # Define as cpu tensors to enable Inductor constant folding
                group["param_lr_cpu"] = torch.tensor(lr_mults, dtype=torch.float32, device="cpu")
                group["param_wd_cpu"] = torch.tensor(wd_mults, dtype=torch.float32, device="cpu")

            eff_lr_all = group["param_lr_cpu"] * group["lr"]
            eff_wd_all = group["param_wd_cpu"] * group["weight_decay"] * group["lr"]

            # Slice the portion corresponding to this rank's shard
            eff_lr_cpu = eff_lr_all[module_idx:module_idx + num_params]
            eff_wd_cpu = eff_wd_all[module_idx:module_idx + num_params]

            # Compute zeropower for the entire chunk in a single, batched call.
            if num_params == 0:
                v_chunk = updated_grads
            else:
                v_chunk = polar_express(updated_grads, split_baddbmm=(ref_param.label == 'mlp'))

            # Note that the head orientation in O is transposed relative to QKV, so red_dim
            # is 'incorrect' for O. However, correcting this showed no improvement. @chrisjmccormick
            red_dim = -1 if (is_gate or param_shape[-2] >= param_shape[-1]) else -2

            v_chunk = apply_normuon_variance_reduction(
                v_chunk, second_momentum_buffer, group["beta2"], red_dim
            )

            v_chunk = v_chunk.view(grad_shape)

            # # "Cautious" weight decay (https://arxiv.org/abs/2510.12402)
            updated_params = torch.empty_like(grad_chunk)
            if num_params > 0:
                # Work on a stacked copy to avoid touching original params
                param_chunk = torch.stack(params[module_idx:module_idx + num_params])

                for local_idx in range(num_params):
                    cautious_wd_and_update_inplace(
                        param_chunk[local_idx],
                        v_chunk[local_idx],
                        eff_wd_cpu[local_idx],
                        eff_lr_cpu[local_idx],
                    )
            else:
                param_chunk = torch.zeros_like(v_chunk)

            updated_params[:num_params].copy_(param_chunk)
            if num_params < chunk_size:
                updated_params[num_params:].zero_()

            stacked_params = torch.empty(
                (padded_num_params, *param_shape),
                dtype=updated_params.dtype,
                device=updated_params.device,
            )

            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_params, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        # Group by label, with explicit ordering for execution control.
        label_order = ['lm_head', 'value_embed', 'scalars']
        params_by_label = defaultdict(list)
        for p in params:
            params_by_label[getattr(p, 'label', None)].append(p)
        param_groups = []
        for label in label_order:
            if label in params_by_label:
                param_groups.append(dict(params=params_by_label[label]))
        # include any unlabeled params at the end (processed last)
        if None in params_by_label:
            param_groups.append(dict(params=params_by_label[None]))
        super().__init__(param_groups, defaults)
        # init state
        for p in params:
            chunk_size = p.size(0) // self.world_size
            exp_avg = torch.zeros_like(p[:chunk_size], dtype=torch.bfloat16, device=p[0].device)
            exp_avg_sq = torch.zeros_like(exp_avg)
            self.state[p] = dict(step=0, exp_avg=exp_avg, exp_avg_sq=exp_avg_sq)
        # DistributedAdam implementation by @vagrawal, @akash5474

        self.should_sync = False

        self._reduce_scatter_hooks = []
        self._reduce_scatter_futures = {}
        self.register_backward_hooks()

    def register_backward_hooks(self):
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                hook = param.register_post_accumulate_grad_hook(self._sync_gradient)
                self._reduce_scatter_hooks.append(hook)

    @torch.no_grad()
    def _sync_gradient(self, param):
        if not self.should_sync:
            return

        grad = param.grad
        rank_size = grad.shape[0] // self.world_size
        grad_slice = torch.empty_like(grad[:rank_size])
        self._reduce_scatter_futures[param] = (
            dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future(),
            grad_slice
        )

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        all_gather_futures: list[torch.Future] = []

        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            for param in group['params']:
                if param not in self._reduce_scatter_futures:
                    continue

                fut, g_slice = self._reduce_scatter_futures[param]
                fut.wait()

                rank_size = param.shape[0] // self.world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]

                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (bias2 ** 0.5 / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                # cautious weight decay
                mask = (update * p_slice) >= 0
                # lr as weight decay schedule
                eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                update.addcmul_(p_slice, mask, value=eff_weight_decay * lr)

                p_slice.add_(other=update, alpha=-1.0)

                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())

        self._reduce_scatter_futures.clear()
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        with torch.no_grad():
            self.weight.zero_()  # @Grad62304977 and others

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))


# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float
    key_shift: bool

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKVO weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # Simplified layout by @chrisjmccormick
        self.qkvo_w = nn.Parameter(torch.empty(self.dim * 4, self.hdim))
        # label all modules for explicit optimizer grouping
        self.qkvo_w.label = 'attn'

        with torch.no_grad():
            self.qkvo_w[:self.dim * 3].uniform_(-bound, bound)  # init QKV weights
            self.qkvo_w[self.dim * 3:].zero_()  # init O weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        self.attn_gate.weight.label = 'attn_gate'

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas, key_shift = attn_args.ve, attn_args.sa_lambdas, attn_args.key_shift
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, sa_lambdas[0] * self.qkvo_w[:self.dim * 3].type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if key_shift:
            # shift keys forward for the stationary head dims. Enables 1-layer induction.
            k[:, 1:, :, self.head_dim//4:self.head_dim//2] = k[:, :-1, :, self.head_dim//4:self.head_dim//2]
            k[:, 1:, :, self.head_dim//4+self.head_dim//2:] = k[:, :-1, :, self.head_dim//4+self.head_dim//2:]
        if ve is not None:
            v = v + ve.view_as(v) # @ KoszarskyB & @Grad62304977

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens,
                                                        max_seqlen_q=max_len, max_seqlen_k=max_len,
                                                        causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, sa_lambdas[1] * self.qkvo_w[self.dim * 3:].type_as(y))  # sa_lambdas[1] pre-multiplied to O @shenberg
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # Transposed layout to match attention weights
        self.c_fc = nn.Parameter(torch.empty(hdim, dim))
        self.c_proj = nn.Parameter(torch.empty(hdim, dim))
        # label all modules for explicit optimizer grouping
        self.c_fc.label = 'mlp'
        self.c_proj.label = 'mlp'
        self.c_proj.lr_mul = 2.

        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.T.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.6 (the 7th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx != 6 else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim)

    def forward(self, x: Tensor, attn_args: AttnArgs):
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        self.num_layers = num_layers
        vocab_size = next_multiple_of_n(vocab_size, n=128)

        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.label = 'smear_gate'

        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        for embed in self.value_embeds:
            nn.init.zeros_(embed.weight)
        for ve in self.value_embeds:
            ve.weight.label = 'value_embed'
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)

        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=100/448, w_s=1.5/448, grad_s=0.75/448)
        nn.init.normal_(self.lm_head.weight, mean=0, std=0.005)
        self.lm_head.weight.label = 'lm_head'
        # Add learnable skip connection weights for decoder layers
        pad = (-num_layers * 4 - 3) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    1.1 * torch.ones(num_layers),  # resid lambdas. 1.1 init such that layer i weight is i^(num_layers-i).
                    0 * torch.ones(num_layers),  # x0 lambdas
                    *[torch.tensor([0.5, 1.0]) for _ in range(num_layers)],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    -1.5 * torch.ones(1),  # skip_lambda -> σ(-1.5) ≈ 0.18
                    torch.ones(pad),
                ]
            )
        )

        self.scalars.label = 'scalars'
        # set learning rates
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.scalars.lr_mul = 5.0
        self.scalars.wd_mul = 0.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        # set configs
        skip_connections = []
        skip_in = [3] # long attention window on layer 3
        skip_out = [6] # no attn op on layer 6
        x_backout = None
        backout_layer = 7

        # set lambdas
        resid_lambdas = self.scalars[: 1 * self.num_layers]
        x0_lambdas = self.scalars[1 * self.num_layers: 2 * self.num_layers]
        sa_lambdas = self.scalars[2 * self.num_layers: 4 * self.num_layers].view(-1, 2)
        smear_lambda = self.scalars[4 * self.num_layers]
        backout_lambda = self.scalars[4 * self.num_layers+1]
        skip_lambda = self.scalars[4 * self.num_layers+2]

        # set block masks and key shift
        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == self.num_layers
        key_shift = [b==long_bm for b in bm_sizes] # apply key shift to long windows

        # weight-tied: use lm_head.weight for embedding lookup
        x = F.embedding(input_seq, self.lm_head.weight)
        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [ve[1], ve[2]] + [None] * (self.num_layers - 5) + [ve[0], ve[1], ve[2]]
        assert len(ve) == self.num_layers

        # smear token embed forward 1 position @classiclarryd
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        for i in range(self.num_layers):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale,
                key_shift = key_shift[i]
            )
            if i in skip_out:
                gate = torch.sigmoid(skip_lambda)  # in (0, 1)
                x = x + gate * skip_connections.pop()
            if i == 0:
                x = (resid_lambdas[0] + x0_lambdas[0]) * x
            else:
                x = resid_lambdas[i] * x + x0_lambdas[i] * x0
            x = self.blocks[i](x, attn_args)
            if i in skip_in:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 7 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()

    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        # Cast to int32 on CPU before transfer to avoid dtype conversion during .to()
        _inputs = _inputs.to(dtype=torch.int32)
        _targets = _targets.to(dtype=torch.int64)
        _cum_lengths = _cum_lengths.to(dtype=torch.int32)

        new_params = yield (
            _inputs.to(device="cuda", non_blocking=True),
            _targets.to(device="cuda", non_blocking=True),
            _cum_lengths.to(device="cuda", non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * new_grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens // new_grad_accum_steps
            max_seq_len = new_max_seq_len


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # batch sizes
    train_bs_schedule: tuple = (8 * 2048 * 8, 16 * 2048 * 8, 24 * 2048 * 8)
    train_bs_extension: int = 24 * 2048 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 1995  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: float = 0.55  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_final: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=11,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=args.val_batch_size // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.weight.data = m.weight.data.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    embed_params + scalar_params + head_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.005,
)
optimizer2 = NorMuon(hidden_matrix_params + gate_params, lr=0.023, momentum=0.95, beta2=0.95, weight_decay=1.2)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: tied to batch size schedule, with cooldown at the end.
def get_lr(step: int):
    if step > args.num_scheduled_iterations:
        return 0.1
    lr_max = 1.0
    x = step / args.num_scheduled_iterations
    if x > 1/3:
       lr_max = 1.51  # (16/8)**0.6
    if x > 2/3:
        lr_max = 1.93  # (24/8)**0.6
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = lr_max * w + (1 - w) * 0.1
        return lr
    return lr_max

def get_ws(step: int):
    # set short window size to half of long window size
    # Higher ws on "extension" steps
    if step >= args.num_scheduled_iterations:
        return args.ws_final // 2, args.ws_final
    x = step / args.num_scheduled_iterations
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]


def get_bs(step: int):
    if step >= args.num_scheduled_iterations:
        return args.train_bs_extension
    x = step / args.num_scheduled_iterations
    bs_idx = int(len(args.train_bs_schedule) * x)
    return args.train_bs_schedule[bs_idx]


def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)
        # disable sync in the next training step for the adam optimizer
        optimizers[0].should_sync = False

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 10
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_bs_schedule[0], args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
ws_schedule = list(args.ws_schedule) + [args.ws_final]
bs_schedule = list(args.train_bs_schedule) + [args.train_bs_extension]
ws_long = ws_schedule[0]
model.train()
model.yarn.reset()
assert len(ws_schedule) == len(bs_schedule), "This warmup assumes len(ws_schedule) == len(bs_schedule)"

for idx in range(len(ws_schedule)):
    send_args = None
    if idx != 0:
        new_ws_long = ws_schedule[idx]
        model.yarn.apply(ws_long, new_ws_long)
        ws_long = new_ws_long
        send_args = (bs_schedule[idx], args.train_max_seq_len, grad_accum_steps)
    for step in range(warmup_steps):
        inputs, targets, cum_seqlens = train_loader.send(send_args)
        if step % 2 == 1:
            optimizers[0].should_sync = True
        model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
        if step % 2 == 0:
            optimizers[1].step()
            optimizers[1].zero_grad(set_to_none=True)
        else:
            for opt in optimizers:
                opt.step()
            model.zero_grad(set_to_none=True)
            optimizers[0].should_sync = False

model.zero_grad(set_to_none=True)
optimizers[0].should_sync = False
model.eval()

# warm up validation too
val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
val_loss = 0
with torch.no_grad():
    for step in range(val_steps):
        inputs, targets, cum_seqlens = next(val_loader)
        ws_idx = step % len(ws_schedule)
        if ws_idx==0:
            model.yarn.reset()
            ws_long = ws_schedule[0]
        else:
            new_ws_long = ws_schedule[ws_idx]
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
        val_loss += model(inputs, targets, cum_seqlens, ws_long // 2, ws_long)

del val_loader, val_loss
model.train()
model.yarn.reset() # rotary buffer is not stored in state_dict
optimizer2.reset() # muon momentum buffers not in state dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

step_batch_size = args.train_bs_schedule[0]
train_loader = distributed_data_generator(args.train_files, step_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)

gc.collect()

training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    new_step_batch_size = get_bs(step)
    send_args = (new_step_batch_size, args.train_max_seq_len, grad_accum_steps) if new_step_batch_size != step_batch_size else None
    step_batch_size = new_step_batch_size
    for idx in range(grad_accum_steps):
        # enable gradient sync for the DistAdam optimizer on the last iteration before we step it
        if idx == grad_accum_steps - 1 and step % 2 == 1:
            optimizers[0].should_sync = True

        inputs, targets, cum_seqlens = train_loader.send(send_args)
        (model(inputs, targets, cum_seqlens, ws_short, ws_long) / grad_accum_steps).backward()
    step_optimizers(step, optimizers, model)

    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.10.12 (main, Nov  4 2025, 08:48:33) [GCC 11.4.0]
Running PyTorch 2.10.0.dev20251204+cu126 compiled for CUDA 12.6
Running Triton version 3.6.0
Sat Dec 20 00:31:46 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:19:00.0 Off |                    0 |
| N/A   30C    P0            119W /  700W |    5858MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:3B:00.0 Off |                    0 |
| N/A   24C    P0            114W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:4C:00.0 Off |                    0 |
| N/A   27C    P0            120W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:5D:00.0 Off |                    0 |
| N/A   28C    P0            122W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:9B:00.0 Off |                    0 |
| N/A   31C    P0            125W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:BB:00.0 Off |                    0 |
| N/A   24C    P0            113W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:CB:00.0 Off |                    0 |
| N/A   29C    P0            121W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:DB:00.0 Off |                    0 |
| N/A   24C    P0            117W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A    308943      C   /home/ubuntu/.venv/bin/python3               1510MiB |
|    0   N/A  N/A    308944      C   /home/ubuntu/.venv/bin/python3                614MiB |
|    0   N/A  N/A    308945      C   /home/ubuntu/.venv/bin/python3                614MiB |
|    0   N/A  N/A    308946      C   /home/ubuntu/.venv/bin/python3                614MiB |
|    0   N/A  N/A    308947      C   /home/ubuntu/.venv/bin/python3                614MiB |
|    0   N/A  N/A    308948      C   /home/ubuntu/.venv/bin/python3                614MiB |
|    0   N/A  N/A    308949      C   /home/ubuntu/.venv/bin/python3                614MiB |
|    0   N/A  N/A    308950      C   /home/ubuntu/.venv/bin/python3                614MiB |
|    1   N/A  N/A    308944      C   /home/ubuntu/.venv/bin/python3               1510MiB |
|    2   N/A  N/A    308945      C   /home/ubuntu/.venv/bin/python3               1510MiB |
|    3   N/A  N/A    308946      C   /home/ubuntu/.venv/bin/python3               1510MiB |
|    4   N/A  N/A    308947      C   /home/ubuntu/.venv/bin/python3               1510MiB |
|    5   N/A  N/A    308948      C   /home/ubuntu/.venv/bin/python3               1510MiB |
|    6   N/A  N/A    308949      C   /home/ubuntu/.venv/bin/python3               1510MiB |
|    7   N/A  N/A    308950      C   /home/ubuntu/.venv/bin/python3               1510MiB |
+-----------------------------------------------------------------------------------------+

====================================================================================================
step:0/2035 val_loss:10.8318 train_time:0ms step_avg:0.03ms
step:1/2035 train_time:75ms step_avg:75.14ms
step:2/2035 train_time:97ms step_avg:48.68ms
step:3/2035 train_time:123ms step_avg:41.01ms
step:4/2035 train_time:156ms step_avg:38.97ms
step:5/2035 train_time:188ms step_avg:37.67ms
step:6/2035 train_time:405ms step_avg:67.51ms
step:7/2035 train_time:434ms step_avg:61.94ms
step:8/2035 train_time:467ms step_avg:58.34ms
step:9/2035 train_time:499ms step_avg:55.49ms
step:10/2035 train_time:532ms step_avg:53.23ms
step:11/2035 train_time:565ms step_avg:51.38ms
step:12/2035 train_time:598ms step_avg:49.86ms
step:13/2035 train_time:631ms step_avg:48.56ms
step:14/2035 train_time:664ms step_avg:47.45ms
step:15/2035 train_time:698ms step_avg:46.50ms
step:16/2035 train_time:731ms step_avg:45.66ms
step:17/2035 train_time:764ms step_avg:44.92ms
step:18/2035 train_time:797ms step_avg:44.27ms
step:19/2035 train_time:830ms step_avg:43.68ms
step:20/2035 train_time:863ms step_avg:43.15ms
step:21/2035 train_time:896ms step_avg:42.67ms
step:22/2035 train_time:929ms step_avg:42.23ms
step:23/2035 train_time:962ms step_avg:41.84ms
step:24/2035 train_time:995ms step_avg:41.48ms
step:25/2035 train_time:1028ms step_avg:41.13ms
step:26/2035 train_time:1061ms step_avg:40.82ms
step:27/2035 train_time:1095ms step_avg:40.54ms
step:28/2035 train_time:1128ms step_avg:40.27ms
step:29/2035 train_time:1161ms step_avg:40.03ms
step:30/2035 train_time:1194ms step_avg:39.80ms
step:31/2035 train_time:1227ms step_avg:39.57ms
step:32/2035 train_time:1260ms step_avg:39.38ms
step:33/2035 train_time:1293ms step_avg:39.18ms
step:34/2035 train_time:1326ms step_avg:39.00ms
step:35/2035 train_time:1360ms step_avg:38.85ms
step:36/2035 train_time:1393ms step_avg:38.69ms
step:37/2035 train_time:1427ms step_avg:38.56ms
step:38/2035 train_time:1460ms step_avg:38.43ms
step:39/2035 train_time:1494ms step_avg:38.30ms
step:40/2035 train_time:1527ms step_avg:38.16ms
step:41/2035 train_time:1560ms step_avg:38.05ms
step:42/2035 train_time:1593ms step_avg:37.93ms
step:43/2035 train_time:1627ms step_avg:37.83ms
step:44/2035 train_time:1660ms step_avg:37.72ms
step:45/2035 train_time:1693ms step_avg:37.62ms
step:46/2035 train_time:1726ms step_avg:37.52ms
step:47/2035 train_time:1759ms step_avg:37.42ms
step:48/2035 train_time:1792ms step_avg:37.33ms
step:49/2035 train_time:1825ms step_avg:37.24ms
step:50/2035 train_time:1858ms step_avg:37.16ms
step:51/2035 train_time:1891ms step_avg:37.07ms
step:52/2035 train_time:1924ms step_avg:37.00ms
step:53/2035 train_time:1957ms step_avg:36.92ms
step:54/2035 train_time:1990ms step_avg:36.85ms
step:55/2035 train_time:2023ms step_avg:36.79ms
step:56/2035 train_time:2056ms step_avg:36.72ms
step:57/2035 train_time:2089ms step_avg:36.65ms
step:58/2035 train_time:2123ms step_avg:36.60ms
step:59/2035 train_time:2155ms step_avg:36.53ms
step:60/2035 train_time:2188ms step_avg:36.47ms
step:61/2035 train_time:2221ms step_avg:36.41ms
step:62/2035 train_time:2254ms step_avg:36.36ms
step:63/2035 train_time:2287ms step_avg:36.31ms
step:64/2035 train_time:2321ms step_avg:36.26ms
step:65/2035 train_time:2353ms step_avg:36.21ms
step:66/2035 train_time:2387ms step_avg:36.16ms
step:67/2035 train_time:2420ms step_avg:36.12ms
step:68/2035 train_time:2453ms step_avg:36.07ms
step:69/2035 train_time:2486ms step_avg:36.03ms
step:70/2035 train_time:2519ms step_avg:35.99ms
step:71/2035 train_time:2553ms step_avg:35.95ms
step:72/2035 train_time:2586ms step_avg:35.91ms
step:73/2035 train_time:2619ms step_avg:35.88ms
step:74/2035 train_time:2652ms step_avg:35.84ms
step:75/2035 train_time:2686ms step_avg:35.81ms
step:76/2035 train_time:2719ms step_avg:35.78ms
step:77/2035 train_time:2752ms step_avg:35.74ms
step:78/2035 train_time:2785ms step_avg:35.71ms
step:79/2035 train_time:2818ms step_avg:35.67ms
step:80/2035 train_time:2851ms step_avg:35.64ms
step:81/2035 train_time:2885ms step_avg:35.61ms
step:82/2035 train_time:2918ms step_avg:35.58ms
step:83/2035 train_time:2951ms step_avg:35.55ms
step:84/2035 train_time:2984ms step_avg:35.52ms
step:85/2035 train_time:3017ms step_avg:35.49ms
step:86/2035 train_time:3050ms step_avg:35.46ms
step:87/2035 train_time:3083ms step_avg:35.43ms
step:88/2035 train_time:3116ms step_avg:35.41ms
step:89/2035 train_time:3149ms step_avg:35.38ms
step:90/2035 train_time:3182ms step_avg:35.36ms
step:91/2035 train_time:3215ms step_avg:35.33ms
step:92/2035 train_time:3248ms step_avg:35.30ms
step:93/2035 train_time:3281ms step_avg:35.28ms
step:94/2035 train_time:3314ms step_avg:35.25ms
step:95/2035 train_time:3347ms step_avg:35.23ms
step:96/2035 train_time:3380ms step_avg:35.21ms
step:97/2035 train_time:3413ms step_avg:35.19ms
step:98/2035 train_time:3446ms step_avg:35.16ms
step:99/2035 train_time:3479ms step_avg:35.14ms
step:100/2035 train_time:3512ms step_avg:35.12ms
step:101/2035 train_time:3545ms step_avg:35.10ms
step:102/2035 train_time:3578ms step_avg:35.08ms
step:103/2035 train_time:3611ms step_avg:35.06ms
step:104/2035 train_time:3644ms step_avg:35.04ms
step:105/2035 train_time:3677ms step_avg:35.02ms
step:106/2035 train_time:3710ms step_avg:35.00ms
step:107/2035 train_time:3743ms step_avg:34.98ms
step:108/2035 train_time:3776ms step_avg:34.96ms
step:109/2035 train_time:3809ms step_avg:34.95ms
step:110/2035 train_time:3843ms step_avg:34.93ms
step:111/2035 train_time:3875ms step_avg:34.91ms
step:112/2035 train_time:3908ms step_avg:34.90ms
step:113/2035 train_time:3942ms step_avg:34.88ms
step:114/2035 train_time:3975ms step_avg:34.87ms
step:115/2035 train_time:4008ms step_avg:34.85ms
step:116/2035 train_time:4041ms step_avg:34.84ms
step:117/2035 train_time:4074ms step_avg:34.82ms
step:118/2035 train_time:4107ms step_avg:34.80ms
step:119/2035 train_time:4140ms step_avg:34.79ms
step:120/2035 train_time:4173ms step_avg:34.77ms
step:121/2035 train_time:4206ms step_avg:34.76ms
step:122/2035 train_time:4239ms step_avg:34.74ms
step:123/2035 train_time:4272ms step_avg:34.73ms
step:124/2035 train_time:4305ms step_avg:34.72ms
step:125/2035 train_time:4338ms step_avg:34.70ms
step:126/2035 train_time:4371ms step_avg:34.69ms
step:127/2035 train_time:4404ms step_avg:34.68ms
step:128/2035 train_time:4437ms step_avg:34.66ms
step:129/2035 train_time:4470ms step_avg:34.65ms
step:130/2035 train_time:4503ms step_avg:34.64ms
step:131/2035 train_time:4536ms step_avg:34.62ms
step:132/2035 train_time:4569ms step_avg:34.61ms
step:133/2035 train_time:4602ms step_avg:34.60ms
step:134/2035 train_time:4635ms step_avg:34.59ms
step:135/2035 train_time:4668ms step_avg:34.58ms
step:136/2035 train_time:4701ms step_avg:34.56ms
step:137/2035 train_time:4734ms step_avg:34.55ms
step:138/2035 train_time:4767ms step_avg:34.54ms
step:139/2035 train_time:4800ms step_avg:34.53ms
step:140/2035 train_time:4833ms step_avg:34.52ms
step:141/2035 train_time:4866ms step_avg:34.51ms
step:142/2035 train_time:4899ms step_avg:34.50ms
step:143/2035 train_time:4932ms step_avg:34.49ms
step:144/2035 train_time:4965ms step_avg:34.48ms
step:145/2035 train_time:4998ms step_avg:34.47ms
step:146/2035 train_time:5031ms step_avg:34.46ms
step:147/2035 train_time:5064ms step_avg:34.45ms
step:148/2035 train_time:5097ms step_avg:34.44ms
step:149/2035 train_time:5130ms step_avg:34.43ms
step:150/2035 train_time:5163ms step_avg:34.42ms
step:151/2035 train_time:5196ms step_avg:34.41ms
step:152/2035 train_time:5229ms step_avg:34.40ms
step:153/2035 train_time:5262ms step_avg:34.39ms
step:154/2035 train_time:5295ms step_avg:34.39ms
step:155/2035 train_time:5328ms step_avg:34.38ms
step:156/2035 train_time:5361ms step_avg:34.37ms
step:157/2035 train_time:5394ms step_avg:34.36ms
step:158/2035 train_time:5427ms step_avg:34.35ms
step:159/2035 train_time:5460ms step_avg:34.34ms
step:160/2035 train_time:5493ms step_avg:34.33ms
step:161/2035 train_time:5526ms step_avg:34.32ms
step:162/2035 train_time:5559ms step_avg:34.31ms
step:163/2035 train_time:5592ms step_avg:34.31ms
step:164/2035 train_time:5625ms step_avg:34.30ms
step:165/2035 train_time:5658ms step_avg:34.29ms
step:166/2035 train_time:5691ms step_avg:34.28ms
step:167/2035 train_time:5724ms step_avg:34.28ms
step:168/2035 train_time:5757ms step_avg:34.27ms
step:169/2035 train_time:5790ms step_avg:34.26ms
step:170/2035 train_time:5823ms step_avg:34.25ms
step:171/2035 train_time:5856ms step_avg:34.25ms
step:172/2035 train_time:5889ms step_avg:34.24ms
step:173/2035 train_time:5923ms step_avg:34.23ms
step:174/2035 train_time:5955ms step_avg:34.23ms
step:175/2035 train_time:5989ms step_avg:34.22ms
step:176/2035 train_time:6022ms step_avg:34.21ms
step:177/2035 train_time:6055ms step_avg:34.21ms
step:178/2035 train_time:6088ms step_avg:34.20ms
step:179/2035 train_time:6121ms step_avg:34.19ms
step:180/2035 train_time:6154ms step_avg:34.19ms
step:181/2035 train_time:6187ms step_avg:34.18ms
step:182/2035 train_time:6220ms step_avg:34.17ms
step:183/2035 train_time:6253ms step_avg:34.17ms
step:184/2035 train_time:6286ms step_avg:34.16ms
step:185/2035 train_time:6318ms step_avg:34.15ms
step:186/2035 train_time:6352ms step_avg:34.15ms
step:187/2035 train_time:6384ms step_avg:34.14ms
step:188/2035 train_time:6418ms step_avg:34.14ms
step:189/2035 train_time:6450ms step_avg:34.13ms
step:190/2035 train_time:6484ms step_avg:34.12ms
step:191/2035 train_time:6517ms step_avg:34.12ms
step:192/2035 train_time:6550ms step_avg:34.11ms
step:193/2035 train_time:6583ms step_avg:34.11ms
step:194/2035 train_time:6616ms step_avg:34.10ms
step:195/2035 train_time:6649ms step_avg:34.10ms
step:196/2035 train_time:6682ms step_avg:34.09ms
step:197/2035 train_time:6714ms step_avg:34.08ms
step:198/2035 train_time:6748ms step_avg:34.08ms
step:199/2035 train_time:6781ms step_avg:34.07ms
step:200/2035 train_time:6814ms step_avg:34.07ms
step:201/2035 train_time:6846ms step_avg:34.06ms
step:202/2035 train_time:6879ms step_avg:34.06ms
step:203/2035 train_time:6912ms step_avg:34.05ms
step:204/2035 train_time:6945ms step_avg:34.05ms
step:205/2035 train_time:6978ms step_avg:34.04ms
step:206/2035 train_time:7011ms step_avg:34.04ms
step:207/2035 train_time:7044ms step_avg:34.03ms
step:208/2035 train_time:7077ms step_avg:34.03ms
step:209/2035 train_time:7110ms step_avg:34.02ms
step:210/2035 train_time:7143ms step_avg:34.02ms
step:211/2035 train_time:7176ms step_avg:34.01ms
step:212/2035 train_time:7209ms step_avg:34.00ms
step:213/2035 train_time:7242ms step_avg:34.00ms
step:214/2035 train_time:7275ms step_avg:33.99ms
step:215/2035 train_time:7308ms step_avg:33.99ms
step:216/2035 train_time:7341ms step_avg:33.99ms
step:217/2035 train_time:7374ms step_avg:33.98ms
step:218/2035 train_time:7407ms step_avg:33.98ms
step:219/2035 train_time:7440ms step_avg:33.97ms
step:220/2035 train_time:7473ms step_avg:33.97ms
step:221/2035 train_time:7506ms step_avg:33.96ms
step:222/2035 train_time:7539ms step_avg:33.96ms
step:223/2035 train_time:7572ms step_avg:33.95ms
step:224/2035 train_time:7605ms step_avg:33.95ms
step:225/2035 train_time:7638ms step_avg:33.95ms
step:226/2035 train_time:7671ms step_avg:33.94ms
step:227/2035 train_time:7704ms step_avg:33.94ms
step:228/2035 train_time:7737ms step_avg:33.93ms
step:229/2035 train_time:7770ms step_avg:33.93ms
step:230/2035 train_time:7802ms step_avg:33.92ms
step:231/2035 train_time:7836ms step_avg:33.92ms
step:232/2035 train_time:7869ms step_avg:33.92ms
step:233/2035 train_time:7902ms step_avg:33.91ms
step:234/2035 train_time:7935ms step_avg:33.91ms
step:235/2035 train_time:7968ms step_avg:33.91ms
step:236/2035 train_time:8001ms step_avg:33.90ms
step:237/2035 train_time:8034ms step_avg:33.90ms
step:238/2035 train_time:8067ms step_avg:33.89ms
step:239/2035 train_time:8100ms step_avg:33.89ms
step:240/2035 train_time:8133ms step_avg:33.89ms
step:241/2035 train_time:8166ms step_avg:33.88ms
step:242/2035 train_time:8198ms step_avg:33.88ms
step:243/2035 train_time:8231ms step_avg:33.87ms
step:244/2035 train_time:8264ms step_avg:33.87ms
step:245/2035 train_time:8297ms step_avg:33.87ms
step:246/2035 train_time:8330ms step_avg:33.86ms
step:247/2035 train_time:8364ms step_avg:33.86ms
step:248/2035 train_time:8397ms step_avg:33.86ms
step:249/2035 train_time:8430ms step_avg:33.85ms
step:250/2035 train_time:8463ms step_avg:33.85ms
step:250/2035 val_loss:4.2740 train_time:8498ms step_avg:33.99ms
step:251/2035 train_time:8517ms step_avg:33.93ms
step:252/2035 train_time:8537ms step_avg:33.88ms
step:253/2035 train_time:8565ms step_avg:33.85ms
step:254/2035 train_time:8598ms step_avg:33.85ms
step:255/2035 train_time:8632ms step_avg:33.85ms
step:256/2035 train_time:8665ms step_avg:33.85ms
step:257/2035 train_time:8699ms step_avg:33.85ms
step:258/2035 train_time:8732ms step_avg:33.85ms
step:259/2035 train_time:8765ms step_avg:33.84ms
step:260/2035 train_time:8798ms step_avg:33.84ms
step:261/2035 train_time:8831ms step_avg:33.84ms
step:262/2035 train_time:8864ms step_avg:33.83ms
step:263/2035 train_time:8897ms step_avg:33.83ms
step:264/2035 train_time:8930ms step_avg:33.82ms
step:265/2035 train_time:8963ms step_avg:33.82ms
step:266/2035 train_time:8996ms step_avg:33.82ms
step:267/2035 train_time:9028ms step_avg:33.81ms
step:268/2035 train_time:9061ms step_avg:33.81ms
step:269/2035 train_time:9094ms step_avg:33.81ms
step:270/2035 train_time:9127ms step_avg:33.80ms
step:271/2035 train_time:9160ms step_avg:33.80ms
step:272/2035 train_time:9192ms step_avg:33.80ms
step:273/2035 train_time:9225ms step_avg:33.79ms
step:274/2035 train_time:9258ms step_avg:33.79ms
step:275/2035 train_time:9291ms step_avg:33.78ms
step:276/2035 train_time:9323ms step_avg:33.78ms
step:277/2035 train_time:9356ms step_avg:33.78ms
step:278/2035 train_time:9389ms step_avg:33.77ms
step:279/2035 train_time:9422ms step_avg:33.77ms
step:280/2035 train_time:9455ms step_avg:33.77ms
step:281/2035 train_time:9488ms step_avg:33.77ms
step:282/2035 train_time:9521ms step_avg:33.76ms
step:283/2035 train_time:9554ms step_avg:33.76ms
step:284/2035 train_time:9587ms step_avg:33.76ms
step:285/2035 train_time:9621ms step_avg:33.76ms
step:286/2035 train_time:9654ms step_avg:33.75ms
step:287/2035 train_time:9687ms step_avg:33.75ms
step:288/2035 train_time:9720ms step_avg:33.75ms
step:289/2035 train_time:9754ms step_avg:33.75ms
step:290/2035 train_time:9787ms step_avg:33.75ms
step:291/2035 train_time:9820ms step_avg:33.75ms
step:292/2035 train_time:9853ms step_avg:33.74ms
step:293/2035 train_time:9886ms step_avg:33.74ms
step:294/2035 train_time:9919ms step_avg:33.74ms
step:295/2035 train_time:9952ms step_avg:33.73ms
step:296/2035 train_time:9984ms step_avg:33.73ms
step:297/2035 train_time:10018ms step_avg:33.73ms
step:298/2035 train_time:10051ms step_avg:33.73ms
step:299/2035 train_time:10083ms step_avg:33.72ms
step:300/2035 train_time:10116ms step_avg:33.72ms
step:301/2035 train_time:10149ms step_avg:33.72ms
step:302/2035 train_time:10182ms step_avg:33.72ms
step:303/2035 train_time:10215ms step_avg:33.71ms
step:304/2035 train_time:10248ms step_avg:33.71ms
step:305/2035 train_time:10281ms step_avg:33.71ms
step:306/2035 train_time:10314ms step_avg:33.70ms
step:307/2035 train_time:10346ms step_avg:33.70ms
step:308/2035 train_time:10379ms step_avg:33.70ms
step:309/2035 train_time:10412ms step_avg:33.70ms
step:310/2035 train_time:10445ms step_avg:33.69ms
step:311/2035 train_time:10478ms step_avg:33.69ms
step:312/2035 train_time:10511ms step_avg:33.69ms
step:313/2035 train_time:10544ms step_avg:33.69ms
step:314/2035 train_time:10577ms step_avg:33.68ms
step:315/2035 train_time:10610ms step_avg:33.68ms
step:316/2035 train_time:10643ms step_avg:33.68ms
step:317/2035 train_time:10676ms step_avg:33.68ms
step:318/2035 train_time:10709ms step_avg:33.67ms
step:319/2035 train_time:10742ms step_avg:33.67ms
step:320/2035 train_time:10774ms step_avg:33.67ms
step:321/2035 train_time:10808ms step_avg:33.67ms
step:322/2035 train_time:10841ms step_avg:33.67ms
step:323/2035 train_time:10875ms step_avg:33.67ms
step:324/2035 train_time:10907ms step_avg:33.67ms
step:325/2035 train_time:10941ms step_avg:33.66ms
step:326/2035 train_time:10974ms step_avg:33.66ms
step:327/2035 train_time:11007ms step_avg:33.66ms
step:328/2035 train_time:11040ms step_avg:33.66ms
step:329/2035 train_time:11073ms step_avg:33.66ms
step:330/2035 train_time:11106ms step_avg:33.65ms
step:331/2035 train_time:11138ms step_avg:33.65ms
step:332/2035 train_time:11171ms step_avg:33.65ms
step:333/2035 train_time:11205ms step_avg:33.65ms
step:334/2035 train_time:11237ms step_avg:33.64ms
step:335/2035 train_time:11270ms step_avg:33.64ms
step:336/2035 train_time:11303ms step_avg:33.64ms
step:337/2035 train_time:11336ms step_avg:33.64ms
step:338/2035 train_time:11369ms step_avg:33.63ms
step:339/2035 train_time:11402ms step_avg:33.63ms
step:340/2035 train_time:11434ms step_avg:33.63ms
step:341/2035 train_time:11467ms step_avg:33.63ms
step:342/2035 train_time:11500ms step_avg:33.63ms
step:343/2035 train_time:11533ms step_avg:33.62ms
step:344/2035 train_time:11567ms step_avg:33.62ms
step:345/2035 train_time:11599ms step_avg:33.62ms
step:346/2035 train_time:11633ms step_avg:33.62ms
step:347/2035 train_time:11665ms step_avg:33.62ms
step:348/2035 train_time:11698ms step_avg:33.62ms
step:349/2035 train_time:11731ms step_avg:33.61ms
step:350/2035 train_time:11764ms step_avg:33.61ms
step:351/2035 train_time:11797ms step_avg:33.61ms
step:352/2035 train_time:11830ms step_avg:33.61ms
step:353/2035 train_time:11863ms step_avg:33.61ms
step:354/2035 train_time:11896ms step_avg:33.60ms
step:355/2035 train_time:11929ms step_avg:33.60ms
step:356/2035 train_time:11962ms step_avg:33.60ms
step:357/2035 train_time:11995ms step_avg:33.60ms
step:358/2035 train_time:12028ms step_avg:33.60ms
step:359/2035 train_time:12061ms step_avg:33.60ms
step:360/2035 train_time:12094ms step_avg:33.59ms
step:361/2035 train_time:12127ms step_avg:33.59ms
step:362/2035 train_time:12160ms step_avg:33.59ms
step:363/2035 train_time:12192ms step_avg:33.59ms
step:364/2035 train_time:12225ms step_avg:33.59ms
step:365/2035 train_time:12258ms step_avg:33.58ms
step:366/2035 train_time:12291ms step_avg:33.58ms
step:367/2035 train_time:12324ms step_avg:33.58ms
step:368/2035 train_time:12357ms step_avg:33.58ms
step:369/2035 train_time:12390ms step_avg:33.58ms
step:370/2035 train_time:12422ms step_avg:33.57ms
step:371/2035 train_time:12456ms step_avg:33.57ms
step:372/2035 train_time:12489ms step_avg:33.57ms
step:373/2035 train_time:12521ms step_avg:33.57ms
step:374/2035 train_time:12554ms step_avg:33.57ms
step:375/2035 train_time:12588ms step_avg:33.57ms
step:376/2035 train_time:12620ms step_avg:33.56ms
step:377/2035 train_time:12653ms step_avg:33.56ms
step:378/2035 train_time:12686ms step_avg:33.56ms
step:379/2035 train_time:12719ms step_avg:33.56ms
step:380/2035 train_time:12752ms step_avg:33.56ms
step:381/2035 train_time:12785ms step_avg:33.56ms
step:382/2035 train_time:12818ms step_avg:33.55ms
step:383/2035 train_time:12851ms step_avg:33.55ms
step:384/2035 train_time:12884ms step_avg:33.55ms
step:385/2035 train_time:12917ms step_avg:33.55ms
step:386/2035 train_time:12950ms step_avg:33.55ms
step:387/2035 train_time:12983ms step_avg:33.55ms
step:388/2035 train_time:13016ms step_avg:33.55ms
step:389/2035 train_time:13049ms step_avg:33.54ms
step:390/2035 train_time:13082ms step_avg:33.54ms
step:391/2035 train_time:13115ms step_avg:33.54ms
step:392/2035 train_time:13148ms step_avg:33.54ms
step:393/2035 train_time:13181ms step_avg:33.54ms
step:394/2035 train_time:13213ms step_avg:33.54ms
step:395/2035 train_time:13247ms step_avg:33.54ms
step:396/2035 train_time:13279ms step_avg:33.53ms
step:397/2035 train_time:13312ms step_avg:33.53ms
step:398/2035 train_time:13345ms step_avg:33.53ms
step:399/2035 train_time:13378ms step_avg:33.53ms
step:400/2035 train_time:13411ms step_avg:33.53ms
step:401/2035 train_time:13444ms step_avg:33.53ms
step:402/2035 train_time:13477ms step_avg:33.52ms
step:403/2035 train_time:13510ms step_avg:33.52ms
step:404/2035 train_time:13543ms step_avg:33.52ms
step:405/2035 train_time:13576ms step_avg:33.52ms
step:406/2035 train_time:13609ms step_avg:33.52ms
step:407/2035 train_time:13641ms step_avg:33.52ms
step:408/2035 train_time:13674ms step_avg:33.52ms
step:409/2035 train_time:13707ms step_avg:33.51ms
step:410/2035 train_time:13740ms step_avg:33.51ms
step:411/2035 train_time:13773ms step_avg:33.51ms
step:412/2035 train_time:13806ms step_avg:33.51ms
step:413/2035 train_time:13839ms step_avg:33.51ms
step:414/2035 train_time:13872ms step_avg:33.51ms
step:415/2035 train_time:13905ms step_avg:33.51ms
step:416/2035 train_time:13938ms step_avg:33.50ms
step:417/2035 train_time:13971ms step_avg:33.50ms
step:418/2035 train_time:14004ms step_avg:33.50ms
step:419/2035 train_time:14037ms step_avg:33.50ms
step:420/2035 train_time:14070ms step_avg:33.50ms
step:421/2035 train_time:14103ms step_avg:33.50ms
step:422/2035 train_time:14135ms step_avg:33.50ms
step:423/2035 train_time:14168ms step_avg:33.49ms
step:424/2035 train_time:14201ms step_avg:33.49ms
step:425/2035 train_time:14234ms step_avg:33.49ms
step:426/2035 train_time:14267ms step_avg:33.49ms
step:427/2035 train_time:14300ms step_avg:33.49ms
step:428/2035 train_time:14333ms step_avg:33.49ms
step:429/2035 train_time:14366ms step_avg:33.49ms
step:430/2035 train_time:14399ms step_avg:33.49ms
step:431/2035 train_time:14432ms step_avg:33.48ms
step:432/2035 train_time:14465ms step_avg:33.48ms
step:433/2035 train_time:14498ms step_avg:33.48ms
step:434/2035 train_time:14530ms step_avg:33.48ms
step:435/2035 train_time:14563ms step_avg:33.48ms
step:436/2035 train_time:14596ms step_avg:33.48ms
step:437/2035 train_time:14629ms step_avg:33.48ms
step:438/2035 train_time:14662ms step_avg:33.48ms
step:439/2035 train_time:14695ms step_avg:33.47ms
step:440/2035 train_time:14728ms step_avg:33.47ms
step:441/2035 train_time:14761ms step_avg:33.47ms
step:442/2035 train_time:14794ms step_avg:33.47ms
step:443/2035 train_time:14828ms step_avg:33.47ms
step:444/2035 train_time:14861ms step_avg:33.47ms
step:445/2035 train_time:14894ms step_avg:33.47ms
step:446/2035 train_time:14927ms step_avg:33.47ms
step:447/2035 train_time:14960ms step_avg:33.47ms
step:448/2035 train_time:14993ms step_avg:33.47ms
step:449/2035 train_time:15026ms step_avg:33.47ms
step:450/2035 train_time:15059ms step_avg:33.46ms
step:451/2035 train_time:15092ms step_avg:33.46ms
step:452/2035 train_time:15125ms step_avg:33.46ms
step:453/2035 train_time:15158ms step_avg:33.46ms
step:454/2035 train_time:15191ms step_avg:33.46ms
step:455/2035 train_time:15224ms step_avg:33.46ms
step:456/2035 train_time:15257ms step_avg:33.46ms
step:457/2035 train_time:15290ms step_avg:33.46ms
step:458/2035 train_time:15323ms step_avg:33.46ms
step:459/2035 train_time:15356ms step_avg:33.46ms
step:460/2035 train_time:15389ms step_avg:33.45ms
step:461/2035 train_time:15422ms step_avg:33.45ms
step:462/2035 train_time:15455ms step_avg:33.45ms
step:463/2035 train_time:15488ms step_avg:33.45ms
step:464/2035 train_time:15520ms step_avg:33.45ms
step:465/2035 train_time:15553ms step_avg:33.45ms
step:466/2035 train_time:15586ms step_avg:33.45ms
step:467/2035 train_time:15619ms step_avg:33.45ms
step:468/2035 train_time:15652ms step_avg:33.44ms
step:469/2035 train_time:15685ms step_avg:33.44ms
step:470/2035 train_time:15718ms step_avg:33.44ms
step:471/2035 train_time:15751ms step_avg:33.44ms
step:472/2035 train_time:15784ms step_avg:33.44ms
step:473/2035 train_time:15817ms step_avg:33.44ms
step:474/2035 train_time:15850ms step_avg:33.44ms
step:475/2035 train_time:15883ms step_avg:33.44ms
step:476/2035 train_time:15915ms step_avg:33.44ms
step:477/2035 train_time:15949ms step_avg:33.44ms
step:478/2035 train_time:15982ms step_avg:33.43ms
step:479/2035 train_time:16015ms step_avg:33.43ms
step:480/2035 train_time:16047ms step_avg:33.43ms
step:481/2035 train_time:16080ms step_avg:33.43ms
step:482/2035 train_time:16113ms step_avg:33.43ms
step:483/2035 train_time:16146ms step_avg:33.43ms
step:484/2035 train_time:16179ms step_avg:33.43ms
step:485/2035 train_time:16212ms step_avg:33.43ms
step:486/2035 train_time:16245ms step_avg:33.43ms
step:487/2035 train_time:16278ms step_avg:33.43ms
step:488/2035 train_time:16311ms step_avg:33.42ms
step:489/2035 train_time:16344ms step_avg:33.42ms
step:490/2035 train_time:16377ms step_avg:33.42ms
step:491/2035 train_time:16410ms step_avg:33.42ms
step:492/2035 train_time:16443ms step_avg:33.42ms
step:493/2035 train_time:16476ms step_avg:33.42ms
step:494/2035 train_time:16509ms step_avg:33.42ms
step:495/2035 train_time:16542ms step_avg:33.42ms
step:496/2035 train_time:16575ms step_avg:33.42ms
step:497/2035 train_time:16608ms step_avg:33.42ms
step:498/2035 train_time:16641ms step_avg:33.41ms
step:499/2035 train_time:16674ms step_avg:33.41ms
step:500/2035 train_time:16706ms step_avg:33.41ms
step:500/2035 val_loss:4.0003 train_time:16742ms step_avg:33.48ms
step:501/2035 train_time:16761ms step_avg:33.46ms
step:502/2035 train_time:16780ms step_avg:33.43ms
step:503/2035 train_time:16811ms step_avg:33.42ms
step:504/2035 train_time:16844ms step_avg:33.42ms
step:505/2035 train_time:16879ms step_avg:33.42ms
step:506/2035 train_time:16913ms step_avg:33.42ms
step:507/2035 train_time:16946ms step_avg:33.42ms
step:508/2035 train_time:16979ms step_avg:33.42ms
step:509/2035 train_time:17012ms step_avg:33.42ms
step:510/2035 train_time:17045ms step_avg:33.42ms
step:511/2035 train_time:17078ms step_avg:33.42ms
step:512/2035 train_time:17111ms step_avg:33.42ms
step:513/2035 train_time:17144ms step_avg:33.42ms
step:514/2035 train_time:17177ms step_avg:33.42ms
step:515/2035 train_time:17210ms step_avg:33.42ms
step:516/2035 train_time:17242ms step_avg:33.42ms
step:517/2035 train_time:17275ms step_avg:33.41ms
step:518/2035 train_time:17308ms step_avg:33.41ms
step:519/2035 train_time:17341ms step_avg:33.41ms
step:520/2035 train_time:17374ms step_avg:33.41ms
step:521/2035 train_time:17406ms step_avg:33.41ms
step:522/2035 train_time:17439ms step_avg:33.41ms
step:523/2035 train_time:17472ms step_avg:33.41ms
step:524/2035 train_time:17505ms step_avg:33.41ms
step:525/2035 train_time:17538ms step_avg:33.41ms
step:526/2035 train_time:17571ms step_avg:33.40ms
step:527/2035 train_time:17604ms step_avg:33.40ms
step:528/2035 train_time:17637ms step_avg:33.40ms
step:529/2035 train_time:17669ms step_avg:33.40ms
step:530/2035 train_time:17702ms step_avg:33.40ms
step:531/2035 train_time:17735ms step_avg:33.40ms
step:532/2035 train_time:17768ms step_avg:33.40ms
step:533/2035 train_time:17802ms step_avg:33.40ms
step:534/2035 train_time:17835ms step_avg:33.40ms
step:535/2035 train_time:17868ms step_avg:33.40ms
step:536/2035 train_time:17901ms step_avg:33.40ms
step:537/2035 train_time:17935ms step_avg:33.40ms
step:538/2035 train_time:17968ms step_avg:33.40ms
step:539/2035 train_time:18001ms step_avg:33.40ms
step:540/2035 train_time:18034ms step_avg:33.40ms
step:541/2035 train_time:18067ms step_avg:33.40ms
step:542/2035 train_time:18101ms step_avg:33.40ms
step:543/2035 train_time:18134ms step_avg:33.40ms
step:544/2035 train_time:18167ms step_avg:33.39ms
step:545/2035 train_time:18200ms step_avg:33.39ms
step:546/2035 train_time:18232ms step_avg:33.39ms
step:547/2035 train_time:18265ms step_avg:33.39ms
step:548/2035 train_time:18298ms step_avg:33.39ms
step:549/2035 train_time:18331ms step_avg:33.39ms
step:550/2035 train_time:18364ms step_avg:33.39ms
step:551/2035 train_time:18397ms step_avg:33.39ms
step:552/2035 train_time:18430ms step_avg:33.39ms
step:553/2035 train_time:18462ms step_avg:33.39ms
step:554/2035 train_time:18495ms step_avg:33.39ms
step:555/2035 train_time:18528ms step_avg:33.38ms
step:556/2035 train_time:18561ms step_avg:33.38ms
step:557/2035 train_time:18594ms step_avg:33.38ms
step:558/2035 train_time:18627ms step_avg:33.38ms
step:559/2035 train_time:18660ms step_avg:33.38ms
step:560/2035 train_time:18693ms step_avg:33.38ms
step:561/2035 train_time:18726ms step_avg:33.38ms
step:562/2035 train_time:18759ms step_avg:33.38ms
step:563/2035 train_time:18792ms step_avg:33.38ms
step:564/2035 train_time:18825ms step_avg:33.38ms
step:565/2035 train_time:18857ms step_avg:33.38ms
step:566/2035 train_time:18890ms step_avg:33.38ms
step:567/2035 train_time:18924ms step_avg:33.38ms
step:568/2035 train_time:18957ms step_avg:33.37ms
step:569/2035 train_time:18990ms step_avg:33.37ms
step:570/2035 train_time:19023ms step_avg:33.37ms
step:571/2035 train_time:19057ms step_avg:33.37ms
step:572/2035 train_time:19089ms step_avg:33.37ms
step:573/2035 train_time:19123ms step_avg:33.37ms
step:574/2035 train_time:19156ms step_avg:33.37ms
step:575/2035 train_time:19188ms step_avg:33.37ms
step:576/2035 train_time:19221ms step_avg:33.37ms
step:577/2035 train_time:19254ms step_avg:33.37ms
step:578/2035 train_time:19287ms step_avg:33.37ms
step:579/2035 train_time:19320ms step_avg:33.37ms
step:580/2035 train_time:19353ms step_avg:33.37ms
step:581/2035 train_time:19386ms step_avg:33.37ms
step:582/2035 train_time:19419ms step_avg:33.37ms
step:583/2035 train_time:19452ms step_avg:33.36ms
step:584/2035 train_time:19485ms step_avg:33.36ms
step:585/2035 train_time:19518ms step_avg:33.36ms
step:586/2035 train_time:19550ms step_avg:33.36ms
step:587/2035 train_time:19583ms step_avg:33.36ms
step:588/2035 train_time:19617ms step_avg:33.36ms
step:589/2035 train_time:19649ms step_avg:33.36ms
step:590/2035 train_time:19682ms step_avg:33.36ms
step:591/2035 train_time:19715ms step_avg:33.36ms
step:592/2035 train_time:19748ms step_avg:33.36ms
step:593/2035 train_time:19781ms step_avg:33.36ms
step:594/2035 train_time:19813ms step_avg:33.36ms
step:595/2035 train_time:19846ms step_avg:33.36ms
step:596/2035 train_time:19879ms step_avg:33.35ms
step:597/2035 train_time:19912ms step_avg:33.35ms
step:598/2035 train_time:19945ms step_avg:33.35ms
step:599/2035 train_time:19978ms step_avg:33.35ms
step:600/2035 train_time:20011ms step_avg:33.35ms
step:601/2035 train_time:20044ms step_avg:33.35ms
step:602/2035 train_time:20077ms step_avg:33.35ms
step:603/2035 train_time:20110ms step_avg:33.35ms
step:604/2035 train_time:20143ms step_avg:33.35ms
step:605/2035 train_time:20176ms step_avg:33.35ms
step:606/2035 train_time:20209ms step_avg:33.35ms
step:607/2035 train_time:20242ms step_avg:33.35ms
step:608/2035 train_time:20275ms step_avg:33.35ms
step:609/2035 train_time:20308ms step_avg:33.35ms
step:610/2035 train_time:20341ms step_avg:33.35ms
step:611/2035 train_time:20373ms step_avg:33.34ms
step:612/2035 train_time:20406ms step_avg:33.34ms
step:613/2035 train_time:20439ms step_avg:33.34ms
step:614/2035 train_time:20472ms step_avg:33.34ms
step:615/2035 train_time:20505ms step_avg:33.34ms
step:616/2035 train_time:20538ms step_avg:33.34ms
step:617/2035 train_time:20571ms step_avg:33.34ms
step:618/2035 train_time:20604ms step_avg:33.34ms
step:619/2035 train_time:20637ms step_avg:33.34ms
step:620/2035 train_time:20670ms step_avg:33.34ms
step:621/2035 train_time:20703ms step_avg:33.34ms
step:622/2035 train_time:20736ms step_avg:33.34ms
step:623/2035 train_time:20769ms step_avg:33.34ms
step:624/2035 train_time:20802ms step_avg:33.34ms
step:625/2035 train_time:20835ms step_avg:33.34ms
step:626/2035 train_time:20868ms step_avg:33.34ms
step:627/2035 train_time:20901ms step_avg:33.33ms
step:628/2035 train_time:20934ms step_avg:33.33ms
step:629/2035 train_time:20967ms step_avg:33.33ms
step:630/2035 train_time:21000ms step_avg:33.33ms
step:631/2035 train_time:21033ms step_avg:33.33ms
step:632/2035 train_time:21066ms step_avg:33.33ms
step:633/2035 train_time:21099ms step_avg:33.33ms
step:634/2035 train_time:21132ms step_avg:33.33ms
step:635/2035 train_time:21165ms step_avg:33.33ms
step:636/2035 train_time:21198ms step_avg:33.33ms
step:637/2035 train_time:21231ms step_avg:33.33ms
step:638/2035 train_time:21264ms step_avg:33.33ms
step:639/2035 train_time:21298ms step_avg:33.33ms
step:640/2035 train_time:21331ms step_avg:33.33ms
step:641/2035 train_time:21364ms step_avg:33.33ms
step:642/2035 train_time:21397ms step_avg:33.33ms
step:643/2035 train_time:21429ms step_avg:33.33ms
step:644/2035 train_time:21462ms step_avg:33.33ms
step:645/2035 train_time:21495ms step_avg:33.33ms
step:646/2035 train_time:21528ms step_avg:33.33ms
step:647/2035 train_time:21561ms step_avg:33.33ms
step:648/2035 train_time:21595ms step_avg:33.32ms
step:649/2035 train_time:21627ms step_avg:33.32ms
step:650/2035 train_time:21660ms step_avg:33.32ms
step:651/2035 train_time:21693ms step_avg:33.32ms
step:652/2035 train_time:21726ms step_avg:33.32ms
step:653/2035 train_time:21759ms step_avg:33.32ms
step:654/2035 train_time:21792ms step_avg:33.32ms
step:655/2035 train_time:21825ms step_avg:33.32ms
step:656/2035 train_time:21858ms step_avg:33.32ms
step:657/2035 train_time:21891ms step_avg:33.32ms
step:658/2035 train_time:21924ms step_avg:33.32ms
step:659/2035 train_time:21957ms step_avg:33.32ms
step:660/2035 train_time:21990ms step_avg:33.32ms
step:661/2035 train_time:22023ms step_avg:33.32ms
step:662/2035 train_time:22056ms step_avg:33.32ms
step:663/2035 train_time:22089ms step_avg:33.32ms
step:664/2035 train_time:22122ms step_avg:33.32ms
step:665/2035 train_time:22155ms step_avg:33.32ms
step:666/2035 train_time:22188ms step_avg:33.32ms
step:667/2035 train_time:22247ms step_avg:33.35ms
step:668/2035 train_time:22307ms step_avg:33.39ms
step:669/2035 train_time:22367ms step_avg:33.43ms
step:670/2035 train_time:22426ms step_avg:33.47ms
step:671/2035 train_time:22488ms step_avg:33.51ms
step:672/2035 train_time:22547ms step_avg:33.55ms
step:673/2035 train_time:22607ms step_avg:33.59ms
step:674/2035 train_time:22667ms step_avg:33.63ms
step:675/2035 train_time:22728ms step_avg:33.67ms
step:676/2035 train_time:22788ms step_avg:33.71ms
step:677/2035 train_time:22848ms step_avg:33.75ms
step:678/2035 train_time:22907ms step_avg:33.79ms
step:679/2035 train_time:22968ms step_avg:33.83ms
step:680/2035 train_time:23027ms step_avg:33.86ms
step:681/2035 train_time:23088ms step_avg:33.90ms
step:682/2035 train_time:23147ms step_avg:33.94ms
step:683/2035 train_time:23207ms step_avg:33.98ms
step:684/2035 train_time:23266ms step_avg:34.01ms
step:685/2035 train_time:23327ms step_avg:34.05ms
step:686/2035 train_time:23386ms step_avg:34.09ms
step:687/2035 train_time:23447ms step_avg:34.13ms
step:688/2035 train_time:23506ms step_avg:34.17ms
step:689/2035 train_time:23567ms step_avg:34.20ms
step:690/2035 train_time:23627ms step_avg:34.24ms
step:691/2035 train_time:23689ms step_avg:34.28ms
step:692/2035 train_time:23748ms step_avg:34.32ms
step:693/2035 train_time:23809ms step_avg:34.36ms
step:694/2035 train_time:23868ms step_avg:34.39ms
step:695/2035 train_time:23929ms step_avg:34.43ms
step:696/2035 train_time:23989ms step_avg:34.47ms
step:697/2035 train_time:24050ms step_avg:34.50ms
step:698/2035 train_time:24109ms step_avg:34.54ms
step:699/2035 train_time:24169ms step_avg:34.58ms
step:700/2035 train_time:24228ms step_avg:34.61ms
step:701/2035 train_time:24289ms step_avg:34.65ms
step:702/2035 train_time:24348ms step_avg:34.68ms
step:703/2035 train_time:24409ms step_avg:34.72ms
step:704/2035 train_time:24469ms step_avg:34.76ms
step:705/2035 train_time:24529ms step_avg:34.79ms
step:706/2035 train_time:24589ms step_avg:34.83ms
step:707/2035 train_time:24649ms step_avg:34.86ms
step:708/2035 train_time:24709ms step_avg:34.90ms
step:709/2035 train_time:24770ms step_avg:34.94ms
step:710/2035 train_time:24829ms step_avg:34.97ms
step:711/2035 train_time:24890ms step_avg:35.01ms
step:712/2035 train_time:24950ms step_avg:35.04ms
step:713/2035 train_time:25010ms step_avg:35.08ms
step:714/2035 train_time:25070ms step_avg:35.11ms
step:715/2035 train_time:25130ms step_avg:35.15ms
step:716/2035 train_time:25189ms step_avg:35.18ms
step:717/2035 train_time:25250ms step_avg:35.22ms
step:718/2035 train_time:25310ms step_avg:35.25ms
step:719/2035 train_time:25370ms step_avg:35.29ms
step:720/2035 train_time:25429ms step_avg:35.32ms
step:721/2035 train_time:25490ms step_avg:35.35ms
step:722/2035 train_time:25549ms step_avg:35.39ms
step:723/2035 train_time:25610ms step_avg:35.42ms
step:724/2035 train_time:25671ms step_avg:35.46ms
step:725/2035 train_time:25731ms step_avg:35.49ms
step:726/2035 train_time:25790ms step_avg:35.52ms
step:727/2035 train_time:25851ms step_avg:35.56ms
step:728/2035 train_time:25911ms step_avg:35.59ms
step:729/2035 train_time:25972ms step_avg:35.63ms
step:730/2035 train_time:26032ms step_avg:35.66ms
step:731/2035 train_time:26092ms step_avg:35.69ms
step:732/2035 train_time:26152ms step_avg:35.73ms
step:733/2035 train_time:26212ms step_avg:35.76ms
step:734/2035 train_time:26272ms step_avg:35.79ms
step:735/2035 train_time:26332ms step_avg:35.83ms
step:736/2035 train_time:26391ms step_avg:35.86ms
step:737/2035 train_time:26452ms step_avg:35.89ms
step:738/2035 train_time:26511ms step_avg:35.92ms
step:739/2035 train_time:26572ms step_avg:35.96ms
step:740/2035 train_time:26632ms step_avg:35.99ms
step:741/2035 train_time:26693ms step_avg:36.02ms
step:742/2035 train_time:26752ms step_avg:36.05ms
step:743/2035 train_time:26813ms step_avg:36.09ms
step:744/2035 train_time:26872ms step_avg:36.12ms
step:745/2035 train_time:26933ms step_avg:36.15ms
step:746/2035 train_time:26993ms step_avg:36.18ms
step:747/2035 train_time:27053ms step_avg:36.22ms
step:748/2035 train_time:27113ms step_avg:36.25ms
step:749/2035 train_time:27173ms step_avg:36.28ms
step:750/2035 train_time:27233ms step_avg:36.31ms
step:750/2035 val_loss:3.8275 train_time:27295ms step_avg:36.39ms
step:751/2035 train_time:27315ms step_avg:36.37ms
step:752/2035 train_time:27354ms step_avg:36.37ms
step:753/2035 train_time:27416ms step_avg:36.41ms
step:754/2035 train_time:27478ms step_avg:36.44ms
step:755/2035 train_time:27540ms step_avg:36.48ms
step:756/2035 train_time:27599ms step_avg:36.51ms
step:757/2035 train_time:27659ms step_avg:36.54ms
step:758/2035 train_time:27718ms step_avg:36.57ms
step:759/2035 train_time:27777ms step_avg:36.60ms
step:760/2035 train_time:27836ms step_avg:36.63ms
step:761/2035 train_time:27896ms step_avg:36.66ms
step:762/2035 train_time:27955ms step_avg:36.69ms
step:763/2035 train_time:28015ms step_avg:36.72ms
step:764/2035 train_time:28074ms step_avg:36.75ms
step:765/2035 train_time:28134ms step_avg:36.78ms
step:766/2035 train_time:28194ms step_avg:36.81ms
step:767/2035 train_time:28256ms step_avg:36.84ms
step:768/2035 train_time:28317ms step_avg:36.87ms
step:769/2035 train_time:28378ms step_avg:36.90ms
step:770/2035 train_time:28439ms step_avg:36.93ms
step:771/2035 train_time:28501ms step_avg:36.97ms
step:772/2035 train_time:28561ms step_avg:37.00ms
step:773/2035 train_time:28622ms step_avg:37.03ms
step:774/2035 train_time:28681ms step_avg:37.06ms
step:775/2035 train_time:28741ms step_avg:37.09ms
step:776/2035 train_time:28800ms step_avg:37.11ms
step:777/2035 train_time:28861ms step_avg:37.14ms
step:778/2035 train_time:28920ms step_avg:37.17ms
step:779/2035 train_time:28980ms step_avg:37.20ms
step:780/2035 train_time:29039ms step_avg:37.23ms
step:781/2035 train_time:29099ms step_avg:37.26ms
step:782/2035 train_time:29158ms step_avg:37.29ms
step:783/2035 train_time:29218ms step_avg:37.32ms
step:784/2035 train_time:29279ms step_avg:37.35ms
step:785/2035 train_time:29341ms step_avg:37.38ms
step:786/2035 train_time:29401ms step_avg:37.41ms
step:787/2035 train_time:29462ms step_avg:37.44ms
step:788/2035 train_time:29522ms step_avg:37.46ms
step:789/2035 train_time:29583ms step_avg:37.49ms
step:790/2035 train_time:29642ms step_avg:37.52ms
step:791/2035 train_time:29703ms step_avg:37.55ms
step:792/2035 train_time:29762ms step_avg:37.58ms
step:793/2035 train_time:29822ms step_avg:37.61ms
step:794/2035 train_time:29881ms step_avg:37.63ms
step:795/2035 train_time:29942ms step_avg:37.66ms
step:796/2035 train_time:30001ms step_avg:37.69ms
step:797/2035 train_time:30062ms step_avg:37.72ms
step:798/2035 train_time:30121ms step_avg:37.75ms
step:799/2035 train_time:30181ms step_avg:37.77ms
step:800/2035 train_time:30241ms step_avg:37.80ms
step:801/2035 train_time:30302ms step_avg:37.83ms
step:802/2035 train_time:30362ms step_avg:37.86ms
step:803/2035 train_time:30423ms step_avg:37.89ms
step:804/2035 train_time:30483ms step_avg:37.91ms
step:805/2035 train_time:30544ms step_avg:37.94ms
step:806/2035 train_time:30603ms step_avg:37.97ms
step:807/2035 train_time:30664ms step_avg:38.00ms
step:808/2035 train_time:30723ms step_avg:38.02ms
step:809/2035 train_time:30783ms step_avg:38.05ms
step:810/2035 train_time:30842ms step_avg:38.08ms
step:811/2035 train_time:30903ms step_avg:38.10ms
step:812/2035 train_time:30962ms step_avg:38.13ms
step:813/2035 train_time:31022ms step_avg:38.16ms
step:814/2035 train_time:31081ms step_avg:38.18ms
step:815/2035 train_time:31142ms step_avg:38.21ms
step:816/2035 train_time:31201ms step_avg:38.24ms
step:817/2035 train_time:31262ms step_avg:38.26ms
step:818/2035 train_time:31322ms step_avg:38.29ms
step:819/2035 train_time:31383ms step_avg:38.32ms
step:820/2035 train_time:31444ms step_avg:38.35ms
step:821/2035 train_time:31504ms step_avg:38.37ms
step:822/2035 train_time:31563ms step_avg:38.40ms
step:823/2035 train_time:31624ms step_avg:38.43ms
step:824/2035 train_time:31683ms step_avg:38.45ms
step:825/2035 train_time:31744ms step_avg:38.48ms
step:826/2035 train_time:31803ms step_avg:38.50ms
step:827/2035 train_time:31863ms step_avg:38.53ms
step:828/2035 train_time:31923ms step_avg:38.55ms
step:829/2035 train_time:31983ms step_avg:38.58ms
step:830/2035 train_time:32042ms step_avg:38.60ms
step:831/2035 train_time:32102ms step_avg:38.63ms
step:832/2035 train_time:32162ms step_avg:38.66ms
step:833/2035 train_time:32222ms step_avg:38.68ms
step:834/2035 train_time:32281ms step_avg:38.71ms
step:835/2035 train_time:32342ms step_avg:38.73ms
step:836/2035 train_time:32402ms step_avg:38.76ms
step:837/2035 train_time:32463ms step_avg:38.78ms
step:838/2035 train_time:32522ms step_avg:38.81ms
step:839/2035 train_time:32583ms step_avg:38.84ms
step:840/2035 train_time:32642ms step_avg:38.86ms
step:841/2035 train_time:32703ms step_avg:38.89ms
step:842/2035 train_time:32762ms step_avg:38.91ms
step:843/2035 train_time:32823ms step_avg:38.94ms
step:844/2035 train_time:32882ms step_avg:38.96ms
step:845/2035 train_time:32942ms step_avg:38.98ms
step:846/2035 train_time:33001ms step_avg:39.01ms
step:847/2035 train_time:33062ms step_avg:39.03ms
step:848/2035 train_time:33122ms step_avg:39.06ms
step:849/2035 train_time:33183ms step_avg:39.08ms
step:850/2035 train_time:33242ms step_avg:39.11ms
step:851/2035 train_time:33303ms step_avg:39.13ms
step:852/2035 train_time:33363ms step_avg:39.16ms
step:853/2035 train_time:33423ms step_avg:39.18ms
step:854/2035 train_time:33483ms step_avg:39.21ms
step:855/2035 train_time:33543ms step_avg:39.23ms
step:856/2035 train_time:33603ms step_avg:39.26ms
step:857/2035 train_time:33663ms step_avg:39.28ms
step:858/2035 train_time:33723ms step_avg:39.30ms
step:859/2035 train_time:33784ms step_avg:39.33ms
step:860/2035 train_time:33843ms step_avg:39.35ms
step:861/2035 train_time:33904ms step_avg:39.38ms
step:862/2035 train_time:33963ms step_avg:39.40ms
step:863/2035 train_time:34024ms step_avg:39.42ms
step:864/2035 train_time:34082ms step_avg:39.45ms
step:865/2035 train_time:34143ms step_avg:39.47ms
step:866/2035 train_time:34203ms step_avg:39.49ms
step:867/2035 train_time:34263ms step_avg:39.52ms
step:868/2035 train_time:34323ms step_avg:39.54ms
step:869/2035 train_time:34384ms step_avg:39.57ms
step:870/2035 train_time:34443ms step_avg:39.59ms
step:871/2035 train_time:34503ms step_avg:39.61ms
step:872/2035 train_time:34563ms step_avg:39.64ms
step:873/2035 train_time:34623ms step_avg:39.66ms
step:874/2035 train_time:34683ms step_avg:39.68ms
step:875/2035 train_time:34743ms step_avg:39.71ms
step:876/2035 train_time:34802ms step_avg:39.73ms
step:877/2035 train_time:34862ms step_avg:39.75ms
step:878/2035 train_time:34922ms step_avg:39.77ms
step:879/2035 train_time:34982ms step_avg:39.80ms
step:880/2035 train_time:35041ms step_avg:39.82ms
step:881/2035 train_time:35102ms step_avg:39.84ms
step:882/2035 train_time:35162ms step_avg:39.87ms
step:883/2035 train_time:35223ms step_avg:39.89ms
step:884/2035 train_time:35282ms step_avg:39.91ms
step:885/2035 train_time:35343ms step_avg:39.94ms
step:886/2035 train_time:35402ms step_avg:39.96ms
step:887/2035 train_time:35463ms step_avg:39.98ms
step:888/2035 train_time:35523ms step_avg:40.00ms
step:889/2035 train_time:35583ms step_avg:40.03ms
step:890/2035 train_time:35643ms step_avg:40.05ms
step:891/2035 train_time:35704ms step_avg:40.07ms
step:892/2035 train_time:35763ms step_avg:40.09ms
step:893/2035 train_time:35823ms step_avg:40.12ms
step:894/2035 train_time:35882ms step_avg:40.14ms
step:895/2035 train_time:35943ms step_avg:40.16ms
step:896/2035 train_time:36002ms step_avg:40.18ms
step:897/2035 train_time:36063ms step_avg:40.20ms
step:898/2035 train_time:36123ms step_avg:40.23ms
step:899/2035 train_time:36183ms step_avg:40.25ms
step:900/2035 train_time:36242ms step_avg:40.27ms
step:901/2035 train_time:36303ms step_avg:40.29ms
step:902/2035 train_time:36362ms step_avg:40.31ms
step:903/2035 train_time:36423ms step_avg:40.34ms
step:904/2035 train_time:36483ms step_avg:40.36ms
step:905/2035 train_time:36543ms step_avg:40.38ms
step:906/2035 train_time:36603ms step_avg:40.40ms
step:907/2035 train_time:36664ms step_avg:40.42ms
step:908/2035 train_time:36723ms step_avg:40.44ms
step:909/2035 train_time:36783ms step_avg:40.47ms
step:910/2035 train_time:36842ms step_avg:40.49ms
step:911/2035 train_time:36903ms step_avg:40.51ms
step:912/2035 train_time:36961ms step_avg:40.53ms
step:913/2035 train_time:37022ms step_avg:40.55ms
step:914/2035 train_time:37081ms step_avg:40.57ms
step:915/2035 train_time:37142ms step_avg:40.59ms
step:916/2035 train_time:37201ms step_avg:40.61ms
step:917/2035 train_time:37262ms step_avg:40.63ms
step:918/2035 train_time:37323ms step_avg:40.66ms
step:919/2035 train_time:37383ms step_avg:40.68ms
step:920/2035 train_time:37442ms step_avg:40.70ms
step:921/2035 train_time:37503ms step_avg:40.72ms
step:922/2035 train_time:37563ms step_avg:40.74ms
step:923/2035 train_time:37623ms step_avg:40.76ms
step:924/2035 train_time:37682ms step_avg:40.78ms
step:925/2035 train_time:37743ms step_avg:40.80ms
step:926/2035 train_time:37802ms step_avg:40.82ms
step:927/2035 train_time:37863ms step_avg:40.84ms
step:928/2035 train_time:37922ms step_avg:40.86ms
step:929/2035 train_time:37983ms step_avg:40.89ms
step:930/2035 train_time:38042ms step_avg:40.91ms
step:931/2035 train_time:38103ms step_avg:40.93ms
step:932/2035 train_time:38163ms step_avg:40.95ms
step:933/2035 train_time:38224ms step_avg:40.97ms
step:934/2035 train_time:38283ms step_avg:40.99ms
step:935/2035 train_time:38344ms step_avg:41.01ms
step:936/2035 train_time:38403ms step_avg:41.03ms
step:937/2035 train_time:38465ms step_avg:41.05ms
step:938/2035 train_time:38524ms step_avg:41.07ms
step:939/2035 train_time:38585ms step_avg:41.09ms
step:940/2035 train_time:38645ms step_avg:41.11ms
step:941/2035 train_time:38706ms step_avg:41.13ms
step:942/2035 train_time:38765ms step_avg:41.15ms
step:943/2035 train_time:38826ms step_avg:41.17ms
step:944/2035 train_time:38885ms step_avg:41.19ms
step:945/2035 train_time:38946ms step_avg:41.21ms
step:946/2035 train_time:39006ms step_avg:41.23ms
step:947/2035 train_time:39066ms step_avg:41.25ms
step:948/2035 train_time:39126ms step_avg:41.27ms
step:949/2035 train_time:39186ms step_avg:41.29ms
step:950/2035 train_time:39245ms step_avg:41.31ms
step:951/2035 train_time:39307ms step_avg:41.33ms
step:952/2035 train_time:39367ms step_avg:41.35ms
step:953/2035 train_time:39427ms step_avg:41.37ms
step:954/2035 train_time:39487ms step_avg:41.39ms
step:955/2035 train_time:39547ms step_avg:41.41ms
step:956/2035 train_time:39607ms step_avg:41.43ms
step:957/2035 train_time:39667ms step_avg:41.45ms
step:958/2035 train_time:39726ms step_avg:41.47ms
step:959/2035 train_time:39787ms step_avg:41.49ms
step:960/2035 train_time:39847ms step_avg:41.51ms
step:961/2035 train_time:39907ms step_avg:41.53ms
step:962/2035 train_time:39967ms step_avg:41.55ms
step:963/2035 train_time:40027ms step_avg:41.56ms
step:964/2035 train_time:40087ms step_avg:41.58ms
step:965/2035 train_time:40147ms step_avg:41.60ms
step:966/2035 train_time:40207ms step_avg:41.62ms
step:967/2035 train_time:40268ms step_avg:41.64ms
step:968/2035 train_time:40327ms step_avg:41.66ms
step:969/2035 train_time:40388ms step_avg:41.68ms
step:970/2035 train_time:40448ms step_avg:41.70ms
step:971/2035 train_time:40508ms step_avg:41.72ms
step:972/2035 train_time:40568ms step_avg:41.74ms
step:973/2035 train_time:40628ms step_avg:41.76ms
step:974/2035 train_time:40687ms step_avg:41.77ms
step:975/2035 train_time:40747ms step_avg:41.79ms
step:976/2035 train_time:40807ms step_avg:41.81ms
step:977/2035 train_time:40868ms step_avg:41.83ms
step:978/2035 train_time:40928ms step_avg:41.85ms
step:979/2035 train_time:40988ms step_avg:41.87ms
step:980/2035 train_time:41046ms step_avg:41.88ms
step:981/2035 train_time:41107ms step_avg:41.90ms
step:982/2035 train_time:41167ms step_avg:41.92ms
step:983/2035 train_time:41227ms step_avg:41.94ms
step:984/2035 train_time:41287ms step_avg:41.96ms
step:985/2035 train_time:41347ms step_avg:41.98ms
step:986/2035 train_time:41407ms step_avg:42.00ms
step:987/2035 train_time:41468ms step_avg:42.01ms
step:988/2035 train_time:41527ms step_avg:42.03ms
step:989/2035 train_time:41588ms step_avg:42.05ms
step:990/2035 train_time:41648ms step_avg:42.07ms
step:991/2035 train_time:41708ms step_avg:42.09ms
step:992/2035 train_time:41768ms step_avg:42.10ms
step:993/2035 train_time:41828ms step_avg:42.12ms
step:994/2035 train_time:41887ms step_avg:42.14ms
step:995/2035 train_time:41947ms step_avg:42.16ms
step:996/2035 train_time:42006ms step_avg:42.17ms
step:997/2035 train_time:42066ms step_avg:42.19ms
step:998/2035 train_time:42126ms step_avg:42.21ms
step:999/2035 train_time:42186ms step_avg:42.23ms
step:1000/2035 train_time:42245ms step_avg:42.25ms
step:1000/2035 val_loss:3.6877 train_time:42308ms step_avg:42.31ms
step:1001/2035 train_time:42328ms step_avg:42.29ms
step:1002/2035 train_time:42368ms step_avg:42.28ms
step:1003/2035 train_time:42431ms step_avg:42.30ms
step:1004/2035 train_time:42494ms step_avg:42.32ms
step:1005/2035 train_time:42556ms step_avg:42.34ms
step:1006/2035 train_time:42615ms step_avg:42.36ms
step:1007/2035 train_time:42675ms step_avg:42.38ms
step:1008/2035 train_time:42734ms step_avg:42.40ms
step:1009/2035 train_time:42795ms step_avg:42.41ms
step:1010/2035 train_time:42854ms step_avg:42.43ms
step:1011/2035 train_time:42914ms step_avg:42.45ms
step:1012/2035 train_time:42973ms step_avg:42.46ms
step:1013/2035 train_time:43032ms step_avg:42.48ms
step:1014/2035 train_time:43091ms step_avg:42.50ms
step:1015/2035 train_time:43152ms step_avg:42.51ms
step:1016/2035 train_time:43211ms step_avg:42.53ms
step:1017/2035 train_time:43272ms step_avg:42.55ms
step:1018/2035 train_time:43332ms step_avg:42.57ms
step:1019/2035 train_time:43394ms step_avg:42.58ms
step:1020/2035 train_time:43454ms step_avg:42.60ms
step:1021/2035 train_time:43517ms step_avg:42.62ms
step:1022/2035 train_time:43577ms step_avg:42.64ms
step:1023/2035 train_time:43637ms step_avg:42.66ms
step:1024/2035 train_time:43697ms step_avg:42.67ms
step:1025/2035 train_time:43757ms step_avg:42.69ms
step:1026/2035 train_time:43816ms step_avg:42.71ms
step:1027/2035 train_time:43876ms step_avg:42.72ms
step:1028/2035 train_time:43935ms step_avg:42.74ms
step:1029/2035 train_time:43996ms step_avg:42.76ms
step:1030/2035 train_time:44055ms step_avg:42.77ms
step:1031/2035 train_time:44115ms step_avg:42.79ms
step:1032/2035 train_time:44175ms step_avg:42.80ms
step:1033/2035 train_time:44236ms step_avg:42.82ms
step:1034/2035 train_time:44296ms step_avg:42.84ms
step:1035/2035 train_time:44358ms step_avg:42.86ms
step:1036/2035 train_time:44418ms step_avg:42.87ms
step:1037/2035 train_time:44479ms step_avg:42.89ms
step:1038/2035 train_time:44538ms step_avg:42.91ms
step:1039/2035 train_time:44600ms step_avg:42.93ms
step:1040/2035 train_time:44659ms step_avg:42.94ms
step:1041/2035 train_time:44719ms step_avg:42.96ms
step:1042/2035 train_time:44778ms step_avg:42.97ms
step:1043/2035 train_time:44839ms step_avg:42.99ms
step:1044/2035 train_time:44898ms step_avg:43.01ms
step:1045/2035 train_time:44958ms step_avg:43.02ms
step:1046/2035 train_time:45017ms step_avg:43.04ms
step:1047/2035 train_time:45077ms step_avg:43.05ms
step:1048/2035 train_time:45137ms step_avg:43.07ms
step:1049/2035 train_time:45198ms step_avg:43.09ms
step:1050/2035 train_time:45257ms step_avg:43.10ms
step:1051/2035 train_time:45318ms step_avg:43.12ms
step:1052/2035 train_time:45378ms step_avg:43.14ms
step:1053/2035 train_time:45439ms step_avg:43.15ms
step:1054/2035 train_time:45499ms step_avg:43.17ms
step:1055/2035 train_time:45560ms step_avg:43.18ms
step:1056/2035 train_time:45619ms step_avg:43.20ms
step:1057/2035 train_time:45680ms step_avg:43.22ms
step:1058/2035 train_time:45739ms step_avg:43.23ms
step:1059/2035 train_time:45799ms step_avg:43.25ms
step:1060/2035 train_time:45859ms step_avg:43.26ms
step:1061/2035 train_time:45919ms step_avg:43.28ms
step:1062/2035 train_time:45979ms step_avg:43.29ms
step:1063/2035 train_time:46039ms step_avg:43.31ms
step:1064/2035 train_time:46098ms step_avg:43.33ms
step:1065/2035 train_time:46159ms step_avg:43.34ms
step:1066/2035 train_time:46218ms step_avg:43.36ms
step:1067/2035 train_time:46279ms step_avg:43.37ms
step:1068/2035 train_time:46339ms step_avg:43.39ms
step:1069/2035 train_time:46400ms step_avg:43.40ms
step:1070/2035 train_time:46459ms step_avg:43.42ms
step:1071/2035 train_time:46520ms step_avg:43.44ms
step:1072/2035 train_time:46580ms step_avg:43.45ms
step:1073/2035 train_time:46641ms step_avg:43.47ms
step:1074/2035 train_time:46700ms step_avg:43.48ms
step:1075/2035 train_time:46761ms step_avg:43.50ms
step:1076/2035 train_time:46820ms step_avg:43.51ms
step:1077/2035 train_time:46881ms step_avg:43.53ms
step:1078/2035 train_time:46940ms step_avg:43.54ms
step:1079/2035 train_time:47001ms step_avg:43.56ms
step:1080/2035 train_time:47060ms step_avg:43.57ms
step:1081/2035 train_time:47120ms step_avg:43.59ms
step:1082/2035 train_time:47180ms step_avg:43.60ms
step:1083/2035 train_time:47240ms step_avg:43.62ms
step:1084/2035 train_time:47299ms step_avg:43.63ms
step:1085/2035 train_time:47360ms step_avg:43.65ms
step:1086/2035 train_time:47420ms step_avg:43.66ms
step:1087/2035 train_time:47481ms step_avg:43.68ms
step:1088/2035 train_time:47541ms step_avg:43.70ms
step:1089/2035 train_time:47601ms step_avg:43.71ms
step:1090/2035 train_time:47660ms step_avg:43.73ms
step:1091/2035 train_time:47721ms step_avg:43.74ms
step:1092/2035 train_time:47780ms step_avg:43.75ms
step:1093/2035 train_time:47841ms step_avg:43.77ms
step:1094/2035 train_time:47900ms step_avg:43.78ms
step:1095/2035 train_time:47960ms step_avg:43.80ms
step:1096/2035 train_time:48020ms step_avg:43.81ms
step:1097/2035 train_time:48080ms step_avg:43.83ms
step:1098/2035 train_time:48139ms step_avg:43.84ms
step:1099/2035 train_time:48199ms step_avg:43.86ms
step:1100/2035 train_time:48259ms step_avg:43.87ms
step:1101/2035 train_time:48320ms step_avg:43.89ms
step:1102/2035 train_time:48380ms step_avg:43.90ms
step:1103/2035 train_time:48440ms step_avg:43.92ms
step:1104/2035 train_time:48500ms step_avg:43.93ms
step:1105/2035 train_time:48560ms step_avg:43.95ms
step:1106/2035 train_time:48620ms step_avg:43.96ms
step:1107/2035 train_time:48680ms step_avg:43.97ms
step:1108/2035 train_time:48739ms step_avg:43.99ms
step:1109/2035 train_time:48800ms step_avg:44.00ms
step:1110/2035 train_time:48859ms step_avg:44.02ms
step:1111/2035 train_time:48920ms step_avg:44.03ms
step:1112/2035 train_time:48980ms step_avg:44.05ms
step:1113/2035 train_time:49040ms step_avg:44.06ms
step:1114/2035 train_time:49099ms step_avg:44.07ms
step:1115/2035 train_time:49159ms step_avg:44.09ms
step:1116/2035 train_time:49218ms step_avg:44.10ms
step:1117/2035 train_time:49279ms step_avg:44.12ms
step:1118/2035 train_time:49339ms step_avg:44.13ms
step:1119/2035 train_time:49399ms step_avg:44.15ms
step:1120/2035 train_time:49459ms step_avg:44.16ms
step:1121/2035 train_time:49519ms step_avg:44.17ms
step:1122/2035 train_time:49579ms step_avg:44.19ms
step:1123/2035 train_time:49639ms step_avg:44.20ms
step:1124/2035 train_time:49699ms step_avg:44.22ms
step:1125/2035 train_time:49759ms step_avg:44.23ms
step:1126/2035 train_time:49818ms step_avg:44.24ms
step:1127/2035 train_time:49878ms step_avg:44.26ms
step:1128/2035 train_time:49938ms step_avg:44.27ms
step:1129/2035 train_time:49998ms step_avg:44.29ms
step:1130/2035 train_time:50057ms step_avg:44.30ms
step:1131/2035 train_time:50118ms step_avg:44.31ms
step:1132/2035 train_time:50178ms step_avg:44.33ms
step:1133/2035 train_time:50239ms step_avg:44.34ms
step:1134/2035 train_time:50298ms step_avg:44.35ms
step:1135/2035 train_time:50359ms step_avg:44.37ms
step:1136/2035 train_time:50419ms step_avg:44.38ms
step:1137/2035 train_time:50480ms step_avg:44.40ms
step:1138/2035 train_time:50539ms step_avg:44.41ms
step:1139/2035 train_time:50599ms step_avg:44.42ms
step:1140/2035 train_time:50658ms step_avg:44.44ms
step:1141/2035 train_time:50718ms step_avg:44.45ms
step:1142/2035 train_time:50777ms step_avg:44.46ms
step:1143/2035 train_time:50838ms step_avg:44.48ms
step:1144/2035 train_time:50897ms step_avg:44.49ms
step:1145/2035 train_time:50958ms step_avg:44.50ms
step:1146/2035 train_time:51017ms step_avg:44.52ms
step:1147/2035 train_time:51078ms step_avg:44.53ms
step:1148/2035 train_time:51138ms step_avg:44.55ms
step:1149/2035 train_time:51199ms step_avg:44.56ms
step:1150/2035 train_time:51259ms step_avg:44.57ms
step:1151/2035 train_time:51319ms step_avg:44.59ms
step:1152/2035 train_time:51379ms step_avg:44.60ms
step:1153/2035 train_time:51439ms step_avg:44.61ms
step:1154/2035 train_time:51499ms step_avg:44.63ms
step:1155/2035 train_time:51561ms step_avg:44.64ms
step:1156/2035 train_time:51620ms step_avg:44.65ms
step:1157/2035 train_time:51680ms step_avg:44.67ms
step:1158/2035 train_time:51740ms step_avg:44.68ms
step:1159/2035 train_time:51800ms step_avg:44.69ms
step:1160/2035 train_time:51859ms step_avg:44.71ms
step:1161/2035 train_time:51919ms step_avg:44.72ms
step:1162/2035 train_time:51979ms step_avg:44.73ms
step:1163/2035 train_time:52039ms step_avg:44.75ms
step:1164/2035 train_time:52098ms step_avg:44.76ms
step:1165/2035 train_time:52159ms step_avg:44.77ms
step:1166/2035 train_time:52218ms step_avg:44.78ms
step:1167/2035 train_time:52280ms step_avg:44.80ms
step:1168/2035 train_time:52339ms step_avg:44.81ms
step:1169/2035 train_time:52399ms step_avg:44.82ms
step:1170/2035 train_time:52459ms step_avg:44.84ms
step:1171/2035 train_time:52520ms step_avg:44.85ms
step:1172/2035 train_time:52579ms step_avg:44.86ms
step:1173/2035 train_time:52640ms step_avg:44.88ms
step:1174/2035 train_time:52699ms step_avg:44.89ms
step:1175/2035 train_time:52760ms step_avg:44.90ms
step:1176/2035 train_time:52820ms step_avg:44.92ms
step:1177/2035 train_time:52880ms step_avg:44.93ms
step:1178/2035 train_time:52940ms step_avg:44.94ms
step:1179/2035 train_time:53000ms step_avg:44.95ms
step:1180/2035 train_time:53059ms step_avg:44.97ms
step:1181/2035 train_time:53120ms step_avg:44.98ms
step:1182/2035 train_time:53179ms step_avg:44.99ms
step:1183/2035 train_time:53240ms step_avg:45.00ms
step:1184/2035 train_time:53300ms step_avg:45.02ms
step:1185/2035 train_time:53360ms step_avg:45.03ms
step:1186/2035 train_time:53420ms step_avg:45.04ms
step:1187/2035 train_time:53480ms step_avg:45.06ms
step:1188/2035 train_time:53540ms step_avg:45.07ms
step:1189/2035 train_time:53601ms step_avg:45.08ms
step:1190/2035 train_time:53660ms step_avg:45.09ms
step:1191/2035 train_time:53721ms step_avg:45.11ms
step:1192/2035 train_time:53781ms step_avg:45.12ms
step:1193/2035 train_time:53841ms step_avg:45.13ms
step:1194/2035 train_time:53901ms step_avg:45.14ms
step:1195/2035 train_time:53962ms step_avg:45.16ms
step:1196/2035 train_time:54020ms step_avg:45.17ms
step:1197/2035 train_time:54080ms step_avg:45.18ms
step:1198/2035 train_time:54140ms step_avg:45.19ms
step:1199/2035 train_time:54200ms step_avg:45.20ms
step:1200/2035 train_time:54260ms step_avg:45.22ms
step:1201/2035 train_time:54322ms step_avg:45.23ms
step:1202/2035 train_time:54381ms step_avg:45.24ms
step:1203/2035 train_time:54441ms step_avg:45.25ms
step:1204/2035 train_time:54500ms step_avg:45.27ms
step:1205/2035 train_time:54561ms step_avg:45.28ms
step:1206/2035 train_time:54621ms step_avg:45.29ms
step:1207/2035 train_time:54681ms step_avg:45.30ms
step:1208/2035 train_time:54740ms step_avg:45.31ms
step:1209/2035 train_time:54800ms step_avg:45.33ms
step:1210/2035 train_time:54860ms step_avg:45.34ms
step:1211/2035 train_time:54921ms step_avg:45.35ms
step:1212/2035 train_time:54980ms step_avg:45.36ms
step:1213/2035 train_time:55041ms step_avg:45.38ms
step:1214/2035 train_time:55099ms step_avg:45.39ms
step:1215/2035 train_time:55160ms step_avg:45.40ms
step:1216/2035 train_time:55220ms step_avg:45.41ms
step:1217/2035 train_time:55280ms step_avg:45.42ms
step:1218/2035 train_time:55339ms step_avg:45.43ms
step:1219/2035 train_time:55400ms step_avg:45.45ms
step:1220/2035 train_time:55460ms step_avg:45.46ms
step:1221/2035 train_time:55520ms step_avg:45.47ms
step:1222/2035 train_time:55580ms step_avg:45.48ms
step:1223/2035 train_time:55640ms step_avg:45.50ms
step:1224/2035 train_time:55700ms step_avg:45.51ms
step:1225/2035 train_time:55760ms step_avg:45.52ms
step:1226/2035 train_time:55819ms step_avg:45.53ms
step:1227/2035 train_time:55880ms step_avg:45.54ms
step:1228/2035 train_time:55939ms step_avg:45.55ms
step:1229/2035 train_time:56000ms step_avg:45.57ms
step:1230/2035 train_time:56059ms step_avg:45.58ms
step:1231/2035 train_time:56119ms step_avg:45.59ms
step:1232/2035 train_time:56179ms step_avg:45.60ms
step:1233/2035 train_time:56239ms step_avg:45.61ms
step:1234/2035 train_time:56298ms step_avg:45.62ms
step:1235/2035 train_time:56360ms step_avg:45.64ms
step:1236/2035 train_time:56419ms step_avg:45.65ms
step:1237/2035 train_time:56480ms step_avg:45.66ms
step:1238/2035 train_time:56539ms step_avg:45.67ms
step:1239/2035 train_time:56600ms step_avg:45.68ms
step:1240/2035 train_time:56659ms step_avg:45.69ms
step:1241/2035 train_time:56720ms step_avg:45.70ms
step:1242/2035 train_time:56779ms step_avg:45.72ms
step:1243/2035 train_time:56840ms step_avg:45.73ms
step:1244/2035 train_time:56899ms step_avg:45.74ms
step:1245/2035 train_time:56959ms step_avg:45.75ms
step:1246/2035 train_time:57019ms step_avg:45.76ms
step:1247/2035 train_time:57080ms step_avg:45.77ms
step:1248/2035 train_time:57139ms step_avg:45.78ms
step:1249/2035 train_time:57200ms step_avg:45.80ms
step:1250/2035 train_time:57259ms step_avg:45.81ms
step:1250/2035 val_loss:3.5705 train_time:57322ms step_avg:45.86ms
step:1251/2035 train_time:57342ms step_avg:45.84ms
step:1252/2035 train_time:57382ms step_avg:45.83ms
step:1253/2035 train_time:57444ms step_avg:45.84ms
step:1254/2035 train_time:57505ms step_avg:45.86ms
step:1255/2035 train_time:57566ms step_avg:45.87ms
step:1256/2035 train_time:57625ms step_avg:45.88ms
step:1257/2035 train_time:57685ms step_avg:45.89ms
step:1258/2035 train_time:57744ms step_avg:45.90ms
step:1259/2035 train_time:57804ms step_avg:45.91ms
step:1260/2035 train_time:57863ms step_avg:45.92ms
step:1261/2035 train_time:57924ms step_avg:45.94ms
step:1262/2035 train_time:57983ms step_avg:45.95ms
step:1263/2035 train_time:58043ms step_avg:45.96ms
step:1264/2035 train_time:58102ms step_avg:45.97ms
step:1265/2035 train_time:58163ms step_avg:45.98ms
step:1266/2035 train_time:58222ms step_avg:45.99ms
step:1267/2035 train_time:58284ms step_avg:46.00ms
step:1268/2035 train_time:58345ms step_avg:46.01ms
step:1269/2035 train_time:58407ms step_avg:46.03ms
step:1270/2035 train_time:58467ms step_avg:46.04ms
step:1271/2035 train_time:58528ms step_avg:46.05ms
step:1272/2035 train_time:58587ms step_avg:46.06ms
step:1273/2035 train_time:58647ms step_avg:46.07ms
step:1274/2035 train_time:58706ms step_avg:46.08ms
step:1275/2035 train_time:58766ms step_avg:46.09ms
step:1276/2035 train_time:58826ms step_avg:46.10ms
step:1277/2035 train_time:58886ms step_avg:46.11ms
step:1278/2035 train_time:58946ms step_avg:46.12ms
step:1279/2035 train_time:59006ms step_avg:46.13ms
step:1280/2035 train_time:59065ms step_avg:46.14ms
step:1281/2035 train_time:59126ms step_avg:46.16ms
step:1282/2035 train_time:59186ms step_avg:46.17ms
step:1283/2035 train_time:59248ms step_avg:46.18ms
step:1284/2035 train_time:59308ms step_avg:46.19ms
step:1285/2035 train_time:59369ms step_avg:46.20ms
step:1286/2035 train_time:59429ms step_avg:46.21ms
step:1287/2035 train_time:59490ms step_avg:46.22ms
step:1288/2035 train_time:59549ms step_avg:46.23ms
step:1289/2035 train_time:59610ms step_avg:46.24ms
step:1290/2035 train_time:59669ms step_avg:46.25ms
step:1291/2035 train_time:59729ms step_avg:46.27ms
step:1292/2035 train_time:59788ms step_avg:46.28ms
step:1293/2035 train_time:59848ms step_avg:46.29ms
step:1294/2035 train_time:59907ms step_avg:46.30ms
step:1295/2035 train_time:59968ms step_avg:46.31ms
step:1296/2035 train_time:60027ms step_avg:46.32ms
step:1297/2035 train_time:60087ms step_avg:46.33ms
step:1298/2035 train_time:60146ms step_avg:46.34ms
step:1299/2035 train_time:60207ms step_avg:46.35ms
step:1300/2035 train_time:60267ms step_avg:46.36ms
step:1301/2035 train_time:60327ms step_avg:46.37ms
step:1302/2035 train_time:60387ms step_avg:46.38ms
step:1303/2035 train_time:60449ms step_avg:46.39ms
step:1304/2035 train_time:60508ms step_avg:46.40ms
step:1305/2035 train_time:60569ms step_avg:46.41ms
step:1306/2035 train_time:60628ms step_avg:46.42ms
step:1307/2035 train_time:60689ms step_avg:46.43ms
step:1308/2035 train_time:60748ms step_avg:46.44ms
step:1309/2035 train_time:60810ms step_avg:46.45ms
step:1310/2035 train_time:60869ms step_avg:46.46ms
step:1311/2035 train_time:60929ms step_avg:46.48ms
step:1312/2035 train_time:60988ms step_avg:46.48ms
step:1313/2035 train_time:61048ms step_avg:46.49ms
step:1314/2035 train_time:61107ms step_avg:46.50ms
step:1315/2035 train_time:61168ms step_avg:46.52ms
step:1316/2035 train_time:61227ms step_avg:46.53ms
step:1317/2035 train_time:61288ms step_avg:46.54ms
step:1318/2035 train_time:61348ms step_avg:46.55ms
step:1319/2035 train_time:61409ms step_avg:46.56ms
step:1320/2035 train_time:61468ms step_avg:46.57ms
step:1321/2035 train_time:61529ms step_avg:46.58ms
step:1322/2035 train_time:61588ms step_avg:46.59ms
step:1323/2035 train_time:61648ms step_avg:46.60ms
step:1324/2035 train_time:61708ms step_avg:46.61ms
step:1325/2035 train_time:61768ms step_avg:46.62ms
step:1326/2035 train_time:61827ms step_avg:46.63ms
step:1327/2035 train_time:61888ms step_avg:46.64ms
step:1328/2035 train_time:61947ms step_avg:46.65ms
step:1329/2035 train_time:62007ms step_avg:46.66ms
step:1330/2035 train_time:62067ms step_avg:46.67ms
step:1331/2035 train_time:62128ms step_avg:46.68ms
step:1332/2035 train_time:62216ms step_avg:46.71ms
step:1333/2035 train_time:62305ms step_avg:46.74ms
step:1334/2035 train_time:62392ms step_avg:46.77ms
step:1335/2035 train_time:62482ms step_avg:46.80ms
step:1336/2035 train_time:62570ms step_avg:46.83ms
step:1337/2035 train_time:62658ms step_avg:46.86ms
step:1338/2035 train_time:62744ms step_avg:46.89ms
step:1339/2035 train_time:62832ms step_avg:46.92ms
step:1340/2035 train_time:62920ms step_avg:46.95ms
step:1341/2035 train_time:63008ms step_avg:46.99ms
step:1342/2035 train_time:63095ms step_avg:47.02ms
step:1343/2035 train_time:63183ms step_avg:47.05ms
step:1344/2035 train_time:63270ms step_avg:47.08ms
step:1345/2035 train_time:63360ms step_avg:47.11ms
step:1346/2035 train_time:63447ms step_avg:47.14ms
step:1347/2035 train_time:63535ms step_avg:47.17ms
step:1348/2035 train_time:63623ms step_avg:47.20ms
step:1349/2035 train_time:63711ms step_avg:47.23ms
step:1350/2035 train_time:63798ms step_avg:47.26ms
step:1351/2035 train_time:63886ms step_avg:47.29ms
step:1352/2035 train_time:63973ms step_avg:47.32ms
step:1353/2035 train_time:64061ms step_avg:47.35ms
step:1354/2035 train_time:64148ms step_avg:47.38ms
step:1355/2035 train_time:64236ms step_avg:47.41ms
step:1356/2035 train_time:64324ms step_avg:47.44ms
step:1357/2035 train_time:64412ms step_avg:47.47ms
step:1358/2035 train_time:64500ms step_avg:47.50ms
step:1359/2035 train_time:64588ms step_avg:47.53ms
step:1360/2035 train_time:64676ms step_avg:47.56ms
step:1361/2035 train_time:64764ms step_avg:47.59ms
step:1362/2035 train_time:64851ms step_avg:47.61ms
step:1363/2035 train_time:64939ms step_avg:47.64ms
step:1364/2035 train_time:65025ms step_avg:47.67ms
step:1365/2035 train_time:65113ms step_avg:47.70ms
step:1366/2035 train_time:65200ms step_avg:47.73ms
step:1367/2035 train_time:65288ms step_avg:47.76ms
step:1368/2035 train_time:65375ms step_avg:47.79ms
step:1369/2035 train_time:65464ms step_avg:47.82ms
step:1370/2035 train_time:65551ms step_avg:47.85ms
step:1371/2035 train_time:65640ms step_avg:47.88ms
step:1372/2035 train_time:65727ms step_avg:47.91ms
step:1373/2035 train_time:65815ms step_avg:47.94ms
step:1374/2035 train_time:65901ms step_avg:47.96ms
step:1375/2035 train_time:65990ms step_avg:47.99ms
step:1376/2035 train_time:66076ms step_avg:48.02ms
step:1377/2035 train_time:66165ms step_avg:48.05ms
step:1378/2035 train_time:66251ms step_avg:48.08ms
step:1379/2035 train_time:66341ms step_avg:48.11ms
step:1380/2035 train_time:66427ms step_avg:48.14ms
step:1381/2035 train_time:66515ms step_avg:48.16ms
step:1382/2035 train_time:66603ms step_avg:48.19ms
step:1383/2035 train_time:66692ms step_avg:48.22ms
step:1384/2035 train_time:66780ms step_avg:48.25ms
step:1385/2035 train_time:66867ms step_avg:48.28ms
step:1386/2035 train_time:66954ms step_avg:48.31ms
step:1387/2035 train_time:67041ms step_avg:48.34ms
step:1388/2035 train_time:67129ms step_avg:48.36ms
step:1389/2035 train_time:67216ms step_avg:48.39ms
step:1390/2035 train_time:67303ms step_avg:48.42ms
step:1391/2035 train_time:67392ms step_avg:48.45ms
step:1392/2035 train_time:67480ms step_avg:48.48ms
step:1393/2035 train_time:67568ms step_avg:48.51ms
step:1394/2035 train_time:67655ms step_avg:48.53ms
step:1395/2035 train_time:67743ms step_avg:48.56ms
step:1396/2035 train_time:67830ms step_avg:48.59ms
step:1397/2035 train_time:67918ms step_avg:48.62ms
step:1398/2035 train_time:68006ms step_avg:48.64ms
step:1399/2035 train_time:68094ms step_avg:48.67ms
step:1400/2035 train_time:68180ms step_avg:48.70ms
step:1401/2035 train_time:68268ms step_avg:48.73ms
step:1402/2035 train_time:68355ms step_avg:48.76ms
step:1403/2035 train_time:68444ms step_avg:48.78ms
step:1404/2035 train_time:68531ms step_avg:48.81ms
step:1405/2035 train_time:68620ms step_avg:48.84ms
step:1406/2035 train_time:68707ms step_avg:48.87ms
step:1407/2035 train_time:68795ms step_avg:48.90ms
step:1408/2035 train_time:68882ms step_avg:48.92ms
step:1409/2035 train_time:68971ms step_avg:48.95ms
step:1410/2035 train_time:69058ms step_avg:48.98ms
step:1411/2035 train_time:69148ms step_avg:49.01ms
step:1412/2035 train_time:69235ms step_avg:49.03ms
step:1413/2035 train_time:69323ms step_avg:49.06ms
step:1414/2035 train_time:69410ms step_avg:49.09ms
step:1415/2035 train_time:69498ms step_avg:49.12ms
step:1416/2035 train_time:69586ms step_avg:49.14ms
step:1417/2035 train_time:69674ms step_avg:49.17ms
step:1418/2035 train_time:69760ms step_avg:49.20ms
step:1419/2035 train_time:69848ms step_avg:49.22ms
step:1420/2035 train_time:69935ms step_avg:49.25ms
step:1421/2035 train_time:70023ms step_avg:49.28ms
step:1422/2035 train_time:70110ms step_avg:49.30ms
step:1423/2035 train_time:70198ms step_avg:49.33ms
step:1424/2035 train_time:70285ms step_avg:49.36ms
step:1425/2035 train_time:70373ms step_avg:49.38ms
step:1426/2035 train_time:70460ms step_avg:49.41ms
step:1427/2035 train_time:70548ms step_avg:49.44ms
step:1428/2035 train_time:70635ms step_avg:49.46ms
step:1429/2035 train_time:70724ms step_avg:49.49ms
step:1430/2035 train_time:70812ms step_avg:49.52ms
step:1431/2035 train_time:70900ms step_avg:49.55ms
step:1432/2035 train_time:70987ms step_avg:49.57ms
step:1433/2035 train_time:71074ms step_avg:49.60ms
step:1434/2035 train_time:71161ms step_avg:49.62ms
step:1435/2035 train_time:71248ms step_avg:49.65ms
step:1436/2035 train_time:71336ms step_avg:49.68ms
step:1437/2035 train_time:71424ms step_avg:49.70ms
step:1438/2035 train_time:71512ms step_avg:49.73ms
step:1439/2035 train_time:71600ms step_avg:49.76ms
step:1440/2035 train_time:71687ms step_avg:49.78ms
step:1441/2035 train_time:71775ms step_avg:49.81ms
step:1442/2035 train_time:71861ms step_avg:49.83ms
step:1443/2035 train_time:71949ms step_avg:49.86ms
step:1444/2035 train_time:72036ms step_avg:49.89ms
step:1445/2035 train_time:72124ms step_avg:49.91ms
step:1446/2035 train_time:72211ms step_avg:49.94ms
step:1447/2035 train_time:72298ms step_avg:49.96ms
step:1448/2035 train_time:72385ms step_avg:49.99ms
step:1449/2035 train_time:72474ms step_avg:50.02ms
step:1450/2035 train_time:72561ms step_avg:50.04ms
step:1451/2035 train_time:72649ms step_avg:50.07ms
step:1452/2035 train_time:72735ms step_avg:50.09ms
step:1453/2035 train_time:72823ms step_avg:50.12ms
step:1454/2035 train_time:72911ms step_avg:50.15ms
step:1455/2035 train_time:72999ms step_avg:50.17ms
step:1456/2035 train_time:73086ms step_avg:50.20ms
step:1457/2035 train_time:73175ms step_avg:50.22ms
step:1458/2035 train_time:73261ms step_avg:50.25ms
step:1459/2035 train_time:73349ms step_avg:50.27ms
step:1460/2035 train_time:73437ms step_avg:50.30ms
step:1461/2035 train_time:73526ms step_avg:50.33ms
step:1462/2035 train_time:73612ms step_avg:50.35ms
step:1463/2035 train_time:73701ms step_avg:50.38ms
step:1464/2035 train_time:73787ms step_avg:50.40ms
step:1465/2035 train_time:73876ms step_avg:50.43ms
step:1466/2035 train_time:73963ms step_avg:50.45ms
step:1467/2035 train_time:74051ms step_avg:50.48ms
step:1468/2035 train_time:74138ms step_avg:50.50ms
step:1469/2035 train_time:74226ms step_avg:50.53ms
step:1470/2035 train_time:74313ms step_avg:50.55ms
step:1471/2035 train_time:74401ms step_avg:50.58ms
step:1472/2035 train_time:74489ms step_avg:50.60ms
step:1473/2035 train_time:74578ms step_avg:50.63ms
step:1474/2035 train_time:74665ms step_avg:50.65ms
step:1475/2035 train_time:74753ms step_avg:50.68ms
step:1476/2035 train_time:74840ms step_avg:50.70ms
step:1477/2035 train_time:74928ms step_avg:50.73ms
step:1478/2035 train_time:75015ms step_avg:50.75ms
step:1479/2035 train_time:75103ms step_avg:50.78ms
step:1480/2035 train_time:75190ms step_avg:50.80ms
step:1481/2035 train_time:75279ms step_avg:50.83ms
step:1482/2035 train_time:75365ms step_avg:50.85ms
step:1483/2035 train_time:75453ms step_avg:50.88ms
step:1484/2035 train_time:75541ms step_avg:50.90ms
step:1485/2035 train_time:75630ms step_avg:50.93ms
step:1486/2035 train_time:75717ms step_avg:50.95ms
step:1487/2035 train_time:75805ms step_avg:50.98ms
step:1488/2035 train_time:75892ms step_avg:51.00ms
step:1489/2035 train_time:75980ms step_avg:51.03ms
step:1490/2035 train_time:76067ms step_avg:51.05ms
step:1491/2035 train_time:76155ms step_avg:51.08ms
step:1492/2035 train_time:76242ms step_avg:51.10ms
step:1493/2035 train_time:76330ms step_avg:51.13ms
step:1494/2035 train_time:76417ms step_avg:51.15ms
step:1495/2035 train_time:76505ms step_avg:51.17ms
step:1496/2035 train_time:76592ms step_avg:51.20ms
step:1497/2035 train_time:76682ms step_avg:51.22ms
step:1498/2035 train_time:76769ms step_avg:51.25ms
step:1499/2035 train_time:76856ms step_avg:51.27ms
step:1500/2035 train_time:76944ms step_avg:51.30ms
step:1500/2035 val_loss:3.4542 train_time:77034ms step_avg:51.36ms
step:1501/2035 train_time:77053ms step_avg:51.33ms
step:1502/2035 train_time:77121ms step_avg:51.35ms
step:1503/2035 train_time:77212ms step_avg:51.37ms
step:1504/2035 train_time:77301ms step_avg:51.40ms
step:1505/2035 train_time:77389ms step_avg:51.42ms
step:1506/2035 train_time:77475ms step_avg:51.44ms
step:1507/2035 train_time:77563ms step_avg:51.47ms
step:1508/2035 train_time:77649ms step_avg:51.49ms
step:1509/2035 train_time:77736ms step_avg:51.51ms
step:1510/2035 train_time:77822ms step_avg:51.54ms
step:1511/2035 train_time:77910ms step_avg:51.56ms
step:1512/2035 train_time:77999ms step_avg:51.59ms
step:1513/2035 train_time:78089ms step_avg:51.61ms
step:1514/2035 train_time:78178ms step_avg:51.64ms
step:1515/2035 train_time:78268ms step_avg:51.66ms
step:1516/2035 train_time:78356ms step_avg:51.69ms
step:1517/2035 train_time:78445ms step_avg:51.71ms
step:1518/2035 train_time:78531ms step_avg:51.73ms
step:1519/2035 train_time:78619ms step_avg:51.76ms
step:1520/2035 train_time:78705ms step_avg:51.78ms
step:1521/2035 train_time:78792ms step_avg:51.80ms
step:1522/2035 train_time:78879ms step_avg:51.83ms
step:1523/2035 train_time:78967ms step_avg:51.85ms
step:1524/2035 train_time:79055ms step_avg:51.87ms
step:1525/2035 train_time:79144ms step_avg:51.90ms
step:1526/2035 train_time:79233ms step_avg:51.92ms
step:1527/2035 train_time:79321ms step_avg:51.95ms
step:1528/2035 train_time:79408ms step_avg:51.97ms
step:1529/2035 train_time:79495ms step_avg:51.99ms
step:1530/2035 train_time:79582ms step_avg:52.01ms
step:1531/2035 train_time:79670ms step_avg:52.04ms
step:1532/2035 train_time:79757ms step_avg:52.06ms
step:1533/2035 train_time:79844ms step_avg:52.08ms
step:1534/2035 train_time:79931ms step_avg:52.11ms
step:1535/2035 train_time:80021ms step_avg:52.13ms
step:1536/2035 train_time:80109ms step_avg:52.15ms
step:1537/2035 train_time:80197ms step_avg:52.18ms
step:1538/2035 train_time:80285ms step_avg:52.20ms
step:1539/2035 train_time:80373ms step_avg:52.22ms
step:1540/2035 train_time:80459ms step_avg:52.25ms
step:1541/2035 train_time:80548ms step_avg:52.27ms
step:1542/2035 train_time:80634ms step_avg:52.29ms
step:1543/2035 train_time:80722ms step_avg:52.31ms
step:1544/2035 train_time:80809ms step_avg:52.34ms
step:1545/2035 train_time:80898ms step_avg:52.36ms
step:1546/2035 train_time:80986ms step_avg:52.38ms
step:1547/2035 train_time:81075ms step_avg:52.41ms
step:1548/2035 train_time:81162ms step_avg:52.43ms
step:1549/2035 train_time:81250ms step_avg:52.45ms
step:1550/2035 train_time:81338ms step_avg:52.48ms
step:1551/2035 train_time:81426ms step_avg:52.50ms
step:1552/2035 train_time:81513ms step_avg:52.52ms
step:1553/2035 train_time:81600ms step_avg:52.54ms
step:1554/2035 train_time:81686ms step_avg:52.57ms
step:1555/2035 train_time:81774ms step_avg:52.59ms
step:1556/2035 train_time:81860ms step_avg:52.61ms
step:1557/2035 train_time:81949ms step_avg:52.63ms
step:1558/2035 train_time:82037ms step_avg:52.66ms
step:1559/2035 train_time:82125ms step_avg:52.68ms
step:1560/2035 train_time:82213ms step_avg:52.70ms
step:1561/2035 train_time:82301ms step_avg:52.72ms
step:1562/2035 train_time:82389ms step_avg:52.75ms
step:1563/2035 train_time:82477ms step_avg:52.77ms
step:1564/2035 train_time:82564ms step_avg:52.79ms
step:1565/2035 train_time:82652ms step_avg:52.81ms
step:1566/2035 train_time:82739ms step_avg:52.83ms
step:1567/2035 train_time:82826ms step_avg:52.86ms
step:1568/2035 train_time:82914ms step_avg:52.88ms
step:1569/2035 train_time:83002ms step_avg:52.90ms
step:1570/2035 train_time:83089ms step_avg:52.92ms
step:1571/2035 train_time:83178ms step_avg:52.95ms
step:1572/2035 train_time:83265ms step_avg:52.97ms
step:1573/2035 train_time:83353ms step_avg:52.99ms
step:1574/2035 train_time:83441ms step_avg:53.01ms
step:1575/2035 train_time:83528ms step_avg:53.03ms
step:1576/2035 train_time:83615ms step_avg:53.06ms
step:1577/2035 train_time:83703ms step_avg:53.08ms
step:1578/2035 train_time:83790ms step_avg:53.10ms
step:1579/2035 train_time:83879ms step_avg:53.12ms
step:1580/2035 train_time:83966ms step_avg:53.14ms
step:1581/2035 train_time:84055ms step_avg:53.17ms
step:1582/2035 train_time:84141ms step_avg:53.19ms
step:1583/2035 train_time:84229ms step_avg:53.21ms
step:1584/2035 train_time:84317ms step_avg:53.23ms
step:1585/2035 train_time:84405ms step_avg:53.25ms
step:1586/2035 train_time:84492ms step_avg:53.27ms
step:1587/2035 train_time:84580ms step_avg:53.30ms
step:1588/2035 train_time:84666ms step_avg:53.32ms
step:1589/2035 train_time:84754ms step_avg:53.34ms
step:1590/2035 train_time:84842ms step_avg:53.36ms
step:1591/2035 train_time:84930ms step_avg:53.38ms
step:1592/2035 train_time:85017ms step_avg:53.40ms
step:1593/2035 train_time:85105ms step_avg:53.42ms
step:1594/2035 train_time:85192ms step_avg:53.45ms
step:1595/2035 train_time:85281ms step_avg:53.47ms
step:1596/2035 train_time:85368ms step_avg:53.49ms
step:1597/2035 train_time:85456ms step_avg:53.51ms
step:1598/2035 train_time:85543ms step_avg:53.53ms
step:1599/2035 train_time:85631ms step_avg:53.55ms
step:1600/2035 train_time:85719ms step_avg:53.57ms
step:1601/2035 train_time:85806ms step_avg:53.60ms
step:1602/2035 train_time:85893ms step_avg:53.62ms
step:1603/2035 train_time:85982ms step_avg:53.64ms
step:1604/2035 train_time:86070ms step_avg:53.66ms
step:1605/2035 train_time:86158ms step_avg:53.68ms
step:1606/2035 train_time:86245ms step_avg:53.70ms
step:1607/2035 train_time:86334ms step_avg:53.72ms
step:1608/2035 train_time:86422ms step_avg:53.74ms
step:1609/2035 train_time:86510ms step_avg:53.77ms
step:1610/2035 train_time:86597ms step_avg:53.79ms
step:1611/2035 train_time:86685ms step_avg:53.81ms
step:1612/2035 train_time:86772ms step_avg:53.83ms
step:1613/2035 train_time:86861ms step_avg:53.85ms
step:1614/2035 train_time:86948ms step_avg:53.87ms
step:1615/2035 train_time:87037ms step_avg:53.89ms
step:1616/2035 train_time:87123ms step_avg:53.91ms
step:1617/2035 train_time:87211ms step_avg:53.93ms
step:1618/2035 train_time:87299ms step_avg:53.95ms
step:1619/2035 train_time:87387ms step_avg:53.98ms
step:1620/2035 train_time:87474ms step_avg:54.00ms
step:1621/2035 train_time:87562ms step_avg:54.02ms
step:1622/2035 train_time:87648ms step_avg:54.04ms
step:1623/2035 train_time:87737ms step_avg:54.06ms
step:1624/2035 train_time:87824ms step_avg:54.08ms
step:1625/2035 train_time:87913ms step_avg:54.10ms
step:1626/2035 train_time:88001ms step_avg:54.12ms
step:1627/2035 train_time:88089ms step_avg:54.14ms
step:1628/2035 train_time:88176ms step_avg:54.16ms
step:1629/2035 train_time:88264ms step_avg:54.18ms
step:1630/2035 train_time:88352ms step_avg:54.20ms
step:1631/2035 train_time:88440ms step_avg:54.22ms
step:1632/2035 train_time:88527ms step_avg:54.24ms
step:1633/2035 train_time:88614ms step_avg:54.26ms
step:1634/2035 train_time:88701ms step_avg:54.28ms
step:1635/2035 train_time:88789ms step_avg:54.30ms
step:1636/2035 train_time:88876ms step_avg:54.33ms
step:1637/2035 train_time:88965ms step_avg:54.35ms
step:1638/2035 train_time:89051ms step_avg:54.37ms
step:1639/2035 train_time:89140ms step_avg:54.39ms
step:1640/2035 train_time:89228ms step_avg:54.41ms
step:1641/2035 train_time:89316ms step_avg:54.43ms
step:1642/2035 train_time:89403ms step_avg:54.45ms
step:1643/2035 train_time:89491ms step_avg:54.47ms
step:1644/2035 train_time:89579ms step_avg:54.49ms
step:1645/2035 train_time:89667ms step_avg:54.51ms
step:1646/2035 train_time:89754ms step_avg:54.53ms
step:1647/2035 train_time:89841ms step_avg:54.55ms
step:1648/2035 train_time:89928ms step_avg:54.57ms
step:1649/2035 train_time:90017ms step_avg:54.59ms
step:1650/2035 train_time:90104ms step_avg:54.61ms
step:1651/2035 train_time:90192ms step_avg:54.63ms
step:1652/2035 train_time:90279ms step_avg:54.65ms
step:1653/2035 train_time:90368ms step_avg:54.67ms
step:1654/2035 train_time:90455ms step_avg:54.69ms
step:1655/2035 train_time:90543ms step_avg:54.71ms
step:1656/2035 train_time:90631ms step_avg:54.73ms
step:1657/2035 train_time:90719ms step_avg:54.75ms
step:1658/2035 train_time:90806ms step_avg:54.77ms
step:1659/2035 train_time:90894ms step_avg:54.79ms
step:1660/2035 train_time:90981ms step_avg:54.81ms
step:1661/2035 train_time:91068ms step_avg:54.83ms
step:1662/2035 train_time:91156ms step_avg:54.85ms
step:1663/2035 train_time:91244ms step_avg:54.87ms
step:1664/2035 train_time:91331ms step_avg:54.89ms
step:1665/2035 train_time:91420ms step_avg:54.91ms
step:1666/2035 train_time:91506ms step_avg:54.93ms
step:1667/2035 train_time:91595ms step_avg:54.95ms
step:1668/2035 train_time:91681ms step_avg:54.96ms
step:1669/2035 train_time:91770ms step_avg:54.98ms
step:1670/2035 train_time:91857ms step_avg:55.00ms
step:1671/2035 train_time:91946ms step_avg:55.02ms
step:1672/2035 train_time:92033ms step_avg:55.04ms
step:1673/2035 train_time:92122ms step_avg:55.06ms
step:1674/2035 train_time:92209ms step_avg:55.08ms
step:1675/2035 train_time:92298ms step_avg:55.10ms
step:1676/2035 train_time:92384ms step_avg:55.12ms
step:1677/2035 train_time:92472ms step_avg:55.14ms
step:1678/2035 train_time:92559ms step_avg:55.16ms
step:1679/2035 train_time:92648ms step_avg:55.18ms
step:1680/2035 train_time:92736ms step_avg:55.20ms
step:1681/2035 train_time:92824ms step_avg:55.22ms
step:1682/2035 train_time:92911ms step_avg:55.24ms
step:1683/2035 train_time:93000ms step_avg:55.26ms
step:1684/2035 train_time:93087ms step_avg:55.28ms
step:1685/2035 train_time:93175ms step_avg:55.30ms
step:1686/2035 train_time:93262ms step_avg:55.32ms
step:1687/2035 train_time:93350ms step_avg:55.33ms
step:1688/2035 train_time:93437ms step_avg:55.35ms
step:1689/2035 train_time:93525ms step_avg:55.37ms
step:1690/2035 train_time:93612ms step_avg:55.39ms
step:1691/2035 train_time:93700ms step_avg:55.41ms
step:1692/2035 train_time:93787ms step_avg:55.43ms
step:1693/2035 train_time:93876ms step_avg:55.45ms
step:1694/2035 train_time:93963ms step_avg:55.47ms
step:1695/2035 train_time:94051ms step_avg:55.49ms
step:1696/2035 train_time:94138ms step_avg:55.51ms
step:1697/2035 train_time:94227ms step_avg:55.53ms
step:1698/2035 train_time:94313ms step_avg:55.54ms
step:1699/2035 train_time:94401ms step_avg:55.56ms
step:1700/2035 train_time:94489ms step_avg:55.58ms
step:1701/2035 train_time:94578ms step_avg:55.60ms
step:1702/2035 train_time:94664ms step_avg:55.62ms
step:1703/2035 train_time:94753ms step_avg:55.64ms
step:1704/2035 train_time:94840ms step_avg:55.66ms
step:1705/2035 train_time:94928ms step_avg:55.68ms
step:1706/2035 train_time:95015ms step_avg:55.69ms
step:1707/2035 train_time:95103ms step_avg:55.71ms
step:1708/2035 train_time:95190ms step_avg:55.73ms
step:1709/2035 train_time:95278ms step_avg:55.75ms
step:1710/2035 train_time:95365ms step_avg:55.77ms
step:1711/2035 train_time:95453ms step_avg:55.79ms
step:1712/2035 train_time:95542ms step_avg:55.81ms
step:1713/2035 train_time:95630ms step_avg:55.83ms
step:1714/2035 train_time:95717ms step_avg:55.84ms
step:1715/2035 train_time:95805ms step_avg:55.86ms
step:1716/2035 train_time:95892ms step_avg:55.88ms
step:1717/2035 train_time:95981ms step_avg:55.90ms
step:1718/2035 train_time:96068ms step_avg:55.92ms
step:1719/2035 train_time:96157ms step_avg:55.94ms
step:1720/2035 train_time:96243ms step_avg:55.96ms
step:1721/2035 train_time:96332ms step_avg:55.97ms
step:1722/2035 train_time:96419ms step_avg:55.99ms
step:1723/2035 train_time:96508ms step_avg:56.01ms
step:1724/2035 train_time:96594ms step_avg:56.03ms
step:1725/2035 train_time:96683ms step_avg:56.05ms
step:1726/2035 train_time:96771ms step_avg:56.07ms
step:1727/2035 train_time:96860ms step_avg:56.09ms
step:1728/2035 train_time:96947ms step_avg:56.10ms
step:1729/2035 train_time:97035ms step_avg:56.12ms
step:1730/2035 train_time:97122ms step_avg:56.14ms
step:1731/2035 train_time:97210ms step_avg:56.16ms
step:1732/2035 train_time:97296ms step_avg:56.18ms
step:1733/2035 train_time:97385ms step_avg:56.19ms
step:1734/2035 train_time:97472ms step_avg:56.21ms
step:1735/2035 train_time:97560ms step_avg:56.23ms
step:1736/2035 train_time:97647ms step_avg:56.25ms
step:1737/2035 train_time:97736ms step_avg:56.27ms
step:1738/2035 train_time:97823ms step_avg:56.28ms
step:1739/2035 train_time:97911ms step_avg:56.30ms
step:1740/2035 train_time:97998ms step_avg:56.32ms
step:1741/2035 train_time:98086ms step_avg:56.34ms
step:1742/2035 train_time:98174ms step_avg:56.36ms
step:1743/2035 train_time:98261ms step_avg:56.37ms
step:1744/2035 train_time:98349ms step_avg:56.39ms
step:1745/2035 train_time:98436ms step_avg:56.41ms
step:1746/2035 train_time:98523ms step_avg:56.43ms
step:1747/2035 train_time:98611ms step_avg:56.45ms
step:1748/2035 train_time:98698ms step_avg:56.46ms
step:1749/2035 train_time:98786ms step_avg:56.48ms
step:1750/2035 train_time:98873ms step_avg:56.50ms
step:1750/2035 val_loss:3.3587 train_time:98964ms step_avg:56.55ms
step:1751/2035 train_time:98984ms step_avg:56.53ms
step:1752/2035 train_time:99053ms step_avg:56.54ms
step:1753/2035 train_time:99145ms step_avg:56.56ms
step:1754/2035 train_time:99232ms step_avg:56.57ms
step:1755/2035 train_time:99319ms step_avg:56.59ms
step:1756/2035 train_time:99406ms step_avg:56.61ms
step:1757/2035 train_time:99493ms step_avg:56.63ms
step:1758/2035 train_time:99579ms step_avg:56.64ms
step:1759/2035 train_time:99666ms step_avg:56.66ms
step:1760/2035 train_time:99753ms step_avg:56.68ms
step:1761/2035 train_time:99840ms step_avg:56.70ms
step:1762/2035 train_time:99932ms step_avg:56.72ms
step:1763/2035 train_time:100020ms step_avg:56.73ms
step:1764/2035 train_time:100109ms step_avg:56.75ms
step:1765/2035 train_time:100198ms step_avg:56.77ms
step:1766/2035 train_time:100286ms step_avg:56.79ms
step:1767/2035 train_time:100373ms step_avg:56.80ms
step:1768/2035 train_time:100460ms step_avg:56.82ms
step:1769/2035 train_time:100547ms step_avg:56.84ms
step:1770/2035 train_time:100634ms step_avg:56.86ms
step:1771/2035 train_time:100721ms step_avg:56.87ms
step:1772/2035 train_time:100807ms step_avg:56.89ms
step:1773/2035 train_time:100897ms step_avg:56.91ms
step:1774/2035 train_time:100984ms step_avg:56.92ms
step:1775/2035 train_time:101074ms step_avg:56.94ms
step:1776/2035 train_time:101162ms step_avg:56.96ms
step:1777/2035 train_time:101250ms step_avg:56.98ms
step:1778/2035 train_time:101337ms step_avg:56.99ms
step:1779/2035 train_time:101424ms step_avg:57.01ms
step:1780/2035 train_time:101511ms step_avg:57.03ms
step:1781/2035 train_time:101598ms step_avg:57.05ms
step:1782/2035 train_time:101685ms step_avg:57.06ms
step:1783/2035 train_time:101773ms step_avg:57.08ms
step:1784/2035 train_time:101860ms step_avg:57.10ms
step:1785/2035 train_time:101947ms step_avg:57.11ms
step:1786/2035 train_time:102036ms step_avg:57.13ms
step:1787/2035 train_time:102125ms step_avg:57.15ms
step:1788/2035 train_time:102213ms step_avg:57.17ms
step:1789/2035 train_time:102301ms step_avg:57.18ms
step:1790/2035 train_time:102388ms step_avg:57.20ms
step:1791/2035 train_time:102476ms step_avg:57.22ms
step:1792/2035 train_time:102564ms step_avg:57.23ms
step:1793/2035 train_time:102651ms step_avg:57.25ms
step:1794/2035 train_time:102737ms step_avg:57.27ms
step:1795/2035 train_time:102825ms step_avg:57.28ms
step:1796/2035 train_time:102912ms step_avg:57.30ms
step:1797/2035 train_time:103002ms step_avg:57.32ms
step:1798/2035 train_time:103089ms step_avg:57.34ms
step:1799/2035 train_time:103178ms step_avg:57.35ms
step:1800/2035 train_time:103265ms step_avg:57.37ms
step:1801/2035 train_time:103354ms step_avg:57.39ms
step:1802/2035 train_time:103441ms step_avg:57.40ms
step:1803/2035 train_time:103529ms step_avg:57.42ms
step:1804/2035 train_time:103615ms step_avg:57.44ms
step:1805/2035 train_time:103704ms step_avg:57.45ms
step:1806/2035 train_time:103790ms step_avg:57.47ms
step:1807/2035 train_time:103878ms step_avg:57.49ms
step:1808/2035 train_time:103964ms step_avg:57.50ms
step:1809/2035 train_time:104052ms step_avg:57.52ms
step:1810/2035 train_time:104140ms step_avg:57.54ms
step:1811/2035 train_time:104228ms step_avg:57.55ms
step:1812/2035 train_time:104315ms step_avg:57.57ms
step:1813/2035 train_time:104403ms step_avg:57.59ms
step:1814/2035 train_time:104491ms step_avg:57.60ms
step:1815/2035 train_time:104579ms step_avg:57.62ms
step:1816/2035 train_time:104666ms step_avg:57.64ms
step:1817/2035 train_time:104754ms step_avg:57.65ms
step:1818/2035 train_time:104841ms step_avg:57.67ms
step:1819/2035 train_time:104929ms step_avg:57.68ms
step:1820/2035 train_time:105016ms step_avg:57.70ms
step:1821/2035 train_time:105105ms step_avg:57.72ms
step:1822/2035 train_time:105192ms step_avg:57.73ms
step:1823/2035 train_time:105281ms step_avg:57.75ms
step:1824/2035 train_time:105367ms step_avg:57.77ms
step:1825/2035 train_time:105455ms step_avg:57.78ms
step:1826/2035 train_time:105543ms step_avg:57.80ms
step:1827/2035 train_time:105631ms step_avg:57.82ms
step:1828/2035 train_time:105718ms step_avg:57.83ms
step:1829/2035 train_time:105806ms step_avg:57.85ms
step:1830/2035 train_time:105892ms step_avg:57.86ms
step:1831/2035 train_time:105982ms step_avg:57.88ms
step:1832/2035 train_time:106069ms step_avg:57.90ms
step:1833/2035 train_time:106158ms step_avg:57.91ms
step:1834/2035 train_time:106245ms step_avg:57.93ms
step:1835/2035 train_time:106332ms step_avg:57.95ms
step:1836/2035 train_time:106420ms step_avg:57.96ms
step:1837/2035 train_time:106507ms step_avg:57.98ms
step:1838/2035 train_time:106594ms step_avg:57.99ms
step:1839/2035 train_time:106682ms step_avg:58.01ms
step:1840/2035 train_time:106769ms step_avg:58.03ms
step:1841/2035 train_time:106856ms step_avg:58.04ms
step:1842/2035 train_time:106944ms step_avg:58.06ms
step:1843/2035 train_time:107033ms step_avg:58.08ms
step:1844/2035 train_time:107120ms step_avg:58.09ms
step:1845/2035 train_time:107208ms step_avg:58.11ms
step:1846/2035 train_time:107295ms step_avg:58.12ms
step:1847/2035 train_time:107384ms step_avg:58.14ms
step:1848/2035 train_time:107472ms step_avg:58.16ms
step:1849/2035 train_time:107560ms step_avg:58.17ms
step:1850/2035 train_time:107648ms step_avg:58.19ms
step:1851/2035 train_time:107735ms step_avg:58.20ms
step:1852/2035 train_time:107822ms step_avg:58.22ms
step:1853/2035 train_time:107909ms step_avg:58.23ms
step:1854/2035 train_time:107997ms step_avg:58.25ms
step:1855/2035 train_time:108085ms step_avg:58.27ms
step:1856/2035 train_time:108172ms step_avg:58.28ms
step:1857/2035 train_time:108261ms step_avg:58.30ms
step:1858/2035 train_time:108347ms step_avg:58.31ms
step:1859/2035 train_time:108436ms step_avg:58.33ms
step:1860/2035 train_time:108522ms step_avg:58.35ms
step:1861/2035 train_time:108610ms step_avg:58.36ms
step:1862/2035 train_time:108697ms step_avg:58.38ms
step:1863/2035 train_time:108786ms step_avg:58.39ms
step:1864/2035 train_time:108873ms step_avg:58.41ms
step:1865/2035 train_time:108961ms step_avg:58.42ms
step:1866/2035 train_time:109048ms step_avg:58.44ms
step:1867/2035 train_time:109136ms step_avg:58.46ms
step:1868/2035 train_time:109223ms step_avg:58.47ms
step:1869/2035 train_time:109311ms step_avg:58.49ms
step:1870/2035 train_time:109399ms step_avg:58.50ms
step:1871/2035 train_time:109487ms step_avg:58.52ms
step:1872/2035 train_time:109574ms step_avg:58.53ms
step:1873/2035 train_time:109662ms step_avg:58.55ms
step:1874/2035 train_time:109749ms step_avg:58.56ms
step:1875/2035 train_time:109837ms step_avg:58.58ms
step:1876/2035 train_time:109924ms step_avg:58.59ms
step:1877/2035 train_time:110012ms step_avg:58.61ms
step:1878/2035 train_time:110099ms step_avg:58.63ms
step:1879/2035 train_time:110187ms step_avg:58.64ms
step:1880/2035 train_time:110274ms step_avg:58.66ms
step:1881/2035 train_time:110362ms step_avg:58.67ms
step:1882/2035 train_time:110449ms step_avg:58.69ms
step:1883/2035 train_time:110537ms step_avg:58.70ms
step:1884/2035 train_time:110624ms step_avg:58.72ms
step:1885/2035 train_time:110713ms step_avg:58.73ms
step:1886/2035 train_time:110800ms step_avg:58.75ms
step:1887/2035 train_time:110888ms step_avg:58.76ms
step:1888/2035 train_time:110977ms step_avg:58.78ms
step:1889/2035 train_time:111064ms step_avg:58.80ms
step:1890/2035 train_time:111152ms step_avg:58.81ms
step:1891/2035 train_time:111240ms step_avg:58.83ms
step:1892/2035 train_time:111327ms step_avg:58.84ms
step:1893/2035 train_time:111415ms step_avg:58.86ms
step:1894/2035 train_time:111501ms step_avg:58.87ms
step:1895/2035 train_time:111589ms step_avg:58.89ms
step:1896/2035 train_time:111677ms step_avg:58.90ms
step:1897/2035 train_time:111765ms step_avg:58.92ms
step:1898/2035 train_time:111852ms step_avg:58.93ms
step:1899/2035 train_time:111941ms step_avg:58.95ms
step:1900/2035 train_time:112028ms step_avg:58.96ms
step:1901/2035 train_time:112117ms step_avg:58.98ms
step:1902/2035 train_time:112204ms step_avg:58.99ms
step:1903/2035 train_time:112292ms step_avg:59.01ms
step:1904/2035 train_time:112379ms step_avg:59.02ms
step:1905/2035 train_time:112467ms step_avg:59.04ms
step:1906/2035 train_time:112554ms step_avg:59.05ms
step:1907/2035 train_time:112642ms step_avg:59.07ms
step:1908/2035 train_time:112729ms step_avg:59.08ms
step:1909/2035 train_time:112818ms step_avg:59.10ms
step:1910/2035 train_time:112905ms step_avg:59.11ms
step:1911/2035 train_time:112993ms step_avg:59.13ms
step:1912/2035 train_time:113080ms step_avg:59.14ms
step:1913/2035 train_time:113168ms step_avg:59.16ms
step:1914/2035 train_time:113256ms step_avg:59.17ms
step:1915/2035 train_time:113345ms step_avg:59.19ms
step:1916/2035 train_time:113432ms step_avg:59.20ms
step:1917/2035 train_time:113520ms step_avg:59.22ms
step:1918/2035 train_time:113607ms step_avg:59.23ms
step:1919/2035 train_time:113694ms step_avg:59.25ms
step:1920/2035 train_time:113782ms step_avg:59.26ms
step:1921/2035 train_time:113870ms step_avg:59.28ms
step:1922/2035 train_time:113958ms step_avg:59.29ms
step:1923/2035 train_time:114047ms step_avg:59.31ms
step:1924/2035 train_time:114134ms step_avg:59.32ms
step:1925/2035 train_time:114222ms step_avg:59.34ms
step:1926/2035 train_time:114309ms step_avg:59.35ms
step:1927/2035 train_time:114397ms step_avg:59.37ms
step:1928/2035 train_time:114484ms step_avg:59.38ms
step:1929/2035 train_time:114573ms step_avg:59.39ms
step:1930/2035 train_time:114660ms step_avg:59.41ms
step:1931/2035 train_time:114748ms step_avg:59.42ms
step:1932/2035 train_time:114834ms step_avg:59.44ms
step:1933/2035 train_time:114923ms step_avg:59.45ms
step:1934/2035 train_time:115010ms step_avg:59.47ms
step:1935/2035 train_time:115100ms step_avg:59.48ms
step:1936/2035 train_time:115187ms step_avg:59.50ms
step:1937/2035 train_time:115275ms step_avg:59.51ms
step:1938/2035 train_time:115362ms step_avg:59.53ms
step:1939/2035 train_time:115449ms step_avg:59.54ms
step:1940/2035 train_time:115536ms step_avg:59.55ms
step:1941/2035 train_time:115625ms step_avg:59.57ms
step:1942/2035 train_time:115713ms step_avg:59.58ms
step:1943/2035 train_time:115801ms step_avg:59.60ms
step:1944/2035 train_time:115887ms step_avg:59.61ms
step:1945/2035 train_time:115976ms step_avg:59.63ms
step:1946/2035 train_time:116063ms step_avg:59.64ms
step:1947/2035 train_time:116151ms step_avg:59.66ms
step:1948/2035 train_time:116238ms step_avg:59.67ms
step:1949/2035 train_time:116326ms step_avg:59.69ms
step:1950/2035 train_time:116413ms step_avg:59.70ms
step:1951/2035 train_time:116502ms step_avg:59.71ms
step:1952/2035 train_time:116589ms step_avg:59.73ms
step:1953/2035 train_time:116677ms step_avg:59.74ms
step:1954/2035 train_time:116763ms step_avg:59.76ms
step:1955/2035 train_time:116851ms step_avg:59.77ms
step:1956/2035 train_time:116938ms step_avg:59.78ms
step:1957/2035 train_time:117026ms step_avg:59.80ms
step:1958/2035 train_time:117113ms step_avg:59.81ms
step:1959/2035 train_time:117202ms step_avg:59.83ms
step:1960/2035 train_time:117288ms step_avg:59.84ms
step:1961/2035 train_time:117377ms step_avg:59.86ms
step:1962/2035 train_time:117465ms step_avg:59.87ms
step:1963/2035 train_time:117553ms step_avg:59.88ms
step:1964/2035 train_time:117639ms step_avg:59.90ms
step:1965/2035 train_time:117727ms step_avg:59.91ms
step:1966/2035 train_time:117813ms step_avg:59.93ms
step:1967/2035 train_time:117901ms step_avg:59.94ms
step:1968/2035 train_time:117988ms step_avg:59.95ms
step:1969/2035 train_time:118077ms step_avg:59.97ms
step:1970/2035 train_time:118164ms step_avg:59.98ms
step:1971/2035 train_time:118253ms step_avg:60.00ms
step:1972/2035 train_time:118340ms step_avg:60.01ms
step:1973/2035 train_time:118428ms step_avg:60.02ms
step:1974/2035 train_time:118515ms step_avg:60.04ms
step:1975/2035 train_time:118603ms step_avg:60.05ms
step:1976/2035 train_time:118690ms step_avg:60.07ms
step:1977/2035 train_time:118778ms step_avg:60.08ms
step:1978/2035 train_time:118866ms step_avg:60.09ms
step:1979/2035 train_time:118954ms step_avg:60.11ms
step:1980/2035 train_time:119041ms step_avg:60.12ms
step:1981/2035 train_time:119128ms step_avg:60.14ms
step:1982/2035 train_time:119215ms step_avg:60.15ms
step:1983/2035 train_time:119305ms step_avg:60.16ms
step:1984/2035 train_time:119392ms step_avg:60.18ms
step:1985/2035 train_time:119480ms step_avg:60.19ms
step:1986/2035 train_time:119567ms step_avg:60.21ms
step:1987/2035 train_time:119655ms step_avg:60.22ms
step:1988/2035 train_time:119742ms step_avg:60.23ms
step:1989/2035 train_time:119831ms step_avg:60.25ms
step:1990/2035 train_time:119918ms step_avg:60.26ms
step:1991/2035 train_time:120007ms step_avg:60.27ms
step:1992/2035 train_time:120093ms step_avg:60.29ms
step:1993/2035 train_time:120181ms step_avg:60.30ms
step:1994/2035 train_time:120269ms step_avg:60.32ms
step:1995/2035 train_time:120357ms step_avg:60.33ms
step:1996/2035 train_time:120444ms step_avg:60.34ms
step:1997/2035 train_time:120532ms step_avg:60.36ms
step:1998/2035 train_time:120619ms step_avg:60.37ms
step:1999/2035 train_time:120707ms step_avg:60.38ms
step:2000/2035 train_time:120794ms step_avg:60.40ms
step:2000/2035 val_loss:3.2859 train_time:120884ms step_avg:60.44ms
step:2001/2035 train_time:120904ms step_avg:60.42ms
step:2002/2035 train_time:120973ms step_avg:60.43ms
step:2003/2035 train_time:121064ms step_avg:60.44ms
step:2004/2035 train_time:121152ms step_avg:60.45ms
step:2005/2035 train_time:121239ms step_avg:60.47ms
step:2006/2035 train_time:121325ms step_avg:60.48ms
step:2007/2035 train_time:121413ms step_avg:60.49ms
step:2008/2035 train_time:121500ms step_avg:60.51ms
step:2009/2035 train_time:121588ms step_avg:60.52ms
step:2010/2035 train_time:121675ms step_avg:60.53ms
step:2011/2035 train_time:121764ms step_avg:60.55ms
step:2012/2035 train_time:121852ms step_avg:60.56ms
step:2013/2035 train_time:121942ms step_avg:60.58ms
step:2014/2035 train_time:122032ms step_avg:60.59ms
step:2015/2035 train_time:122121ms step_avg:60.61ms
step:2016/2035 train_time:122208ms step_avg:60.62ms
step:2017/2035 train_time:122297ms step_avg:60.63ms
step:2018/2035 train_time:122383ms step_avg:60.65ms
step:2019/2035 train_time:122471ms step_avg:60.66ms
step:2020/2035 train_time:122557ms step_avg:60.67ms
step:2021/2035 train_time:122645ms step_avg:60.69ms
step:2022/2035 train_time:122732ms step_avg:60.70ms
step:2023/2035 train_time:122821ms step_avg:60.71ms
step:2024/2035 train_time:122910ms step_avg:60.73ms
step:2025/2035 train_time:123000ms step_avg:60.74ms
step:2026/2035 train_time:123088ms step_avg:60.75ms
step:2027/2035 train_time:123177ms step_avg:60.77ms
step:2028/2035 train_time:123265ms step_avg:60.78ms
step:2029/2035 train_time:123353ms step_avg:60.80ms
step:2030/2035 train_time:123440ms step_avg:60.81ms
step:2031/2035 train_time:123528ms step_avg:60.82ms
step:2032/2035 train_time:123615ms step_avg:60.83ms
step:2033/2035 train_time:123703ms step_avg:60.85ms
step:2034/2035 train_time:123791ms step_avg:60.86ms
step:2035/2035 train_time:123879ms step_avg:60.87ms
step:2035/2035 val_loss:3.2790 train_time:123969ms step_avg:60.92ms
peak memory allocated: 29634 MiB reserved: 44496 MiB

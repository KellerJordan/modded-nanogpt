import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from collections import defaultdict
from itertools import accumulate
from pathlib import Path
import gc

os.environ["PYTORCH_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64


# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng

@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor, split_baddbmm: bool = False):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    # Select batched vs unbatched
    if split_baddbmm:
        BX_matmul = torch.bmm if X.ndim > 2 else torch.mm
    else:
        aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A

        # Referencing X twice causes pytorch to make a defensive copy,
        # resulting in a cudaMemcpyAsync in baddbmm.
        # For large matrices (i.e., the mlp weights), it's faster to split
        # the operation into two kernels to avoid this.
        if split_baddbmm:
            BX_matmul(B, X, out=C)  # C = B @ X
            C.add_(X, alpha=a)      # C = C + a*X  (in-place, X only read)
        else:
            aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X

        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X


# -----------------------------------------------------------------------------
# Compiled helpers for NorMuon by @chrisjmccormick

@torch.compile(dynamic=False, fullgraph=True)
def cautious_wd_and_update_inplace(p, v, wd_tensor, lr_tensor):
    """Cautious weight decay + parameter update. wd_tensor and lr_tensor are 0-D CPU tensors."""
    mask = (v * p) >= 0
    wd_factor = wd_tensor.to(p.dtype)
    lr_factor = lr_tensor.to(p.dtype)
    p.copy_(p - (p * mask * wd_factor * lr_factor) - (v * lr_factor))


@torch.compile(dynamic=False, fullgraph=True)
def apply_normuon_variance_reduction(v_chunk, second_momentum_buffer, beta2, red_dim):
    """NorMuon variance reduction. Algebraically fuses the normalization steps to minimize memory ops."""
    v_mean = v_chunk.float().square().mean(dim=red_dim, keepdim=True)
    red_dim_size = v_chunk.size(red_dim)
    v_norm_sq = v_mean.sum(dim=(-2, -1), keepdim=True).mul_(red_dim_size)
    v_norm = v_norm_sq.sqrt_()
    second_momentum_buffer.lerp_(v_mean.to(dtype=second_momentum_buffer.dtype), 1 - beta2)
    step_size = second_momentum_buffer.clamp_min(1e-10).rsqrt_()
    scaled_sq_sum = (v_mean * red_dim_size) * step_size.float().square()
    v_norm_new = scaled_sq_sum.sum(dim=(-2, -1), keepdim=True).sqrt_()
    final_scale = step_size * (v_norm / v_norm_new.clamp_min_(1e-10))
    return v_chunk.mul_(final_scale.type_as(v_chunk))


# -----------------------------------------------------------------------------
# NorMuon optimizer

class NorMuon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).

    Differences from standard Muon:
    - Newton-Shulz is replaced with Polar Express for the orthogonalization step
    - NorMuon adds a low-rank variance estimator similar to Adafactor. https://arxiv.org/pdf/2510.05491
    - small 1D parameters handled here instead of in Adam
    - Cautious weight decay, a gated version of decoupled weight decay
    - Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as
    needed on the forward pass. This enables attn and mlp weights to be contained within the same
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn.
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, beta2=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, beta2=beta2)
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def reset(self):
        # expose a reset for clearing buffers
        for group in self.param_groups:
            group["momentum_buffer"].zero_()
            group["second_momentum_buffer"].zero_()

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per module.
        """
        groups = defaultdict(list)
        for param in params:
            groups[param.label].append(param)

        param_groups = []
        for module_name, group_params in groups.items():
            chunk_size = (len(group_params) + self.world_size - 1) // self.world_size
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))

        return param_groups

    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn
        and mlp params when a param group is split across GPUs.
        """
        module_group_order = ['smear_gate', 'attn_gate', 'attn', 'mlp']
        params_list = list(params)
        params_list.sort(key=lambda x: module_group_order.index(x.label))

        idx = 0
        group_sizes = [1, 10, 16, 16]
        assert len(params_list) == sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            chunk_size = (size + self.world_size - 1) // self.world_size
            group_params = params_list[idx: idx + size]
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))
            idx += size

        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient distributed step by @YouJiacheng, @KonstantinWilleke, @alexrgilbert,
        # @adricarda, @tuttyfrutyee, @vdlad, @ryanyang0, @vagrawal, @varunneal, @chrisjmccormick
        rank = dist.get_rank()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            stacked_grads = torch.empty(
                (padded_num_params, *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device
            )
            for i, p in enumerate(params):
                stacked_grads[i].copy_(p.grad, non_blocking=True)
            if len(params) < padded_num_params:
                stacked_grads[len(params):].zero_()

            grad_chunk = torch.empty_like(stacked_grads[:chunk_size])

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(dict(grad_chunk=grad_chunk, reduce_future=reduce_future))

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = group["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            start_idx = rank * chunk_size
            module_idx = start_idx if start_idx < len(params) else 0

            num_params = min(chunk_size, max(0, len(params) - start_idx))  # num params for this rank

            if "momentum_buffer" not in group:
                group["momentum_buffer"]  = torch.zeros_like(grad_chunk[:num_params])
            momentum_buffer = group["momentum_buffer"]
            # Apply momentum update to the persistent momentum buffer in-place
            momentum_buffer.lerp_(grad_chunk[:num_params], 1 - group["momentum"])
            updated_grads = grad_chunk[:num_params].lerp_(momentum_buffer, group["momentum"])

            grad_shape = updated_grads.shape
            if params[module_idx].label == 'attn':

                for p in params[module_idx:module_idx + num_params]:
                    assert p.label == 'attn'

                updated_grads = updated_grads.view(4 * grad_shape[0], grad_shape[1] // 4, grad_shape[2])

            ref_param = params[module_idx]
            param_shape = ref_param.shape

            # The below shape-based heuristic assumes that matrices have their input along the
            # row dimension and their output along the columns. Gates are an exception.
            is_gate = ref_param.label in ['smear_gate', 'attn_gate']

            if "second_momentum_buffer" not in group:
                if is_gate:
                    group["second_momentum_buffer"] = torch.zeros_like(updated_grads[..., :, :1])
                else:
                    group["second_momentum_buffer"] = (torch.zeros_like(updated_grads[..., :, :1])
                        if param_shape[-2] >= param_shape[-1] else torch.zeros_like(updated_grads[..., :1, :])
                    )
            second_momentum_buffer = group["second_momentum_buffer"]

            if "param_lr_cpu" not in group:
                # Define multipliers for ALL params in this group (global, not per-shard)
                lr_mults = []
                wd_mults = []
                for p in params:
                    # Increase learning rate for modules with larger inputs than outputs.
                    # This shape check also assumes rows=input, columns=output, so take care
                    # when changing memory layouts. @chrisjmccormick
                    shape = p.shape
                    if len(shape) >= 2:
                        shape_mult = max(1.0, shape[-2] / shape[-1]) ** 0.5
                    else:
                        shape_mult = 1.0
                    lr_mults.append(shape_mult * getattr(p, "lr_mul", 1.0))
                    wd_mults.append(getattr(p, "wd_mul", 1.0))
                # Define as cpu tensors to enable Inductor constant folding
                group["param_lr_cpu"] = torch.tensor(lr_mults, dtype=torch.float32, device="cpu")
                group["param_wd_cpu"] = torch.tensor(wd_mults, dtype=torch.float32, device="cpu")

            eff_lr_all = group["param_lr_cpu"] * group["lr"]
            eff_wd_all = group["param_wd_cpu"] * group["weight_decay"] * group["lr"]

            # Slice the portion corresponding to this rank's shard
            eff_lr_cpu = eff_lr_all[module_idx:module_idx + num_params]
            eff_wd_cpu = eff_wd_all[module_idx:module_idx + num_params]

            # Compute zeropower for the entire chunk in a single, batched call.
            if num_params == 0:
                v_chunk = updated_grads
            else:
                v_chunk = polar_express(updated_grads, split_baddbmm=(ref_param.label == 'mlp'))

            # Note that the head orientation in O is transposed relative to QKV, so red_dim
            # is 'incorrect' for O. However, correcting this showed no improvement. @chrisjmccormick
            red_dim = -1 if (is_gate or param_shape[-2] >= param_shape[-1]) else -2

            v_chunk = apply_normuon_variance_reduction(
                v_chunk, second_momentum_buffer, group["beta2"], red_dim
            )

            v_chunk = v_chunk.view(grad_shape)

            # # "Cautious" weight decay (https://arxiv.org/abs/2510.12402)
            updated_params = torch.empty_like(grad_chunk)
            if num_params > 0:
                # Work on a stacked copy to avoid touching original params
                param_chunk = torch.stack(params[module_idx:module_idx + num_params])

                for local_idx in range(num_params):
                    cautious_wd_and_update_inplace(
                        param_chunk[local_idx],
                        v_chunk[local_idx],
                        eff_wd_cpu[local_idx],
                        eff_lr_cpu[local_idx],
                    )
            else:
                param_chunk = torch.zeros_like(v_chunk)

            updated_params[:num_params].copy_(param_chunk)
            if num_params < chunk_size:
                updated_params[num_params:].zero_()

            stacked_params = torch.empty(
                (padded_num_params, *param_shape),
                dtype=updated_params.dtype,
                device=updated_params.device,
            )

            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_params, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        # Group by label, with explicit ordering for execution control.
        label_order = ['lm_head', 'value_embed', 'scalars']
        params_by_label = defaultdict(list)
        for p in params:
            params_by_label[getattr(p, 'label', None)].append(p)
        param_groups = []
        for label in label_order:
            if label in params_by_label:
                param_groups.append(dict(params=params_by_label[label]))
        # include any unlabeled params at the end (processed last)
        if None in params_by_label:
            param_groups.append(dict(params=params_by_label[None]))
        super().__init__(param_groups, defaults)
        # init state
        for p in params:
            chunk_size = p.size(0) // self.world_size
            exp_avg = torch.zeros_like(p[:chunk_size], dtype=torch.bfloat16, device=p[0].device)
            exp_avg_sq = torch.zeros_like(exp_avg)
            self.state[p] = dict(step=0, exp_avg=exp_avg, exp_avg_sq=exp_avg_sq)
        # DistributedAdam implementation by @vagrawal, @akash5474

        self.should_sync = False

        self._reduce_scatter_hooks = []
        self._reduce_scatter_futures = {}
        self.register_backward_hooks()

    def register_backward_hooks(self):
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                hook = param.register_post_accumulate_grad_hook(self._sync_gradient)
                self._reduce_scatter_hooks.append(hook)

    @torch.no_grad()
    def _sync_gradient(self, param):
        if not self.should_sync:
            return

        grad = param.grad
        rank_size = grad.shape[0] // self.world_size
        grad_slice = torch.empty_like(grad[:rank_size])
        self._reduce_scatter_futures[param] = (
            dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future(),
            grad_slice
        )

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        all_gather_futures: list[torch.Future] = []

        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            for param in group['params']:
                if param not in self._reduce_scatter_futures:
                    continue

                fut, g_slice = self._reduce_scatter_futures[param]
                fut.wait()

                rank_size = param.shape[0] // self.world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]

                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (bias2 ** 0.5 / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                # cautious weight decay
                mask = (update * p_slice) >= 0
                # lr as weight decay schedule
                eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                update.addcmul_(p_slice, mask, value=eff_weight_decay * lr)

                p_slice.add_(other=update, alpha=-1.0)

                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())

        self._reduce_scatter_futures.clear()
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        with torch.no_grad():
            self.weight.zero_()  # @Grad62304977 and others

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))


# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float
    key_shift: bool

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKVO weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # Simplified layout by @chrisjmccormick
        self.qkvo_w = nn.Parameter(torch.empty(self.dim * 4, self.hdim))
        # label all modules for explicit optimizer grouping
        self.qkvo_w.label = 'attn'

        with torch.no_grad():
            self.qkvo_w[:self.dim * 3].uniform_(-bound, bound)  # init QKV weights
            self.qkvo_w[self.dim * 3:].zero_()  # init O weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        self.attn_gate.weight.label = 'attn_gate'

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas, key_shift = attn_args.ve, attn_args.sa_lambdas, attn_args.key_shift
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, sa_lambdas[0] * self.qkvo_w[:self.dim * 3].type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if key_shift:
            # shift keys forward for the stationary head dims. Enables 1-layer induction.
            k[:, 1:, :, self.head_dim//4:self.head_dim//2] = k[:, :-1, :, self.head_dim//4:self.head_dim//2]
            k[:, 1:, :, self.head_dim//4+self.head_dim//2:] = k[:, :-1, :, self.head_dim//4+self.head_dim//2:]
        if ve is not None:
            v = v + ve.view_as(v) # @ KoszarskyB & @Grad62304977

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens,
                                                        max_seqlen_q=max_len, max_seqlen_k=max_len,
                                                        causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, sa_lambdas[1] * self.qkvo_w[self.dim * 3:].type_as(y))  # sa_lambdas[1] pre-multiplied to O @shenberg
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # Transposed layout to match attention weights
        self.c_fc = nn.Parameter(torch.empty(hdim, dim))
        self.c_proj = nn.Parameter(torch.empty(hdim, dim))
        # label all modules for explicit optimizer grouping
        self.c_fc.label = 'mlp'
        self.c_proj.label = 'mlp'
        self.c_proj.lr_mul = 2.

        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.T.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.6 (the 7th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx != 6 else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim)

    def forward(self, x: Tensor, attn_args: AttnArgs):
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        self.num_layers = num_layers
        vocab_size = next_multiple_of_n(vocab_size, n=128)

        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.label = 'smear_gate'

        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        for embed in self.value_embeds:
            nn.init.zeros_(embed.weight)
        for ve in self.value_embeds:
            ve.weight.label = 'value_embed'
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)

        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=100/448, w_s=1.5/448, grad_s=0.75/448)
        nn.init.normal_(self.lm_head.weight, mean=0, std=0.005)
        self.lm_head.weight.label = 'lm_head'
        # Add learnable skip connection weights for decoder layers
        pad = (-num_layers * 4 - 3) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    1.1 * torch.ones(num_layers),  # resid lambdas. 1.1 init such that layer i weight is i^(num_layers-i).
                    0 * torch.ones(num_layers),  # x0 lambdas
                    *[torch.tensor([0.5, 1.0]) for _ in range(num_layers)],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    -1.5 * torch.ones(1),  # skip_lambda -> σ(-1.5) ≈ 0.18
                    torch.ones(pad),
                ]
            )
        )

        self.scalars.label = 'scalars'
        # set learning rates
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.scalars.lr_mul = 5.0
        self.scalars.wd_mul = 0.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        # set configs
        skip_connections = []
        skip_in = [3] # long attention window on layer 3
        skip_out = [6] # no attn op on layer 6
        x_backout = None
        backout_layer = 7

        # set lambdas
        resid_lambdas = self.scalars[: 1 * self.num_layers]
        x0_lambdas = self.scalars[1 * self.num_layers: 2 * self.num_layers]
        sa_lambdas = self.scalars[2 * self.num_layers: 4 * self.num_layers].view(-1, 2)
        smear_lambda = self.scalars[4 * self.num_layers]
        backout_lambda = self.scalars[4 * self.num_layers+1]
        skip_lambda = self.scalars[4 * self.num_layers+2]

        # set block masks and key shift
        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == self.num_layers
        key_shift = [b==long_bm for b in bm_sizes] # apply key shift to long windows

        # weight-tied: use lm_head.weight for embedding lookup
        x = F.embedding(input_seq, self.lm_head.weight)
        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [ve[1], ve[2]] + [None] * (self.num_layers - 5) + [ve[0], ve[1], ve[2]]
        assert len(ve) == self.num_layers

        # smear token embed forward 1 position @classiclarryd
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        for i in range(self.num_layers):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale,
                key_shift = key_shift[i]
            )
            if i in skip_out:
                gate = torch.sigmoid(skip_lambda)  # in (0, 1)
                x = x + gate * skip_connections.pop()
            if i == 0:
                x = (resid_lambdas[0] + x0_lambdas[0]) * x
            else:
                x = resid_lambdas[i] * x + x0_lambdas[i] * x0
            x = self.blocks[i](x, attn_args)
            if i in skip_in:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 7 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()

    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        # Cast to int32 on CPU before transfer to avoid dtype conversion during .to()
        _inputs = _inputs.to(dtype=torch.int32)
        _targets = _targets.to(dtype=torch.int64)
        _cum_lengths = _cum_lengths.to(dtype=torch.int32)

        new_params = yield (
            _inputs.to(device="cuda", non_blocking=True),
            _targets.to(device="cuda", non_blocking=True),
            _cum_lengths.to(device="cuda", non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * new_grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens // new_grad_accum_steps
            max_seq_len = new_max_seq_len


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # batch sizes
    train_bs_schedule: tuple = (8 * 2048 * 8, 16 * 2048 * 8, 24 * 2048 * 8)
    train_bs_extension: int = 24 * 2048 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 1995  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: float = 0.55  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_final: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=11,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=args.val_batch_size // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.weight.data = m.weight.data.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    embed_params + scalar_params + head_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.005,
)
optimizer2 = NorMuon(hidden_matrix_params + gate_params, lr=0.023, momentum=0.95, beta2=0.95, weight_decay=1.2)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: tied to batch size schedule, with cooldown at the end.
def get_lr(step: int):
    if step > args.num_scheduled_iterations:
        return 0.1
    lr_max = 1.0
    x = step / args.num_scheduled_iterations
    if x > 1/3:
       lr_max = 1.51  # (16/8)**0.6
    if x > 2/3:
        lr_max = 1.93  # (24/8)**0.6
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = lr_max * w + (1 - w) * 0.1
        return lr
    return lr_max

def get_ws(step: int):
    # set short window size to half of long window size
    # Higher ws on "extension" steps
    if step >= args.num_scheduled_iterations:
        return args.ws_final // 2, args.ws_final
    x = step / args.num_scheduled_iterations
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]


def get_bs(step: int):
    if step >= args.num_scheduled_iterations:
        return args.train_bs_extension
    x = step / args.num_scheduled_iterations
    bs_idx = int(len(args.train_bs_schedule) * x)
    return args.train_bs_schedule[bs_idx]


def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)
        # disable sync in the next training step for the adam optimizer
        optimizers[0].should_sync = False

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 10
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_bs_schedule[0], args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
ws_schedule = list(args.ws_schedule) + [args.ws_final]
bs_schedule = list(args.train_bs_schedule) + [args.train_bs_extension]
ws_long = ws_schedule[0]
model.train()
model.yarn.reset()
assert len(ws_schedule) == len(bs_schedule), "This warmup assumes len(ws_schedule) == len(bs_schedule)"

for idx in range(len(ws_schedule)):
    send_args = None
    if idx != 0:
        new_ws_long = ws_schedule[idx]
        model.yarn.apply(ws_long, new_ws_long)
        ws_long = new_ws_long
        send_args = (bs_schedule[idx], args.train_max_seq_len, grad_accum_steps)
    for step in range(warmup_steps):
        inputs, targets, cum_seqlens = train_loader.send(send_args)
        if step % 2 == 1:
            optimizers[0].should_sync = True
        model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
        if step % 2 == 0:
            optimizers[1].step()
            optimizers[1].zero_grad(set_to_none=True)
        else:
            for opt in optimizers:
                opt.step()
            model.zero_grad(set_to_none=True)
            optimizers[0].should_sync = False

model.zero_grad(set_to_none=True)
optimizers[0].should_sync = False
model.eval()

# warm up validation too
val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
val_loss = 0
with torch.no_grad():
    for step in range(val_steps):
        inputs, targets, cum_seqlens = next(val_loader)
        ws_idx = step % len(ws_schedule)
        if ws_idx==0:
            model.yarn.reset()
            ws_long = ws_schedule[0]
        else:
            new_ws_long = ws_schedule[ws_idx]
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
        val_loss += model(inputs, targets, cum_seqlens, ws_long // 2, ws_long)

del val_loader, val_loss
model.train()
model.yarn.reset() # rotary buffer is not stored in state_dict
optimizer2.reset() # muon momentum buffers not in state dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

step_batch_size = args.train_bs_schedule[0]
train_loader = distributed_data_generator(args.train_files, step_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)

gc.collect()

training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    new_step_batch_size = get_bs(step)
    send_args = (new_step_batch_size, args.train_max_seq_len, grad_accum_steps) if new_step_batch_size != step_batch_size else None
    step_batch_size = new_step_batch_size
    for idx in range(grad_accum_steps):
        # enable gradient sync for the DistAdam optimizer on the last iteration before we step it
        if idx == grad_accum_steps - 1 and step % 2 == 1:
            optimizers[0].should_sync = True

        inputs, targets, cum_seqlens = train_loader.send(send_args)
        (model(inputs, targets, cum_seqlens, ws_short, ws_long) / grad_accum_steps).backward()
    step_optimizers(step, optimizers, model)

    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.10.12 (main, Nov  4 2025, 08:48:33) [GCC 11.4.0]
Running PyTorch 2.10.0.dev20251204+cu126 compiled for CUDA 12.6
Running Triton version 3.6.0
Sat Dec 20 00:24:44 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:19:00.0 Off |                    0 |
| N/A   29C    P0            120W /  700W |    5858MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:3B:00.0 Off |                    0 |
| N/A   23C    P0            114W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:4C:00.0 Off |                    0 |
| N/A   27C    P0            120W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:5D:00.0 Off |                    0 |
| N/A   28C    P0            121W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:9B:00.0 Off |                    0 |
| N/A   29C    P0            124W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:BB:00.0 Off |                    0 |
| N/A   22C    P0            112W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:CB:00.0 Off |                    0 |
| N/A   29C    P0            121W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:DB:00.0 Off |                    0 |
| N/A   24C    P0            116W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A    305058      C   /home/ubuntu/.venv/bin/python3               1510MiB |
|    0   N/A  N/A    305059      C   /home/ubuntu/.venv/bin/python3                614MiB |
|    0   N/A  N/A    305060      C   /home/ubuntu/.venv/bin/python3                614MiB |
|    0   N/A  N/A    305061      C   /home/ubuntu/.venv/bin/python3                614MiB |
|    0   N/A  N/A    305062      C   /home/ubuntu/.venv/bin/python3                614MiB |
|    0   N/A  N/A    305063      C   /home/ubuntu/.venv/bin/python3                614MiB |
|    0   N/A  N/A    305064      C   /home/ubuntu/.venv/bin/python3                614MiB |
|    0   N/A  N/A    305065      C   /home/ubuntu/.venv/bin/python3                614MiB |
|    1   N/A  N/A    305059      C   /home/ubuntu/.venv/bin/python3               1510MiB |
|    2   N/A  N/A    305060      C   /home/ubuntu/.venv/bin/python3               1510MiB |
|    3   N/A  N/A    305061      C   /home/ubuntu/.venv/bin/python3               1510MiB |
|    4   N/A  N/A    305062      C   /home/ubuntu/.venv/bin/python3               1510MiB |
|    5   N/A  N/A    305063      C   /home/ubuntu/.venv/bin/python3               1510MiB |
|    6   N/A  N/A    305064      C   /home/ubuntu/.venv/bin/python3               1510MiB |
|    7   N/A  N/A    305065      C   /home/ubuntu/.venv/bin/python3               1510MiB |
+-----------------------------------------------------------------------------------------+

====================================================================================================
step:0/2035 val_loss:10.8352 train_time:0ms step_avg:0.03ms
step:1/2035 train_time:69ms step_avg:69.11ms
step:2/2035 train_time:91ms step_avg:45.40ms
step:3/2035 train_time:113ms step_avg:37.81ms
step:4/2035 train_time:146ms step_avg:36.54ms
step:5/2035 train_time:179ms step_avg:35.78ms
step:6/2035 train_time:396ms step_avg:66.04ms
step:7/2035 train_time:417ms step_avg:59.52ms
step:8/2035 train_time:450ms step_avg:56.20ms
step:9/2035 train_time:482ms step_avg:53.60ms
step:10/2035 train_time:515ms step_avg:51.53ms
step:11/2035 train_time:549ms step_avg:49.89ms
step:12/2035 train_time:582ms step_avg:48.49ms
step:13/2035 train_time:615ms step_avg:47.31ms
step:14/2035 train_time:648ms step_avg:46.29ms
step:15/2035 train_time:681ms step_avg:45.42ms
step:16/2035 train_time:714ms step_avg:44.64ms
step:17/2035 train_time:748ms step_avg:43.99ms
step:18/2035 train_time:781ms step_avg:43.37ms
step:19/2035 train_time:814ms step_avg:42.85ms
step:20/2035 train_time:847ms step_avg:42.36ms
step:21/2035 train_time:880ms step_avg:41.91ms
step:22/2035 train_time:913ms step_avg:41.50ms
step:23/2035 train_time:946ms step_avg:41.14ms
step:24/2035 train_time:979ms step_avg:40.80ms
step:25/2035 train_time:1012ms step_avg:40.49ms
step:26/2035 train_time:1045ms step_avg:40.21ms
step:27/2035 train_time:1078ms step_avg:39.94ms
step:28/2035 train_time:1111ms step_avg:39.70ms
step:29/2035 train_time:1145ms step_avg:39.47ms
step:30/2035 train_time:1178ms step_avg:39.25ms
step:31/2035 train_time:1211ms step_avg:39.06ms
step:32/2035 train_time:1244ms step_avg:38.87ms
step:33/2035 train_time:1277ms step_avg:38.70ms
step:34/2035 train_time:1310ms step_avg:38.53ms
step:35/2035 train_time:1345ms step_avg:38.43ms
step:36/2035 train_time:1378ms step_avg:38.27ms
step:37/2035 train_time:1413ms step_avg:38.18ms
step:38/2035 train_time:1446ms step_avg:38.04ms
step:39/2035 train_time:1480ms step_avg:37.95ms
step:40/2035 train_time:1513ms step_avg:37.82ms
step:41/2035 train_time:1547ms step_avg:37.73ms
step:42/2035 train_time:1580ms step_avg:37.62ms
step:43/2035 train_time:1613ms step_avg:37.52ms
step:44/2035 train_time:1646ms step_avg:37.42ms
step:45/2035 train_time:1680ms step_avg:37.34ms
step:46/2035 train_time:1713ms step_avg:37.24ms
step:47/2035 train_time:1746ms step_avg:37.15ms
step:48/2035 train_time:1779ms step_avg:37.07ms
step:49/2035 train_time:1812ms step_avg:36.99ms
step:50/2035 train_time:1846ms step_avg:36.91ms
step:51/2035 train_time:1879ms step_avg:36.83ms
step:52/2035 train_time:1911ms step_avg:36.76ms
step:53/2035 train_time:1945ms step_avg:36.69ms
step:54/2035 train_time:1977ms step_avg:36.62ms
step:55/2035 train_time:2011ms step_avg:36.55ms
step:56/2035 train_time:2043ms step_avg:36.49ms
step:57/2035 train_time:2076ms step_avg:36.43ms
step:58/2035 train_time:2109ms step_avg:36.37ms
step:59/2035 train_time:2143ms step_avg:36.32ms
step:60/2035 train_time:2176ms step_avg:36.26ms
step:61/2035 train_time:2209ms step_avg:36.21ms
step:62/2035 train_time:2241ms step_avg:36.15ms
step:63/2035 train_time:2275ms step_avg:36.11ms
step:64/2035 train_time:2308ms step_avg:36.06ms
step:65/2035 train_time:2341ms step_avg:36.01ms
step:66/2035 train_time:2374ms step_avg:35.97ms
step:67/2035 train_time:2408ms step_avg:35.94ms
step:68/2035 train_time:2441ms step_avg:35.89ms
step:69/2035 train_time:2474ms step_avg:35.86ms
step:70/2035 train_time:2507ms step_avg:35.81ms
step:71/2035 train_time:2541ms step_avg:35.79ms
step:72/2035 train_time:2574ms step_avg:35.75ms
step:73/2035 train_time:2607ms step_avg:35.71ms
step:74/2035 train_time:2640ms step_avg:35.68ms
step:75/2035 train_time:2673ms step_avg:35.64ms
step:76/2035 train_time:2706ms step_avg:35.61ms
step:77/2035 train_time:2739ms step_avg:35.58ms
step:78/2035 train_time:2772ms step_avg:35.54ms
step:79/2035 train_time:2806ms step_avg:35.51ms
step:80/2035 train_time:2838ms step_avg:35.48ms
step:81/2035 train_time:2872ms step_avg:35.46ms
step:82/2035 train_time:2905ms step_avg:35.43ms
step:83/2035 train_time:2938ms step_avg:35.40ms
step:84/2035 train_time:2971ms step_avg:35.37ms
step:85/2035 train_time:3004ms step_avg:35.34ms
step:86/2035 train_time:3037ms step_avg:35.31ms
step:87/2035 train_time:3070ms step_avg:35.29ms
step:88/2035 train_time:3103ms step_avg:35.26ms
step:89/2035 train_time:3136ms step_avg:35.24ms
step:90/2035 train_time:3169ms step_avg:35.21ms
step:91/2035 train_time:3202ms step_avg:35.19ms
step:92/2035 train_time:3235ms step_avg:35.17ms
step:93/2035 train_time:3269ms step_avg:35.15ms
step:94/2035 train_time:3301ms step_avg:35.12ms
step:95/2035 train_time:3335ms step_avg:35.10ms
step:96/2035 train_time:3368ms step_avg:35.08ms
step:97/2035 train_time:3401ms step_avg:35.06ms
step:98/2035 train_time:3434ms step_avg:35.04ms
step:99/2035 train_time:3468ms step_avg:35.03ms
step:100/2035 train_time:3501ms step_avg:35.01ms
step:101/2035 train_time:3534ms step_avg:34.99ms
step:102/2035 train_time:3567ms step_avg:34.97ms
step:103/2035 train_time:3600ms step_avg:34.95ms
step:104/2035 train_time:3633ms step_avg:34.93ms
step:105/2035 train_time:3666ms step_avg:34.92ms
step:106/2035 train_time:3699ms step_avg:34.90ms
step:107/2035 train_time:3732ms step_avg:34.88ms
step:108/2035 train_time:3765ms step_avg:34.87ms
step:109/2035 train_time:3798ms step_avg:34.85ms
step:110/2035 train_time:3831ms step_avg:34.83ms
step:111/2035 train_time:3865ms step_avg:34.82ms
step:112/2035 train_time:3898ms step_avg:34.80ms
step:113/2035 train_time:3931ms step_avg:34.79ms
step:114/2035 train_time:3964ms step_avg:34.77ms
step:115/2035 train_time:3997ms step_avg:34.75ms
step:116/2035 train_time:4030ms step_avg:34.74ms
step:117/2035 train_time:4063ms step_avg:34.72ms
step:118/2035 train_time:4096ms step_avg:34.71ms
step:119/2035 train_time:4129ms step_avg:34.70ms
step:120/2035 train_time:4162ms step_avg:34.68ms
step:121/2035 train_time:4195ms step_avg:34.67ms
step:122/2035 train_time:4228ms step_avg:34.66ms
step:123/2035 train_time:4261ms step_avg:34.64ms
step:124/2035 train_time:4294ms step_avg:34.63ms
step:125/2035 train_time:4327ms step_avg:34.62ms
step:126/2035 train_time:4360ms step_avg:34.60ms
step:127/2035 train_time:4393ms step_avg:34.59ms
step:128/2035 train_time:4426ms step_avg:34.58ms
step:129/2035 train_time:4459ms step_avg:34.57ms
step:130/2035 train_time:4492ms step_avg:34.55ms
step:131/2035 train_time:4525ms step_avg:34.54ms
step:132/2035 train_time:4558ms step_avg:34.53ms
step:133/2035 train_time:4591ms step_avg:34.52ms
step:134/2035 train_time:4624ms step_avg:34.51ms
step:135/2035 train_time:4658ms step_avg:34.50ms
step:136/2035 train_time:4691ms step_avg:34.49ms
step:137/2035 train_time:4724ms step_avg:34.48ms
step:138/2035 train_time:4757ms step_avg:34.47ms
step:139/2035 train_time:4790ms step_avg:34.46ms
step:140/2035 train_time:4824ms step_avg:34.45ms
step:141/2035 train_time:4856ms step_avg:34.44ms
step:142/2035 train_time:4889ms step_avg:34.43ms
step:143/2035 train_time:4922ms step_avg:34.42ms
step:144/2035 train_time:4955ms step_avg:34.41ms
step:145/2035 train_time:4988ms step_avg:34.40ms
step:146/2035 train_time:5021ms step_avg:34.39ms
step:147/2035 train_time:5055ms step_avg:34.38ms
step:148/2035 train_time:5087ms step_avg:34.37ms
step:149/2035 train_time:5120ms step_avg:34.37ms
step:150/2035 train_time:5153ms step_avg:34.35ms
step:151/2035 train_time:5187ms step_avg:34.35ms
step:152/2035 train_time:5219ms step_avg:34.34ms
step:153/2035 train_time:5252ms step_avg:34.33ms
step:154/2035 train_time:5285ms step_avg:34.32ms
step:155/2035 train_time:5318ms step_avg:34.31ms
step:156/2035 train_time:5351ms step_avg:34.30ms
step:157/2035 train_time:5385ms step_avg:34.30ms
step:158/2035 train_time:5417ms step_avg:34.29ms
step:159/2035 train_time:5451ms step_avg:34.28ms
step:160/2035 train_time:5484ms step_avg:34.27ms
step:161/2035 train_time:5517ms step_avg:34.27ms
step:162/2035 train_time:5550ms step_avg:34.26ms
step:163/2035 train_time:5584ms step_avg:34.26ms
step:164/2035 train_time:5617ms step_avg:34.25ms
step:165/2035 train_time:5650ms step_avg:34.24ms
step:166/2035 train_time:5683ms step_avg:34.23ms
step:167/2035 train_time:5716ms step_avg:34.23ms
step:168/2035 train_time:5749ms step_avg:34.22ms
step:169/2035 train_time:5782ms step_avg:34.21ms
step:170/2035 train_time:5815ms step_avg:34.20ms
step:171/2035 train_time:5848ms step_avg:34.20ms
step:172/2035 train_time:5881ms step_avg:34.19ms
step:173/2035 train_time:5914ms step_avg:34.19ms
step:174/2035 train_time:5947ms step_avg:34.18ms
step:175/2035 train_time:5980ms step_avg:34.17ms
step:176/2035 train_time:6013ms step_avg:34.16ms
step:177/2035 train_time:6046ms step_avg:34.16ms
step:178/2035 train_time:6078ms step_avg:34.15ms
step:179/2035 train_time:6112ms step_avg:34.14ms
step:180/2035 train_time:6145ms step_avg:34.14ms
step:181/2035 train_time:6178ms step_avg:34.13ms
step:182/2035 train_time:6211ms step_avg:34.12ms
step:183/2035 train_time:6244ms step_avg:34.12ms
step:184/2035 train_time:6276ms step_avg:34.11ms
step:185/2035 train_time:6310ms step_avg:34.11ms
step:186/2035 train_time:6343ms step_avg:34.10ms
step:187/2035 train_time:6376ms step_avg:34.10ms
step:188/2035 train_time:6409ms step_avg:34.09ms
step:189/2035 train_time:6442ms step_avg:34.08ms
step:190/2035 train_time:6474ms step_avg:34.08ms
step:191/2035 train_time:6508ms step_avg:34.07ms
step:192/2035 train_time:6540ms step_avg:34.06ms
step:193/2035 train_time:6574ms step_avg:34.06ms
step:194/2035 train_time:6607ms step_avg:34.05ms
step:195/2035 train_time:6639ms step_avg:34.05ms
step:196/2035 train_time:6672ms step_avg:34.04ms
step:197/2035 train_time:6705ms step_avg:34.04ms
step:198/2035 train_time:6738ms step_avg:34.03ms
step:199/2035 train_time:6772ms step_avg:34.03ms
step:200/2035 train_time:6805ms step_avg:34.02ms
step:201/2035 train_time:6838ms step_avg:34.02ms
step:202/2035 train_time:6871ms step_avg:34.01ms
step:203/2035 train_time:6904ms step_avg:34.01ms
step:204/2035 train_time:6937ms step_avg:34.00ms
step:205/2035 train_time:6970ms step_avg:34.00ms
step:206/2035 train_time:7003ms step_avg:33.99ms
step:207/2035 train_time:7036ms step_avg:33.99ms
step:208/2035 train_time:7069ms step_avg:33.98ms
step:209/2035 train_time:7102ms step_avg:33.98ms
step:210/2035 train_time:7134ms step_avg:33.97ms
step:211/2035 train_time:7167ms step_avg:33.97ms
step:212/2035 train_time:7200ms step_avg:33.96ms
step:213/2035 train_time:7233ms step_avg:33.96ms
step:214/2035 train_time:7266ms step_avg:33.96ms
step:215/2035 train_time:7299ms step_avg:33.95ms
step:216/2035 train_time:7332ms step_avg:33.95ms
step:217/2035 train_time:7365ms step_avg:33.94ms
step:218/2035 train_time:7398ms step_avg:33.94ms
step:219/2035 train_time:7431ms step_avg:33.93ms
step:220/2035 train_time:7464ms step_avg:33.93ms
step:221/2035 train_time:7497ms step_avg:33.92ms
step:222/2035 train_time:7530ms step_avg:33.92ms
step:223/2035 train_time:7563ms step_avg:33.91ms
step:224/2035 train_time:7596ms step_avg:33.91ms
step:225/2035 train_time:7629ms step_avg:33.90ms
step:226/2035 train_time:7662ms step_avg:33.90ms
step:227/2035 train_time:7695ms step_avg:33.90ms
step:228/2035 train_time:7728ms step_avg:33.89ms
step:229/2035 train_time:7761ms step_avg:33.89ms
step:230/2035 train_time:7793ms step_avg:33.88ms
step:231/2035 train_time:7827ms step_avg:33.89ms
step:232/2035 train_time:7860ms step_avg:33.88ms
step:233/2035 train_time:7894ms step_avg:33.88ms
step:234/2035 train_time:7926ms step_avg:33.87ms
step:235/2035 train_time:7959ms step_avg:33.87ms
step:236/2035 train_time:7992ms step_avg:33.87ms
step:237/2035 train_time:8025ms step_avg:33.86ms
step:238/2035 train_time:8058ms step_avg:33.86ms
step:239/2035 train_time:8091ms step_avg:33.86ms
step:240/2035 train_time:8124ms step_avg:33.85ms
step:241/2035 train_time:8157ms step_avg:33.85ms
step:242/2035 train_time:8190ms step_avg:33.84ms
step:243/2035 train_time:8223ms step_avg:33.84ms
step:244/2035 train_time:8255ms step_avg:33.83ms
step:245/2035 train_time:8289ms step_avg:33.83ms
step:246/2035 train_time:8321ms step_avg:33.83ms
step:247/2035 train_time:8355ms step_avg:33.83ms
step:248/2035 train_time:8388ms step_avg:33.82ms
step:249/2035 train_time:8421ms step_avg:33.82ms
step:250/2035 train_time:8454ms step_avg:33.82ms
step:250/2035 val_loss:4.2606 train_time:8490ms step_avg:33.96ms
step:251/2035 train_time:8510ms step_avg:33.90ms
step:252/2035 train_time:8529ms step_avg:33.85ms
step:253/2035 train_time:8559ms step_avg:33.83ms
step:254/2035 train_time:8592ms step_avg:33.83ms
step:255/2035 train_time:8628ms step_avg:33.83ms
step:256/2035 train_time:8661ms step_avg:33.83ms
step:257/2035 train_time:8695ms step_avg:33.83ms
step:258/2035 train_time:8728ms step_avg:33.83ms
step:259/2035 train_time:8762ms step_avg:33.83ms
step:260/2035 train_time:8795ms step_avg:33.83ms
step:261/2035 train_time:8828ms step_avg:33.82ms
step:262/2035 train_time:8861ms step_avg:33.82ms
step:263/2035 train_time:8894ms step_avg:33.82ms
step:264/2035 train_time:8926ms step_avg:33.81ms
step:265/2035 train_time:8959ms step_avg:33.81ms
step:266/2035 train_time:8992ms step_avg:33.81ms
step:267/2035 train_time:9025ms step_avg:33.80ms
step:268/2035 train_time:9058ms step_avg:33.80ms
step:269/2035 train_time:9091ms step_avg:33.79ms
step:270/2035 train_time:9123ms step_avg:33.79ms
step:271/2035 train_time:9157ms step_avg:33.79ms
step:272/2035 train_time:9190ms step_avg:33.79ms
step:273/2035 train_time:9223ms step_avg:33.78ms
step:274/2035 train_time:9255ms step_avg:33.78ms
step:275/2035 train_time:9288ms step_avg:33.78ms
step:276/2035 train_time:9321ms step_avg:33.77ms
step:277/2035 train_time:9354ms step_avg:33.77ms
step:278/2035 train_time:9386ms step_avg:33.76ms
step:279/2035 train_time:9420ms step_avg:33.76ms
step:280/2035 train_time:9453ms step_avg:33.76ms
step:281/2035 train_time:9486ms step_avg:33.76ms
step:282/2035 train_time:9518ms step_avg:33.75ms
step:283/2035 train_time:9552ms step_avg:33.75ms
step:284/2035 train_time:9585ms step_avg:33.75ms
step:285/2035 train_time:9618ms step_avg:33.75ms
step:286/2035 train_time:9651ms step_avg:33.75ms
step:287/2035 train_time:9685ms step_avg:33.74ms
step:288/2035 train_time:9718ms step_avg:33.74ms
step:289/2035 train_time:9751ms step_avg:33.74ms
step:290/2035 train_time:9784ms step_avg:33.74ms
step:291/2035 train_time:9818ms step_avg:33.74ms
step:292/2035 train_time:9851ms step_avg:33.73ms
step:293/2035 train_time:9884ms step_avg:33.73ms
step:294/2035 train_time:9917ms step_avg:33.73ms
step:295/2035 train_time:9950ms step_avg:33.73ms
step:296/2035 train_time:9983ms step_avg:33.73ms
step:297/2035 train_time:10016ms step_avg:33.72ms
step:298/2035 train_time:10049ms step_avg:33.72ms
step:299/2035 train_time:10082ms step_avg:33.72ms
step:300/2035 train_time:10115ms step_avg:33.72ms
step:301/2035 train_time:10148ms step_avg:33.71ms
step:302/2035 train_time:10181ms step_avg:33.71ms
step:303/2035 train_time:10214ms step_avg:33.71ms
step:304/2035 train_time:10246ms step_avg:33.70ms
step:305/2035 train_time:10279ms step_avg:33.70ms
step:306/2035 train_time:10312ms step_avg:33.70ms
step:307/2035 train_time:10345ms step_avg:33.70ms
step:308/2035 train_time:10378ms step_avg:33.69ms
step:309/2035 train_time:10411ms step_avg:33.69ms
step:310/2035 train_time:10444ms step_avg:33.69ms
step:311/2035 train_time:10477ms step_avg:33.69ms
step:312/2035 train_time:10510ms step_avg:33.69ms
step:313/2035 train_time:10543ms step_avg:33.68ms
step:314/2035 train_time:10576ms step_avg:33.68ms
step:315/2035 train_time:10610ms step_avg:33.68ms
step:316/2035 train_time:10642ms step_avg:33.68ms
step:317/2035 train_time:10676ms step_avg:33.68ms
step:318/2035 train_time:10709ms step_avg:33.68ms
step:319/2035 train_time:10743ms step_avg:33.68ms
step:320/2035 train_time:10775ms step_avg:33.67ms
step:321/2035 train_time:10809ms step_avg:33.67ms
step:322/2035 train_time:10842ms step_avg:33.67ms
step:323/2035 train_time:10875ms step_avg:33.67ms
step:324/2035 train_time:10907ms step_avg:33.67ms
step:325/2035 train_time:10941ms step_avg:33.66ms
step:326/2035 train_time:10974ms step_avg:33.66ms
step:327/2035 train_time:11007ms step_avg:33.66ms
step:328/2035 train_time:11040ms step_avg:33.66ms
step:329/2035 train_time:11073ms step_avg:33.66ms
step:330/2035 train_time:11106ms step_avg:33.65ms
step:331/2035 train_time:11139ms step_avg:33.65ms
step:332/2035 train_time:11172ms step_avg:33.65ms
step:333/2035 train_time:11205ms step_avg:33.65ms
step:334/2035 train_time:11237ms step_avg:33.64ms
step:335/2035 train_time:11270ms step_avg:33.64ms
step:336/2035 train_time:11303ms step_avg:33.64ms
step:337/2035 train_time:11336ms step_avg:33.64ms
step:338/2035 train_time:11369ms step_avg:33.64ms
step:339/2035 train_time:11402ms step_avg:33.63ms
step:340/2035 train_time:11434ms step_avg:33.63ms
step:341/2035 train_time:11468ms step_avg:33.63ms
step:342/2035 train_time:11501ms step_avg:33.63ms
step:343/2035 train_time:11534ms step_avg:33.63ms
step:344/2035 train_time:11567ms step_avg:33.62ms
step:345/2035 train_time:11600ms step_avg:33.62ms
step:346/2035 train_time:11634ms step_avg:33.62ms
step:347/2035 train_time:11666ms step_avg:33.62ms
step:348/2035 train_time:11699ms step_avg:33.62ms
step:349/2035 train_time:11732ms step_avg:33.62ms
step:350/2035 train_time:11765ms step_avg:33.61ms
step:351/2035 train_time:11798ms step_avg:33.61ms
step:352/2035 train_time:11831ms step_avg:33.61ms
step:353/2035 train_time:11865ms step_avg:33.61ms
step:354/2035 train_time:11897ms step_avg:33.61ms
step:355/2035 train_time:11931ms step_avg:33.61ms
step:356/2035 train_time:11964ms step_avg:33.61ms
step:357/2035 train_time:11997ms step_avg:33.61ms
step:358/2035 train_time:12030ms step_avg:33.60ms
step:359/2035 train_time:12064ms step_avg:33.60ms
step:360/2035 train_time:12096ms step_avg:33.60ms
step:361/2035 train_time:12130ms step_avg:33.60ms
step:362/2035 train_time:12162ms step_avg:33.60ms
step:363/2035 train_time:12196ms step_avg:33.60ms
step:364/2035 train_time:12229ms step_avg:33.60ms
step:365/2035 train_time:12262ms step_avg:33.59ms
step:366/2035 train_time:12294ms step_avg:33.59ms
step:367/2035 train_time:12327ms step_avg:33.59ms
step:368/2035 train_time:12360ms step_avg:33.59ms
step:369/2035 train_time:12393ms step_avg:33.59ms
step:370/2035 train_time:12426ms step_avg:33.58ms
step:371/2035 train_time:12459ms step_avg:33.58ms
step:372/2035 train_time:12492ms step_avg:33.58ms
step:373/2035 train_time:12525ms step_avg:33.58ms
step:374/2035 train_time:12558ms step_avg:33.58ms
step:375/2035 train_time:12591ms step_avg:33.58ms
step:376/2035 train_time:12624ms step_avg:33.57ms
step:377/2035 train_time:12657ms step_avg:33.57ms
step:378/2035 train_time:12690ms step_avg:33.57ms
step:379/2035 train_time:12723ms step_avg:33.57ms
step:380/2035 train_time:12755ms step_avg:33.57ms
step:381/2035 train_time:12789ms step_avg:33.57ms
step:382/2035 train_time:12821ms step_avg:33.56ms
step:383/2035 train_time:12854ms step_avg:33.56ms
step:384/2035 train_time:12887ms step_avg:33.56ms
step:385/2035 train_time:12920ms step_avg:33.56ms
step:386/2035 train_time:12953ms step_avg:33.56ms
step:387/2035 train_time:12986ms step_avg:33.56ms
step:388/2035 train_time:13019ms step_avg:33.55ms
step:389/2035 train_time:13052ms step_avg:33.55ms
step:390/2035 train_time:13085ms step_avg:33.55ms
step:391/2035 train_time:13118ms step_avg:33.55ms
step:392/2035 train_time:13151ms step_avg:33.55ms
step:393/2035 train_time:13184ms step_avg:33.55ms
step:394/2035 train_time:13217ms step_avg:33.55ms
step:395/2035 train_time:13250ms step_avg:33.54ms
step:396/2035 train_time:13283ms step_avg:33.54ms
step:397/2035 train_time:13316ms step_avg:33.54ms
step:398/2035 train_time:13349ms step_avg:33.54ms
step:399/2035 train_time:13382ms step_avg:33.54ms
step:400/2035 train_time:13414ms step_avg:33.54ms
step:401/2035 train_time:13447ms step_avg:33.53ms
step:402/2035 train_time:13480ms step_avg:33.53ms
step:403/2035 train_time:13513ms step_avg:33.53ms
step:404/2035 train_time:13545ms step_avg:33.53ms
step:405/2035 train_time:13578ms step_avg:33.53ms
step:406/2035 train_time:13611ms step_avg:33.53ms
step:407/2035 train_time:13644ms step_avg:33.52ms
step:408/2035 train_time:13677ms step_avg:33.52ms
step:409/2035 train_time:13710ms step_avg:33.52ms
step:410/2035 train_time:13743ms step_avg:33.52ms
step:411/2035 train_time:13776ms step_avg:33.52ms
step:412/2035 train_time:13809ms step_avg:33.52ms
step:413/2035 train_time:13842ms step_avg:33.51ms
step:414/2035 train_time:13874ms step_avg:33.51ms
step:415/2035 train_time:13907ms step_avg:33.51ms
step:416/2035 train_time:13940ms step_avg:33.51ms
step:417/2035 train_time:13974ms step_avg:33.51ms
step:418/2035 train_time:14007ms step_avg:33.51ms
step:419/2035 train_time:14040ms step_avg:33.51ms
step:420/2035 train_time:14073ms step_avg:33.51ms
step:421/2035 train_time:14106ms step_avg:33.51ms
step:422/2035 train_time:14139ms step_avg:33.50ms
step:423/2035 train_time:14172ms step_avg:33.50ms
step:424/2035 train_time:14205ms step_avg:33.50ms
step:425/2035 train_time:14238ms step_avg:33.50ms
step:426/2035 train_time:14271ms step_avg:33.50ms
step:427/2035 train_time:14304ms step_avg:33.50ms
step:428/2035 train_time:14336ms step_avg:33.50ms
step:429/2035 train_time:14370ms step_avg:33.50ms
step:430/2035 train_time:14403ms step_avg:33.49ms
step:431/2035 train_time:14436ms step_avg:33.49ms
step:432/2035 train_time:14469ms step_avg:33.49ms
step:433/2035 train_time:14502ms step_avg:33.49ms
step:434/2035 train_time:14535ms step_avg:33.49ms
step:435/2035 train_time:14567ms step_avg:33.49ms
step:436/2035 train_time:14600ms step_avg:33.49ms
step:437/2035 train_time:14633ms step_avg:33.49ms
step:438/2035 train_time:14666ms step_avg:33.48ms
step:439/2035 train_time:14699ms step_avg:33.48ms
step:440/2035 train_time:14732ms step_avg:33.48ms
step:441/2035 train_time:14765ms step_avg:33.48ms
step:442/2035 train_time:14798ms step_avg:33.48ms
step:443/2035 train_time:14831ms step_avg:33.48ms
step:444/2035 train_time:14864ms step_avg:33.48ms
step:445/2035 train_time:14897ms step_avg:33.48ms
step:446/2035 train_time:14929ms step_avg:33.47ms
step:447/2035 train_time:14963ms step_avg:33.47ms
step:448/2035 train_time:14995ms step_avg:33.47ms
step:449/2035 train_time:15029ms step_avg:33.47ms
step:450/2035 train_time:15061ms step_avg:33.47ms
step:451/2035 train_time:15095ms step_avg:33.47ms
step:452/2035 train_time:15128ms step_avg:33.47ms
step:453/2035 train_time:15161ms step_avg:33.47ms
step:454/2035 train_time:15194ms step_avg:33.47ms
step:455/2035 train_time:15227ms step_avg:33.47ms
step:456/2035 train_time:15260ms step_avg:33.46ms
step:457/2035 train_time:15293ms step_avg:33.46ms
step:458/2035 train_time:15326ms step_avg:33.46ms
step:459/2035 train_time:15359ms step_avg:33.46ms
step:460/2035 train_time:15392ms step_avg:33.46ms
step:461/2035 train_time:15425ms step_avg:33.46ms
step:462/2035 train_time:15458ms step_avg:33.46ms
step:463/2035 train_time:15491ms step_avg:33.46ms
step:464/2035 train_time:15524ms step_avg:33.46ms
step:465/2035 train_time:15557ms step_avg:33.46ms
step:466/2035 train_time:15590ms step_avg:33.45ms
step:467/2035 train_time:15623ms step_avg:33.45ms
step:468/2035 train_time:15655ms step_avg:33.45ms
step:469/2035 train_time:15688ms step_avg:33.45ms
step:470/2035 train_time:15721ms step_avg:33.45ms
step:471/2035 train_time:15754ms step_avg:33.45ms
step:472/2035 train_time:15787ms step_avg:33.45ms
step:473/2035 train_time:15820ms step_avg:33.45ms
step:474/2035 train_time:15853ms step_avg:33.45ms
step:475/2035 train_time:15886ms step_avg:33.44ms
step:476/2035 train_time:15919ms step_avg:33.44ms
step:477/2035 train_time:15952ms step_avg:33.44ms
step:478/2035 train_time:15985ms step_avg:33.44ms
step:479/2035 train_time:16018ms step_avg:33.44ms
step:480/2035 train_time:16051ms step_avg:33.44ms
step:481/2035 train_time:16084ms step_avg:33.44ms
step:482/2035 train_time:16117ms step_avg:33.44ms
step:483/2035 train_time:16150ms step_avg:33.44ms
step:484/2035 train_time:16183ms step_avg:33.44ms
step:485/2035 train_time:16216ms step_avg:33.44ms
step:486/2035 train_time:16249ms step_avg:33.43ms
step:487/2035 train_time:16282ms step_avg:33.43ms
step:488/2035 train_time:16315ms step_avg:33.43ms
step:489/2035 train_time:16349ms step_avg:33.43ms
step:490/2035 train_time:16381ms step_avg:33.43ms
step:491/2035 train_time:16414ms step_avg:33.43ms
step:492/2035 train_time:16447ms step_avg:33.43ms
step:493/2035 train_time:16480ms step_avg:33.43ms
step:494/2035 train_time:16513ms step_avg:33.43ms
step:495/2035 train_time:16546ms step_avg:33.43ms
step:496/2035 train_time:16579ms step_avg:33.42ms
step:497/2035 train_time:16612ms step_avg:33.42ms
step:498/2035 train_time:16645ms step_avg:33.42ms
step:499/2035 train_time:16678ms step_avg:33.42ms
step:500/2035 train_time:16711ms step_avg:33.42ms
step:500/2035 val_loss:3.9943 train_time:16746ms step_avg:33.49ms
step:501/2035 train_time:16765ms step_avg:33.46ms
step:502/2035 train_time:16784ms step_avg:33.43ms
step:503/2035 train_time:16815ms step_avg:33.43ms
step:504/2035 train_time:16848ms step_avg:33.43ms
step:505/2035 train_time:16882ms step_avg:33.43ms
step:506/2035 train_time:16915ms step_avg:33.43ms
step:507/2035 train_time:16949ms step_avg:33.43ms
step:508/2035 train_time:16982ms step_avg:33.43ms
step:509/2035 train_time:17015ms step_avg:33.43ms
step:510/2035 train_time:17048ms step_avg:33.43ms
step:511/2035 train_time:17081ms step_avg:33.43ms
step:512/2035 train_time:17114ms step_avg:33.43ms
step:513/2035 train_time:17147ms step_avg:33.42ms
step:514/2035 train_time:17179ms step_avg:33.42ms
step:515/2035 train_time:17212ms step_avg:33.42ms
step:516/2035 train_time:17245ms step_avg:33.42ms
step:517/2035 train_time:17278ms step_avg:33.42ms
step:518/2035 train_time:17311ms step_avg:33.42ms
step:519/2035 train_time:17344ms step_avg:33.42ms
step:520/2035 train_time:17377ms step_avg:33.42ms
step:521/2035 train_time:17410ms step_avg:33.42ms
step:522/2035 train_time:17443ms step_avg:33.41ms
step:523/2035 train_time:17475ms step_avg:33.41ms
step:524/2035 train_time:17508ms step_avg:33.41ms
step:525/2035 train_time:17541ms step_avg:33.41ms
step:526/2035 train_time:17574ms step_avg:33.41ms
step:527/2035 train_time:17607ms step_avg:33.41ms
step:528/2035 train_time:17640ms step_avg:33.41ms
step:529/2035 train_time:17673ms step_avg:33.41ms
step:530/2035 train_time:17706ms step_avg:33.41ms
step:531/2035 train_time:17739ms step_avg:33.41ms
step:532/2035 train_time:17772ms step_avg:33.41ms
step:533/2035 train_time:17806ms step_avg:33.41ms
step:534/2035 train_time:17839ms step_avg:33.41ms
step:535/2035 train_time:17873ms step_avg:33.41ms
step:536/2035 train_time:17906ms step_avg:33.41ms
step:537/2035 train_time:17939ms step_avg:33.41ms
step:538/2035 train_time:17972ms step_avg:33.41ms
step:539/2035 train_time:18005ms step_avg:33.41ms
step:540/2035 train_time:18038ms step_avg:33.40ms
step:541/2035 train_time:18071ms step_avg:33.40ms
step:542/2035 train_time:18104ms step_avg:33.40ms
step:543/2035 train_time:18138ms step_avg:33.40ms
step:544/2035 train_time:18170ms step_avg:33.40ms
step:545/2035 train_time:18204ms step_avg:33.40ms
step:546/2035 train_time:18236ms step_avg:33.40ms
step:547/2035 train_time:18269ms step_avg:33.40ms
step:548/2035 train_time:18302ms step_avg:33.40ms
step:549/2035 train_time:18335ms step_avg:33.40ms
step:550/2035 train_time:18368ms step_avg:33.40ms
step:551/2035 train_time:18401ms step_avg:33.40ms
step:552/2035 train_time:18434ms step_avg:33.39ms
step:553/2035 train_time:18467ms step_avg:33.39ms
step:554/2035 train_time:18500ms step_avg:33.39ms
step:555/2035 train_time:18533ms step_avg:33.39ms
step:556/2035 train_time:18566ms step_avg:33.39ms
step:557/2035 train_time:18598ms step_avg:33.39ms
step:558/2035 train_time:18631ms step_avg:33.39ms
step:559/2035 train_time:18665ms step_avg:33.39ms
step:560/2035 train_time:18697ms step_avg:33.39ms
step:561/2035 train_time:18730ms step_avg:33.39ms
step:562/2035 train_time:18764ms step_avg:33.39ms
step:563/2035 train_time:18797ms step_avg:33.39ms
step:564/2035 train_time:18829ms step_avg:33.39ms
step:565/2035 train_time:18862ms step_avg:33.38ms
step:566/2035 train_time:18895ms step_avg:33.38ms
step:567/2035 train_time:18928ms step_avg:33.38ms
step:568/2035 train_time:18961ms step_avg:33.38ms
step:569/2035 train_time:18994ms step_avg:33.38ms
step:570/2035 train_time:19027ms step_avg:33.38ms
step:571/2035 train_time:19060ms step_avg:33.38ms
step:572/2035 train_time:19093ms step_avg:33.38ms
step:573/2035 train_time:19127ms step_avg:33.38ms
step:574/2035 train_time:19159ms step_avg:33.38ms
step:575/2035 train_time:19192ms step_avg:33.38ms
step:576/2035 train_time:19225ms step_avg:33.38ms
step:577/2035 train_time:19258ms step_avg:33.38ms
step:578/2035 train_time:19291ms step_avg:33.38ms
step:579/2035 train_time:19324ms step_avg:33.38ms
step:580/2035 train_time:19357ms step_avg:33.37ms
step:581/2035 train_time:19390ms step_avg:33.37ms
step:582/2035 train_time:19423ms step_avg:33.37ms
step:583/2035 train_time:19456ms step_avg:33.37ms
step:584/2035 train_time:19489ms step_avg:33.37ms
step:585/2035 train_time:19522ms step_avg:33.37ms
step:586/2035 train_time:19554ms step_avg:33.37ms
step:587/2035 train_time:19587ms step_avg:33.37ms
step:588/2035 train_time:19620ms step_avg:33.37ms
step:589/2035 train_time:19653ms step_avg:33.37ms
step:590/2035 train_time:19686ms step_avg:33.37ms
step:591/2035 train_time:19719ms step_avg:33.37ms
step:592/2035 train_time:19752ms step_avg:33.36ms
step:593/2035 train_time:19785ms step_avg:33.36ms
step:594/2035 train_time:19818ms step_avg:33.36ms
step:595/2035 train_time:19851ms step_avg:33.36ms
step:596/2035 train_time:19884ms step_avg:33.36ms
step:597/2035 train_time:19917ms step_avg:33.36ms
step:598/2035 train_time:19949ms step_avg:33.36ms
step:599/2035 train_time:19982ms step_avg:33.36ms
step:600/2035 train_time:20015ms step_avg:33.36ms
step:601/2035 train_time:20048ms step_avg:33.36ms
step:602/2035 train_time:20081ms step_avg:33.36ms
step:603/2035 train_time:20114ms step_avg:33.36ms
step:604/2035 train_time:20147ms step_avg:33.36ms
step:605/2035 train_time:20180ms step_avg:33.36ms
step:606/2035 train_time:20213ms step_avg:33.35ms
step:607/2035 train_time:20246ms step_avg:33.35ms
step:608/2035 train_time:20279ms step_avg:33.35ms
step:609/2035 train_time:20312ms step_avg:33.35ms
step:610/2035 train_time:20345ms step_avg:33.35ms
step:611/2035 train_time:20379ms step_avg:33.35ms
step:612/2035 train_time:20411ms step_avg:33.35ms
step:613/2035 train_time:20444ms step_avg:33.35ms
step:614/2035 train_time:20477ms step_avg:33.35ms
step:615/2035 train_time:20510ms step_avg:33.35ms
step:616/2035 train_time:20543ms step_avg:33.35ms
step:617/2035 train_time:20576ms step_avg:33.35ms
step:618/2035 train_time:20609ms step_avg:33.35ms
step:619/2035 train_time:20643ms step_avg:33.35ms
step:620/2035 train_time:20675ms step_avg:33.35ms
step:621/2035 train_time:20709ms step_avg:33.35ms
step:622/2035 train_time:20741ms step_avg:33.35ms
step:623/2035 train_time:20774ms step_avg:33.35ms
step:624/2035 train_time:20807ms step_avg:33.34ms
step:625/2035 train_time:20840ms step_avg:33.34ms
step:626/2035 train_time:20873ms step_avg:33.34ms
step:627/2035 train_time:20907ms step_avg:33.34ms
step:628/2035 train_time:20940ms step_avg:33.34ms
step:629/2035 train_time:20973ms step_avg:33.34ms
step:630/2035 train_time:21006ms step_avg:33.34ms
step:631/2035 train_time:21038ms step_avg:33.34ms
step:632/2035 train_time:21071ms step_avg:33.34ms
step:633/2035 train_time:21105ms step_avg:33.34ms
step:634/2035 train_time:21138ms step_avg:33.34ms
step:635/2035 train_time:21171ms step_avg:33.34ms
step:636/2035 train_time:21204ms step_avg:33.34ms
step:637/2035 train_time:21237ms step_avg:33.34ms
step:638/2035 train_time:21270ms step_avg:33.34ms
step:639/2035 train_time:21303ms step_avg:33.34ms
step:640/2035 train_time:21336ms step_avg:33.34ms
step:641/2035 train_time:21369ms step_avg:33.34ms
step:642/2035 train_time:21402ms step_avg:33.34ms
step:643/2035 train_time:21435ms step_avg:33.34ms
step:644/2035 train_time:21468ms step_avg:33.33ms
step:645/2035 train_time:21501ms step_avg:33.33ms
step:646/2035 train_time:21533ms step_avg:33.33ms
step:647/2035 train_time:21567ms step_avg:33.33ms
step:648/2035 train_time:21600ms step_avg:33.33ms
step:649/2035 train_time:21633ms step_avg:33.33ms
step:650/2035 train_time:21666ms step_avg:33.33ms
step:651/2035 train_time:21699ms step_avg:33.33ms
step:652/2035 train_time:21732ms step_avg:33.33ms
step:653/2035 train_time:21765ms step_avg:33.33ms
step:654/2035 train_time:21798ms step_avg:33.33ms
step:655/2035 train_time:21831ms step_avg:33.33ms
step:656/2035 train_time:21864ms step_avg:33.33ms
step:657/2035 train_time:21897ms step_avg:33.33ms
step:658/2035 train_time:21930ms step_avg:33.33ms
step:659/2035 train_time:21963ms step_avg:33.33ms
step:660/2035 train_time:21996ms step_avg:33.33ms
step:661/2035 train_time:22029ms step_avg:33.33ms
step:662/2035 train_time:22062ms step_avg:33.33ms
step:663/2035 train_time:22095ms step_avg:33.33ms
step:664/2035 train_time:22128ms step_avg:33.33ms
step:665/2035 train_time:22162ms step_avg:33.33ms
step:666/2035 train_time:22196ms step_avg:33.33ms
step:667/2035 train_time:22254ms step_avg:33.37ms
step:668/2035 train_time:22313ms step_avg:33.40ms
step:669/2035 train_time:22374ms step_avg:33.44ms
step:670/2035 train_time:22433ms step_avg:33.48ms
step:671/2035 train_time:22494ms step_avg:33.52ms
step:672/2035 train_time:22553ms step_avg:33.56ms
step:673/2035 train_time:22614ms step_avg:33.60ms
step:674/2035 train_time:22673ms step_avg:33.64ms
step:675/2035 train_time:22733ms step_avg:33.68ms
step:676/2035 train_time:22793ms step_avg:33.72ms
step:677/2035 train_time:22853ms step_avg:33.76ms
step:678/2035 train_time:22913ms step_avg:33.79ms
step:679/2035 train_time:22974ms step_avg:33.84ms
step:680/2035 train_time:23033ms step_avg:33.87ms
step:681/2035 train_time:23095ms step_avg:33.91ms
step:682/2035 train_time:23154ms step_avg:33.95ms
step:683/2035 train_time:23215ms step_avg:33.99ms
step:684/2035 train_time:23275ms step_avg:34.03ms
step:685/2035 train_time:23335ms step_avg:34.07ms
step:686/2035 train_time:23394ms step_avg:34.10ms
step:687/2035 train_time:23455ms step_avg:34.14ms
step:688/2035 train_time:23514ms step_avg:34.18ms
step:689/2035 train_time:23575ms step_avg:34.22ms
step:690/2035 train_time:23634ms step_avg:34.25ms
step:691/2035 train_time:23695ms step_avg:34.29ms
step:692/2035 train_time:23755ms step_avg:34.33ms
step:693/2035 train_time:23815ms step_avg:34.37ms
step:694/2035 train_time:23875ms step_avg:34.40ms
step:695/2035 train_time:23936ms step_avg:34.44ms
step:696/2035 train_time:23995ms step_avg:34.48ms
step:697/2035 train_time:24056ms step_avg:34.51ms
step:698/2035 train_time:24116ms step_avg:34.55ms
step:699/2035 train_time:24176ms step_avg:34.59ms
step:700/2035 train_time:24236ms step_avg:34.62ms
step:701/2035 train_time:24296ms step_avg:34.66ms
step:702/2035 train_time:24355ms step_avg:34.69ms
step:703/2035 train_time:24416ms step_avg:34.73ms
step:704/2035 train_time:24475ms step_avg:34.77ms
step:705/2035 train_time:24536ms step_avg:34.80ms
step:706/2035 train_time:24595ms step_avg:34.84ms
step:707/2035 train_time:24655ms step_avg:34.87ms
step:708/2035 train_time:24715ms step_avg:34.91ms
step:709/2035 train_time:24776ms step_avg:34.94ms
step:710/2035 train_time:24835ms step_avg:34.98ms
step:711/2035 train_time:24896ms step_avg:35.01ms
step:712/2035 train_time:24955ms step_avg:35.05ms
step:713/2035 train_time:25016ms step_avg:35.08ms
step:714/2035 train_time:25075ms step_avg:35.12ms
step:715/2035 train_time:25135ms step_avg:35.15ms
step:716/2035 train_time:25195ms step_avg:35.19ms
step:717/2035 train_time:25256ms step_avg:35.22ms
step:718/2035 train_time:25316ms step_avg:35.26ms
step:719/2035 train_time:25377ms step_avg:35.30ms
step:720/2035 train_time:25437ms step_avg:35.33ms
step:721/2035 train_time:25498ms step_avg:35.36ms
step:722/2035 train_time:25557ms step_avg:35.40ms
step:723/2035 train_time:25619ms step_avg:35.43ms
step:724/2035 train_time:25678ms step_avg:35.47ms
step:725/2035 train_time:25738ms step_avg:35.50ms
step:726/2035 train_time:25797ms step_avg:35.53ms
step:727/2035 train_time:25858ms step_avg:35.57ms
step:728/2035 train_time:25917ms step_avg:35.60ms
step:729/2035 train_time:25979ms step_avg:35.64ms
step:730/2035 train_time:26038ms step_avg:35.67ms
step:731/2035 train_time:26099ms step_avg:35.70ms
step:732/2035 train_time:26159ms step_avg:35.74ms
step:733/2035 train_time:26220ms step_avg:35.77ms
step:734/2035 train_time:26279ms step_avg:35.80ms
step:735/2035 train_time:26339ms step_avg:35.84ms
step:736/2035 train_time:26398ms step_avg:35.87ms
step:737/2035 train_time:26459ms step_avg:35.90ms
step:738/2035 train_time:26518ms step_avg:35.93ms
step:739/2035 train_time:26579ms step_avg:35.97ms
step:740/2035 train_time:26638ms step_avg:36.00ms
step:741/2035 train_time:26698ms step_avg:36.03ms
step:742/2035 train_time:26758ms step_avg:36.06ms
step:743/2035 train_time:26818ms step_avg:36.09ms
step:744/2035 train_time:26878ms step_avg:36.13ms
step:745/2035 train_time:26938ms step_avg:36.16ms
step:746/2035 train_time:26998ms step_avg:36.19ms
step:747/2035 train_time:27058ms step_avg:36.22ms
step:748/2035 train_time:27117ms step_avg:36.25ms
step:749/2035 train_time:27178ms step_avg:36.29ms
step:750/2035 train_time:27237ms step_avg:36.32ms
step:750/2035 val_loss:3.8310 train_time:27299ms step_avg:36.40ms
step:751/2035 train_time:27320ms step_avg:36.38ms
step:752/2035 train_time:27360ms step_avg:36.38ms
step:753/2035 train_time:27423ms step_avg:36.42ms
step:754/2035 train_time:27484ms step_avg:36.45ms
step:755/2035 train_time:27544ms step_avg:36.48ms
step:756/2035 train_time:27603ms step_avg:36.51ms
step:757/2035 train_time:27663ms step_avg:36.54ms
step:758/2035 train_time:27722ms step_avg:36.57ms
step:759/2035 train_time:27781ms step_avg:36.60ms
step:760/2035 train_time:27840ms step_avg:36.63ms
step:761/2035 train_time:27900ms step_avg:36.66ms
step:762/2035 train_time:27958ms step_avg:36.69ms
step:763/2035 train_time:28019ms step_avg:36.72ms
step:764/2035 train_time:28078ms step_avg:36.75ms
step:765/2035 train_time:28138ms step_avg:36.78ms
step:766/2035 train_time:28196ms step_avg:36.81ms
step:767/2035 train_time:28257ms step_avg:36.84ms
step:768/2035 train_time:28317ms step_avg:36.87ms
step:769/2035 train_time:28378ms step_avg:36.90ms
step:770/2035 train_time:28439ms step_avg:36.93ms
step:771/2035 train_time:28500ms step_avg:36.97ms
step:772/2035 train_time:28559ms step_avg:36.99ms
step:773/2035 train_time:28620ms step_avg:37.02ms
step:774/2035 train_time:28679ms step_avg:37.05ms
step:775/2035 train_time:28739ms step_avg:37.08ms
step:776/2035 train_time:28798ms step_avg:37.11ms
step:777/2035 train_time:28859ms step_avg:37.14ms
step:778/2035 train_time:28918ms step_avg:37.17ms
step:779/2035 train_time:28977ms step_avg:37.20ms
step:780/2035 train_time:29036ms step_avg:37.23ms
step:781/2035 train_time:29096ms step_avg:37.25ms
step:782/2035 train_time:29154ms step_avg:37.28ms
step:783/2035 train_time:29215ms step_avg:37.31ms
step:784/2035 train_time:29275ms step_avg:37.34ms
step:785/2035 train_time:29336ms step_avg:37.37ms
step:786/2035 train_time:29396ms step_avg:37.40ms
step:787/2035 train_time:29457ms step_avg:37.43ms
step:788/2035 train_time:29516ms step_avg:37.46ms
step:789/2035 train_time:29577ms step_avg:37.49ms
step:790/2035 train_time:29636ms step_avg:37.51ms
step:791/2035 train_time:29698ms step_avg:37.54ms
step:792/2035 train_time:29756ms step_avg:37.57ms
step:793/2035 train_time:29817ms step_avg:37.60ms
step:794/2035 train_time:29876ms step_avg:37.63ms
step:795/2035 train_time:29936ms step_avg:37.66ms
step:796/2035 train_time:29994ms step_avg:37.68ms
step:797/2035 train_time:30055ms step_avg:37.71ms
step:798/2035 train_time:30114ms step_avg:37.74ms
step:799/2035 train_time:30173ms step_avg:37.76ms
step:800/2035 train_time:30232ms step_avg:37.79ms
step:801/2035 train_time:30294ms step_avg:37.82ms
step:802/2035 train_time:30354ms step_avg:37.85ms
step:803/2035 train_time:30416ms step_avg:37.88ms
step:804/2035 train_time:30475ms step_avg:37.90ms
step:805/2035 train_time:30536ms step_avg:37.93ms
step:806/2035 train_time:30596ms step_avg:37.96ms
step:807/2035 train_time:30657ms step_avg:37.99ms
step:808/2035 train_time:30716ms step_avg:38.01ms
step:809/2035 train_time:30777ms step_avg:38.04ms
step:810/2035 train_time:30836ms step_avg:38.07ms
step:811/2035 train_time:30896ms step_avg:38.10ms
step:812/2035 train_time:30955ms step_avg:38.12ms
step:813/2035 train_time:31015ms step_avg:38.15ms
step:814/2035 train_time:31074ms step_avg:38.17ms
step:815/2035 train_time:31135ms step_avg:38.20ms
step:816/2035 train_time:31194ms step_avg:38.23ms
step:817/2035 train_time:31255ms step_avg:38.26ms
step:818/2035 train_time:31314ms step_avg:38.28ms
step:819/2035 train_time:31375ms step_avg:38.31ms
step:820/2035 train_time:31435ms step_avg:38.34ms
step:821/2035 train_time:31496ms step_avg:38.36ms
step:822/2035 train_time:31555ms step_avg:38.39ms
step:823/2035 train_time:31616ms step_avg:38.42ms
step:824/2035 train_time:31676ms step_avg:38.44ms
step:825/2035 train_time:31737ms step_avg:38.47ms
step:826/2035 train_time:31796ms step_avg:38.49ms
step:827/2035 train_time:31856ms step_avg:38.52ms
step:828/2035 train_time:31915ms step_avg:38.54ms
step:829/2035 train_time:31975ms step_avg:38.57ms
step:830/2035 train_time:32034ms step_avg:38.59ms
step:831/2035 train_time:32094ms step_avg:38.62ms
step:832/2035 train_time:32153ms step_avg:38.65ms
step:833/2035 train_time:32214ms step_avg:38.67ms
step:834/2035 train_time:32274ms step_avg:38.70ms
step:835/2035 train_time:32334ms step_avg:38.72ms
step:836/2035 train_time:32394ms step_avg:38.75ms
step:837/2035 train_time:32455ms step_avg:38.78ms
step:838/2035 train_time:32514ms step_avg:38.80ms
step:839/2035 train_time:32574ms step_avg:38.83ms
step:840/2035 train_time:32634ms step_avg:38.85ms
step:841/2035 train_time:32695ms step_avg:38.88ms
step:842/2035 train_time:32755ms step_avg:38.90ms
step:843/2035 train_time:32815ms step_avg:38.93ms
step:844/2035 train_time:32874ms step_avg:38.95ms
step:845/2035 train_time:32935ms step_avg:38.98ms
step:846/2035 train_time:32993ms step_avg:39.00ms
step:847/2035 train_time:33054ms step_avg:39.02ms
step:848/2035 train_time:33114ms step_avg:39.05ms
step:849/2035 train_time:33174ms step_avg:39.07ms
step:850/2035 train_time:33233ms step_avg:39.10ms
step:851/2035 train_time:33293ms step_avg:39.12ms
step:852/2035 train_time:33353ms step_avg:39.15ms
step:853/2035 train_time:33413ms step_avg:39.17ms
step:854/2035 train_time:33473ms step_avg:39.20ms
step:855/2035 train_time:33533ms step_avg:39.22ms
step:856/2035 train_time:33593ms step_avg:39.24ms
step:857/2035 train_time:33653ms step_avg:39.27ms
step:858/2035 train_time:33713ms step_avg:39.29ms
step:859/2035 train_time:33773ms step_avg:39.32ms
step:860/2035 train_time:33833ms step_avg:39.34ms
step:861/2035 train_time:33893ms step_avg:39.36ms
step:862/2035 train_time:33952ms step_avg:39.39ms
step:863/2035 train_time:34013ms step_avg:39.41ms
step:864/2035 train_time:34072ms step_avg:39.44ms
step:865/2035 train_time:34133ms step_avg:39.46ms
step:866/2035 train_time:34192ms step_avg:39.48ms
step:867/2035 train_time:34252ms step_avg:39.51ms
step:868/2035 train_time:34312ms step_avg:39.53ms
step:869/2035 train_time:34372ms step_avg:39.55ms
step:870/2035 train_time:34432ms step_avg:39.58ms
step:871/2035 train_time:34492ms step_avg:39.60ms
step:872/2035 train_time:34552ms step_avg:39.62ms
step:873/2035 train_time:34613ms step_avg:39.65ms
step:874/2035 train_time:34673ms step_avg:39.67ms
step:875/2035 train_time:34734ms step_avg:39.70ms
step:876/2035 train_time:34794ms step_avg:39.72ms
step:877/2035 train_time:34855ms step_avg:39.74ms
step:878/2035 train_time:34914ms step_avg:39.77ms
step:879/2035 train_time:34975ms step_avg:39.79ms
step:880/2035 train_time:35034ms step_avg:39.81ms
step:881/2035 train_time:35094ms step_avg:39.83ms
step:882/2035 train_time:35153ms step_avg:39.86ms
step:883/2035 train_time:35214ms step_avg:39.88ms
step:884/2035 train_time:35274ms step_avg:39.90ms
step:885/2035 train_time:35335ms step_avg:39.93ms
step:886/2035 train_time:35394ms step_avg:39.95ms
step:887/2035 train_time:35455ms step_avg:39.97ms
step:888/2035 train_time:35515ms step_avg:39.99ms
step:889/2035 train_time:35575ms step_avg:40.02ms
step:890/2035 train_time:35635ms step_avg:40.04ms
step:891/2035 train_time:35695ms step_avg:40.06ms
step:892/2035 train_time:35754ms step_avg:40.08ms
step:893/2035 train_time:35815ms step_avg:40.11ms
step:894/2035 train_time:35875ms step_avg:40.13ms
step:895/2035 train_time:35935ms step_avg:40.15ms
step:896/2035 train_time:35994ms step_avg:40.17ms
step:897/2035 train_time:36054ms step_avg:40.19ms
step:898/2035 train_time:36113ms step_avg:40.22ms
step:899/2035 train_time:36173ms step_avg:40.24ms
step:900/2035 train_time:36232ms step_avg:40.26ms
step:901/2035 train_time:36293ms step_avg:40.28ms
step:902/2035 train_time:36352ms step_avg:40.30ms
step:903/2035 train_time:36413ms step_avg:40.32ms
step:904/2035 train_time:36472ms step_avg:40.35ms
step:905/2035 train_time:36533ms step_avg:40.37ms
step:906/2035 train_time:36592ms step_avg:40.39ms
step:907/2035 train_time:36652ms step_avg:40.41ms
step:908/2035 train_time:36712ms step_avg:40.43ms
step:909/2035 train_time:36772ms step_avg:40.45ms
step:910/2035 train_time:36832ms step_avg:40.47ms
step:911/2035 train_time:36892ms step_avg:40.50ms
step:912/2035 train_time:36951ms step_avg:40.52ms
step:913/2035 train_time:37012ms step_avg:40.54ms
step:914/2035 train_time:37071ms step_avg:40.56ms
step:915/2035 train_time:37132ms step_avg:40.58ms
step:916/2035 train_time:37191ms step_avg:40.60ms
step:917/2035 train_time:37252ms step_avg:40.62ms
step:918/2035 train_time:37312ms step_avg:40.65ms
step:919/2035 train_time:37373ms step_avg:40.67ms
step:920/2035 train_time:37433ms step_avg:40.69ms
step:921/2035 train_time:37493ms step_avg:40.71ms
step:922/2035 train_time:37553ms step_avg:40.73ms
step:923/2035 train_time:37613ms step_avg:40.75ms
step:924/2035 train_time:37672ms step_avg:40.77ms
step:925/2035 train_time:37732ms step_avg:40.79ms
step:926/2035 train_time:37793ms step_avg:40.81ms
step:927/2035 train_time:37853ms step_avg:40.83ms
step:928/2035 train_time:37913ms step_avg:40.85ms
step:929/2035 train_time:37973ms step_avg:40.88ms
step:930/2035 train_time:38032ms step_avg:40.89ms
step:931/2035 train_time:38092ms step_avg:40.92ms
step:932/2035 train_time:38151ms step_avg:40.93ms
step:933/2035 train_time:38212ms step_avg:40.96ms
step:934/2035 train_time:38272ms step_avg:40.98ms
step:935/2035 train_time:38333ms step_avg:41.00ms
step:936/2035 train_time:38392ms step_avg:41.02ms
step:937/2035 train_time:38453ms step_avg:41.04ms
step:938/2035 train_time:38512ms step_avg:41.06ms
step:939/2035 train_time:38572ms step_avg:41.08ms
step:940/2035 train_time:38632ms step_avg:41.10ms
step:941/2035 train_time:38692ms step_avg:41.12ms
step:942/2035 train_time:38751ms step_avg:41.14ms
step:943/2035 train_time:38812ms step_avg:41.16ms
step:944/2035 train_time:38871ms step_avg:41.18ms
step:945/2035 train_time:38931ms step_avg:41.20ms
step:946/2035 train_time:38990ms step_avg:41.22ms
step:947/2035 train_time:39051ms step_avg:41.24ms
step:948/2035 train_time:39110ms step_avg:41.25ms
step:949/2035 train_time:39170ms step_avg:41.27ms
step:950/2035 train_time:39229ms step_avg:41.29ms
step:951/2035 train_time:39290ms step_avg:41.31ms
step:952/2035 train_time:39351ms step_avg:41.33ms
step:953/2035 train_time:39412ms step_avg:41.36ms
step:954/2035 train_time:39473ms step_avg:41.38ms
step:955/2035 train_time:39533ms step_avg:41.40ms
step:956/2035 train_time:39592ms step_avg:41.41ms
step:957/2035 train_time:39653ms step_avg:41.43ms
step:958/2035 train_time:39713ms step_avg:41.45ms
step:959/2035 train_time:39774ms step_avg:41.47ms
step:960/2035 train_time:39833ms step_avg:41.49ms
step:961/2035 train_time:39893ms step_avg:41.51ms
step:962/2035 train_time:39953ms step_avg:41.53ms
step:963/2035 train_time:40014ms step_avg:41.55ms
step:964/2035 train_time:40073ms step_avg:41.57ms
step:965/2035 train_time:40133ms step_avg:41.59ms
step:966/2035 train_time:40193ms step_avg:41.61ms
step:967/2035 train_time:40254ms step_avg:41.63ms
step:968/2035 train_time:40314ms step_avg:41.65ms
step:969/2035 train_time:40375ms step_avg:41.67ms
step:970/2035 train_time:40434ms step_avg:41.68ms
step:971/2035 train_time:40495ms step_avg:41.70ms
step:972/2035 train_time:40554ms step_avg:41.72ms
step:973/2035 train_time:40615ms step_avg:41.74ms
step:974/2035 train_time:40674ms step_avg:41.76ms
step:975/2035 train_time:40734ms step_avg:41.78ms
step:976/2035 train_time:40793ms step_avg:41.80ms
step:977/2035 train_time:40854ms step_avg:41.82ms
step:978/2035 train_time:40913ms step_avg:41.83ms
step:979/2035 train_time:40974ms step_avg:41.85ms
step:980/2035 train_time:41034ms step_avg:41.87ms
step:981/2035 train_time:41094ms step_avg:41.89ms
step:982/2035 train_time:41153ms step_avg:41.91ms
step:983/2035 train_time:41213ms step_avg:41.93ms
step:984/2035 train_time:41272ms step_avg:41.94ms
step:985/2035 train_time:41333ms step_avg:41.96ms
step:986/2035 train_time:41392ms step_avg:41.98ms
step:987/2035 train_time:41453ms step_avg:42.00ms
step:988/2035 train_time:41513ms step_avg:42.02ms
step:989/2035 train_time:41573ms step_avg:42.04ms
step:990/2035 train_time:41632ms step_avg:42.05ms
step:991/2035 train_time:41693ms step_avg:42.07ms
step:992/2035 train_time:41752ms step_avg:42.09ms
step:993/2035 train_time:41812ms step_avg:42.11ms
step:994/2035 train_time:41872ms step_avg:42.12ms
step:995/2035 train_time:41932ms step_avg:42.14ms
step:996/2035 train_time:41992ms step_avg:42.16ms
step:997/2035 train_time:42053ms step_avg:42.18ms
step:998/2035 train_time:42112ms step_avg:42.20ms
step:999/2035 train_time:42173ms step_avg:42.22ms
step:1000/2035 train_time:42233ms step_avg:42.23ms
step:1000/2035 val_loss:3.6838 train_time:42295ms step_avg:42.30ms
step:1001/2035 train_time:42315ms step_avg:42.27ms
step:1002/2035 train_time:42355ms step_avg:42.27ms
step:1003/2035 train_time:42416ms step_avg:42.29ms
step:1004/2035 train_time:42480ms step_avg:42.31ms
step:1005/2035 train_time:42541ms step_avg:42.33ms
step:1006/2035 train_time:42601ms step_avg:42.35ms
step:1007/2035 train_time:42661ms step_avg:42.36ms
step:1008/2035 train_time:42719ms step_avg:42.38ms
step:1009/2035 train_time:42780ms step_avg:42.40ms
step:1010/2035 train_time:42839ms step_avg:42.41ms
step:1011/2035 train_time:42899ms step_avg:42.43ms
step:1012/2035 train_time:42958ms step_avg:42.45ms
step:1013/2035 train_time:43018ms step_avg:42.47ms
step:1014/2035 train_time:43077ms step_avg:42.48ms
step:1015/2035 train_time:43137ms step_avg:42.50ms
step:1016/2035 train_time:43196ms step_avg:42.52ms
step:1017/2035 train_time:43258ms step_avg:42.53ms
step:1018/2035 train_time:43318ms step_avg:42.55ms
step:1019/2035 train_time:43380ms step_avg:42.57ms
step:1020/2035 train_time:43441ms step_avg:42.59ms
step:1021/2035 train_time:43502ms step_avg:42.61ms
step:1022/2035 train_time:43562ms step_avg:42.62ms
step:1023/2035 train_time:43623ms step_avg:42.64ms
step:1024/2035 train_time:43683ms step_avg:42.66ms
step:1025/2035 train_time:43743ms step_avg:42.68ms
step:1026/2035 train_time:43802ms step_avg:42.69ms
step:1027/2035 train_time:43863ms step_avg:42.71ms
step:1028/2035 train_time:43922ms step_avg:42.73ms
step:1029/2035 train_time:43983ms step_avg:42.74ms
step:1030/2035 train_time:44041ms step_avg:42.76ms
step:1031/2035 train_time:44102ms step_avg:42.78ms
step:1032/2035 train_time:44161ms step_avg:42.79ms
step:1033/2035 train_time:44222ms step_avg:42.81ms
step:1034/2035 train_time:44282ms step_avg:42.83ms
step:1035/2035 train_time:44344ms step_avg:42.84ms
step:1036/2035 train_time:44403ms step_avg:42.86ms
step:1037/2035 train_time:44465ms step_avg:42.88ms
step:1038/2035 train_time:44525ms step_avg:42.89ms
step:1039/2035 train_time:44585ms step_avg:42.91ms
step:1040/2035 train_time:44644ms step_avg:42.93ms
step:1041/2035 train_time:44705ms step_avg:42.94ms
step:1042/2035 train_time:44765ms step_avg:42.96ms
step:1043/2035 train_time:44826ms step_avg:42.98ms
step:1044/2035 train_time:44885ms step_avg:42.99ms
step:1045/2035 train_time:44946ms step_avg:43.01ms
step:1046/2035 train_time:45005ms step_avg:43.03ms
step:1047/2035 train_time:45066ms step_avg:43.04ms
step:1048/2035 train_time:45125ms step_avg:43.06ms
step:1049/2035 train_time:45186ms step_avg:43.08ms
step:1050/2035 train_time:45246ms step_avg:43.09ms
step:1051/2035 train_time:45307ms step_avg:43.11ms
step:1052/2035 train_time:45367ms step_avg:43.12ms
step:1053/2035 train_time:45427ms step_avg:43.14ms
step:1054/2035 train_time:45487ms step_avg:43.16ms
step:1055/2035 train_time:45548ms step_avg:43.17ms
step:1056/2035 train_time:45607ms step_avg:43.19ms
step:1057/2035 train_time:45668ms step_avg:43.21ms
step:1058/2035 train_time:45727ms step_avg:43.22ms
step:1059/2035 train_time:45788ms step_avg:43.24ms
step:1060/2035 train_time:45847ms step_avg:43.25ms
step:1061/2035 train_time:45908ms step_avg:43.27ms
step:1062/2035 train_time:45967ms step_avg:43.28ms
step:1063/2035 train_time:46028ms step_avg:43.30ms
step:1064/2035 train_time:46086ms step_avg:43.31ms
step:1065/2035 train_time:46147ms step_avg:43.33ms
step:1066/2035 train_time:46206ms step_avg:43.35ms
step:1067/2035 train_time:46268ms step_avg:43.36ms
step:1068/2035 train_time:46327ms step_avg:43.38ms
step:1069/2035 train_time:46388ms step_avg:43.39ms
step:1070/2035 train_time:46447ms step_avg:43.41ms
step:1071/2035 train_time:46508ms step_avg:43.42ms
step:1072/2035 train_time:46568ms step_avg:43.44ms
step:1073/2035 train_time:46628ms step_avg:43.46ms
step:1074/2035 train_time:46687ms step_avg:43.47ms
step:1075/2035 train_time:46748ms step_avg:43.49ms
step:1076/2035 train_time:46807ms step_avg:43.50ms
step:1077/2035 train_time:46867ms step_avg:43.52ms
step:1078/2035 train_time:46926ms step_avg:43.53ms
step:1079/2035 train_time:46987ms step_avg:43.55ms
step:1080/2035 train_time:47046ms step_avg:43.56ms
step:1081/2035 train_time:47106ms step_avg:43.58ms
step:1082/2035 train_time:47165ms step_avg:43.59ms
step:1083/2035 train_time:47225ms step_avg:43.61ms
step:1084/2035 train_time:47285ms step_avg:43.62ms
step:1085/2035 train_time:47347ms step_avg:43.64ms
step:1086/2035 train_time:47406ms step_avg:43.65ms
step:1087/2035 train_time:47467ms step_avg:43.67ms
step:1088/2035 train_time:47526ms step_avg:43.68ms
step:1089/2035 train_time:47587ms step_avg:43.70ms
step:1090/2035 train_time:47646ms step_avg:43.71ms
step:1091/2035 train_time:47707ms step_avg:43.73ms
step:1092/2035 train_time:47767ms step_avg:43.74ms
step:1093/2035 train_time:47828ms step_avg:43.76ms
step:1094/2035 train_time:47887ms step_avg:43.77ms
step:1095/2035 train_time:47947ms step_avg:43.79ms
step:1096/2035 train_time:48006ms step_avg:43.80ms
step:1097/2035 train_time:48067ms step_avg:43.82ms
step:1098/2035 train_time:48126ms step_avg:43.83ms
step:1099/2035 train_time:48187ms step_avg:43.85ms
step:1100/2035 train_time:48247ms step_avg:43.86ms
step:1101/2035 train_time:48307ms step_avg:43.88ms
step:1102/2035 train_time:48367ms step_avg:43.89ms
step:1103/2035 train_time:48427ms step_avg:43.91ms
step:1104/2035 train_time:48487ms step_avg:43.92ms
step:1105/2035 train_time:48547ms step_avg:43.93ms
step:1106/2035 train_time:48607ms step_avg:43.95ms
step:1107/2035 train_time:48667ms step_avg:43.96ms
step:1108/2035 train_time:48726ms step_avg:43.98ms
step:1109/2035 train_time:48787ms step_avg:43.99ms
step:1110/2035 train_time:48846ms step_avg:44.01ms
step:1111/2035 train_time:48906ms step_avg:44.02ms
step:1112/2035 train_time:48966ms step_avg:44.03ms
step:1113/2035 train_time:49026ms step_avg:44.05ms
step:1114/2035 train_time:49084ms step_avg:44.06ms
step:1115/2035 train_time:49145ms step_avg:44.08ms
step:1116/2035 train_time:49204ms step_avg:44.09ms
step:1117/2035 train_time:49265ms step_avg:44.10ms
step:1118/2035 train_time:49325ms step_avg:44.12ms
step:1119/2035 train_time:49386ms step_avg:44.13ms
step:1120/2035 train_time:49445ms step_avg:44.15ms
step:1121/2035 train_time:49506ms step_avg:44.16ms
step:1122/2035 train_time:49566ms step_avg:44.18ms
step:1123/2035 train_time:49627ms step_avg:44.19ms
step:1124/2035 train_time:49686ms step_avg:44.20ms
step:1125/2035 train_time:49747ms step_avg:44.22ms
step:1126/2035 train_time:49806ms step_avg:44.23ms
step:1127/2035 train_time:49866ms step_avg:44.25ms
step:1128/2035 train_time:49926ms step_avg:44.26ms
step:1129/2035 train_time:49986ms step_avg:44.27ms
step:1130/2035 train_time:50045ms step_avg:44.29ms
step:1131/2035 train_time:50106ms step_avg:44.30ms
step:1132/2035 train_time:50165ms step_avg:44.32ms
step:1133/2035 train_time:50226ms step_avg:44.33ms
step:1134/2035 train_time:50286ms step_avg:44.34ms
step:1135/2035 train_time:50346ms step_avg:44.36ms
step:1136/2035 train_time:50406ms step_avg:44.37ms
step:1137/2035 train_time:50467ms step_avg:44.39ms
step:1138/2035 train_time:50527ms step_avg:44.40ms
step:1139/2035 train_time:50588ms step_avg:44.41ms
step:1140/2035 train_time:50647ms step_avg:44.43ms
step:1141/2035 train_time:50707ms step_avg:44.44ms
step:1142/2035 train_time:50766ms step_avg:44.45ms
step:1143/2035 train_time:50826ms step_avg:44.47ms
step:1144/2035 train_time:50885ms step_avg:44.48ms
step:1145/2035 train_time:50946ms step_avg:44.49ms
step:1146/2035 train_time:51005ms step_avg:44.51ms
step:1147/2035 train_time:51066ms step_avg:44.52ms
step:1148/2035 train_time:51126ms step_avg:44.53ms
step:1149/2035 train_time:51187ms step_avg:44.55ms
step:1150/2035 train_time:51246ms step_avg:44.56ms
step:1151/2035 train_time:51307ms step_avg:44.58ms
step:1152/2035 train_time:51367ms step_avg:44.59ms
step:1153/2035 train_time:51428ms step_avg:44.60ms
step:1154/2035 train_time:51487ms step_avg:44.62ms
step:1155/2035 train_time:51548ms step_avg:44.63ms
step:1156/2035 train_time:51607ms step_avg:44.64ms
step:1157/2035 train_time:51668ms step_avg:44.66ms
step:1158/2035 train_time:51727ms step_avg:44.67ms
step:1159/2035 train_time:51788ms step_avg:44.68ms
step:1160/2035 train_time:51847ms step_avg:44.70ms
step:1161/2035 train_time:51907ms step_avg:44.71ms
step:1162/2035 train_time:51966ms step_avg:44.72ms
step:1163/2035 train_time:52026ms step_avg:44.73ms
step:1164/2035 train_time:52086ms step_avg:44.75ms
step:1165/2035 train_time:52146ms step_avg:44.76ms
step:1166/2035 train_time:52206ms step_avg:44.77ms
step:1167/2035 train_time:52267ms step_avg:44.79ms
step:1168/2035 train_time:52327ms step_avg:44.80ms
step:1169/2035 train_time:52387ms step_avg:44.81ms
step:1170/2035 train_time:52447ms step_avg:44.83ms
step:1171/2035 train_time:52507ms step_avg:44.84ms
step:1172/2035 train_time:52567ms step_avg:44.85ms
step:1173/2035 train_time:52627ms step_avg:44.87ms
step:1174/2035 train_time:52686ms step_avg:44.88ms
step:1175/2035 train_time:52747ms step_avg:44.89ms
step:1176/2035 train_time:52806ms step_avg:44.90ms
step:1177/2035 train_time:52868ms step_avg:44.92ms
step:1178/2035 train_time:52927ms step_avg:44.93ms
step:1179/2035 train_time:52988ms step_avg:44.94ms
step:1180/2035 train_time:53047ms step_avg:44.96ms
step:1181/2035 train_time:53108ms step_avg:44.97ms
step:1182/2035 train_time:53167ms step_avg:44.98ms
step:1183/2035 train_time:53228ms step_avg:44.99ms
step:1184/2035 train_time:53287ms step_avg:45.01ms
step:1185/2035 train_time:53348ms step_avg:45.02ms
step:1186/2035 train_time:53407ms step_avg:45.03ms
step:1187/2035 train_time:53468ms step_avg:45.05ms
step:1188/2035 train_time:53528ms step_avg:45.06ms
step:1189/2035 train_time:53588ms step_avg:45.07ms
step:1190/2035 train_time:53647ms step_avg:45.08ms
step:1191/2035 train_time:53708ms step_avg:45.09ms
step:1192/2035 train_time:53767ms step_avg:45.11ms
step:1193/2035 train_time:53827ms step_avg:45.12ms
step:1194/2035 train_time:53886ms step_avg:45.13ms
step:1195/2035 train_time:53948ms step_avg:45.14ms
step:1196/2035 train_time:54007ms step_avg:45.16ms
step:1197/2035 train_time:54067ms step_avg:45.17ms
step:1198/2035 train_time:54126ms step_avg:45.18ms
step:1199/2035 train_time:54187ms step_avg:45.19ms
step:1200/2035 train_time:54246ms step_avg:45.21ms
step:1201/2035 train_time:54307ms step_avg:45.22ms
step:1202/2035 train_time:54367ms step_avg:45.23ms
step:1203/2035 train_time:54428ms step_avg:45.24ms
step:1204/2035 train_time:54487ms step_avg:45.26ms
step:1205/2035 train_time:54548ms step_avg:45.27ms
step:1206/2035 train_time:54608ms step_avg:45.28ms
step:1207/2035 train_time:54669ms step_avg:45.29ms
step:1208/2035 train_time:54728ms step_avg:45.30ms
step:1209/2035 train_time:54789ms step_avg:45.32ms
step:1210/2035 train_time:54848ms step_avg:45.33ms
step:1211/2035 train_time:54909ms step_avg:45.34ms
step:1212/2035 train_time:54969ms step_avg:45.35ms
step:1213/2035 train_time:55029ms step_avg:45.37ms
step:1214/2035 train_time:55088ms step_avg:45.38ms
step:1215/2035 train_time:55149ms step_avg:45.39ms
step:1216/2035 train_time:55208ms step_avg:45.40ms
step:1217/2035 train_time:55268ms step_avg:45.41ms
step:1218/2035 train_time:55327ms step_avg:45.42ms
step:1219/2035 train_time:55388ms step_avg:45.44ms
step:1220/2035 train_time:55447ms step_avg:45.45ms
step:1221/2035 train_time:55508ms step_avg:45.46ms
step:1222/2035 train_time:55567ms step_avg:45.47ms
step:1223/2035 train_time:55628ms step_avg:45.48ms
step:1224/2035 train_time:55687ms step_avg:45.50ms
step:1225/2035 train_time:55747ms step_avg:45.51ms
step:1226/2035 train_time:55807ms step_avg:45.52ms
step:1227/2035 train_time:55868ms step_avg:45.53ms
step:1228/2035 train_time:55927ms step_avg:45.54ms
step:1229/2035 train_time:55988ms step_avg:45.56ms
step:1230/2035 train_time:56046ms step_avg:45.57ms
step:1231/2035 train_time:56107ms step_avg:45.58ms
step:1232/2035 train_time:56166ms step_avg:45.59ms
step:1233/2035 train_time:56227ms step_avg:45.60ms
step:1234/2035 train_time:56286ms step_avg:45.61ms
step:1235/2035 train_time:56347ms step_avg:45.63ms
step:1236/2035 train_time:56406ms step_avg:45.64ms
step:1237/2035 train_time:56467ms step_avg:45.65ms
step:1238/2035 train_time:56526ms step_avg:45.66ms
step:1239/2035 train_time:56587ms step_avg:45.67ms
step:1240/2035 train_time:56647ms step_avg:45.68ms
step:1241/2035 train_time:56708ms step_avg:45.70ms
step:1242/2035 train_time:56768ms step_avg:45.71ms
step:1243/2035 train_time:56828ms step_avg:45.72ms
step:1244/2035 train_time:56888ms step_avg:45.73ms
step:1245/2035 train_time:56949ms step_avg:45.74ms
step:1246/2035 train_time:57008ms step_avg:45.75ms
step:1247/2035 train_time:57068ms step_avg:45.76ms
step:1248/2035 train_time:57127ms step_avg:45.78ms
step:1249/2035 train_time:57188ms step_avg:45.79ms
step:1250/2035 train_time:57247ms step_avg:45.80ms
step:1250/2035 val_loss:3.5695 train_time:57310ms step_avg:45.85ms
step:1251/2035 train_time:57329ms step_avg:45.83ms
step:1252/2035 train_time:57371ms step_avg:45.82ms
step:1253/2035 train_time:57433ms step_avg:45.84ms
step:1254/2035 train_time:57494ms step_avg:45.85ms
step:1255/2035 train_time:57556ms step_avg:45.86ms
step:1256/2035 train_time:57617ms step_avg:45.87ms
step:1257/2035 train_time:57676ms step_avg:45.88ms
step:1258/2035 train_time:57735ms step_avg:45.89ms
step:1259/2035 train_time:57795ms step_avg:45.91ms
step:1260/2035 train_time:57853ms step_avg:45.92ms
step:1261/2035 train_time:57914ms step_avg:45.93ms
step:1262/2035 train_time:57973ms step_avg:45.94ms
step:1263/2035 train_time:58033ms step_avg:45.95ms
step:1264/2035 train_time:58093ms step_avg:45.96ms
step:1265/2035 train_time:58153ms step_avg:45.97ms
step:1266/2035 train_time:58212ms step_avg:45.98ms
step:1267/2035 train_time:58275ms step_avg:45.99ms
step:1268/2035 train_time:58336ms step_avg:46.01ms
step:1269/2035 train_time:58398ms step_avg:46.02ms
step:1270/2035 train_time:58458ms step_avg:46.03ms
step:1271/2035 train_time:58520ms step_avg:46.04ms
step:1272/2035 train_time:58580ms step_avg:46.05ms
step:1273/2035 train_time:58641ms step_avg:46.07ms
step:1274/2035 train_time:58700ms step_avg:46.08ms
step:1275/2035 train_time:58760ms step_avg:46.09ms
step:1276/2035 train_time:58819ms step_avg:46.10ms
step:1277/2035 train_time:58879ms step_avg:46.11ms
step:1278/2035 train_time:58938ms step_avg:46.12ms
step:1279/2035 train_time:58998ms step_avg:46.13ms
step:1280/2035 train_time:59057ms step_avg:46.14ms
step:1281/2035 train_time:59118ms step_avg:46.15ms
step:1282/2035 train_time:59177ms step_avg:46.16ms
step:1283/2035 train_time:59238ms step_avg:46.17ms
step:1284/2035 train_time:59299ms step_avg:46.18ms
step:1285/2035 train_time:59360ms step_avg:46.19ms
step:1286/2035 train_time:59420ms step_avg:46.21ms
step:1287/2035 train_time:59481ms step_avg:46.22ms
step:1288/2035 train_time:59541ms step_avg:46.23ms
step:1289/2035 train_time:59602ms step_avg:46.24ms
step:1290/2035 train_time:59661ms step_avg:46.25ms
step:1291/2035 train_time:59721ms step_avg:46.26ms
step:1292/2035 train_time:59781ms step_avg:46.27ms
step:1293/2035 train_time:59841ms step_avg:46.28ms
step:1294/2035 train_time:59900ms step_avg:46.29ms
step:1295/2035 train_time:59961ms step_avg:46.30ms
step:1296/2035 train_time:60019ms step_avg:46.31ms
step:1297/2035 train_time:60080ms step_avg:46.32ms
step:1298/2035 train_time:60139ms step_avg:46.33ms
step:1299/2035 train_time:60199ms step_avg:46.34ms
step:1300/2035 train_time:60259ms step_avg:46.35ms
step:1301/2035 train_time:60321ms step_avg:46.36ms
step:1302/2035 train_time:60380ms step_avg:46.38ms
step:1303/2035 train_time:60442ms step_avg:46.39ms
step:1304/2035 train_time:60501ms step_avg:46.40ms
step:1305/2035 train_time:60562ms step_avg:46.41ms
step:1306/2035 train_time:60621ms step_avg:46.42ms
step:1307/2035 train_time:60681ms step_avg:46.43ms
step:1308/2035 train_time:60740ms step_avg:46.44ms
step:1309/2035 train_time:60801ms step_avg:46.45ms
step:1310/2035 train_time:60860ms step_avg:46.46ms
step:1311/2035 train_time:60921ms step_avg:46.47ms
step:1312/2035 train_time:60979ms step_avg:46.48ms
step:1313/2035 train_time:61040ms step_avg:46.49ms
step:1314/2035 train_time:61100ms step_avg:46.50ms
step:1315/2035 train_time:61161ms step_avg:46.51ms
step:1316/2035 train_time:61220ms step_avg:46.52ms
step:1317/2035 train_time:61281ms step_avg:46.53ms
step:1318/2035 train_time:61340ms step_avg:46.54ms
step:1319/2035 train_time:61400ms step_avg:46.55ms
step:1320/2035 train_time:61460ms step_avg:46.56ms
step:1321/2035 train_time:61520ms step_avg:46.57ms
step:1322/2035 train_time:61579ms step_avg:46.58ms
step:1323/2035 train_time:61641ms step_avg:46.59ms
step:1324/2035 train_time:61700ms step_avg:46.60ms
step:1325/2035 train_time:61760ms step_avg:46.61ms
step:1326/2035 train_time:61819ms step_avg:46.62ms
step:1327/2035 train_time:61879ms step_avg:46.63ms
step:1328/2035 train_time:61938ms step_avg:46.64ms
step:1329/2035 train_time:61998ms step_avg:46.65ms
step:1330/2035 train_time:62058ms step_avg:46.66ms
step:1331/2035 train_time:62119ms step_avg:46.67ms
step:1332/2035 train_time:62206ms step_avg:46.70ms
step:1333/2035 train_time:62295ms step_avg:46.73ms
step:1334/2035 train_time:62382ms step_avg:46.76ms
step:1335/2035 train_time:62471ms step_avg:46.79ms
step:1336/2035 train_time:62558ms step_avg:46.82ms
step:1337/2035 train_time:62645ms step_avg:46.86ms
step:1338/2035 train_time:62732ms step_avg:46.88ms
step:1339/2035 train_time:62820ms step_avg:46.92ms
step:1340/2035 train_time:62907ms step_avg:46.95ms
step:1341/2035 train_time:62996ms step_avg:46.98ms
step:1342/2035 train_time:63083ms step_avg:47.01ms
step:1343/2035 train_time:63172ms step_avg:47.04ms
step:1344/2035 train_time:63258ms step_avg:47.07ms
step:1345/2035 train_time:63346ms step_avg:47.10ms
step:1346/2035 train_time:63433ms step_avg:47.13ms
step:1347/2035 train_time:63522ms step_avg:47.16ms
step:1348/2035 train_time:63609ms step_avg:47.19ms
step:1349/2035 train_time:63697ms step_avg:47.22ms
step:1350/2035 train_time:63785ms step_avg:47.25ms
step:1351/2035 train_time:63873ms step_avg:47.28ms
step:1352/2035 train_time:63959ms step_avg:47.31ms
step:1353/2035 train_time:64047ms step_avg:47.34ms
step:1354/2035 train_time:64134ms step_avg:47.37ms
step:1355/2035 train_time:64222ms step_avg:47.40ms
step:1356/2035 train_time:64310ms step_avg:47.43ms
step:1357/2035 train_time:64398ms step_avg:47.46ms
step:1358/2035 train_time:64486ms step_avg:47.49ms
step:1359/2035 train_time:64574ms step_avg:47.52ms
step:1360/2035 train_time:64661ms step_avg:47.54ms
step:1361/2035 train_time:64749ms step_avg:47.57ms
step:1362/2035 train_time:64836ms step_avg:47.60ms
step:1363/2035 train_time:64924ms step_avg:47.63ms
step:1364/2035 train_time:65012ms step_avg:47.66ms
step:1365/2035 train_time:65099ms step_avg:47.69ms
step:1366/2035 train_time:65186ms step_avg:47.72ms
step:1367/2035 train_time:65275ms step_avg:47.75ms
step:1368/2035 train_time:65362ms step_avg:47.78ms
step:1369/2035 train_time:65452ms step_avg:47.81ms
step:1370/2035 train_time:65539ms step_avg:47.84ms
step:1371/2035 train_time:65627ms step_avg:47.87ms
step:1372/2035 train_time:65715ms step_avg:47.90ms
step:1373/2035 train_time:65803ms step_avg:47.93ms
step:1374/2035 train_time:65890ms step_avg:47.95ms
step:1375/2035 train_time:65978ms step_avg:47.98ms
step:1376/2035 train_time:66065ms step_avg:48.01ms
step:1377/2035 train_time:66154ms step_avg:48.04ms
step:1378/2035 train_time:66241ms step_avg:48.07ms
step:1379/2035 train_time:66329ms step_avg:48.10ms
step:1380/2035 train_time:66415ms step_avg:48.13ms
step:1381/2035 train_time:66503ms step_avg:48.16ms
step:1382/2035 train_time:66590ms step_avg:48.18ms
step:1383/2035 train_time:66678ms step_avg:48.21ms
step:1384/2035 train_time:66766ms step_avg:48.24ms
step:1385/2035 train_time:66854ms step_avg:48.27ms
step:1386/2035 train_time:66941ms step_avg:48.30ms
step:1387/2035 train_time:67030ms step_avg:48.33ms
step:1388/2035 train_time:67117ms step_avg:48.35ms
step:1389/2035 train_time:67205ms step_avg:48.38ms
step:1390/2035 train_time:67292ms step_avg:48.41ms
step:1391/2035 train_time:67381ms step_avg:48.44ms
step:1392/2035 train_time:67467ms step_avg:48.47ms
step:1393/2035 train_time:67556ms step_avg:48.50ms
step:1394/2035 train_time:67643ms step_avg:48.52ms
step:1395/2035 train_time:67731ms step_avg:48.55ms
step:1396/2035 train_time:67818ms step_avg:48.58ms
step:1397/2035 train_time:67906ms step_avg:48.61ms
step:1398/2035 train_time:67994ms step_avg:48.64ms
step:1399/2035 train_time:68082ms step_avg:48.66ms
step:1400/2035 train_time:68169ms step_avg:48.69ms
step:1401/2035 train_time:68257ms step_avg:48.72ms
step:1402/2035 train_time:68343ms step_avg:48.75ms
step:1403/2035 train_time:68431ms step_avg:48.77ms
step:1404/2035 train_time:68518ms step_avg:48.80ms
step:1405/2035 train_time:68607ms step_avg:48.83ms
step:1406/2035 train_time:68693ms step_avg:48.86ms
step:1407/2035 train_time:68782ms step_avg:48.89ms
step:1408/2035 train_time:68868ms step_avg:48.91ms
step:1409/2035 train_time:68956ms step_avg:48.94ms
step:1410/2035 train_time:69043ms step_avg:48.97ms
step:1411/2035 train_time:69131ms step_avg:48.99ms
step:1412/2035 train_time:69217ms step_avg:49.02ms
step:1413/2035 train_time:69306ms step_avg:49.05ms
step:1414/2035 train_time:69393ms step_avg:49.08ms
step:1415/2035 train_time:69481ms step_avg:49.10ms
step:1416/2035 train_time:69568ms step_avg:49.13ms
step:1417/2035 train_time:69656ms step_avg:49.16ms
step:1418/2035 train_time:69742ms step_avg:49.18ms
step:1419/2035 train_time:69831ms step_avg:49.21ms
step:1420/2035 train_time:69918ms step_avg:49.24ms
step:1421/2035 train_time:70005ms step_avg:49.26ms
step:1422/2035 train_time:70093ms step_avg:49.29ms
step:1423/2035 train_time:70180ms step_avg:49.32ms
step:1424/2035 train_time:70268ms step_avg:49.35ms
step:1425/2035 train_time:70355ms step_avg:49.37ms
step:1426/2035 train_time:70442ms step_avg:49.40ms
step:1427/2035 train_time:70530ms step_avg:49.43ms
step:1428/2035 train_time:70617ms step_avg:49.45ms
step:1429/2035 train_time:70705ms step_avg:49.48ms
step:1430/2035 train_time:70793ms step_avg:49.51ms
step:1431/2035 train_time:70881ms step_avg:49.53ms
step:1432/2035 train_time:70967ms step_avg:49.56ms
step:1433/2035 train_time:71056ms step_avg:49.59ms
step:1434/2035 train_time:71142ms step_avg:49.61ms
step:1435/2035 train_time:71231ms step_avg:49.64ms
step:1436/2035 train_time:71317ms step_avg:49.66ms
step:1437/2035 train_time:71405ms step_avg:49.69ms
step:1438/2035 train_time:71492ms step_avg:49.72ms
step:1439/2035 train_time:71580ms step_avg:49.74ms
step:1440/2035 train_time:71667ms step_avg:49.77ms
step:1441/2035 train_time:71755ms step_avg:49.80ms
step:1442/2035 train_time:71842ms step_avg:49.82ms
step:1443/2035 train_time:71930ms step_avg:49.85ms
step:1444/2035 train_time:72017ms step_avg:49.87ms
step:1445/2035 train_time:72105ms step_avg:49.90ms
step:1446/2035 train_time:72191ms step_avg:49.92ms
step:1447/2035 train_time:72280ms step_avg:49.95ms
step:1448/2035 train_time:72366ms step_avg:49.98ms
step:1449/2035 train_time:72455ms step_avg:50.00ms
step:1450/2035 train_time:72542ms step_avg:50.03ms
step:1451/2035 train_time:72629ms step_avg:50.05ms
step:1452/2035 train_time:72716ms step_avg:50.08ms
step:1453/2035 train_time:72803ms step_avg:50.11ms
step:1454/2035 train_time:72891ms step_avg:50.13ms
step:1455/2035 train_time:72979ms step_avg:50.16ms
step:1456/2035 train_time:73066ms step_avg:50.18ms
step:1457/2035 train_time:73154ms step_avg:50.21ms
step:1458/2035 train_time:73241ms step_avg:50.23ms
step:1459/2035 train_time:73330ms step_avg:50.26ms
step:1460/2035 train_time:73416ms step_avg:50.28ms
step:1461/2035 train_time:73504ms step_avg:50.31ms
step:1462/2035 train_time:73591ms step_avg:50.34ms
step:1463/2035 train_time:73679ms step_avg:50.36ms
step:1464/2035 train_time:73766ms step_avg:50.39ms
step:1465/2035 train_time:73854ms step_avg:50.41ms
step:1466/2035 train_time:73941ms step_avg:50.44ms
step:1467/2035 train_time:74029ms step_avg:50.46ms
step:1468/2035 train_time:74115ms step_avg:50.49ms
step:1469/2035 train_time:74204ms step_avg:50.51ms
step:1470/2035 train_time:74291ms step_avg:50.54ms
step:1471/2035 train_time:74378ms step_avg:50.56ms
step:1472/2035 train_time:74466ms step_avg:50.59ms
step:1473/2035 train_time:74554ms step_avg:50.61ms
step:1474/2035 train_time:74641ms step_avg:50.64ms
step:1475/2035 train_time:74730ms step_avg:50.66ms
step:1476/2035 train_time:74815ms step_avg:50.69ms
step:1477/2035 train_time:74904ms step_avg:50.71ms
step:1478/2035 train_time:74991ms step_avg:50.74ms
step:1479/2035 train_time:75079ms step_avg:50.76ms
step:1480/2035 train_time:75166ms step_avg:50.79ms
step:1481/2035 train_time:75255ms step_avg:50.81ms
step:1482/2035 train_time:75342ms step_avg:50.84ms
step:1483/2035 train_time:75429ms step_avg:50.86ms
step:1484/2035 train_time:75516ms step_avg:50.89ms
step:1485/2035 train_time:75604ms step_avg:50.91ms
step:1486/2035 train_time:75692ms step_avg:50.94ms
step:1487/2035 train_time:75779ms step_avg:50.96ms
step:1488/2035 train_time:75867ms step_avg:50.99ms
step:1489/2035 train_time:75955ms step_avg:51.01ms
step:1490/2035 train_time:76042ms step_avg:51.03ms
step:1491/2035 train_time:76130ms step_avg:51.06ms
step:1492/2035 train_time:76217ms step_avg:51.08ms
step:1493/2035 train_time:76305ms step_avg:51.11ms
step:1494/2035 train_time:76392ms step_avg:51.13ms
step:1495/2035 train_time:76480ms step_avg:51.16ms
step:1496/2035 train_time:76567ms step_avg:51.18ms
step:1497/2035 train_time:76655ms step_avg:51.21ms
step:1498/2035 train_time:76742ms step_avg:51.23ms
step:1499/2035 train_time:76831ms step_avg:51.25ms
step:1500/2035 train_time:76917ms step_avg:51.28ms
step:1500/2035 val_loss:3.4543 train_time:77007ms step_avg:51.34ms
step:1501/2035 train_time:77027ms step_avg:51.32ms
step:1502/2035 train_time:77095ms step_avg:51.33ms
step:1503/2035 train_time:77187ms step_avg:51.36ms
step:1504/2035 train_time:77277ms step_avg:51.38ms
step:1505/2035 train_time:77365ms step_avg:51.41ms
step:1506/2035 train_time:77452ms step_avg:51.43ms
step:1507/2035 train_time:77540ms step_avg:51.45ms
step:1508/2035 train_time:77626ms step_avg:51.48ms
step:1509/2035 train_time:77714ms step_avg:51.50ms
step:1510/2035 train_time:77801ms step_avg:51.52ms
step:1511/2035 train_time:77890ms step_avg:51.55ms
step:1512/2035 train_time:77978ms step_avg:51.57ms
step:1513/2035 train_time:78067ms step_avg:51.60ms
step:1514/2035 train_time:78155ms step_avg:51.62ms
step:1515/2035 train_time:78244ms step_avg:51.65ms
step:1516/2035 train_time:78332ms step_avg:51.67ms
step:1517/2035 train_time:78420ms step_avg:51.69ms
step:1518/2035 train_time:78506ms step_avg:51.72ms
step:1519/2035 train_time:78594ms step_avg:51.74ms
step:1520/2035 train_time:78680ms step_avg:51.76ms
step:1521/2035 train_time:78767ms step_avg:51.79ms
step:1522/2035 train_time:78853ms step_avg:51.81ms
step:1523/2035 train_time:78941ms step_avg:51.83ms
step:1524/2035 train_time:79028ms step_avg:51.86ms
step:1525/2035 train_time:79117ms step_avg:51.88ms
step:1526/2035 train_time:79204ms step_avg:51.90ms
step:1527/2035 train_time:79293ms step_avg:51.93ms
step:1528/2035 train_time:79381ms step_avg:51.95ms
step:1529/2035 train_time:79470ms step_avg:51.98ms
step:1530/2035 train_time:79557ms step_avg:52.00ms
step:1531/2035 train_time:79645ms step_avg:52.02ms
step:1532/2035 train_time:79731ms step_avg:52.04ms
step:1533/2035 train_time:79819ms step_avg:52.07ms
step:1534/2035 train_time:79905ms step_avg:52.09ms
step:1535/2035 train_time:79994ms step_avg:52.11ms
step:1536/2035 train_time:80081ms step_avg:52.14ms
step:1537/2035 train_time:80170ms step_avg:52.16ms
step:1538/2035 train_time:80259ms step_avg:52.18ms
step:1539/2035 train_time:80347ms step_avg:52.21ms
step:1540/2035 train_time:80434ms step_avg:52.23ms
step:1541/2035 train_time:80522ms step_avg:52.25ms
step:1542/2035 train_time:80609ms step_avg:52.28ms
step:1543/2035 train_time:80696ms step_avg:52.30ms
step:1544/2035 train_time:80783ms step_avg:52.32ms
step:1545/2035 train_time:80871ms step_avg:52.34ms
step:1546/2035 train_time:80958ms step_avg:52.37ms
step:1547/2035 train_time:81047ms step_avg:52.39ms
step:1548/2035 train_time:81134ms step_avg:52.41ms
step:1549/2035 train_time:81223ms step_avg:52.44ms
step:1550/2035 train_time:81310ms step_avg:52.46ms
step:1551/2035 train_time:81399ms step_avg:52.48ms
step:1552/2035 train_time:81485ms step_avg:52.50ms
step:1553/2035 train_time:81573ms step_avg:52.53ms
step:1554/2035 train_time:81661ms step_avg:52.55ms
step:1555/2035 train_time:81749ms step_avg:52.57ms
step:1556/2035 train_time:81835ms step_avg:52.59ms
step:1557/2035 train_time:81923ms step_avg:52.62ms
step:1558/2035 train_time:82010ms step_avg:52.64ms
step:1559/2035 train_time:82099ms step_avg:52.66ms
step:1560/2035 train_time:82186ms step_avg:52.68ms
step:1561/2035 train_time:82274ms step_avg:52.71ms
step:1562/2035 train_time:82361ms step_avg:52.73ms
step:1563/2035 train_time:82450ms step_avg:52.75ms
step:1564/2035 train_time:82537ms step_avg:52.77ms
step:1565/2035 train_time:82625ms step_avg:52.80ms
step:1566/2035 train_time:82711ms step_avg:52.82ms
step:1567/2035 train_time:82800ms step_avg:52.84ms
step:1568/2035 train_time:82886ms step_avg:52.86ms
step:1569/2035 train_time:82974ms step_avg:52.88ms
step:1570/2035 train_time:83062ms step_avg:52.91ms
step:1571/2035 train_time:83149ms step_avg:52.93ms
step:1572/2035 train_time:83237ms step_avg:52.95ms
step:1573/2035 train_time:83325ms step_avg:52.97ms
step:1574/2035 train_time:83413ms step_avg:52.99ms
step:1575/2035 train_time:83501ms step_avg:53.02ms
step:1576/2035 train_time:83588ms step_avg:53.04ms
step:1577/2035 train_time:83676ms step_avg:53.06ms
step:1578/2035 train_time:83763ms step_avg:53.08ms
step:1579/2035 train_time:83850ms step_avg:53.10ms
step:1580/2035 train_time:83937ms step_avg:53.12ms
step:1581/2035 train_time:84025ms step_avg:53.15ms
step:1582/2035 train_time:84113ms step_avg:53.17ms
step:1583/2035 train_time:84201ms step_avg:53.19ms
step:1584/2035 train_time:84289ms step_avg:53.21ms
step:1585/2035 train_time:84378ms step_avg:53.24ms
step:1586/2035 train_time:84465ms step_avg:53.26ms
step:1587/2035 train_time:84553ms step_avg:53.28ms
step:1588/2035 train_time:84641ms step_avg:53.30ms
step:1589/2035 train_time:84729ms step_avg:53.32ms
step:1590/2035 train_time:84816ms step_avg:53.34ms
step:1591/2035 train_time:84904ms step_avg:53.37ms
step:1592/2035 train_time:84991ms step_avg:53.39ms
step:1593/2035 train_time:85080ms step_avg:53.41ms
step:1594/2035 train_time:85167ms step_avg:53.43ms
step:1595/2035 train_time:85255ms step_avg:53.45ms
step:1596/2035 train_time:85342ms step_avg:53.47ms
step:1597/2035 train_time:85431ms step_avg:53.49ms
step:1598/2035 train_time:85518ms step_avg:53.52ms
step:1599/2035 train_time:85607ms step_avg:53.54ms
step:1600/2035 train_time:85693ms step_avg:53.56ms
step:1601/2035 train_time:85781ms step_avg:53.58ms
step:1602/2035 train_time:85869ms step_avg:53.60ms
step:1603/2035 train_time:85957ms step_avg:53.62ms
step:1604/2035 train_time:86044ms step_avg:53.64ms
step:1605/2035 train_time:86131ms step_avg:53.66ms
step:1606/2035 train_time:86219ms step_avg:53.69ms
step:1607/2035 train_time:86307ms step_avg:53.71ms
step:1608/2035 train_time:86394ms step_avg:53.73ms
step:1609/2035 train_time:86482ms step_avg:53.75ms
step:1610/2035 train_time:86569ms step_avg:53.77ms
step:1611/2035 train_time:86658ms step_avg:53.79ms
step:1612/2035 train_time:86744ms step_avg:53.81ms
step:1613/2035 train_time:86831ms step_avg:53.83ms
step:1614/2035 train_time:86919ms step_avg:53.85ms
step:1615/2035 train_time:87007ms step_avg:53.87ms
step:1616/2035 train_time:87094ms step_avg:53.89ms
step:1617/2035 train_time:87182ms step_avg:53.92ms
step:1618/2035 train_time:87269ms step_avg:53.94ms
step:1619/2035 train_time:87358ms step_avg:53.96ms
step:1620/2035 train_time:87445ms step_avg:53.98ms
step:1621/2035 train_time:87534ms step_avg:54.00ms
step:1622/2035 train_time:87621ms step_avg:54.02ms
step:1623/2035 train_time:87709ms step_avg:54.04ms
step:1624/2035 train_time:87797ms step_avg:54.06ms
step:1625/2035 train_time:87885ms step_avg:54.08ms
step:1626/2035 train_time:87974ms step_avg:54.10ms
step:1627/2035 train_time:88062ms step_avg:54.13ms
step:1628/2035 train_time:88149ms step_avg:54.15ms
step:1629/2035 train_time:88237ms step_avg:54.17ms
step:1630/2035 train_time:88324ms step_avg:54.19ms
step:1631/2035 train_time:88412ms step_avg:54.21ms
step:1632/2035 train_time:88500ms step_avg:54.23ms
step:1633/2035 train_time:88588ms step_avg:54.25ms
step:1634/2035 train_time:88675ms step_avg:54.27ms
step:1635/2035 train_time:88762ms step_avg:54.29ms
step:1636/2035 train_time:88849ms step_avg:54.31ms
step:1637/2035 train_time:88938ms step_avg:54.33ms
step:1638/2035 train_time:89025ms step_avg:54.35ms
step:1639/2035 train_time:89114ms step_avg:54.37ms
step:1640/2035 train_time:89202ms step_avg:54.39ms
step:1641/2035 train_time:89290ms step_avg:54.41ms
step:1642/2035 train_time:89377ms step_avg:54.43ms
step:1643/2035 train_time:89465ms step_avg:54.45ms
step:1644/2035 train_time:89552ms step_avg:54.47ms
step:1645/2035 train_time:89643ms step_avg:54.49ms
step:1646/2035 train_time:89730ms step_avg:54.51ms
step:1647/2035 train_time:89819ms step_avg:54.53ms
step:1648/2035 train_time:89905ms step_avg:54.55ms
step:1649/2035 train_time:89993ms step_avg:54.57ms
step:1650/2035 train_time:90079ms step_avg:54.59ms
step:1651/2035 train_time:90167ms step_avg:54.61ms
step:1652/2035 train_time:90254ms step_avg:54.63ms
step:1653/2035 train_time:90343ms step_avg:54.65ms
step:1654/2035 train_time:90430ms step_avg:54.67ms
step:1655/2035 train_time:90517ms step_avg:54.69ms
step:1656/2035 train_time:90604ms step_avg:54.71ms
step:1657/2035 train_time:90693ms step_avg:54.73ms
step:1658/2035 train_time:90779ms step_avg:54.75ms
step:1659/2035 train_time:90867ms step_avg:54.77ms
step:1660/2035 train_time:90955ms step_avg:54.79ms
step:1661/2035 train_time:91043ms step_avg:54.81ms
step:1662/2035 train_time:91131ms step_avg:54.83ms
step:1663/2035 train_time:91219ms step_avg:54.85ms
step:1664/2035 train_time:91306ms step_avg:54.87ms
step:1665/2035 train_time:91394ms step_avg:54.89ms
step:1666/2035 train_time:91480ms step_avg:54.91ms
step:1667/2035 train_time:91568ms step_avg:54.93ms
step:1668/2035 train_time:91656ms step_avg:54.95ms
step:1669/2035 train_time:91744ms step_avg:54.97ms
step:1670/2035 train_time:91831ms step_avg:54.99ms
step:1671/2035 train_time:91920ms step_avg:55.01ms
step:1672/2035 train_time:92006ms step_avg:55.03ms
step:1673/2035 train_time:92095ms step_avg:55.05ms
step:1674/2035 train_time:92182ms step_avg:55.07ms
step:1675/2035 train_time:92270ms step_avg:55.09ms
step:1676/2035 train_time:92357ms step_avg:55.11ms
step:1677/2035 train_time:92446ms step_avg:55.13ms
step:1678/2035 train_time:92532ms step_avg:55.14ms
step:1679/2035 train_time:92622ms step_avg:55.16ms
step:1680/2035 train_time:92709ms step_avg:55.18ms
step:1681/2035 train_time:92798ms step_avg:55.20ms
step:1682/2035 train_time:92884ms step_avg:55.22ms
step:1683/2035 train_time:92973ms step_avg:55.24ms
step:1684/2035 train_time:93059ms step_avg:55.26ms
step:1685/2035 train_time:93148ms step_avg:55.28ms
step:1686/2035 train_time:93234ms step_avg:55.30ms
step:1687/2035 train_time:93323ms step_avg:55.32ms
step:1688/2035 train_time:93409ms step_avg:55.34ms
step:1689/2035 train_time:93497ms step_avg:55.36ms
step:1690/2035 train_time:93585ms step_avg:55.38ms
step:1691/2035 train_time:93673ms step_avg:55.39ms
step:1692/2035 train_time:93759ms step_avg:55.41ms
step:1693/2035 train_time:93848ms step_avg:55.43ms
step:1694/2035 train_time:93935ms step_avg:55.45ms
step:1695/2035 train_time:94024ms step_avg:55.47ms
step:1696/2035 train_time:94111ms step_avg:55.49ms
step:1697/2035 train_time:94199ms step_avg:55.51ms
step:1698/2035 train_time:94285ms step_avg:55.53ms
step:1699/2035 train_time:94374ms step_avg:55.55ms
step:1700/2035 train_time:94461ms step_avg:55.57ms
step:1701/2035 train_time:94551ms step_avg:55.59ms
step:1702/2035 train_time:94638ms step_avg:55.60ms
step:1703/2035 train_time:94727ms step_avg:55.62ms
step:1704/2035 train_time:94814ms step_avg:55.64ms
step:1705/2035 train_time:94902ms step_avg:55.66ms
step:1706/2035 train_time:94989ms step_avg:55.68ms
step:1707/2035 train_time:95077ms step_avg:55.70ms
step:1708/2035 train_time:95164ms step_avg:55.72ms
step:1709/2035 train_time:95252ms step_avg:55.74ms
step:1710/2035 train_time:95339ms step_avg:55.75ms
step:1711/2035 train_time:95427ms step_avg:55.77ms
step:1712/2035 train_time:95515ms step_avg:55.79ms
step:1713/2035 train_time:95604ms step_avg:55.81ms
step:1714/2035 train_time:95690ms step_avg:55.83ms
step:1715/2035 train_time:95779ms step_avg:55.85ms
step:1716/2035 train_time:95866ms step_avg:55.87ms
step:1717/2035 train_time:95954ms step_avg:55.88ms
step:1718/2035 train_time:96041ms step_avg:55.90ms
step:1719/2035 train_time:96129ms step_avg:55.92ms
step:1720/2035 train_time:96216ms step_avg:55.94ms
step:1721/2035 train_time:96305ms step_avg:55.96ms
step:1722/2035 train_time:96392ms step_avg:55.98ms
step:1723/2035 train_time:96481ms step_avg:56.00ms
step:1724/2035 train_time:96567ms step_avg:56.01ms
step:1725/2035 train_time:96656ms step_avg:56.03ms
step:1726/2035 train_time:96743ms step_avg:56.05ms
step:1727/2035 train_time:96831ms step_avg:56.07ms
step:1728/2035 train_time:96918ms step_avg:56.09ms
step:1729/2035 train_time:97006ms step_avg:56.11ms
step:1730/2035 train_time:97093ms step_avg:56.12ms
step:1731/2035 train_time:97181ms step_avg:56.14ms
step:1732/2035 train_time:97268ms step_avg:56.16ms
step:1733/2035 train_time:97356ms step_avg:56.18ms
step:1734/2035 train_time:97443ms step_avg:56.20ms
step:1735/2035 train_time:97532ms step_avg:56.21ms
step:1736/2035 train_time:97619ms step_avg:56.23ms
step:1737/2035 train_time:97707ms step_avg:56.25ms
step:1738/2035 train_time:97794ms step_avg:56.27ms
step:1739/2035 train_time:97882ms step_avg:56.29ms
step:1740/2035 train_time:97969ms step_avg:56.30ms
step:1741/2035 train_time:98057ms step_avg:56.32ms
step:1742/2035 train_time:98144ms step_avg:56.34ms
step:1743/2035 train_time:98233ms step_avg:56.36ms
step:1744/2035 train_time:98320ms step_avg:56.38ms
step:1745/2035 train_time:98408ms step_avg:56.39ms
step:1746/2035 train_time:98495ms step_avg:56.41ms
step:1747/2035 train_time:98584ms step_avg:56.43ms
step:1748/2035 train_time:98671ms step_avg:56.45ms
step:1749/2035 train_time:98759ms step_avg:56.47ms
step:1750/2035 train_time:98846ms step_avg:56.48ms
step:1750/2035 val_loss:3.3574 train_time:98935ms step_avg:56.53ms
step:1751/2035 train_time:98955ms step_avg:56.51ms
step:1752/2035 train_time:99022ms step_avg:56.52ms
step:1753/2035 train_time:99113ms step_avg:56.54ms
step:1754/2035 train_time:99202ms step_avg:56.56ms
step:1755/2035 train_time:99289ms step_avg:56.57ms
step:1756/2035 train_time:99375ms step_avg:56.59ms
step:1757/2035 train_time:99463ms step_avg:56.61ms
step:1758/2035 train_time:99550ms step_avg:56.63ms
step:1759/2035 train_time:99637ms step_avg:56.64ms
step:1760/2035 train_time:99724ms step_avg:56.66ms
step:1761/2035 train_time:99810ms step_avg:56.68ms
step:1762/2035 train_time:99898ms step_avg:56.70ms
step:1763/2035 train_time:99987ms step_avg:56.71ms
step:1764/2035 train_time:100076ms step_avg:56.73ms
step:1765/2035 train_time:100165ms step_avg:56.75ms
step:1766/2035 train_time:100253ms step_avg:56.77ms
step:1767/2035 train_time:100341ms step_avg:56.79ms
step:1768/2035 train_time:100428ms step_avg:56.80ms
step:1769/2035 train_time:100515ms step_avg:56.82ms
step:1770/2035 train_time:100601ms step_avg:56.84ms
step:1771/2035 train_time:100690ms step_avg:56.85ms
step:1772/2035 train_time:100777ms step_avg:56.87ms
step:1773/2035 train_time:100866ms step_avg:56.89ms
step:1774/2035 train_time:100954ms step_avg:56.91ms
step:1775/2035 train_time:101042ms step_avg:56.93ms
step:1776/2035 train_time:101129ms step_avg:56.94ms
step:1777/2035 train_time:101218ms step_avg:56.96ms
step:1778/2035 train_time:101305ms step_avg:56.98ms
step:1779/2035 train_time:101393ms step_avg:56.99ms
step:1780/2035 train_time:101479ms step_avg:57.01ms
step:1781/2035 train_time:101567ms step_avg:57.03ms
step:1782/2035 train_time:101652ms step_avg:57.04ms
step:1783/2035 train_time:101740ms step_avg:57.06ms
step:1784/2035 train_time:101827ms step_avg:57.08ms
step:1785/2035 train_time:101917ms step_avg:57.10ms
step:1786/2035 train_time:102006ms step_avg:57.11ms
step:1787/2035 train_time:102095ms step_avg:57.13ms
step:1788/2035 train_time:102183ms step_avg:57.15ms
step:1789/2035 train_time:102270ms step_avg:57.17ms
step:1790/2035 train_time:102357ms step_avg:57.18ms
step:1791/2035 train_time:102444ms step_avg:57.20ms
step:1792/2035 train_time:102530ms step_avg:57.22ms
step:1793/2035 train_time:102617ms step_avg:57.23ms
step:1794/2035 train_time:102704ms step_avg:57.25ms
step:1795/2035 train_time:102792ms step_avg:57.27ms
step:1796/2035 train_time:102879ms step_avg:57.28ms
step:1797/2035 train_time:102969ms step_avg:57.30ms
step:1798/2035 train_time:103057ms step_avg:57.32ms
step:1799/2035 train_time:103146ms step_avg:57.34ms
step:1800/2035 train_time:103234ms step_avg:57.35ms
step:1801/2035 train_time:103321ms step_avg:57.37ms
step:1802/2035 train_time:103408ms step_avg:57.38ms
step:1803/2035 train_time:103495ms step_avg:57.40ms
step:1804/2035 train_time:103582ms step_avg:57.42ms
step:1805/2035 train_time:103670ms step_avg:57.44ms
step:1806/2035 train_time:103757ms step_avg:57.45ms
step:1807/2035 train_time:103846ms step_avg:57.47ms
step:1808/2035 train_time:103933ms step_avg:57.49ms
step:1809/2035 train_time:104021ms step_avg:57.50ms
step:1810/2035 train_time:104108ms step_avg:57.52ms
step:1811/2035 train_time:104197ms step_avg:57.54ms
step:1812/2035 train_time:104285ms step_avg:57.55ms
step:1813/2035 train_time:104373ms step_avg:57.57ms
step:1814/2035 train_time:104460ms step_avg:57.59ms
step:1815/2035 train_time:104548ms step_avg:57.60ms
step:1816/2035 train_time:104634ms step_avg:57.62ms
step:1817/2035 train_time:104723ms step_avg:57.63ms
step:1818/2035 train_time:104809ms step_avg:57.65ms
step:1819/2035 train_time:104898ms step_avg:57.67ms
step:1820/2035 train_time:104985ms step_avg:57.68ms
step:1821/2035 train_time:105074ms step_avg:57.70ms
step:1822/2035 train_time:105160ms step_avg:57.72ms
step:1823/2035 train_time:105249ms step_avg:57.73ms
step:1824/2035 train_time:105336ms step_avg:57.75ms
step:1825/2035 train_time:105425ms step_avg:57.77ms
step:1826/2035 train_time:105511ms step_avg:57.78ms
step:1827/2035 train_time:105598ms step_avg:57.80ms
step:1828/2035 train_time:105685ms step_avg:57.81ms
step:1829/2035 train_time:105774ms step_avg:57.83ms
step:1830/2035 train_time:105860ms step_avg:57.85ms
step:1831/2035 train_time:105949ms step_avg:57.86ms
step:1832/2035 train_time:106036ms step_avg:57.88ms
step:1833/2035 train_time:106125ms step_avg:57.90ms
step:1834/2035 train_time:106211ms step_avg:57.91ms
step:1835/2035 train_time:106299ms step_avg:57.93ms
step:1836/2035 train_time:106387ms step_avg:57.94ms
step:1837/2035 train_time:106474ms step_avg:57.96ms
step:1838/2035 train_time:106561ms step_avg:57.98ms
step:1839/2035 train_time:106649ms step_avg:57.99ms
step:1840/2035 train_time:106736ms step_avg:58.01ms
step:1841/2035 train_time:106824ms step_avg:58.03ms
step:1842/2035 train_time:106911ms step_avg:58.04ms
step:1843/2035 train_time:106999ms step_avg:58.06ms
step:1844/2035 train_time:107086ms step_avg:58.07ms
step:1845/2035 train_time:107174ms step_avg:58.09ms
step:1846/2035 train_time:107261ms step_avg:58.10ms
step:1847/2035 train_time:107349ms step_avg:58.12ms
step:1848/2035 train_time:107437ms step_avg:58.14ms
step:1849/2035 train_time:107526ms step_avg:58.15ms
step:1850/2035 train_time:107612ms step_avg:58.17ms
step:1851/2035 train_time:107700ms step_avg:58.18ms
step:1852/2035 train_time:107787ms step_avg:58.20ms
step:1853/2035 train_time:107875ms step_avg:58.22ms
step:1854/2035 train_time:107963ms step_avg:58.23ms
step:1855/2035 train_time:108051ms step_avg:58.25ms
step:1856/2035 train_time:108138ms step_avg:58.26ms
step:1857/2035 train_time:108225ms step_avg:58.28ms
step:1858/2035 train_time:108312ms step_avg:58.29ms
step:1859/2035 train_time:108400ms step_avg:58.31ms
step:1860/2035 train_time:108487ms step_avg:58.33ms
step:1861/2035 train_time:108575ms step_avg:58.34ms
step:1862/2035 train_time:108662ms step_avg:58.36ms
step:1863/2035 train_time:108750ms step_avg:58.37ms
step:1864/2035 train_time:108837ms step_avg:58.39ms
step:1865/2035 train_time:108926ms step_avg:58.41ms
step:1866/2035 train_time:109013ms step_avg:58.42ms
step:1867/2035 train_time:109102ms step_avg:58.44ms
step:1868/2035 train_time:109188ms step_avg:58.45ms
step:1869/2035 train_time:109277ms step_avg:58.47ms
step:1870/2035 train_time:109364ms step_avg:58.48ms
step:1871/2035 train_time:109452ms step_avg:58.50ms
step:1872/2035 train_time:109539ms step_avg:58.51ms
step:1873/2035 train_time:109626ms step_avg:58.53ms
step:1874/2035 train_time:109713ms step_avg:58.55ms
step:1875/2035 train_time:109802ms step_avg:58.56ms
step:1876/2035 train_time:109889ms step_avg:58.58ms
step:1877/2035 train_time:109976ms step_avg:58.59ms
step:1878/2035 train_time:110065ms step_avg:58.61ms
step:1879/2035 train_time:110153ms step_avg:58.62ms
step:1880/2035 train_time:110240ms step_avg:58.64ms
step:1881/2035 train_time:110328ms step_avg:58.65ms
step:1882/2035 train_time:110416ms step_avg:58.67ms
step:1883/2035 train_time:110504ms step_avg:58.69ms
step:1884/2035 train_time:110591ms step_avg:58.70ms
step:1885/2035 train_time:110679ms step_avg:58.72ms
step:1886/2035 train_time:110766ms step_avg:58.73ms
step:1887/2035 train_time:110855ms step_avg:58.75ms
step:1888/2035 train_time:110942ms step_avg:58.76ms
step:1889/2035 train_time:111030ms step_avg:58.78ms
step:1890/2035 train_time:111117ms step_avg:58.79ms
step:1891/2035 train_time:111206ms step_avg:58.81ms
step:1892/2035 train_time:111293ms step_avg:58.82ms
step:1893/2035 train_time:111381ms step_avg:58.84ms
step:1894/2035 train_time:111469ms step_avg:58.85ms
step:1895/2035 train_time:111556ms step_avg:58.87ms
step:1896/2035 train_time:111642ms step_avg:58.88ms
step:1897/2035 train_time:111730ms step_avg:58.90ms
step:1898/2035 train_time:111818ms step_avg:58.91ms
step:1899/2035 train_time:111906ms step_avg:58.93ms
step:1900/2035 train_time:111993ms step_avg:58.94ms
step:1901/2035 train_time:112082ms step_avg:58.96ms
step:1902/2035 train_time:112169ms step_avg:58.97ms
step:1903/2035 train_time:112257ms step_avg:58.99ms
step:1904/2035 train_time:112345ms step_avg:59.00ms
step:1905/2035 train_time:112432ms step_avg:59.02ms
step:1906/2035 train_time:112519ms step_avg:59.03ms
step:1907/2035 train_time:112606ms step_avg:59.05ms
step:1908/2035 train_time:112693ms step_avg:59.06ms
step:1909/2035 train_time:112781ms step_avg:59.08ms
step:1910/2035 train_time:112868ms step_avg:59.09ms
step:1911/2035 train_time:112956ms step_avg:59.11ms
step:1912/2035 train_time:113043ms step_avg:59.12ms
step:1913/2035 train_time:113132ms step_avg:59.14ms
step:1914/2035 train_time:113218ms step_avg:59.15ms
step:1915/2035 train_time:113307ms step_avg:59.17ms
step:1916/2035 train_time:113395ms step_avg:59.18ms
step:1917/2035 train_time:113482ms step_avg:59.20ms
step:1918/2035 train_time:113569ms step_avg:59.21ms
step:1919/2035 train_time:113657ms step_avg:59.23ms
step:1920/2035 train_time:113744ms step_avg:59.24ms
step:1921/2035 train_time:113832ms step_avg:59.26ms
step:1922/2035 train_time:113919ms step_avg:59.27ms
step:1923/2035 train_time:114008ms step_avg:59.29ms
step:1924/2035 train_time:114095ms step_avg:59.30ms
step:1925/2035 train_time:114184ms step_avg:59.32ms
step:1926/2035 train_time:114272ms step_avg:59.33ms
step:1927/2035 train_time:114361ms step_avg:59.35ms
step:1928/2035 train_time:114448ms step_avg:59.36ms
step:1929/2035 train_time:114536ms step_avg:59.38ms
step:1930/2035 train_time:114622ms step_avg:59.39ms
step:1931/2035 train_time:114710ms step_avg:59.40ms
step:1932/2035 train_time:114797ms step_avg:59.42ms
step:1933/2035 train_time:114885ms step_avg:59.43ms
step:1934/2035 train_time:114972ms step_avg:59.45ms
step:1935/2035 train_time:115060ms step_avg:59.46ms
step:1936/2035 train_time:115147ms step_avg:59.48ms
step:1937/2035 train_time:115235ms step_avg:59.49ms
step:1938/2035 train_time:115323ms step_avg:59.51ms
step:1939/2035 train_time:115411ms step_avg:59.52ms
step:1940/2035 train_time:115498ms step_avg:59.54ms
step:1941/2035 train_time:115586ms step_avg:59.55ms
step:1942/2035 train_time:115673ms step_avg:59.56ms
step:1943/2035 train_time:115760ms step_avg:59.58ms
step:1944/2035 train_time:115848ms step_avg:59.59ms
step:1945/2035 train_time:115936ms step_avg:59.61ms
step:1946/2035 train_time:116022ms step_avg:59.62ms
step:1947/2035 train_time:116111ms step_avg:59.64ms
step:1948/2035 train_time:116198ms step_avg:59.65ms
step:1949/2035 train_time:116287ms step_avg:59.66ms
step:1950/2035 train_time:116374ms step_avg:59.68ms
step:1951/2035 train_time:116463ms step_avg:59.69ms
step:1952/2035 train_time:116550ms step_avg:59.71ms
step:1953/2035 train_time:116637ms step_avg:59.72ms
step:1954/2035 train_time:116724ms step_avg:59.74ms
step:1955/2035 train_time:116813ms step_avg:59.75ms
step:1956/2035 train_time:116900ms step_avg:59.76ms
step:1957/2035 train_time:116989ms step_avg:59.78ms
step:1958/2035 train_time:117076ms step_avg:59.79ms
step:1959/2035 train_time:117165ms step_avg:59.81ms
step:1960/2035 train_time:117252ms step_avg:59.82ms
step:1961/2035 train_time:117340ms step_avg:59.84ms
step:1962/2035 train_time:117427ms step_avg:59.85ms
step:1963/2035 train_time:117515ms step_avg:59.86ms
step:1964/2035 train_time:117602ms step_avg:59.88ms
step:1965/2035 train_time:117689ms step_avg:59.89ms
step:1966/2035 train_time:117776ms step_avg:59.91ms
step:1967/2035 train_time:117863ms step_avg:59.92ms
step:1968/2035 train_time:117950ms step_avg:59.93ms
step:1969/2035 train_time:118039ms step_avg:59.95ms
step:1970/2035 train_time:118126ms step_avg:59.96ms
step:1971/2035 train_time:118214ms step_avg:59.98ms
step:1972/2035 train_time:118302ms step_avg:59.99ms
step:1973/2035 train_time:118390ms step_avg:60.01ms
step:1974/2035 train_time:118477ms step_avg:60.02ms
step:1975/2035 train_time:118566ms step_avg:60.03ms
step:1976/2035 train_time:118652ms step_avg:60.05ms
step:1977/2035 train_time:118740ms step_avg:60.06ms
step:1978/2035 train_time:118826ms step_avg:60.07ms
step:1979/2035 train_time:118915ms step_avg:60.09ms
step:1980/2035 train_time:119002ms step_avg:60.10ms
step:1981/2035 train_time:119091ms step_avg:60.12ms
step:1982/2035 train_time:119177ms step_avg:60.13ms
step:1983/2035 train_time:119266ms step_avg:60.14ms
step:1984/2035 train_time:119352ms step_avg:60.16ms
step:1985/2035 train_time:119441ms step_avg:60.17ms
step:1986/2035 train_time:119528ms step_avg:60.19ms
step:1987/2035 train_time:119617ms step_avg:60.20ms
step:1988/2035 train_time:119704ms step_avg:60.21ms
step:1989/2035 train_time:119792ms step_avg:60.23ms
step:1990/2035 train_time:119879ms step_avg:60.24ms
step:1991/2035 train_time:119966ms step_avg:60.25ms
step:1992/2035 train_time:120053ms step_avg:60.27ms
step:1993/2035 train_time:120141ms step_avg:60.28ms
step:1994/2035 train_time:120229ms step_avg:60.30ms
step:1995/2035 train_time:120317ms step_avg:60.31ms
step:1996/2035 train_time:120405ms step_avg:60.32ms
step:1997/2035 train_time:120494ms step_avg:60.34ms
step:1998/2035 train_time:120581ms step_avg:60.35ms
step:1999/2035 train_time:120670ms step_avg:60.36ms
step:2000/2035 train_time:120757ms step_avg:60.38ms
step:2000/2035 val_loss:3.2843 train_time:120848ms step_avg:60.42ms
step:2001/2035 train_time:120868ms step_avg:60.40ms
step:2002/2035 train_time:120937ms step_avg:60.41ms
step:2003/2035 train_time:121026ms step_avg:60.42ms
step:2004/2035 train_time:121113ms step_avg:60.44ms
step:2005/2035 train_time:121201ms step_avg:60.45ms
step:2006/2035 train_time:121288ms step_avg:60.46ms
step:2007/2035 train_time:121375ms step_avg:60.48ms
step:2008/2035 train_time:121462ms step_avg:60.49ms
step:2009/2035 train_time:121550ms step_avg:60.50ms
step:2010/2035 train_time:121638ms step_avg:60.52ms
step:2011/2035 train_time:121727ms step_avg:60.53ms
step:2012/2035 train_time:121815ms step_avg:60.54ms
step:2013/2035 train_time:121905ms step_avg:60.56ms
step:2014/2035 train_time:121993ms step_avg:60.57ms
step:2015/2035 train_time:122081ms step_avg:60.59ms
step:2016/2035 train_time:122168ms step_avg:60.60ms
step:2017/2035 train_time:122256ms step_avg:60.61ms
step:2018/2035 train_time:122342ms step_avg:60.63ms
step:2019/2035 train_time:122430ms step_avg:60.64ms
step:2020/2035 train_time:122517ms step_avg:60.65ms
step:2021/2035 train_time:122606ms step_avg:60.67ms
step:2022/2035 train_time:122693ms step_avg:60.68ms
step:2023/2035 train_time:122782ms step_avg:60.69ms
step:2024/2035 train_time:122871ms step_avg:60.71ms
step:2025/2035 train_time:122961ms step_avg:60.72ms
step:2026/2035 train_time:123049ms step_avg:60.73ms
step:2027/2035 train_time:123137ms step_avg:60.75ms
step:2028/2035 train_time:123225ms step_avg:60.76ms
step:2029/2035 train_time:123313ms step_avg:60.78ms
step:2030/2035 train_time:123400ms step_avg:60.79ms
step:2031/2035 train_time:123488ms step_avg:60.80ms
step:2032/2035 train_time:123575ms step_avg:60.81ms
step:2033/2035 train_time:123663ms step_avg:60.83ms
step:2034/2035 train_time:123750ms step_avg:60.84ms
step:2035/2035 train_time:123839ms step_avg:60.85ms
step:2035/2035 val_loss:3.2776 train_time:123929ms step_avg:60.90ms
peak memory allocated: 29634 MiB reserved: 38276 MiB

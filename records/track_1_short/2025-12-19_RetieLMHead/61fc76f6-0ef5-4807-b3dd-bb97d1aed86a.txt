import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from collections import defaultdict
from itertools import accumulate
from pathlib import Path
import gc

os.environ["PYTORCH_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64


# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng

@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor, split_baddbmm: bool = False):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    # Select batched vs unbatched
    if split_baddbmm:
        BX_matmul = torch.bmm if X.ndim > 2 else torch.mm
    else:
        aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A

        # Referencing X twice causes pytorch to make a defensive copy,
        # resulting in a cudaMemcpyAsync in baddbmm.
        # For large matrices (i.e., the mlp weights), it's faster to split
        # the operation into two kernels to avoid this.
        if split_baddbmm:
            BX_matmul(B, X, out=C)  # C = B @ X
            C.add_(X, alpha=a)      # C = C + a*X  (in-place, X only read)
        else:
            aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X

        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X


# -----------------------------------------------------------------------------
# Compiled helpers for NorMuon by @chrisjmccormick

@torch.compile(dynamic=False, fullgraph=True)
def cautious_wd_and_update_inplace(p, v, wd_tensor, lr_tensor):
    """Cautious weight decay + parameter update. wd_tensor and lr_tensor are 0-D CPU tensors."""
    mask = (v * p) >= 0
    wd_factor = wd_tensor.to(p.dtype)
    lr_factor = lr_tensor.to(p.dtype)
    p.copy_(p - (p * mask * wd_factor * lr_factor) - (v * lr_factor))


@torch.compile(dynamic=False, fullgraph=True)
def apply_normuon_variance_reduction(v_chunk, second_momentum_buffer, beta2, red_dim):
    """NorMuon variance reduction. Algebraically fuses the normalization steps to minimize memory ops."""
    v_mean = v_chunk.float().square().mean(dim=red_dim, keepdim=True)
    red_dim_size = v_chunk.size(red_dim)
    v_norm_sq = v_mean.sum(dim=(-2, -1), keepdim=True).mul_(red_dim_size)
    v_norm = v_norm_sq.sqrt_()
    second_momentum_buffer.lerp_(v_mean.to(dtype=second_momentum_buffer.dtype), 1 - beta2)
    step_size = second_momentum_buffer.clamp_min(1e-10).rsqrt_()
    scaled_sq_sum = (v_mean * red_dim_size) * step_size.float().square()
    v_norm_new = scaled_sq_sum.sum(dim=(-2, -1), keepdim=True).sqrt_()
    final_scale = step_size * (v_norm / v_norm_new.clamp_min_(1e-10))
    return v_chunk.mul_(final_scale.type_as(v_chunk))


# -----------------------------------------------------------------------------
# NorMuon optimizer

class NorMuon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).

    Differences from standard Muon:
    - Newton-Shulz is replaced with Polar Express for the orthogonalization step
    - NorMuon adds a low-rank variance estimator similar to Adafactor. https://arxiv.org/pdf/2510.05491
    - small 1D parameters handled here instead of in Adam
    - Cautious weight decay, a gated version of decoupled weight decay
    - Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as
    needed on the forward pass. This enables attn and mlp weights to be contained within the same
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn.
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, beta2=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, beta2=beta2)
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def reset(self):
        # expose a reset for clearing buffers
        for group in self.param_groups:
            group["momentum_buffer"].zero_()
            group["second_momentum_buffer"].zero_()

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per module.
        """
        groups = defaultdict(list)
        for param in params:
            groups[param.label].append(param)

        param_groups = []
        for module_name, group_params in groups.items():
            chunk_size = (len(group_params) + self.world_size - 1) // self.world_size
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))

        return param_groups

    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn
        and mlp params when a param group is split across GPUs.
        """
        module_group_order = ['smear_gate', 'attn_gate', 'attn', 'mlp']
        params_list = list(params)
        params_list.sort(key=lambda x: module_group_order.index(x.label))

        idx = 0
        group_sizes = [1, 10, 16, 16]
        assert len(params_list) == sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            chunk_size = (size + self.world_size - 1) // self.world_size
            group_params = params_list[idx: idx + size]
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))
            idx += size

        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient distributed step by @YouJiacheng, @KonstantinWilleke, @alexrgilbert,
        # @adricarda, @tuttyfrutyee, @vdlad, @ryanyang0, @vagrawal, @varunneal, @chrisjmccormick
        rank = dist.get_rank()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            stacked_grads = torch.empty(
                (padded_num_params, *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device
            )
            for i, p in enumerate(params):
                stacked_grads[i].copy_(p.grad, non_blocking=True)
            if len(params) < padded_num_params:
                stacked_grads[len(params):].zero_()

            grad_chunk = torch.empty_like(stacked_grads[:chunk_size])

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(dict(grad_chunk=grad_chunk, reduce_future=reduce_future))

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = group["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            start_idx = rank * chunk_size
            module_idx = start_idx if start_idx < len(params) else 0

            num_params = min(chunk_size, max(0, len(params) - start_idx))  # num params for this rank

            if "momentum_buffer" not in group:
                group["momentum_buffer"]  = torch.zeros_like(grad_chunk[:num_params])
            momentum_buffer = group["momentum_buffer"]
            # Apply momentum update to the persistent momentum buffer in-place
            momentum_buffer.lerp_(grad_chunk[:num_params], 1 - group["momentum"])
            updated_grads = grad_chunk[:num_params].lerp_(momentum_buffer, group["momentum"])

            grad_shape = updated_grads.shape
            if params[module_idx].label == 'attn':

                for p in params[module_idx:module_idx + num_params]:
                    assert p.label == 'attn'

                updated_grads = updated_grads.view(4 * grad_shape[0], grad_shape[1] // 4, grad_shape[2])

            ref_param = params[module_idx]
            param_shape = ref_param.shape

            # The below shape-based heuristic assumes that matrices have their input along the
            # row dimension and their output along the columns. Gates are an exception.
            is_gate = ref_param.label in ['smear_gate', 'attn_gate']

            if "second_momentum_buffer" not in group:
                if is_gate:
                    group["second_momentum_buffer"] = torch.zeros_like(updated_grads[..., :, :1])
                else:
                    group["second_momentum_buffer"] = (torch.zeros_like(updated_grads[..., :, :1])
                        if param_shape[-2] >= param_shape[-1] else torch.zeros_like(updated_grads[..., :1, :])
                    )
            second_momentum_buffer = group["second_momentum_buffer"]

            if "param_lr_cpu" not in group:
                # Define multipliers for ALL params in this group (global, not per-shard)
                lr_mults = []
                wd_mults = []
                for p in params:
                    # Increase learning rate for modules with larger inputs than outputs.
                    # This shape check also assumes rows=input, columns=output, so take care
                    # when changing memory layouts. @chrisjmccormick
                    shape = p.shape
                    if len(shape) >= 2:
                        shape_mult = max(1.0, shape[-2] / shape[-1]) ** 0.5
                    else:
                        shape_mult = 1.0
                    lr_mults.append(shape_mult * getattr(p, "lr_mul", 1.0))
                    wd_mults.append(getattr(p, "wd_mul", 1.0))
                # Define as cpu tensors to enable Inductor constant folding
                group["param_lr_cpu"] = torch.tensor(lr_mults, dtype=torch.float32, device="cpu")
                group["param_wd_cpu"] = torch.tensor(wd_mults, dtype=torch.float32, device="cpu")

            eff_lr_all = group["param_lr_cpu"] * group["lr"]
            eff_wd_all = group["param_wd_cpu"] * group["weight_decay"] * group["lr"]

            # Slice the portion corresponding to this rank's shard
            eff_lr_cpu = eff_lr_all[module_idx:module_idx + num_params]
            eff_wd_cpu = eff_wd_all[module_idx:module_idx + num_params]

            # Compute zeropower for the entire chunk in a single, batched call.
            if num_params == 0:
                v_chunk = updated_grads
            else:
                v_chunk = polar_express(updated_grads, split_baddbmm=(ref_param.label == 'mlp'))

            # Note that the head orientation in O is transposed relative to QKV, so red_dim
            # is 'incorrect' for O. However, correcting this showed no improvement. @chrisjmccormick
            red_dim = -1 if (is_gate or param_shape[-2] >= param_shape[-1]) else -2

            v_chunk = apply_normuon_variance_reduction(
                v_chunk, second_momentum_buffer, group["beta2"], red_dim
            )

            v_chunk = v_chunk.view(grad_shape)

            # # "Cautious" weight decay (https://arxiv.org/abs/2510.12402)
            updated_params = torch.empty_like(grad_chunk)
            if num_params > 0:
                # Work on a stacked copy to avoid touching original params
                param_chunk = torch.stack(params[module_idx:module_idx + num_params])

                for local_idx in range(num_params):
                    cautious_wd_and_update_inplace(
                        param_chunk[local_idx],
                        v_chunk[local_idx],
                        eff_wd_cpu[local_idx],
                        eff_lr_cpu[local_idx],
                    )
            else:
                param_chunk = torch.zeros_like(v_chunk)

            updated_params[:num_params].copy_(param_chunk)
            if num_params < chunk_size:
                updated_params[num_params:].zero_()

            stacked_params = torch.empty(
                (padded_num_params, *param_shape),
                dtype=updated_params.dtype,
                device=updated_params.device,
            )

            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_params, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        # Group by label, with explicit ordering for execution control.
        label_order = ['lm_head', 'value_embed', 'scalars']
        params_by_label = defaultdict(list)
        for p in params:
            params_by_label[getattr(p, 'label', None)].append(p)
        param_groups = []
        for label in label_order:
            if label in params_by_label:
                param_groups.append(dict(params=params_by_label[label]))
        # include any unlabeled params at the end (processed last)
        if None in params_by_label:
            param_groups.append(dict(params=params_by_label[None]))
        super().__init__(param_groups, defaults)
        # init state
        for p in params:
            chunk_size = p.size(0) // self.world_size
            exp_avg = torch.zeros_like(p[:chunk_size], dtype=torch.bfloat16, device=p[0].device)
            exp_avg_sq = torch.zeros_like(exp_avg)
            self.state[p] = dict(step=0, exp_avg=exp_avg, exp_avg_sq=exp_avg_sq)
        # DistributedAdam implementation by @vagrawal, @akash5474

        self.should_sync = False

        self._reduce_scatter_hooks = []
        self._reduce_scatter_futures = {}
        self.register_backward_hooks()

    def register_backward_hooks(self):
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                hook = param.register_post_accumulate_grad_hook(self._sync_gradient)
                self._reduce_scatter_hooks.append(hook)

    @torch.no_grad()
    def _sync_gradient(self, param):
        if not self.should_sync:
            return

        grad = param.grad
        rank_size = grad.shape[0] // self.world_size
        grad_slice = torch.empty_like(grad[:rank_size])
        self._reduce_scatter_futures[param] = (
            dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future(),
            grad_slice
        )

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        all_gather_futures: list[torch.Future] = []

        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            for param in group['params']:
                if param not in self._reduce_scatter_futures:
                    continue

                fut, g_slice = self._reduce_scatter_futures[param]
                fut.wait()

                rank_size = param.shape[0] // self.world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]

                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (bias2 ** 0.5 / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                # cautious weight decay
                mask = (update * p_slice) >= 0
                # lr as weight decay schedule
                eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                update.addcmul_(p_slice, mask, value=eff_weight_decay * lr)

                p_slice.add_(other=update, alpha=-1.0)

                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())

        self._reduce_scatter_futures.clear()
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        with torch.no_grad():
            self.weight.zero_()  # @Grad62304977 and others

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))


# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float
    key_shift: bool

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKVO weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # Simplified layout by @chrisjmccormick
        self.qkvo_w = nn.Parameter(torch.empty(self.dim * 4, self.hdim))
        # label all modules for explicit optimizer grouping
        self.qkvo_w.label = 'attn'

        with torch.no_grad():
            self.qkvo_w[:self.dim * 3].uniform_(-bound, bound)  # init QKV weights
            self.qkvo_w[self.dim * 3:].zero_()  # init O weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        self.attn_gate.weight.label = 'attn_gate'

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas, key_shift = attn_args.ve, attn_args.sa_lambdas, attn_args.key_shift
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, sa_lambdas[0] * self.qkvo_w[:self.dim * 3].type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if key_shift:
            # shift keys forward for the stationary head dims. Enables 1-layer induction.
            k[:, 1:, :, self.head_dim//4:self.head_dim//2] = k[:, :-1, :, self.head_dim//4:self.head_dim//2]
            k[:, 1:, :, self.head_dim//4+self.head_dim//2:] = k[:, :-1, :, self.head_dim//4+self.head_dim//2:]
        if ve is not None:
            v = v + ve.view_as(v) # @ KoszarskyB & @Grad62304977

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens,
                                                        max_seqlen_q=max_len, max_seqlen_k=max_len,
                                                        causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, sa_lambdas[1] * self.qkvo_w[self.dim * 3:].type_as(y))  # sa_lambdas[1] pre-multiplied to O @shenberg
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # Transposed layout to match attention weights
        self.c_fc = nn.Parameter(torch.empty(hdim, dim))
        self.c_proj = nn.Parameter(torch.empty(hdim, dim))
        # label all modules for explicit optimizer grouping
        self.c_fc.label = 'mlp'
        self.c_proj.label = 'mlp'
        self.c_proj.lr_mul = 2.

        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.T.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.6 (the 7th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx != 6 else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim)

    def forward(self, x: Tensor, attn_args: AttnArgs):
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        self.num_layers = num_layers
        vocab_size = next_multiple_of_n(vocab_size, n=128)

        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.label = 'smear_gate'

        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        for embed in self.value_embeds:
            nn.init.zeros_(embed.weight)
        for ve in self.value_embeds:
            ve.weight.label = 'value_embed'
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)

        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=100/448, w_s=1.5/448, grad_s=0.75/448)
        nn.init.normal_(self.lm_head.weight, mean=0, std=0.005)
        self.lm_head.weight.label = 'lm_head'
        # Add learnable skip connection weights for decoder layers
        pad = (-num_layers * 4 - 3) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    1.1 * torch.ones(num_layers),  # resid lambdas. 1.1 init such that layer i weight is i^(num_layers-i).
                    0 * torch.ones(num_layers),  # x0 lambdas
                    *[torch.tensor([0.5, 1.0]) for _ in range(num_layers)],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    -1.5 * torch.ones(1),  # skip_lambda -> σ(-1.5) ≈ 0.18
                    torch.ones(pad),
                ]
            )
        )

        self.scalars.label = 'scalars'
        # set learning rates
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.scalars.lr_mul = 5.0
        self.scalars.wd_mul = 0.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        # set configs
        skip_connections = []
        skip_in = [3] # long attention window on layer 3
        skip_out = [6] # no attn op on layer 6
        x_backout = None
        backout_layer = 7

        # set lambdas
        resid_lambdas = self.scalars[: 1 * self.num_layers]
        x0_lambdas = self.scalars[1 * self.num_layers: 2 * self.num_layers]
        sa_lambdas = self.scalars[2 * self.num_layers: 4 * self.num_layers].view(-1, 2)
        smear_lambda = self.scalars[4 * self.num_layers]
        backout_lambda = self.scalars[4 * self.num_layers+1]
        skip_lambda = self.scalars[4 * self.num_layers+2]

        # set block masks and key shift
        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == self.num_layers
        key_shift = [b==long_bm for b in bm_sizes] # apply key shift to long windows

        # weight-tied: use lm_head.weight for embedding lookup
        x = F.embedding(input_seq, self.lm_head.weight)
        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [ve[1], ve[2]] + [None] * (self.num_layers - 5) + [ve[0], ve[1], ve[2]]
        assert len(ve) == self.num_layers

        # smear token embed forward 1 position @classiclarryd
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        for i in range(self.num_layers):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale,
                key_shift = key_shift[i]
            )
            if i in skip_out:
                gate = torch.sigmoid(skip_lambda)  # in (0, 1)
                x = x + gate * skip_connections.pop()
            if i == 0:
                x = (resid_lambdas[0] + x0_lambdas[0]) * x
            else:
                x = resid_lambdas[i] * x + x0_lambdas[i] * x0
            x = self.blocks[i](x, attn_args)
            if i in skip_in:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 7 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()

    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        # Cast to int32 on CPU before transfer to avoid dtype conversion during .to()
        _inputs = _inputs.to(dtype=torch.int32)
        _targets = _targets.to(dtype=torch.int64)
        _cum_lengths = _cum_lengths.to(dtype=torch.int32)

        new_params = yield (
            _inputs.to(device="cuda", non_blocking=True),
            _targets.to(device="cuda", non_blocking=True),
            _cum_lengths.to(device="cuda", non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * new_grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens // new_grad_accum_steps
            max_seq_len = new_max_seq_len


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # batch sizes
    train_bs_schedule: tuple = (8 * 2048 * 8, 16 * 2048 * 8, 24 * 2048 * 8)
    train_bs_extension: int = 24 * 2048 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 1995  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: float = 0.55  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_final: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=11,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=args.val_batch_size // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.weight.data = m.weight.data.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    embed_params + scalar_params + head_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.005,
)
optimizer2 = NorMuon(hidden_matrix_params + gate_params, lr=0.023, momentum=0.95, beta2=0.95, weight_decay=1.2)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: tied to batch size schedule, with cooldown at the end.
def get_lr(step: int):
    if step > args.num_scheduled_iterations:
        return 0.1
    lr_max = 1.0
    x = step / args.num_scheduled_iterations
    if x > 1/3:
       lr_max = 1.51  # (16/8)**0.6
    if x > 2/3:
        lr_max = 1.93  # (24/8)**0.6
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = lr_max * w + (1 - w) * 0.1
        return lr
    return lr_max

def get_ws(step: int):
    # set short window size to half of long window size
    # Higher ws on "extension" steps
    if step >= args.num_scheduled_iterations:
        return args.ws_final // 2, args.ws_final
    x = step / args.num_scheduled_iterations
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]


def get_bs(step: int):
    if step >= args.num_scheduled_iterations:
        return args.train_bs_extension
    x = step / args.num_scheduled_iterations
    bs_idx = int(len(args.train_bs_schedule) * x)
    return args.train_bs_schedule[bs_idx]


def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)
        # disable sync in the next training step for the adam optimizer
        optimizers[0].should_sync = False

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 10
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_bs_schedule[0], args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
ws_schedule = list(args.ws_schedule) + [args.ws_final]
bs_schedule = list(args.train_bs_schedule) + [args.train_bs_extension]
ws_long = ws_schedule[0]
model.train()
model.yarn.reset()
assert len(ws_schedule) == len(bs_schedule), "This warmup assumes len(ws_schedule) == len(bs_schedule)"

for idx in range(len(ws_schedule)):
    send_args = None
    if idx != 0:
        new_ws_long = ws_schedule[idx]
        model.yarn.apply(ws_long, new_ws_long)
        ws_long = new_ws_long
        send_args = (bs_schedule[idx], args.train_max_seq_len, grad_accum_steps)
    for step in range(warmup_steps):
        inputs, targets, cum_seqlens = train_loader.send(send_args)
        if step % 2 == 1:
            optimizers[0].should_sync = True
        model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
        if step % 2 == 0:
            optimizers[1].step()
            optimizers[1].zero_grad(set_to_none=True)
        else:
            for opt in optimizers:
                opt.step()
            model.zero_grad(set_to_none=True)
            optimizers[0].should_sync = False

model.zero_grad(set_to_none=True)
optimizers[0].should_sync = False
model.eval()

# warm up validation too
val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
val_loss = 0
with torch.no_grad():
    for step in range(val_steps):
        inputs, targets, cum_seqlens = next(val_loader)
        ws_idx = step % len(ws_schedule)
        if ws_idx==0:
            model.yarn.reset()
            ws_long = ws_schedule[0]
        else:
            new_ws_long = ws_schedule[ws_idx]
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
        val_loss += model(inputs, targets, cum_seqlens, ws_long // 2, ws_long)

del val_loader, val_loss
model.train()
model.yarn.reset() # rotary buffer is not stored in state_dict
optimizer2.reset() # muon momentum buffers not in state dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

step_batch_size = args.train_bs_schedule[0]
train_loader = distributed_data_generator(args.train_files, step_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)

gc.collect()

training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    new_step_batch_size = get_bs(step)
    send_args = (new_step_batch_size, args.train_max_seq_len, grad_accum_steps) if new_step_batch_size != step_batch_size else None
    step_batch_size = new_step_batch_size
    for idx in range(grad_accum_steps):
        # enable gradient sync for the DistAdam optimizer on the last iteration before we step it
        if idx == grad_accum_steps - 1 and step % 2 == 1:
            optimizers[0].should_sync = True

        inputs, targets, cum_seqlens = train_loader.send(send_args)
        (model(inputs, targets, cum_seqlens, ws_short, ws_long) / grad_accum_steps).backward()
    step_optimizers(step, optimizers, model)

    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.10.12 (main, Nov  4 2025, 08:48:33) [GCC 11.4.0]
Running PyTorch 2.10.0.dev20251204+cu126 compiled for CUDA 12.6
Running Triton version 3.6.0
Sat Dec 20 00:21:12 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:19:00.0 Off |                    0 |
| N/A   28C    P0            118W /  700W |    5858MiB /  81559MiB |      1%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:3B:00.0 Off |                    0 |
| N/A   23C    P0            114W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:4C:00.0 Off |                    0 |
| N/A   27C    P0            119W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:5D:00.0 Off |                    0 |
| N/A   27C    P0            121W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:9B:00.0 Off |                    0 |
| N/A   29C    P0            122W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:BB:00.0 Off |                    0 |
| N/A   22C    P0            111W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:CB:00.0 Off |                    0 |
| N/A   28C    P0            121W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:DB:00.0 Off |                    0 |
| N/A   23C    P0            117W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A    303114      C   /home/ubuntu/.venv/bin/python3               1510MiB |
|    0   N/A  N/A    303115      C   /home/ubuntu/.venv/bin/python3                614MiB |
|    0   N/A  N/A    303116      C   /home/ubuntu/.venv/bin/python3                614MiB |
|    0   N/A  N/A    303117      C   /home/ubuntu/.venv/bin/python3                614MiB |
|    0   N/A  N/A    303118      C   /home/ubuntu/.venv/bin/python3                614MiB |
|    0   N/A  N/A    303119      C   /home/ubuntu/.venv/bin/python3                614MiB |
|    0   N/A  N/A    303120      C   /home/ubuntu/.venv/bin/python3                614MiB |
|    0   N/A  N/A    303121      C   /home/ubuntu/.venv/bin/python3                614MiB |
|    1   N/A  N/A    303115      C   /home/ubuntu/.venv/bin/python3               1510MiB |
|    2   N/A  N/A    303116      C   /home/ubuntu/.venv/bin/python3               1510MiB |
|    3   N/A  N/A    303117      C   /home/ubuntu/.venv/bin/python3               1510MiB |
|    4   N/A  N/A    303118      C   /home/ubuntu/.venv/bin/python3               1510MiB |
|    5   N/A  N/A    303119      C   /home/ubuntu/.venv/bin/python3               1510MiB |
|    6   N/A  N/A    303120      C   /home/ubuntu/.venv/bin/python3               1510MiB |
|    7   N/A  N/A    303121      C   /home/ubuntu/.venv/bin/python3               1510MiB |
+-----------------------------------------------------------------------------------------+

====================================================================================================
step:0/2035 val_loss:10.8334 train_time:0ms step_avg:0.04ms
step:1/2035 train_time:87ms step_avg:86.80ms
step:2/2035 train_time:113ms step_avg:56.44ms
step:3/2035 train_time:135ms step_avg:45.09ms
step:4/2035 train_time:159ms step_avg:39.73ms
step:5/2035 train_time:189ms step_avg:37.73ms
step:6/2035 train_time:426ms step_avg:70.97ms
step:7/2035 train_time:446ms step_avg:63.75ms
step:8/2035 train_time:469ms step_avg:58.58ms
step:9/2035 train_time:501ms step_avg:55.70ms
step:10/2035 train_time:534ms step_avg:53.40ms
step:11/2035 train_time:567ms step_avg:51.56ms
step:12/2035 train_time:600ms step_avg:50.01ms
step:13/2035 train_time:634ms step_avg:48.73ms
step:14/2035 train_time:666ms step_avg:47.60ms
step:15/2035 train_time:700ms step_avg:46.65ms
step:16/2035 train_time:733ms step_avg:45.81ms
step:17/2035 train_time:766ms step_avg:45.04ms
step:18/2035 train_time:800ms step_avg:44.42ms
step:19/2035 train_time:832ms step_avg:43.77ms
step:20/2035 train_time:865ms step_avg:43.23ms
step:21/2035 train_time:898ms step_avg:42.74ms
step:22/2035 train_time:931ms step_avg:42.30ms
step:23/2035 train_time:964ms step_avg:41.90ms
step:24/2035 train_time:997ms step_avg:41.54ms
step:25/2035 train_time:1030ms step_avg:41.20ms
step:26/2035 train_time:1063ms step_avg:40.88ms
step:27/2035 train_time:1096ms step_avg:40.61ms
step:28/2035 train_time:1130ms step_avg:40.34ms
step:29/2035 train_time:1163ms step_avg:40.09ms
step:30/2035 train_time:1196ms step_avg:39.85ms
step:31/2035 train_time:1229ms step_avg:39.64ms
step:32/2035 train_time:1262ms step_avg:39.43ms
step:33/2035 train_time:1295ms step_avg:39.26ms
step:34/2035 train_time:1328ms step_avg:39.07ms
step:35/2035 train_time:1362ms step_avg:38.92ms
step:36/2035 train_time:1395ms step_avg:38.76ms
step:37/2035 train_time:1430ms step_avg:38.64ms
step:38/2035 train_time:1463ms step_avg:38.49ms
step:39/2035 train_time:1497ms step_avg:38.38ms
step:40/2035 train_time:1530ms step_avg:38.25ms
step:41/2035 train_time:1563ms step_avg:38.12ms
step:42/2035 train_time:1596ms step_avg:38.00ms
step:43/2035 train_time:1630ms step_avg:37.91ms
step:44/2035 train_time:1663ms step_avg:37.79ms
step:45/2035 train_time:1696ms step_avg:37.70ms
step:46/2035 train_time:1730ms step_avg:37.60ms
step:47/2035 train_time:1763ms step_avg:37.51ms
step:48/2035 train_time:1796ms step_avg:37.41ms
step:49/2035 train_time:1829ms step_avg:37.33ms
step:50/2035 train_time:1862ms step_avg:37.24ms
step:51/2035 train_time:1896ms step_avg:37.17ms
step:52/2035 train_time:1929ms step_avg:37.09ms
step:53/2035 train_time:1962ms step_avg:37.02ms
step:54/2035 train_time:1995ms step_avg:36.94ms
step:55/2035 train_time:2028ms step_avg:36.87ms
step:56/2035 train_time:2061ms step_avg:36.80ms
step:57/2035 train_time:2094ms step_avg:36.74ms
step:58/2035 train_time:2127ms step_avg:36.68ms
step:59/2035 train_time:2160ms step_avg:36.61ms
step:60/2035 train_time:2193ms step_avg:36.55ms
step:61/2035 train_time:2226ms step_avg:36.49ms
step:62/2035 train_time:2259ms step_avg:36.44ms
step:63/2035 train_time:2292ms step_avg:36.39ms
step:64/2035 train_time:2326ms step_avg:36.34ms
step:65/2035 train_time:2359ms step_avg:36.29ms
step:66/2035 train_time:2392ms step_avg:36.24ms
step:67/2035 train_time:2425ms step_avg:36.20ms
step:68/2035 train_time:2458ms step_avg:36.15ms
step:69/2035 train_time:2492ms step_avg:36.11ms
step:70/2035 train_time:2524ms step_avg:36.06ms
step:71/2035 train_time:2558ms step_avg:36.02ms
step:72/2035 train_time:2591ms step_avg:35.98ms
step:73/2035 train_time:2624ms step_avg:35.95ms
step:74/2035 train_time:2657ms step_avg:35.91ms
step:75/2035 train_time:2691ms step_avg:35.88ms
step:76/2035 train_time:2724ms step_avg:35.84ms
step:77/2035 train_time:2758ms step_avg:35.81ms
step:78/2035 train_time:2790ms step_avg:35.77ms
step:79/2035 train_time:2824ms step_avg:35.75ms
step:80/2035 train_time:2857ms step_avg:35.71ms
step:81/2035 train_time:2891ms step_avg:35.69ms
step:82/2035 train_time:2924ms step_avg:35.65ms
step:83/2035 train_time:2957ms step_avg:35.63ms
step:84/2035 train_time:2990ms step_avg:35.60ms
step:85/2035 train_time:3023ms step_avg:35.56ms
step:86/2035 train_time:3056ms step_avg:35.53ms
step:87/2035 train_time:3089ms step_avg:35.50ms
step:88/2035 train_time:3122ms step_avg:35.47ms
step:89/2035 train_time:3155ms step_avg:35.45ms
step:90/2035 train_time:3189ms step_avg:35.43ms
step:91/2035 train_time:3221ms step_avg:35.40ms
step:92/2035 train_time:3254ms step_avg:35.37ms
step:93/2035 train_time:3287ms step_avg:35.35ms
step:94/2035 train_time:3320ms step_avg:35.32ms
step:95/2035 train_time:3354ms step_avg:35.31ms
step:96/2035 train_time:3387ms step_avg:35.28ms
step:97/2035 train_time:3420ms step_avg:35.26ms
step:98/2035 train_time:3453ms step_avg:35.24ms
step:99/2035 train_time:3486ms step_avg:35.22ms
step:100/2035 train_time:3519ms step_avg:35.19ms
step:101/2035 train_time:3552ms step_avg:35.17ms
step:102/2035 train_time:3585ms step_avg:35.15ms
step:103/2035 train_time:3619ms step_avg:35.13ms
step:104/2035 train_time:3651ms step_avg:35.11ms
step:105/2035 train_time:3685ms step_avg:35.10ms
step:106/2035 train_time:3718ms step_avg:35.08ms
step:107/2035 train_time:3752ms step_avg:35.06ms
step:108/2035 train_time:3784ms step_avg:35.04ms
step:109/2035 train_time:3818ms step_avg:35.03ms
step:110/2035 train_time:3851ms step_avg:35.01ms
step:111/2035 train_time:3884ms step_avg:34.99ms
step:112/2035 train_time:3917ms step_avg:34.97ms
step:113/2035 train_time:3950ms step_avg:34.96ms
step:114/2035 train_time:3983ms step_avg:34.94ms
step:115/2035 train_time:4016ms step_avg:34.92ms
step:116/2035 train_time:4050ms step_avg:34.91ms
step:117/2035 train_time:4082ms step_avg:34.89ms
step:118/2035 train_time:4116ms step_avg:34.88ms
step:119/2035 train_time:4149ms step_avg:34.86ms
step:120/2035 train_time:4182ms step_avg:34.85ms
step:121/2035 train_time:4216ms step_avg:34.84ms
step:122/2035 train_time:4249ms step_avg:34.83ms
step:123/2035 train_time:4282ms step_avg:34.81ms
step:124/2035 train_time:4315ms step_avg:34.80ms
step:125/2035 train_time:4348ms step_avg:34.79ms
step:126/2035 train_time:4381ms step_avg:34.77ms
step:127/2035 train_time:4415ms step_avg:34.76ms
step:128/2035 train_time:4448ms step_avg:34.75ms
step:129/2035 train_time:4480ms step_avg:34.73ms
step:130/2035 train_time:4514ms step_avg:34.72ms
step:131/2035 train_time:4546ms step_avg:34.70ms
step:132/2035 train_time:4579ms step_avg:34.69ms
step:133/2035 train_time:4612ms step_avg:34.68ms
step:134/2035 train_time:4645ms step_avg:34.67ms
step:135/2035 train_time:4679ms step_avg:34.66ms
step:136/2035 train_time:4712ms step_avg:34.65ms
step:137/2035 train_time:4745ms step_avg:34.63ms
step:138/2035 train_time:4778ms step_avg:34.62ms
step:139/2035 train_time:4811ms step_avg:34.61ms
step:140/2035 train_time:4844ms step_avg:34.60ms
step:141/2035 train_time:4878ms step_avg:34.59ms
step:142/2035 train_time:4911ms step_avg:34.58ms
step:143/2035 train_time:4943ms step_avg:34.57ms
step:144/2035 train_time:4976ms step_avg:34.56ms
step:145/2035 train_time:5010ms step_avg:34.55ms
step:146/2035 train_time:5042ms step_avg:34.54ms
step:147/2035 train_time:5076ms step_avg:34.53ms
step:148/2035 train_time:5109ms step_avg:34.52ms
step:149/2035 train_time:5142ms step_avg:34.51ms
step:150/2035 train_time:5174ms step_avg:34.50ms
step:151/2035 train_time:5208ms step_avg:34.49ms
step:152/2035 train_time:5240ms step_avg:34.48ms
step:153/2035 train_time:5274ms step_avg:34.47ms
step:154/2035 train_time:5307ms step_avg:34.46ms
step:155/2035 train_time:5340ms step_avg:34.45ms
step:156/2035 train_time:5373ms step_avg:34.45ms
step:157/2035 train_time:5406ms step_avg:34.43ms
step:158/2035 train_time:5439ms step_avg:34.42ms
step:159/2035 train_time:5472ms step_avg:34.42ms
step:160/2035 train_time:5505ms step_avg:34.41ms
step:161/2035 train_time:5538ms step_avg:34.40ms
step:162/2035 train_time:5571ms step_avg:34.39ms
step:163/2035 train_time:5604ms step_avg:34.38ms
step:164/2035 train_time:5637ms step_avg:34.37ms
step:165/2035 train_time:5670ms step_avg:34.36ms
step:166/2035 train_time:5702ms step_avg:34.35ms
step:167/2035 train_time:5736ms step_avg:34.34ms
step:168/2035 train_time:5768ms step_avg:34.34ms
step:169/2035 train_time:5801ms step_avg:34.33ms
step:170/2035 train_time:5834ms step_avg:34.32ms
step:171/2035 train_time:5867ms step_avg:34.31ms
step:172/2035 train_time:5900ms step_avg:34.30ms
step:173/2035 train_time:5933ms step_avg:34.30ms
step:174/2035 train_time:5966ms step_avg:34.29ms
step:175/2035 train_time:6000ms step_avg:34.28ms
step:176/2035 train_time:6033ms step_avg:34.28ms
step:177/2035 train_time:6065ms step_avg:34.27ms
step:178/2035 train_time:6098ms step_avg:34.26ms
step:179/2035 train_time:6131ms step_avg:34.25ms
step:180/2035 train_time:6164ms step_avg:34.25ms
step:181/2035 train_time:6197ms step_avg:34.24ms
step:182/2035 train_time:6230ms step_avg:34.23ms
step:183/2035 train_time:6263ms step_avg:34.22ms
step:184/2035 train_time:6296ms step_avg:34.22ms
step:185/2035 train_time:6329ms step_avg:34.21ms
step:186/2035 train_time:6362ms step_avg:34.20ms
step:187/2035 train_time:6395ms step_avg:34.20ms
step:188/2035 train_time:6428ms step_avg:34.19ms
step:189/2035 train_time:6461ms step_avg:34.19ms
step:190/2035 train_time:6494ms step_avg:34.18ms
step:191/2035 train_time:6527ms step_avg:34.17ms
step:192/2035 train_time:6560ms step_avg:34.16ms
step:193/2035 train_time:6593ms step_avg:34.16ms
step:194/2035 train_time:6626ms step_avg:34.16ms
step:195/2035 train_time:6659ms step_avg:34.15ms
step:196/2035 train_time:6692ms step_avg:34.14ms
step:197/2035 train_time:6725ms step_avg:34.14ms
step:198/2035 train_time:6758ms step_avg:34.13ms
step:199/2035 train_time:6791ms step_avg:34.13ms
step:200/2035 train_time:6824ms step_avg:34.12ms
step:201/2035 train_time:6857ms step_avg:34.11ms
step:202/2035 train_time:6890ms step_avg:34.11ms
step:203/2035 train_time:6923ms step_avg:34.10ms
step:204/2035 train_time:6956ms step_avg:34.10ms
step:205/2035 train_time:6989ms step_avg:34.09ms
step:206/2035 train_time:7022ms step_avg:34.09ms
step:207/2035 train_time:7055ms step_avg:34.08ms
step:208/2035 train_time:7088ms step_avg:34.08ms
step:209/2035 train_time:7121ms step_avg:34.07ms
step:210/2035 train_time:7154ms step_avg:34.07ms
step:211/2035 train_time:7187ms step_avg:34.06ms
step:212/2035 train_time:7219ms step_avg:34.05ms
step:213/2035 train_time:7253ms step_avg:34.05ms
step:214/2035 train_time:7286ms step_avg:34.05ms
step:215/2035 train_time:7319ms step_avg:34.04ms
step:216/2035 train_time:7352ms step_avg:34.04ms
step:217/2035 train_time:7385ms step_avg:34.03ms
step:218/2035 train_time:7418ms step_avg:34.03ms
step:219/2035 train_time:7451ms step_avg:34.02ms
step:220/2035 train_time:7484ms step_avg:34.02ms
step:221/2035 train_time:7517ms step_avg:34.01ms
step:222/2035 train_time:7550ms step_avg:34.01ms
step:223/2035 train_time:7583ms step_avg:34.01ms
step:224/2035 train_time:7616ms step_avg:34.00ms
step:225/2035 train_time:7649ms step_avg:34.00ms
step:226/2035 train_time:7682ms step_avg:33.99ms
step:227/2035 train_time:7716ms step_avg:33.99ms
step:228/2035 train_time:7748ms step_avg:33.98ms
step:229/2035 train_time:7782ms step_avg:33.98ms
step:230/2035 train_time:7815ms step_avg:33.98ms
step:231/2035 train_time:7848ms step_avg:33.97ms
step:232/2035 train_time:7881ms step_avg:33.97ms
step:233/2035 train_time:7914ms step_avg:33.97ms
step:234/2035 train_time:7947ms step_avg:33.96ms
step:235/2035 train_time:7980ms step_avg:33.96ms
step:236/2035 train_time:8013ms step_avg:33.95ms
step:237/2035 train_time:8046ms step_avg:33.95ms
step:238/2035 train_time:8078ms step_avg:33.94ms
step:239/2035 train_time:8111ms step_avg:33.94ms
step:240/2035 train_time:8144ms step_avg:33.93ms
step:241/2035 train_time:8177ms step_avg:33.93ms
step:242/2035 train_time:8210ms step_avg:33.93ms
step:243/2035 train_time:8243ms step_avg:33.92ms
step:244/2035 train_time:8276ms step_avg:33.92ms
step:245/2035 train_time:8309ms step_avg:33.91ms
step:246/2035 train_time:8342ms step_avg:33.91ms
step:247/2035 train_time:8375ms step_avg:33.91ms
step:248/2035 train_time:8408ms step_avg:33.90ms
step:249/2035 train_time:8442ms step_avg:33.90ms
step:250/2035 train_time:8475ms step_avg:33.90ms
step:250/2035 val_loss:4.2593 train_time:8510ms step_avg:34.04ms
step:251/2035 train_time:8531ms step_avg:33.99ms
step:252/2035 train_time:8552ms step_avg:33.94ms
step:253/2035 train_time:8577ms step_avg:33.90ms
step:254/2035 train_time:8610ms step_avg:33.90ms
step:255/2035 train_time:8646ms step_avg:33.91ms
step:256/2035 train_time:8679ms step_avg:33.90ms
step:257/2035 train_time:8713ms step_avg:33.90ms
step:258/2035 train_time:8746ms step_avg:33.90ms
step:259/2035 train_time:8778ms step_avg:33.89ms
step:260/2035 train_time:8811ms step_avg:33.89ms
step:261/2035 train_time:8844ms step_avg:33.89ms
step:262/2035 train_time:8877ms step_avg:33.88ms
step:263/2035 train_time:8910ms step_avg:33.88ms
step:264/2035 train_time:8943ms step_avg:33.87ms
step:265/2035 train_time:8975ms step_avg:33.87ms
step:266/2035 train_time:9008ms step_avg:33.87ms
step:267/2035 train_time:9041ms step_avg:33.86ms
step:268/2035 train_time:9074ms step_avg:33.86ms
step:269/2035 train_time:9107ms step_avg:33.85ms
step:270/2035 train_time:9140ms step_avg:33.85ms
step:271/2035 train_time:9172ms step_avg:33.85ms
step:272/2035 train_time:9205ms step_avg:33.84ms
step:273/2035 train_time:9238ms step_avg:33.84ms
step:274/2035 train_time:9271ms step_avg:33.83ms
step:275/2035 train_time:9304ms step_avg:33.83ms
step:276/2035 train_time:9336ms step_avg:33.83ms
step:277/2035 train_time:9369ms step_avg:33.82ms
step:278/2035 train_time:9402ms step_avg:33.82ms
step:279/2035 train_time:9435ms step_avg:33.82ms
step:280/2035 train_time:9468ms step_avg:33.81ms
step:281/2035 train_time:9501ms step_avg:33.81ms
step:282/2035 train_time:9534ms step_avg:33.81ms
step:283/2035 train_time:9567ms step_avg:33.81ms
step:284/2035 train_time:9600ms step_avg:33.80ms
step:285/2035 train_time:9633ms step_avg:33.80ms
step:286/2035 train_time:9666ms step_avg:33.80ms
step:287/2035 train_time:9700ms step_avg:33.80ms
step:288/2035 train_time:9733ms step_avg:33.79ms
step:289/2035 train_time:9766ms step_avg:33.79ms
step:290/2035 train_time:9799ms step_avg:33.79ms
step:291/2035 train_time:9832ms step_avg:33.79ms
step:292/2035 train_time:9864ms step_avg:33.78ms
step:293/2035 train_time:9898ms step_avg:33.78ms
step:294/2035 train_time:9930ms step_avg:33.78ms
step:295/2035 train_time:9964ms step_avg:33.78ms
step:296/2035 train_time:9997ms step_avg:33.77ms
step:297/2035 train_time:10030ms step_avg:33.77ms
step:298/2035 train_time:10063ms step_avg:33.77ms
step:299/2035 train_time:10096ms step_avg:33.76ms
step:300/2035 train_time:10128ms step_avg:33.76ms
step:301/2035 train_time:10161ms step_avg:33.76ms
step:302/2035 train_time:10194ms step_avg:33.76ms
step:303/2035 train_time:10227ms step_avg:33.75ms
step:304/2035 train_time:10260ms step_avg:33.75ms
step:305/2035 train_time:10293ms step_avg:33.75ms
step:306/2035 train_time:10326ms step_avg:33.74ms
step:307/2035 train_time:10359ms step_avg:33.74ms
step:308/2035 train_time:10391ms step_avg:33.74ms
step:309/2035 train_time:10424ms step_avg:33.74ms
step:310/2035 train_time:10457ms step_avg:33.73ms
step:311/2035 train_time:10490ms step_avg:33.73ms
step:312/2035 train_time:10523ms step_avg:33.73ms
step:313/2035 train_time:10556ms step_avg:33.73ms
step:314/2035 train_time:10589ms step_avg:33.72ms
step:315/2035 train_time:10622ms step_avg:33.72ms
step:316/2035 train_time:10655ms step_avg:33.72ms
step:317/2035 train_time:10689ms step_avg:33.72ms
step:318/2035 train_time:10722ms step_avg:33.72ms
step:319/2035 train_time:10755ms step_avg:33.72ms
step:320/2035 train_time:10788ms step_avg:33.71ms
step:321/2035 train_time:10821ms step_avg:33.71ms
step:322/2035 train_time:10854ms step_avg:33.71ms
step:323/2035 train_time:10887ms step_avg:33.71ms
step:324/2035 train_time:10920ms step_avg:33.70ms
step:325/2035 train_time:10953ms step_avg:33.70ms
step:326/2035 train_time:10986ms step_avg:33.70ms
step:327/2035 train_time:11019ms step_avg:33.70ms
step:328/2035 train_time:11052ms step_avg:33.69ms
step:329/2035 train_time:11085ms step_avg:33.69ms
step:330/2035 train_time:11118ms step_avg:33.69ms
step:331/2035 train_time:11151ms step_avg:33.69ms
step:332/2035 train_time:11184ms step_avg:33.69ms
step:333/2035 train_time:11216ms step_avg:33.68ms
step:334/2035 train_time:11249ms step_avg:33.68ms
step:335/2035 train_time:11282ms step_avg:33.68ms
step:336/2035 train_time:11315ms step_avg:33.68ms
step:337/2035 train_time:11348ms step_avg:33.67ms
step:338/2035 train_time:11381ms step_avg:33.67ms
step:339/2035 train_time:11413ms step_avg:33.67ms
step:340/2035 train_time:11446ms step_avg:33.67ms
step:341/2035 train_time:11479ms step_avg:33.66ms
step:342/2035 train_time:11512ms step_avg:33.66ms
step:343/2035 train_time:11545ms step_avg:33.66ms
step:344/2035 train_time:11578ms step_avg:33.66ms
step:345/2035 train_time:11611ms step_avg:33.66ms
step:346/2035 train_time:11644ms step_avg:33.65ms
step:347/2035 train_time:11677ms step_avg:33.65ms
step:348/2035 train_time:11710ms step_avg:33.65ms
step:349/2035 train_time:11743ms step_avg:33.65ms
step:350/2035 train_time:11776ms step_avg:33.65ms
step:351/2035 train_time:11809ms step_avg:33.64ms
step:352/2035 train_time:11842ms step_avg:33.64ms
step:353/2035 train_time:11875ms step_avg:33.64ms
step:354/2035 train_time:11908ms step_avg:33.64ms
step:355/2035 train_time:11941ms step_avg:33.64ms
step:356/2035 train_time:11974ms step_avg:33.63ms
step:357/2035 train_time:12007ms step_avg:33.63ms
step:358/2035 train_time:12040ms step_avg:33.63ms
step:359/2035 train_time:12073ms step_avg:33.63ms
step:360/2035 train_time:12106ms step_avg:33.63ms
step:361/2035 train_time:12139ms step_avg:33.63ms
step:362/2035 train_time:12172ms step_avg:33.62ms
step:363/2035 train_time:12205ms step_avg:33.62ms
step:364/2035 train_time:12238ms step_avg:33.62ms
step:365/2035 train_time:12271ms step_avg:33.62ms
step:366/2035 train_time:12304ms step_avg:33.62ms
step:367/2035 train_time:12337ms step_avg:33.61ms
step:368/2035 train_time:12370ms step_avg:33.61ms
step:369/2035 train_time:12402ms step_avg:33.61ms
step:370/2035 train_time:12435ms step_avg:33.61ms
step:371/2035 train_time:12468ms step_avg:33.61ms
step:372/2035 train_time:12501ms step_avg:33.60ms
step:373/2035 train_time:12534ms step_avg:33.60ms
step:374/2035 train_time:12567ms step_avg:33.60ms
step:375/2035 train_time:12600ms step_avg:33.60ms
step:376/2035 train_time:12633ms step_avg:33.60ms
step:377/2035 train_time:12666ms step_avg:33.60ms
step:378/2035 train_time:12699ms step_avg:33.60ms
step:379/2035 train_time:12732ms step_avg:33.59ms
step:380/2035 train_time:12765ms step_avg:33.59ms
step:381/2035 train_time:12798ms step_avg:33.59ms
step:382/2035 train_time:12830ms step_avg:33.59ms
step:383/2035 train_time:12864ms step_avg:33.59ms
step:384/2035 train_time:12897ms step_avg:33.59ms
step:385/2035 train_time:12930ms step_avg:33.58ms
step:386/2035 train_time:12963ms step_avg:33.58ms
step:387/2035 train_time:12996ms step_avg:33.58ms
step:388/2035 train_time:13029ms step_avg:33.58ms
step:389/2035 train_time:13062ms step_avg:33.58ms
step:390/2035 train_time:13095ms step_avg:33.58ms
step:391/2035 train_time:13128ms step_avg:33.57ms
step:392/2035 train_time:13160ms step_avg:33.57ms
step:393/2035 train_time:13193ms step_avg:33.57ms
step:394/2035 train_time:13226ms step_avg:33.57ms
step:395/2035 train_time:13259ms step_avg:33.57ms
step:396/2035 train_time:13292ms step_avg:33.57ms
step:397/2035 train_time:13325ms step_avg:33.56ms
step:398/2035 train_time:13357ms step_avg:33.56ms
step:399/2035 train_time:13390ms step_avg:33.56ms
step:400/2035 train_time:13423ms step_avg:33.56ms
step:401/2035 train_time:13456ms step_avg:33.56ms
step:402/2035 train_time:13489ms step_avg:33.55ms
step:403/2035 train_time:13522ms step_avg:33.55ms
step:404/2035 train_time:13555ms step_avg:33.55ms
step:405/2035 train_time:13588ms step_avg:33.55ms
step:406/2035 train_time:13621ms step_avg:33.55ms
step:407/2035 train_time:13654ms step_avg:33.55ms
step:408/2035 train_time:13687ms step_avg:33.55ms
step:409/2035 train_time:13720ms step_avg:33.54ms
step:410/2035 train_time:13752ms step_avg:33.54ms
step:411/2035 train_time:13785ms step_avg:33.54ms
step:412/2035 train_time:13818ms step_avg:33.54ms
step:413/2035 train_time:13851ms step_avg:33.54ms
step:414/2035 train_time:13884ms step_avg:33.54ms
step:415/2035 train_time:13917ms step_avg:33.53ms
step:416/2035 train_time:13950ms step_avg:33.53ms
step:417/2035 train_time:13983ms step_avg:33.53ms
step:418/2035 train_time:14016ms step_avg:33.53ms
step:419/2035 train_time:14049ms step_avg:33.53ms
step:420/2035 train_time:14082ms step_avg:33.53ms
step:421/2035 train_time:14115ms step_avg:33.53ms
step:422/2035 train_time:14147ms step_avg:33.52ms
step:423/2035 train_time:14180ms step_avg:33.52ms
step:424/2035 train_time:14213ms step_avg:33.52ms
step:425/2035 train_time:14246ms step_avg:33.52ms
step:426/2035 train_time:14279ms step_avg:33.52ms
step:427/2035 train_time:14312ms step_avg:33.52ms
step:428/2035 train_time:14345ms step_avg:33.52ms
step:429/2035 train_time:14378ms step_avg:33.51ms
step:430/2035 train_time:14411ms step_avg:33.51ms
step:431/2035 train_time:14444ms step_avg:33.51ms
step:432/2035 train_time:14477ms step_avg:33.51ms
step:433/2035 train_time:14510ms step_avg:33.51ms
step:434/2035 train_time:14543ms step_avg:33.51ms
step:435/2035 train_time:14576ms step_avg:33.51ms
step:436/2035 train_time:14608ms step_avg:33.51ms
step:437/2035 train_time:14642ms step_avg:33.50ms
step:438/2035 train_time:14674ms step_avg:33.50ms
step:439/2035 train_time:14708ms step_avg:33.50ms
step:440/2035 train_time:14741ms step_avg:33.50ms
step:441/2035 train_time:14773ms step_avg:33.50ms
step:442/2035 train_time:14806ms step_avg:33.50ms
step:443/2035 train_time:14839ms step_avg:33.50ms
step:444/2035 train_time:14872ms step_avg:33.50ms
step:445/2035 train_time:14905ms step_avg:33.49ms
step:446/2035 train_time:14938ms step_avg:33.49ms
step:447/2035 train_time:14971ms step_avg:33.49ms
step:448/2035 train_time:15004ms step_avg:33.49ms
step:449/2035 train_time:15036ms step_avg:33.49ms
step:450/2035 train_time:15069ms step_avg:33.49ms
step:451/2035 train_time:15103ms step_avg:33.49ms
step:452/2035 train_time:15135ms step_avg:33.49ms
step:453/2035 train_time:15169ms step_avg:33.48ms
step:454/2035 train_time:15202ms step_avg:33.48ms
step:455/2035 train_time:15234ms step_avg:33.48ms
step:456/2035 train_time:15267ms step_avg:33.48ms
step:457/2035 train_time:15300ms step_avg:33.48ms
step:458/2035 train_time:15333ms step_avg:33.48ms
step:459/2035 train_time:15366ms step_avg:33.48ms
step:460/2035 train_time:15399ms step_avg:33.48ms
step:461/2035 train_time:15432ms step_avg:33.47ms
step:462/2035 train_time:15465ms step_avg:33.47ms
step:463/2035 train_time:15498ms step_avg:33.47ms
step:464/2035 train_time:15531ms step_avg:33.47ms
step:465/2035 train_time:15564ms step_avg:33.47ms
step:466/2035 train_time:15597ms step_avg:33.47ms
step:467/2035 train_time:15630ms step_avg:33.47ms
step:468/2035 train_time:15663ms step_avg:33.47ms
step:469/2035 train_time:15696ms step_avg:33.47ms
step:470/2035 train_time:15729ms step_avg:33.47ms
step:471/2035 train_time:15762ms step_avg:33.47ms
step:472/2035 train_time:15795ms step_avg:33.46ms
step:473/2035 train_time:15828ms step_avg:33.46ms
step:474/2035 train_time:15861ms step_avg:33.46ms
step:475/2035 train_time:15894ms step_avg:33.46ms
step:476/2035 train_time:15927ms step_avg:33.46ms
step:477/2035 train_time:15960ms step_avg:33.46ms
step:478/2035 train_time:15993ms step_avg:33.46ms
step:479/2035 train_time:16026ms step_avg:33.46ms
step:480/2035 train_time:16059ms step_avg:33.46ms
step:481/2035 train_time:16092ms step_avg:33.46ms
step:482/2035 train_time:16125ms step_avg:33.45ms
step:483/2035 train_time:16158ms step_avg:33.45ms
step:484/2035 train_time:16191ms step_avg:33.45ms
step:485/2035 train_time:16224ms step_avg:33.45ms
step:486/2035 train_time:16257ms step_avg:33.45ms
step:487/2035 train_time:16290ms step_avg:33.45ms
step:488/2035 train_time:16323ms step_avg:33.45ms
step:489/2035 train_time:16356ms step_avg:33.45ms
step:490/2035 train_time:16389ms step_avg:33.45ms
step:491/2035 train_time:16423ms step_avg:33.45ms
step:492/2035 train_time:16456ms step_avg:33.45ms
step:493/2035 train_time:16488ms step_avg:33.45ms
step:494/2035 train_time:16521ms step_avg:33.44ms
step:495/2035 train_time:16554ms step_avg:33.44ms
step:496/2035 train_time:16587ms step_avg:33.44ms
step:497/2035 train_time:16620ms step_avg:33.44ms
step:498/2035 train_time:16653ms step_avg:33.44ms
step:499/2035 train_time:16686ms step_avg:33.44ms
step:500/2035 train_time:16719ms step_avg:33.44ms
step:500/2035 val_loss:3.9974 train_time:16754ms step_avg:33.51ms
step:501/2035 train_time:16775ms step_avg:33.48ms
step:502/2035 train_time:16797ms step_avg:33.46ms
step:503/2035 train_time:16822ms step_avg:33.44ms
step:504/2035 train_time:16854ms step_avg:33.44ms
step:505/2035 train_time:16889ms step_avg:33.44ms
step:506/2035 train_time:16922ms step_avg:33.44ms
step:507/2035 train_time:16957ms step_avg:33.44ms
step:508/2035 train_time:16989ms step_avg:33.44ms
step:509/2035 train_time:17023ms step_avg:33.44ms
step:510/2035 train_time:17055ms step_avg:33.44ms
step:511/2035 train_time:17089ms step_avg:33.44ms
step:512/2035 train_time:17122ms step_avg:33.44ms
step:513/2035 train_time:17155ms step_avg:33.44ms
step:514/2035 train_time:17187ms step_avg:33.44ms
step:515/2035 train_time:17220ms step_avg:33.44ms
step:516/2035 train_time:17253ms step_avg:33.44ms
step:517/2035 train_time:17286ms step_avg:33.43ms
step:518/2035 train_time:17319ms step_avg:33.43ms
step:519/2035 train_time:17351ms step_avg:33.43ms
step:520/2035 train_time:17384ms step_avg:33.43ms
step:521/2035 train_time:17417ms step_avg:33.43ms
step:522/2035 train_time:17450ms step_avg:33.43ms
step:523/2035 train_time:17483ms step_avg:33.43ms
step:524/2035 train_time:17516ms step_avg:33.43ms
step:525/2035 train_time:17548ms step_avg:33.43ms
step:526/2035 train_time:17581ms step_avg:33.42ms
step:527/2035 train_time:17614ms step_avg:33.42ms
step:528/2035 train_time:17647ms step_avg:33.42ms
step:529/2035 train_time:17680ms step_avg:33.42ms
step:530/2035 train_time:17713ms step_avg:33.42ms
step:531/2035 train_time:17746ms step_avg:33.42ms
step:532/2035 train_time:17779ms step_avg:33.42ms
step:533/2035 train_time:17813ms step_avg:33.42ms
step:534/2035 train_time:17845ms step_avg:33.42ms
step:535/2035 train_time:17879ms step_avg:33.42ms
step:536/2035 train_time:17912ms step_avg:33.42ms
step:537/2035 train_time:17946ms step_avg:33.42ms
step:538/2035 train_time:17979ms step_avg:33.42ms
step:539/2035 train_time:18012ms step_avg:33.42ms
step:540/2035 train_time:18045ms step_avg:33.42ms
step:541/2035 train_time:18078ms step_avg:33.42ms
step:542/2035 train_time:18112ms step_avg:33.42ms
step:543/2035 train_time:18144ms step_avg:33.42ms
step:544/2035 train_time:18178ms step_avg:33.42ms
step:545/2035 train_time:18210ms step_avg:33.41ms
step:546/2035 train_time:18243ms step_avg:33.41ms
step:547/2035 train_time:18276ms step_avg:33.41ms
step:548/2035 train_time:18309ms step_avg:33.41ms
step:549/2035 train_time:18342ms step_avg:33.41ms
step:550/2035 train_time:18375ms step_avg:33.41ms
step:551/2035 train_time:18408ms step_avg:33.41ms
step:552/2035 train_time:18441ms step_avg:33.41ms
step:553/2035 train_time:18473ms step_avg:33.41ms
step:554/2035 train_time:18506ms step_avg:33.40ms
step:555/2035 train_time:18539ms step_avg:33.40ms
step:556/2035 train_time:18572ms step_avg:33.40ms
step:557/2035 train_time:18605ms step_avg:33.40ms
step:558/2035 train_time:18638ms step_avg:33.40ms
step:559/2035 train_time:18671ms step_avg:33.40ms
step:560/2035 train_time:18704ms step_avg:33.40ms
step:561/2035 train_time:18737ms step_avg:33.40ms
step:562/2035 train_time:18770ms step_avg:33.40ms
step:563/2035 train_time:18803ms step_avg:33.40ms
step:564/2035 train_time:18836ms step_avg:33.40ms
step:565/2035 train_time:18870ms step_avg:33.40ms
step:566/2035 train_time:18903ms step_avg:33.40ms
step:567/2035 train_time:18936ms step_avg:33.40ms
step:568/2035 train_time:18969ms step_avg:33.40ms
step:569/2035 train_time:19002ms step_avg:33.40ms
step:570/2035 train_time:19035ms step_avg:33.39ms
step:571/2035 train_time:19068ms step_avg:33.39ms
step:572/2035 train_time:19101ms step_avg:33.39ms
step:573/2035 train_time:19135ms step_avg:33.39ms
step:574/2035 train_time:19167ms step_avg:33.39ms
step:575/2035 train_time:19201ms step_avg:33.39ms
step:576/2035 train_time:19234ms step_avg:33.39ms
step:577/2035 train_time:19266ms step_avg:33.39ms
step:578/2035 train_time:19299ms step_avg:33.39ms
step:579/2035 train_time:19332ms step_avg:33.39ms
step:580/2035 train_time:19365ms step_avg:33.39ms
step:581/2035 train_time:19398ms step_avg:33.39ms
step:582/2035 train_time:19431ms step_avg:33.39ms
step:583/2035 train_time:19463ms step_avg:33.38ms
step:584/2035 train_time:19496ms step_avg:33.38ms
step:585/2035 train_time:19529ms step_avg:33.38ms
step:586/2035 train_time:19562ms step_avg:33.38ms
step:587/2035 train_time:19595ms step_avg:33.38ms
step:588/2035 train_time:19628ms step_avg:33.38ms
step:589/2035 train_time:19661ms step_avg:33.38ms
step:590/2035 train_time:19694ms step_avg:33.38ms
step:591/2035 train_time:19727ms step_avg:33.38ms
step:592/2035 train_time:19760ms step_avg:33.38ms
step:593/2035 train_time:19793ms step_avg:33.38ms
step:594/2035 train_time:19826ms step_avg:33.38ms
step:595/2035 train_time:19859ms step_avg:33.38ms
step:596/2035 train_time:19892ms step_avg:33.38ms
step:597/2035 train_time:19926ms step_avg:33.38ms
step:598/2035 train_time:19958ms step_avg:33.38ms
step:599/2035 train_time:19992ms step_avg:33.38ms
step:600/2035 train_time:20025ms step_avg:33.37ms
step:601/2035 train_time:20058ms step_avg:33.37ms
step:602/2035 train_time:20091ms step_avg:33.37ms
step:603/2035 train_time:20125ms step_avg:33.37ms
step:604/2035 train_time:20157ms step_avg:33.37ms
step:605/2035 train_time:20191ms step_avg:33.37ms
step:606/2035 train_time:20224ms step_avg:33.37ms
step:607/2035 train_time:20257ms step_avg:33.37ms
step:608/2035 train_time:20289ms step_avg:33.37ms
step:609/2035 train_time:20322ms step_avg:33.37ms
step:610/2035 train_time:20355ms step_avg:33.37ms
step:611/2035 train_time:20388ms step_avg:33.37ms
step:612/2035 train_time:20421ms step_avg:33.37ms
step:613/2035 train_time:20454ms step_avg:33.37ms
step:614/2035 train_time:20487ms step_avg:33.37ms
step:615/2035 train_time:20520ms step_avg:33.37ms
step:616/2035 train_time:20553ms step_avg:33.36ms
step:617/2035 train_time:20586ms step_avg:33.36ms
step:618/2035 train_time:20618ms step_avg:33.36ms
step:619/2035 train_time:20651ms step_avg:33.36ms
step:620/2035 train_time:20684ms step_avg:33.36ms
step:621/2035 train_time:20717ms step_avg:33.36ms
step:622/2035 train_time:20751ms step_avg:33.36ms
step:623/2035 train_time:20784ms step_avg:33.36ms
step:624/2035 train_time:20817ms step_avg:33.36ms
step:625/2035 train_time:20849ms step_avg:33.36ms
step:626/2035 train_time:20882ms step_avg:33.36ms
step:627/2035 train_time:20916ms step_avg:33.36ms
step:628/2035 train_time:20949ms step_avg:33.36ms
step:629/2035 train_time:20982ms step_avg:33.36ms
step:630/2035 train_time:21015ms step_avg:33.36ms
step:631/2035 train_time:21048ms step_avg:33.36ms
step:632/2035 train_time:21081ms step_avg:33.36ms
step:633/2035 train_time:21115ms step_avg:33.36ms
step:634/2035 train_time:21148ms step_avg:33.36ms
step:635/2035 train_time:21181ms step_avg:33.36ms
step:636/2035 train_time:21213ms step_avg:33.35ms
step:637/2035 train_time:21247ms step_avg:33.35ms
step:638/2035 train_time:21280ms step_avg:33.35ms
step:639/2035 train_time:21313ms step_avg:33.35ms
step:640/2035 train_time:21345ms step_avg:33.35ms
step:641/2035 train_time:21378ms step_avg:33.35ms
step:642/2035 train_time:21412ms step_avg:33.35ms
step:643/2035 train_time:21445ms step_avg:33.35ms
step:644/2035 train_time:21478ms step_avg:33.35ms
step:645/2035 train_time:21511ms step_avg:33.35ms
step:646/2035 train_time:21543ms step_avg:33.35ms
step:647/2035 train_time:21577ms step_avg:33.35ms
step:648/2035 train_time:21609ms step_avg:33.35ms
step:649/2035 train_time:21643ms step_avg:33.35ms
step:650/2035 train_time:21676ms step_avg:33.35ms
step:651/2035 train_time:21709ms step_avg:33.35ms
step:652/2035 train_time:21741ms step_avg:33.35ms
step:653/2035 train_time:21774ms step_avg:33.35ms
step:654/2035 train_time:21808ms step_avg:33.34ms
step:655/2035 train_time:21841ms step_avg:33.34ms
step:656/2035 train_time:21874ms step_avg:33.34ms
step:657/2035 train_time:21907ms step_avg:33.34ms
step:658/2035 train_time:21939ms step_avg:33.34ms
step:659/2035 train_time:21973ms step_avg:33.34ms
step:660/2035 train_time:22006ms step_avg:33.34ms
step:661/2035 train_time:22039ms step_avg:33.34ms
step:662/2035 train_time:22072ms step_avg:33.34ms
step:663/2035 train_time:22105ms step_avg:33.34ms
step:664/2035 train_time:22138ms step_avg:33.34ms
step:665/2035 train_time:22171ms step_avg:33.34ms
step:666/2035 train_time:22205ms step_avg:33.34ms
step:667/2035 train_time:22263ms step_avg:33.38ms
step:668/2035 train_time:22322ms step_avg:33.42ms
step:669/2035 train_time:22383ms step_avg:33.46ms
step:670/2035 train_time:22442ms step_avg:33.50ms
step:671/2035 train_time:22503ms step_avg:33.54ms
step:672/2035 train_time:22562ms step_avg:33.57ms
step:673/2035 train_time:22623ms step_avg:33.62ms
step:674/2035 train_time:22682ms step_avg:33.65ms
step:675/2035 train_time:22743ms step_avg:33.69ms
step:676/2035 train_time:22802ms step_avg:33.73ms
step:677/2035 train_time:22863ms step_avg:33.77ms
step:678/2035 train_time:22923ms step_avg:33.81ms
step:679/2035 train_time:22985ms step_avg:33.85ms
step:680/2035 train_time:23044ms step_avg:33.89ms
step:681/2035 train_time:23105ms step_avg:33.93ms
step:682/2035 train_time:23165ms step_avg:33.97ms
step:683/2035 train_time:23226ms step_avg:34.01ms
step:684/2035 train_time:23284ms step_avg:34.04ms
step:685/2035 train_time:23346ms step_avg:34.08ms
step:686/2035 train_time:23405ms step_avg:34.12ms
step:687/2035 train_time:23465ms step_avg:34.16ms
step:688/2035 train_time:23525ms step_avg:34.19ms
step:689/2035 train_time:23586ms step_avg:34.23ms
step:690/2035 train_time:23646ms step_avg:34.27ms
step:691/2035 train_time:23707ms step_avg:34.31ms
step:692/2035 train_time:23766ms step_avg:34.34ms
step:693/2035 train_time:23827ms step_avg:34.38ms
step:694/2035 train_time:23887ms step_avg:34.42ms
step:695/2035 train_time:23948ms step_avg:34.46ms
step:696/2035 train_time:24007ms step_avg:34.49ms
step:697/2035 train_time:24067ms step_avg:34.53ms
step:698/2035 train_time:24126ms step_avg:34.57ms
step:699/2035 train_time:24187ms step_avg:34.60ms
step:700/2035 train_time:24246ms step_avg:34.64ms
step:701/2035 train_time:24308ms step_avg:34.68ms
step:702/2035 train_time:24367ms step_avg:34.71ms
step:703/2035 train_time:24428ms step_avg:34.75ms
step:704/2035 train_time:24486ms step_avg:34.78ms
step:705/2035 train_time:24547ms step_avg:34.82ms
step:706/2035 train_time:24607ms step_avg:34.85ms
step:707/2035 train_time:24669ms step_avg:34.89ms
step:708/2035 train_time:24729ms step_avg:34.93ms
step:709/2035 train_time:24790ms step_avg:34.96ms
step:710/2035 train_time:24849ms step_avg:35.00ms
step:711/2035 train_time:24910ms step_avg:35.03ms
step:712/2035 train_time:24969ms step_avg:35.07ms
step:713/2035 train_time:25030ms step_avg:35.10ms
step:714/2035 train_time:25089ms step_avg:35.14ms
step:715/2035 train_time:25149ms step_avg:35.17ms
step:716/2035 train_time:25208ms step_avg:35.21ms
step:717/2035 train_time:25269ms step_avg:35.24ms
step:718/2035 train_time:25329ms step_avg:35.28ms
step:719/2035 train_time:25390ms step_avg:35.31ms
step:720/2035 train_time:25449ms step_avg:35.35ms
step:721/2035 train_time:25509ms step_avg:35.38ms
step:722/2035 train_time:25569ms step_avg:35.41ms
step:723/2035 train_time:25630ms step_avg:35.45ms
step:724/2035 train_time:25689ms step_avg:35.48ms
step:725/2035 train_time:25750ms step_avg:35.52ms
step:726/2035 train_time:25810ms step_avg:35.55ms
step:727/2035 train_time:25871ms step_avg:35.59ms
step:728/2035 train_time:25930ms step_avg:35.62ms
step:729/2035 train_time:25991ms step_avg:35.65ms
step:730/2035 train_time:26050ms step_avg:35.69ms
step:731/2035 train_time:26111ms step_avg:35.72ms
step:732/2035 train_time:26170ms step_avg:35.75ms
step:733/2035 train_time:26230ms step_avg:35.78ms
step:734/2035 train_time:26290ms step_avg:35.82ms
step:735/2035 train_time:26351ms step_avg:35.85ms
step:736/2035 train_time:26410ms step_avg:35.88ms
step:737/2035 train_time:26470ms step_avg:35.92ms
step:738/2035 train_time:26529ms step_avg:35.95ms
step:739/2035 train_time:26590ms step_avg:35.98ms
step:740/2035 train_time:26649ms step_avg:36.01ms
step:741/2035 train_time:26710ms step_avg:36.05ms
step:742/2035 train_time:26769ms step_avg:36.08ms
step:743/2035 train_time:26830ms step_avg:36.11ms
step:744/2035 train_time:26889ms step_avg:36.14ms
step:745/2035 train_time:26950ms step_avg:36.17ms
step:746/2035 train_time:27009ms step_avg:36.21ms
step:747/2035 train_time:27071ms step_avg:36.24ms
step:748/2035 train_time:27129ms step_avg:36.27ms
step:749/2035 train_time:27190ms step_avg:36.30ms
step:750/2035 train_time:27249ms step_avg:36.33ms
step:750/2035 val_loss:3.8347 train_time:27312ms step_avg:36.42ms
step:751/2035 train_time:27335ms step_avg:36.40ms
step:752/2035 train_time:27371ms step_avg:36.40ms
step:753/2035 train_time:27433ms step_avg:36.43ms
step:754/2035 train_time:27494ms step_avg:36.46ms
step:755/2035 train_time:27555ms step_avg:36.50ms
step:756/2035 train_time:27614ms step_avg:36.53ms
step:757/2035 train_time:27675ms step_avg:36.56ms
step:758/2035 train_time:27733ms step_avg:36.59ms
step:759/2035 train_time:27793ms step_avg:36.62ms
step:760/2035 train_time:27852ms step_avg:36.65ms
step:761/2035 train_time:27913ms step_avg:36.68ms
step:762/2035 train_time:27972ms step_avg:36.71ms
step:763/2035 train_time:28031ms step_avg:36.74ms
step:764/2035 train_time:28090ms step_avg:36.77ms
step:765/2035 train_time:28150ms step_avg:36.80ms
step:766/2035 train_time:28210ms step_avg:36.83ms
step:767/2035 train_time:28273ms step_avg:36.86ms
step:768/2035 train_time:28333ms step_avg:36.89ms
step:769/2035 train_time:28394ms step_avg:36.92ms
step:770/2035 train_time:28455ms step_avg:36.95ms
step:771/2035 train_time:28515ms step_avg:36.99ms
step:772/2035 train_time:28575ms step_avg:37.01ms
step:773/2035 train_time:28636ms step_avg:37.05ms
step:774/2035 train_time:28695ms step_avg:37.07ms
step:775/2035 train_time:28755ms step_avg:37.10ms
step:776/2035 train_time:28814ms step_avg:37.13ms
step:777/2035 train_time:28875ms step_avg:37.16ms
step:778/2035 train_time:28934ms step_avg:37.19ms
step:779/2035 train_time:28994ms step_avg:37.22ms
step:780/2035 train_time:29053ms step_avg:37.25ms
step:781/2035 train_time:29113ms step_avg:37.28ms
step:782/2035 train_time:29172ms step_avg:37.30ms
step:783/2035 train_time:29233ms step_avg:37.33ms
step:784/2035 train_time:29293ms step_avg:37.36ms
step:785/2035 train_time:29354ms step_avg:37.39ms
step:786/2035 train_time:29414ms step_avg:37.42ms
step:787/2035 train_time:29475ms step_avg:37.45ms
step:788/2035 train_time:29536ms step_avg:37.48ms
step:789/2035 train_time:29597ms step_avg:37.51ms
step:790/2035 train_time:29656ms step_avg:37.54ms
step:791/2035 train_time:29717ms step_avg:37.57ms
step:792/2035 train_time:29777ms step_avg:37.60ms
step:793/2035 train_time:29838ms step_avg:37.63ms
step:794/2035 train_time:29897ms step_avg:37.65ms
step:795/2035 train_time:29958ms step_avg:37.68ms
step:796/2035 train_time:30017ms step_avg:37.71ms
step:797/2035 train_time:30078ms step_avg:37.74ms
step:798/2035 train_time:30137ms step_avg:37.77ms
step:799/2035 train_time:30198ms step_avg:37.79ms
step:800/2035 train_time:30257ms step_avg:37.82ms
step:801/2035 train_time:30319ms step_avg:37.85ms
step:802/2035 train_time:30379ms step_avg:37.88ms
step:803/2035 train_time:30439ms step_avg:37.91ms
step:804/2035 train_time:30499ms step_avg:37.93ms
step:805/2035 train_time:30559ms step_avg:37.96ms
step:806/2035 train_time:30618ms step_avg:37.99ms
step:807/2035 train_time:30679ms step_avg:38.02ms
step:808/2035 train_time:30737ms step_avg:38.04ms
step:809/2035 train_time:30798ms step_avg:38.07ms
step:810/2035 train_time:30857ms step_avg:38.10ms
step:811/2035 train_time:30918ms step_avg:38.12ms
step:812/2035 train_time:30977ms step_avg:38.15ms
step:813/2035 train_time:31037ms step_avg:38.18ms
step:814/2035 train_time:31096ms step_avg:38.20ms
step:815/2035 train_time:31158ms step_avg:38.23ms
step:816/2035 train_time:31217ms step_avg:38.26ms
step:817/2035 train_time:31278ms step_avg:38.28ms
step:818/2035 train_time:31338ms step_avg:38.31ms
step:819/2035 train_time:31399ms step_avg:38.34ms
step:820/2035 train_time:31459ms step_avg:38.36ms
step:821/2035 train_time:31521ms step_avg:38.39ms
step:822/2035 train_time:31580ms step_avg:38.42ms
step:823/2035 train_time:31640ms step_avg:38.45ms
step:824/2035 train_time:31700ms step_avg:38.47ms
step:825/2035 train_time:31760ms step_avg:38.50ms
step:826/2035 train_time:31819ms step_avg:38.52ms
step:827/2035 train_time:31879ms step_avg:38.55ms
step:828/2035 train_time:31938ms step_avg:38.57ms
step:829/2035 train_time:31999ms step_avg:38.60ms
step:830/2035 train_time:32058ms step_avg:38.62ms
step:831/2035 train_time:32119ms step_avg:38.65ms
step:832/2035 train_time:32178ms step_avg:38.68ms
step:833/2035 train_time:32239ms step_avg:38.70ms
step:834/2035 train_time:32298ms step_avg:38.73ms
step:835/2035 train_time:32359ms step_avg:38.75ms
step:836/2035 train_time:32418ms step_avg:38.78ms
step:837/2035 train_time:32479ms step_avg:38.80ms
step:838/2035 train_time:32538ms step_avg:38.83ms
step:839/2035 train_time:32598ms step_avg:38.85ms
step:840/2035 train_time:32658ms step_avg:38.88ms
step:841/2035 train_time:32719ms step_avg:38.90ms
step:842/2035 train_time:32778ms step_avg:38.93ms
step:843/2035 train_time:32838ms step_avg:38.95ms
step:844/2035 train_time:32897ms step_avg:38.98ms
step:845/2035 train_time:32958ms step_avg:39.00ms
step:846/2035 train_time:33017ms step_avg:39.03ms
step:847/2035 train_time:33077ms step_avg:39.05ms
step:848/2035 train_time:33136ms step_avg:39.08ms
step:849/2035 train_time:33197ms step_avg:39.10ms
step:850/2035 train_time:33256ms step_avg:39.13ms
step:851/2035 train_time:33317ms step_avg:39.15ms
step:852/2035 train_time:33377ms step_avg:39.18ms
step:853/2035 train_time:33438ms step_avg:39.20ms
step:854/2035 train_time:33497ms step_avg:39.22ms
step:855/2035 train_time:33559ms step_avg:39.25ms
step:856/2035 train_time:33618ms step_avg:39.27ms
step:857/2035 train_time:33679ms step_avg:39.30ms
step:858/2035 train_time:33738ms step_avg:39.32ms
step:859/2035 train_time:33799ms step_avg:39.35ms
step:860/2035 train_time:33859ms step_avg:39.37ms
step:861/2035 train_time:33919ms step_avg:39.40ms
step:862/2035 train_time:33978ms step_avg:39.42ms
step:863/2035 train_time:34039ms step_avg:39.44ms
step:864/2035 train_time:34098ms step_avg:39.47ms
step:865/2035 train_time:34158ms step_avg:39.49ms
step:866/2035 train_time:34217ms step_avg:39.51ms
step:867/2035 train_time:34278ms step_avg:39.54ms
step:868/2035 train_time:34337ms step_avg:39.56ms
step:869/2035 train_time:34399ms step_avg:39.58ms
step:870/2035 train_time:34459ms step_avg:39.61ms
step:871/2035 train_time:34519ms step_avg:39.63ms
step:872/2035 train_time:34578ms step_avg:39.65ms
step:873/2035 train_time:34639ms step_avg:39.68ms
step:874/2035 train_time:34698ms step_avg:39.70ms
step:875/2035 train_time:34760ms step_avg:39.73ms
step:876/2035 train_time:34820ms step_avg:39.75ms
step:877/2035 train_time:34880ms step_avg:39.77ms
step:878/2035 train_time:34939ms step_avg:39.79ms
step:879/2035 train_time:35000ms step_avg:39.82ms
step:880/2035 train_time:35059ms step_avg:39.84ms
step:881/2035 train_time:35119ms step_avg:39.86ms
step:882/2035 train_time:35179ms step_avg:39.89ms
step:883/2035 train_time:35239ms step_avg:39.91ms
step:884/2035 train_time:35298ms step_avg:39.93ms
step:885/2035 train_time:35359ms step_avg:39.95ms
step:886/2035 train_time:35418ms step_avg:39.97ms
step:887/2035 train_time:35479ms step_avg:40.00ms
step:888/2035 train_time:35538ms step_avg:40.02ms
step:889/2035 train_time:35599ms step_avg:40.04ms
step:890/2035 train_time:35658ms step_avg:40.07ms
step:891/2035 train_time:35719ms step_avg:40.09ms
step:892/2035 train_time:35778ms step_avg:40.11ms
step:893/2035 train_time:35839ms step_avg:40.13ms
step:894/2035 train_time:35898ms step_avg:40.15ms
step:895/2035 train_time:35958ms step_avg:40.18ms
step:896/2035 train_time:36017ms step_avg:40.20ms
step:897/2035 train_time:36078ms step_avg:40.22ms
step:898/2035 train_time:36137ms step_avg:40.24ms
step:899/2035 train_time:36198ms step_avg:40.26ms
step:900/2035 train_time:36257ms step_avg:40.29ms
step:901/2035 train_time:36318ms step_avg:40.31ms
step:902/2035 train_time:36377ms step_avg:40.33ms
step:903/2035 train_time:36437ms step_avg:40.35ms
step:904/2035 train_time:36496ms step_avg:40.37ms
step:905/2035 train_time:36557ms step_avg:40.39ms
step:906/2035 train_time:36617ms step_avg:40.42ms
step:907/2035 train_time:36677ms step_avg:40.44ms
step:908/2035 train_time:36737ms step_avg:40.46ms
step:909/2035 train_time:36797ms step_avg:40.48ms
step:910/2035 train_time:36857ms step_avg:40.50ms
step:911/2035 train_time:36917ms step_avg:40.52ms
step:912/2035 train_time:36976ms step_avg:40.54ms
step:913/2035 train_time:37036ms step_avg:40.57ms
step:914/2035 train_time:37095ms step_avg:40.59ms
step:915/2035 train_time:37156ms step_avg:40.61ms
step:916/2035 train_time:37216ms step_avg:40.63ms
step:917/2035 train_time:37277ms step_avg:40.65ms
step:918/2035 train_time:37336ms step_avg:40.67ms
step:919/2035 train_time:37397ms step_avg:40.69ms
step:920/2035 train_time:37456ms step_avg:40.71ms
step:921/2035 train_time:37517ms step_avg:40.74ms
step:922/2035 train_time:37577ms step_avg:40.76ms
step:923/2035 train_time:37637ms step_avg:40.78ms
step:924/2035 train_time:37696ms step_avg:40.80ms
step:925/2035 train_time:37758ms step_avg:40.82ms
step:926/2035 train_time:37818ms step_avg:40.84ms
step:927/2035 train_time:37879ms step_avg:40.86ms
step:928/2035 train_time:37938ms step_avg:40.88ms
step:929/2035 train_time:37998ms step_avg:40.90ms
step:930/2035 train_time:38058ms step_avg:40.92ms
step:931/2035 train_time:38119ms step_avg:40.94ms
step:932/2035 train_time:38178ms step_avg:40.96ms
step:933/2035 train_time:38238ms step_avg:40.98ms
step:934/2035 train_time:38298ms step_avg:41.00ms
step:935/2035 train_time:38358ms step_avg:41.02ms
step:936/2035 train_time:38417ms step_avg:41.04ms
step:937/2035 train_time:38478ms step_avg:41.07ms
step:938/2035 train_time:38537ms step_avg:41.08ms
step:939/2035 train_time:38597ms step_avg:41.10ms
step:940/2035 train_time:38657ms step_avg:41.12ms
step:941/2035 train_time:38718ms step_avg:41.15ms
step:942/2035 train_time:38778ms step_avg:41.17ms
step:943/2035 train_time:38838ms step_avg:41.19ms
step:944/2035 train_time:38897ms step_avg:41.20ms
step:945/2035 train_time:38958ms step_avg:41.23ms
step:946/2035 train_time:39017ms step_avg:41.24ms
step:947/2035 train_time:39077ms step_avg:41.26ms
step:948/2035 train_time:39136ms step_avg:41.28ms
step:949/2035 train_time:39197ms step_avg:41.30ms
step:950/2035 train_time:39257ms step_avg:41.32ms
step:951/2035 train_time:39318ms step_avg:41.34ms
step:952/2035 train_time:39377ms step_avg:41.36ms
step:953/2035 train_time:39438ms step_avg:41.38ms
step:954/2035 train_time:39497ms step_avg:41.40ms
step:955/2035 train_time:39558ms step_avg:41.42ms
step:956/2035 train_time:39618ms step_avg:41.44ms
step:957/2035 train_time:39678ms step_avg:41.46ms
step:958/2035 train_time:39737ms step_avg:41.48ms
step:959/2035 train_time:39797ms step_avg:41.50ms
step:960/2035 train_time:39857ms step_avg:41.52ms
step:961/2035 train_time:39918ms step_avg:41.54ms
step:962/2035 train_time:39978ms step_avg:41.56ms
step:963/2035 train_time:40038ms step_avg:41.58ms
step:964/2035 train_time:40097ms step_avg:41.59ms
step:965/2035 train_time:40158ms step_avg:41.61ms
step:966/2035 train_time:40217ms step_avg:41.63ms
step:967/2035 train_time:40279ms step_avg:41.65ms
step:968/2035 train_time:40337ms step_avg:41.67ms
step:969/2035 train_time:40398ms step_avg:41.69ms
step:970/2035 train_time:40457ms step_avg:41.71ms
step:971/2035 train_time:40518ms step_avg:41.73ms
step:972/2035 train_time:40578ms step_avg:41.75ms
step:973/2035 train_time:40639ms step_avg:41.77ms
step:974/2035 train_time:40698ms step_avg:41.78ms
step:975/2035 train_time:40760ms step_avg:41.80ms
step:976/2035 train_time:40819ms step_avg:41.82ms
step:977/2035 train_time:40880ms step_avg:41.84ms
step:978/2035 train_time:40940ms step_avg:41.86ms
step:979/2035 train_time:41000ms step_avg:41.88ms
step:980/2035 train_time:41059ms step_avg:41.90ms
step:981/2035 train_time:41120ms step_avg:41.92ms
step:982/2035 train_time:41180ms step_avg:41.93ms
step:983/2035 train_time:41241ms step_avg:41.95ms
step:984/2035 train_time:41300ms step_avg:41.97ms
step:985/2035 train_time:41360ms step_avg:41.99ms
step:986/2035 train_time:41420ms step_avg:42.01ms
step:987/2035 train_time:41480ms step_avg:42.03ms
step:988/2035 train_time:41539ms step_avg:42.04ms
step:989/2035 train_time:41600ms step_avg:42.06ms
step:990/2035 train_time:41658ms step_avg:42.08ms
step:991/2035 train_time:41719ms step_avg:42.10ms
step:992/2035 train_time:41778ms step_avg:42.12ms
step:993/2035 train_time:41838ms step_avg:42.13ms
step:994/2035 train_time:41898ms step_avg:42.15ms
step:995/2035 train_time:41959ms step_avg:42.17ms
step:996/2035 train_time:42017ms step_avg:42.19ms
step:997/2035 train_time:42078ms step_avg:42.20ms
step:998/2035 train_time:42137ms step_avg:42.22ms
step:999/2035 train_time:42199ms step_avg:42.24ms
step:1000/2035 train_time:42259ms step_avg:42.26ms
step:1000/2035 val_loss:3.6840 train_time:42322ms step_avg:42.32ms
step:1001/2035 train_time:42343ms step_avg:42.30ms
step:1002/2035 train_time:42382ms step_avg:42.30ms
step:1003/2035 train_time:42444ms step_avg:42.32ms
step:1004/2035 train_time:42506ms step_avg:42.34ms
step:1005/2035 train_time:42568ms step_avg:42.36ms
step:1006/2035 train_time:42627ms step_avg:42.37ms
step:1007/2035 train_time:42687ms step_avg:42.39ms
step:1008/2035 train_time:42746ms step_avg:42.41ms
step:1009/2035 train_time:42806ms step_avg:42.42ms
step:1010/2035 train_time:42864ms step_avg:42.44ms
step:1011/2035 train_time:42924ms step_avg:42.46ms
step:1012/2035 train_time:42984ms step_avg:42.47ms
step:1013/2035 train_time:43044ms step_avg:42.49ms
step:1014/2035 train_time:43103ms step_avg:42.51ms
step:1015/2035 train_time:43162ms step_avg:42.52ms
step:1016/2035 train_time:43222ms step_avg:42.54ms
step:1017/2035 train_time:43284ms step_avg:42.56ms
step:1018/2035 train_time:43345ms step_avg:42.58ms
step:1019/2035 train_time:43407ms step_avg:42.60ms
step:1020/2035 train_time:43467ms step_avg:42.62ms
step:1021/2035 train_time:43529ms step_avg:42.63ms
step:1022/2035 train_time:43588ms step_avg:42.65ms
step:1023/2035 train_time:43649ms step_avg:42.67ms
step:1024/2035 train_time:43708ms step_avg:42.68ms
step:1025/2035 train_time:43769ms step_avg:42.70ms
step:1026/2035 train_time:43829ms step_avg:42.72ms
step:1027/2035 train_time:43889ms step_avg:42.73ms
step:1028/2035 train_time:43948ms step_avg:42.75ms
step:1029/2035 train_time:44008ms step_avg:42.77ms
step:1030/2035 train_time:44067ms step_avg:42.78ms
step:1031/2035 train_time:44127ms step_avg:42.80ms
step:1032/2035 train_time:44187ms step_avg:42.82ms
step:1033/2035 train_time:44248ms step_avg:42.83ms
step:1034/2035 train_time:44309ms step_avg:42.85ms
step:1035/2035 train_time:44370ms step_avg:42.87ms
step:1036/2035 train_time:44431ms step_avg:42.89ms
step:1037/2035 train_time:44492ms step_avg:42.90ms
step:1038/2035 train_time:44552ms step_avg:42.92ms
step:1039/2035 train_time:44613ms step_avg:42.94ms
step:1040/2035 train_time:44673ms step_avg:42.95ms
step:1041/2035 train_time:44734ms step_avg:42.97ms
step:1042/2035 train_time:44793ms step_avg:42.99ms
step:1043/2035 train_time:44854ms step_avg:43.01ms
step:1044/2035 train_time:44914ms step_avg:43.02ms
step:1045/2035 train_time:44974ms step_avg:43.04ms
step:1046/2035 train_time:45033ms step_avg:43.05ms
step:1047/2035 train_time:45093ms step_avg:43.07ms
step:1048/2035 train_time:45153ms step_avg:43.08ms
step:1049/2035 train_time:45214ms step_avg:43.10ms
step:1050/2035 train_time:45274ms step_avg:43.12ms
step:1051/2035 train_time:45335ms step_avg:43.13ms
step:1052/2035 train_time:45395ms step_avg:43.15ms
step:1053/2035 train_time:45455ms step_avg:43.17ms
step:1054/2035 train_time:45516ms step_avg:43.18ms
step:1055/2035 train_time:45577ms step_avg:43.20ms
step:1056/2035 train_time:45636ms step_avg:43.22ms
step:1057/2035 train_time:45697ms step_avg:43.23ms
step:1058/2035 train_time:45756ms step_avg:43.25ms
step:1059/2035 train_time:45817ms step_avg:43.26ms
step:1060/2035 train_time:45877ms step_avg:43.28ms
step:1061/2035 train_time:45937ms step_avg:43.30ms
step:1062/2035 train_time:45997ms step_avg:43.31ms
step:1063/2035 train_time:46057ms step_avg:43.33ms
step:1064/2035 train_time:46116ms step_avg:43.34ms
step:1065/2035 train_time:46177ms step_avg:43.36ms
step:1066/2035 train_time:46237ms step_avg:43.37ms
step:1067/2035 train_time:46297ms step_avg:43.39ms
step:1068/2035 train_time:46356ms step_avg:43.40ms
step:1069/2035 train_time:46417ms step_avg:43.42ms
step:1070/2035 train_time:46477ms step_avg:43.44ms
step:1071/2035 train_time:46537ms step_avg:43.45ms
step:1072/2035 train_time:46597ms step_avg:43.47ms
step:1073/2035 train_time:46657ms step_avg:43.48ms
step:1074/2035 train_time:46716ms step_avg:43.50ms
step:1075/2035 train_time:46777ms step_avg:43.51ms
step:1076/2035 train_time:46836ms step_avg:43.53ms
step:1077/2035 train_time:46897ms step_avg:43.54ms
step:1078/2035 train_time:46956ms step_avg:43.56ms
step:1079/2035 train_time:47017ms step_avg:43.57ms
step:1080/2035 train_time:47076ms step_avg:43.59ms
step:1081/2035 train_time:47136ms step_avg:43.60ms
step:1082/2035 train_time:47196ms step_avg:43.62ms
step:1083/2035 train_time:47256ms step_avg:43.63ms
step:1084/2035 train_time:47316ms step_avg:43.65ms
step:1085/2035 train_time:47377ms step_avg:43.67ms
step:1086/2035 train_time:47437ms step_avg:43.68ms
step:1087/2035 train_time:47498ms step_avg:43.70ms
step:1088/2035 train_time:47557ms step_avg:43.71ms
step:1089/2035 train_time:47618ms step_avg:43.73ms
step:1090/2035 train_time:47677ms step_avg:43.74ms
step:1091/2035 train_time:47738ms step_avg:43.76ms
step:1092/2035 train_time:47797ms step_avg:43.77ms
step:1093/2035 train_time:47858ms step_avg:43.79ms
step:1094/2035 train_time:47917ms step_avg:43.80ms
step:1095/2035 train_time:47977ms step_avg:43.81ms
step:1096/2035 train_time:48036ms step_avg:43.83ms
step:1097/2035 train_time:48097ms step_avg:43.84ms
step:1098/2035 train_time:48157ms step_avg:43.86ms
step:1099/2035 train_time:48218ms step_avg:43.87ms
step:1100/2035 train_time:48277ms step_avg:43.89ms
step:1101/2035 train_time:48337ms step_avg:43.90ms
step:1102/2035 train_time:48397ms step_avg:43.92ms
step:1103/2035 train_time:48457ms step_avg:43.93ms
step:1104/2035 train_time:48517ms step_avg:43.95ms
step:1105/2035 train_time:48578ms step_avg:43.96ms
step:1106/2035 train_time:48637ms step_avg:43.98ms
step:1107/2035 train_time:48698ms step_avg:43.99ms
step:1108/2035 train_time:48757ms step_avg:44.00ms
step:1109/2035 train_time:48819ms step_avg:44.02ms
step:1110/2035 train_time:48878ms step_avg:44.03ms
step:1111/2035 train_time:48939ms step_avg:44.05ms
step:1112/2035 train_time:48998ms step_avg:44.06ms
step:1113/2035 train_time:49058ms step_avg:44.08ms
step:1114/2035 train_time:49118ms step_avg:44.09ms
step:1115/2035 train_time:49179ms step_avg:44.11ms
step:1116/2035 train_time:49239ms step_avg:44.12ms
step:1117/2035 train_time:49299ms step_avg:44.14ms
step:1118/2035 train_time:49358ms step_avg:44.15ms
step:1119/2035 train_time:49419ms step_avg:44.16ms
step:1120/2035 train_time:49478ms step_avg:44.18ms
step:1121/2035 train_time:49538ms step_avg:44.19ms
step:1122/2035 train_time:49598ms step_avg:44.20ms
step:1123/2035 train_time:49659ms step_avg:44.22ms
step:1124/2035 train_time:49718ms step_avg:44.23ms
step:1125/2035 train_time:49779ms step_avg:44.25ms
step:1126/2035 train_time:49838ms step_avg:44.26ms
step:1127/2035 train_time:49899ms step_avg:44.28ms
step:1128/2035 train_time:49959ms step_avg:44.29ms
step:1129/2035 train_time:50020ms step_avg:44.30ms
step:1130/2035 train_time:50079ms step_avg:44.32ms
step:1131/2035 train_time:50140ms step_avg:44.33ms
step:1132/2035 train_time:50199ms step_avg:44.35ms
step:1133/2035 train_time:50260ms step_avg:44.36ms
step:1134/2035 train_time:50320ms step_avg:44.37ms
step:1135/2035 train_time:50380ms step_avg:44.39ms
step:1136/2035 train_time:50440ms step_avg:44.40ms
step:1137/2035 train_time:50500ms step_avg:44.41ms
step:1138/2035 train_time:50559ms step_avg:44.43ms
step:1139/2035 train_time:50619ms step_avg:44.44ms
step:1140/2035 train_time:50679ms step_avg:44.46ms
step:1141/2035 train_time:50739ms step_avg:44.47ms
step:1142/2035 train_time:50799ms step_avg:44.48ms
step:1143/2035 train_time:50860ms step_avg:44.50ms
step:1144/2035 train_time:50920ms step_avg:44.51ms
step:1145/2035 train_time:50980ms step_avg:44.52ms
step:1146/2035 train_time:51039ms step_avg:44.54ms
step:1147/2035 train_time:51100ms step_avg:44.55ms
step:1148/2035 train_time:51160ms step_avg:44.56ms
step:1149/2035 train_time:51220ms step_avg:44.58ms
step:1150/2035 train_time:51279ms step_avg:44.59ms
step:1151/2035 train_time:51341ms step_avg:44.61ms
step:1152/2035 train_time:51400ms step_avg:44.62ms
step:1153/2035 train_time:51460ms step_avg:44.63ms
step:1154/2035 train_time:51520ms step_avg:44.64ms
step:1155/2035 train_time:51580ms step_avg:44.66ms
step:1156/2035 train_time:51639ms step_avg:44.67ms
step:1157/2035 train_time:51699ms step_avg:44.68ms
step:1158/2035 train_time:51759ms step_avg:44.70ms
step:1159/2035 train_time:51820ms step_avg:44.71ms
step:1160/2035 train_time:51879ms step_avg:44.72ms
step:1161/2035 train_time:51939ms step_avg:44.74ms
step:1162/2035 train_time:51998ms step_avg:44.75ms
step:1163/2035 train_time:52059ms step_avg:44.76ms
step:1164/2035 train_time:52118ms step_avg:44.77ms
step:1165/2035 train_time:52179ms step_avg:44.79ms
step:1166/2035 train_time:52238ms step_avg:44.80ms
step:1167/2035 train_time:52299ms step_avg:44.81ms
step:1168/2035 train_time:52358ms step_avg:44.83ms
step:1169/2035 train_time:52419ms step_avg:44.84ms
step:1170/2035 train_time:52479ms step_avg:44.85ms
step:1171/2035 train_time:52539ms step_avg:44.87ms
step:1172/2035 train_time:52598ms step_avg:44.88ms
step:1173/2035 train_time:52659ms step_avg:44.89ms
step:1174/2035 train_time:52718ms step_avg:44.90ms
step:1175/2035 train_time:52777ms step_avg:44.92ms
step:1176/2035 train_time:52837ms step_avg:44.93ms
step:1177/2035 train_time:52897ms step_avg:44.94ms
step:1178/2035 train_time:52956ms step_avg:44.95ms
step:1179/2035 train_time:53018ms step_avg:44.97ms
step:1180/2035 train_time:53077ms step_avg:44.98ms
step:1181/2035 train_time:53137ms step_avg:44.99ms
step:1182/2035 train_time:53196ms step_avg:45.01ms
step:1183/2035 train_time:53257ms step_avg:45.02ms
step:1184/2035 train_time:53316ms step_avg:45.03ms
step:1185/2035 train_time:53377ms step_avg:45.04ms
step:1186/2035 train_time:53437ms step_avg:45.06ms
step:1187/2035 train_time:53497ms step_avg:45.07ms
step:1188/2035 train_time:53556ms step_avg:45.08ms
step:1189/2035 train_time:53617ms step_avg:45.09ms
step:1190/2035 train_time:53677ms step_avg:45.11ms
step:1191/2035 train_time:53737ms step_avg:45.12ms
step:1192/2035 train_time:53796ms step_avg:45.13ms
step:1193/2035 train_time:53857ms step_avg:45.14ms
step:1194/2035 train_time:53916ms step_avg:45.16ms
step:1195/2035 train_time:53977ms step_avg:45.17ms
step:1196/2035 train_time:54036ms step_avg:45.18ms
step:1197/2035 train_time:54096ms step_avg:45.19ms
step:1198/2035 train_time:54156ms step_avg:45.21ms
step:1199/2035 train_time:54217ms step_avg:45.22ms
step:1200/2035 train_time:54276ms step_avg:45.23ms
step:1201/2035 train_time:54337ms step_avg:45.24ms
step:1202/2035 train_time:54396ms step_avg:45.25ms
step:1203/2035 train_time:54457ms step_avg:45.27ms
step:1204/2035 train_time:54517ms step_avg:45.28ms
step:1205/2035 train_time:54578ms step_avg:45.29ms
step:1206/2035 train_time:54637ms step_avg:45.30ms
step:1207/2035 train_time:54699ms step_avg:45.32ms
step:1208/2035 train_time:54758ms step_avg:45.33ms
step:1209/2035 train_time:54818ms step_avg:45.34ms
step:1210/2035 train_time:54877ms step_avg:45.35ms
step:1211/2035 train_time:54939ms step_avg:45.37ms
step:1212/2035 train_time:54998ms step_avg:45.38ms
step:1213/2035 train_time:55059ms step_avg:45.39ms
step:1214/2035 train_time:55118ms step_avg:45.40ms
step:1215/2035 train_time:55179ms step_avg:45.41ms
step:1216/2035 train_time:55238ms step_avg:45.43ms
step:1217/2035 train_time:55299ms step_avg:45.44ms
step:1218/2035 train_time:55358ms step_avg:45.45ms
step:1219/2035 train_time:55419ms step_avg:45.46ms
step:1220/2035 train_time:55477ms step_avg:45.47ms
step:1221/2035 train_time:55538ms step_avg:45.49ms
step:1222/2035 train_time:55597ms step_avg:45.50ms
step:1223/2035 train_time:55658ms step_avg:45.51ms
step:1224/2035 train_time:55717ms step_avg:45.52ms
step:1225/2035 train_time:55778ms step_avg:45.53ms
step:1226/2035 train_time:55837ms step_avg:45.54ms
step:1227/2035 train_time:55898ms step_avg:45.56ms
step:1228/2035 train_time:55958ms step_avg:45.57ms
step:1229/2035 train_time:56018ms step_avg:45.58ms
step:1230/2035 train_time:56078ms step_avg:45.59ms
step:1231/2035 train_time:56139ms step_avg:45.60ms
step:1232/2035 train_time:56199ms step_avg:45.62ms
step:1233/2035 train_time:56259ms step_avg:45.63ms
step:1234/2035 train_time:56318ms step_avg:45.64ms
step:1235/2035 train_time:56379ms step_avg:45.65ms
step:1236/2035 train_time:56438ms step_avg:45.66ms
step:1237/2035 train_time:56498ms step_avg:45.67ms
step:1238/2035 train_time:56558ms step_avg:45.69ms
step:1239/2035 train_time:56619ms step_avg:45.70ms
step:1240/2035 train_time:56678ms step_avg:45.71ms
step:1241/2035 train_time:56738ms step_avg:45.72ms
step:1242/2035 train_time:56797ms step_avg:45.73ms
step:1243/2035 train_time:56858ms step_avg:45.74ms
step:1244/2035 train_time:56918ms step_avg:45.75ms
step:1245/2035 train_time:56978ms step_avg:45.77ms
step:1246/2035 train_time:57037ms step_avg:45.78ms
step:1247/2035 train_time:57098ms step_avg:45.79ms
step:1248/2035 train_time:57157ms step_avg:45.80ms
step:1249/2035 train_time:57218ms step_avg:45.81ms
step:1250/2035 train_time:57277ms step_avg:45.82ms
step:1250/2035 val_loss:3.5681 train_time:57340ms step_avg:45.87ms
step:1251/2035 train_time:57363ms step_avg:45.85ms
step:1252/2035 train_time:57399ms step_avg:45.85ms
step:1253/2035 train_time:57463ms step_avg:45.86ms
step:1254/2035 train_time:57524ms step_avg:45.87ms
step:1255/2035 train_time:57585ms step_avg:45.88ms
step:1256/2035 train_time:57645ms step_avg:45.90ms
step:1257/2035 train_time:57704ms step_avg:45.91ms
step:1258/2035 train_time:57763ms step_avg:45.92ms
step:1259/2035 train_time:57823ms step_avg:45.93ms
step:1260/2035 train_time:57881ms step_avg:45.94ms
step:1261/2035 train_time:57940ms step_avg:45.95ms
step:1262/2035 train_time:57999ms step_avg:45.96ms
step:1263/2035 train_time:58059ms step_avg:45.97ms
step:1264/2035 train_time:58118ms step_avg:45.98ms
step:1265/2035 train_time:58178ms step_avg:45.99ms
step:1266/2035 train_time:58237ms step_avg:46.00ms
step:1267/2035 train_time:58298ms step_avg:46.01ms
step:1268/2035 train_time:58359ms step_avg:46.02ms
step:1269/2035 train_time:58422ms step_avg:46.04ms
step:1270/2035 train_time:58483ms step_avg:46.05ms
step:1271/2035 train_time:58544ms step_avg:46.06ms
step:1272/2035 train_time:58604ms step_avg:46.07ms
step:1273/2035 train_time:58665ms step_avg:46.08ms
step:1274/2035 train_time:58724ms step_avg:46.09ms
step:1275/2035 train_time:58784ms step_avg:46.11ms
step:1276/2035 train_time:58843ms step_avg:46.11ms
step:1277/2035 train_time:58902ms step_avg:46.13ms
step:1278/2035 train_time:58961ms step_avg:46.14ms
step:1279/2035 train_time:59021ms step_avg:46.15ms
step:1280/2035 train_time:59079ms step_avg:46.16ms
step:1281/2035 train_time:59140ms step_avg:46.17ms
step:1282/2035 train_time:59199ms step_avg:46.18ms
step:1283/2035 train_time:59260ms step_avg:46.19ms
step:1284/2035 train_time:59320ms step_avg:46.20ms
step:1285/2035 train_time:59381ms step_avg:46.21ms
step:1286/2035 train_time:59441ms step_avg:46.22ms
step:1287/2035 train_time:59503ms step_avg:46.23ms
step:1288/2035 train_time:59562ms step_avg:46.24ms
step:1289/2035 train_time:59623ms step_avg:46.26ms
step:1290/2035 train_time:59683ms step_avg:46.27ms
step:1291/2035 train_time:59744ms step_avg:46.28ms
step:1292/2035 train_time:59803ms step_avg:46.29ms
step:1293/2035 train_time:59864ms step_avg:46.30ms
step:1294/2035 train_time:59923ms step_avg:46.31ms
step:1295/2035 train_time:59983ms step_avg:46.32ms
step:1296/2035 train_time:60041ms step_avg:46.33ms
step:1297/2035 train_time:60102ms step_avg:46.34ms
step:1298/2035 train_time:60161ms step_avg:46.35ms
step:1299/2035 train_time:60222ms step_avg:46.36ms
step:1300/2035 train_time:60281ms step_avg:46.37ms
step:1301/2035 train_time:60342ms step_avg:46.38ms
step:1302/2035 train_time:60402ms step_avg:46.39ms
step:1303/2035 train_time:60464ms step_avg:46.40ms
step:1304/2035 train_time:60524ms step_avg:46.41ms
step:1305/2035 train_time:60585ms step_avg:46.43ms
step:1306/2035 train_time:60645ms step_avg:46.44ms
step:1307/2035 train_time:60706ms step_avg:46.45ms
step:1308/2035 train_time:60765ms step_avg:46.46ms
step:1309/2035 train_time:60826ms step_avg:46.47ms
step:1310/2035 train_time:60884ms step_avg:46.48ms
step:1311/2035 train_time:60945ms step_avg:46.49ms
step:1312/2035 train_time:61004ms step_avg:46.50ms
step:1313/2035 train_time:61064ms step_avg:46.51ms
step:1314/2035 train_time:61124ms step_avg:46.52ms
step:1315/2035 train_time:61184ms step_avg:46.53ms
step:1316/2035 train_time:61243ms step_avg:46.54ms
step:1317/2035 train_time:61305ms step_avg:46.55ms
step:1318/2035 train_time:61365ms step_avg:46.56ms
step:1319/2035 train_time:61426ms step_avg:46.57ms
step:1320/2035 train_time:61486ms step_avg:46.58ms
step:1321/2035 train_time:61547ms step_avg:46.59ms
step:1322/2035 train_time:61606ms step_avg:46.60ms
step:1323/2035 train_time:61667ms step_avg:46.61ms
step:1324/2035 train_time:61726ms step_avg:46.62ms
step:1325/2035 train_time:61787ms step_avg:46.63ms
step:1326/2035 train_time:61846ms step_avg:46.64ms
step:1327/2035 train_time:61907ms step_avg:46.65ms
step:1328/2035 train_time:61966ms step_avg:46.66ms
step:1329/2035 train_time:62027ms step_avg:46.67ms
step:1330/2035 train_time:62086ms step_avg:46.68ms
step:1331/2035 train_time:62147ms step_avg:46.69ms
step:1332/2035 train_time:62234ms step_avg:46.72ms
step:1333/2035 train_time:62322ms step_avg:46.75ms
step:1334/2035 train_time:62410ms step_avg:46.78ms
step:1335/2035 train_time:62500ms step_avg:46.82ms
step:1336/2035 train_time:62587ms step_avg:46.85ms
step:1337/2035 train_time:62674ms step_avg:46.88ms
step:1338/2035 train_time:62762ms step_avg:46.91ms
step:1339/2035 train_time:62850ms step_avg:46.94ms
step:1340/2035 train_time:62937ms step_avg:46.97ms
step:1341/2035 train_time:63024ms step_avg:47.00ms
step:1342/2035 train_time:63111ms step_avg:47.03ms
step:1343/2035 train_time:63200ms step_avg:47.06ms
step:1344/2035 train_time:63286ms step_avg:47.09ms
step:1345/2035 train_time:63374ms step_avg:47.12ms
step:1346/2035 train_time:63462ms step_avg:47.15ms
step:1347/2035 train_time:63551ms step_avg:47.18ms
step:1348/2035 train_time:63638ms step_avg:47.21ms
step:1349/2035 train_time:63726ms step_avg:47.24ms
step:1350/2035 train_time:63813ms step_avg:47.27ms
step:1351/2035 train_time:63901ms step_avg:47.30ms
step:1352/2035 train_time:63988ms step_avg:47.33ms
step:1353/2035 train_time:64075ms step_avg:47.36ms
step:1354/2035 train_time:64162ms step_avg:47.39ms
step:1355/2035 train_time:64250ms step_avg:47.42ms
step:1356/2035 train_time:64337ms step_avg:47.45ms
step:1357/2035 train_time:64425ms step_avg:47.48ms
step:1358/2035 train_time:64514ms step_avg:47.51ms
step:1359/2035 train_time:64603ms step_avg:47.54ms
step:1360/2035 train_time:64690ms step_avg:47.57ms
step:1361/2035 train_time:64778ms step_avg:47.60ms
step:1362/2035 train_time:64864ms step_avg:47.62ms
step:1363/2035 train_time:64952ms step_avg:47.65ms
step:1364/2035 train_time:65039ms step_avg:47.68ms
step:1365/2035 train_time:65127ms step_avg:47.71ms
step:1366/2035 train_time:65213ms step_avg:47.74ms
step:1367/2035 train_time:65302ms step_avg:47.77ms
step:1368/2035 train_time:65388ms step_avg:47.80ms
step:1369/2035 train_time:65477ms step_avg:47.83ms
step:1370/2035 train_time:65564ms step_avg:47.86ms
step:1371/2035 train_time:65652ms step_avg:47.89ms
step:1372/2035 train_time:65740ms step_avg:47.92ms
step:1373/2035 train_time:65829ms step_avg:47.95ms
step:1374/2035 train_time:65915ms step_avg:47.97ms
step:1375/2035 train_time:66004ms step_avg:48.00ms
step:1376/2035 train_time:66091ms step_avg:48.03ms
step:1377/2035 train_time:66178ms step_avg:48.06ms
step:1378/2035 train_time:66265ms step_avg:48.09ms
step:1379/2035 train_time:66353ms step_avg:48.12ms
step:1380/2035 train_time:66440ms step_avg:48.15ms
step:1381/2035 train_time:66529ms step_avg:48.17ms
step:1382/2035 train_time:66615ms step_avg:48.20ms
step:1383/2035 train_time:66703ms step_avg:48.23ms
step:1384/2035 train_time:66790ms step_avg:48.26ms
step:1385/2035 train_time:66879ms step_avg:48.29ms
step:1386/2035 train_time:66965ms step_avg:48.32ms
step:1387/2035 train_time:67053ms step_avg:48.34ms
step:1388/2035 train_time:67140ms step_avg:48.37ms
step:1389/2035 train_time:67229ms step_avg:48.40ms
step:1390/2035 train_time:67316ms step_avg:48.43ms
step:1391/2035 train_time:67404ms step_avg:48.46ms
step:1392/2035 train_time:67492ms step_avg:48.49ms
step:1393/2035 train_time:67581ms step_avg:48.51ms
step:1394/2035 train_time:67668ms step_avg:48.54ms
step:1395/2035 train_time:67756ms step_avg:48.57ms
step:1396/2035 train_time:67843ms step_avg:48.60ms
step:1397/2035 train_time:67931ms step_avg:48.63ms
step:1398/2035 train_time:68017ms step_avg:48.65ms
step:1399/2035 train_time:68106ms step_avg:48.68ms
step:1400/2035 train_time:68193ms step_avg:48.71ms
step:1401/2035 train_time:68281ms step_avg:48.74ms
step:1402/2035 train_time:68368ms step_avg:48.76ms
step:1403/2035 train_time:68456ms step_avg:48.79ms
step:1404/2035 train_time:68543ms step_avg:48.82ms
step:1405/2035 train_time:68631ms step_avg:48.85ms
step:1406/2035 train_time:68718ms step_avg:48.87ms
step:1407/2035 train_time:68806ms step_avg:48.90ms
step:1408/2035 train_time:68893ms step_avg:48.93ms
step:1409/2035 train_time:68981ms step_avg:48.96ms
step:1410/2035 train_time:69068ms step_avg:48.98ms
step:1411/2035 train_time:69157ms step_avg:49.01ms
step:1412/2035 train_time:69243ms step_avg:49.04ms
step:1413/2035 train_time:69332ms step_avg:49.07ms
step:1414/2035 train_time:69419ms step_avg:49.09ms
step:1415/2035 train_time:69506ms step_avg:49.12ms
step:1416/2035 train_time:69594ms step_avg:49.15ms
step:1417/2035 train_time:69682ms step_avg:49.18ms
step:1418/2035 train_time:69769ms step_avg:49.20ms
step:1419/2035 train_time:69858ms step_avg:49.23ms
step:1420/2035 train_time:69944ms step_avg:49.26ms
step:1421/2035 train_time:70031ms step_avg:49.28ms
step:1422/2035 train_time:70118ms step_avg:49.31ms
step:1423/2035 train_time:70206ms step_avg:49.34ms
step:1424/2035 train_time:70294ms step_avg:49.36ms
step:1425/2035 train_time:70382ms step_avg:49.39ms
step:1426/2035 train_time:70470ms step_avg:49.42ms
step:1427/2035 train_time:70558ms step_avg:49.44ms
step:1428/2035 train_time:70645ms step_avg:49.47ms
step:1429/2035 train_time:70734ms step_avg:49.50ms
step:1430/2035 train_time:70821ms step_avg:49.53ms
step:1431/2035 train_time:70909ms step_avg:49.55ms
step:1432/2035 train_time:70995ms step_avg:49.58ms
step:1433/2035 train_time:71083ms step_avg:49.60ms
step:1434/2035 train_time:71170ms step_avg:49.63ms
step:1435/2035 train_time:71259ms step_avg:49.66ms
step:1436/2035 train_time:71345ms step_avg:49.68ms
step:1437/2035 train_time:71433ms step_avg:49.71ms
step:1438/2035 train_time:71520ms step_avg:49.74ms
step:1439/2035 train_time:71609ms step_avg:49.76ms
step:1440/2035 train_time:71696ms step_avg:49.79ms
step:1441/2035 train_time:71783ms step_avg:49.81ms
step:1442/2035 train_time:71870ms step_avg:49.84ms
step:1443/2035 train_time:71958ms step_avg:49.87ms
step:1444/2035 train_time:72045ms step_avg:49.89ms
step:1445/2035 train_time:72133ms step_avg:49.92ms
step:1446/2035 train_time:72221ms step_avg:49.95ms
step:1447/2035 train_time:72309ms step_avg:49.97ms
step:1448/2035 train_time:72395ms step_avg:50.00ms
step:1449/2035 train_time:72483ms step_avg:50.02ms
step:1450/2035 train_time:72571ms step_avg:50.05ms
step:1451/2035 train_time:72658ms step_avg:50.07ms
step:1452/2035 train_time:72745ms step_avg:50.10ms
step:1453/2035 train_time:72833ms step_avg:50.13ms
step:1454/2035 train_time:72920ms step_avg:50.15ms
step:1455/2035 train_time:73008ms step_avg:50.18ms
step:1456/2035 train_time:73095ms step_avg:50.20ms
step:1457/2035 train_time:73184ms step_avg:50.23ms
step:1458/2035 train_time:73271ms step_avg:50.25ms
step:1459/2035 train_time:73358ms step_avg:50.28ms
step:1460/2035 train_time:73445ms step_avg:50.30ms
step:1461/2035 train_time:73533ms step_avg:50.33ms
step:1462/2035 train_time:73620ms step_avg:50.36ms
step:1463/2035 train_time:73709ms step_avg:50.38ms
step:1464/2035 train_time:73796ms step_avg:50.41ms
step:1465/2035 train_time:73884ms step_avg:50.43ms
step:1466/2035 train_time:73972ms step_avg:50.46ms
step:1467/2035 train_time:74059ms step_avg:50.48ms
step:1468/2035 train_time:74146ms step_avg:50.51ms
step:1469/2035 train_time:74235ms step_avg:50.53ms
step:1470/2035 train_time:74321ms step_avg:50.56ms
step:1471/2035 train_time:74410ms step_avg:50.58ms
step:1472/2035 train_time:74497ms step_avg:50.61ms
step:1473/2035 train_time:74586ms step_avg:50.64ms
step:1474/2035 train_time:74672ms step_avg:50.66ms
step:1475/2035 train_time:74761ms step_avg:50.69ms
step:1476/2035 train_time:74848ms step_avg:50.71ms
step:1477/2035 train_time:74936ms step_avg:50.74ms
step:1478/2035 train_time:75023ms step_avg:50.76ms
step:1479/2035 train_time:75111ms step_avg:50.78ms
step:1480/2035 train_time:75198ms step_avg:50.81ms
step:1481/2035 train_time:75286ms step_avg:50.83ms
step:1482/2035 train_time:75373ms step_avg:50.86ms
step:1483/2035 train_time:75461ms step_avg:50.88ms
step:1484/2035 train_time:75548ms step_avg:50.91ms
step:1485/2035 train_time:75636ms step_avg:50.93ms
step:1486/2035 train_time:75723ms step_avg:50.96ms
step:1487/2035 train_time:75811ms step_avg:50.98ms
step:1488/2035 train_time:75900ms step_avg:51.01ms
step:1489/2035 train_time:75988ms step_avg:51.03ms
step:1490/2035 train_time:76075ms step_avg:51.06ms
step:1491/2035 train_time:76163ms step_avg:51.08ms
step:1492/2035 train_time:76250ms step_avg:51.11ms
step:1493/2035 train_time:76339ms step_avg:51.13ms
step:1494/2035 train_time:76426ms step_avg:51.16ms
step:1495/2035 train_time:76514ms step_avg:51.18ms
step:1496/2035 train_time:76601ms step_avg:51.20ms
step:1497/2035 train_time:76689ms step_avg:51.23ms
step:1498/2035 train_time:76776ms step_avg:51.25ms
step:1499/2035 train_time:76864ms step_avg:51.28ms
step:1500/2035 train_time:76952ms step_avg:51.30ms
step:1500/2035 val_loss:3.4522 train_time:77042ms step_avg:51.36ms
step:1501/2035 train_time:77063ms step_avg:51.34ms
step:1502/2035 train_time:77131ms step_avg:51.35ms
step:1503/2035 train_time:77223ms step_avg:51.38ms
step:1504/2035 train_time:77312ms step_avg:51.40ms
step:1505/2035 train_time:77400ms step_avg:51.43ms
step:1506/2035 train_time:77486ms step_avg:51.45ms
step:1507/2035 train_time:77573ms step_avg:51.47ms
step:1508/2035 train_time:77659ms step_avg:51.50ms
step:1509/2035 train_time:77746ms step_avg:51.52ms
step:1510/2035 train_time:77833ms step_avg:51.55ms
step:1511/2035 train_time:77920ms step_avg:51.57ms
step:1512/2035 train_time:78007ms step_avg:51.59ms
step:1513/2035 train_time:78097ms step_avg:51.62ms
step:1514/2035 train_time:78186ms step_avg:51.64ms
step:1515/2035 train_time:78277ms step_avg:51.67ms
step:1516/2035 train_time:78364ms step_avg:51.69ms
step:1517/2035 train_time:78453ms step_avg:51.72ms
step:1518/2035 train_time:78539ms step_avg:51.74ms
step:1519/2035 train_time:78626ms step_avg:51.76ms
step:1520/2035 train_time:78713ms step_avg:51.78ms
step:1521/2035 train_time:78800ms step_avg:51.81ms
step:1522/2035 train_time:78887ms step_avg:51.83ms
step:1523/2035 train_time:78974ms step_avg:51.85ms
step:1524/2035 train_time:79063ms step_avg:51.88ms
step:1525/2035 train_time:79152ms step_avg:51.90ms
step:1526/2035 train_time:79241ms step_avg:51.93ms
step:1527/2035 train_time:79329ms step_avg:51.95ms
step:1528/2035 train_time:79417ms step_avg:51.97ms
step:1529/2035 train_time:79504ms step_avg:52.00ms
step:1530/2035 train_time:79591ms step_avg:52.02ms
step:1531/2035 train_time:79679ms step_avg:52.04ms
step:1532/2035 train_time:79765ms step_avg:52.07ms
step:1533/2035 train_time:79853ms step_avg:52.09ms
step:1534/2035 train_time:79940ms step_avg:52.11ms
step:1535/2035 train_time:80029ms step_avg:52.14ms
step:1536/2035 train_time:80117ms step_avg:52.16ms
step:1537/2035 train_time:80206ms step_avg:52.18ms
step:1538/2035 train_time:80294ms step_avg:52.21ms
step:1539/2035 train_time:80383ms step_avg:52.23ms
step:1540/2035 train_time:80470ms step_avg:52.25ms
step:1541/2035 train_time:80558ms step_avg:52.28ms
step:1542/2035 train_time:80644ms step_avg:52.30ms
step:1543/2035 train_time:80732ms step_avg:52.32ms
step:1544/2035 train_time:80818ms step_avg:52.34ms
step:1545/2035 train_time:80906ms step_avg:52.37ms
step:1546/2035 train_time:80994ms step_avg:52.39ms
step:1547/2035 train_time:81084ms step_avg:52.41ms
step:1548/2035 train_time:81170ms step_avg:52.44ms
step:1549/2035 train_time:81259ms step_avg:52.46ms
step:1550/2035 train_time:81347ms step_avg:52.48ms
step:1551/2035 train_time:81436ms step_avg:52.51ms
step:1552/2035 train_time:81523ms step_avg:52.53ms
step:1553/2035 train_time:81610ms step_avg:52.55ms
step:1554/2035 train_time:81698ms step_avg:52.57ms
step:1555/2035 train_time:81786ms step_avg:52.60ms
step:1556/2035 train_time:81873ms step_avg:52.62ms
step:1557/2035 train_time:81961ms step_avg:52.64ms
step:1558/2035 train_time:82049ms step_avg:52.66ms
step:1559/2035 train_time:82137ms step_avg:52.69ms
step:1560/2035 train_time:82224ms step_avg:52.71ms
step:1561/2035 train_time:82312ms step_avg:52.73ms
step:1562/2035 train_time:82400ms step_avg:52.75ms
step:1563/2035 train_time:82487ms step_avg:52.78ms
step:1564/2035 train_time:82575ms step_avg:52.80ms
step:1565/2035 train_time:82662ms step_avg:52.82ms
step:1566/2035 train_time:82749ms step_avg:52.84ms
step:1567/2035 train_time:82836ms step_avg:52.86ms
step:1568/2035 train_time:82924ms step_avg:52.88ms
step:1569/2035 train_time:83012ms step_avg:52.91ms
step:1570/2035 train_time:83098ms step_avg:52.93ms
step:1571/2035 train_time:83187ms step_avg:52.95ms
step:1572/2035 train_time:83274ms step_avg:52.97ms
step:1573/2035 train_time:83363ms step_avg:53.00ms
step:1574/2035 train_time:83450ms step_avg:53.02ms
step:1575/2035 train_time:83538ms step_avg:53.04ms
step:1576/2035 train_time:83624ms step_avg:53.06ms
step:1577/2035 train_time:83712ms step_avg:53.08ms
step:1578/2035 train_time:83799ms step_avg:53.10ms
step:1579/2035 train_time:83887ms step_avg:53.13ms
step:1580/2035 train_time:83974ms step_avg:53.15ms
step:1581/2035 train_time:84063ms step_avg:53.17ms
step:1582/2035 train_time:84149ms step_avg:53.19ms
step:1583/2035 train_time:84237ms step_avg:53.21ms
step:1584/2035 train_time:84324ms step_avg:53.23ms
step:1585/2035 train_time:84413ms step_avg:53.26ms
step:1586/2035 train_time:84499ms step_avg:53.28ms
step:1587/2035 train_time:84588ms step_avg:53.30ms
step:1588/2035 train_time:84674ms step_avg:53.32ms
step:1589/2035 train_time:84763ms step_avg:53.34ms
step:1590/2035 train_time:84849ms step_avg:53.36ms
step:1591/2035 train_time:84937ms step_avg:53.39ms
step:1592/2035 train_time:85024ms step_avg:53.41ms
step:1593/2035 train_time:85113ms step_avg:53.43ms
step:1594/2035 train_time:85200ms step_avg:53.45ms
step:1595/2035 train_time:85289ms step_avg:53.47ms
step:1596/2035 train_time:85376ms step_avg:53.49ms
step:1597/2035 train_time:85465ms step_avg:53.52ms
step:1598/2035 train_time:85552ms step_avg:53.54ms
step:1599/2035 train_time:85641ms step_avg:53.56ms
step:1600/2035 train_time:85727ms step_avg:53.58ms
step:1601/2035 train_time:85815ms step_avg:53.60ms
step:1602/2035 train_time:85902ms step_avg:53.62ms
step:1603/2035 train_time:85990ms step_avg:53.64ms
step:1604/2035 train_time:86077ms step_avg:53.66ms
step:1605/2035 train_time:86165ms step_avg:53.69ms
step:1606/2035 train_time:86252ms step_avg:53.71ms
step:1607/2035 train_time:86340ms step_avg:53.73ms
step:1608/2035 train_time:86427ms step_avg:53.75ms
step:1609/2035 train_time:86515ms step_avg:53.77ms
step:1610/2035 train_time:86603ms step_avg:53.79ms
step:1611/2035 train_time:86691ms step_avg:53.81ms
step:1612/2035 train_time:86778ms step_avg:53.83ms
step:1613/2035 train_time:86866ms step_avg:53.85ms
step:1614/2035 train_time:86953ms step_avg:53.87ms
step:1615/2035 train_time:87041ms step_avg:53.90ms
step:1616/2035 train_time:87128ms step_avg:53.92ms
step:1617/2035 train_time:87216ms step_avg:53.94ms
step:1618/2035 train_time:87302ms step_avg:53.96ms
step:1619/2035 train_time:87391ms step_avg:53.98ms
step:1620/2035 train_time:87478ms step_avg:54.00ms
step:1621/2035 train_time:87567ms step_avg:54.02ms
step:1622/2035 train_time:87654ms step_avg:54.04ms
step:1623/2035 train_time:87744ms step_avg:54.06ms
step:1624/2035 train_time:87831ms step_avg:54.08ms
step:1625/2035 train_time:87919ms step_avg:54.10ms
step:1626/2035 train_time:88007ms step_avg:54.12ms
step:1627/2035 train_time:88094ms step_avg:54.14ms
step:1628/2035 train_time:88182ms step_avg:54.17ms
step:1629/2035 train_time:88270ms step_avg:54.19ms
step:1630/2035 train_time:88356ms step_avg:54.21ms
step:1631/2035 train_time:88445ms step_avg:54.23ms
step:1632/2035 train_time:88532ms step_avg:54.25ms
step:1633/2035 train_time:88620ms step_avg:54.27ms
step:1634/2035 train_time:88706ms step_avg:54.29ms
step:1635/2035 train_time:88795ms step_avg:54.31ms
step:1636/2035 train_time:88884ms step_avg:54.33ms
step:1637/2035 train_time:88972ms step_avg:54.35ms
step:1638/2035 train_time:89059ms step_avg:54.37ms
step:1639/2035 train_time:89147ms step_avg:54.39ms
step:1640/2035 train_time:89235ms step_avg:54.41ms
step:1641/2035 train_time:89323ms step_avg:54.43ms
step:1642/2035 train_time:89409ms step_avg:54.45ms
step:1643/2035 train_time:89498ms step_avg:54.47ms
step:1644/2035 train_time:89586ms step_avg:54.49ms
step:1645/2035 train_time:89674ms step_avg:54.51ms
step:1646/2035 train_time:89761ms step_avg:54.53ms
step:1647/2035 train_time:89849ms step_avg:54.55ms
step:1648/2035 train_time:89936ms step_avg:54.57ms
step:1649/2035 train_time:90024ms step_avg:54.59ms
step:1650/2035 train_time:90110ms step_avg:54.61ms
step:1651/2035 train_time:90199ms step_avg:54.63ms
step:1652/2035 train_time:90286ms step_avg:54.65ms
step:1653/2035 train_time:90375ms step_avg:54.67ms
step:1654/2035 train_time:90462ms step_avg:54.69ms
step:1655/2035 train_time:90550ms step_avg:54.71ms
step:1656/2035 train_time:90637ms step_avg:54.73ms
step:1657/2035 train_time:90726ms step_avg:54.75ms
step:1658/2035 train_time:90813ms step_avg:54.77ms
step:1659/2035 train_time:90902ms step_avg:54.79ms
step:1660/2035 train_time:90989ms step_avg:54.81ms
step:1661/2035 train_time:91077ms step_avg:54.83ms
step:1662/2035 train_time:91165ms step_avg:54.85ms
step:1663/2035 train_time:91253ms step_avg:54.87ms
step:1664/2035 train_time:91339ms step_avg:54.89ms
step:1665/2035 train_time:91428ms step_avg:54.91ms
step:1666/2035 train_time:91514ms step_avg:54.93ms
step:1667/2035 train_time:91602ms step_avg:54.95ms
step:1668/2035 train_time:91689ms step_avg:54.97ms
step:1669/2035 train_time:91777ms step_avg:54.99ms
step:1670/2035 train_time:91864ms step_avg:55.01ms
step:1671/2035 train_time:91952ms step_avg:55.03ms
step:1672/2035 train_time:92039ms step_avg:55.05ms
step:1673/2035 train_time:92128ms step_avg:55.07ms
step:1674/2035 train_time:92215ms step_avg:55.09ms
step:1675/2035 train_time:92303ms step_avg:55.11ms
step:1676/2035 train_time:92391ms step_avg:55.13ms
step:1677/2035 train_time:92480ms step_avg:55.15ms
step:1678/2035 train_time:92567ms step_avg:55.16ms
step:1679/2035 train_time:92654ms step_avg:55.18ms
step:1680/2035 train_time:92741ms step_avg:55.20ms
step:1681/2035 train_time:92829ms step_avg:55.22ms
step:1682/2035 train_time:92916ms step_avg:55.24ms
step:1683/2035 train_time:93005ms step_avg:55.26ms
step:1684/2035 train_time:93092ms step_avg:55.28ms
step:1685/2035 train_time:93180ms step_avg:55.30ms
step:1686/2035 train_time:93267ms step_avg:55.32ms
step:1687/2035 train_time:93356ms step_avg:55.34ms
step:1688/2035 train_time:93443ms step_avg:55.36ms
step:1689/2035 train_time:93530ms step_avg:55.38ms
step:1690/2035 train_time:93618ms step_avg:55.40ms
step:1691/2035 train_time:93706ms step_avg:55.41ms
step:1692/2035 train_time:93793ms step_avg:55.43ms
step:1693/2035 train_time:93882ms step_avg:55.45ms
step:1694/2035 train_time:93969ms step_avg:55.47ms
step:1695/2035 train_time:94057ms step_avg:55.49ms
step:1696/2035 train_time:94144ms step_avg:55.51ms
step:1697/2035 train_time:94234ms step_avg:55.53ms
step:1698/2035 train_time:94320ms step_avg:55.55ms
step:1699/2035 train_time:94408ms step_avg:55.57ms
step:1700/2035 train_time:94495ms step_avg:55.59ms
step:1701/2035 train_time:94583ms step_avg:55.60ms
step:1702/2035 train_time:94671ms step_avg:55.62ms
step:1703/2035 train_time:94760ms step_avg:55.64ms
step:1704/2035 train_time:94846ms step_avg:55.66ms
step:1705/2035 train_time:94935ms step_avg:55.68ms
step:1706/2035 train_time:95022ms step_avg:55.70ms
step:1707/2035 train_time:95110ms step_avg:55.72ms
step:1708/2035 train_time:95197ms step_avg:55.74ms
step:1709/2035 train_time:95285ms step_avg:55.75ms
step:1710/2035 train_time:95372ms step_avg:55.77ms
step:1711/2035 train_time:95460ms step_avg:55.79ms
step:1712/2035 train_time:95547ms step_avg:55.81ms
step:1713/2035 train_time:95635ms step_avg:55.83ms
step:1714/2035 train_time:95722ms step_avg:55.85ms
step:1715/2035 train_time:95810ms step_avg:55.87ms
step:1716/2035 train_time:95897ms step_avg:55.88ms
step:1717/2035 train_time:95986ms step_avg:55.90ms
step:1718/2035 train_time:96074ms step_avg:55.92ms
step:1719/2035 train_time:96161ms step_avg:55.94ms
step:1720/2035 train_time:96248ms step_avg:55.96ms
step:1721/2035 train_time:96336ms step_avg:55.98ms
step:1722/2035 train_time:96422ms step_avg:55.99ms
step:1723/2035 train_time:96510ms step_avg:56.01ms
step:1724/2035 train_time:96597ms step_avg:56.03ms
step:1725/2035 train_time:96687ms step_avg:56.05ms
step:1726/2035 train_time:96775ms step_avg:56.07ms
step:1727/2035 train_time:96862ms step_avg:56.09ms
step:1728/2035 train_time:96949ms step_avg:56.10ms
step:1729/2035 train_time:97038ms step_avg:56.12ms
step:1730/2035 train_time:97125ms step_avg:56.14ms
step:1731/2035 train_time:97213ms step_avg:56.16ms
step:1732/2035 train_time:97300ms step_avg:56.18ms
step:1733/2035 train_time:97387ms step_avg:56.20ms
step:1734/2035 train_time:97474ms step_avg:56.21ms
step:1735/2035 train_time:97562ms step_avg:56.23ms
step:1736/2035 train_time:97650ms step_avg:56.25ms
step:1737/2035 train_time:97738ms step_avg:56.27ms
step:1738/2035 train_time:97825ms step_avg:56.29ms
step:1739/2035 train_time:97913ms step_avg:56.30ms
step:1740/2035 train_time:98001ms step_avg:56.32ms
step:1741/2035 train_time:98088ms step_avg:56.34ms
step:1742/2035 train_time:98175ms step_avg:56.36ms
step:1743/2035 train_time:98264ms step_avg:56.38ms
step:1744/2035 train_time:98351ms step_avg:56.39ms
step:1745/2035 train_time:98439ms step_avg:56.41ms
step:1746/2035 train_time:98525ms step_avg:56.43ms
step:1747/2035 train_time:98614ms step_avg:56.45ms
step:1748/2035 train_time:98702ms step_avg:56.47ms
step:1749/2035 train_time:98790ms step_avg:56.48ms
step:1750/2035 train_time:98878ms step_avg:56.50ms
step:1750/2035 val_loss:3.3575 train_time:98968ms step_avg:56.55ms
step:1751/2035 train_time:98989ms step_avg:56.53ms
step:1752/2035 train_time:99057ms step_avg:56.54ms
step:1753/2035 train_time:99150ms step_avg:56.56ms
step:1754/2035 train_time:99237ms step_avg:56.58ms
step:1755/2035 train_time:99326ms step_avg:56.60ms
step:1756/2035 train_time:99413ms step_avg:56.61ms
step:1757/2035 train_time:99501ms step_avg:56.63ms
step:1758/2035 train_time:99586ms step_avg:56.65ms
step:1759/2035 train_time:99674ms step_avg:56.66ms
step:1760/2035 train_time:99760ms step_avg:56.68ms
step:1761/2035 train_time:99847ms step_avg:56.70ms
step:1762/2035 train_time:99934ms step_avg:56.72ms
step:1763/2035 train_time:100024ms step_avg:56.74ms
step:1764/2035 train_time:100113ms step_avg:56.75ms
step:1765/2035 train_time:100203ms step_avg:56.77ms
step:1766/2035 train_time:100290ms step_avg:56.79ms
step:1767/2035 train_time:100379ms step_avg:56.81ms
step:1768/2035 train_time:100466ms step_avg:56.82ms
step:1769/2035 train_time:100553ms step_avg:56.84ms
step:1770/2035 train_time:100639ms step_avg:56.86ms
step:1771/2035 train_time:100726ms step_avg:56.88ms
step:1772/2035 train_time:100813ms step_avg:56.89ms
step:1773/2035 train_time:100901ms step_avg:56.91ms
step:1774/2035 train_time:100988ms step_avg:56.93ms
step:1775/2035 train_time:101078ms step_avg:56.95ms
step:1776/2035 train_time:101166ms step_avg:56.96ms
step:1777/2035 train_time:101254ms step_avg:56.98ms
step:1778/2035 train_time:101341ms step_avg:57.00ms
step:1779/2035 train_time:101431ms step_avg:57.02ms
step:1780/2035 train_time:101517ms step_avg:57.03ms
step:1781/2035 train_time:101604ms step_avg:57.05ms
step:1782/2035 train_time:101691ms step_avg:57.07ms
step:1783/2035 train_time:101778ms step_avg:57.08ms
step:1784/2035 train_time:101865ms step_avg:57.10ms
step:1785/2035 train_time:101954ms step_avg:57.12ms
step:1786/2035 train_time:102042ms step_avg:57.13ms
step:1787/2035 train_time:102130ms step_avg:57.15ms
step:1788/2035 train_time:102218ms step_avg:57.17ms
step:1789/2035 train_time:102306ms step_avg:57.19ms
step:1790/2035 train_time:102394ms step_avg:57.20ms
step:1791/2035 train_time:102483ms step_avg:57.22ms
step:1792/2035 train_time:102569ms step_avg:57.24ms
step:1793/2035 train_time:102656ms step_avg:57.25ms
step:1794/2035 train_time:102743ms step_avg:57.27ms
step:1795/2035 train_time:102831ms step_avg:57.29ms
step:1796/2035 train_time:102917ms step_avg:57.30ms
step:1797/2035 train_time:103006ms step_avg:57.32ms
step:1798/2035 train_time:103094ms step_avg:57.34ms
step:1799/2035 train_time:103182ms step_avg:57.36ms
step:1800/2035 train_time:103270ms step_avg:57.37ms
step:1801/2035 train_time:103358ms step_avg:57.39ms
step:1802/2035 train_time:103446ms step_avg:57.41ms
step:1803/2035 train_time:103534ms step_avg:57.42ms
step:1804/2035 train_time:103620ms step_avg:57.44ms
step:1805/2035 train_time:103709ms step_avg:57.46ms
step:1806/2035 train_time:103795ms step_avg:57.47ms
step:1807/2035 train_time:103882ms step_avg:57.49ms
step:1808/2035 train_time:103969ms step_avg:57.50ms
step:1809/2035 train_time:104056ms step_avg:57.52ms
step:1810/2035 train_time:104144ms step_avg:57.54ms
step:1811/2035 train_time:104233ms step_avg:57.56ms
step:1812/2035 train_time:104321ms step_avg:57.57ms
step:1813/2035 train_time:104411ms step_avg:57.59ms
step:1814/2035 train_time:104497ms step_avg:57.61ms
step:1815/2035 train_time:104586ms step_avg:57.62ms
step:1816/2035 train_time:104674ms step_avg:57.64ms
step:1817/2035 train_time:104761ms step_avg:57.66ms
step:1818/2035 train_time:104848ms step_avg:57.67ms
step:1819/2035 train_time:104936ms step_avg:57.69ms
step:1820/2035 train_time:105023ms step_avg:57.70ms
step:1821/2035 train_time:105110ms step_avg:57.72ms
step:1822/2035 train_time:105198ms step_avg:57.74ms
step:1823/2035 train_time:105287ms step_avg:57.75ms
step:1824/2035 train_time:105373ms step_avg:57.77ms
step:1825/2035 train_time:105462ms step_avg:57.79ms
step:1826/2035 train_time:105549ms step_avg:57.80ms
step:1827/2035 train_time:105637ms step_avg:57.82ms
step:1828/2035 train_time:105724ms step_avg:57.84ms
step:1829/2035 train_time:105811ms step_avg:57.85ms
step:1830/2035 train_time:105898ms step_avg:57.87ms
step:1831/2035 train_time:105986ms step_avg:57.88ms
step:1832/2035 train_time:106073ms step_avg:57.90ms
step:1833/2035 train_time:106161ms step_avg:57.92ms
step:1834/2035 train_time:106249ms step_avg:57.93ms
step:1835/2035 train_time:106338ms step_avg:57.95ms
step:1836/2035 train_time:106424ms step_avg:57.97ms
step:1837/2035 train_time:106512ms step_avg:57.98ms
step:1838/2035 train_time:106599ms step_avg:58.00ms
step:1839/2035 train_time:106687ms step_avg:58.01ms
step:1840/2035 train_time:106774ms step_avg:58.03ms
step:1841/2035 train_time:106862ms step_avg:58.05ms
step:1842/2035 train_time:106948ms step_avg:58.06ms
step:1843/2035 train_time:107036ms step_avg:58.08ms
step:1844/2035 train_time:107124ms step_avg:58.09ms
step:1845/2035 train_time:107212ms step_avg:58.11ms
step:1846/2035 train_time:107300ms step_avg:58.13ms
step:1847/2035 train_time:107388ms step_avg:58.14ms
step:1848/2035 train_time:107475ms step_avg:58.16ms
step:1849/2035 train_time:107563ms step_avg:58.17ms
step:1850/2035 train_time:107651ms step_avg:58.19ms
step:1851/2035 train_time:107738ms step_avg:58.21ms
step:1852/2035 train_time:107825ms step_avg:58.22ms
step:1853/2035 train_time:107914ms step_avg:58.24ms
step:1854/2035 train_time:108002ms step_avg:58.25ms
step:1855/2035 train_time:108090ms step_avg:58.27ms
step:1856/2035 train_time:108177ms step_avg:58.29ms
step:1857/2035 train_time:108266ms step_avg:58.30ms
step:1858/2035 train_time:108353ms step_avg:58.32ms
step:1859/2035 train_time:108441ms step_avg:58.33ms
step:1860/2035 train_time:108528ms step_avg:58.35ms
step:1861/2035 train_time:108615ms step_avg:58.36ms
step:1862/2035 train_time:108702ms step_avg:58.38ms
step:1863/2035 train_time:108791ms step_avg:58.40ms
step:1864/2035 train_time:108877ms step_avg:58.41ms
step:1865/2035 train_time:108966ms step_avg:58.43ms
step:1866/2035 train_time:109052ms step_avg:58.44ms
step:1867/2035 train_time:109139ms step_avg:58.46ms
step:1868/2035 train_time:109227ms step_avg:58.47ms
step:1869/2035 train_time:109316ms step_avg:58.49ms
step:1870/2035 train_time:109403ms step_avg:58.50ms
step:1871/2035 train_time:109491ms step_avg:58.52ms
step:1872/2035 train_time:109577ms step_avg:58.53ms
step:1873/2035 train_time:109666ms step_avg:58.55ms
step:1874/2035 train_time:109752ms step_avg:58.57ms
step:1875/2035 train_time:109840ms step_avg:58.58ms
step:1876/2035 train_time:109927ms step_avg:58.60ms
step:1877/2035 train_time:110016ms step_avg:58.61ms
step:1878/2035 train_time:110102ms step_avg:58.63ms
step:1879/2035 train_time:110191ms step_avg:58.64ms
step:1880/2035 train_time:110279ms step_avg:58.66ms
step:1881/2035 train_time:110367ms step_avg:58.67ms
step:1882/2035 train_time:110454ms step_avg:58.69ms
step:1883/2035 train_time:110542ms step_avg:58.71ms
step:1884/2035 train_time:110630ms step_avg:58.72ms
step:1885/2035 train_time:110717ms step_avg:58.74ms
step:1886/2035 train_time:110804ms step_avg:58.75ms
step:1887/2035 train_time:110894ms step_avg:58.77ms
step:1888/2035 train_time:110981ms step_avg:58.78ms
step:1889/2035 train_time:111069ms step_avg:58.80ms
step:1890/2035 train_time:111155ms step_avg:58.81ms
step:1891/2035 train_time:111244ms step_avg:58.83ms
step:1892/2035 train_time:111332ms step_avg:58.84ms
step:1893/2035 train_time:111419ms step_avg:58.86ms
step:1894/2035 train_time:111506ms step_avg:58.87ms
step:1895/2035 train_time:111596ms step_avg:58.89ms
step:1896/2035 train_time:111683ms step_avg:58.90ms
step:1897/2035 train_time:111772ms step_avg:58.92ms
step:1898/2035 train_time:111858ms step_avg:58.93ms
step:1899/2035 train_time:111946ms step_avg:58.95ms
step:1900/2035 train_time:112033ms step_avg:58.96ms
step:1901/2035 train_time:112121ms step_avg:58.98ms
step:1902/2035 train_time:112208ms step_avg:58.99ms
step:1903/2035 train_time:112296ms step_avg:59.01ms
step:1904/2035 train_time:112383ms step_avg:59.02ms
step:1905/2035 train_time:112471ms step_avg:59.04ms
step:1906/2035 train_time:112558ms step_avg:59.05ms
step:1907/2035 train_time:112646ms step_avg:59.07ms
step:1908/2035 train_time:112732ms step_avg:59.08ms
step:1909/2035 train_time:112820ms step_avg:59.10ms
step:1910/2035 train_time:112907ms step_avg:59.11ms
step:1911/2035 train_time:112996ms step_avg:59.13ms
step:1912/2035 train_time:113083ms step_avg:59.14ms
step:1913/2035 train_time:113171ms step_avg:59.16ms
step:1914/2035 train_time:113258ms step_avg:59.17ms
step:1915/2035 train_time:113347ms step_avg:59.19ms
step:1916/2035 train_time:113434ms step_avg:59.20ms
step:1917/2035 train_time:113522ms step_avg:59.22ms
step:1918/2035 train_time:113609ms step_avg:59.23ms
step:1919/2035 train_time:113698ms step_avg:59.25ms
step:1920/2035 train_time:113785ms step_avg:59.26ms
step:1921/2035 train_time:113874ms step_avg:59.28ms
step:1922/2035 train_time:113961ms step_avg:59.29ms
step:1923/2035 train_time:114049ms step_avg:59.31ms
step:1924/2035 train_time:114135ms step_avg:59.32ms
step:1925/2035 train_time:114223ms step_avg:59.34ms
step:1926/2035 train_time:114310ms step_avg:59.35ms
step:1927/2035 train_time:114399ms step_avg:59.37ms
step:1928/2035 train_time:114485ms step_avg:59.38ms
step:1929/2035 train_time:114574ms step_avg:59.40ms
step:1930/2035 train_time:114661ms step_avg:59.41ms
step:1931/2035 train_time:114749ms step_avg:59.42ms
step:1932/2035 train_time:114836ms step_avg:59.44ms
step:1933/2035 train_time:114924ms step_avg:59.45ms
step:1934/2035 train_time:115012ms step_avg:59.47ms
step:1935/2035 train_time:115099ms step_avg:59.48ms
step:1936/2035 train_time:115187ms step_avg:59.50ms
step:1937/2035 train_time:115274ms step_avg:59.51ms
step:1938/2035 train_time:115362ms step_avg:59.53ms
step:1939/2035 train_time:115450ms step_avg:59.54ms
step:1940/2035 train_time:115537ms step_avg:59.56ms
step:1941/2035 train_time:115625ms step_avg:59.57ms
step:1942/2035 train_time:115712ms step_avg:59.58ms
step:1943/2035 train_time:115800ms step_avg:59.60ms
step:1944/2035 train_time:115887ms step_avg:59.61ms
step:1945/2035 train_time:115975ms step_avg:59.63ms
step:1946/2035 train_time:116064ms step_avg:59.64ms
step:1947/2035 train_time:116152ms step_avg:59.66ms
step:1948/2035 train_time:116239ms step_avg:59.67ms
step:1949/2035 train_time:116327ms step_avg:59.69ms
step:1950/2035 train_time:116414ms step_avg:59.70ms
step:1951/2035 train_time:116502ms step_avg:59.71ms
step:1952/2035 train_time:116589ms step_avg:59.73ms
step:1953/2035 train_time:116677ms step_avg:59.74ms
step:1954/2035 train_time:116764ms step_avg:59.76ms
step:1955/2035 train_time:116853ms step_avg:59.77ms
step:1956/2035 train_time:116940ms step_avg:59.79ms
step:1957/2035 train_time:117029ms step_avg:59.80ms
step:1958/2035 train_time:117117ms step_avg:59.81ms
step:1959/2035 train_time:117205ms step_avg:59.83ms
step:1960/2035 train_time:117292ms step_avg:59.84ms
step:1961/2035 train_time:117380ms step_avg:59.86ms
step:1962/2035 train_time:117466ms step_avg:59.87ms
step:1963/2035 train_time:117554ms step_avg:59.88ms
step:1964/2035 train_time:117641ms step_avg:59.90ms
step:1965/2035 train_time:117729ms step_avg:59.91ms
step:1966/2035 train_time:117816ms step_avg:59.93ms
step:1967/2035 train_time:117904ms step_avg:59.94ms
step:1968/2035 train_time:117991ms step_avg:59.95ms
step:1969/2035 train_time:118079ms step_avg:59.97ms
step:1970/2035 train_time:118166ms step_avg:59.98ms
step:1971/2035 train_time:118254ms step_avg:60.00ms
step:1972/2035 train_time:118341ms step_avg:60.01ms
step:1973/2035 train_time:118430ms step_avg:60.03ms
step:1974/2035 train_time:118516ms step_avg:60.04ms
step:1975/2035 train_time:118605ms step_avg:60.05ms
step:1976/2035 train_time:118692ms step_avg:60.07ms
step:1977/2035 train_time:118780ms step_avg:60.08ms
step:1978/2035 train_time:118867ms step_avg:60.09ms
step:1979/2035 train_time:118956ms step_avg:60.11ms
step:1980/2035 train_time:119044ms step_avg:60.12ms
step:1981/2035 train_time:119132ms step_avg:60.14ms
step:1982/2035 train_time:119218ms step_avg:60.15ms
step:1983/2035 train_time:119306ms step_avg:60.16ms
step:1984/2035 train_time:119393ms step_avg:60.18ms
step:1985/2035 train_time:119482ms step_avg:60.19ms
step:1986/2035 train_time:119569ms step_avg:60.21ms
step:1987/2035 train_time:119657ms step_avg:60.22ms
step:1988/2035 train_time:119744ms step_avg:60.23ms
step:1989/2035 train_time:119832ms step_avg:60.25ms
step:1990/2035 train_time:119919ms step_avg:60.26ms
step:1991/2035 train_time:120008ms step_avg:60.28ms
step:1992/2035 train_time:120095ms step_avg:60.29ms
step:1993/2035 train_time:120183ms step_avg:60.30ms
step:1994/2035 train_time:120270ms step_avg:60.32ms
step:1995/2035 train_time:120359ms step_avg:60.33ms
step:1996/2035 train_time:120446ms step_avg:60.34ms
step:1997/2035 train_time:120535ms step_avg:60.36ms
step:1998/2035 train_time:120622ms step_avg:60.37ms
step:1999/2035 train_time:120710ms step_avg:60.39ms
step:2000/2035 train_time:120797ms step_avg:60.40ms
step:2000/2035 val_loss:3.2839 train_time:120887ms step_avg:60.44ms
step:2001/2035 train_time:120907ms step_avg:60.42ms
step:2002/2035 train_time:120977ms step_avg:60.43ms
step:2003/2035 train_time:121068ms step_avg:60.44ms
step:2004/2035 train_time:121154ms step_avg:60.46ms
step:2005/2035 train_time:121241ms step_avg:60.47ms
step:2006/2035 train_time:121328ms step_avg:60.48ms
step:2007/2035 train_time:121416ms step_avg:60.50ms
step:2008/2035 train_time:121502ms step_avg:60.51ms
step:2009/2035 train_time:121591ms step_avg:60.52ms
step:2010/2035 train_time:121678ms step_avg:60.54ms
step:2011/2035 train_time:121767ms step_avg:60.55ms
step:2012/2035 train_time:121856ms step_avg:60.56ms
step:2013/2035 train_time:121947ms step_avg:60.58ms
step:2014/2035 train_time:122036ms step_avg:60.59ms
step:2015/2035 train_time:122124ms step_avg:60.61ms
step:2016/2035 train_time:122211ms step_avg:60.62ms
step:2017/2035 train_time:122298ms step_avg:60.63ms
step:2018/2035 train_time:122384ms step_avg:60.65ms
step:2019/2035 train_time:122471ms step_avg:60.66ms
step:2020/2035 train_time:122558ms step_avg:60.67ms
step:2021/2035 train_time:122647ms step_avg:60.69ms
step:2022/2035 train_time:122735ms step_avg:60.70ms
step:2023/2035 train_time:122824ms step_avg:60.71ms
step:2024/2035 train_time:122912ms step_avg:60.73ms
step:2025/2035 train_time:123002ms step_avg:60.74ms
step:2026/2035 train_time:123090ms step_avg:60.76ms
step:2027/2035 train_time:123179ms step_avg:60.77ms
step:2028/2035 train_time:123266ms step_avg:60.78ms
step:2029/2035 train_time:123353ms step_avg:60.80ms
step:2030/2035 train_time:123440ms step_avg:60.81ms
step:2031/2035 train_time:123528ms step_avg:60.82ms
step:2032/2035 train_time:123615ms step_avg:60.83ms
step:2033/2035 train_time:123703ms step_avg:60.85ms
step:2034/2035 train_time:123791ms step_avg:60.86ms
step:2035/2035 train_time:123881ms step_avg:60.87ms
step:2035/2035 val_loss:3.2767 train_time:123970ms step_avg:60.92ms
peak memory allocated: 29634 MiB reserved: 38376 MiB

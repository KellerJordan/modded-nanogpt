import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from collections import defaultdict
from itertools import accumulate
from pathlib import Path
import gc

os.environ["PYTORCH_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64


# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng

@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor, split_baddbmm: bool = False):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    # Select batched vs unbatched
    if split_baddbmm:
        BX_matmul = torch.bmm if X.ndim > 2 else torch.mm
    else:
        aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A

        # Referencing X twice causes pytorch to make a defensive copy,
        # resulting in a cudaMemcpyAsync in baddbmm.
        # For large matrices (i.e., the mlp weights), it's faster to split
        # the operation into two kernels to avoid this.
        if split_baddbmm:
            BX_matmul(B, X, out=C)  # C = B @ X
            C.add_(X, alpha=a)      # C = C + a*X  (in-place, X only read)
        else:
            aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X

        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X


# -----------------------------------------------------------------------------
# Compiled helpers for NorMuon by @chrisjmccormick

@torch.compile(dynamic=False, fullgraph=True)
def cautious_wd_and_update_inplace(p, v, wd_tensor, lr_tensor):
    """Cautious weight decay + parameter update. wd_tensor and lr_tensor are 0-D CPU tensors."""
    mask = (v * p) >= 0
    wd_factor = wd_tensor.to(p.dtype)
    lr_factor = lr_tensor.to(p.dtype)
    p.copy_(p - (p * mask * wd_factor * lr_factor) - (v * lr_factor))


@torch.compile(dynamic=False, fullgraph=True)
def apply_normuon_variance_reduction(v_chunk, second_momentum_buffer, beta2, red_dim):
    """NorMuon variance reduction. Algebraically fuses the normalization steps to minimize memory ops."""
    v_mean = v_chunk.float().square().mean(dim=red_dim, keepdim=True)
    red_dim_size = v_chunk.size(red_dim)
    v_norm_sq = v_mean.sum(dim=(-2, -1), keepdim=True).mul_(red_dim_size)
    v_norm = v_norm_sq.sqrt_()
    second_momentum_buffer.lerp_(v_mean.to(dtype=second_momentum_buffer.dtype), 1 - beta2)
    step_size = second_momentum_buffer.clamp_min(1e-10).rsqrt_()
    scaled_sq_sum = (v_mean * red_dim_size) * step_size.float().square()
    v_norm_new = scaled_sq_sum.sum(dim=(-2, -1), keepdim=True).sqrt_()
    final_scale = step_size * (v_norm / v_norm_new.clamp_min_(1e-10))
    return v_chunk.mul_(final_scale.type_as(v_chunk))


# -----------------------------------------------------------------------------
# NorMuon optimizer

class NorMuon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).

    Differences from standard Muon:
    - Newton-Shulz is replaced with Polar Express for the orthogonalization step
    - NorMuon adds a low-rank variance estimator similar to Adafactor. https://arxiv.org/pdf/2510.05491
    - small 1D parameters handled here instead of in Adam
    - Cautious weight decay, a gated version of decoupled weight decay
    - Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as
    needed on the forward pass. This enables attn and mlp weights to be contained within the same
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn.
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, beta2=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, beta2=beta2)
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def reset(self):
        # expose a reset for clearing buffers
        for group in self.param_groups:
            group["momentum_buffer"].zero_()
            group["second_momentum_buffer"].zero_()

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per module.
        """
        groups = defaultdict(list)
        for param in params:
            groups[param.label].append(param)

        param_groups = []
        for module_name, group_params in groups.items():
            chunk_size = (len(group_params) + self.world_size - 1) // self.world_size
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))

        return param_groups

    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn
        and mlp params when a param group is split across GPUs.
        """
        module_group_order = ['smear_gate', 'attn_gate', 'attn', 'mlp']
        params_list = list(params)
        params_list.sort(key=lambda x: module_group_order.index(x.label))

        idx = 0
        group_sizes = [1, 10, 16, 16]
        assert len(params_list) == sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            chunk_size = (size + self.world_size - 1) // self.world_size
            group_params = params_list[idx: idx + size]
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))
            idx += size

        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient distributed step by @YouJiacheng, @KonstantinWilleke, @alexrgilbert,
        # @adricarda, @tuttyfrutyee, @vdlad, @ryanyang0, @vagrawal, @varunneal, @chrisjmccormick
        rank = dist.get_rank()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            stacked_grads = torch.empty(
                (padded_num_params, *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device
            )
            for i, p in enumerate(params):
                stacked_grads[i].copy_(p.grad, non_blocking=True)
            if len(params) < padded_num_params:
                stacked_grads[len(params):].zero_()

            grad_chunk = torch.empty_like(stacked_grads[:chunk_size])

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(dict(grad_chunk=grad_chunk, reduce_future=reduce_future))

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = group["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            start_idx = rank * chunk_size
            module_idx = start_idx if start_idx < len(params) else 0

            num_params = min(chunk_size, max(0, len(params) - start_idx))  # num params for this rank

            if "momentum_buffer" not in group:
                group["momentum_buffer"]  = torch.zeros_like(grad_chunk[:num_params])
            momentum_buffer = group["momentum_buffer"]
            # Apply momentum update to the persistent momentum buffer in-place
            momentum_buffer.lerp_(grad_chunk[:num_params], 1 - group["momentum"])
            updated_grads = grad_chunk[:num_params].lerp_(momentum_buffer, group["momentum"])

            grad_shape = updated_grads.shape
            if params[module_idx].label == 'attn':

                for p in params[module_idx:module_idx + num_params]:
                    assert p.label == 'attn'

                updated_grads = updated_grads.view(4 * grad_shape[0], grad_shape[1] // 4, grad_shape[2])

            ref_param = params[module_idx]
            param_shape = ref_param.shape

            # The below shape-based heuristic assumes that matrices have their input along the
            # row dimension and their output along the columns. Gates are an exception.
            is_gate = ref_param.label in ['smear_gate', 'attn_gate']

            if "second_momentum_buffer" not in group:
                if is_gate:
                    group["second_momentum_buffer"] = torch.zeros_like(updated_grads[..., :, :1])
                else:
                    group["second_momentum_buffer"] = (torch.zeros_like(updated_grads[..., :, :1])
                        if param_shape[-2] >= param_shape[-1] else torch.zeros_like(updated_grads[..., :1, :])
                    )
            second_momentum_buffer = group["second_momentum_buffer"]

            if "param_lr_cpu" not in group:
                # Define multipliers for ALL params in this group (global, not per-shard)
                lr_mults = []
                wd_mults = []
                for p in params:
                    # Increase learning rate for modules with larger inputs than outputs.
                    # This shape check also assumes rows=input, columns=output, so take care
                    # when changing memory layouts. @chrisjmccormick
                    shape = p.shape
                    if len(shape) >= 2:
                        shape_mult = max(1.0, shape[-2] / shape[-1]) ** 0.5
                    else:
                        shape_mult = 1.0
                    lr_mults.append(shape_mult * getattr(p, "lr_mul", 1.0))
                    wd_mults.append(getattr(p, "wd_mul", 1.0))
                # Define as cpu tensors to enable Inductor constant folding
                group["param_lr_cpu"] = torch.tensor(lr_mults, dtype=torch.float32, device="cpu")
                group["param_wd_cpu"] = torch.tensor(wd_mults, dtype=torch.float32, device="cpu")

            eff_lr_all = group["param_lr_cpu"] * group["lr"]
            eff_wd_all = group["param_wd_cpu"] * group["weight_decay"] * group["lr"]

            # Slice the portion corresponding to this rank's shard
            eff_lr_cpu = eff_lr_all[module_idx:module_idx + num_params]
            eff_wd_cpu = eff_wd_all[module_idx:module_idx + num_params]

            # Compute zeropower for the entire chunk in a single, batched call.
            if num_params == 0:
                v_chunk = updated_grads
            else:
                v_chunk = polar_express(updated_grads, split_baddbmm=(ref_param.label == 'mlp'))

            # Note that the head orientation in O is transposed relative to QKV, so red_dim
            # is 'incorrect' for O. However, correcting this showed no improvement. @chrisjmccormick
            red_dim = -1 if (is_gate or param_shape[-2] >= param_shape[-1]) else -2

            v_chunk = apply_normuon_variance_reduction(
                v_chunk, second_momentum_buffer, group["beta2"], red_dim
            )

            v_chunk = v_chunk.view(grad_shape)

            # # "Cautious" weight decay (https://arxiv.org/abs/2510.12402)
            updated_params = torch.empty_like(grad_chunk)
            if num_params > 0:
                # Work on a stacked copy to avoid touching original params
                param_chunk = torch.stack(params[module_idx:module_idx + num_params])

                for local_idx in range(num_params):
                    cautious_wd_and_update_inplace(
                        param_chunk[local_idx],
                        v_chunk[local_idx],
                        eff_wd_cpu[local_idx],
                        eff_lr_cpu[local_idx],
                    )
            else:
                param_chunk = torch.zeros_like(v_chunk)

            updated_params[:num_params].copy_(param_chunk)
            if num_params < chunk_size:
                updated_params[num_params:].zero_()

            stacked_params = torch.empty(
                (padded_num_params, *param_shape),
                dtype=updated_params.dtype,
                device=updated_params.device,
            )

            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_params, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        # Group by label, with explicit ordering for execution control.
        label_order = ['lm_head', 'value_embed', 'scalars']
        params_by_label = defaultdict(list)
        for p in params:
            params_by_label[getattr(p, 'label', None)].append(p)
        param_groups = []
        for label in label_order:
            if label in params_by_label:
                param_groups.append(dict(params=params_by_label[label]))
        # include any unlabeled params at the end (processed last)
        if None in params_by_label:
            param_groups.append(dict(params=params_by_label[None]))
        super().__init__(param_groups, defaults)
        # init state
        for p in params:
            chunk_size = p.size(0) // self.world_size
            exp_avg = torch.zeros_like(p[:chunk_size], dtype=torch.bfloat16, device=p[0].device)
            exp_avg_sq = torch.zeros_like(exp_avg)
            self.state[p] = dict(step=0, exp_avg=exp_avg, exp_avg_sq=exp_avg_sq)
        # DistributedAdam implementation by @vagrawal, @akash5474

        self.should_sync = False

        self._reduce_scatter_hooks = []
        self._reduce_scatter_futures = {}
        self.register_backward_hooks()

    def register_backward_hooks(self):
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                hook = param.register_post_accumulate_grad_hook(self._sync_gradient)
                self._reduce_scatter_hooks.append(hook)

    @torch.no_grad()
    def _sync_gradient(self, param):
        if not self.should_sync:
            return

        grad = param.grad
        rank_size = grad.shape[0] // self.world_size
        grad_slice = torch.empty_like(grad[:rank_size])
        self._reduce_scatter_futures[param] = (
            dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future(),
            grad_slice
        )

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        all_gather_futures: list[torch.Future] = []

        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            for param in group['params']:
                if param not in self._reduce_scatter_futures:
                    continue

                fut, g_slice = self._reduce_scatter_futures[param]
                fut.wait()

                rank_size = param.shape[0] // self.world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]

                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (bias2 ** 0.5 / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                # cautious weight decay
                mask = (update * p_slice) >= 0
                # lr as weight decay schedule
                eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                update.addcmul_(p_slice, mask, value=eff_weight_decay * lr)

                p_slice.add_(other=update, alpha=-1.0)

                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())

        self._reduce_scatter_futures.clear()
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        with torch.no_grad():
            self.weight.zero_()  # @Grad62304977 and others

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))


# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float
    key_shift: bool

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKVO weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # Simplified layout by @chrisjmccormick
        self.qkvo_w = nn.Parameter(torch.empty(self.dim * 4, self.hdim))
        # label all modules for explicit optimizer grouping
        self.qkvo_w.label = 'attn'

        with torch.no_grad():
            self.qkvo_w[:self.dim * 3].uniform_(-bound, bound)  # init QKV weights
            self.qkvo_w[self.dim * 3:].zero_()  # init O weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        self.attn_gate.weight.label = 'attn_gate'

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas, key_shift = attn_args.ve, attn_args.sa_lambdas, attn_args.key_shift
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, sa_lambdas[0] * self.qkvo_w[:self.dim * 3].type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if key_shift:
            # shift keys forward for the stationary head dims. Enables 1-layer induction.
            k[:, 1:, :, self.head_dim//4:self.head_dim//2] = k[:, :-1, :, self.head_dim//4:self.head_dim//2]
            k[:, 1:, :, self.head_dim//4+self.head_dim//2:] = k[:, :-1, :, self.head_dim//4+self.head_dim//2:]
        if ve is not None:
            v = v + ve.view_as(v) # @ KoszarskyB & @Grad62304977

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens,
                                                        max_seqlen_q=max_len, max_seqlen_k=max_len,
                                                        causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, sa_lambdas[1] * self.qkvo_w[self.dim * 3:].type_as(y))  # sa_lambdas[1] pre-multiplied to O @shenberg
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # Transposed layout to match attention weights
        self.c_fc = nn.Parameter(torch.empty(hdim, dim))
        self.c_proj = nn.Parameter(torch.empty(hdim, dim))
        # label all modules for explicit optimizer grouping
        self.c_fc.label = 'mlp'
        self.c_proj.label = 'mlp'
        self.c_proj.lr_mul = 2.

        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.T.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.6 (the 7th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx != 6 else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim)

    def forward(self, x: Tensor, attn_args: AttnArgs):
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        self.num_layers = num_layers
        vocab_size = next_multiple_of_n(vocab_size, n=128)

        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.label = 'smear_gate'

        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        for embed in self.value_embeds:
            nn.init.zeros_(embed.weight)
        for ve in self.value_embeds:
            ve.weight.label = 'value_embed'
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)

        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=100/448, w_s=1.5/448, grad_s=0.75/448)
        nn.init.normal_(self.lm_head.weight, mean=0, std=0.005)
        self.lm_head.weight.label = 'lm_head'
        # Add learnable skip connection weights for decoder layers
        pad = (-num_layers * 4 - 3) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    1.1 * torch.ones(num_layers),  # resid lambdas. 1.1 init such that layer i weight is i^(num_layers-i).
                    0 * torch.ones(num_layers),  # x0 lambdas
                    *[torch.tensor([0.5, 1.0]) for _ in range(num_layers)],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    -1.5 * torch.ones(1),  # skip_lambda -> σ(-1.5) ≈ 0.18
                    torch.ones(pad),
                ]
            )
        )

        self.scalars.label = 'scalars'
        # set learning rates
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.scalars.lr_mul = 5.0
        self.scalars.wd_mul = 0.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        # set configs
        skip_connections = []
        skip_in = [3] # long attention window on layer 3
        skip_out = [6] # no attn op on layer 6
        x_backout = None
        backout_layer = 7

        # set lambdas
        resid_lambdas = self.scalars[: 1 * self.num_layers]
        x0_lambdas = self.scalars[1 * self.num_layers: 2 * self.num_layers]
        sa_lambdas = self.scalars[2 * self.num_layers: 4 * self.num_layers].view(-1, 2)
        smear_lambda = self.scalars[4 * self.num_layers]
        backout_lambda = self.scalars[4 * self.num_layers+1]
        skip_lambda = self.scalars[4 * self.num_layers+2]

        # set block masks and key shift
        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == self.num_layers
        key_shift = [b==long_bm for b in bm_sizes] # apply key shift to long windows

        # weight-tied: use lm_head.weight for embedding lookup
        x = F.embedding(input_seq, self.lm_head.weight)
        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [ve[1], ve[2]] + [None] * (self.num_layers - 5) + [ve[0], ve[1], ve[2]]
        assert len(ve) == self.num_layers

        # smear token embed forward 1 position @classiclarryd
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        for i in range(self.num_layers):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale,
                key_shift = key_shift[i]
            )
            if i in skip_out:
                gate = torch.sigmoid(skip_lambda)  # in (0, 1)
                x = x + gate * skip_connections.pop()
            if i == 0:
                x = (resid_lambdas[0] + x0_lambdas[0]) * x
            else:
                x = resid_lambdas[i] * x + x0_lambdas[i] * x0
            x = self.blocks[i](x, attn_args)
            if i in skip_in:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 7 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()

    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        # Cast to int32 on CPU before transfer to avoid dtype conversion during .to()
        _inputs = _inputs.to(dtype=torch.int32)
        _targets = _targets.to(dtype=torch.int64)
        _cum_lengths = _cum_lengths.to(dtype=torch.int32)

        new_params = yield (
            _inputs.to(device="cuda", non_blocking=True),
            _targets.to(device="cuda", non_blocking=True),
            _cum_lengths.to(device="cuda", non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * new_grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens // new_grad_accum_steps
            max_seq_len = new_max_seq_len


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # batch sizes
    train_bs_schedule: tuple = (8 * 2048 * 8, 16 * 2048 * 8, 24 * 2048 * 8)
    train_bs_extension: int = 24 * 2048 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 1995  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: float = 0.55  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_final: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=11,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=args.val_batch_size // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.weight.data = m.weight.data.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    embed_params + scalar_params + head_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.005,
)
optimizer2 = NorMuon(hidden_matrix_params + gate_params, lr=0.023, momentum=0.95, beta2=0.95, weight_decay=1.2)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: tied to batch size schedule, with cooldown at the end.
def get_lr(step: int):
    if step > args.num_scheduled_iterations:
        return 0.1
    lr_max = 1.0
    x = step / args.num_scheduled_iterations
    if x > 1/3:
       lr_max = 1.51  # (16/8)**0.6
    if x > 2/3:
        lr_max = 1.93  # (24/8)**0.6
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = lr_max * w + (1 - w) * 0.1
        return lr
    return lr_max

def get_ws(step: int):
    # set short window size to half of long window size
    # Higher ws on "extension" steps
    if step >= args.num_scheduled_iterations:
        return args.ws_final // 2, args.ws_final
    x = step / args.num_scheduled_iterations
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]


def get_bs(step: int):
    if step >= args.num_scheduled_iterations:
        return args.train_bs_extension
    x = step / args.num_scheduled_iterations
    bs_idx = int(len(args.train_bs_schedule) * x)
    return args.train_bs_schedule[bs_idx]


def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)
        # disable sync in the next training step for the adam optimizer
        optimizers[0].should_sync = False

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 10
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_bs_schedule[0], args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
ws_schedule = list(args.ws_schedule) + [args.ws_final]
bs_schedule = list(args.train_bs_schedule) + [args.train_bs_extension]
ws_long = ws_schedule[0]
model.train()
model.yarn.reset()
assert len(ws_schedule) == len(bs_schedule), "This warmup assumes len(ws_schedule) == len(bs_schedule)"

for idx in range(len(ws_schedule)):
    send_args = None
    if idx != 0:
        new_ws_long = ws_schedule[idx]
        model.yarn.apply(ws_long, new_ws_long)
        ws_long = new_ws_long
        send_args = (bs_schedule[idx], args.train_max_seq_len, grad_accum_steps)
    for step in range(warmup_steps):
        inputs, targets, cum_seqlens = train_loader.send(send_args)
        if step % 2 == 1:
            optimizers[0].should_sync = True
        model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
        if step % 2 == 0:
            optimizers[1].step()
            optimizers[1].zero_grad(set_to_none=True)
        else:
            for opt in optimizers:
                opt.step()
            model.zero_grad(set_to_none=True)
            optimizers[0].should_sync = False

model.zero_grad(set_to_none=True)
optimizers[0].should_sync = False
model.eval()

# warm up validation too
val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
val_loss = 0
with torch.no_grad():
    for step in range(val_steps):
        inputs, targets, cum_seqlens = next(val_loader)
        ws_idx = step % len(ws_schedule)
        if ws_idx==0:
            model.yarn.reset()
            ws_long = ws_schedule[0]
        else:
            new_ws_long = ws_schedule[ws_idx]
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
        val_loss += model(inputs, targets, cum_seqlens, ws_long // 2, ws_long)

del val_loader, val_loss
model.train()
model.yarn.reset() # rotary buffer is not stored in state_dict
optimizer2.reset() # muon momentum buffers not in state dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

step_batch_size = args.train_bs_schedule[0]
train_loader = distributed_data_generator(args.train_files, step_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)

gc.collect()

training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    new_step_batch_size = get_bs(step)
    send_args = (new_step_batch_size, args.train_max_seq_len, grad_accum_steps) if new_step_batch_size != step_batch_size else None
    step_batch_size = new_step_batch_size
    for idx in range(grad_accum_steps):
        # enable gradient sync for the DistAdam optimizer on the last iteration before we step it
        if idx == grad_accum_steps - 1 and step % 2 == 1:
            optimizers[0].should_sync = True

        inputs, targets, cum_seqlens = train_loader.send(send_args)
        (model(inputs, targets, cum_seqlens, ws_short, ws_long) / grad_accum_steps).backward()
    step_optimizers(step, optimizers, model)

    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.10.12 (main, Nov  4 2025, 08:48:33) [GCC 11.4.0]
Running PyTorch 2.10.0.dev20251204+cu126 compiled for CUDA 12.6
Running Triton version 3.6.0
Sat Dec 20 00:35:18 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:19:00.0 Off |                    0 |
| N/A   30C    P0            119W /  700W |    5858MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:3B:00.0 Off |                    0 |
| N/A   25C    P0            114W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:4C:00.0 Off |                    0 |
| N/A   27C    P0            119W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:5D:00.0 Off |                    0 |
| N/A   28C    P0            121W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:9B:00.0 Off |                    0 |
| N/A   31C    P0            124W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:BB:00.0 Off |                    0 |
| N/A   24C    P0            113W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:CB:00.0 Off |                    0 |
| N/A   29C    P0            121W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:DB:00.0 Off |                    0 |
| N/A   24C    P0            117W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A    310882      C   /home/ubuntu/.venv/bin/python3               1510MiB |
|    0   N/A  N/A    310883      C   /home/ubuntu/.venv/bin/python3                614MiB |
|    0   N/A  N/A    310884      C   /home/ubuntu/.venv/bin/python3                614MiB |
|    0   N/A  N/A    310885      C   /home/ubuntu/.venv/bin/python3                614MiB |
|    0   N/A  N/A    310886      C   /home/ubuntu/.venv/bin/python3                614MiB |
|    0   N/A  N/A    310887      C   /home/ubuntu/.venv/bin/python3                614MiB |
|    0   N/A  N/A    310888      C   /home/ubuntu/.venv/bin/python3                614MiB |
|    0   N/A  N/A    310889      C   /home/ubuntu/.venv/bin/python3                614MiB |
|    1   N/A  N/A    310883      C   /home/ubuntu/.venv/bin/python3               1510MiB |
|    2   N/A  N/A    310884      C   /home/ubuntu/.venv/bin/python3               1510MiB |
|    3   N/A  N/A    310885      C   /home/ubuntu/.venv/bin/python3               1510MiB |
|    4   N/A  N/A    310886      C   /home/ubuntu/.venv/bin/python3               1510MiB |
|    5   N/A  N/A    310887      C   /home/ubuntu/.venv/bin/python3               1510MiB |
|    6   N/A  N/A    310888      C   /home/ubuntu/.venv/bin/python3               1510MiB |
|    7   N/A  N/A    310889      C   /home/ubuntu/.venv/bin/python3               1510MiB |
+-----------------------------------------------------------------------------------------+

====================================================================================================
step:0/2035 val_loss:10.8375 train_time:0ms step_avg:0.03ms
step:1/2035 train_time:88ms step_avg:87.64ms
step:2/2035 train_time:114ms step_avg:56.78ms
step:3/2035 train_time:136ms step_avg:45.20ms
step:4/2035 train_time:162ms step_avg:40.60ms
step:5/2035 train_time:195ms step_avg:38.99ms
step:6/2035 train_time:421ms step_avg:70.21ms
step:7/2035 train_time:440ms step_avg:62.86ms
step:8/2035 train_time:473ms step_avg:59.09ms
step:9/2035 train_time:505ms step_avg:56.15ms
step:10/2035 train_time:538ms step_avg:53.82ms
step:11/2035 train_time:571ms step_avg:51.93ms
step:12/2035 train_time:604ms step_avg:50.33ms
step:13/2035 train_time:637ms step_avg:49.03ms
step:14/2035 train_time:670ms step_avg:47.88ms
step:15/2035 train_time:704ms step_avg:46.90ms
step:16/2035 train_time:736ms step_avg:46.03ms
step:17/2035 train_time:770ms step_avg:45.31ms
step:18/2035 train_time:803ms step_avg:44.64ms
step:19/2035 train_time:837ms step_avg:44.04ms
step:20/2035 train_time:870ms step_avg:43.50ms
step:21/2035 train_time:903ms step_avg:42.99ms
step:22/2035 train_time:936ms step_avg:42.54ms
step:23/2035 train_time:969ms step_avg:42.13ms
step:24/2035 train_time:1002ms step_avg:41.75ms
step:25/2035 train_time:1035ms step_avg:41.41ms
step:26/2035 train_time:1068ms step_avg:41.09ms
step:27/2035 train_time:1101ms step_avg:40.79ms
step:28/2035 train_time:1134ms step_avg:40.51ms
step:29/2035 train_time:1167ms step_avg:40.25ms
step:30/2035 train_time:1200ms step_avg:40.01ms
step:31/2035 train_time:1234ms step_avg:39.79ms
step:32/2035 train_time:1267ms step_avg:39.59ms
step:33/2035 train_time:1300ms step_avg:39.39ms
step:34/2035 train_time:1333ms step_avg:39.21ms
step:35/2035 train_time:1367ms step_avg:39.06ms
step:36/2035 train_time:1400ms step_avg:38.90ms
step:37/2035 train_time:1434ms step_avg:38.75ms
step:38/2035 train_time:1467ms step_avg:38.61ms
step:39/2035 train_time:1501ms step_avg:38.48ms
step:40/2035 train_time:1534ms step_avg:38.34ms
step:41/2035 train_time:1567ms step_avg:38.22ms
step:42/2035 train_time:1600ms step_avg:38.11ms
step:43/2035 train_time:1634ms step_avg:37.99ms
step:44/2035 train_time:1667ms step_avg:37.88ms
step:45/2035 train_time:1700ms step_avg:37.77ms
step:46/2035 train_time:1733ms step_avg:37.67ms
step:47/2035 train_time:1766ms step_avg:37.58ms
step:48/2035 train_time:1800ms step_avg:37.49ms
step:49/2035 train_time:1833ms step_avg:37.40ms
step:50/2035 train_time:1866ms step_avg:37.32ms
step:51/2035 train_time:1899ms step_avg:37.24ms
step:52/2035 train_time:1932ms step_avg:37.16ms
step:53/2035 train_time:1965ms step_avg:37.08ms
step:54/2035 train_time:1998ms step_avg:37.01ms
step:55/2035 train_time:2032ms step_avg:36.94ms
step:56/2035 train_time:2065ms step_avg:36.87ms
step:57/2035 train_time:2098ms step_avg:36.80ms
step:58/2035 train_time:2131ms step_avg:36.74ms
step:59/2035 train_time:2164ms step_avg:36.68ms
step:60/2035 train_time:2197ms step_avg:36.61ms
step:61/2035 train_time:2230ms step_avg:36.56ms
step:62/2035 train_time:2263ms step_avg:36.50ms
step:63/2035 train_time:2296ms step_avg:36.45ms
step:64/2035 train_time:2329ms step_avg:36.40ms
step:65/2035 train_time:2363ms step_avg:36.35ms
step:66/2035 train_time:2396ms step_avg:36.30ms
step:67/2035 train_time:2429ms step_avg:36.25ms
step:68/2035 train_time:2462ms step_avg:36.21ms
step:69/2035 train_time:2495ms step_avg:36.16ms
step:70/2035 train_time:2528ms step_avg:36.11ms
step:71/2035 train_time:2561ms step_avg:36.07ms
step:72/2035 train_time:2594ms step_avg:36.03ms
step:73/2035 train_time:2628ms step_avg:36.00ms
step:74/2035 train_time:2661ms step_avg:35.96ms
step:75/2035 train_time:2694ms step_avg:35.92ms
step:76/2035 train_time:2727ms step_avg:35.88ms
step:77/2035 train_time:2760ms step_avg:35.85ms
step:78/2035 train_time:2793ms step_avg:35.81ms
step:79/2035 train_time:2827ms step_avg:35.78ms
step:80/2035 train_time:2860ms step_avg:35.74ms
step:81/2035 train_time:2893ms step_avg:35.71ms
step:82/2035 train_time:2926ms step_avg:35.68ms
step:83/2035 train_time:2959ms step_avg:35.64ms
step:84/2035 train_time:2991ms step_avg:35.61ms
step:85/2035 train_time:3024ms step_avg:35.58ms
step:86/2035 train_time:3057ms step_avg:35.55ms
step:87/2035 train_time:3091ms step_avg:35.52ms
step:88/2035 train_time:3123ms step_avg:35.49ms
step:89/2035 train_time:3157ms step_avg:35.47ms
step:90/2035 train_time:3190ms step_avg:35.44ms
step:91/2035 train_time:3223ms step_avg:35.41ms
step:92/2035 train_time:3255ms step_avg:35.38ms
step:93/2035 train_time:3289ms step_avg:35.36ms
step:94/2035 train_time:3321ms step_avg:35.33ms
step:95/2035 train_time:3355ms step_avg:35.31ms
step:96/2035 train_time:3388ms step_avg:35.29ms
step:97/2035 train_time:3421ms step_avg:35.27ms
step:98/2035 train_time:3454ms step_avg:35.24ms
step:99/2035 train_time:3487ms step_avg:35.22ms
step:100/2035 train_time:3520ms step_avg:35.20ms
step:101/2035 train_time:3553ms step_avg:35.18ms
step:102/2035 train_time:3586ms step_avg:35.15ms
step:103/2035 train_time:3619ms step_avg:35.14ms
step:104/2035 train_time:3652ms step_avg:35.11ms
step:105/2035 train_time:3685ms step_avg:35.10ms
step:106/2035 train_time:3718ms step_avg:35.08ms
step:107/2035 train_time:3751ms step_avg:35.06ms
step:108/2035 train_time:3784ms step_avg:35.04ms
step:109/2035 train_time:3817ms step_avg:35.02ms
step:110/2035 train_time:3850ms step_avg:35.00ms
step:111/2035 train_time:3884ms step_avg:34.99ms
step:112/2035 train_time:3917ms step_avg:34.97ms
step:113/2035 train_time:3950ms step_avg:34.96ms
step:114/2035 train_time:3983ms step_avg:34.94ms
step:115/2035 train_time:4016ms step_avg:34.92ms
step:116/2035 train_time:4049ms step_avg:34.91ms
step:117/2035 train_time:4082ms step_avg:34.89ms
step:118/2035 train_time:4115ms step_avg:34.87ms
step:119/2035 train_time:4149ms step_avg:34.86ms
step:120/2035 train_time:4181ms step_avg:34.85ms
step:121/2035 train_time:4215ms step_avg:34.84ms
step:122/2035 train_time:4248ms step_avg:34.82ms
step:123/2035 train_time:4281ms step_avg:34.81ms
step:124/2035 train_time:4314ms step_avg:34.79ms
step:125/2035 train_time:4347ms step_avg:34.78ms
step:126/2035 train_time:4380ms step_avg:34.76ms
step:127/2035 train_time:4413ms step_avg:34.75ms
step:128/2035 train_time:4446ms step_avg:34.73ms
step:129/2035 train_time:4479ms step_avg:34.72ms
step:130/2035 train_time:4512ms step_avg:34.71ms
step:131/2035 train_time:4544ms step_avg:34.69ms
step:132/2035 train_time:4577ms step_avg:34.68ms
step:133/2035 train_time:4610ms step_avg:34.66ms
step:134/2035 train_time:4644ms step_avg:34.66ms
step:135/2035 train_time:4677ms step_avg:34.64ms
step:136/2035 train_time:4710ms step_avg:34.63ms
step:137/2035 train_time:4743ms step_avg:34.62ms
step:138/2035 train_time:4776ms step_avg:34.61ms
step:139/2035 train_time:4809ms step_avg:34.60ms
step:140/2035 train_time:4842ms step_avg:34.58ms
step:141/2035 train_time:4875ms step_avg:34.57ms
step:142/2035 train_time:4908ms step_avg:34.56ms
step:143/2035 train_time:4941ms step_avg:34.55ms
step:144/2035 train_time:4974ms step_avg:34.54ms
step:145/2035 train_time:5007ms step_avg:34.53ms
step:146/2035 train_time:5040ms step_avg:34.52ms
step:147/2035 train_time:5073ms step_avg:34.51ms
step:148/2035 train_time:5106ms step_avg:34.50ms
step:149/2035 train_time:5139ms step_avg:34.49ms
step:150/2035 train_time:5172ms step_avg:34.48ms
step:151/2035 train_time:5205ms step_avg:34.47ms
step:152/2035 train_time:5238ms step_avg:34.46ms
step:153/2035 train_time:5272ms step_avg:34.46ms
step:154/2035 train_time:5305ms step_avg:34.45ms
step:155/2035 train_time:5338ms step_avg:34.44ms
step:156/2035 train_time:5371ms step_avg:34.43ms
step:157/2035 train_time:5403ms step_avg:34.42ms
step:158/2035 train_time:5436ms step_avg:34.41ms
step:159/2035 train_time:5470ms step_avg:34.40ms
step:160/2035 train_time:5503ms step_avg:34.39ms
step:161/2035 train_time:5536ms step_avg:34.38ms
step:162/2035 train_time:5569ms step_avg:34.37ms
step:163/2035 train_time:5602ms step_avg:34.37ms
step:164/2035 train_time:5635ms step_avg:34.36ms
step:165/2035 train_time:5668ms step_avg:34.35ms
step:166/2035 train_time:5701ms step_avg:34.34ms
step:167/2035 train_time:5735ms step_avg:34.34ms
step:168/2035 train_time:5767ms step_avg:34.33ms
step:169/2035 train_time:5801ms step_avg:34.32ms
step:170/2035 train_time:5834ms step_avg:34.32ms
step:171/2035 train_time:5867ms step_avg:34.31ms
step:172/2035 train_time:5900ms step_avg:34.30ms
step:173/2035 train_time:5933ms step_avg:34.29ms
step:174/2035 train_time:5966ms step_avg:34.29ms
step:175/2035 train_time:5999ms step_avg:34.28ms
step:176/2035 train_time:6032ms step_avg:34.27ms
step:177/2035 train_time:6065ms step_avg:34.27ms
step:178/2035 train_time:6098ms step_avg:34.26ms
step:179/2035 train_time:6131ms step_avg:34.25ms
step:180/2035 train_time:6164ms step_avg:34.25ms
step:181/2035 train_time:6197ms step_avg:34.24ms
step:182/2035 train_time:6230ms step_avg:34.23ms
step:183/2035 train_time:6263ms step_avg:34.22ms
step:184/2035 train_time:6296ms step_avg:34.22ms
step:185/2035 train_time:6329ms step_avg:34.21ms
step:186/2035 train_time:6362ms step_avg:34.20ms
step:187/2035 train_time:6395ms step_avg:34.20ms
step:188/2035 train_time:6428ms step_avg:34.19ms
step:189/2035 train_time:6461ms step_avg:34.18ms
step:190/2035 train_time:6494ms step_avg:34.18ms
step:191/2035 train_time:6527ms step_avg:34.17ms
step:192/2035 train_time:6560ms step_avg:34.17ms
step:193/2035 train_time:6593ms step_avg:34.16ms
step:194/2035 train_time:6626ms step_avg:34.15ms
step:195/2035 train_time:6659ms step_avg:34.15ms
step:196/2035 train_time:6692ms step_avg:34.14ms
step:197/2035 train_time:6725ms step_avg:34.14ms
step:198/2035 train_time:6757ms step_avg:34.13ms
step:199/2035 train_time:6791ms step_avg:34.12ms
step:200/2035 train_time:6824ms step_avg:34.12ms
step:201/2035 train_time:6857ms step_avg:34.11ms
step:202/2035 train_time:6890ms step_avg:34.11ms
step:203/2035 train_time:6923ms step_avg:34.10ms
step:204/2035 train_time:6955ms step_avg:34.10ms
step:205/2035 train_time:6988ms step_avg:34.09ms
step:206/2035 train_time:7021ms step_avg:34.08ms
step:207/2035 train_time:7054ms step_avg:34.08ms
step:208/2035 train_time:7087ms step_avg:34.07ms
step:209/2035 train_time:7120ms step_avg:34.07ms
step:210/2035 train_time:7153ms step_avg:34.06ms
step:211/2035 train_time:7186ms step_avg:34.06ms
step:212/2035 train_time:7219ms step_avg:34.05ms
step:213/2035 train_time:7252ms step_avg:34.05ms
step:214/2035 train_time:7285ms step_avg:34.04ms
step:215/2035 train_time:7318ms step_avg:34.04ms
step:216/2035 train_time:7351ms step_avg:34.03ms
step:217/2035 train_time:7384ms step_avg:34.03ms
step:218/2035 train_time:7417ms step_avg:34.02ms
step:219/2035 train_time:7450ms step_avg:34.02ms
step:220/2035 train_time:7483ms step_avg:34.02ms
step:221/2035 train_time:7516ms step_avg:34.01ms
step:222/2035 train_time:7549ms step_avg:34.01ms
step:223/2035 train_time:7582ms step_avg:34.00ms
step:224/2035 train_time:7615ms step_avg:34.00ms
step:225/2035 train_time:7648ms step_avg:33.99ms
step:226/2035 train_time:7681ms step_avg:33.99ms
step:227/2035 train_time:7714ms step_avg:33.98ms
step:228/2035 train_time:7747ms step_avg:33.98ms
step:229/2035 train_time:7780ms step_avg:33.97ms
step:230/2035 train_time:7813ms step_avg:33.97ms
step:231/2035 train_time:7846ms step_avg:33.97ms
step:232/2035 train_time:7879ms step_avg:33.96ms
step:233/2035 train_time:7912ms step_avg:33.96ms
step:234/2035 train_time:7945ms step_avg:33.95ms
step:235/2035 train_time:7978ms step_avg:33.95ms
step:236/2035 train_time:8011ms step_avg:33.95ms
step:237/2035 train_time:8044ms step_avg:33.94ms
step:238/2035 train_time:8077ms step_avg:33.94ms
step:239/2035 train_time:8110ms step_avg:33.93ms
step:240/2035 train_time:8143ms step_avg:33.93ms
step:241/2035 train_time:8176ms step_avg:33.93ms
step:242/2035 train_time:8209ms step_avg:33.92ms
step:243/2035 train_time:8242ms step_avg:33.92ms
step:244/2035 train_time:8275ms step_avg:33.91ms
step:245/2035 train_time:8308ms step_avg:33.91ms
step:246/2035 train_time:8341ms step_avg:33.91ms
step:247/2035 train_time:8374ms step_avg:33.90ms
step:248/2035 train_time:8407ms step_avg:33.90ms
step:249/2035 train_time:8440ms step_avg:33.90ms
step:250/2035 train_time:8473ms step_avg:33.89ms
step:250/2035 val_loss:4.2667 train_time:8508ms step_avg:34.03ms
step:251/2035 train_time:8528ms step_avg:33.98ms
step:252/2035 train_time:8548ms step_avg:33.92ms
step:253/2035 train_time:8577ms step_avg:33.90ms
step:254/2035 train_time:8610ms step_avg:33.90ms
step:255/2035 train_time:8645ms step_avg:33.90ms
step:256/2035 train_time:8678ms step_avg:33.90ms
step:257/2035 train_time:8712ms step_avg:33.90ms
step:258/2035 train_time:8745ms step_avg:33.90ms
step:259/2035 train_time:8779ms step_avg:33.89ms
step:260/2035 train_time:8812ms step_avg:33.89ms
step:261/2035 train_time:8844ms step_avg:33.89ms
step:262/2035 train_time:8878ms step_avg:33.88ms
step:263/2035 train_time:8910ms step_avg:33.88ms
step:264/2035 train_time:8943ms step_avg:33.87ms
step:265/2035 train_time:8976ms step_avg:33.87ms
step:266/2035 train_time:9009ms step_avg:33.87ms
step:267/2035 train_time:9041ms step_avg:33.86ms
step:268/2035 train_time:9074ms step_avg:33.86ms
step:269/2035 train_time:9107ms step_avg:33.86ms
step:270/2035 train_time:9140ms step_avg:33.85ms
step:271/2035 train_time:9173ms step_avg:33.85ms
step:272/2035 train_time:9206ms step_avg:33.85ms
step:273/2035 train_time:9239ms step_avg:33.84ms
step:274/2035 train_time:9272ms step_avg:33.84ms
step:275/2035 train_time:9305ms step_avg:33.84ms
step:276/2035 train_time:9338ms step_avg:33.83ms
step:277/2035 train_time:9371ms step_avg:33.83ms
step:278/2035 train_time:9404ms step_avg:33.83ms
step:279/2035 train_time:9437ms step_avg:33.82ms
step:280/2035 train_time:9470ms step_avg:33.82ms
step:281/2035 train_time:9503ms step_avg:33.82ms
step:282/2035 train_time:9536ms step_avg:33.81ms
step:283/2035 train_time:9569ms step_avg:33.81ms
step:284/2035 train_time:9602ms step_avg:33.81ms
step:285/2035 train_time:9636ms step_avg:33.81ms
step:286/2035 train_time:9668ms step_avg:33.81ms
step:287/2035 train_time:9702ms step_avg:33.80ms
step:288/2035 train_time:9735ms step_avg:33.80ms
step:289/2035 train_time:9768ms step_avg:33.80ms
step:290/2035 train_time:9801ms step_avg:33.80ms
step:291/2035 train_time:9834ms step_avg:33.79ms
step:292/2035 train_time:9867ms step_avg:33.79ms
step:293/2035 train_time:9900ms step_avg:33.79ms
step:294/2035 train_time:9932ms step_avg:33.78ms
step:295/2035 train_time:9965ms step_avg:33.78ms
step:296/2035 train_time:9998ms step_avg:33.78ms
step:297/2035 train_time:10031ms step_avg:33.78ms
step:298/2035 train_time:10064ms step_avg:33.77ms
step:299/2035 train_time:10097ms step_avg:33.77ms
step:300/2035 train_time:10130ms step_avg:33.77ms
step:301/2035 train_time:10163ms step_avg:33.76ms
step:302/2035 train_time:10196ms step_avg:33.76ms
step:303/2035 train_time:10229ms step_avg:33.76ms
step:304/2035 train_time:10262ms step_avg:33.76ms
step:305/2035 train_time:10295ms step_avg:33.76ms
step:306/2035 train_time:10328ms step_avg:33.75ms
step:307/2035 train_time:10361ms step_avg:33.75ms
step:308/2035 train_time:10394ms step_avg:33.75ms
step:309/2035 train_time:10427ms step_avg:33.74ms
step:310/2035 train_time:10460ms step_avg:33.74ms
step:311/2035 train_time:10493ms step_avg:33.74ms
step:312/2035 train_time:10525ms step_avg:33.74ms
step:313/2035 train_time:10559ms step_avg:33.73ms
step:314/2035 train_time:10591ms step_avg:33.73ms
step:315/2035 train_time:10625ms step_avg:33.73ms
step:316/2035 train_time:10658ms step_avg:33.73ms
step:317/2035 train_time:10691ms step_avg:33.73ms
step:318/2035 train_time:10724ms step_avg:33.72ms
step:319/2035 train_time:10758ms step_avg:33.72ms
step:320/2035 train_time:10790ms step_avg:33.72ms
step:321/2035 train_time:10824ms step_avg:33.72ms
step:322/2035 train_time:10856ms step_avg:33.72ms
step:323/2035 train_time:10890ms step_avg:33.71ms
step:324/2035 train_time:10922ms step_avg:33.71ms
step:325/2035 train_time:10956ms step_avg:33.71ms
step:326/2035 train_time:10988ms step_avg:33.71ms
step:327/2035 train_time:11021ms step_avg:33.70ms
step:328/2035 train_time:11054ms step_avg:33.70ms
step:329/2035 train_time:11087ms step_avg:33.70ms
step:330/2035 train_time:11120ms step_avg:33.70ms
step:331/2035 train_time:11153ms step_avg:33.70ms
step:332/2035 train_time:11186ms step_avg:33.69ms
step:333/2035 train_time:11219ms step_avg:33.69ms
step:334/2035 train_time:11252ms step_avg:33.69ms
step:335/2035 train_time:11285ms step_avg:33.69ms
step:336/2035 train_time:11318ms step_avg:33.68ms
step:337/2035 train_time:11351ms step_avg:33.68ms
step:338/2035 train_time:11384ms step_avg:33.68ms
step:339/2035 train_time:11417ms step_avg:33.68ms
step:340/2035 train_time:11450ms step_avg:33.68ms
step:341/2035 train_time:11482ms step_avg:33.67ms
step:342/2035 train_time:11515ms step_avg:33.67ms
step:343/2035 train_time:11548ms step_avg:33.67ms
step:344/2035 train_time:11581ms step_avg:33.67ms
step:345/2035 train_time:11614ms step_avg:33.67ms
step:346/2035 train_time:11647ms step_avg:33.66ms
step:347/2035 train_time:11681ms step_avg:33.66ms
step:348/2035 train_time:11714ms step_avg:33.66ms
step:349/2035 train_time:11747ms step_avg:33.66ms
step:350/2035 train_time:11779ms step_avg:33.66ms
step:351/2035 train_time:11813ms step_avg:33.65ms
step:352/2035 train_time:11845ms step_avg:33.65ms
step:353/2035 train_time:11879ms step_avg:33.65ms
step:354/2035 train_time:11911ms step_avg:33.65ms
step:355/2035 train_time:11944ms step_avg:33.65ms
step:356/2035 train_time:11977ms step_avg:33.64ms
step:357/2035 train_time:12010ms step_avg:33.64ms
step:358/2035 train_time:12043ms step_avg:33.64ms
step:359/2035 train_time:12076ms step_avg:33.64ms
step:360/2035 train_time:12109ms step_avg:33.64ms
step:361/2035 train_time:12142ms step_avg:33.63ms
step:362/2035 train_time:12175ms step_avg:33.63ms
step:363/2035 train_time:12208ms step_avg:33.63ms
step:364/2035 train_time:12240ms step_avg:33.63ms
step:365/2035 train_time:12274ms step_avg:33.63ms
step:366/2035 train_time:12306ms step_avg:33.62ms
step:367/2035 train_time:12340ms step_avg:33.62ms
step:368/2035 train_time:12372ms step_avg:33.62ms
step:369/2035 train_time:12405ms step_avg:33.62ms
step:370/2035 train_time:12438ms step_avg:33.62ms
step:371/2035 train_time:12471ms step_avg:33.62ms
step:372/2035 train_time:12504ms step_avg:33.61ms
step:373/2035 train_time:12537ms step_avg:33.61ms
step:374/2035 train_time:12570ms step_avg:33.61ms
step:375/2035 train_time:12603ms step_avg:33.61ms
step:376/2035 train_time:12636ms step_avg:33.61ms
step:377/2035 train_time:12669ms step_avg:33.60ms
step:378/2035 train_time:12702ms step_avg:33.60ms
step:379/2035 train_time:12735ms step_avg:33.60ms
step:380/2035 train_time:12768ms step_avg:33.60ms
step:381/2035 train_time:12801ms step_avg:33.60ms
step:382/2035 train_time:12833ms step_avg:33.60ms
step:383/2035 train_time:12867ms step_avg:33.59ms
step:384/2035 train_time:12899ms step_avg:33.59ms
step:385/2035 train_time:12933ms step_avg:33.59ms
step:386/2035 train_time:12966ms step_avg:33.59ms
step:387/2035 train_time:12999ms step_avg:33.59ms
step:388/2035 train_time:13032ms step_avg:33.59ms
step:389/2035 train_time:13064ms step_avg:33.58ms
step:390/2035 train_time:13097ms step_avg:33.58ms
step:391/2035 train_time:13130ms step_avg:33.58ms
step:392/2035 train_time:13163ms step_avg:33.58ms
step:393/2035 train_time:13196ms step_avg:33.58ms
step:394/2035 train_time:13229ms step_avg:33.58ms
step:395/2035 train_time:13262ms step_avg:33.57ms
step:396/2035 train_time:13295ms step_avg:33.57ms
step:397/2035 train_time:13328ms step_avg:33.57ms
step:398/2035 train_time:13361ms step_avg:33.57ms
step:399/2035 train_time:13394ms step_avg:33.57ms
step:400/2035 train_time:13426ms step_avg:33.57ms
step:401/2035 train_time:13459ms step_avg:33.56ms
step:402/2035 train_time:13492ms step_avg:33.56ms
step:403/2035 train_time:13525ms step_avg:33.56ms
step:404/2035 train_time:13558ms step_avg:33.56ms
step:405/2035 train_time:13591ms step_avg:33.56ms
step:406/2035 train_time:13624ms step_avg:33.56ms
step:407/2035 train_time:13657ms step_avg:33.56ms
step:408/2035 train_time:13690ms step_avg:33.55ms
step:409/2035 train_time:13724ms step_avg:33.55ms
step:410/2035 train_time:13757ms step_avg:33.55ms
step:411/2035 train_time:13789ms step_avg:33.55ms
step:412/2035 train_time:13822ms step_avg:33.55ms
step:413/2035 train_time:13855ms step_avg:33.55ms
step:414/2035 train_time:13888ms step_avg:33.55ms
step:415/2035 train_time:13921ms step_avg:33.55ms
step:416/2035 train_time:13954ms step_avg:33.54ms
step:417/2035 train_time:13987ms step_avg:33.54ms
step:418/2035 train_time:14020ms step_avg:33.54ms
step:419/2035 train_time:14053ms step_avg:33.54ms
step:420/2035 train_time:14086ms step_avg:33.54ms
step:421/2035 train_time:14119ms step_avg:33.54ms
step:422/2035 train_time:14152ms step_avg:33.54ms
step:423/2035 train_time:14185ms step_avg:33.53ms
step:424/2035 train_time:14218ms step_avg:33.53ms
step:425/2035 train_time:14251ms step_avg:33.53ms
step:426/2035 train_time:14284ms step_avg:33.53ms
step:427/2035 train_time:14317ms step_avg:33.53ms
step:428/2035 train_time:14350ms step_avg:33.53ms
step:429/2035 train_time:14383ms step_avg:33.53ms
step:430/2035 train_time:14416ms step_avg:33.53ms
step:431/2035 train_time:14449ms step_avg:33.52ms
step:432/2035 train_time:14481ms step_avg:33.52ms
step:433/2035 train_time:14514ms step_avg:33.52ms
step:434/2035 train_time:14547ms step_avg:33.52ms
step:435/2035 train_time:14580ms step_avg:33.52ms
step:436/2035 train_time:14613ms step_avg:33.52ms
step:437/2035 train_time:14646ms step_avg:33.52ms
step:438/2035 train_time:14679ms step_avg:33.51ms
step:439/2035 train_time:14713ms step_avg:33.51ms
step:440/2035 train_time:14745ms step_avg:33.51ms
step:441/2035 train_time:14779ms step_avg:33.51ms
step:442/2035 train_time:14811ms step_avg:33.51ms
step:443/2035 train_time:14845ms step_avg:33.51ms
step:444/2035 train_time:14878ms step_avg:33.51ms
step:445/2035 train_time:14911ms step_avg:33.51ms
step:446/2035 train_time:14944ms step_avg:33.51ms
step:447/2035 train_time:14977ms step_avg:33.51ms
step:448/2035 train_time:15010ms step_avg:33.50ms
step:449/2035 train_time:15043ms step_avg:33.50ms
step:450/2035 train_time:15076ms step_avg:33.50ms
step:451/2035 train_time:15109ms step_avg:33.50ms
step:452/2035 train_time:15141ms step_avg:33.50ms
step:453/2035 train_time:15174ms step_avg:33.50ms
step:454/2035 train_time:15207ms step_avg:33.50ms
step:455/2035 train_time:15240ms step_avg:33.49ms
step:456/2035 train_time:15273ms step_avg:33.49ms
step:457/2035 train_time:15306ms step_avg:33.49ms
step:458/2035 train_time:15339ms step_avg:33.49ms
step:459/2035 train_time:15372ms step_avg:33.49ms
step:460/2035 train_time:15405ms step_avg:33.49ms
step:461/2035 train_time:15438ms step_avg:33.49ms
step:462/2035 train_time:15471ms step_avg:33.49ms
step:463/2035 train_time:15504ms step_avg:33.49ms
step:464/2035 train_time:15537ms step_avg:33.48ms
step:465/2035 train_time:15570ms step_avg:33.48ms
step:466/2035 train_time:15603ms step_avg:33.48ms
step:467/2035 train_time:15636ms step_avg:33.48ms
step:468/2035 train_time:15669ms step_avg:33.48ms
step:469/2035 train_time:15702ms step_avg:33.48ms
step:470/2035 train_time:15735ms step_avg:33.48ms
step:471/2035 train_time:15767ms step_avg:33.48ms
step:472/2035 train_time:15800ms step_avg:33.47ms
step:473/2035 train_time:15833ms step_avg:33.47ms
step:474/2035 train_time:15866ms step_avg:33.47ms
step:475/2035 train_time:15899ms step_avg:33.47ms
step:476/2035 train_time:15932ms step_avg:33.47ms
step:477/2035 train_time:15965ms step_avg:33.47ms
step:478/2035 train_time:15998ms step_avg:33.47ms
step:479/2035 train_time:16031ms step_avg:33.47ms
step:480/2035 train_time:16063ms step_avg:33.47ms
step:481/2035 train_time:16096ms step_avg:33.46ms
step:482/2035 train_time:16129ms step_avg:33.46ms
step:483/2035 train_time:16162ms step_avg:33.46ms
step:484/2035 train_time:16195ms step_avg:33.46ms
step:485/2035 train_time:16228ms step_avg:33.46ms
step:486/2035 train_time:16261ms step_avg:33.46ms
step:487/2035 train_time:16294ms step_avg:33.46ms
step:488/2035 train_time:16327ms step_avg:33.46ms
step:489/2035 train_time:16360ms step_avg:33.46ms
step:490/2035 train_time:16393ms step_avg:33.45ms
step:491/2035 train_time:16426ms step_avg:33.45ms
step:492/2035 train_time:16459ms step_avg:33.45ms
step:493/2035 train_time:16492ms step_avg:33.45ms
step:494/2035 train_time:16525ms step_avg:33.45ms
step:495/2035 train_time:16558ms step_avg:33.45ms
step:496/2035 train_time:16591ms step_avg:33.45ms
step:497/2035 train_time:16624ms step_avg:33.45ms
step:498/2035 train_time:16656ms step_avg:33.45ms
step:499/2035 train_time:16690ms step_avg:33.45ms
step:500/2035 train_time:16722ms step_avg:33.44ms
step:500/2035 val_loss:4.0003 train_time:16758ms step_avg:33.52ms
step:501/2035 train_time:16779ms step_avg:33.49ms
step:502/2035 train_time:16799ms step_avg:33.46ms
step:503/2035 train_time:16826ms step_avg:33.45ms
step:504/2035 train_time:16859ms step_avg:33.45ms
step:505/2035 train_time:16893ms step_avg:33.45ms
step:506/2035 train_time:16926ms step_avg:33.45ms
step:507/2035 train_time:16959ms step_avg:33.45ms
step:508/2035 train_time:16992ms step_avg:33.45ms
step:509/2035 train_time:17024ms step_avg:33.45ms
step:510/2035 train_time:17057ms step_avg:33.45ms
step:511/2035 train_time:17090ms step_avg:33.44ms
step:512/2035 train_time:17123ms step_avg:33.44ms
step:513/2035 train_time:17156ms step_avg:33.44ms
step:514/2035 train_time:17188ms step_avg:33.44ms
step:515/2035 train_time:17221ms step_avg:33.44ms
step:516/2035 train_time:17254ms step_avg:33.44ms
step:517/2035 train_time:17287ms step_avg:33.44ms
step:518/2035 train_time:17320ms step_avg:33.44ms
step:519/2035 train_time:17352ms step_avg:33.43ms
step:520/2035 train_time:17385ms step_avg:33.43ms
step:521/2035 train_time:17418ms step_avg:33.43ms
step:522/2035 train_time:17451ms step_avg:33.43ms
step:523/2035 train_time:17484ms step_avg:33.43ms
step:524/2035 train_time:17516ms step_avg:33.43ms
step:525/2035 train_time:17549ms step_avg:33.43ms
step:526/2035 train_time:17582ms step_avg:33.43ms
step:527/2035 train_time:17615ms step_avg:33.43ms
step:528/2035 train_time:17648ms step_avg:33.42ms
step:529/2035 train_time:17681ms step_avg:33.42ms
step:530/2035 train_time:17714ms step_avg:33.42ms
step:531/2035 train_time:17748ms step_avg:33.42ms
step:532/2035 train_time:17781ms step_avg:33.42ms
step:533/2035 train_time:17814ms step_avg:33.42ms
step:534/2035 train_time:17848ms step_avg:33.42ms
step:535/2035 train_time:17881ms step_avg:33.42ms
step:536/2035 train_time:17914ms step_avg:33.42ms
step:537/2035 train_time:17948ms step_avg:33.42ms
step:538/2035 train_time:17980ms step_avg:33.42ms
step:539/2035 train_time:18013ms step_avg:33.42ms
step:540/2035 train_time:18046ms step_avg:33.42ms
step:541/2035 train_time:18079ms step_avg:33.42ms
step:542/2035 train_time:18112ms step_avg:33.42ms
step:543/2035 train_time:18145ms step_avg:33.42ms
step:544/2035 train_time:18178ms step_avg:33.41ms
step:545/2035 train_time:18211ms step_avg:33.41ms
step:546/2035 train_time:18243ms step_avg:33.41ms
step:547/2035 train_time:18276ms step_avg:33.41ms
step:548/2035 train_time:18309ms step_avg:33.41ms
step:549/2035 train_time:18342ms step_avg:33.41ms
step:550/2035 train_time:18375ms step_avg:33.41ms
step:551/2035 train_time:18407ms step_avg:33.41ms
step:552/2035 train_time:18440ms step_avg:33.41ms
step:553/2035 train_time:18473ms step_avg:33.41ms
step:554/2035 train_time:18506ms step_avg:33.40ms
step:555/2035 train_time:18539ms step_avg:33.40ms
step:556/2035 train_time:18572ms step_avg:33.40ms
step:557/2035 train_time:18605ms step_avg:33.40ms
step:558/2035 train_time:18637ms step_avg:33.40ms
step:559/2035 train_time:18671ms step_avg:33.40ms
step:560/2035 train_time:18703ms step_avg:33.40ms
step:561/2035 train_time:18737ms step_avg:33.40ms
step:562/2035 train_time:18770ms step_avg:33.40ms
step:563/2035 train_time:18803ms step_avg:33.40ms
step:564/2035 train_time:18836ms step_avg:33.40ms
step:565/2035 train_time:18869ms step_avg:33.40ms
step:566/2035 train_time:18902ms step_avg:33.40ms
step:567/2035 train_time:18935ms step_avg:33.40ms
step:568/2035 train_time:18968ms step_avg:33.39ms
step:569/2035 train_time:19002ms step_avg:33.39ms
step:570/2035 train_time:19034ms step_avg:33.39ms
step:571/2035 train_time:19068ms step_avg:33.39ms
step:572/2035 train_time:19100ms step_avg:33.39ms
step:573/2035 train_time:19134ms step_avg:33.39ms
step:574/2035 train_time:19167ms step_avg:33.39ms
step:575/2035 train_time:19200ms step_avg:33.39ms
step:576/2035 train_time:19232ms step_avg:33.39ms
step:577/2035 train_time:19266ms step_avg:33.39ms
step:578/2035 train_time:19298ms step_avg:33.39ms
step:579/2035 train_time:19331ms step_avg:33.39ms
step:580/2035 train_time:19364ms step_avg:33.39ms
step:581/2035 train_time:19397ms step_avg:33.39ms
step:582/2035 train_time:19430ms step_avg:33.39ms
step:583/2035 train_time:19463ms step_avg:33.38ms
step:584/2035 train_time:19496ms step_avg:33.38ms
step:585/2035 train_time:19529ms step_avg:33.38ms
step:586/2035 train_time:19562ms step_avg:33.38ms
step:587/2035 train_time:19595ms step_avg:33.38ms
step:588/2035 train_time:19627ms step_avg:33.38ms
step:589/2035 train_time:19660ms step_avg:33.38ms
step:590/2035 train_time:19693ms step_avg:33.38ms
step:591/2035 train_time:19726ms step_avg:33.38ms
step:592/2035 train_time:19759ms step_avg:33.38ms
step:593/2035 train_time:19792ms step_avg:33.38ms
step:594/2035 train_time:19825ms step_avg:33.38ms
step:595/2035 train_time:19859ms step_avg:33.38ms
step:596/2035 train_time:19891ms step_avg:33.37ms
step:597/2035 train_time:19925ms step_avg:33.38ms
step:598/2035 train_time:19958ms step_avg:33.37ms
step:599/2035 train_time:19991ms step_avg:33.37ms
step:600/2035 train_time:20024ms step_avg:33.37ms
step:601/2035 train_time:20057ms step_avg:33.37ms
step:602/2035 train_time:20090ms step_avg:33.37ms
step:603/2035 train_time:20123ms step_avg:33.37ms
step:604/2035 train_time:20156ms step_avg:33.37ms
step:605/2035 train_time:20189ms step_avg:33.37ms
step:606/2035 train_time:20222ms step_avg:33.37ms
step:607/2035 train_time:20254ms step_avg:33.37ms
step:608/2035 train_time:20287ms step_avg:33.37ms
step:609/2035 train_time:20320ms step_avg:33.37ms
step:610/2035 train_time:20353ms step_avg:33.37ms
step:611/2035 train_time:20386ms step_avg:33.37ms
step:612/2035 train_time:20419ms step_avg:33.36ms
step:613/2035 train_time:20452ms step_avg:33.36ms
step:614/2035 train_time:20485ms step_avg:33.36ms
step:615/2035 train_time:20518ms step_avg:33.36ms
step:616/2035 train_time:20550ms step_avg:33.36ms
step:617/2035 train_time:20583ms step_avg:33.36ms
step:618/2035 train_time:20616ms step_avg:33.36ms
step:619/2035 train_time:20649ms step_avg:33.36ms
step:620/2035 train_time:20682ms step_avg:33.36ms
step:621/2035 train_time:20715ms step_avg:33.36ms
step:622/2035 train_time:20749ms step_avg:33.36ms
step:623/2035 train_time:20781ms step_avg:33.36ms
step:624/2035 train_time:20814ms step_avg:33.36ms
step:625/2035 train_time:20847ms step_avg:33.36ms
step:626/2035 train_time:20881ms step_avg:33.36ms
step:627/2035 train_time:20914ms step_avg:33.36ms
step:628/2035 train_time:20946ms step_avg:33.35ms
step:629/2035 train_time:20980ms step_avg:33.35ms
step:630/2035 train_time:21013ms step_avg:33.35ms
step:631/2035 train_time:21046ms step_avg:33.35ms
step:632/2035 train_time:21079ms step_avg:33.35ms
step:633/2035 train_time:21112ms step_avg:33.35ms
step:634/2035 train_time:21145ms step_avg:33.35ms
step:635/2035 train_time:21178ms step_avg:33.35ms
step:636/2035 train_time:21211ms step_avg:33.35ms
step:637/2035 train_time:21244ms step_avg:33.35ms
step:638/2035 train_time:21277ms step_avg:33.35ms
step:639/2035 train_time:21310ms step_avg:33.35ms
step:640/2035 train_time:21343ms step_avg:33.35ms
step:641/2035 train_time:21376ms step_avg:33.35ms
step:642/2035 train_time:21409ms step_avg:33.35ms
step:643/2035 train_time:21442ms step_avg:33.35ms
step:644/2035 train_time:21475ms step_avg:33.35ms
step:645/2035 train_time:21507ms step_avg:33.34ms
step:646/2035 train_time:21541ms step_avg:33.34ms
step:647/2035 train_time:21573ms step_avg:33.34ms
step:648/2035 train_time:21607ms step_avg:33.34ms
step:649/2035 train_time:21639ms step_avg:33.34ms
step:650/2035 train_time:21672ms step_avg:33.34ms
step:651/2035 train_time:21705ms step_avg:33.34ms
step:652/2035 train_time:21737ms step_avg:33.34ms
step:653/2035 train_time:21771ms step_avg:33.34ms
step:654/2035 train_time:21804ms step_avg:33.34ms
step:655/2035 train_time:21837ms step_avg:33.34ms
step:656/2035 train_time:21870ms step_avg:33.34ms
step:657/2035 train_time:21903ms step_avg:33.34ms
step:658/2035 train_time:21936ms step_avg:33.34ms
step:659/2035 train_time:21969ms step_avg:33.34ms
step:660/2035 train_time:22002ms step_avg:33.34ms
step:661/2035 train_time:22035ms step_avg:33.34ms
step:662/2035 train_time:22068ms step_avg:33.34ms
step:663/2035 train_time:22101ms step_avg:33.33ms
step:664/2035 train_time:22134ms step_avg:33.33ms
step:665/2035 train_time:22167ms step_avg:33.33ms
step:666/2035 train_time:22201ms step_avg:33.33ms
step:667/2035 train_time:22259ms step_avg:33.37ms
step:668/2035 train_time:22319ms step_avg:33.41ms
step:669/2035 train_time:22379ms step_avg:33.45ms
step:670/2035 train_time:22439ms step_avg:33.49ms
step:671/2035 train_time:22500ms step_avg:33.53ms
step:672/2035 train_time:22559ms step_avg:33.57ms
step:673/2035 train_time:22619ms step_avg:33.61ms
step:674/2035 train_time:22679ms step_avg:33.65ms
step:675/2035 train_time:22739ms step_avg:33.69ms
step:676/2035 train_time:22799ms step_avg:33.73ms
step:677/2035 train_time:22860ms step_avg:33.77ms
step:678/2035 train_time:22920ms step_avg:33.80ms
step:679/2035 train_time:22981ms step_avg:33.85ms
step:680/2035 train_time:23040ms step_avg:33.88ms
step:681/2035 train_time:23101ms step_avg:33.92ms
step:682/2035 train_time:23160ms step_avg:33.96ms
step:683/2035 train_time:23220ms step_avg:34.00ms
step:684/2035 train_time:23279ms step_avg:34.03ms
step:685/2035 train_time:23339ms step_avg:34.07ms
step:686/2035 train_time:23398ms step_avg:34.11ms
step:687/2035 train_time:23459ms step_avg:34.15ms
step:688/2035 train_time:23518ms step_avg:34.18ms
step:689/2035 train_time:23579ms step_avg:34.22ms
step:690/2035 train_time:23639ms step_avg:34.26ms
step:691/2035 train_time:23700ms step_avg:34.30ms
step:692/2035 train_time:23759ms step_avg:34.33ms
step:693/2035 train_time:23821ms step_avg:34.37ms
step:694/2035 train_time:23881ms step_avg:34.41ms
step:695/2035 train_time:23942ms step_avg:34.45ms
step:696/2035 train_time:24001ms step_avg:34.48ms
step:697/2035 train_time:24062ms step_avg:34.52ms
step:698/2035 train_time:24121ms step_avg:34.56ms
step:699/2035 train_time:24181ms step_avg:34.59ms
step:700/2035 train_time:24240ms step_avg:34.63ms
step:701/2035 train_time:24300ms step_avg:34.67ms
step:702/2035 train_time:24359ms step_avg:34.70ms
step:703/2035 train_time:24420ms step_avg:34.74ms
step:704/2035 train_time:24479ms step_avg:34.77ms
step:705/2035 train_time:24540ms step_avg:34.81ms
step:706/2035 train_time:24600ms step_avg:34.84ms
step:707/2035 train_time:24660ms step_avg:34.88ms
step:708/2035 train_time:24719ms step_avg:34.91ms
step:709/2035 train_time:24780ms step_avg:34.95ms
step:710/2035 train_time:24840ms step_avg:34.99ms
step:711/2035 train_time:24901ms step_avg:35.02ms
step:712/2035 train_time:24961ms step_avg:35.06ms
step:713/2035 train_time:25022ms step_avg:35.09ms
step:714/2035 train_time:25081ms step_avg:35.13ms
step:715/2035 train_time:25142ms step_avg:35.16ms
step:716/2035 train_time:25202ms step_avg:35.20ms
step:717/2035 train_time:25262ms step_avg:35.23ms
step:718/2035 train_time:25322ms step_avg:35.27ms
step:719/2035 train_time:25382ms step_avg:35.30ms
step:720/2035 train_time:25441ms step_avg:35.34ms
step:721/2035 train_time:25501ms step_avg:35.37ms
step:722/2035 train_time:25561ms step_avg:35.40ms
step:723/2035 train_time:25621ms step_avg:35.44ms
step:724/2035 train_time:25681ms step_avg:35.47ms
step:725/2035 train_time:25741ms step_avg:35.50ms
step:726/2035 train_time:25800ms step_avg:35.54ms
step:727/2035 train_time:25860ms step_avg:35.57ms
step:728/2035 train_time:25920ms step_avg:35.60ms
step:729/2035 train_time:25981ms step_avg:35.64ms
step:730/2035 train_time:26041ms step_avg:35.67ms
step:731/2035 train_time:26101ms step_avg:35.71ms
step:732/2035 train_time:26160ms step_avg:35.74ms
step:733/2035 train_time:26220ms step_avg:35.77ms
step:734/2035 train_time:26279ms step_avg:35.80ms
step:735/2035 train_time:26339ms step_avg:35.84ms
step:736/2035 train_time:26399ms step_avg:35.87ms
step:737/2035 train_time:26459ms step_avg:35.90ms
step:738/2035 train_time:26518ms step_avg:35.93ms
step:739/2035 train_time:26579ms step_avg:35.97ms
step:740/2035 train_time:26638ms step_avg:36.00ms
step:741/2035 train_time:26700ms step_avg:36.03ms
step:742/2035 train_time:26758ms step_avg:36.06ms
step:743/2035 train_time:26819ms step_avg:36.10ms
step:744/2035 train_time:26880ms step_avg:36.13ms
step:745/2035 train_time:26941ms step_avg:36.16ms
step:746/2035 train_time:27000ms step_avg:36.19ms
step:747/2035 train_time:27060ms step_avg:36.23ms
step:748/2035 train_time:27120ms step_avg:36.26ms
step:749/2035 train_time:27181ms step_avg:36.29ms
step:750/2035 train_time:27240ms step_avg:36.32ms
step:750/2035 val_loss:3.8275 train_time:27302ms step_avg:36.40ms
step:751/2035 train_time:27323ms step_avg:36.38ms
step:752/2035 train_time:27362ms step_avg:36.39ms
step:753/2035 train_time:27424ms step_avg:36.42ms
step:754/2035 train_time:27486ms step_avg:36.45ms
step:755/2035 train_time:27547ms step_avg:36.49ms
step:756/2035 train_time:27607ms step_avg:36.52ms
step:757/2035 train_time:27668ms step_avg:36.55ms
step:758/2035 train_time:27727ms step_avg:36.58ms
step:759/2035 train_time:27787ms step_avg:36.61ms
step:760/2035 train_time:27846ms step_avg:36.64ms
step:761/2035 train_time:27906ms step_avg:36.67ms
step:762/2035 train_time:27965ms step_avg:36.70ms
step:763/2035 train_time:28025ms step_avg:36.73ms
step:764/2035 train_time:28083ms step_avg:36.76ms
step:765/2035 train_time:28143ms step_avg:36.79ms
step:766/2035 train_time:28202ms step_avg:36.82ms
step:767/2035 train_time:28264ms step_avg:36.85ms
step:768/2035 train_time:28324ms step_avg:36.88ms
step:769/2035 train_time:28386ms step_avg:36.91ms
step:770/2035 train_time:28447ms step_avg:36.94ms
step:771/2035 train_time:28509ms step_avg:36.98ms
step:772/2035 train_time:28569ms step_avg:37.01ms
step:773/2035 train_time:28630ms step_avg:37.04ms
step:774/2035 train_time:28689ms step_avg:37.07ms
step:775/2035 train_time:28749ms step_avg:37.10ms
step:776/2035 train_time:28808ms step_avg:37.12ms
step:777/2035 train_time:28868ms step_avg:37.15ms
step:778/2035 train_time:28927ms step_avg:37.18ms
step:779/2035 train_time:28987ms step_avg:37.21ms
step:780/2035 train_time:29046ms step_avg:37.24ms
step:781/2035 train_time:29106ms step_avg:37.27ms
step:782/2035 train_time:29165ms step_avg:37.29ms
step:783/2035 train_time:29225ms step_avg:37.32ms
step:784/2035 train_time:29286ms step_avg:37.35ms
step:785/2035 train_time:29348ms step_avg:37.39ms
step:786/2035 train_time:29408ms step_avg:37.41ms
step:787/2035 train_time:29470ms step_avg:37.45ms
step:788/2035 train_time:29531ms step_avg:37.48ms
step:789/2035 train_time:29591ms step_avg:37.50ms
step:790/2035 train_time:29650ms step_avg:37.53ms
step:791/2035 train_time:29711ms step_avg:37.56ms
step:792/2035 train_time:29770ms step_avg:37.59ms
step:793/2035 train_time:29831ms step_avg:37.62ms
step:794/2035 train_time:29890ms step_avg:37.64ms
step:795/2035 train_time:29950ms step_avg:37.67ms
step:796/2035 train_time:30009ms step_avg:37.70ms
step:797/2035 train_time:30069ms step_avg:37.73ms
step:798/2035 train_time:30128ms step_avg:37.75ms
step:799/2035 train_time:30189ms step_avg:37.78ms
step:800/2035 train_time:30249ms step_avg:37.81ms
step:801/2035 train_time:30310ms step_avg:37.84ms
step:802/2035 train_time:30370ms step_avg:37.87ms
step:803/2035 train_time:30432ms step_avg:37.90ms
step:804/2035 train_time:30491ms step_avg:37.92ms
step:805/2035 train_time:30552ms step_avg:37.95ms
step:806/2035 train_time:30611ms step_avg:37.98ms
step:807/2035 train_time:30673ms step_avg:38.01ms
step:808/2035 train_time:30733ms step_avg:38.04ms
step:809/2035 train_time:30793ms step_avg:38.06ms
step:810/2035 train_time:30852ms step_avg:38.09ms
step:811/2035 train_time:30912ms step_avg:38.12ms
step:812/2035 train_time:30971ms step_avg:38.14ms
step:813/2035 train_time:31031ms step_avg:38.17ms
step:814/2035 train_time:31091ms step_avg:38.19ms
step:815/2035 train_time:31151ms step_avg:38.22ms
step:816/2035 train_time:31210ms step_avg:38.25ms
step:817/2035 train_time:31271ms step_avg:38.28ms
step:818/2035 train_time:31331ms step_avg:38.30ms
step:819/2035 train_time:31392ms step_avg:38.33ms
step:820/2035 train_time:31452ms step_avg:38.36ms
step:821/2035 train_time:31513ms step_avg:38.38ms
step:822/2035 train_time:31572ms step_avg:38.41ms
step:823/2035 train_time:31633ms step_avg:38.44ms
step:824/2035 train_time:31692ms step_avg:38.46ms
step:825/2035 train_time:31753ms step_avg:38.49ms
step:826/2035 train_time:31812ms step_avg:38.51ms
step:827/2035 train_time:31873ms step_avg:38.54ms
step:828/2035 train_time:31932ms step_avg:38.57ms
step:829/2035 train_time:31992ms step_avg:38.59ms
step:830/2035 train_time:32051ms step_avg:38.62ms
step:831/2035 train_time:32111ms step_avg:38.64ms
step:832/2035 train_time:32171ms step_avg:38.67ms
step:833/2035 train_time:32231ms step_avg:38.69ms
step:834/2035 train_time:32290ms step_avg:38.72ms
step:835/2035 train_time:32352ms step_avg:38.74ms
step:836/2035 train_time:32411ms step_avg:38.77ms
step:837/2035 train_time:32472ms step_avg:38.80ms
step:838/2035 train_time:32532ms step_avg:38.82ms
step:839/2035 train_time:32592ms step_avg:38.85ms
step:840/2035 train_time:32651ms step_avg:38.87ms
step:841/2035 train_time:32712ms step_avg:38.90ms
step:842/2035 train_time:32771ms step_avg:38.92ms
step:843/2035 train_time:32832ms step_avg:38.95ms
step:844/2035 train_time:32891ms step_avg:38.97ms
step:845/2035 train_time:32952ms step_avg:39.00ms
step:846/2035 train_time:33011ms step_avg:39.02ms
step:847/2035 train_time:33072ms step_avg:39.05ms
step:848/2035 train_time:33131ms step_avg:39.07ms
step:849/2035 train_time:33192ms step_avg:39.10ms
step:850/2035 train_time:33251ms step_avg:39.12ms
step:851/2035 train_time:33312ms step_avg:39.14ms
step:852/2035 train_time:33371ms step_avg:39.17ms
step:853/2035 train_time:33433ms step_avg:39.19ms
step:854/2035 train_time:33493ms step_avg:39.22ms
step:855/2035 train_time:33553ms step_avg:39.24ms
step:856/2035 train_time:33612ms step_avg:39.27ms
step:857/2035 train_time:33673ms step_avg:39.29ms
step:858/2035 train_time:33732ms step_avg:39.31ms
step:859/2035 train_time:33792ms step_avg:39.34ms
step:860/2035 train_time:33851ms step_avg:39.36ms
step:861/2035 train_time:33911ms step_avg:39.39ms
step:862/2035 train_time:33970ms step_avg:39.41ms
step:863/2035 train_time:34031ms step_avg:39.43ms
step:864/2035 train_time:34090ms step_avg:39.46ms
step:865/2035 train_time:34151ms step_avg:39.48ms
step:866/2035 train_time:34210ms step_avg:39.50ms
step:867/2035 train_time:34271ms step_avg:39.53ms
step:868/2035 train_time:34330ms step_avg:39.55ms
step:869/2035 train_time:34391ms step_avg:39.58ms
step:870/2035 train_time:34451ms step_avg:39.60ms
step:871/2035 train_time:34512ms step_avg:39.62ms
step:872/2035 train_time:34572ms step_avg:39.65ms
step:873/2035 train_time:34633ms step_avg:39.67ms
step:874/2035 train_time:34692ms step_avg:39.69ms
step:875/2035 train_time:34753ms step_avg:39.72ms
step:876/2035 train_time:34812ms step_avg:39.74ms
step:877/2035 train_time:34873ms step_avg:39.76ms
step:878/2035 train_time:34932ms step_avg:39.79ms
step:879/2035 train_time:34993ms step_avg:39.81ms
step:880/2035 train_time:35052ms step_avg:39.83ms
step:881/2035 train_time:35112ms step_avg:39.85ms
step:882/2035 train_time:35171ms step_avg:39.88ms
step:883/2035 train_time:35232ms step_avg:39.90ms
step:884/2035 train_time:35291ms step_avg:39.92ms
step:885/2035 train_time:35352ms step_avg:39.95ms
step:886/2035 train_time:35411ms step_avg:39.97ms
step:887/2035 train_time:35472ms step_avg:39.99ms
step:888/2035 train_time:35531ms step_avg:40.01ms
step:889/2035 train_time:35591ms step_avg:40.04ms
step:890/2035 train_time:35651ms step_avg:40.06ms
step:891/2035 train_time:35712ms step_avg:40.08ms
step:892/2035 train_time:35771ms step_avg:40.10ms
step:893/2035 train_time:35832ms step_avg:40.13ms
step:894/2035 train_time:35891ms step_avg:40.15ms
step:895/2035 train_time:35952ms step_avg:40.17ms
step:896/2035 train_time:36011ms step_avg:40.19ms
step:897/2035 train_time:36071ms step_avg:40.21ms
step:898/2035 train_time:36130ms step_avg:40.23ms
step:899/2035 train_time:36190ms step_avg:40.26ms
step:900/2035 train_time:36250ms step_avg:40.28ms
step:901/2035 train_time:36311ms step_avg:40.30ms
step:902/2035 train_time:36370ms step_avg:40.32ms
step:903/2035 train_time:36431ms step_avg:40.34ms
step:904/2035 train_time:36490ms step_avg:40.37ms
step:905/2035 train_time:36551ms step_avg:40.39ms
step:906/2035 train_time:36610ms step_avg:40.41ms
step:907/2035 train_time:36670ms step_avg:40.43ms
step:908/2035 train_time:36730ms step_avg:40.45ms
step:909/2035 train_time:36791ms step_avg:40.47ms
step:910/2035 train_time:36850ms step_avg:40.50ms
step:911/2035 train_time:36911ms step_avg:40.52ms
step:912/2035 train_time:36970ms step_avg:40.54ms
step:913/2035 train_time:37032ms step_avg:40.56ms
step:914/2035 train_time:37091ms step_avg:40.58ms
step:915/2035 train_time:37151ms step_avg:40.60ms
step:916/2035 train_time:37210ms step_avg:40.62ms
step:917/2035 train_time:37270ms step_avg:40.64ms
step:918/2035 train_time:37330ms step_avg:40.66ms
step:919/2035 train_time:37391ms step_avg:40.69ms
step:920/2035 train_time:37450ms step_avg:40.71ms
step:921/2035 train_time:37510ms step_avg:40.73ms
step:922/2035 train_time:37570ms step_avg:40.75ms
step:923/2035 train_time:37630ms step_avg:40.77ms
step:924/2035 train_time:37690ms step_avg:40.79ms
step:925/2035 train_time:37751ms step_avg:40.81ms
step:926/2035 train_time:37811ms step_avg:40.83ms
step:927/2035 train_time:37871ms step_avg:40.85ms
step:928/2035 train_time:37931ms step_avg:40.87ms
step:929/2035 train_time:37991ms step_avg:40.89ms
step:930/2035 train_time:38050ms step_avg:40.91ms
step:931/2035 train_time:38112ms step_avg:40.94ms
step:932/2035 train_time:38171ms step_avg:40.96ms
step:933/2035 train_time:38231ms step_avg:40.98ms
step:934/2035 train_time:38291ms step_avg:41.00ms
step:935/2035 train_time:38351ms step_avg:41.02ms
step:936/2035 train_time:38410ms step_avg:41.04ms
step:937/2035 train_time:38470ms step_avg:41.06ms
step:938/2035 train_time:38529ms step_avg:41.08ms
step:939/2035 train_time:38591ms step_avg:41.10ms
step:940/2035 train_time:38650ms step_avg:41.12ms
step:941/2035 train_time:38711ms step_avg:41.14ms
step:942/2035 train_time:38770ms step_avg:41.16ms
step:943/2035 train_time:38830ms step_avg:41.18ms
step:944/2035 train_time:38890ms step_avg:41.20ms
step:945/2035 train_time:38950ms step_avg:41.22ms
step:946/2035 train_time:39009ms step_avg:41.24ms
step:947/2035 train_time:39070ms step_avg:41.26ms
step:948/2035 train_time:39129ms step_avg:41.28ms
step:949/2035 train_time:39190ms step_avg:41.30ms
step:950/2035 train_time:39249ms step_avg:41.31ms
step:951/2035 train_time:39309ms step_avg:41.33ms
step:952/2035 train_time:39369ms step_avg:41.35ms
step:953/2035 train_time:39429ms step_avg:41.37ms
step:954/2035 train_time:39489ms step_avg:41.39ms
step:955/2035 train_time:39550ms step_avg:41.41ms
step:956/2035 train_time:39610ms step_avg:41.43ms
step:957/2035 train_time:39670ms step_avg:41.45ms
step:958/2035 train_time:39730ms step_avg:41.47ms
step:959/2035 train_time:39790ms step_avg:41.49ms
step:960/2035 train_time:39850ms step_avg:41.51ms
step:961/2035 train_time:39910ms step_avg:41.53ms
step:962/2035 train_time:39969ms step_avg:41.55ms
step:963/2035 train_time:40030ms step_avg:41.57ms
step:964/2035 train_time:40089ms step_avg:41.59ms
step:965/2035 train_time:40151ms step_avg:41.61ms
step:966/2035 train_time:40209ms step_avg:41.62ms
step:967/2035 train_time:40270ms step_avg:41.64ms
step:968/2035 train_time:40329ms step_avg:41.66ms
step:969/2035 train_time:40390ms step_avg:41.68ms
step:970/2035 train_time:40450ms step_avg:41.70ms
step:971/2035 train_time:40510ms step_avg:41.72ms
step:972/2035 train_time:40569ms step_avg:41.74ms
step:973/2035 train_time:40630ms step_avg:41.76ms
step:974/2035 train_time:40690ms step_avg:41.78ms
step:975/2035 train_time:40750ms step_avg:41.80ms
step:976/2035 train_time:40809ms step_avg:41.81ms
step:977/2035 train_time:40870ms step_avg:41.83ms
step:978/2035 train_time:40930ms step_avg:41.85ms
step:979/2035 train_time:40990ms step_avg:41.87ms
step:980/2035 train_time:41049ms step_avg:41.89ms
step:981/2035 train_time:41111ms step_avg:41.91ms
step:982/2035 train_time:41170ms step_avg:41.92ms
step:983/2035 train_time:41230ms step_avg:41.94ms
step:984/2035 train_time:41289ms step_avg:41.96ms
step:985/2035 train_time:41350ms step_avg:41.98ms
step:986/2035 train_time:41409ms step_avg:42.00ms
step:987/2035 train_time:41470ms step_avg:42.02ms
step:988/2035 train_time:41530ms step_avg:42.03ms
step:989/2035 train_time:41591ms step_avg:42.05ms
step:990/2035 train_time:41650ms step_avg:42.07ms
step:991/2035 train_time:41710ms step_avg:42.09ms
step:992/2035 train_time:41769ms step_avg:42.11ms
step:993/2035 train_time:41829ms step_avg:42.12ms
step:994/2035 train_time:41888ms step_avg:42.14ms
step:995/2035 train_time:41949ms step_avg:42.16ms
step:996/2035 train_time:42008ms step_avg:42.18ms
step:997/2035 train_time:42069ms step_avg:42.20ms
step:998/2035 train_time:42128ms step_avg:42.21ms
step:999/2035 train_time:42189ms step_avg:42.23ms
step:1000/2035 train_time:42249ms step_avg:42.25ms
step:1000/2035 val_loss:3.6871 train_time:42311ms step_avg:42.31ms
step:1001/2035 train_time:42331ms step_avg:42.29ms
step:1002/2035 train_time:42371ms step_avg:42.29ms
step:1003/2035 train_time:42433ms step_avg:42.31ms
step:1004/2035 train_time:42495ms step_avg:42.33ms
step:1005/2035 train_time:42556ms step_avg:42.34ms
step:1006/2035 train_time:42615ms step_avg:42.36ms
step:1007/2035 train_time:42675ms step_avg:42.38ms
step:1008/2035 train_time:42733ms step_avg:42.39ms
step:1009/2035 train_time:42793ms step_avg:42.41ms
step:1010/2035 train_time:42852ms step_avg:42.43ms
step:1011/2035 train_time:42912ms step_avg:42.44ms
step:1012/2035 train_time:42970ms step_avg:42.46ms
step:1013/2035 train_time:43030ms step_avg:42.48ms
step:1014/2035 train_time:43089ms step_avg:42.49ms
step:1015/2035 train_time:43149ms step_avg:42.51ms
step:1016/2035 train_time:43207ms step_avg:42.53ms
step:1017/2035 train_time:43269ms step_avg:42.55ms
step:1018/2035 train_time:43329ms step_avg:42.56ms
step:1019/2035 train_time:43391ms step_avg:42.58ms
step:1020/2035 train_time:43451ms step_avg:42.60ms
step:1021/2035 train_time:43513ms step_avg:42.62ms
step:1022/2035 train_time:43573ms step_avg:42.63ms
step:1023/2035 train_time:43633ms step_avg:42.65ms
step:1024/2035 train_time:43692ms step_avg:42.67ms
step:1025/2035 train_time:43752ms step_avg:42.68ms
step:1026/2035 train_time:43811ms step_avg:42.70ms
step:1027/2035 train_time:43871ms step_avg:42.72ms
step:1028/2035 train_time:43929ms step_avg:42.73ms
step:1029/2035 train_time:43989ms step_avg:42.75ms
step:1030/2035 train_time:44048ms step_avg:42.76ms
step:1031/2035 train_time:44107ms step_avg:42.78ms
step:1032/2035 train_time:44166ms step_avg:42.80ms
step:1033/2035 train_time:44226ms step_avg:42.81ms
step:1034/2035 train_time:44285ms step_avg:42.83ms
step:1035/2035 train_time:44346ms step_avg:42.85ms
step:1036/2035 train_time:44406ms step_avg:42.86ms
step:1037/2035 train_time:44467ms step_avg:42.88ms
step:1038/2035 train_time:44528ms step_avg:42.90ms
step:1039/2035 train_time:44588ms step_avg:42.91ms
step:1040/2035 train_time:44648ms step_avg:42.93ms
step:1041/2035 train_time:44708ms step_avg:42.95ms
step:1042/2035 train_time:44768ms step_avg:42.96ms
step:1043/2035 train_time:44828ms step_avg:42.98ms
step:1044/2035 train_time:44887ms step_avg:42.99ms
step:1045/2035 train_time:44947ms step_avg:43.01ms
step:1046/2035 train_time:45007ms step_avg:43.03ms
step:1047/2035 train_time:45067ms step_avg:43.04ms
step:1048/2035 train_time:45127ms step_avg:43.06ms
step:1049/2035 train_time:45186ms step_avg:43.08ms
step:1050/2035 train_time:45246ms step_avg:43.09ms
step:1051/2035 train_time:45306ms step_avg:43.11ms
step:1052/2035 train_time:45365ms step_avg:43.12ms
step:1053/2035 train_time:45428ms step_avg:43.14ms
step:1054/2035 train_time:45488ms step_avg:43.16ms
step:1055/2035 train_time:45549ms step_avg:43.17ms
step:1056/2035 train_time:45608ms step_avg:43.19ms
step:1057/2035 train_time:45670ms step_avg:43.21ms
step:1058/2035 train_time:45730ms step_avg:43.22ms
step:1059/2035 train_time:45790ms step_avg:43.24ms
step:1060/2035 train_time:45849ms step_avg:43.25ms
step:1061/2035 train_time:45909ms step_avg:43.27ms
step:1062/2035 train_time:45968ms step_avg:43.28ms
step:1063/2035 train_time:46029ms step_avg:43.30ms
step:1064/2035 train_time:46087ms step_avg:43.32ms
step:1065/2035 train_time:46148ms step_avg:43.33ms
step:1066/2035 train_time:46207ms step_avg:43.35ms
step:1067/2035 train_time:46268ms step_avg:43.36ms
step:1068/2035 train_time:46328ms step_avg:43.38ms
step:1069/2035 train_time:46389ms step_avg:43.39ms
step:1070/2035 train_time:46449ms step_avg:43.41ms
step:1071/2035 train_time:46510ms step_avg:43.43ms
step:1072/2035 train_time:46569ms step_avg:43.44ms
step:1073/2035 train_time:46629ms step_avg:43.46ms
step:1074/2035 train_time:46689ms step_avg:43.47ms
step:1075/2035 train_time:46750ms step_avg:43.49ms
step:1076/2035 train_time:46810ms step_avg:43.50ms
step:1077/2035 train_time:46870ms step_avg:43.52ms
step:1078/2035 train_time:46929ms step_avg:43.53ms
step:1079/2035 train_time:46989ms step_avg:43.55ms
step:1080/2035 train_time:47048ms step_avg:43.56ms
step:1081/2035 train_time:47108ms step_avg:43.58ms
step:1082/2035 train_time:47167ms step_avg:43.59ms
step:1083/2035 train_time:47228ms step_avg:43.61ms
step:1084/2035 train_time:47287ms step_avg:43.62ms
step:1085/2035 train_time:47348ms step_avg:43.64ms
step:1086/2035 train_time:47408ms step_avg:43.65ms
step:1087/2035 train_time:47468ms step_avg:43.67ms
step:1088/2035 train_time:47527ms step_avg:43.68ms
step:1089/2035 train_time:47588ms step_avg:43.70ms
step:1090/2035 train_time:47647ms step_avg:43.71ms
step:1091/2035 train_time:47708ms step_avg:43.73ms
step:1092/2035 train_time:47767ms step_avg:43.74ms
step:1093/2035 train_time:47828ms step_avg:43.76ms
step:1094/2035 train_time:47888ms step_avg:43.77ms
step:1095/2035 train_time:47948ms step_avg:43.79ms
step:1096/2035 train_time:48007ms step_avg:43.80ms
step:1097/2035 train_time:48067ms step_avg:43.82ms
step:1098/2035 train_time:48127ms step_avg:43.83ms
step:1099/2035 train_time:48187ms step_avg:43.85ms
step:1100/2035 train_time:48247ms step_avg:43.86ms
step:1101/2035 train_time:48307ms step_avg:43.88ms
step:1102/2035 train_time:48366ms step_avg:43.89ms
step:1103/2035 train_time:48428ms step_avg:43.91ms
step:1104/2035 train_time:48488ms step_avg:43.92ms
step:1105/2035 train_time:48548ms step_avg:43.93ms
step:1106/2035 train_time:48608ms step_avg:43.95ms
step:1107/2035 train_time:48668ms step_avg:43.96ms
step:1108/2035 train_time:48728ms step_avg:43.98ms
step:1109/2035 train_time:48788ms step_avg:43.99ms
step:1110/2035 train_time:48848ms step_avg:44.01ms
step:1111/2035 train_time:48909ms step_avg:44.02ms
step:1112/2035 train_time:48968ms step_avg:44.04ms
step:1113/2035 train_time:49028ms step_avg:44.05ms
step:1114/2035 train_time:49087ms step_avg:44.06ms
step:1115/2035 train_time:49147ms step_avg:44.08ms
step:1116/2035 train_time:49206ms step_avg:44.09ms
step:1117/2035 train_time:49267ms step_avg:44.11ms
step:1118/2035 train_time:49327ms step_avg:44.12ms
step:1119/2035 train_time:49387ms step_avg:44.13ms
step:1120/2035 train_time:49446ms step_avg:44.15ms
step:1121/2035 train_time:49507ms step_avg:44.16ms
step:1122/2035 train_time:49566ms step_avg:44.18ms
step:1123/2035 train_time:49627ms step_avg:44.19ms
step:1124/2035 train_time:49687ms step_avg:44.21ms
step:1125/2035 train_time:49748ms step_avg:44.22ms
step:1126/2035 train_time:49807ms step_avg:44.23ms
step:1127/2035 train_time:49868ms step_avg:44.25ms
step:1128/2035 train_time:49928ms step_avg:44.26ms
step:1129/2035 train_time:49988ms step_avg:44.28ms
step:1130/2035 train_time:50047ms step_avg:44.29ms
step:1131/2035 train_time:50108ms step_avg:44.30ms
step:1132/2035 train_time:50168ms step_avg:44.32ms
step:1133/2035 train_time:50228ms step_avg:44.33ms
step:1134/2035 train_time:50288ms step_avg:44.35ms
step:1135/2035 train_time:50348ms step_avg:44.36ms
step:1136/2035 train_time:50407ms step_avg:44.37ms
step:1137/2035 train_time:50468ms step_avg:44.39ms
step:1138/2035 train_time:50527ms step_avg:44.40ms
step:1139/2035 train_time:50587ms step_avg:44.41ms
step:1140/2035 train_time:50647ms step_avg:44.43ms
step:1141/2035 train_time:50707ms step_avg:44.44ms
step:1142/2035 train_time:50767ms step_avg:44.45ms
step:1143/2035 train_time:50827ms step_avg:44.47ms
step:1144/2035 train_time:50886ms step_avg:44.48ms
step:1145/2035 train_time:50947ms step_avg:44.49ms
step:1146/2035 train_time:51006ms step_avg:44.51ms
step:1147/2035 train_time:51066ms step_avg:44.52ms
step:1148/2035 train_time:51126ms step_avg:44.53ms
step:1149/2035 train_time:51186ms step_avg:44.55ms
step:1150/2035 train_time:51245ms step_avg:44.56ms
step:1151/2035 train_time:51306ms step_avg:44.57ms
step:1152/2035 train_time:51365ms step_avg:44.59ms
step:1153/2035 train_time:51426ms step_avg:44.60ms
step:1154/2035 train_time:51485ms step_avg:44.61ms
step:1155/2035 train_time:51546ms step_avg:44.63ms
step:1156/2035 train_time:51605ms step_avg:44.64ms
step:1157/2035 train_time:51666ms step_avg:44.66ms
step:1158/2035 train_time:51726ms step_avg:44.67ms
step:1159/2035 train_time:51786ms step_avg:44.68ms
step:1160/2035 train_time:51845ms step_avg:44.69ms
step:1161/2035 train_time:51906ms step_avg:44.71ms
step:1162/2035 train_time:51966ms step_avg:44.72ms
step:1163/2035 train_time:52026ms step_avg:44.73ms
step:1164/2035 train_time:52086ms step_avg:44.75ms
step:1165/2035 train_time:52147ms step_avg:44.76ms
step:1166/2035 train_time:52206ms step_avg:44.77ms
step:1167/2035 train_time:52267ms step_avg:44.79ms
step:1168/2035 train_time:52326ms step_avg:44.80ms
step:1169/2035 train_time:52387ms step_avg:44.81ms
step:1170/2035 train_time:52446ms step_avg:44.83ms
step:1171/2035 train_time:52507ms step_avg:44.84ms
step:1172/2035 train_time:52566ms step_avg:44.85ms
step:1173/2035 train_time:52627ms step_avg:44.87ms
step:1174/2035 train_time:52686ms step_avg:44.88ms
step:1175/2035 train_time:52746ms step_avg:44.89ms
step:1176/2035 train_time:52806ms step_avg:44.90ms
step:1177/2035 train_time:52867ms step_avg:44.92ms
step:1178/2035 train_time:52927ms step_avg:44.93ms
step:1179/2035 train_time:52987ms step_avg:44.94ms
step:1180/2035 train_time:53046ms step_avg:44.95ms
step:1181/2035 train_time:53106ms step_avg:44.97ms
step:1182/2035 train_time:53166ms step_avg:44.98ms
step:1183/2035 train_time:53226ms step_avg:44.99ms
step:1184/2035 train_time:53286ms step_avg:45.00ms
step:1185/2035 train_time:53346ms step_avg:45.02ms
step:1186/2035 train_time:53405ms step_avg:45.03ms
step:1187/2035 train_time:53466ms step_avg:45.04ms
step:1188/2035 train_time:53525ms step_avg:45.05ms
step:1189/2035 train_time:53586ms step_avg:45.07ms
step:1190/2035 train_time:53645ms step_avg:45.08ms
step:1191/2035 train_time:53705ms step_avg:45.09ms
step:1192/2035 train_time:53765ms step_avg:45.10ms
step:1193/2035 train_time:53827ms step_avg:45.12ms
step:1194/2035 train_time:53886ms step_avg:45.13ms
step:1195/2035 train_time:53947ms step_avg:45.14ms
step:1196/2035 train_time:54006ms step_avg:45.16ms
step:1197/2035 train_time:54066ms step_avg:45.17ms
step:1198/2035 train_time:54125ms step_avg:45.18ms
step:1199/2035 train_time:54186ms step_avg:45.19ms
step:1200/2035 train_time:54245ms step_avg:45.20ms
step:1201/2035 train_time:54305ms step_avg:45.22ms
step:1202/2035 train_time:54364ms step_avg:45.23ms
step:1203/2035 train_time:54425ms step_avg:45.24ms
step:1204/2035 train_time:54484ms step_avg:45.25ms
step:1205/2035 train_time:54544ms step_avg:45.27ms
step:1206/2035 train_time:54603ms step_avg:45.28ms
step:1207/2035 train_time:54664ms step_avg:45.29ms
step:1208/2035 train_time:54723ms step_avg:45.30ms
step:1209/2035 train_time:54784ms step_avg:45.31ms
step:1210/2035 train_time:54843ms step_avg:45.33ms
step:1211/2035 train_time:54904ms step_avg:45.34ms
step:1212/2035 train_time:54963ms step_avg:45.35ms
step:1213/2035 train_time:55025ms step_avg:45.36ms
step:1214/2035 train_time:55084ms step_avg:45.37ms
step:1215/2035 train_time:55144ms step_avg:45.39ms
step:1216/2035 train_time:55203ms step_avg:45.40ms
step:1217/2035 train_time:55264ms step_avg:45.41ms
step:1218/2035 train_time:55324ms step_avg:45.42ms
step:1219/2035 train_time:55384ms step_avg:45.43ms
step:1220/2035 train_time:55443ms step_avg:45.45ms
step:1221/2035 train_time:55504ms step_avg:45.46ms
step:1222/2035 train_time:55563ms step_avg:45.47ms
step:1223/2035 train_time:55624ms step_avg:45.48ms
step:1224/2035 train_time:55683ms step_avg:45.49ms
step:1225/2035 train_time:55744ms step_avg:45.51ms
step:1226/2035 train_time:55803ms step_avg:45.52ms
step:1227/2035 train_time:55864ms step_avg:45.53ms
step:1228/2035 train_time:55924ms step_avg:45.54ms
step:1229/2035 train_time:55985ms step_avg:45.55ms
step:1230/2035 train_time:56044ms step_avg:45.56ms
step:1231/2035 train_time:56105ms step_avg:45.58ms
step:1232/2035 train_time:56164ms step_avg:45.59ms
step:1233/2035 train_time:56226ms step_avg:45.60ms
step:1234/2035 train_time:56285ms step_avg:45.61ms
step:1235/2035 train_time:56345ms step_avg:45.62ms
step:1236/2035 train_time:56404ms step_avg:45.63ms
step:1237/2035 train_time:56465ms step_avg:45.65ms
step:1238/2035 train_time:56524ms step_avg:45.66ms
step:1239/2035 train_time:56585ms step_avg:45.67ms
step:1240/2035 train_time:56644ms step_avg:45.68ms
step:1241/2035 train_time:56705ms step_avg:45.69ms
step:1242/2035 train_time:56765ms step_avg:45.70ms
step:1243/2035 train_time:56826ms step_avg:45.72ms
step:1244/2035 train_time:56886ms step_avg:45.73ms
step:1245/2035 train_time:56946ms step_avg:45.74ms
step:1246/2035 train_time:57006ms step_avg:45.75ms
step:1247/2035 train_time:57066ms step_avg:45.76ms
step:1248/2035 train_time:57126ms step_avg:45.77ms
step:1249/2035 train_time:57186ms step_avg:45.79ms
step:1250/2035 train_time:57246ms step_avg:45.80ms
step:1250/2035 val_loss:3.5703 train_time:57308ms step_avg:45.85ms
step:1251/2035 train_time:57329ms step_avg:45.83ms
step:1252/2035 train_time:57367ms step_avg:45.82ms
step:1253/2035 train_time:57430ms step_avg:45.83ms
step:1254/2035 train_time:57492ms step_avg:45.85ms
step:1255/2035 train_time:57553ms step_avg:45.86ms
step:1256/2035 train_time:57612ms step_avg:45.87ms
step:1257/2035 train_time:57672ms step_avg:45.88ms
step:1258/2035 train_time:57731ms step_avg:45.89ms
step:1259/2035 train_time:57791ms step_avg:45.90ms
step:1260/2035 train_time:57849ms step_avg:45.91ms
step:1261/2035 train_time:57909ms step_avg:45.92ms
step:1262/2035 train_time:57967ms step_avg:45.93ms
step:1263/2035 train_time:58026ms step_avg:45.94ms
step:1264/2035 train_time:58085ms step_avg:45.95ms
step:1265/2035 train_time:58145ms step_avg:45.96ms
step:1266/2035 train_time:58204ms step_avg:45.97ms
step:1267/2035 train_time:58267ms step_avg:45.99ms
step:1268/2035 train_time:58328ms step_avg:46.00ms
step:1269/2035 train_time:58389ms step_avg:46.01ms
step:1270/2035 train_time:58450ms step_avg:46.02ms
step:1271/2035 train_time:58513ms step_avg:46.04ms
step:1272/2035 train_time:58572ms step_avg:46.05ms
step:1273/2035 train_time:58633ms step_avg:46.06ms
step:1274/2035 train_time:58692ms step_avg:46.07ms
step:1275/2035 train_time:58752ms step_avg:46.08ms
step:1276/2035 train_time:58810ms step_avg:46.09ms
step:1277/2035 train_time:58870ms step_avg:46.10ms
step:1278/2035 train_time:58929ms step_avg:46.11ms
step:1279/2035 train_time:58990ms step_avg:46.12ms
step:1280/2035 train_time:59049ms step_avg:46.13ms
step:1281/2035 train_time:59109ms step_avg:46.14ms
step:1282/2035 train_time:59169ms step_avg:46.15ms
step:1283/2035 train_time:59230ms step_avg:46.16ms
step:1284/2035 train_time:59289ms step_avg:46.18ms
step:1285/2035 train_time:59350ms step_avg:46.19ms
step:1286/2035 train_time:59410ms step_avg:46.20ms
step:1287/2035 train_time:59472ms step_avg:46.21ms
step:1288/2035 train_time:59531ms step_avg:46.22ms
step:1289/2035 train_time:59591ms step_avg:46.23ms
step:1290/2035 train_time:59651ms step_avg:46.24ms
step:1291/2035 train_time:59711ms step_avg:46.25ms
step:1292/2035 train_time:59770ms step_avg:46.26ms
step:1293/2035 train_time:59830ms step_avg:46.27ms
step:1294/2035 train_time:59889ms step_avg:46.28ms
step:1295/2035 train_time:59949ms step_avg:46.29ms
step:1296/2035 train_time:60008ms step_avg:46.30ms
step:1297/2035 train_time:60067ms step_avg:46.31ms
step:1298/2035 train_time:60127ms step_avg:46.32ms
step:1299/2035 train_time:60187ms step_avg:46.33ms
step:1300/2035 train_time:60247ms step_avg:46.34ms
step:1301/2035 train_time:60308ms step_avg:46.35ms
step:1302/2035 train_time:60368ms step_avg:46.37ms
step:1303/2035 train_time:60429ms step_avg:46.38ms
step:1304/2035 train_time:60489ms step_avg:46.39ms
step:1305/2035 train_time:60550ms step_avg:46.40ms
step:1306/2035 train_time:60609ms step_avg:46.41ms
step:1307/2035 train_time:60670ms step_avg:46.42ms
step:1308/2035 train_time:60729ms step_avg:46.43ms
step:1309/2035 train_time:60790ms step_avg:46.44ms
step:1310/2035 train_time:60849ms step_avg:46.45ms
step:1311/2035 train_time:60909ms step_avg:46.46ms
step:1312/2035 train_time:60968ms step_avg:46.47ms
step:1313/2035 train_time:61028ms step_avg:46.48ms
step:1314/2035 train_time:61087ms step_avg:46.49ms
step:1315/2035 train_time:61148ms step_avg:46.50ms
step:1316/2035 train_time:61207ms step_avg:46.51ms
step:1317/2035 train_time:61268ms step_avg:46.52ms
step:1318/2035 train_time:61327ms step_avg:46.53ms
step:1319/2035 train_time:61388ms step_avg:46.54ms
step:1320/2035 train_time:61447ms step_avg:46.55ms
step:1321/2035 train_time:61509ms step_avg:46.56ms
step:1322/2035 train_time:61568ms step_avg:46.57ms
step:1323/2035 train_time:61629ms step_avg:46.58ms
step:1324/2035 train_time:61688ms step_avg:46.59ms
step:1325/2035 train_time:61749ms step_avg:46.60ms
step:1326/2035 train_time:61808ms step_avg:46.61ms
step:1327/2035 train_time:61868ms step_avg:46.62ms
step:1328/2035 train_time:61927ms step_avg:46.63ms
step:1329/2035 train_time:61987ms step_avg:46.64ms
step:1330/2035 train_time:62046ms step_avg:46.65ms
step:1331/2035 train_time:62107ms step_avg:46.66ms
step:1332/2035 train_time:62194ms step_avg:46.69ms
step:1333/2035 train_time:62282ms step_avg:46.72ms
step:1334/2035 train_time:62369ms step_avg:46.75ms
step:1335/2035 train_time:62458ms step_avg:46.78ms
step:1336/2035 train_time:62545ms step_avg:46.81ms
step:1337/2035 train_time:62633ms step_avg:46.85ms
step:1338/2035 train_time:62719ms step_avg:46.88ms
step:1339/2035 train_time:62808ms step_avg:46.91ms
step:1340/2035 train_time:62894ms step_avg:46.94ms
step:1341/2035 train_time:62983ms step_avg:46.97ms
step:1342/2035 train_time:63069ms step_avg:47.00ms
step:1343/2035 train_time:63157ms step_avg:47.03ms
step:1344/2035 train_time:63243ms step_avg:47.06ms
step:1345/2035 train_time:63331ms step_avg:47.09ms
step:1346/2035 train_time:63418ms step_avg:47.12ms
step:1347/2035 train_time:63506ms step_avg:47.15ms
step:1348/2035 train_time:63594ms step_avg:47.18ms
step:1349/2035 train_time:63683ms step_avg:47.21ms
step:1350/2035 train_time:63771ms step_avg:47.24ms
step:1351/2035 train_time:63859ms step_avg:47.27ms
step:1352/2035 train_time:63946ms step_avg:47.30ms
step:1353/2035 train_time:64033ms step_avg:47.33ms
step:1354/2035 train_time:64120ms step_avg:47.36ms
step:1355/2035 train_time:64208ms step_avg:47.39ms
step:1356/2035 train_time:64294ms step_avg:47.41ms
step:1357/2035 train_time:64382ms step_avg:47.44ms
step:1358/2035 train_time:64469ms step_avg:47.47ms
step:1359/2035 train_time:64559ms step_avg:47.50ms
step:1360/2035 train_time:64646ms step_avg:47.53ms
step:1361/2035 train_time:64735ms step_avg:47.56ms
step:1362/2035 train_time:64822ms step_avg:47.59ms
step:1363/2035 train_time:64911ms step_avg:47.62ms
step:1364/2035 train_time:64998ms step_avg:47.65ms
step:1365/2035 train_time:65086ms step_avg:47.68ms
step:1366/2035 train_time:65172ms step_avg:47.71ms
step:1367/2035 train_time:65261ms step_avg:47.74ms
step:1368/2035 train_time:65347ms step_avg:47.77ms
step:1369/2035 train_time:65436ms step_avg:47.80ms
step:1370/2035 train_time:65523ms step_avg:47.83ms
step:1371/2035 train_time:65612ms step_avg:47.86ms
step:1372/2035 train_time:65700ms step_avg:47.89ms
step:1373/2035 train_time:65788ms step_avg:47.92ms
step:1374/2035 train_time:65875ms step_avg:47.94ms
step:1375/2035 train_time:65963ms step_avg:47.97ms
step:1376/2035 train_time:66050ms step_avg:48.00ms
step:1377/2035 train_time:66138ms step_avg:48.03ms
step:1378/2035 train_time:66226ms step_avg:48.06ms
step:1379/2035 train_time:66314ms step_avg:48.09ms
step:1380/2035 train_time:66400ms step_avg:48.12ms
step:1381/2035 train_time:66487ms step_avg:48.14ms
step:1382/2035 train_time:66575ms step_avg:48.17ms
step:1383/2035 train_time:66664ms step_avg:48.20ms
step:1384/2035 train_time:66751ms step_avg:48.23ms
step:1385/2035 train_time:66839ms step_avg:48.26ms
step:1386/2035 train_time:66926ms step_avg:48.29ms
step:1387/2035 train_time:67015ms step_avg:48.32ms
step:1388/2035 train_time:67101ms step_avg:48.34ms
step:1389/2035 train_time:67189ms step_avg:48.37ms
step:1390/2035 train_time:67277ms step_avg:48.40ms
step:1391/2035 train_time:67366ms step_avg:48.43ms
step:1392/2035 train_time:67452ms step_avg:48.46ms
step:1393/2035 train_time:67540ms step_avg:48.49ms
step:1394/2035 train_time:67628ms step_avg:48.51ms
step:1395/2035 train_time:67716ms step_avg:48.54ms
step:1396/2035 train_time:67803ms step_avg:48.57ms
step:1397/2035 train_time:67892ms step_avg:48.60ms
step:1398/2035 train_time:67978ms step_avg:48.63ms
step:1399/2035 train_time:68067ms step_avg:48.65ms
step:1400/2035 train_time:68154ms step_avg:48.68ms
step:1401/2035 train_time:68242ms step_avg:48.71ms
step:1402/2035 train_time:68329ms step_avg:48.74ms
step:1403/2035 train_time:68416ms step_avg:48.76ms
step:1404/2035 train_time:68503ms step_avg:48.79ms
step:1405/2035 train_time:68592ms step_avg:48.82ms
step:1406/2035 train_time:68679ms step_avg:48.85ms
step:1407/2035 train_time:68767ms step_avg:48.87ms
step:1408/2035 train_time:68854ms step_avg:48.90ms
step:1409/2035 train_time:68942ms step_avg:48.93ms
step:1410/2035 train_time:69029ms step_avg:48.96ms
step:1411/2035 train_time:69117ms step_avg:48.98ms
step:1412/2035 train_time:69204ms step_avg:49.01ms
step:1413/2035 train_time:69292ms step_avg:49.04ms
step:1414/2035 train_time:69379ms step_avg:49.07ms
step:1415/2035 train_time:69467ms step_avg:49.09ms
step:1416/2035 train_time:69553ms step_avg:49.12ms
step:1417/2035 train_time:69642ms step_avg:49.15ms
step:1418/2035 train_time:69729ms step_avg:49.17ms
step:1419/2035 train_time:69818ms step_avg:49.20ms
step:1420/2035 train_time:69904ms step_avg:49.23ms
step:1421/2035 train_time:69992ms step_avg:49.26ms
step:1422/2035 train_time:70080ms step_avg:49.28ms
step:1423/2035 train_time:70169ms step_avg:49.31ms
step:1424/2035 train_time:70256ms step_avg:49.34ms
step:1425/2035 train_time:70345ms step_avg:49.36ms
step:1426/2035 train_time:70432ms step_avg:49.39ms
step:1427/2035 train_time:70520ms step_avg:49.42ms
step:1428/2035 train_time:70607ms step_avg:49.44ms
step:1429/2035 train_time:70695ms step_avg:49.47ms
step:1430/2035 train_time:70782ms step_avg:49.50ms
step:1431/2035 train_time:70869ms step_avg:49.52ms
step:1432/2035 train_time:70956ms step_avg:49.55ms
step:1433/2035 train_time:71045ms step_avg:49.58ms
step:1434/2035 train_time:71131ms step_avg:49.60ms
step:1435/2035 train_time:71220ms step_avg:49.63ms
step:1436/2035 train_time:71307ms step_avg:49.66ms
step:1437/2035 train_time:71395ms step_avg:49.68ms
step:1438/2035 train_time:71481ms step_avg:49.71ms
step:1439/2035 train_time:71569ms step_avg:49.74ms
step:1440/2035 train_time:71656ms step_avg:49.76ms
step:1441/2035 train_time:71745ms step_avg:49.79ms
step:1442/2035 train_time:71832ms step_avg:49.81ms
step:1443/2035 train_time:71920ms step_avg:49.84ms
step:1444/2035 train_time:72007ms step_avg:49.87ms
step:1445/2035 train_time:72095ms step_avg:49.89ms
step:1446/2035 train_time:72182ms step_avg:49.92ms
step:1447/2035 train_time:72270ms step_avg:49.94ms
step:1448/2035 train_time:72356ms step_avg:49.97ms
step:1449/2035 train_time:72445ms step_avg:50.00ms
step:1450/2035 train_time:72532ms step_avg:50.02ms
step:1451/2035 train_time:72620ms step_avg:50.05ms
step:1452/2035 train_time:72707ms step_avg:50.07ms
step:1453/2035 train_time:72795ms step_avg:50.10ms
step:1454/2035 train_time:72881ms step_avg:50.12ms
step:1455/2035 train_time:72969ms step_avg:50.15ms
step:1456/2035 train_time:73057ms step_avg:50.18ms
step:1457/2035 train_time:73145ms step_avg:50.20ms
step:1458/2035 train_time:73232ms step_avg:50.23ms
step:1459/2035 train_time:73321ms step_avg:50.25ms
step:1460/2035 train_time:73408ms step_avg:50.28ms
step:1461/2035 train_time:73497ms step_avg:50.31ms
step:1462/2035 train_time:73584ms step_avg:50.33ms
step:1463/2035 train_time:73671ms step_avg:50.36ms
step:1464/2035 train_time:73758ms step_avg:50.38ms
step:1465/2035 train_time:73846ms step_avg:50.41ms
step:1466/2035 train_time:73933ms step_avg:50.43ms
step:1467/2035 train_time:74021ms step_avg:50.46ms
step:1468/2035 train_time:74109ms step_avg:50.48ms
step:1469/2035 train_time:74197ms step_avg:50.51ms
step:1470/2035 train_time:74285ms step_avg:50.53ms
step:1471/2035 train_time:74373ms step_avg:50.56ms
step:1472/2035 train_time:74460ms step_avg:50.58ms
step:1473/2035 train_time:74548ms step_avg:50.61ms
step:1474/2035 train_time:74635ms step_avg:50.63ms
step:1475/2035 train_time:74723ms step_avg:50.66ms
step:1476/2035 train_time:74809ms step_avg:50.68ms
step:1477/2035 train_time:74898ms step_avg:50.71ms
step:1478/2035 train_time:74985ms step_avg:50.73ms
step:1479/2035 train_time:75074ms step_avg:50.76ms
step:1480/2035 train_time:75161ms step_avg:50.78ms
step:1481/2035 train_time:75248ms step_avg:50.81ms
step:1482/2035 train_time:75335ms step_avg:50.83ms
step:1483/2035 train_time:75423ms step_avg:50.86ms
step:1484/2035 train_time:75510ms step_avg:50.88ms
step:1485/2035 train_time:75599ms step_avg:50.91ms
step:1486/2035 train_time:75685ms step_avg:50.93ms
step:1487/2035 train_time:75773ms step_avg:50.96ms
step:1488/2035 train_time:75860ms step_avg:50.98ms
step:1489/2035 train_time:75948ms step_avg:51.01ms
step:1490/2035 train_time:76036ms step_avg:51.03ms
step:1491/2035 train_time:76124ms step_avg:51.06ms
step:1492/2035 train_time:76211ms step_avg:51.08ms
step:1493/2035 train_time:76300ms step_avg:51.10ms
step:1494/2035 train_time:76387ms step_avg:51.13ms
step:1495/2035 train_time:76475ms step_avg:51.15ms
step:1496/2035 train_time:76562ms step_avg:51.18ms
step:1497/2035 train_time:76650ms step_avg:51.20ms
step:1498/2035 train_time:76736ms step_avg:51.23ms
step:1499/2035 train_time:76825ms step_avg:51.25ms
step:1500/2035 train_time:76912ms step_avg:51.27ms
step:1500/2035 val_loss:3.4543 train_time:77004ms step_avg:51.34ms
step:1501/2035 train_time:77025ms step_avg:51.32ms
step:1502/2035 train_time:77093ms step_avg:51.33ms
step:1503/2035 train_time:77187ms step_avg:51.36ms
step:1504/2035 train_time:77275ms step_avg:51.38ms
step:1505/2035 train_time:77363ms step_avg:51.40ms
step:1506/2035 train_time:77449ms step_avg:51.43ms
step:1507/2035 train_time:77536ms step_avg:51.45ms
step:1508/2035 train_time:77622ms step_avg:51.47ms
step:1509/2035 train_time:77709ms step_avg:51.50ms
step:1510/2035 train_time:77795ms step_avg:51.52ms
step:1511/2035 train_time:77882ms step_avg:51.54ms
step:1512/2035 train_time:77968ms step_avg:51.57ms
step:1513/2035 train_time:78058ms step_avg:51.59ms
step:1514/2035 train_time:78148ms step_avg:51.62ms
step:1515/2035 train_time:78238ms step_avg:51.64ms
step:1516/2035 train_time:78327ms step_avg:51.67ms
step:1517/2035 train_time:78415ms step_avg:51.69ms
step:1518/2035 train_time:78501ms step_avg:51.71ms
step:1519/2035 train_time:78588ms step_avg:51.74ms
step:1520/2035 train_time:78674ms step_avg:51.76ms
step:1521/2035 train_time:78761ms step_avg:51.78ms
step:1522/2035 train_time:78847ms step_avg:51.80ms
step:1523/2035 train_time:78934ms step_avg:51.83ms
step:1524/2035 train_time:79022ms step_avg:51.85ms
step:1525/2035 train_time:79112ms step_avg:51.88ms
step:1526/2035 train_time:79200ms step_avg:51.90ms
step:1527/2035 train_time:79289ms step_avg:51.92ms
step:1528/2035 train_time:79376ms step_avg:51.95ms
step:1529/2035 train_time:79465ms step_avg:51.97ms
step:1530/2035 train_time:79551ms step_avg:51.99ms
step:1531/2035 train_time:79639ms step_avg:52.02ms
step:1532/2035 train_time:79726ms step_avg:52.04ms
step:1533/2035 train_time:79814ms step_avg:52.06ms
step:1534/2035 train_time:79900ms step_avg:52.09ms
step:1535/2035 train_time:79989ms step_avg:52.11ms
step:1536/2035 train_time:80078ms step_avg:52.13ms
step:1537/2035 train_time:80168ms step_avg:52.16ms
step:1538/2035 train_time:80255ms step_avg:52.18ms
step:1539/2035 train_time:80344ms step_avg:52.21ms
step:1540/2035 train_time:80431ms step_avg:52.23ms
step:1541/2035 train_time:80519ms step_avg:52.25ms
step:1542/2035 train_time:80605ms step_avg:52.27ms
step:1543/2035 train_time:80693ms step_avg:52.30ms
step:1544/2035 train_time:80779ms step_avg:52.32ms
step:1545/2035 train_time:80867ms step_avg:52.34ms
step:1546/2035 train_time:80953ms step_avg:52.36ms
step:1547/2035 train_time:81042ms step_avg:52.39ms
step:1548/2035 train_time:81129ms step_avg:52.41ms
step:1549/2035 train_time:81217ms step_avg:52.43ms
step:1550/2035 train_time:81306ms step_avg:52.46ms
step:1551/2035 train_time:81395ms step_avg:52.48ms
step:1552/2035 train_time:81482ms step_avg:52.50ms
step:1553/2035 train_time:81570ms step_avg:52.52ms
step:1554/2035 train_time:81656ms step_avg:52.55ms
step:1555/2035 train_time:81744ms step_avg:52.57ms
step:1556/2035 train_time:81830ms step_avg:52.59ms
step:1557/2035 train_time:81918ms step_avg:52.61ms
step:1558/2035 train_time:82006ms step_avg:52.64ms
step:1559/2035 train_time:82094ms step_avg:52.66ms
step:1560/2035 train_time:82181ms step_avg:52.68ms
step:1561/2035 train_time:82270ms step_avg:52.70ms
step:1562/2035 train_time:82357ms step_avg:52.73ms
step:1563/2035 train_time:82446ms step_avg:52.75ms
step:1564/2035 train_time:82533ms step_avg:52.77ms
step:1565/2035 train_time:82620ms step_avg:52.79ms
step:1566/2035 train_time:82707ms step_avg:52.81ms
step:1567/2035 train_time:82794ms step_avg:52.84ms
step:1568/2035 train_time:82881ms step_avg:52.86ms
step:1569/2035 train_time:82968ms step_avg:52.88ms
step:1570/2035 train_time:83056ms step_avg:52.90ms
step:1571/2035 train_time:83144ms step_avg:52.92ms
step:1572/2035 train_time:83231ms step_avg:52.95ms
step:1573/2035 train_time:83318ms step_avg:52.97ms
step:1574/2035 train_time:83406ms step_avg:52.99ms
step:1575/2035 train_time:83494ms step_avg:53.01ms
step:1576/2035 train_time:83580ms step_avg:53.03ms
step:1577/2035 train_time:83668ms step_avg:53.06ms
step:1578/2035 train_time:83754ms step_avg:53.08ms
step:1579/2035 train_time:83842ms step_avg:53.10ms
step:1580/2035 train_time:83928ms step_avg:53.12ms
step:1581/2035 train_time:84017ms step_avg:53.14ms
step:1582/2035 train_time:84104ms step_avg:53.16ms
step:1583/2035 train_time:84192ms step_avg:53.19ms
step:1584/2035 train_time:84280ms step_avg:53.21ms
step:1585/2035 train_time:84369ms step_avg:53.23ms
step:1586/2035 train_time:84456ms step_avg:53.25ms
step:1587/2035 train_time:84545ms step_avg:53.27ms
step:1588/2035 train_time:84632ms step_avg:53.29ms
step:1589/2035 train_time:84720ms step_avg:53.32ms
step:1590/2035 train_time:84806ms step_avg:53.34ms
step:1591/2035 train_time:84894ms step_avg:53.36ms
step:1592/2035 train_time:84981ms step_avg:53.38ms
step:1593/2035 train_time:85070ms step_avg:53.40ms
step:1594/2035 train_time:85156ms step_avg:53.42ms
step:1595/2035 train_time:85246ms step_avg:53.45ms
step:1596/2035 train_time:85333ms step_avg:53.47ms
step:1597/2035 train_time:85421ms step_avg:53.49ms
step:1598/2035 train_time:85508ms step_avg:53.51ms
step:1599/2035 train_time:85597ms step_avg:53.53ms
step:1600/2035 train_time:85684ms step_avg:53.55ms
step:1601/2035 train_time:85771ms step_avg:53.57ms
step:1602/2035 train_time:85858ms step_avg:53.59ms
step:1603/2035 train_time:85948ms step_avg:53.62ms
step:1604/2035 train_time:86035ms step_avg:53.64ms
step:1605/2035 train_time:86122ms step_avg:53.66ms
step:1606/2035 train_time:86210ms step_avg:53.68ms
step:1607/2035 train_time:86298ms step_avg:53.70ms
step:1608/2035 train_time:86385ms step_avg:53.72ms
step:1609/2035 train_time:86473ms step_avg:53.74ms
step:1610/2035 train_time:86560ms step_avg:53.76ms
step:1611/2035 train_time:86649ms step_avg:53.79ms
step:1612/2035 train_time:86735ms step_avg:53.81ms
step:1613/2035 train_time:86823ms step_avg:53.83ms
step:1614/2035 train_time:86909ms step_avg:53.85ms
step:1615/2035 train_time:86997ms step_avg:53.87ms
step:1616/2035 train_time:87084ms step_avg:53.89ms
step:1617/2035 train_time:87172ms step_avg:53.91ms
step:1618/2035 train_time:87260ms step_avg:53.93ms
step:1619/2035 train_time:87350ms step_avg:53.95ms
step:1620/2035 train_time:87437ms step_avg:53.97ms
step:1621/2035 train_time:87526ms step_avg:53.99ms
step:1622/2035 train_time:87612ms step_avg:54.01ms
step:1623/2035 train_time:87701ms step_avg:54.04ms
step:1624/2035 train_time:87788ms step_avg:54.06ms
step:1625/2035 train_time:87876ms step_avg:54.08ms
step:1626/2035 train_time:87963ms step_avg:54.10ms
step:1627/2035 train_time:88051ms step_avg:54.12ms
step:1628/2035 train_time:88138ms step_avg:54.14ms
step:1629/2035 train_time:88226ms step_avg:54.16ms
step:1630/2035 train_time:88314ms step_avg:54.18ms
step:1631/2035 train_time:88402ms step_avg:54.20ms
step:1632/2035 train_time:88489ms step_avg:54.22ms
step:1633/2035 train_time:88577ms step_avg:54.24ms
step:1634/2035 train_time:88664ms step_avg:54.26ms
step:1635/2035 train_time:88751ms step_avg:54.28ms
step:1636/2035 train_time:88839ms step_avg:54.30ms
step:1637/2035 train_time:88929ms step_avg:54.32ms
step:1638/2035 train_time:89016ms step_avg:54.34ms
step:1639/2035 train_time:89105ms step_avg:54.37ms
step:1640/2035 train_time:89192ms step_avg:54.39ms
step:1641/2035 train_time:89279ms step_avg:54.41ms
step:1642/2035 train_time:89366ms step_avg:54.43ms
step:1643/2035 train_time:89455ms step_avg:54.45ms
step:1644/2035 train_time:89541ms step_avg:54.47ms
step:1645/2035 train_time:89629ms step_avg:54.49ms
step:1646/2035 train_time:89716ms step_avg:54.51ms
step:1647/2035 train_time:89804ms step_avg:54.53ms
step:1648/2035 train_time:89891ms step_avg:54.55ms
step:1649/2035 train_time:89979ms step_avg:54.57ms
step:1650/2035 train_time:90066ms step_avg:54.59ms
step:1651/2035 train_time:90154ms step_avg:54.61ms
step:1652/2035 train_time:90241ms step_avg:54.63ms
step:1653/2035 train_time:90329ms step_avg:54.65ms
step:1654/2035 train_time:90415ms step_avg:54.66ms
step:1655/2035 train_time:90503ms step_avg:54.68ms
step:1656/2035 train_time:90590ms step_avg:54.70ms
step:1657/2035 train_time:90678ms step_avg:54.72ms
step:1658/2035 train_time:90765ms step_avg:54.74ms
step:1659/2035 train_time:90853ms step_avg:54.76ms
step:1660/2035 train_time:90940ms step_avg:54.78ms
step:1661/2035 train_time:91029ms step_avg:54.80ms
step:1662/2035 train_time:91117ms step_avg:54.82ms
step:1663/2035 train_time:91205ms step_avg:54.84ms
step:1664/2035 train_time:91292ms step_avg:54.86ms
step:1665/2035 train_time:91380ms step_avg:54.88ms
step:1666/2035 train_time:91467ms step_avg:54.90ms
step:1667/2035 train_time:91555ms step_avg:54.92ms
step:1668/2035 train_time:91642ms step_avg:54.94ms
step:1669/2035 train_time:91730ms step_avg:54.96ms
step:1670/2035 train_time:91817ms step_avg:54.98ms
step:1671/2035 train_time:91906ms step_avg:55.00ms
step:1672/2035 train_time:91992ms step_avg:55.02ms
step:1673/2035 train_time:92081ms step_avg:55.04ms
step:1674/2035 train_time:92169ms step_avg:55.06ms
step:1675/2035 train_time:92257ms step_avg:55.08ms
step:1676/2035 train_time:92345ms step_avg:55.10ms
step:1677/2035 train_time:92433ms step_avg:55.12ms
step:1678/2035 train_time:92520ms step_avg:55.14ms
step:1679/2035 train_time:92608ms step_avg:55.16ms
step:1680/2035 train_time:92695ms step_avg:55.18ms
step:1681/2035 train_time:92784ms step_avg:55.20ms
step:1682/2035 train_time:92871ms step_avg:55.21ms
step:1683/2035 train_time:92959ms step_avg:55.23ms
step:1684/2035 train_time:93046ms step_avg:55.25ms
step:1685/2035 train_time:93135ms step_avg:55.27ms
step:1686/2035 train_time:93221ms step_avg:55.29ms
step:1687/2035 train_time:93310ms step_avg:55.31ms
step:1688/2035 train_time:93397ms step_avg:55.33ms
step:1689/2035 train_time:93485ms step_avg:55.35ms
step:1690/2035 train_time:93571ms step_avg:55.37ms
step:1691/2035 train_time:93659ms step_avg:55.39ms
step:1692/2035 train_time:93746ms step_avg:55.41ms
step:1693/2035 train_time:93834ms step_avg:55.42ms
step:1694/2035 train_time:93922ms step_avg:55.44ms
step:1695/2035 train_time:94010ms step_avg:55.46ms
step:1696/2035 train_time:94097ms step_avg:55.48ms
step:1697/2035 train_time:94185ms step_avg:55.50ms
step:1698/2035 train_time:94272ms step_avg:55.52ms
step:1699/2035 train_time:94360ms step_avg:55.54ms
step:1700/2035 train_time:94446ms step_avg:55.56ms
step:1701/2035 train_time:94535ms step_avg:55.58ms
step:1702/2035 train_time:94621ms step_avg:55.59ms
step:1703/2035 train_time:94709ms step_avg:55.61ms
step:1704/2035 train_time:94796ms step_avg:55.63ms
step:1705/2035 train_time:94885ms step_avg:55.65ms
step:1706/2035 train_time:94972ms step_avg:55.67ms
step:1707/2035 train_time:95061ms step_avg:55.69ms
step:1708/2035 train_time:95148ms step_avg:55.71ms
step:1709/2035 train_time:95236ms step_avg:55.73ms
step:1710/2035 train_time:95323ms step_avg:55.74ms
step:1711/2035 train_time:95411ms step_avg:55.76ms
step:1712/2035 train_time:95499ms step_avg:55.78ms
step:1713/2035 train_time:95588ms step_avg:55.80ms
step:1714/2035 train_time:95675ms step_avg:55.82ms
step:1715/2035 train_time:95764ms step_avg:55.84ms
step:1716/2035 train_time:95851ms step_avg:55.86ms
step:1717/2035 train_time:95939ms step_avg:55.88ms
step:1718/2035 train_time:96026ms step_avg:55.89ms
step:1719/2035 train_time:96114ms step_avg:55.91ms
step:1720/2035 train_time:96201ms step_avg:55.93ms
step:1721/2035 train_time:96290ms step_avg:55.95ms
step:1722/2035 train_time:96377ms step_avg:55.97ms
step:1723/2035 train_time:96466ms step_avg:55.99ms
step:1724/2035 train_time:96554ms step_avg:56.01ms
step:1725/2035 train_time:96642ms step_avg:56.02ms
step:1726/2035 train_time:96728ms step_avg:56.04ms
step:1727/2035 train_time:96817ms step_avg:56.06ms
step:1728/2035 train_time:96904ms step_avg:56.08ms
step:1729/2035 train_time:96993ms step_avg:56.10ms
step:1730/2035 train_time:97080ms step_avg:56.12ms
step:1731/2035 train_time:97168ms step_avg:56.13ms
step:1732/2035 train_time:97255ms step_avg:56.15ms
step:1733/2035 train_time:97343ms step_avg:56.17ms
step:1734/2035 train_time:97430ms step_avg:56.19ms
step:1735/2035 train_time:97518ms step_avg:56.21ms
step:1736/2035 train_time:97604ms step_avg:56.22ms
step:1737/2035 train_time:97694ms step_avg:56.24ms
step:1738/2035 train_time:97780ms step_avg:56.26ms
step:1739/2035 train_time:97869ms step_avg:56.28ms
step:1740/2035 train_time:97956ms step_avg:56.30ms
step:1741/2035 train_time:98046ms step_avg:56.32ms
step:1742/2035 train_time:98133ms step_avg:56.33ms
step:1743/2035 train_time:98221ms step_avg:56.35ms
step:1744/2035 train_time:98307ms step_avg:56.37ms
step:1745/2035 train_time:98397ms step_avg:56.39ms
step:1746/2035 train_time:98483ms step_avg:56.40ms
step:1747/2035 train_time:98571ms step_avg:56.42ms
step:1748/2035 train_time:98658ms step_avg:56.44ms
step:1749/2035 train_time:98747ms step_avg:56.46ms
step:1750/2035 train_time:98834ms step_avg:56.48ms
step:1750/2035 val_loss:3.3564 train_time:98923ms step_avg:56.53ms
step:1751/2035 train_time:98944ms step_avg:56.51ms
step:1752/2035 train_time:99013ms step_avg:56.51ms
step:1753/2035 train_time:99107ms step_avg:56.54ms
step:1754/2035 train_time:99195ms step_avg:56.55ms
step:1755/2035 train_time:99285ms step_avg:56.57ms
step:1756/2035 train_time:99371ms step_avg:56.59ms
step:1757/2035 train_time:99458ms step_avg:56.61ms
step:1758/2035 train_time:99544ms step_avg:56.62ms
step:1759/2035 train_time:99630ms step_avg:56.64ms
step:1760/2035 train_time:99716ms step_avg:56.66ms
step:1761/2035 train_time:99803ms step_avg:56.67ms
step:1762/2035 train_time:99890ms step_avg:56.69ms
step:1763/2035 train_time:99979ms step_avg:56.71ms
step:1764/2035 train_time:100068ms step_avg:56.73ms
step:1765/2035 train_time:100157ms step_avg:56.75ms
step:1766/2035 train_time:100245ms step_avg:56.76ms
step:1767/2035 train_time:100333ms step_avg:56.78ms
step:1768/2035 train_time:100420ms step_avg:56.80ms
step:1769/2035 train_time:100507ms step_avg:56.82ms
step:1770/2035 train_time:100594ms step_avg:56.83ms
step:1771/2035 train_time:100680ms step_avg:56.85ms
step:1772/2035 train_time:100766ms step_avg:56.87ms
step:1773/2035 train_time:100854ms step_avg:56.88ms
step:1774/2035 train_time:100942ms step_avg:56.90ms
step:1775/2035 train_time:101032ms step_avg:56.92ms
step:1776/2035 train_time:101120ms step_avg:56.94ms
step:1777/2035 train_time:101209ms step_avg:56.96ms
step:1778/2035 train_time:101297ms step_avg:56.97ms
step:1779/2035 train_time:101386ms step_avg:56.99ms
step:1780/2035 train_time:101472ms step_avg:57.01ms
step:1781/2035 train_time:101559ms step_avg:57.02ms
step:1782/2035 train_time:101644ms step_avg:57.04ms
step:1783/2035 train_time:101732ms step_avg:57.06ms
step:1784/2035 train_time:101818ms step_avg:57.07ms
step:1785/2035 train_time:101907ms step_avg:57.09ms
step:1786/2035 train_time:101994ms step_avg:57.11ms
step:1787/2035 train_time:102083ms step_avg:57.13ms
step:1788/2035 train_time:102171ms step_avg:57.14ms
step:1789/2035 train_time:102259ms step_avg:57.16ms
step:1790/2035 train_time:102347ms step_avg:57.18ms
step:1791/2035 train_time:102434ms step_avg:57.19ms
step:1792/2035 train_time:102521ms step_avg:57.21ms
step:1793/2035 train_time:102609ms step_avg:57.23ms
step:1794/2035 train_time:102695ms step_avg:57.24ms
step:1795/2035 train_time:102784ms step_avg:57.26ms
step:1796/2035 train_time:102870ms step_avg:57.28ms
step:1797/2035 train_time:102958ms step_avg:57.29ms
step:1798/2035 train_time:103046ms step_avg:57.31ms
step:1799/2035 train_time:103135ms step_avg:57.33ms
step:1800/2035 train_time:103222ms step_avg:57.35ms
step:1801/2035 train_time:103311ms step_avg:57.36ms
step:1802/2035 train_time:103398ms step_avg:57.38ms
step:1803/2035 train_time:103487ms step_avg:57.40ms
step:1804/2035 train_time:103573ms step_avg:57.41ms
step:1805/2035 train_time:103662ms step_avg:57.43ms
step:1806/2035 train_time:103749ms step_avg:57.45ms
step:1807/2035 train_time:103835ms step_avg:57.46ms
step:1808/2035 train_time:103922ms step_avg:57.48ms
step:1809/2035 train_time:104011ms step_avg:57.50ms
step:1810/2035 train_time:104098ms step_avg:57.51ms
step:1811/2035 train_time:104187ms step_avg:57.53ms
step:1812/2035 train_time:104275ms step_avg:57.55ms
step:1813/2035 train_time:104364ms step_avg:57.56ms
step:1814/2035 train_time:104451ms step_avg:57.58ms
step:1815/2035 train_time:104539ms step_avg:57.60ms
step:1816/2035 train_time:104626ms step_avg:57.61ms
step:1817/2035 train_time:104713ms step_avg:57.63ms
step:1818/2035 train_time:104799ms step_avg:57.65ms
step:1819/2035 train_time:104887ms step_avg:57.66ms
step:1820/2035 train_time:104974ms step_avg:57.68ms
step:1821/2035 train_time:105063ms step_avg:57.70ms
step:1822/2035 train_time:105150ms step_avg:57.71ms
step:1823/2035 train_time:105239ms step_avg:57.73ms
step:1824/2035 train_time:105325ms step_avg:57.74ms
step:1825/2035 train_time:105413ms step_avg:57.76ms
step:1826/2035 train_time:105500ms step_avg:57.78ms
step:1827/2035 train_time:105588ms step_avg:57.79ms
step:1828/2035 train_time:105675ms step_avg:57.81ms
step:1829/2035 train_time:105763ms step_avg:57.83ms
step:1830/2035 train_time:105849ms step_avg:57.84ms
step:1831/2035 train_time:105937ms step_avg:57.86ms
step:1832/2035 train_time:106024ms step_avg:57.87ms
step:1833/2035 train_time:106112ms step_avg:57.89ms
step:1834/2035 train_time:106199ms step_avg:57.91ms
step:1835/2035 train_time:106288ms step_avg:57.92ms
step:1836/2035 train_time:106375ms step_avg:57.94ms
step:1837/2035 train_time:106463ms step_avg:57.96ms
step:1838/2035 train_time:106550ms step_avg:57.97ms
step:1839/2035 train_time:106638ms step_avg:57.99ms
step:1840/2035 train_time:106725ms step_avg:58.00ms
step:1841/2035 train_time:106812ms step_avg:58.02ms
step:1842/2035 train_time:106900ms step_avg:58.03ms
step:1843/2035 train_time:106989ms step_avg:58.05ms
step:1844/2035 train_time:107076ms step_avg:58.07ms
step:1845/2035 train_time:107164ms step_avg:58.08ms
step:1846/2035 train_time:107251ms step_avg:58.10ms
step:1847/2035 train_time:107340ms step_avg:58.12ms
step:1848/2035 train_time:107427ms step_avg:58.13ms
step:1849/2035 train_time:107515ms step_avg:58.15ms
step:1850/2035 train_time:107602ms step_avg:58.16ms
step:1851/2035 train_time:107690ms step_avg:58.18ms
step:1852/2035 train_time:107777ms step_avg:58.20ms
step:1853/2035 train_time:107866ms step_avg:58.21ms
step:1854/2035 train_time:107953ms step_avg:58.23ms
step:1855/2035 train_time:108041ms step_avg:58.24ms
step:1856/2035 train_time:108128ms step_avg:58.26ms
step:1857/2035 train_time:108216ms step_avg:58.27ms
step:1858/2035 train_time:108303ms step_avg:58.29ms
step:1859/2035 train_time:108391ms step_avg:58.31ms
step:1860/2035 train_time:108478ms step_avg:58.32ms
step:1861/2035 train_time:108566ms step_avg:58.34ms
step:1862/2035 train_time:108654ms step_avg:58.35ms
step:1863/2035 train_time:108743ms step_avg:58.37ms
step:1864/2035 train_time:108830ms step_avg:58.39ms
step:1865/2035 train_time:108918ms step_avg:58.40ms
step:1866/2035 train_time:109005ms step_avg:58.42ms
step:1867/2035 train_time:109093ms step_avg:58.43ms
step:1868/2035 train_time:109180ms step_avg:58.45ms
step:1869/2035 train_time:109269ms step_avg:58.46ms
step:1870/2035 train_time:109355ms step_avg:58.48ms
step:1871/2035 train_time:109444ms step_avg:58.49ms
step:1872/2035 train_time:109531ms step_avg:58.51ms
step:1873/2035 train_time:109619ms step_avg:58.53ms
step:1874/2035 train_time:109705ms step_avg:58.54ms
step:1875/2035 train_time:109793ms step_avg:58.56ms
step:1876/2035 train_time:109880ms step_avg:58.57ms
step:1877/2035 train_time:109967ms step_avg:58.59ms
step:1878/2035 train_time:110054ms step_avg:58.60ms
step:1879/2035 train_time:110144ms step_avg:58.62ms
step:1880/2035 train_time:110231ms step_avg:58.63ms
step:1881/2035 train_time:110319ms step_avg:58.65ms
step:1882/2035 train_time:110406ms step_avg:58.66ms
step:1883/2035 train_time:110494ms step_avg:58.68ms
step:1884/2035 train_time:110581ms step_avg:58.69ms
step:1885/2035 train_time:110670ms step_avg:58.71ms
step:1886/2035 train_time:110756ms step_avg:58.73ms
step:1887/2035 train_time:110846ms step_avg:58.74ms
step:1888/2035 train_time:110932ms step_avg:58.76ms
step:1889/2035 train_time:111020ms step_avg:58.77ms
step:1890/2035 train_time:111107ms step_avg:58.79ms
step:1891/2035 train_time:111195ms step_avg:58.80ms
step:1892/2035 train_time:111283ms step_avg:58.82ms
step:1893/2035 train_time:111370ms step_avg:58.83ms
step:1894/2035 train_time:111457ms step_avg:58.85ms
step:1895/2035 train_time:111546ms step_avg:58.86ms
step:1896/2035 train_time:111633ms step_avg:58.88ms
step:1897/2035 train_time:111721ms step_avg:58.89ms
step:1898/2035 train_time:111808ms step_avg:58.91ms
step:1899/2035 train_time:111896ms step_avg:58.92ms
step:1900/2035 train_time:111983ms step_avg:58.94ms
step:1901/2035 train_time:112071ms step_avg:58.95ms
step:1902/2035 train_time:112157ms step_avg:58.97ms
step:1903/2035 train_time:112246ms step_avg:58.98ms
step:1904/2035 train_time:112332ms step_avg:59.00ms
step:1905/2035 train_time:112421ms step_avg:59.01ms
step:1906/2035 train_time:112508ms step_avg:59.03ms
step:1907/2035 train_time:112596ms step_avg:59.04ms
step:1908/2035 train_time:112683ms step_avg:59.06ms
step:1909/2035 train_time:112770ms step_avg:59.07ms
step:1910/2035 train_time:112857ms step_avg:59.09ms
step:1911/2035 train_time:112946ms step_avg:59.10ms
step:1912/2035 train_time:113032ms step_avg:59.12ms
step:1913/2035 train_time:113120ms step_avg:59.13ms
step:1914/2035 train_time:113206ms step_avg:59.15ms
step:1915/2035 train_time:113294ms step_avg:59.16ms
step:1916/2035 train_time:113382ms step_avg:59.18ms
step:1917/2035 train_time:113470ms step_avg:59.19ms
step:1918/2035 train_time:113557ms step_avg:59.21ms
step:1919/2035 train_time:113645ms step_avg:59.22ms
step:1920/2035 train_time:113732ms step_avg:59.24ms
step:1921/2035 train_time:113820ms step_avg:59.25ms
step:1922/2035 train_time:113906ms step_avg:59.26ms
step:1923/2035 train_time:113994ms step_avg:59.28ms
step:1924/2035 train_time:114082ms step_avg:59.29ms
step:1925/2035 train_time:114170ms step_avg:59.31ms
step:1926/2035 train_time:114256ms step_avg:59.32ms
step:1927/2035 train_time:114345ms step_avg:59.34ms
step:1928/2035 train_time:114432ms step_avg:59.35ms
step:1929/2035 train_time:114521ms step_avg:59.37ms
step:1930/2035 train_time:114608ms step_avg:59.38ms
step:1931/2035 train_time:114697ms step_avg:59.40ms
step:1932/2035 train_time:114783ms step_avg:59.41ms
step:1933/2035 train_time:114871ms step_avg:59.43ms
step:1934/2035 train_time:114957ms step_avg:59.44ms
step:1935/2035 train_time:115046ms step_avg:59.46ms
step:1936/2035 train_time:115133ms step_avg:59.47ms
step:1937/2035 train_time:115221ms step_avg:59.48ms
step:1938/2035 train_time:115309ms step_avg:59.50ms
step:1939/2035 train_time:115397ms step_avg:59.51ms
step:1940/2035 train_time:115484ms step_avg:59.53ms
step:1941/2035 train_time:115572ms step_avg:59.54ms
step:1942/2035 train_time:115659ms step_avg:59.56ms
step:1943/2035 train_time:115747ms step_avg:59.57ms
step:1944/2035 train_time:115834ms step_avg:59.59ms
step:1945/2035 train_time:115923ms step_avg:59.60ms
step:1946/2035 train_time:116010ms step_avg:59.61ms
step:1947/2035 train_time:116098ms step_avg:59.63ms
step:1948/2035 train_time:116185ms step_avg:59.64ms
step:1949/2035 train_time:116272ms step_avg:59.66ms
step:1950/2035 train_time:116360ms step_avg:59.67ms
step:1951/2035 train_time:116448ms step_avg:59.69ms
step:1952/2035 train_time:116536ms step_avg:59.70ms
step:1953/2035 train_time:116625ms step_avg:59.72ms
step:1954/2035 train_time:116711ms step_avg:59.73ms
step:1955/2035 train_time:116799ms step_avg:59.74ms
step:1956/2035 train_time:116885ms step_avg:59.76ms
step:1957/2035 train_time:116974ms step_avg:59.77ms
step:1958/2035 train_time:117061ms step_avg:59.79ms
step:1959/2035 train_time:117149ms step_avg:59.80ms
step:1960/2035 train_time:117235ms step_avg:59.81ms
step:1961/2035 train_time:117324ms step_avg:59.83ms
step:1962/2035 train_time:117412ms step_avg:59.84ms
step:1963/2035 train_time:117500ms step_avg:59.86ms
step:1964/2035 train_time:117586ms step_avg:59.87ms
step:1965/2035 train_time:117674ms step_avg:59.89ms
step:1966/2035 train_time:117761ms step_avg:59.90ms
step:1967/2035 train_time:117849ms step_avg:59.91ms
step:1968/2035 train_time:117936ms step_avg:59.93ms
step:1969/2035 train_time:118024ms step_avg:59.94ms
step:1970/2035 train_time:118111ms step_avg:59.95ms
step:1971/2035 train_time:118199ms step_avg:59.97ms
step:1972/2035 train_time:118285ms step_avg:59.98ms
step:1973/2035 train_time:118373ms step_avg:60.00ms
step:1974/2035 train_time:118460ms step_avg:60.01ms
step:1975/2035 train_time:118549ms step_avg:60.02ms
step:1976/2035 train_time:118635ms step_avg:60.04ms
step:1977/2035 train_time:118724ms step_avg:60.05ms
step:1978/2035 train_time:118811ms step_avg:60.07ms
step:1979/2035 train_time:118900ms step_avg:60.08ms
step:1980/2035 train_time:118988ms step_avg:60.09ms
step:1981/2035 train_time:119076ms step_avg:60.11ms
step:1982/2035 train_time:119162ms step_avg:60.12ms
step:1983/2035 train_time:119251ms step_avg:60.14ms
step:1984/2035 train_time:119337ms step_avg:60.15ms
step:1985/2035 train_time:119425ms step_avg:60.16ms
step:1986/2035 train_time:119512ms step_avg:60.18ms
step:1987/2035 train_time:119600ms step_avg:60.19ms
step:1988/2035 train_time:119687ms step_avg:60.20ms
step:1989/2035 train_time:119775ms step_avg:60.22ms
step:1990/2035 train_time:119862ms step_avg:60.23ms
step:1991/2035 train_time:119950ms step_avg:60.25ms
step:1992/2035 train_time:120037ms step_avg:60.26ms
step:1993/2035 train_time:120125ms step_avg:60.27ms
step:1994/2035 train_time:120212ms step_avg:60.29ms
step:1995/2035 train_time:120301ms step_avg:60.30ms
step:1996/2035 train_time:120387ms step_avg:60.31ms
step:1997/2035 train_time:120476ms step_avg:60.33ms
step:1998/2035 train_time:120563ms step_avg:60.34ms
step:1999/2035 train_time:120651ms step_avg:60.36ms
step:2000/2035 train_time:120740ms step_avg:60.37ms
step:2000/2035 val_loss:3.2832 train_time:120829ms step_avg:60.41ms
step:2001/2035 train_time:120850ms step_avg:60.39ms
step:2002/2035 train_time:120918ms step_avg:60.40ms
step:2003/2035 train_time:121010ms step_avg:60.41ms
step:2004/2035 train_time:121099ms step_avg:60.43ms
step:2005/2035 train_time:121187ms step_avg:60.44ms
step:2006/2035 train_time:121273ms step_avg:60.46ms
step:2007/2035 train_time:121360ms step_avg:60.47ms
step:2008/2035 train_time:121446ms step_avg:60.48ms
step:2009/2035 train_time:121533ms step_avg:60.49ms
step:2010/2035 train_time:121620ms step_avg:60.51ms
step:2011/2035 train_time:121709ms step_avg:60.52ms
step:2012/2035 train_time:121798ms step_avg:60.54ms
step:2013/2035 train_time:121889ms step_avg:60.55ms
step:2014/2035 train_time:121979ms step_avg:60.57ms
step:2015/2035 train_time:122068ms step_avg:60.58ms
step:2016/2035 train_time:122155ms step_avg:60.59ms
step:2017/2035 train_time:122243ms step_avg:60.61ms
step:2018/2035 train_time:122330ms step_avg:60.62ms
step:2019/2035 train_time:122419ms step_avg:60.63ms
step:2020/2035 train_time:122505ms step_avg:60.65ms
step:2021/2035 train_time:122593ms step_avg:60.66ms
step:2022/2035 train_time:122679ms step_avg:60.67ms
step:2023/2035 train_time:122768ms step_avg:60.69ms
step:2024/2035 train_time:122857ms step_avg:60.70ms
step:2025/2035 train_time:122947ms step_avg:60.71ms
step:2026/2035 train_time:123037ms step_avg:60.73ms
step:2027/2035 train_time:123126ms step_avg:60.74ms
step:2028/2035 train_time:123213ms step_avg:60.76ms
step:2029/2035 train_time:123301ms step_avg:60.77ms
step:2030/2035 train_time:123388ms step_avg:60.78ms
step:2031/2035 train_time:123475ms step_avg:60.80ms
step:2032/2035 train_time:123562ms step_avg:60.81ms
step:2033/2035 train_time:123650ms step_avg:60.82ms
step:2034/2035 train_time:123737ms step_avg:60.83ms
step:2035/2035 train_time:123826ms step_avg:60.85ms
step:2035/2035 val_loss:3.2760 train_time:123914ms step_avg:60.89ms
peak memory allocated: 29512 MiB reserved: 44036 MiB

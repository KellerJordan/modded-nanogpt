import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from collections import defaultdict
from itertools import accumulate
from pathlib import Path
import gc

os.environ["PYTORCH_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64


# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng

@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor, split_baddbmm: bool = False):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    # Select batched vs unbatched
    if split_baddbmm:
        BX_matmul = torch.bmm if X.ndim > 2 else torch.mm
    else:
        aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A

        # Referencing X twice causes pytorch to make a defensive copy,
        # resulting in a cudaMemcpyAsync in baddbmm.
        # For large matrices (i.e., the mlp weights), it's faster to split
        # the operation into two kernels to avoid this.
        if split_baddbmm:
            BX_matmul(B, X, out=C)  # C = B @ X
            C.add_(X, alpha=a)      # C = C + a*X  (in-place, X only read)
        else:
            aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X

        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X


# -----------------------------------------------------------------------------
# Compiled helpers for NorMuon by @chrisjmccormick

@torch.compile(dynamic=False, fullgraph=True)
def cautious_wd_and_update_inplace(p, v, wd_tensor, lr_tensor):
    """Cautious weight decay + parameter update. wd_tensor and lr_tensor are 0-D CPU tensors."""
    mask = (v * p) >= 0
    wd_factor = wd_tensor.to(p.dtype)
    lr_factor = lr_tensor.to(p.dtype)
    p.copy_(p - (p * mask * wd_factor * lr_factor) - (v * lr_factor))


@torch.compile(dynamic=False, fullgraph=True)
def apply_normuon_variance_reduction(v_chunk, second_momentum_buffer, beta2, red_dim):
    """NorMuon variance reduction. Algebraically fuses the normalization steps to minimize memory ops."""
    v_mean = v_chunk.float().square().mean(dim=red_dim, keepdim=True)
    red_dim_size = v_chunk.size(red_dim)
    v_norm_sq = v_mean.sum(dim=(-2, -1), keepdim=True).mul_(red_dim_size)
    v_norm = v_norm_sq.sqrt_()
    second_momentum_buffer.lerp_(v_mean.to(dtype=second_momentum_buffer.dtype), 1 - beta2)
    step_size = second_momentum_buffer.clamp_min(1e-10).rsqrt_()
    scaled_sq_sum = (v_mean * red_dim_size) * step_size.float().square()
    v_norm_new = scaled_sq_sum.sum(dim=(-2, -1), keepdim=True).sqrt_()
    final_scale = step_size * (v_norm / v_norm_new.clamp_min_(1e-10))
    return v_chunk.mul_(final_scale.type_as(v_chunk))


# -----------------------------------------------------------------------------
# NorMuon optimizer

class NorMuon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).

    Differences from standard Muon:
    - Newton-Shulz is replaced with Polar Express for the orthogonalization step
    - NorMuon adds a low-rank variance estimator similar to Adafactor. https://arxiv.org/pdf/2510.05491
    - small 1D parameters handled here instead of in Adam
    - Cautious weight decay, a gated version of decoupled weight decay
    - Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as
    needed on the forward pass. This enables attn and mlp weights to be contained within the same
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn.
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, beta2=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, beta2=beta2)
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def reset(self):
        # expose a reset for clearing buffers
        for group in self.param_groups:
            group["momentum_buffer"].zero_()
            group["second_momentum_buffer"].zero_()

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per module.
        """
        groups = defaultdict(list)
        for param in params:
            groups[param.label].append(param)

        param_groups = []
        for module_name, group_params in groups.items():
            chunk_size = (len(group_params) + self.world_size - 1) // self.world_size
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))

        return param_groups

    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn
        and mlp params when a param group is split across GPUs.
        """
        module_group_order = ['smear_gate', 'attn_gate', 'attn', 'mlp']
        params_list = list(params)
        params_list.sort(key=lambda x: module_group_order.index(x.label))

        idx = 0
        group_sizes = [1, 10, 16, 16]
        assert len(params_list) == sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            chunk_size = (size + self.world_size - 1) // self.world_size
            group_params = params_list[idx: idx + size]
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))
            idx += size

        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient distributed step by @YouJiacheng, @KonstantinWilleke, @alexrgilbert,
        # @adricarda, @tuttyfrutyee, @vdlad, @ryanyang0, @vagrawal, @varunneal, @chrisjmccormick
        rank = dist.get_rank()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            stacked_grads = torch.empty(
                (padded_num_params, *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device
            )
            for i, p in enumerate(params):
                stacked_grads[i].copy_(p.grad, non_blocking=True)
            if len(params) < padded_num_params:
                stacked_grads[len(params):].zero_()

            grad_chunk = torch.empty_like(stacked_grads[:chunk_size])

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(dict(grad_chunk=grad_chunk, reduce_future=reduce_future))

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = group["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            start_idx = rank * chunk_size
            module_idx = start_idx if start_idx < len(params) else 0

            num_params = min(chunk_size, max(0, len(params) - start_idx))  # num params for this rank

            if "momentum_buffer" not in group:
                group["momentum_buffer"]  = torch.zeros_like(grad_chunk[:num_params])
            momentum_buffer = group["momentum_buffer"]
            # Apply momentum update to the persistent momentum buffer in-place
            momentum_buffer.lerp_(grad_chunk[:num_params], 1 - group["momentum"])
            updated_grads = grad_chunk[:num_params].lerp_(momentum_buffer, group["momentum"])

            grad_shape = updated_grads.shape
            if params[module_idx].label == 'attn':

                for p in params[module_idx:module_idx + num_params]:
                    assert p.label == 'attn'

                updated_grads = updated_grads.view(4 * grad_shape[0], grad_shape[1] // 4, grad_shape[2])

            ref_param = params[module_idx]
            param_shape = ref_param.shape

            # The below shape-based heuristic assumes that matrices have their input along the
            # row dimension and their output along the columns. Gates are an exception.
            is_gate = ref_param.label in ['smear_gate', 'attn_gate']

            if "second_momentum_buffer" not in group:
                if is_gate:
                    group["second_momentum_buffer"] = torch.zeros_like(updated_grads[..., :, :1])
                else:
                    group["second_momentum_buffer"] = (torch.zeros_like(updated_grads[..., :, :1])
                        if param_shape[-2] >= param_shape[-1] else torch.zeros_like(updated_grads[..., :1, :])
                    )
            second_momentum_buffer = group["second_momentum_buffer"]

            if "param_lr_cpu" not in group:
                # Define multipliers for ALL params in this group (global, not per-shard)
                lr_mults = []
                wd_mults = []
                for p in params:
                    # Increase learning rate for modules with larger inputs than outputs.
                    # This shape check also assumes rows=input, columns=output, so take care
                    # when changing memory layouts. @chrisjmccormick
                    shape = p.shape
                    if len(shape) >= 2:
                        shape_mult = max(1.0, shape[-2] / shape[-1]) ** 0.5
                    else:
                        shape_mult = 1.0
                    lr_mults.append(shape_mult * getattr(p, "lr_mul", 1.0))
                    wd_mults.append(getattr(p, "wd_mul", 1.0))
                # Define as cpu tensors to enable Inductor constant folding
                group["param_lr_cpu"] = torch.tensor(lr_mults, dtype=torch.float32, device="cpu")
                group["param_wd_cpu"] = torch.tensor(wd_mults, dtype=torch.float32, device="cpu")

            eff_lr_all = group["param_lr_cpu"] * group["lr"]
            eff_wd_all = group["param_wd_cpu"] * group["weight_decay"] * group["lr"]

            # Slice the portion corresponding to this rank's shard
            eff_lr_cpu = eff_lr_all[module_idx:module_idx + num_params]
            eff_wd_cpu = eff_wd_all[module_idx:module_idx + num_params]

            # Compute zeropower for the entire chunk in a single, batched call.
            if num_params == 0:
                v_chunk = updated_grads
            else:
                v_chunk = polar_express(updated_grads, split_baddbmm=(ref_param.label == 'mlp'))

            # Note that the head orientation in O is transposed relative to QKV, so red_dim
            # is 'incorrect' for O. However, correcting this showed no improvement. @chrisjmccormick
            red_dim = -1 if (is_gate or param_shape[-2] >= param_shape[-1]) else -2

            v_chunk = apply_normuon_variance_reduction(
                v_chunk, second_momentum_buffer, group["beta2"], red_dim
            )

            v_chunk = v_chunk.view(grad_shape)

            # # "Cautious" weight decay (https://arxiv.org/abs/2510.12402)
            updated_params = torch.empty_like(grad_chunk)
            if num_params > 0:
                # Work on a stacked copy to avoid touching original params
                param_chunk = torch.stack(params[module_idx:module_idx + num_params])

                for local_idx in range(num_params):
                    cautious_wd_and_update_inplace(
                        param_chunk[local_idx],
                        v_chunk[local_idx],
                        eff_wd_cpu[local_idx],
                        eff_lr_cpu[local_idx],
                    )
            else:
                param_chunk = torch.zeros_like(v_chunk)

            updated_params[:num_params].copy_(param_chunk)
            if num_params < chunk_size:
                updated_params[num_params:].zero_()

            stacked_params = torch.empty(
                (padded_num_params, *param_shape),
                dtype=updated_params.dtype,
                device=updated_params.device,
            )

            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_params, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        # Group by label, with explicit ordering for execution control.
        label_order = ['lm_head', 'value_embed', 'scalars']
        params_by_label = defaultdict(list)
        for p in params:
            params_by_label[getattr(p, 'label', None)].append(p)
        param_groups = []
        for label in label_order:
            if label in params_by_label:
                param_groups.append(dict(params=params_by_label[label]))
        # include any unlabeled params at the end (processed last)
        if None in params_by_label:
            param_groups.append(dict(params=params_by_label[None]))
        super().__init__(param_groups, defaults)
        # init state
        for p in params:
            chunk_size = p.size(0) // self.world_size
            exp_avg = torch.zeros_like(p[:chunk_size], dtype=torch.bfloat16, device=p[0].device)
            exp_avg_sq = torch.zeros_like(exp_avg)
            self.state[p] = dict(step=0, exp_avg=exp_avg, exp_avg_sq=exp_avg_sq)
        # DistributedAdam implementation by @vagrawal, @akash5474

        self.should_sync = False

        self._reduce_scatter_hooks = []
        self._reduce_scatter_futures = {}
        self.register_backward_hooks()

    def register_backward_hooks(self):
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                hook = param.register_post_accumulate_grad_hook(self._sync_gradient)
                self._reduce_scatter_hooks.append(hook)

    @torch.no_grad()
    def _sync_gradient(self, param):
        if not self.should_sync:
            return

        grad = param.grad
        rank_size = grad.shape[0] // self.world_size
        grad_slice = torch.empty_like(grad[:rank_size])
        self._reduce_scatter_futures[param] = (
            dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future(),
            grad_slice
        )

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        all_gather_futures: list[torch.Future] = []

        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            for param in group['params']:
                if param not in self._reduce_scatter_futures:
                    continue

                fut, g_slice = self._reduce_scatter_futures[param]
                fut.wait()

                rank_size = param.shape[0] // self.world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]

                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (bias2 ** 0.5 / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                # cautious weight decay
                mask = (update * p_slice) >= 0
                # lr as weight decay schedule
                eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                update.addcmul_(p_slice, mask, value=eff_weight_decay * lr)

                p_slice.add_(other=update, alpha=-1.0)

                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())

        self._reduce_scatter_futures.clear()
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        with torch.no_grad():
            self.weight.zero_()  # @Grad62304977 and others

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))


# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float
    key_shift: bool

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKVO weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # Simplified layout by @chrisjmccormick
        self.qkvo_w = nn.Parameter(torch.empty(self.dim * 4, self.hdim))
        # label all modules for explicit optimizer grouping
        self.qkvo_w.label = 'attn'

        with torch.no_grad():
            self.qkvo_w[:self.dim * 3].uniform_(-bound, bound)  # init QKV weights
            self.qkvo_w[self.dim * 3:].zero_()  # init O weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        self.attn_gate.weight.label = 'attn_gate'

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas, key_shift = attn_args.ve, attn_args.sa_lambdas, attn_args.key_shift
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, sa_lambdas[0] * self.qkvo_w[:self.dim * 3].type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if key_shift:
            # shift keys forward for the stationary head dims. Enables 1-layer induction.
            k[:, 1:, :, self.head_dim//4:self.head_dim//2] = k[:, :-1, :, self.head_dim//4:self.head_dim//2]
            k[:, 1:, :, self.head_dim//4+self.head_dim//2:] = k[:, :-1, :, self.head_dim//4+self.head_dim//2:]
        if ve is not None:
            v = v + ve.view_as(v) # @ KoszarskyB & @Grad62304977

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens,
                                                        max_seqlen_q=max_len, max_seqlen_k=max_len,
                                                        causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, sa_lambdas[1] * self.qkvo_w[self.dim * 3:].type_as(y))  # sa_lambdas[1] pre-multiplied to O @shenberg
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # Transposed layout to match attention weights
        self.c_fc = nn.Parameter(torch.empty(hdim, dim))
        self.c_proj = nn.Parameter(torch.empty(hdim, dim))
        # label all modules for explicit optimizer grouping
        self.c_fc.label = 'mlp'
        self.c_proj.label = 'mlp'
        self.c_proj.lr_mul = 2.

        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.T.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.6 (the 7th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx != 6 else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim)

    def forward(self, x: Tensor, attn_args: AttnArgs):
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        self.num_layers = num_layers
        vocab_size = next_multiple_of_n(vocab_size, n=128)

        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.label = 'smear_gate'

        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        for embed in self.value_embeds:
            nn.init.zeros_(embed.weight)
        for ve in self.value_embeds:
            ve.weight.label = 'value_embed'
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)

        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=100/448, w_s=1.5/448, grad_s=0.75/448)
        nn.init.normal_(self.lm_head.weight, mean=0, std=0.005)
        self.lm_head.weight.label = 'lm_head'
        # Add learnable skip connection weights for decoder layers
        pad = (-num_layers * 4 - 3) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    1.1 * torch.ones(num_layers),  # resid lambdas. 1.1 init such that layer i weight is i^(num_layers-i).
                    0 * torch.ones(num_layers),  # x0 lambdas
                    *[torch.tensor([0.5, 1.0]) for _ in range(num_layers)],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    -1.5 * torch.ones(1),  # skip_lambda -> σ(-1.5) ≈ 0.18
                    torch.ones(pad),
                ]
            )
        )

        self.scalars.label = 'scalars'
        # set learning rates
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.scalars.lr_mul = 5.0
        self.scalars.wd_mul = 0.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        # set configs
        skip_connections = []
        skip_in = [3] # long attention window on layer 3
        skip_out = [6] # no attn op on layer 6
        x_backout = None
        backout_layer = 7

        # set lambdas
        resid_lambdas = self.scalars[: 1 * self.num_layers]
        x0_lambdas = self.scalars[1 * self.num_layers: 2 * self.num_layers]
        sa_lambdas = self.scalars[2 * self.num_layers: 4 * self.num_layers].view(-1, 2)
        smear_lambda = self.scalars[4 * self.num_layers]
        backout_lambda = self.scalars[4 * self.num_layers+1]
        skip_lambda = self.scalars[4 * self.num_layers+2]

        # set block masks and key shift
        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == self.num_layers
        key_shift = [b==long_bm for b in bm_sizes] # apply key shift to long windows

        # weight-tied: use lm_head.weight for embedding lookup
        x = F.embedding(input_seq, self.lm_head.weight)
        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [ve[1], ve[2]] + [None] * (self.num_layers - 5) + [ve[0], ve[1], ve[2]]
        assert len(ve) == self.num_layers

        # smear token embed forward 1 position @classiclarryd
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        for i in range(self.num_layers):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale,
                key_shift = key_shift[i]
            )
            if i in skip_out:
                gate = torch.sigmoid(skip_lambda)  # in (0, 1)
                x = x + gate * skip_connections.pop()
            if i == 0:
                x = (resid_lambdas[0] + x0_lambdas[0]) * x
            else:
                x = resid_lambdas[i] * x + x0_lambdas[i] * x0
            x = self.blocks[i](x, attn_args)
            if i in skip_in:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 7 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()

    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        # Cast to int32 on CPU before transfer to avoid dtype conversion during .to()
        _inputs = _inputs.to(dtype=torch.int32)
        _targets = _targets.to(dtype=torch.int64)
        _cum_lengths = _cum_lengths.to(dtype=torch.int32)

        new_params = yield (
            _inputs.to(device="cuda", non_blocking=True),
            _targets.to(device="cuda", non_blocking=True),
            _cum_lengths.to(device="cuda", non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * new_grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens // new_grad_accum_steps
            max_seq_len = new_max_seq_len


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # batch sizes
    train_bs_schedule: tuple = (8 * 2048 * 8, 16 * 2048 * 8, 24 * 2048 * 8)
    train_bs_extension: int = 24 * 2048 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 1995  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: float = 0.55  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_final: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=11,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=args.val_batch_size // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.weight.data = m.weight.data.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    embed_params + scalar_params + head_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.005,
)
optimizer2 = NorMuon(hidden_matrix_params + gate_params, lr=0.023, momentum=0.95, beta2=0.95, weight_decay=1.2)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: tied to batch size schedule, with cooldown at the end.
def get_lr(step: int):
    if step > args.num_scheduled_iterations:
        return 0.1
    lr_max = 1.0
    x = step / args.num_scheduled_iterations
    if x > 1/3:
       lr_max = 1.51  # (16/8)**0.6
    if x > 2/3:
        lr_max = 1.93  # (24/8)**0.6
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = lr_max * w + (1 - w) * 0.1
        return lr
    return lr_max

def get_ws(step: int):
    # set short window size to half of long window size
    # Higher ws on "extension" steps
    if step >= args.num_scheduled_iterations:
        return args.ws_final // 2, args.ws_final
    x = step / args.num_scheduled_iterations
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]


def get_bs(step: int):
    if step >= args.num_scheduled_iterations:
        return args.train_bs_extension
    x = step / args.num_scheduled_iterations
    bs_idx = int(len(args.train_bs_schedule) * x)
    return args.train_bs_schedule[bs_idx]


def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)
        # disable sync in the next training step for the adam optimizer
        optimizers[0].should_sync = False

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 10
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_bs_schedule[0], args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
ws_schedule = list(args.ws_schedule) + [args.ws_final]
bs_schedule = list(args.train_bs_schedule) + [args.train_bs_extension]
ws_long = ws_schedule[0]
model.train()
model.yarn.reset()
assert len(ws_schedule) == len(bs_schedule), "This warmup assumes len(ws_schedule) == len(bs_schedule)"

for idx in range(len(ws_schedule)):
    send_args = None
    if idx != 0:
        new_ws_long = ws_schedule[idx]
        model.yarn.apply(ws_long, new_ws_long)
        ws_long = new_ws_long
        send_args = (bs_schedule[idx], args.train_max_seq_len, grad_accum_steps)
    for step in range(warmup_steps):
        inputs, targets, cum_seqlens = train_loader.send(send_args)
        if step % 2 == 1:
            optimizers[0].should_sync = True
        model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
        if step % 2 == 0:
            optimizers[1].step()
            optimizers[1].zero_grad(set_to_none=True)
        else:
            for opt in optimizers:
                opt.step()
            model.zero_grad(set_to_none=True)
            optimizers[0].should_sync = False

model.zero_grad(set_to_none=True)
optimizers[0].should_sync = False
model.eval()

# warm up validation too
val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
val_loss = 0
with torch.no_grad():
    for step in range(val_steps):
        inputs, targets, cum_seqlens = next(val_loader)
        ws_idx = step % len(ws_schedule)
        if ws_idx==0:
            model.yarn.reset()
            ws_long = ws_schedule[0]
        else:
            new_ws_long = ws_schedule[ws_idx]
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
        val_loss += model(inputs, targets, cum_seqlens, ws_long // 2, ws_long)

del val_loader, val_loss
model.train()
model.yarn.reset() # rotary buffer is not stored in state_dict
optimizer2.reset() # muon momentum buffers not in state dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

step_batch_size = args.train_bs_schedule[0]
train_loader = distributed_data_generator(args.train_files, step_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)

gc.collect()

training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    new_step_batch_size = get_bs(step)
    send_args = (new_step_batch_size, args.train_max_seq_len, grad_accum_steps) if new_step_batch_size != step_batch_size else None
    step_batch_size = new_step_batch_size
    for idx in range(grad_accum_steps):
        # enable gradient sync for the DistAdam optimizer on the last iteration before we step it
        if idx == grad_accum_steps - 1 and step % 2 == 1:
            optimizers[0].should_sync = True

        inputs, targets, cum_seqlens = train_loader.send(send_args)
        (model(inputs, targets, cum_seqlens, ws_short, ws_long) / grad_accum_steps).backward()
    step_optimizers(step, optimizers, model)

    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.10.12 (main, Nov  4 2025, 08:48:33) [GCC 11.4.0]
Running PyTorch 2.10.0.dev20251204+cu126 compiled for CUDA 12.6
Running Triton version 3.6.0
Sat Dec 20 00:14:07 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:19:00.0 Off |                    0 |
| N/A   23C    P0            116W /  700W |    5858MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:3B:00.0 Off |                    0 |
| N/A   20C    P0            114W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:4C:00.0 Off |                    0 |
| N/A   25C    P0            118W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:5D:00.0 Off |                    0 |
| N/A   23C    P0            118W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:9B:00.0 Off |                    0 |
| N/A   24C    P0            119W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:BB:00.0 Off |                    0 |
| N/A   19C    P0            110W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:CB:00.0 Off |                    0 |
| N/A   24C    P0            119W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:DB:00.0 Off |                    0 |
| N/A   21C    P0            114W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A    299036      C   /home/ubuntu/.venv/bin/python3               1510MiB |
|    0   N/A  N/A    299037      C   /home/ubuntu/.venv/bin/python3                614MiB |
|    0   N/A  N/A    299038      C   /home/ubuntu/.venv/bin/python3                614MiB |
|    0   N/A  N/A    299039      C   /home/ubuntu/.venv/bin/python3                614MiB |
|    0   N/A  N/A    299040      C   /home/ubuntu/.venv/bin/python3                614MiB |
|    0   N/A  N/A    299041      C   /home/ubuntu/.venv/bin/python3                614MiB |
|    0   N/A  N/A    299042      C   /home/ubuntu/.venv/bin/python3                614MiB |
|    0   N/A  N/A    299043      C   /home/ubuntu/.venv/bin/python3                614MiB |
|    1   N/A  N/A    299037      C   /home/ubuntu/.venv/bin/python3               1510MiB |
|    2   N/A  N/A    299038      C   /home/ubuntu/.venv/bin/python3               1510MiB |
|    3   N/A  N/A    299039      C   /home/ubuntu/.venv/bin/python3               1510MiB |
|    4   N/A  N/A    299040      C   /home/ubuntu/.venv/bin/python3               1510MiB |
|    5   N/A  N/A    299041      C   /home/ubuntu/.venv/bin/python3               1510MiB |
|    6   N/A  N/A    299042      C   /home/ubuntu/.venv/bin/python3               1510MiB |
|    7   N/A  N/A    299043      C   /home/ubuntu/.venv/bin/python3               1510MiB |
+-----------------------------------------------------------------------------------------+

====================================================================================================
step:0/2035 val_loss:10.8360 train_time:0ms step_avg:0.03ms
step:1/2035 train_time:76ms step_avg:75.74ms
step:2/2035 train_time:98ms step_avg:49.20ms
step:3/2035 train_time:119ms step_avg:39.52ms
step:4/2035 train_time:149ms step_avg:37.33ms
step:5/2035 train_time:182ms step_avg:36.44ms
step:6/2035 train_time:410ms step_avg:68.31ms
step:7/2035 train_time:428ms step_avg:61.15ms
step:8/2035 train_time:455ms step_avg:56.82ms
step:9/2035 train_time:487ms step_avg:54.16ms
step:10/2035 train_time:520ms step_avg:52.04ms
step:11/2035 train_time:553ms step_avg:50.31ms
step:12/2035 train_time:586ms step_avg:48.86ms
step:13/2035 train_time:620ms step_avg:47.67ms
step:14/2035 train_time:653ms step_avg:46.62ms
step:15/2035 train_time:686ms step_avg:45.73ms
step:16/2035 train_time:719ms step_avg:44.94ms
step:17/2035 train_time:752ms step_avg:44.25ms
step:18/2035 train_time:786ms step_avg:43.64ms
step:19/2035 train_time:819ms step_avg:43.09ms
step:20/2035 train_time:852ms step_avg:42.59ms
step:21/2035 train_time:885ms step_avg:42.15ms
step:22/2035 train_time:918ms step_avg:41.74ms
step:23/2035 train_time:951ms step_avg:41.37ms
step:24/2035 train_time:985ms step_avg:41.04ms
step:25/2035 train_time:1018ms step_avg:40.71ms
step:26/2035 train_time:1051ms step_avg:40.42ms
step:27/2035 train_time:1084ms step_avg:40.14ms
step:28/2035 train_time:1117ms step_avg:39.89ms
step:29/2035 train_time:1150ms step_avg:39.66ms
step:30/2035 train_time:1183ms step_avg:39.43ms
step:31/2035 train_time:1216ms step_avg:39.23ms
step:32/2035 train_time:1249ms step_avg:39.03ms
step:33/2035 train_time:1282ms step_avg:38.86ms
step:34/2035 train_time:1315ms step_avg:38.69ms
step:35/2035 train_time:1349ms step_avg:38.55ms
step:36/2035 train_time:1383ms step_avg:38.41ms
step:37/2035 train_time:1416ms step_avg:38.28ms
step:38/2035 train_time:1449ms step_avg:38.14ms
step:39/2035 train_time:1483ms step_avg:38.03ms
step:40/2035 train_time:1516ms step_avg:37.90ms
step:41/2035 train_time:1549ms step_avg:37.79ms
step:42/2035 train_time:1582ms step_avg:37.68ms
step:43/2035 train_time:1616ms step_avg:37.57ms
step:44/2035 train_time:1649ms step_avg:37.47ms
step:45/2035 train_time:1682ms step_avg:37.38ms
step:46/2035 train_time:1715ms step_avg:37.28ms
step:47/2035 train_time:1749ms step_avg:37.21ms
step:48/2035 train_time:1782ms step_avg:37.12ms
step:49/2035 train_time:1815ms step_avg:37.04ms
step:50/2035 train_time:1848ms step_avg:36.97ms
step:51/2035 train_time:1881ms step_avg:36.89ms
step:52/2035 train_time:1914ms step_avg:36.81ms
step:53/2035 train_time:1948ms step_avg:36.75ms
step:54/2035 train_time:1981ms step_avg:36.69ms
step:55/2035 train_time:2014ms step_avg:36.62ms
step:56/2035 train_time:2047ms step_avg:36.56ms
step:57/2035 train_time:2080ms step_avg:36.49ms
step:58/2035 train_time:2113ms step_avg:36.44ms
step:59/2035 train_time:2147ms step_avg:36.38ms
step:60/2035 train_time:2180ms step_avg:36.33ms
step:61/2035 train_time:2213ms step_avg:36.28ms
step:62/2035 train_time:2246ms step_avg:36.22ms
step:63/2035 train_time:2279ms step_avg:36.17ms
step:64/2035 train_time:2312ms step_avg:36.12ms
step:65/2035 train_time:2345ms step_avg:36.08ms
step:66/2035 train_time:2378ms step_avg:36.03ms
step:67/2035 train_time:2412ms step_avg:35.99ms
step:68/2035 train_time:2445ms step_avg:35.95ms
step:69/2035 train_time:2478ms step_avg:35.91ms
step:70/2035 train_time:2511ms step_avg:35.87ms
step:71/2035 train_time:2544ms step_avg:35.83ms
step:72/2035 train_time:2577ms step_avg:35.80ms
step:73/2035 train_time:2610ms step_avg:35.76ms
step:74/2035 train_time:2643ms step_avg:35.72ms
step:75/2035 train_time:2677ms step_avg:35.69ms
step:76/2035 train_time:2710ms step_avg:35.65ms
step:77/2035 train_time:2743ms step_avg:35.62ms
step:78/2035 train_time:2776ms step_avg:35.59ms
step:79/2035 train_time:2809ms step_avg:35.56ms
step:80/2035 train_time:2842ms step_avg:35.53ms
step:81/2035 train_time:2876ms step_avg:35.50ms
step:82/2035 train_time:2908ms step_avg:35.47ms
step:83/2035 train_time:2941ms step_avg:35.44ms
step:84/2035 train_time:2974ms step_avg:35.41ms
step:85/2035 train_time:3008ms step_avg:35.39ms
step:86/2035 train_time:3041ms step_avg:35.36ms
step:87/2035 train_time:3074ms step_avg:35.34ms
step:88/2035 train_time:3107ms step_avg:35.31ms
step:89/2035 train_time:3140ms step_avg:35.28ms
step:90/2035 train_time:3173ms step_avg:35.26ms
step:91/2035 train_time:3207ms step_avg:35.24ms
step:92/2035 train_time:3239ms step_avg:35.21ms
step:93/2035 train_time:3273ms step_avg:35.19ms
step:94/2035 train_time:3306ms step_avg:35.17ms
step:95/2035 train_time:3339ms step_avg:35.15ms
step:96/2035 train_time:3372ms step_avg:35.12ms
step:97/2035 train_time:3405ms step_avg:35.11ms
step:98/2035 train_time:3438ms step_avg:35.08ms
step:99/2035 train_time:3472ms step_avg:35.07ms
step:100/2035 train_time:3505ms step_avg:35.05ms
step:101/2035 train_time:3538ms step_avg:35.03ms
step:102/2035 train_time:3571ms step_avg:35.01ms
step:103/2035 train_time:3604ms step_avg:34.99ms
step:104/2035 train_time:3637ms step_avg:34.97ms
step:105/2035 train_time:3670ms step_avg:34.96ms
step:106/2035 train_time:3703ms step_avg:34.94ms
step:107/2035 train_time:3736ms step_avg:34.92ms
step:108/2035 train_time:3769ms step_avg:34.90ms
step:109/2035 train_time:3803ms step_avg:34.89ms
step:110/2035 train_time:3835ms step_avg:34.87ms
step:111/2035 train_time:3869ms step_avg:34.85ms
step:112/2035 train_time:3902ms step_avg:34.83ms
step:113/2035 train_time:3935ms step_avg:34.82ms
step:114/2035 train_time:3968ms step_avg:34.80ms
step:115/2035 train_time:4000ms step_avg:34.79ms
step:116/2035 train_time:4033ms step_avg:34.77ms
step:117/2035 train_time:4067ms step_avg:34.76ms
step:118/2035 train_time:4100ms step_avg:34.74ms
step:119/2035 train_time:4133ms step_avg:34.73ms
step:120/2035 train_time:4166ms step_avg:34.71ms
step:121/2035 train_time:4199ms step_avg:34.70ms
step:122/2035 train_time:4232ms step_avg:34.69ms
step:123/2035 train_time:4265ms step_avg:34.67ms
step:124/2035 train_time:4298ms step_avg:34.66ms
step:125/2035 train_time:4331ms step_avg:34.65ms
step:126/2035 train_time:4364ms step_avg:34.64ms
step:127/2035 train_time:4397ms step_avg:34.63ms
step:128/2035 train_time:4430ms step_avg:34.61ms
step:129/2035 train_time:4464ms step_avg:34.60ms
step:130/2035 train_time:4497ms step_avg:34.59ms
step:131/2035 train_time:4530ms step_avg:34.58ms
step:132/2035 train_time:4563ms step_avg:34.57ms
step:133/2035 train_time:4596ms step_avg:34.56ms
step:134/2035 train_time:4629ms step_avg:34.54ms
step:135/2035 train_time:4662ms step_avg:34.54ms
step:136/2035 train_time:4695ms step_avg:34.52ms
step:137/2035 train_time:4729ms step_avg:34.52ms
step:138/2035 train_time:4762ms step_avg:34.50ms
step:139/2035 train_time:4795ms step_avg:34.50ms
step:140/2035 train_time:4828ms step_avg:34.49ms
step:141/2035 train_time:4861ms step_avg:34.48ms
step:142/2035 train_time:4894ms step_avg:34.46ms
step:143/2035 train_time:4927ms step_avg:34.46ms
step:144/2035 train_time:4960ms step_avg:34.45ms
step:145/2035 train_time:4993ms step_avg:34.43ms
step:146/2035 train_time:5026ms step_avg:34.42ms
step:147/2035 train_time:5059ms step_avg:34.41ms
step:148/2035 train_time:5091ms step_avg:34.40ms
step:149/2035 train_time:5125ms step_avg:34.40ms
step:150/2035 train_time:5158ms step_avg:34.39ms
step:151/2035 train_time:5191ms step_avg:34.38ms
step:152/2035 train_time:5224ms step_avg:34.37ms
step:153/2035 train_time:5257ms step_avg:34.36ms
step:154/2035 train_time:5290ms step_avg:34.35ms
step:155/2035 train_time:5323ms step_avg:34.34ms
step:156/2035 train_time:5356ms step_avg:34.33ms
step:157/2035 train_time:5389ms step_avg:34.33ms
step:158/2035 train_time:5422ms step_avg:34.32ms
step:159/2035 train_time:5456ms step_avg:34.31ms
step:160/2035 train_time:5488ms step_avg:34.30ms
step:161/2035 train_time:5522ms step_avg:34.30ms
step:162/2035 train_time:5555ms step_avg:34.29ms
step:163/2035 train_time:5588ms step_avg:34.28ms
step:164/2035 train_time:5621ms step_avg:34.27ms
step:165/2035 train_time:5654ms step_avg:34.27ms
step:166/2035 train_time:5687ms step_avg:34.26ms
step:167/2035 train_time:5720ms step_avg:34.25ms
step:168/2035 train_time:5753ms step_avg:34.24ms
step:169/2035 train_time:5786ms step_avg:34.24ms
step:170/2035 train_time:5819ms step_avg:34.23ms
step:171/2035 train_time:5852ms step_avg:34.22ms
step:172/2035 train_time:5885ms step_avg:34.21ms
step:173/2035 train_time:5918ms step_avg:34.21ms
step:174/2035 train_time:5951ms step_avg:34.20ms
step:175/2035 train_time:5984ms step_avg:34.19ms
step:176/2035 train_time:6017ms step_avg:34.19ms
step:177/2035 train_time:6050ms step_avg:34.18ms
step:178/2035 train_time:6083ms step_avg:34.17ms
step:179/2035 train_time:6116ms step_avg:34.17ms
step:180/2035 train_time:6149ms step_avg:34.16ms
step:181/2035 train_time:6182ms step_avg:34.15ms
step:182/2035 train_time:6215ms step_avg:34.15ms
step:183/2035 train_time:6248ms step_avg:34.14ms
step:184/2035 train_time:6281ms step_avg:34.13ms
step:185/2035 train_time:6313ms step_avg:34.13ms
step:186/2035 train_time:6346ms step_avg:34.12ms
step:187/2035 train_time:6379ms step_avg:34.11ms
step:188/2035 train_time:6412ms step_avg:34.11ms
step:189/2035 train_time:6445ms step_avg:34.10ms
step:190/2035 train_time:6478ms step_avg:34.09ms
step:191/2035 train_time:6511ms step_avg:34.09ms
step:192/2035 train_time:6544ms step_avg:34.08ms
step:193/2035 train_time:6577ms step_avg:34.08ms
step:194/2035 train_time:6610ms step_avg:34.07ms
step:195/2035 train_time:6643ms step_avg:34.06ms
step:196/2035 train_time:6676ms step_avg:34.06ms
step:197/2035 train_time:6709ms step_avg:34.06ms
step:198/2035 train_time:6742ms step_avg:34.05ms
step:199/2035 train_time:6775ms step_avg:34.05ms
step:200/2035 train_time:6808ms step_avg:34.04ms
step:201/2035 train_time:6842ms step_avg:34.04ms
step:202/2035 train_time:6875ms step_avg:34.03ms
step:203/2035 train_time:6908ms step_avg:34.03ms
step:204/2035 train_time:6941ms step_avg:34.02ms
step:205/2035 train_time:6974ms step_avg:34.02ms
step:206/2035 train_time:7007ms step_avg:34.02ms
step:207/2035 train_time:7040ms step_avg:34.01ms
step:208/2035 train_time:7073ms step_avg:34.00ms
step:209/2035 train_time:7105ms step_avg:34.00ms
step:210/2035 train_time:7138ms step_avg:33.99ms
step:211/2035 train_time:7171ms step_avg:33.99ms
step:212/2035 train_time:7204ms step_avg:33.98ms
step:213/2035 train_time:7237ms step_avg:33.98ms
step:214/2035 train_time:7270ms step_avg:33.97ms
step:215/2035 train_time:7303ms step_avg:33.97ms
step:216/2035 train_time:7336ms step_avg:33.96ms
step:217/2035 train_time:7369ms step_avg:33.96ms
step:218/2035 train_time:7402ms step_avg:33.95ms
step:219/2035 train_time:7435ms step_avg:33.95ms
step:220/2035 train_time:7468ms step_avg:33.95ms
step:221/2035 train_time:7501ms step_avg:33.94ms
step:222/2035 train_time:7534ms step_avg:33.94ms
step:223/2035 train_time:7567ms step_avg:33.93ms
step:224/2035 train_time:7600ms step_avg:33.93ms
step:225/2035 train_time:7633ms step_avg:33.92ms
step:226/2035 train_time:7666ms step_avg:33.92ms
step:227/2035 train_time:7699ms step_avg:33.91ms
step:228/2035 train_time:7732ms step_avg:33.91ms
step:229/2035 train_time:7765ms step_avg:33.91ms
step:230/2035 train_time:7798ms step_avg:33.90ms
step:231/2035 train_time:7831ms step_avg:33.90ms
step:232/2035 train_time:7864ms step_avg:33.89ms
step:233/2035 train_time:7896ms step_avg:33.89ms
step:234/2035 train_time:7930ms step_avg:33.89ms
step:235/2035 train_time:7962ms step_avg:33.88ms
step:236/2035 train_time:7995ms step_avg:33.88ms
step:237/2035 train_time:8028ms step_avg:33.87ms
step:238/2035 train_time:8061ms step_avg:33.87ms
step:239/2035 train_time:8094ms step_avg:33.87ms
step:240/2035 train_time:8127ms step_avg:33.86ms
step:241/2035 train_time:8159ms step_avg:33.86ms
step:242/2035 train_time:8192ms step_avg:33.85ms
step:243/2035 train_time:8225ms step_avg:33.85ms
step:244/2035 train_time:8258ms step_avg:33.85ms
step:245/2035 train_time:8291ms step_avg:33.84ms
step:246/2035 train_time:8324ms step_avg:33.84ms
step:247/2035 train_time:8357ms step_avg:33.83ms
step:248/2035 train_time:8390ms step_avg:33.83ms
step:249/2035 train_time:8423ms step_avg:33.83ms
step:250/2035 train_time:8456ms step_avg:33.82ms
step:250/2035 val_loss:4.2632 train_time:8491ms step_avg:33.97ms
step:251/2035 train_time:8511ms step_avg:33.91ms
step:252/2035 train_time:8532ms step_avg:33.86ms
step:253/2035 train_time:8562ms step_avg:33.84ms
step:254/2035 train_time:8595ms step_avg:33.84ms
step:255/2035 train_time:8629ms step_avg:33.84ms
step:256/2035 train_time:8662ms step_avg:33.84ms
step:257/2035 train_time:8696ms step_avg:33.84ms
step:258/2035 train_time:8729ms step_avg:33.84ms
step:259/2035 train_time:8763ms step_avg:33.83ms
step:260/2035 train_time:8796ms step_avg:33.83ms
step:261/2035 train_time:8829ms step_avg:33.83ms
step:262/2035 train_time:8862ms step_avg:33.82ms
step:263/2035 train_time:8895ms step_avg:33.82ms
step:264/2035 train_time:8928ms step_avg:33.82ms
step:265/2035 train_time:8960ms step_avg:33.81ms
step:266/2035 train_time:8993ms step_avg:33.81ms
step:267/2035 train_time:9026ms step_avg:33.80ms
step:268/2035 train_time:9059ms step_avg:33.80ms
step:269/2035 train_time:9091ms step_avg:33.80ms
step:270/2035 train_time:9124ms step_avg:33.79ms
step:271/2035 train_time:9157ms step_avg:33.79ms
step:272/2035 train_time:9190ms step_avg:33.79ms
step:273/2035 train_time:9223ms step_avg:33.78ms
step:274/2035 train_time:9256ms step_avg:33.78ms
step:275/2035 train_time:9288ms step_avg:33.78ms
step:276/2035 train_time:9321ms step_avg:33.77ms
step:277/2035 train_time:9354ms step_avg:33.77ms
step:278/2035 train_time:9387ms step_avg:33.77ms
step:279/2035 train_time:9420ms step_avg:33.76ms
step:280/2035 train_time:9453ms step_avg:33.76ms
step:281/2035 train_time:9486ms step_avg:33.76ms
step:282/2035 train_time:9519ms step_avg:33.75ms
step:283/2035 train_time:9552ms step_avg:33.75ms
step:284/2035 train_time:9585ms step_avg:33.75ms
step:285/2035 train_time:9619ms step_avg:33.75ms
step:286/2035 train_time:9652ms step_avg:33.75ms
step:287/2035 train_time:9685ms step_avg:33.75ms
step:288/2035 train_time:9718ms step_avg:33.74ms
step:289/2035 train_time:9752ms step_avg:33.74ms
step:290/2035 train_time:9785ms step_avg:33.74ms
step:291/2035 train_time:9817ms step_avg:33.74ms
step:292/2035 train_time:9851ms step_avg:33.73ms
step:293/2035 train_time:9883ms step_avg:33.73ms
step:294/2035 train_time:9916ms step_avg:33.73ms
step:295/2035 train_time:9949ms step_avg:33.72ms
step:296/2035 train_time:9982ms step_avg:33.72ms
step:297/2035 train_time:10014ms step_avg:33.72ms
step:298/2035 train_time:10047ms step_avg:33.72ms
step:299/2035 train_time:10080ms step_avg:33.71ms
step:300/2035 train_time:10113ms step_avg:33.71ms
step:301/2035 train_time:10146ms step_avg:33.71ms
step:302/2035 train_time:10179ms step_avg:33.70ms
step:303/2035 train_time:10212ms step_avg:33.70ms
step:304/2035 train_time:10245ms step_avg:33.70ms
step:305/2035 train_time:10277ms step_avg:33.70ms
step:306/2035 train_time:10310ms step_avg:33.69ms
step:307/2035 train_time:10343ms step_avg:33.69ms
step:308/2035 train_time:10376ms step_avg:33.69ms
step:309/2035 train_time:10409ms step_avg:33.68ms
step:310/2035 train_time:10442ms step_avg:33.68ms
step:311/2035 train_time:10474ms step_avg:33.68ms
step:312/2035 train_time:10507ms step_avg:33.68ms
step:313/2035 train_time:10540ms step_avg:33.67ms
step:314/2035 train_time:10573ms step_avg:33.67ms
step:315/2035 train_time:10606ms step_avg:33.67ms
step:316/2035 train_time:10639ms step_avg:33.67ms
step:317/2035 train_time:10673ms step_avg:33.67ms
step:318/2035 train_time:10705ms step_avg:33.66ms
step:319/2035 train_time:10739ms step_avg:33.66ms
step:320/2035 train_time:10772ms step_avg:33.66ms
step:321/2035 train_time:10805ms step_avg:33.66ms
step:322/2035 train_time:10838ms step_avg:33.66ms
step:323/2035 train_time:10872ms step_avg:33.66ms
step:324/2035 train_time:10904ms step_avg:33.66ms
step:325/2035 train_time:10938ms step_avg:33.65ms
step:326/2035 train_time:10971ms step_avg:33.65ms
step:327/2035 train_time:11004ms step_avg:33.65ms
step:328/2035 train_time:11037ms step_avg:33.65ms
step:329/2035 train_time:11070ms step_avg:33.65ms
step:330/2035 train_time:11103ms step_avg:33.65ms
step:331/2035 train_time:11136ms step_avg:33.64ms
step:332/2035 train_time:11169ms step_avg:33.64ms
step:333/2035 train_time:11202ms step_avg:33.64ms
step:334/2035 train_time:11235ms step_avg:33.64ms
step:335/2035 train_time:11267ms step_avg:33.63ms
step:336/2035 train_time:11300ms step_avg:33.63ms
step:337/2035 train_time:11333ms step_avg:33.63ms
step:338/2035 train_time:11366ms step_avg:33.63ms
step:339/2035 train_time:11399ms step_avg:33.62ms
step:340/2035 train_time:11432ms step_avg:33.62ms
step:341/2035 train_time:11465ms step_avg:33.62ms
step:342/2035 train_time:11498ms step_avg:33.62ms
step:343/2035 train_time:11530ms step_avg:33.62ms
step:344/2035 train_time:11564ms step_avg:33.61ms
step:345/2035 train_time:11596ms step_avg:33.61ms
step:346/2035 train_time:11629ms step_avg:33.61ms
step:347/2035 train_time:11662ms step_avg:33.61ms
step:348/2035 train_time:11695ms step_avg:33.61ms
step:349/2035 train_time:11728ms step_avg:33.61ms
step:350/2035 train_time:11761ms step_avg:33.60ms
step:351/2035 train_time:11795ms step_avg:33.60ms
step:352/2035 train_time:11827ms step_avg:33.60ms
step:353/2035 train_time:11860ms step_avg:33.60ms
step:354/2035 train_time:11893ms step_avg:33.60ms
step:355/2035 train_time:11926ms step_avg:33.60ms
step:356/2035 train_time:11959ms step_avg:33.59ms
step:357/2035 train_time:11992ms step_avg:33.59ms
step:358/2035 train_time:12025ms step_avg:33.59ms
step:359/2035 train_time:12058ms step_avg:33.59ms
step:360/2035 train_time:12092ms step_avg:33.59ms
step:361/2035 train_time:12124ms step_avg:33.58ms
step:362/2035 train_time:12158ms step_avg:33.58ms
step:363/2035 train_time:12190ms step_avg:33.58ms
step:364/2035 train_time:12223ms step_avg:33.58ms
step:365/2035 train_time:12256ms step_avg:33.58ms
step:366/2035 train_time:12289ms step_avg:33.58ms
step:367/2035 train_time:12322ms step_avg:33.57ms
step:368/2035 train_time:12355ms step_avg:33.57ms
step:369/2035 train_time:12388ms step_avg:33.57ms
step:370/2035 train_time:12421ms step_avg:33.57ms
step:371/2035 train_time:12453ms step_avg:33.57ms
step:372/2035 train_time:12486ms step_avg:33.57ms
step:373/2035 train_time:12520ms step_avg:33.56ms
step:374/2035 train_time:12553ms step_avg:33.56ms
step:375/2035 train_time:12585ms step_avg:33.56ms
step:376/2035 train_time:12618ms step_avg:33.56ms
step:377/2035 train_time:12651ms step_avg:33.56ms
step:378/2035 train_time:12684ms step_avg:33.56ms
step:379/2035 train_time:12717ms step_avg:33.55ms
step:380/2035 train_time:12750ms step_avg:33.55ms
step:381/2035 train_time:12783ms step_avg:33.55ms
step:382/2035 train_time:12816ms step_avg:33.55ms
step:383/2035 train_time:12850ms step_avg:33.55ms
step:384/2035 train_time:12883ms step_avg:33.55ms
step:385/2035 train_time:12915ms step_avg:33.55ms
step:386/2035 train_time:12948ms step_avg:33.55ms
step:387/2035 train_time:12981ms step_avg:33.54ms
step:388/2035 train_time:13015ms step_avg:33.54ms
step:389/2035 train_time:13047ms step_avg:33.54ms
step:390/2035 train_time:13080ms step_avg:33.54ms
step:391/2035 train_time:13113ms step_avg:33.54ms
step:392/2035 train_time:13146ms step_avg:33.54ms
step:393/2035 train_time:13179ms step_avg:33.53ms
step:394/2035 train_time:13212ms step_avg:33.53ms
step:395/2035 train_time:13245ms step_avg:33.53ms
step:396/2035 train_time:13278ms step_avg:33.53ms
step:397/2035 train_time:13311ms step_avg:33.53ms
step:398/2035 train_time:13344ms step_avg:33.53ms
step:399/2035 train_time:13377ms step_avg:33.53ms
step:400/2035 train_time:13410ms step_avg:33.52ms
step:401/2035 train_time:13443ms step_avg:33.52ms
step:402/2035 train_time:13476ms step_avg:33.52ms
step:403/2035 train_time:13509ms step_avg:33.52ms
step:404/2035 train_time:13542ms step_avg:33.52ms
step:405/2035 train_time:13575ms step_avg:33.52ms
step:406/2035 train_time:13607ms step_avg:33.52ms
step:407/2035 train_time:13640ms step_avg:33.51ms
step:408/2035 train_time:13673ms step_avg:33.51ms
step:409/2035 train_time:13706ms step_avg:33.51ms
step:410/2035 train_time:13739ms step_avg:33.51ms
step:411/2035 train_time:13772ms step_avg:33.51ms
step:412/2035 train_time:13805ms step_avg:33.51ms
step:413/2035 train_time:13839ms step_avg:33.51ms
step:414/2035 train_time:13872ms step_avg:33.51ms
step:415/2035 train_time:13905ms step_avg:33.51ms
step:416/2035 train_time:13938ms step_avg:33.50ms
step:417/2035 train_time:13971ms step_avg:33.50ms
step:418/2035 train_time:14004ms step_avg:33.50ms
step:419/2035 train_time:14036ms step_avg:33.50ms
step:420/2035 train_time:14070ms step_avg:33.50ms
step:421/2035 train_time:14102ms step_avg:33.50ms
step:422/2035 train_time:14135ms step_avg:33.50ms
step:423/2035 train_time:14168ms step_avg:33.49ms
step:424/2035 train_time:14201ms step_avg:33.49ms
step:425/2035 train_time:14234ms step_avg:33.49ms
step:426/2035 train_time:14267ms step_avg:33.49ms
step:427/2035 train_time:14300ms step_avg:33.49ms
step:428/2035 train_time:14333ms step_avg:33.49ms
step:429/2035 train_time:14366ms step_avg:33.49ms
step:430/2035 train_time:14399ms step_avg:33.49ms
step:431/2035 train_time:14432ms step_avg:33.48ms
step:432/2035 train_time:14465ms step_avg:33.48ms
step:433/2035 train_time:14497ms step_avg:33.48ms
step:434/2035 train_time:14530ms step_avg:33.48ms
step:435/2035 train_time:14563ms step_avg:33.48ms
step:436/2035 train_time:14596ms step_avg:33.48ms
step:437/2035 train_time:14629ms step_avg:33.48ms
step:438/2035 train_time:14662ms step_avg:33.47ms
step:439/2035 train_time:14695ms step_avg:33.47ms
step:440/2035 train_time:14728ms step_avg:33.47ms
step:441/2035 train_time:14761ms step_avg:33.47ms
step:442/2035 train_time:14794ms step_avg:33.47ms
step:443/2035 train_time:14827ms step_avg:33.47ms
step:444/2035 train_time:14860ms step_avg:33.47ms
step:445/2035 train_time:14893ms step_avg:33.47ms
step:446/2035 train_time:14926ms step_avg:33.47ms
step:447/2035 train_time:14959ms step_avg:33.47ms
step:448/2035 train_time:14992ms step_avg:33.46ms
step:449/2035 train_time:15026ms step_avg:33.46ms
step:450/2035 train_time:15059ms step_avg:33.46ms
step:451/2035 train_time:15092ms step_avg:33.46ms
step:452/2035 train_time:15125ms step_avg:33.46ms
step:453/2035 train_time:15158ms step_avg:33.46ms
step:454/2035 train_time:15191ms step_avg:33.46ms
step:455/2035 train_time:15224ms step_avg:33.46ms
step:456/2035 train_time:15258ms step_avg:33.46ms
step:457/2035 train_time:15290ms step_avg:33.46ms
step:458/2035 train_time:15323ms step_avg:33.46ms
step:459/2035 train_time:15356ms step_avg:33.46ms
step:460/2035 train_time:15389ms step_avg:33.45ms
step:461/2035 train_time:15422ms step_avg:33.45ms
step:462/2035 train_time:15455ms step_avg:33.45ms
step:463/2035 train_time:15488ms step_avg:33.45ms
step:464/2035 train_time:15521ms step_avg:33.45ms
step:465/2035 train_time:15554ms step_avg:33.45ms
step:466/2035 train_time:15587ms step_avg:33.45ms
step:467/2035 train_time:15620ms step_avg:33.45ms
step:468/2035 train_time:15653ms step_avg:33.45ms
step:469/2035 train_time:15685ms step_avg:33.44ms
step:470/2035 train_time:15718ms step_avg:33.44ms
step:471/2035 train_time:15751ms step_avg:33.44ms
step:472/2035 train_time:15784ms step_avg:33.44ms
step:473/2035 train_time:15817ms step_avg:33.44ms
step:474/2035 train_time:15850ms step_avg:33.44ms
step:475/2035 train_time:15883ms step_avg:33.44ms
step:476/2035 train_time:15916ms step_avg:33.44ms
step:477/2035 train_time:15949ms step_avg:33.44ms
step:478/2035 train_time:15982ms step_avg:33.44ms
step:479/2035 train_time:16015ms step_avg:33.43ms
step:480/2035 train_time:16048ms step_avg:33.43ms
step:481/2035 train_time:16081ms step_avg:33.43ms
step:482/2035 train_time:16114ms step_avg:33.43ms
step:483/2035 train_time:16147ms step_avg:33.43ms
step:484/2035 train_time:16180ms step_avg:33.43ms
step:485/2035 train_time:16213ms step_avg:33.43ms
step:486/2035 train_time:16245ms step_avg:33.43ms
step:487/2035 train_time:16279ms step_avg:33.43ms
step:488/2035 train_time:16312ms step_avg:33.43ms
step:489/2035 train_time:16345ms step_avg:33.43ms
step:490/2035 train_time:16378ms step_avg:33.42ms
step:491/2035 train_time:16411ms step_avg:33.42ms
step:492/2035 train_time:16444ms step_avg:33.42ms
step:493/2035 train_time:16477ms step_avg:33.42ms
step:494/2035 train_time:16510ms step_avg:33.42ms
step:495/2035 train_time:16543ms step_avg:33.42ms
step:496/2035 train_time:16576ms step_avg:33.42ms
step:497/2035 train_time:16608ms step_avg:33.42ms
step:498/2035 train_time:16641ms step_avg:33.42ms
step:499/2035 train_time:16674ms step_avg:33.42ms
step:500/2035 train_time:16707ms step_avg:33.41ms
step:500/2035 val_loss:3.9937 train_time:16743ms step_avg:33.49ms
step:501/2035 train_time:16763ms step_avg:33.46ms
step:502/2035 train_time:16782ms step_avg:33.43ms
step:503/2035 train_time:16811ms step_avg:33.42ms
step:504/2035 train_time:16844ms step_avg:33.42ms
step:505/2035 train_time:16879ms step_avg:33.42ms
step:506/2035 train_time:16912ms step_avg:33.42ms
step:507/2035 train_time:16947ms step_avg:33.43ms
step:508/2035 train_time:16980ms step_avg:33.42ms
step:509/2035 train_time:17013ms step_avg:33.42ms
step:510/2035 train_time:17046ms step_avg:33.42ms
step:511/2035 train_time:17079ms step_avg:33.42ms
step:512/2035 train_time:17112ms step_avg:33.42ms
step:513/2035 train_time:17144ms step_avg:33.42ms
step:514/2035 train_time:17177ms step_avg:33.42ms
step:515/2035 train_time:17210ms step_avg:33.42ms
step:516/2035 train_time:17243ms step_avg:33.42ms
step:517/2035 train_time:17275ms step_avg:33.41ms
step:518/2035 train_time:17308ms step_avg:33.41ms
step:519/2035 train_time:17341ms step_avg:33.41ms
step:520/2035 train_time:17374ms step_avg:33.41ms
step:521/2035 train_time:17407ms step_avg:33.41ms
step:522/2035 train_time:17439ms step_avg:33.41ms
step:523/2035 train_time:17472ms step_avg:33.41ms
step:524/2035 train_time:17505ms step_avg:33.41ms
step:525/2035 train_time:17538ms step_avg:33.41ms
step:526/2035 train_time:17571ms step_avg:33.40ms
step:527/2035 train_time:17604ms step_avg:33.40ms
step:528/2035 train_time:17637ms step_avg:33.40ms
step:529/2035 train_time:17670ms step_avg:33.40ms
step:530/2035 train_time:17702ms step_avg:33.40ms
step:531/2035 train_time:17736ms step_avg:33.40ms
step:532/2035 train_time:17769ms step_avg:33.40ms
step:533/2035 train_time:17802ms step_avg:33.40ms
step:534/2035 train_time:17835ms step_avg:33.40ms
step:535/2035 train_time:17869ms step_avg:33.40ms
step:536/2035 train_time:17902ms step_avg:33.40ms
step:537/2035 train_time:17935ms step_avg:33.40ms
step:538/2035 train_time:17968ms step_avg:33.40ms
step:539/2035 train_time:18002ms step_avg:33.40ms
step:540/2035 train_time:18035ms step_avg:33.40ms
step:541/2035 train_time:18069ms step_avg:33.40ms
step:542/2035 train_time:18102ms step_avg:33.40ms
step:543/2035 train_time:18135ms step_avg:33.40ms
step:544/2035 train_time:18168ms step_avg:33.40ms
step:545/2035 train_time:18201ms step_avg:33.40ms
step:546/2035 train_time:18234ms step_avg:33.40ms
step:547/2035 train_time:18267ms step_avg:33.39ms
step:548/2035 train_time:18300ms step_avg:33.39ms
step:549/2035 train_time:18333ms step_avg:33.39ms
step:550/2035 train_time:18366ms step_avg:33.39ms
step:551/2035 train_time:18398ms step_avg:33.39ms
step:552/2035 train_time:18431ms step_avg:33.39ms
step:553/2035 train_time:18464ms step_avg:33.39ms
step:554/2035 train_time:18497ms step_avg:33.39ms
step:555/2035 train_time:18529ms step_avg:33.39ms
step:556/2035 train_time:18563ms step_avg:33.39ms
step:557/2035 train_time:18595ms step_avg:33.38ms
step:558/2035 train_time:18628ms step_avg:33.38ms
step:559/2035 train_time:18661ms step_avg:33.38ms
step:560/2035 train_time:18694ms step_avg:33.38ms
step:561/2035 train_time:18726ms step_avg:33.38ms
step:562/2035 train_time:18759ms step_avg:33.38ms
step:563/2035 train_time:18793ms step_avg:33.38ms
step:564/2035 train_time:18825ms step_avg:33.38ms
step:565/2035 train_time:18858ms step_avg:33.38ms
step:566/2035 train_time:18891ms step_avg:33.38ms
step:567/2035 train_time:18925ms step_avg:33.38ms
step:568/2035 train_time:18957ms step_avg:33.38ms
step:569/2035 train_time:18991ms step_avg:33.38ms
step:570/2035 train_time:19023ms step_avg:33.37ms
step:571/2035 train_time:19057ms step_avg:33.37ms
step:572/2035 train_time:19090ms step_avg:33.37ms
step:573/2035 train_time:19123ms step_avg:33.37ms
step:574/2035 train_time:19156ms step_avg:33.37ms
step:575/2035 train_time:19190ms step_avg:33.37ms
step:576/2035 train_time:19223ms step_avg:33.37ms
step:577/2035 train_time:19256ms step_avg:33.37ms
step:578/2035 train_time:19289ms step_avg:33.37ms
step:579/2035 train_time:19322ms step_avg:33.37ms
step:580/2035 train_time:19355ms step_avg:33.37ms
step:581/2035 train_time:19388ms step_avg:33.37ms
step:582/2035 train_time:19420ms step_avg:33.37ms
step:583/2035 train_time:19453ms step_avg:33.37ms
step:584/2035 train_time:19486ms step_avg:33.37ms
step:585/2035 train_time:19519ms step_avg:33.37ms
step:586/2035 train_time:19552ms step_avg:33.37ms
step:587/2035 train_time:19585ms step_avg:33.36ms
step:588/2035 train_time:19618ms step_avg:33.36ms
step:589/2035 train_time:19651ms step_avg:33.36ms
step:590/2035 train_time:19684ms step_avg:33.36ms
step:591/2035 train_time:19716ms step_avg:33.36ms
step:592/2035 train_time:19749ms step_avg:33.36ms
step:593/2035 train_time:19782ms step_avg:33.36ms
step:594/2035 train_time:19815ms step_avg:33.36ms
step:595/2035 train_time:19848ms step_avg:33.36ms
step:596/2035 train_time:19881ms step_avg:33.36ms
step:597/2035 train_time:19914ms step_avg:33.36ms
step:598/2035 train_time:19947ms step_avg:33.36ms
step:599/2035 train_time:19980ms step_avg:33.36ms
step:600/2035 train_time:20013ms step_avg:33.35ms
step:601/2035 train_time:20045ms step_avg:33.35ms
step:602/2035 train_time:20078ms step_avg:33.35ms
step:603/2035 train_time:20112ms step_avg:33.35ms
step:604/2035 train_time:20145ms step_avg:33.35ms
step:605/2035 train_time:20178ms step_avg:33.35ms
step:606/2035 train_time:20210ms step_avg:33.35ms
step:607/2035 train_time:20244ms step_avg:33.35ms
step:608/2035 train_time:20276ms step_avg:33.35ms
step:609/2035 train_time:20310ms step_avg:33.35ms
step:610/2035 train_time:20343ms step_avg:33.35ms
step:611/2035 train_time:20376ms step_avg:33.35ms
step:612/2035 train_time:20409ms step_avg:33.35ms
step:613/2035 train_time:20442ms step_avg:33.35ms
step:614/2035 train_time:20475ms step_avg:33.35ms
step:615/2035 train_time:20508ms step_avg:33.35ms
step:616/2035 train_time:20541ms step_avg:33.34ms
step:617/2035 train_time:20574ms step_avg:33.34ms
step:618/2035 train_time:20606ms step_avg:33.34ms
step:619/2035 train_time:20639ms step_avg:33.34ms
step:620/2035 train_time:20672ms step_avg:33.34ms
step:621/2035 train_time:20705ms step_avg:33.34ms
step:622/2035 train_time:20738ms step_avg:33.34ms
step:623/2035 train_time:20771ms step_avg:33.34ms
step:624/2035 train_time:20804ms step_avg:33.34ms
step:625/2035 train_time:20836ms step_avg:33.34ms
step:626/2035 train_time:20869ms step_avg:33.34ms
step:627/2035 train_time:20902ms step_avg:33.34ms
step:628/2035 train_time:20935ms step_avg:33.34ms
step:629/2035 train_time:20969ms step_avg:33.34ms
step:630/2035 train_time:21002ms step_avg:33.34ms
step:631/2035 train_time:21035ms step_avg:33.34ms
step:632/2035 train_time:21068ms step_avg:33.34ms
step:633/2035 train_time:21101ms step_avg:33.34ms
step:634/2035 train_time:21134ms step_avg:33.33ms
step:635/2035 train_time:21167ms step_avg:33.33ms
step:636/2035 train_time:21200ms step_avg:33.33ms
step:637/2035 train_time:21233ms step_avg:33.33ms
step:638/2035 train_time:21266ms step_avg:33.33ms
step:639/2035 train_time:21299ms step_avg:33.33ms
step:640/2035 train_time:21332ms step_avg:33.33ms
step:641/2035 train_time:21365ms step_avg:33.33ms
step:642/2035 train_time:21398ms step_avg:33.33ms
step:643/2035 train_time:21431ms step_avg:33.33ms
step:644/2035 train_time:21464ms step_avg:33.33ms
step:645/2035 train_time:21497ms step_avg:33.33ms
step:646/2035 train_time:21530ms step_avg:33.33ms
step:647/2035 train_time:21563ms step_avg:33.33ms
step:648/2035 train_time:21596ms step_avg:33.33ms
step:649/2035 train_time:21629ms step_avg:33.33ms
step:650/2035 train_time:21662ms step_avg:33.33ms
step:651/2035 train_time:21695ms step_avg:33.33ms
step:652/2035 train_time:21728ms step_avg:33.33ms
step:653/2035 train_time:21761ms step_avg:33.32ms
step:654/2035 train_time:21794ms step_avg:33.32ms
step:655/2035 train_time:21827ms step_avg:33.32ms
step:656/2035 train_time:21860ms step_avg:33.32ms
step:657/2035 train_time:21893ms step_avg:33.32ms
step:658/2035 train_time:21926ms step_avg:33.32ms
step:659/2035 train_time:21959ms step_avg:33.32ms
step:660/2035 train_time:21992ms step_avg:33.32ms
step:661/2035 train_time:22025ms step_avg:33.32ms
step:662/2035 train_time:22058ms step_avg:33.32ms
step:663/2035 train_time:22091ms step_avg:33.32ms
step:664/2035 train_time:22124ms step_avg:33.32ms
step:665/2035 train_time:22157ms step_avg:33.32ms
step:666/2035 train_time:22191ms step_avg:33.32ms
step:667/2035 train_time:22249ms step_avg:33.36ms
step:668/2035 train_time:22309ms step_avg:33.40ms
step:669/2035 train_time:22369ms step_avg:33.44ms
step:670/2035 train_time:22428ms step_avg:33.47ms
step:671/2035 train_time:22489ms step_avg:33.52ms
step:672/2035 train_time:22548ms step_avg:33.55ms
step:673/2035 train_time:22608ms step_avg:33.59ms
step:674/2035 train_time:22668ms step_avg:33.63ms
step:675/2035 train_time:22728ms step_avg:33.67ms
step:676/2035 train_time:22787ms step_avg:33.71ms
step:677/2035 train_time:22848ms step_avg:33.75ms
step:678/2035 train_time:22908ms step_avg:33.79ms
step:679/2035 train_time:22968ms step_avg:33.83ms
step:680/2035 train_time:23028ms step_avg:33.86ms
step:681/2035 train_time:23088ms step_avg:33.90ms
step:682/2035 train_time:23147ms step_avg:33.94ms
step:683/2035 train_time:23207ms step_avg:33.98ms
step:684/2035 train_time:23266ms step_avg:34.01ms
step:685/2035 train_time:23326ms step_avg:34.05ms
step:686/2035 train_time:23385ms step_avg:34.09ms
step:687/2035 train_time:23446ms step_avg:34.13ms
step:688/2035 train_time:23506ms step_avg:34.17ms
step:689/2035 train_time:23566ms step_avg:34.20ms
step:690/2035 train_time:23625ms step_avg:34.24ms
step:691/2035 train_time:23685ms step_avg:34.28ms
step:692/2035 train_time:23745ms step_avg:34.31ms
step:693/2035 train_time:23804ms step_avg:34.35ms
step:694/2035 train_time:23864ms step_avg:34.39ms
step:695/2035 train_time:23924ms step_avg:34.42ms
step:696/2035 train_time:23983ms step_avg:34.46ms
step:697/2035 train_time:24044ms step_avg:34.50ms
step:698/2035 train_time:24103ms step_avg:34.53ms
step:699/2035 train_time:24164ms step_avg:34.57ms
step:700/2035 train_time:24223ms step_avg:34.60ms
step:701/2035 train_time:24283ms step_avg:34.64ms
step:702/2035 train_time:24342ms step_avg:34.68ms
step:703/2035 train_time:24402ms step_avg:34.71ms
step:704/2035 train_time:24462ms step_avg:34.75ms
step:705/2035 train_time:24522ms step_avg:34.78ms
step:706/2035 train_time:24582ms step_avg:34.82ms
step:707/2035 train_time:24643ms step_avg:34.86ms
step:708/2035 train_time:24702ms step_avg:34.89ms
step:709/2035 train_time:24762ms step_avg:34.93ms
step:710/2035 train_time:24822ms step_avg:34.96ms
step:711/2035 train_time:24882ms step_avg:35.00ms
step:712/2035 train_time:24941ms step_avg:35.03ms
step:713/2035 train_time:25002ms step_avg:35.07ms
step:714/2035 train_time:25061ms step_avg:35.10ms
step:715/2035 train_time:25122ms step_avg:35.14ms
step:716/2035 train_time:25181ms step_avg:35.17ms
step:717/2035 train_time:25242ms step_avg:35.21ms
step:718/2035 train_time:25301ms step_avg:35.24ms
step:719/2035 train_time:25362ms step_avg:35.27ms
step:720/2035 train_time:25421ms step_avg:35.31ms
step:721/2035 train_time:25482ms step_avg:35.34ms
step:722/2035 train_time:25542ms step_avg:35.38ms
step:723/2035 train_time:25603ms step_avg:35.41ms
step:724/2035 train_time:25662ms step_avg:35.44ms
step:725/2035 train_time:25722ms step_avg:35.48ms
step:726/2035 train_time:25781ms step_avg:35.51ms
step:727/2035 train_time:25842ms step_avg:35.55ms
step:728/2035 train_time:25902ms step_avg:35.58ms
step:729/2035 train_time:25963ms step_avg:35.61ms
step:730/2035 train_time:26022ms step_avg:35.65ms
step:731/2035 train_time:26082ms step_avg:35.68ms
step:732/2035 train_time:26141ms step_avg:35.71ms
step:733/2035 train_time:26202ms step_avg:35.75ms
step:734/2035 train_time:26261ms step_avg:35.78ms
step:735/2035 train_time:26322ms step_avg:35.81ms
step:736/2035 train_time:26381ms step_avg:35.84ms
step:737/2035 train_time:26442ms step_avg:35.88ms
step:738/2035 train_time:26501ms step_avg:35.91ms
step:739/2035 train_time:26562ms step_avg:35.94ms
step:740/2035 train_time:26621ms step_avg:35.97ms
step:741/2035 train_time:26682ms step_avg:36.01ms
step:742/2035 train_time:26741ms step_avg:36.04ms
step:743/2035 train_time:26802ms step_avg:36.07ms
step:744/2035 train_time:26861ms step_avg:36.10ms
step:745/2035 train_time:26921ms step_avg:36.14ms
step:746/2035 train_time:26981ms step_avg:36.17ms
step:747/2035 train_time:27042ms step_avg:36.20ms
step:748/2035 train_time:27101ms step_avg:36.23ms
step:749/2035 train_time:27162ms step_avg:36.26ms
step:750/2035 train_time:27221ms step_avg:36.29ms
step:750/2035 val_loss:3.8426 train_time:27284ms step_avg:36.38ms
step:751/2035 train_time:27304ms step_avg:36.36ms
step:752/2035 train_time:27343ms step_avg:36.36ms
step:753/2035 train_time:27405ms step_avg:36.39ms
step:754/2035 train_time:27466ms step_avg:36.43ms
step:755/2035 train_time:27528ms step_avg:36.46ms
step:756/2035 train_time:27587ms step_avg:36.49ms
step:757/2035 train_time:27648ms step_avg:36.52ms
step:758/2035 train_time:27706ms step_avg:36.55ms
step:759/2035 train_time:27766ms step_avg:36.58ms
step:760/2035 train_time:27825ms step_avg:36.61ms
step:761/2035 train_time:27884ms step_avg:36.64ms
step:762/2035 train_time:27943ms step_avg:36.67ms
step:763/2035 train_time:28003ms step_avg:36.70ms
step:764/2035 train_time:28062ms step_avg:36.73ms
step:765/2035 train_time:28122ms step_avg:36.76ms
step:766/2035 train_time:28181ms step_avg:36.79ms
step:767/2035 train_time:28243ms step_avg:36.82ms
step:768/2035 train_time:28302ms step_avg:36.85ms
step:769/2035 train_time:28364ms step_avg:36.88ms
step:770/2035 train_time:28425ms step_avg:36.92ms
step:771/2035 train_time:28486ms step_avg:36.95ms
step:772/2035 train_time:28547ms step_avg:36.98ms
step:773/2035 train_time:28608ms step_avg:37.01ms
step:774/2035 train_time:28667ms step_avg:37.04ms
step:775/2035 train_time:28727ms step_avg:37.07ms
step:776/2035 train_time:28786ms step_avg:37.10ms
step:777/2035 train_time:28846ms step_avg:37.12ms
step:778/2035 train_time:28904ms step_avg:37.15ms
step:779/2035 train_time:28964ms step_avg:37.18ms
step:780/2035 train_time:29024ms step_avg:37.21ms
step:781/2035 train_time:29083ms step_avg:37.24ms
step:782/2035 train_time:29142ms step_avg:37.27ms
step:783/2035 train_time:29202ms step_avg:37.29ms
step:784/2035 train_time:29261ms step_avg:37.32ms
step:785/2035 train_time:29322ms step_avg:37.35ms
step:786/2035 train_time:29383ms step_avg:37.38ms
step:787/2035 train_time:29443ms step_avg:37.41ms
step:788/2035 train_time:29502ms step_avg:37.44ms
step:789/2035 train_time:29563ms step_avg:37.47ms
step:790/2035 train_time:29623ms step_avg:37.50ms
step:791/2035 train_time:29683ms step_avg:37.53ms
step:792/2035 train_time:29742ms step_avg:37.55ms
step:793/2035 train_time:29803ms step_avg:37.58ms
step:794/2035 train_time:29862ms step_avg:37.61ms
step:795/2035 train_time:29922ms step_avg:37.64ms
step:796/2035 train_time:29981ms step_avg:37.66ms
step:797/2035 train_time:30040ms step_avg:37.69ms
step:798/2035 train_time:30099ms step_avg:37.72ms
step:799/2035 train_time:30159ms step_avg:37.75ms
step:800/2035 train_time:30218ms step_avg:37.77ms
step:801/2035 train_time:30278ms step_avg:37.80ms
step:802/2035 train_time:30337ms step_avg:37.83ms
step:803/2035 train_time:30398ms step_avg:37.86ms
step:804/2035 train_time:30458ms step_avg:37.88ms
step:805/2035 train_time:30519ms step_avg:37.91ms
step:806/2035 train_time:30579ms step_avg:37.94ms
step:807/2035 train_time:30639ms step_avg:37.97ms
step:808/2035 train_time:30699ms step_avg:37.99ms
step:809/2035 train_time:30759ms step_avg:38.02ms
step:810/2035 train_time:30818ms step_avg:38.05ms
step:811/2035 train_time:30878ms step_avg:38.07ms
step:812/2035 train_time:30937ms step_avg:38.10ms
step:813/2035 train_time:30997ms step_avg:38.13ms
step:814/2035 train_time:31056ms step_avg:38.15ms
step:815/2035 train_time:31116ms step_avg:38.18ms
step:816/2035 train_time:31175ms step_avg:38.20ms
step:817/2035 train_time:31236ms step_avg:38.23ms
step:818/2035 train_time:31296ms step_avg:38.26ms
step:819/2035 train_time:31357ms step_avg:38.29ms
step:820/2035 train_time:31417ms step_avg:38.31ms
step:821/2035 train_time:31479ms step_avg:38.34ms
step:822/2035 train_time:31538ms step_avg:38.37ms
step:823/2035 train_time:31599ms step_avg:38.39ms
step:824/2035 train_time:31659ms step_avg:38.42ms
step:825/2035 train_time:31719ms step_avg:38.45ms
step:826/2035 train_time:31778ms step_avg:38.47ms
step:827/2035 train_time:31838ms step_avg:38.50ms
step:828/2035 train_time:31897ms step_avg:38.52ms
step:829/2035 train_time:31958ms step_avg:38.55ms
step:830/2035 train_time:32016ms step_avg:38.57ms
step:831/2035 train_time:32076ms step_avg:38.60ms
step:832/2035 train_time:32135ms step_avg:38.62ms
step:833/2035 train_time:32195ms step_avg:38.65ms
step:834/2035 train_time:32255ms step_avg:38.68ms
step:835/2035 train_time:32316ms step_avg:38.70ms
step:836/2035 train_time:32376ms step_avg:38.73ms
step:837/2035 train_time:32437ms step_avg:38.75ms
step:838/2035 train_time:32498ms step_avg:38.78ms
step:839/2035 train_time:32558ms step_avg:38.81ms
step:840/2035 train_time:32618ms step_avg:38.83ms
step:841/2035 train_time:32678ms step_avg:38.86ms
step:842/2035 train_time:32738ms step_avg:38.88ms
step:843/2035 train_time:32799ms step_avg:38.91ms
step:844/2035 train_time:32858ms step_avg:38.93ms
step:845/2035 train_time:32918ms step_avg:38.96ms
step:846/2035 train_time:32976ms step_avg:38.98ms
step:847/2035 train_time:33037ms step_avg:39.00ms
step:848/2035 train_time:33096ms step_avg:39.03ms
step:849/2035 train_time:33156ms step_avg:39.05ms
step:850/2035 train_time:33215ms step_avg:39.08ms
step:851/2035 train_time:33276ms step_avg:39.10ms
step:852/2035 train_time:33335ms step_avg:39.13ms
step:853/2035 train_time:33397ms step_avg:39.15ms
step:854/2035 train_time:33456ms step_avg:39.18ms
step:855/2035 train_time:33516ms step_avg:39.20ms
step:856/2035 train_time:33576ms step_avg:39.22ms
step:857/2035 train_time:33636ms step_avg:39.25ms
step:858/2035 train_time:33696ms step_avg:39.27ms
step:859/2035 train_time:33757ms step_avg:39.30ms
step:860/2035 train_time:33816ms step_avg:39.32ms
step:861/2035 train_time:33876ms step_avg:39.35ms
step:862/2035 train_time:33935ms step_avg:39.37ms
step:863/2035 train_time:33996ms step_avg:39.39ms
step:864/2035 train_time:34054ms step_avg:39.41ms
step:865/2035 train_time:34114ms step_avg:39.44ms
step:866/2035 train_time:34173ms step_avg:39.46ms
step:867/2035 train_time:34233ms step_avg:39.48ms
step:868/2035 train_time:34293ms step_avg:39.51ms
step:869/2035 train_time:34353ms step_avg:39.53ms
step:870/2035 train_time:34413ms step_avg:39.56ms
step:871/2035 train_time:34474ms step_avg:39.58ms
step:872/2035 train_time:34533ms step_avg:39.60ms
step:873/2035 train_time:34594ms step_avg:39.63ms
step:874/2035 train_time:34654ms step_avg:39.65ms
step:875/2035 train_time:34715ms step_avg:39.67ms
step:876/2035 train_time:34776ms step_avg:39.70ms
step:877/2035 train_time:34836ms step_avg:39.72ms
step:878/2035 train_time:34896ms step_avg:39.74ms
step:879/2035 train_time:34957ms step_avg:39.77ms
step:880/2035 train_time:35016ms step_avg:39.79ms
step:881/2035 train_time:35076ms step_avg:39.81ms
step:882/2035 train_time:35135ms step_avg:39.84ms
step:883/2035 train_time:35195ms step_avg:39.86ms
step:884/2035 train_time:35254ms step_avg:39.88ms
step:885/2035 train_time:35315ms step_avg:39.90ms
step:886/2035 train_time:35374ms step_avg:39.93ms
step:887/2035 train_time:35434ms step_avg:39.95ms
step:888/2035 train_time:35493ms step_avg:39.97ms
step:889/2035 train_time:35554ms step_avg:39.99ms
step:890/2035 train_time:35613ms step_avg:40.02ms
step:891/2035 train_time:35674ms step_avg:40.04ms
step:892/2035 train_time:35733ms step_avg:40.06ms
step:893/2035 train_time:35794ms step_avg:40.08ms
step:894/2035 train_time:35853ms step_avg:40.10ms
step:895/2035 train_time:35914ms step_avg:40.13ms
step:896/2035 train_time:35973ms step_avg:40.15ms
step:897/2035 train_time:36033ms step_avg:40.17ms
step:898/2035 train_time:36092ms step_avg:40.19ms
step:899/2035 train_time:36152ms step_avg:40.21ms
step:900/2035 train_time:36212ms step_avg:40.24ms
step:901/2035 train_time:36272ms step_avg:40.26ms
step:902/2035 train_time:36331ms step_avg:40.28ms
step:903/2035 train_time:36392ms step_avg:40.30ms
step:904/2035 train_time:36451ms step_avg:40.32ms
step:905/2035 train_time:36513ms step_avg:40.35ms
step:906/2035 train_time:36572ms step_avg:40.37ms
step:907/2035 train_time:36633ms step_avg:40.39ms
step:908/2035 train_time:36692ms step_avg:40.41ms
step:909/2035 train_time:36753ms step_avg:40.43ms
step:910/2035 train_time:36813ms step_avg:40.45ms
step:911/2035 train_time:36874ms step_avg:40.48ms
step:912/2035 train_time:36933ms step_avg:40.50ms
step:913/2035 train_time:36993ms step_avg:40.52ms
step:914/2035 train_time:37052ms step_avg:40.54ms
step:915/2035 train_time:37113ms step_avg:40.56ms
step:916/2035 train_time:37172ms step_avg:40.58ms
step:917/2035 train_time:37233ms step_avg:40.60ms
step:918/2035 train_time:37292ms step_avg:40.62ms
step:919/2035 train_time:37352ms step_avg:40.64ms
step:920/2035 train_time:37411ms step_avg:40.66ms
step:921/2035 train_time:37472ms step_avg:40.69ms
step:922/2035 train_time:37531ms step_avg:40.71ms
step:923/2035 train_time:37592ms step_avg:40.73ms
step:924/2035 train_time:37652ms step_avg:40.75ms
step:925/2035 train_time:37713ms step_avg:40.77ms
step:926/2035 train_time:37772ms step_avg:40.79ms
step:927/2035 train_time:37834ms step_avg:40.81ms
step:928/2035 train_time:37893ms step_avg:40.83ms
step:929/2035 train_time:37953ms step_avg:40.85ms
step:930/2035 train_time:38012ms step_avg:40.87ms
step:931/2035 train_time:38072ms step_avg:40.89ms
step:932/2035 train_time:38132ms step_avg:40.91ms
step:933/2035 train_time:38192ms step_avg:40.93ms
step:934/2035 train_time:38251ms step_avg:40.95ms
step:935/2035 train_time:38311ms step_avg:40.97ms
step:936/2035 train_time:38370ms step_avg:40.99ms
step:937/2035 train_time:38430ms step_avg:41.01ms
step:938/2035 train_time:38489ms step_avg:41.03ms
step:939/2035 train_time:38550ms step_avg:41.05ms
step:940/2035 train_time:38610ms step_avg:41.07ms
step:941/2035 train_time:38671ms step_avg:41.10ms
step:942/2035 train_time:38730ms step_avg:41.12ms
step:943/2035 train_time:38791ms step_avg:41.14ms
step:944/2035 train_time:38851ms step_avg:41.16ms
step:945/2035 train_time:38912ms step_avg:41.18ms
step:946/2035 train_time:38971ms step_avg:41.20ms
step:947/2035 train_time:39032ms step_avg:41.22ms
step:948/2035 train_time:39091ms step_avg:41.24ms
step:949/2035 train_time:39152ms step_avg:41.26ms
step:950/2035 train_time:39211ms step_avg:41.27ms
step:951/2035 train_time:39271ms step_avg:41.29ms
step:952/2035 train_time:39330ms step_avg:41.31ms
step:953/2035 train_time:39391ms step_avg:41.33ms
step:954/2035 train_time:39450ms step_avg:41.35ms
step:955/2035 train_time:39511ms step_avg:41.37ms
step:956/2035 train_time:39570ms step_avg:41.39ms
step:957/2035 train_time:39631ms step_avg:41.41ms
step:958/2035 train_time:39691ms step_avg:41.43ms
step:959/2035 train_time:39752ms step_avg:41.45ms
step:960/2035 train_time:39812ms step_avg:41.47ms
step:961/2035 train_time:39872ms step_avg:41.49ms
step:962/2035 train_time:39932ms step_avg:41.51ms
step:963/2035 train_time:39992ms step_avg:41.53ms
step:964/2035 train_time:40052ms step_avg:41.55ms
step:965/2035 train_time:40113ms step_avg:41.57ms
step:966/2035 train_time:40173ms step_avg:41.59ms
step:967/2035 train_time:40233ms step_avg:41.61ms
step:968/2035 train_time:40293ms step_avg:41.62ms
step:969/2035 train_time:40353ms step_avg:41.64ms
step:970/2035 train_time:40412ms step_avg:41.66ms
step:971/2035 train_time:40473ms step_avg:41.68ms
step:972/2035 train_time:40532ms step_avg:41.70ms
step:973/2035 train_time:40593ms step_avg:41.72ms
step:974/2035 train_time:40652ms step_avg:41.74ms
step:975/2035 train_time:40713ms step_avg:41.76ms
step:976/2035 train_time:40772ms step_avg:41.77ms
step:977/2035 train_time:40833ms step_avg:41.79ms
step:978/2035 train_time:40893ms step_avg:41.81ms
step:979/2035 train_time:40954ms step_avg:41.83ms
step:980/2035 train_time:41013ms step_avg:41.85ms
step:981/2035 train_time:41074ms step_avg:41.87ms
step:982/2035 train_time:41134ms step_avg:41.89ms
step:983/2035 train_time:41194ms step_avg:41.91ms
step:984/2035 train_time:41253ms step_avg:41.92ms
step:985/2035 train_time:41314ms step_avg:41.94ms
step:986/2035 train_time:41374ms step_avg:41.96ms
step:987/2035 train_time:41434ms step_avg:41.98ms
step:988/2035 train_time:41493ms step_avg:42.00ms
step:989/2035 train_time:41554ms step_avg:42.02ms
step:990/2035 train_time:41613ms step_avg:42.03ms
step:991/2035 train_time:41673ms step_avg:42.05ms
step:992/2035 train_time:41733ms step_avg:42.07ms
step:993/2035 train_time:41793ms step_avg:42.09ms
step:994/2035 train_time:41852ms step_avg:42.11ms
step:995/2035 train_time:41914ms step_avg:42.12ms
step:996/2035 train_time:41973ms step_avg:42.14ms
step:997/2035 train_time:42034ms step_avg:42.16ms
step:998/2035 train_time:42094ms step_avg:42.18ms
step:999/2035 train_time:42155ms step_avg:42.20ms
step:1000/2035 train_time:42214ms step_avg:42.21ms
step:1000/2035 val_loss:3.6812 train_time:42276ms step_avg:42.28ms
step:1001/2035 train_time:42297ms step_avg:42.26ms
step:1002/2035 train_time:42335ms step_avg:42.25ms
step:1003/2035 train_time:42397ms step_avg:42.27ms
step:1004/2035 train_time:42458ms step_avg:42.29ms
step:1005/2035 train_time:42519ms step_avg:42.31ms
step:1006/2035 train_time:42578ms step_avg:42.32ms
step:1007/2035 train_time:42639ms step_avg:42.34ms
step:1008/2035 train_time:42697ms step_avg:42.36ms
step:1009/2035 train_time:42758ms step_avg:42.38ms
step:1010/2035 train_time:42816ms step_avg:42.39ms
step:1011/2035 train_time:42877ms step_avg:42.41ms
step:1012/2035 train_time:42936ms step_avg:42.43ms
step:1013/2035 train_time:42996ms step_avg:42.44ms
step:1014/2035 train_time:43056ms step_avg:42.46ms
step:1015/2035 train_time:43116ms step_avg:42.48ms
step:1016/2035 train_time:43175ms step_avg:42.50ms
step:1017/2035 train_time:43238ms step_avg:42.52ms
step:1018/2035 train_time:43299ms step_avg:42.53ms
step:1019/2035 train_time:43361ms step_avg:42.55ms
step:1020/2035 train_time:43420ms step_avg:42.57ms
step:1021/2035 train_time:43481ms step_avg:42.59ms
step:1022/2035 train_time:43542ms step_avg:42.60ms
step:1023/2035 train_time:43603ms step_avg:42.62ms
step:1024/2035 train_time:43663ms step_avg:42.64ms
step:1025/2035 train_time:43723ms step_avg:42.66ms
step:1026/2035 train_time:43782ms step_avg:42.67ms
step:1027/2035 train_time:43842ms step_avg:42.69ms
step:1028/2035 train_time:43901ms step_avg:42.71ms
step:1029/2035 train_time:43962ms step_avg:42.72ms
step:1030/2035 train_time:44021ms step_avg:42.74ms
step:1031/2035 train_time:44082ms step_avg:42.76ms
step:1032/2035 train_time:44141ms step_avg:42.77ms
step:1033/2035 train_time:44202ms step_avg:42.79ms
step:1034/2035 train_time:44262ms step_avg:42.81ms
step:1035/2035 train_time:44323ms step_avg:42.82ms
step:1036/2035 train_time:44384ms step_avg:42.84ms
step:1037/2035 train_time:44446ms step_avg:42.86ms
step:1038/2035 train_time:44505ms step_avg:42.88ms
step:1039/2035 train_time:44565ms step_avg:42.89ms
step:1040/2035 train_time:44625ms step_avg:42.91ms
step:1041/2035 train_time:44685ms step_avg:42.93ms
step:1042/2035 train_time:44744ms step_avg:42.94ms
step:1043/2035 train_time:44804ms step_avg:42.96ms
step:1044/2035 train_time:44863ms step_avg:42.97ms
step:1045/2035 train_time:44923ms step_avg:42.99ms
step:1046/2035 train_time:44982ms step_avg:43.00ms
step:1047/2035 train_time:45044ms step_avg:43.02ms
step:1048/2035 train_time:45103ms step_avg:43.04ms
step:1049/2035 train_time:45164ms step_avg:43.05ms
step:1050/2035 train_time:45224ms step_avg:43.07ms
step:1051/2035 train_time:45285ms step_avg:43.09ms
step:1052/2035 train_time:45345ms step_avg:43.10ms
step:1053/2035 train_time:45406ms step_avg:43.12ms
step:1054/2035 train_time:45465ms step_avg:43.14ms
step:1055/2035 train_time:45526ms step_avg:43.15ms
step:1056/2035 train_time:45585ms step_avg:43.17ms
step:1057/2035 train_time:45646ms step_avg:43.18ms
step:1058/2035 train_time:45705ms step_avg:43.20ms
step:1059/2035 train_time:45766ms step_avg:43.22ms
step:1060/2035 train_time:45825ms step_avg:43.23ms
step:1061/2035 train_time:45885ms step_avg:43.25ms
step:1062/2035 train_time:45944ms step_avg:43.26ms
step:1063/2035 train_time:46004ms step_avg:43.28ms
step:1064/2035 train_time:46063ms step_avg:43.29ms
step:1065/2035 train_time:46124ms step_avg:43.31ms
step:1066/2035 train_time:46183ms step_avg:43.32ms
step:1067/2035 train_time:46244ms step_avg:43.34ms
step:1068/2035 train_time:46303ms step_avg:43.35ms
step:1069/2035 train_time:46364ms step_avg:43.37ms
step:1070/2035 train_time:46423ms step_avg:43.39ms
step:1071/2035 train_time:46485ms step_avg:43.40ms
step:1072/2035 train_time:46545ms step_avg:43.42ms
step:1073/2035 train_time:46605ms step_avg:43.43ms
step:1074/2035 train_time:46664ms step_avg:43.45ms
step:1075/2035 train_time:46725ms step_avg:43.46ms
step:1076/2035 train_time:46784ms step_avg:43.48ms
step:1077/2035 train_time:46844ms step_avg:43.50ms
step:1078/2035 train_time:46903ms step_avg:43.51ms
step:1079/2035 train_time:46964ms step_avg:43.53ms
step:1080/2035 train_time:47023ms step_avg:43.54ms
step:1081/2035 train_time:47083ms step_avg:43.56ms
step:1082/2035 train_time:47143ms step_avg:43.57ms
step:1083/2035 train_time:47203ms step_avg:43.59ms
step:1084/2035 train_time:47263ms step_avg:43.60ms
step:1085/2035 train_time:47324ms step_avg:43.62ms
step:1086/2035 train_time:47383ms step_avg:43.63ms
step:1087/2035 train_time:47444ms step_avg:43.65ms
step:1088/2035 train_time:47504ms step_avg:43.66ms
step:1089/2035 train_time:47564ms step_avg:43.68ms
step:1090/2035 train_time:47623ms step_avg:43.69ms
step:1091/2035 train_time:47684ms step_avg:43.71ms
step:1092/2035 train_time:47743ms step_avg:43.72ms
step:1093/2035 train_time:47804ms step_avg:43.74ms
step:1094/2035 train_time:47863ms step_avg:43.75ms
step:1095/2035 train_time:47923ms step_avg:43.77ms
step:1096/2035 train_time:47983ms step_avg:43.78ms
step:1097/2035 train_time:48044ms step_avg:43.80ms
step:1098/2035 train_time:48102ms step_avg:43.81ms
step:1099/2035 train_time:48164ms step_avg:43.82ms
step:1100/2035 train_time:48223ms step_avg:43.84ms
step:1101/2035 train_time:48284ms step_avg:43.86ms
step:1102/2035 train_time:48344ms step_avg:43.87ms
step:1103/2035 train_time:48405ms step_avg:43.88ms
step:1104/2035 train_time:48465ms step_avg:43.90ms
step:1105/2035 train_time:48525ms step_avg:43.91ms
step:1106/2035 train_time:48584ms step_avg:43.93ms
step:1107/2035 train_time:48645ms step_avg:43.94ms
step:1108/2035 train_time:48704ms step_avg:43.96ms
step:1109/2035 train_time:48764ms step_avg:43.97ms
step:1110/2035 train_time:48824ms step_avg:43.99ms
step:1111/2035 train_time:48884ms step_avg:44.00ms
step:1112/2035 train_time:48944ms step_avg:44.01ms
step:1113/2035 train_time:49005ms step_avg:44.03ms
step:1114/2035 train_time:49064ms step_avg:44.04ms
step:1115/2035 train_time:49125ms step_avg:44.06ms
step:1116/2035 train_time:49184ms step_avg:44.07ms
step:1117/2035 train_time:49245ms step_avg:44.09ms
step:1118/2035 train_time:49304ms step_avg:44.10ms
step:1119/2035 train_time:49365ms step_avg:44.12ms
step:1120/2035 train_time:49424ms step_avg:44.13ms
step:1121/2035 train_time:49484ms step_avg:44.14ms
step:1122/2035 train_time:49543ms step_avg:44.16ms
step:1123/2035 train_time:49603ms step_avg:44.17ms
step:1124/2035 train_time:49663ms step_avg:44.18ms
step:1125/2035 train_time:49724ms step_avg:44.20ms
step:1126/2035 train_time:49783ms step_avg:44.21ms
step:1127/2035 train_time:49844ms step_avg:44.23ms
step:1128/2035 train_time:49904ms step_avg:44.24ms
step:1129/2035 train_time:49965ms step_avg:44.26ms
step:1130/2035 train_time:50023ms step_avg:44.27ms
step:1131/2035 train_time:50084ms step_avg:44.28ms
step:1132/2035 train_time:50144ms step_avg:44.30ms
step:1133/2035 train_time:50205ms step_avg:44.31ms
step:1134/2035 train_time:50264ms step_avg:44.32ms
step:1135/2035 train_time:50325ms step_avg:44.34ms
step:1136/2035 train_time:50384ms step_avg:44.35ms
step:1137/2035 train_time:50445ms step_avg:44.37ms
step:1138/2035 train_time:50504ms step_avg:44.38ms
step:1139/2035 train_time:50564ms step_avg:44.39ms
step:1140/2035 train_time:50623ms step_avg:44.41ms
step:1141/2035 train_time:50683ms step_avg:44.42ms
step:1142/2035 train_time:50743ms step_avg:44.43ms
step:1143/2035 train_time:50803ms step_avg:44.45ms
step:1144/2035 train_time:50862ms step_avg:44.46ms
step:1145/2035 train_time:50923ms step_avg:44.47ms
step:1146/2035 train_time:50982ms step_avg:44.49ms
step:1147/2035 train_time:51043ms step_avg:44.50ms
step:1148/2035 train_time:51102ms step_avg:44.51ms
step:1149/2035 train_time:51163ms step_avg:44.53ms
step:1150/2035 train_time:51222ms step_avg:44.54ms
step:1151/2035 train_time:51283ms step_avg:44.56ms
step:1152/2035 train_time:51343ms step_avg:44.57ms
step:1153/2035 train_time:51404ms step_avg:44.58ms
step:1154/2035 train_time:51463ms step_avg:44.60ms
step:1155/2035 train_time:51524ms step_avg:44.61ms
step:1156/2035 train_time:51583ms step_avg:44.62ms
step:1157/2035 train_time:51644ms step_avg:44.64ms
step:1158/2035 train_time:51703ms step_avg:44.65ms
step:1159/2035 train_time:51764ms step_avg:44.66ms
step:1160/2035 train_time:51823ms step_avg:44.68ms
step:1161/2035 train_time:51884ms step_avg:44.69ms
step:1162/2035 train_time:51944ms step_avg:44.70ms
step:1163/2035 train_time:52004ms step_avg:44.72ms
step:1164/2035 train_time:52063ms step_avg:44.73ms
step:1165/2035 train_time:52124ms step_avg:44.74ms
step:1166/2035 train_time:52183ms step_avg:44.75ms
step:1167/2035 train_time:52244ms step_avg:44.77ms
step:1168/2035 train_time:52304ms step_avg:44.78ms
step:1169/2035 train_time:52365ms step_avg:44.79ms
step:1170/2035 train_time:52424ms step_avg:44.81ms
step:1171/2035 train_time:52485ms step_avg:44.82ms
step:1172/2035 train_time:52544ms step_avg:44.83ms
step:1173/2035 train_time:52605ms step_avg:44.85ms
step:1174/2035 train_time:52664ms step_avg:44.86ms
step:1175/2035 train_time:52724ms step_avg:44.87ms
step:1176/2035 train_time:52784ms step_avg:44.88ms
step:1177/2035 train_time:52844ms step_avg:44.90ms
step:1178/2035 train_time:52903ms step_avg:44.91ms
step:1179/2035 train_time:52964ms step_avg:44.92ms
step:1180/2035 train_time:53023ms step_avg:44.93ms
step:1181/2035 train_time:53083ms step_avg:44.95ms
step:1182/2035 train_time:53143ms step_avg:44.96ms
step:1183/2035 train_time:53203ms step_avg:44.97ms
step:1184/2035 train_time:53262ms step_avg:44.98ms
step:1185/2035 train_time:53323ms step_avg:45.00ms
step:1186/2035 train_time:53383ms step_avg:45.01ms
step:1187/2035 train_time:53444ms step_avg:45.02ms
step:1188/2035 train_time:53503ms step_avg:45.04ms
step:1189/2035 train_time:53564ms step_avg:45.05ms
step:1190/2035 train_time:53623ms step_avg:45.06ms
step:1191/2035 train_time:53683ms step_avg:45.07ms
step:1192/2035 train_time:53742ms step_avg:45.09ms
step:1193/2035 train_time:53803ms step_avg:45.10ms
step:1194/2035 train_time:53863ms step_avg:45.11ms
step:1195/2035 train_time:53923ms step_avg:45.12ms
step:1196/2035 train_time:53982ms step_avg:45.14ms
step:1197/2035 train_time:54043ms step_avg:45.15ms
step:1198/2035 train_time:54103ms step_avg:45.16ms
step:1199/2035 train_time:54164ms step_avg:45.17ms
step:1200/2035 train_time:54223ms step_avg:45.19ms
step:1201/2035 train_time:54283ms step_avg:45.20ms
step:1202/2035 train_time:54342ms step_avg:45.21ms
step:1203/2035 train_time:54403ms step_avg:45.22ms
step:1204/2035 train_time:54463ms step_avg:45.23ms
step:1205/2035 train_time:54524ms step_avg:45.25ms
step:1206/2035 train_time:54583ms step_avg:45.26ms
step:1207/2035 train_time:54644ms step_avg:45.27ms
step:1208/2035 train_time:54702ms step_avg:45.28ms
step:1209/2035 train_time:54763ms step_avg:45.30ms
step:1210/2035 train_time:54822ms step_avg:45.31ms
step:1211/2035 train_time:54882ms step_avg:45.32ms
step:1212/2035 train_time:54941ms step_avg:45.33ms
step:1213/2035 train_time:55002ms step_avg:45.34ms
step:1214/2035 train_time:55061ms step_avg:45.36ms
step:1215/2035 train_time:55122ms step_avg:45.37ms
step:1216/2035 train_time:55181ms step_avg:45.38ms
step:1217/2035 train_time:55243ms step_avg:45.39ms
step:1218/2035 train_time:55302ms step_avg:45.40ms
step:1219/2035 train_time:55363ms step_avg:45.42ms
step:1220/2035 train_time:55422ms step_avg:45.43ms
step:1221/2035 train_time:55483ms step_avg:45.44ms
step:1222/2035 train_time:55543ms step_avg:45.45ms
step:1223/2035 train_time:55603ms step_avg:45.46ms
step:1224/2035 train_time:55662ms step_avg:45.48ms
step:1225/2035 train_time:55722ms step_avg:45.49ms
step:1226/2035 train_time:55782ms step_avg:45.50ms
step:1227/2035 train_time:55843ms step_avg:45.51ms
step:1228/2035 train_time:55902ms step_avg:45.52ms
step:1229/2035 train_time:55963ms step_avg:45.54ms
step:1230/2035 train_time:56022ms step_avg:45.55ms
step:1231/2035 train_time:56084ms step_avg:45.56ms
step:1232/2035 train_time:56144ms step_avg:45.57ms
step:1233/2035 train_time:56205ms step_avg:45.58ms
step:1234/2035 train_time:56264ms step_avg:45.59ms
step:1235/2035 train_time:56324ms step_avg:45.61ms
step:1236/2035 train_time:56384ms step_avg:45.62ms
step:1237/2035 train_time:56444ms step_avg:45.63ms
step:1238/2035 train_time:56504ms step_avg:45.64ms
step:1239/2035 train_time:56565ms step_avg:45.65ms
step:1240/2035 train_time:56624ms step_avg:45.66ms
step:1241/2035 train_time:56685ms step_avg:45.68ms
step:1242/2035 train_time:56744ms step_avg:45.69ms
step:1243/2035 train_time:56804ms step_avg:45.70ms
step:1244/2035 train_time:56864ms step_avg:45.71ms
step:1245/2035 train_time:56924ms step_avg:45.72ms
step:1246/2035 train_time:56983ms step_avg:45.73ms
step:1247/2035 train_time:57044ms step_avg:45.75ms
step:1248/2035 train_time:57104ms step_avg:45.76ms
step:1249/2035 train_time:57165ms step_avg:45.77ms
step:1250/2035 train_time:57224ms step_avg:45.78ms
step:1250/2035 val_loss:3.5678 train_time:57286ms step_avg:45.83ms
step:1251/2035 train_time:57307ms step_avg:45.81ms
step:1252/2035 train_time:57346ms step_avg:45.80ms
step:1253/2035 train_time:57407ms step_avg:45.82ms
step:1254/2035 train_time:57471ms step_avg:45.83ms
step:1255/2035 train_time:57531ms step_avg:45.84ms
step:1256/2035 train_time:57590ms step_avg:45.85ms
step:1257/2035 train_time:57650ms step_avg:45.86ms
step:1258/2035 train_time:57709ms step_avg:45.87ms
step:1259/2035 train_time:57769ms step_avg:45.89ms
step:1260/2035 train_time:57828ms step_avg:45.90ms
step:1261/2035 train_time:57888ms step_avg:45.91ms
step:1262/2035 train_time:57946ms step_avg:45.92ms
step:1263/2035 train_time:58006ms step_avg:45.93ms
step:1264/2035 train_time:58065ms step_avg:45.94ms
step:1265/2035 train_time:58124ms step_avg:45.95ms
step:1266/2035 train_time:58183ms step_avg:45.96ms
step:1267/2035 train_time:58245ms step_avg:45.97ms
step:1268/2035 train_time:58306ms step_avg:45.98ms
step:1269/2035 train_time:58367ms step_avg:45.99ms
step:1270/2035 train_time:58428ms step_avg:46.01ms
step:1271/2035 train_time:58490ms step_avg:46.02ms
step:1272/2035 train_time:58550ms step_avg:46.03ms
step:1273/2035 train_time:58610ms step_avg:46.04ms
step:1274/2035 train_time:58670ms step_avg:46.05ms
step:1275/2035 train_time:58729ms step_avg:46.06ms
step:1276/2035 train_time:58789ms step_avg:46.07ms
step:1277/2035 train_time:58848ms step_avg:46.08ms
step:1278/2035 train_time:58907ms step_avg:46.09ms
step:1279/2035 train_time:58967ms step_avg:46.10ms
step:1280/2035 train_time:59026ms step_avg:46.11ms
step:1281/2035 train_time:59086ms step_avg:46.13ms
step:1282/2035 train_time:59146ms step_avg:46.14ms
step:1283/2035 train_time:59207ms step_avg:46.15ms
step:1284/2035 train_time:59266ms step_avg:46.16ms
step:1285/2035 train_time:59327ms step_avg:46.17ms
step:1286/2035 train_time:59387ms step_avg:46.18ms
step:1287/2035 train_time:59448ms step_avg:46.19ms
step:1288/2035 train_time:59509ms step_avg:46.20ms
step:1289/2035 train_time:59570ms step_avg:46.21ms
step:1290/2035 train_time:59630ms step_avg:46.22ms
step:1291/2035 train_time:59690ms step_avg:46.24ms
step:1292/2035 train_time:59749ms step_avg:46.25ms
step:1293/2035 train_time:59808ms step_avg:46.26ms
step:1294/2035 train_time:59867ms step_avg:46.27ms
step:1295/2035 train_time:59927ms step_avg:46.28ms
step:1296/2035 train_time:59986ms step_avg:46.29ms
step:1297/2035 train_time:60046ms step_avg:46.30ms
step:1298/2035 train_time:60105ms step_avg:46.31ms
step:1299/2035 train_time:60166ms step_avg:46.32ms
step:1300/2035 train_time:60225ms step_avg:46.33ms
step:1301/2035 train_time:60286ms step_avg:46.34ms
step:1302/2035 train_time:60346ms step_avg:46.35ms
step:1303/2035 train_time:60406ms step_avg:46.36ms
step:1304/2035 train_time:60466ms step_avg:46.37ms
step:1305/2035 train_time:60528ms step_avg:46.38ms
step:1306/2035 train_time:60588ms step_avg:46.39ms
step:1307/2035 train_time:60649ms step_avg:46.40ms
step:1308/2035 train_time:60708ms step_avg:46.41ms
step:1309/2035 train_time:60768ms step_avg:46.42ms
step:1310/2035 train_time:60827ms step_avg:46.43ms
step:1311/2035 train_time:60887ms step_avg:46.44ms
step:1312/2035 train_time:60946ms step_avg:46.45ms
step:1313/2035 train_time:61007ms step_avg:46.46ms
step:1314/2035 train_time:61066ms step_avg:46.47ms
step:1315/2035 train_time:61126ms step_avg:46.48ms
step:1316/2035 train_time:61185ms step_avg:46.49ms
step:1317/2035 train_time:61246ms step_avg:46.50ms
step:1318/2035 train_time:61305ms step_avg:46.51ms
step:1319/2035 train_time:61366ms step_avg:46.52ms
step:1320/2035 train_time:61426ms step_avg:46.53ms
step:1321/2035 train_time:61487ms step_avg:46.55ms
step:1322/2035 train_time:61548ms step_avg:46.56ms
step:1323/2035 train_time:61608ms step_avg:46.57ms
step:1324/2035 train_time:61667ms step_avg:46.58ms
step:1325/2035 train_time:61728ms step_avg:46.59ms
step:1326/2035 train_time:61787ms step_avg:46.60ms
step:1327/2035 train_time:61847ms step_avg:46.61ms
step:1328/2035 train_time:61907ms step_avg:46.62ms
step:1329/2035 train_time:61967ms step_avg:46.63ms
step:1330/2035 train_time:62026ms step_avg:46.64ms
step:1331/2035 train_time:62086ms step_avg:46.65ms
step:1332/2035 train_time:62173ms step_avg:46.68ms
step:1333/2035 train_time:62262ms step_avg:46.71ms
step:1334/2035 train_time:62349ms step_avg:46.74ms
step:1335/2035 train_time:62438ms step_avg:46.77ms
step:1336/2035 train_time:62525ms step_avg:46.80ms
step:1337/2035 train_time:62614ms step_avg:46.83ms
step:1338/2035 train_time:62701ms step_avg:46.86ms
step:1339/2035 train_time:62789ms step_avg:46.89ms
step:1340/2035 train_time:62876ms step_avg:46.92ms
step:1341/2035 train_time:62964ms step_avg:46.95ms
step:1342/2035 train_time:63051ms step_avg:46.98ms
step:1343/2035 train_time:63138ms step_avg:47.01ms
step:1344/2035 train_time:63225ms step_avg:47.04ms
step:1345/2035 train_time:63313ms step_avg:47.07ms
step:1346/2035 train_time:63401ms step_avg:47.10ms
step:1347/2035 train_time:63490ms step_avg:47.13ms
step:1348/2035 train_time:63576ms step_avg:47.16ms
step:1349/2035 train_time:63665ms step_avg:47.19ms
step:1350/2035 train_time:63753ms step_avg:47.22ms
step:1351/2035 train_time:63841ms step_avg:47.25ms
step:1352/2035 train_time:63927ms step_avg:47.28ms
step:1353/2035 train_time:64015ms step_avg:47.31ms
step:1354/2035 train_time:64102ms step_avg:47.34ms
step:1355/2035 train_time:64189ms step_avg:47.37ms
step:1356/2035 train_time:64275ms step_avg:47.40ms
step:1357/2035 train_time:64363ms step_avg:47.43ms
step:1358/2035 train_time:64452ms step_avg:47.46ms
step:1359/2035 train_time:64540ms step_avg:47.49ms
step:1360/2035 train_time:64627ms step_avg:47.52ms
step:1361/2035 train_time:64715ms step_avg:47.55ms
step:1362/2035 train_time:64803ms step_avg:47.58ms
step:1363/2035 train_time:64890ms step_avg:47.61ms
step:1364/2035 train_time:64977ms step_avg:47.64ms
step:1365/2035 train_time:65065ms step_avg:47.67ms
step:1366/2035 train_time:65152ms step_avg:47.70ms
step:1367/2035 train_time:65240ms step_avg:47.72ms
step:1368/2035 train_time:65327ms step_avg:47.75ms
step:1369/2035 train_time:65414ms step_avg:47.78ms
step:1370/2035 train_time:65502ms step_avg:47.81ms
step:1371/2035 train_time:65590ms step_avg:47.84ms
step:1372/2035 train_time:65677ms step_avg:47.87ms
step:1373/2035 train_time:65766ms step_avg:47.90ms
step:1374/2035 train_time:65853ms step_avg:47.93ms
step:1375/2035 train_time:65942ms step_avg:47.96ms
step:1376/2035 train_time:66029ms step_avg:47.99ms
step:1377/2035 train_time:66117ms step_avg:48.02ms
step:1378/2035 train_time:66204ms step_avg:48.04ms
step:1379/2035 train_time:66292ms step_avg:48.07ms
step:1380/2035 train_time:66378ms step_avg:48.10ms
step:1381/2035 train_time:66466ms step_avg:48.13ms
step:1382/2035 train_time:66553ms step_avg:48.16ms
step:1383/2035 train_time:66641ms step_avg:48.19ms
step:1384/2035 train_time:66729ms step_avg:48.21ms
step:1385/2035 train_time:66817ms step_avg:48.24ms
step:1386/2035 train_time:66904ms step_avg:48.27ms
step:1387/2035 train_time:66992ms step_avg:48.30ms
step:1388/2035 train_time:67079ms step_avg:48.33ms
step:1389/2035 train_time:67167ms step_avg:48.36ms
step:1390/2035 train_time:67253ms step_avg:48.38ms
step:1391/2035 train_time:67342ms step_avg:48.41ms
step:1392/2035 train_time:67429ms step_avg:48.44ms
step:1393/2035 train_time:67517ms step_avg:48.47ms
step:1394/2035 train_time:67604ms step_avg:48.50ms
step:1395/2035 train_time:67693ms step_avg:48.53ms
step:1396/2035 train_time:67780ms step_avg:48.55ms
step:1397/2035 train_time:67868ms step_avg:48.58ms
step:1398/2035 train_time:67955ms step_avg:48.61ms
step:1399/2035 train_time:68043ms step_avg:48.64ms
step:1400/2035 train_time:68132ms step_avg:48.67ms
step:1401/2035 train_time:68220ms step_avg:48.69ms
step:1402/2035 train_time:68307ms step_avg:48.72ms
step:1403/2035 train_time:68395ms step_avg:48.75ms
step:1404/2035 train_time:68482ms step_avg:48.78ms
step:1405/2035 train_time:68571ms step_avg:48.81ms
step:1406/2035 train_time:68657ms step_avg:48.83ms
step:1407/2035 train_time:68745ms step_avg:48.86ms
step:1408/2035 train_time:68832ms step_avg:48.89ms
step:1409/2035 train_time:68920ms step_avg:48.91ms
step:1410/2035 train_time:69007ms step_avg:48.94ms
step:1411/2035 train_time:69095ms step_avg:48.97ms
step:1412/2035 train_time:69181ms step_avg:49.00ms
step:1413/2035 train_time:69270ms step_avg:49.02ms
step:1414/2035 train_time:69357ms step_avg:49.05ms
step:1415/2035 train_time:69445ms step_avg:49.08ms
step:1416/2035 train_time:69532ms step_avg:49.10ms
step:1417/2035 train_time:69621ms step_avg:49.13ms
step:1418/2035 train_time:69707ms step_avg:49.16ms
step:1419/2035 train_time:69796ms step_avg:49.19ms
step:1420/2035 train_time:69883ms step_avg:49.21ms
step:1421/2035 train_time:69973ms step_avg:49.24ms
step:1422/2035 train_time:70060ms step_avg:49.27ms
step:1423/2035 train_time:70148ms step_avg:49.30ms
step:1424/2035 train_time:70235ms step_avg:49.32ms
step:1425/2035 train_time:70323ms step_avg:49.35ms
step:1426/2035 train_time:70410ms step_avg:49.38ms
step:1427/2035 train_time:70498ms step_avg:49.40ms
step:1428/2035 train_time:70585ms step_avg:49.43ms
step:1429/2035 train_time:70674ms step_avg:49.46ms
step:1430/2035 train_time:70761ms step_avg:49.48ms
step:1431/2035 train_time:70849ms step_avg:49.51ms
step:1432/2035 train_time:70936ms step_avg:49.54ms
step:1433/2035 train_time:71025ms step_avg:49.56ms
step:1434/2035 train_time:71111ms step_avg:49.59ms
step:1435/2035 train_time:71199ms step_avg:49.62ms
step:1436/2035 train_time:71286ms step_avg:49.64ms
step:1437/2035 train_time:71374ms step_avg:49.67ms
step:1438/2035 train_time:71461ms step_avg:49.69ms
step:1439/2035 train_time:71549ms step_avg:49.72ms
step:1440/2035 train_time:71636ms step_avg:49.75ms
step:1441/2035 train_time:71725ms step_avg:49.77ms
step:1442/2035 train_time:71811ms step_avg:49.80ms
step:1443/2035 train_time:71899ms step_avg:49.83ms
step:1444/2035 train_time:71986ms step_avg:49.85ms
step:1445/2035 train_time:72074ms step_avg:49.88ms
step:1446/2035 train_time:72161ms step_avg:49.90ms
step:1447/2035 train_time:72250ms step_avg:49.93ms
step:1448/2035 train_time:72337ms step_avg:49.96ms
step:1449/2035 train_time:72425ms step_avg:49.98ms
step:1450/2035 train_time:72512ms step_avg:50.01ms
step:1451/2035 train_time:72599ms step_avg:50.03ms
step:1452/2035 train_time:72687ms step_avg:50.06ms
step:1453/2035 train_time:72775ms step_avg:50.09ms
step:1454/2035 train_time:72862ms step_avg:50.11ms
step:1455/2035 train_time:72950ms step_avg:50.14ms
step:1456/2035 train_time:73037ms step_avg:50.16ms
step:1457/2035 train_time:73126ms step_avg:50.19ms
step:1458/2035 train_time:73212ms step_avg:50.21ms
step:1459/2035 train_time:73300ms step_avg:50.24ms
step:1460/2035 train_time:73387ms step_avg:50.27ms
step:1461/2035 train_time:73476ms step_avg:50.29ms
step:1462/2035 train_time:73563ms step_avg:50.32ms
step:1463/2035 train_time:73651ms step_avg:50.34ms
step:1464/2035 train_time:73737ms step_avg:50.37ms
step:1465/2035 train_time:73826ms step_avg:50.39ms
step:1466/2035 train_time:73912ms step_avg:50.42ms
step:1467/2035 train_time:74001ms step_avg:50.44ms
step:1468/2035 train_time:74088ms step_avg:50.47ms
step:1469/2035 train_time:74177ms step_avg:50.49ms
step:1470/2035 train_time:74265ms step_avg:50.52ms
step:1471/2035 train_time:74354ms step_avg:50.55ms
step:1472/2035 train_time:74441ms step_avg:50.57ms
step:1473/2035 train_time:74529ms step_avg:50.60ms
step:1474/2035 train_time:74616ms step_avg:50.62ms
step:1475/2035 train_time:74704ms step_avg:50.65ms
step:1476/2035 train_time:74791ms step_avg:50.67ms
step:1477/2035 train_time:74879ms step_avg:50.70ms
step:1478/2035 train_time:74966ms step_avg:50.72ms
step:1479/2035 train_time:75054ms step_avg:50.75ms
step:1480/2035 train_time:75140ms step_avg:50.77ms
step:1481/2035 train_time:75229ms step_avg:50.80ms
step:1482/2035 train_time:75316ms step_avg:50.82ms
step:1483/2035 train_time:75404ms step_avg:50.85ms
step:1484/2035 train_time:75491ms step_avg:50.87ms
step:1485/2035 train_time:75580ms step_avg:50.90ms
step:1486/2035 train_time:75667ms step_avg:50.92ms
step:1487/2035 train_time:75755ms step_avg:50.94ms
step:1488/2035 train_time:75842ms step_avg:50.97ms
step:1489/2035 train_time:75930ms step_avg:50.99ms
step:1490/2035 train_time:76017ms step_avg:51.02ms
step:1491/2035 train_time:76106ms step_avg:51.04ms
step:1492/2035 train_time:76192ms step_avg:51.07ms
step:1493/2035 train_time:76281ms step_avg:51.09ms
step:1494/2035 train_time:76368ms step_avg:51.12ms
step:1495/2035 train_time:76456ms step_avg:51.14ms
step:1496/2035 train_time:76544ms step_avg:51.17ms
step:1497/2035 train_time:76632ms step_avg:51.19ms
step:1498/2035 train_time:76718ms step_avg:51.21ms
step:1499/2035 train_time:76806ms step_avg:51.24ms
step:1500/2035 train_time:76893ms step_avg:51.26ms
step:1500/2035 val_loss:3.4535 train_time:76984ms step_avg:51.32ms
step:1501/2035 train_time:77005ms step_avg:51.30ms
step:1502/2035 train_time:77073ms step_avg:51.31ms
step:1503/2035 train_time:77165ms step_avg:51.34ms
step:1504/2035 train_time:77252ms step_avg:51.36ms
step:1505/2035 train_time:77339ms step_avg:51.39ms
step:1506/2035 train_time:77425ms step_avg:51.41ms
step:1507/2035 train_time:77513ms step_avg:51.44ms
step:1508/2035 train_time:77598ms step_avg:51.46ms
step:1509/2035 train_time:77685ms step_avg:51.48ms
step:1510/2035 train_time:77772ms step_avg:51.50ms
step:1511/2035 train_time:77859ms step_avg:51.53ms
step:1512/2035 train_time:77947ms step_avg:51.55ms
step:1513/2035 train_time:78037ms step_avg:51.58ms
step:1514/2035 train_time:78125ms step_avg:51.60ms
step:1515/2035 train_time:78215ms step_avg:51.63ms
step:1516/2035 train_time:78302ms step_avg:51.65ms
step:1517/2035 train_time:78391ms step_avg:51.67ms
step:1518/2035 train_time:78478ms step_avg:51.70ms
step:1519/2035 train_time:78565ms step_avg:51.72ms
step:1520/2035 train_time:78651ms step_avg:51.74ms
step:1521/2035 train_time:78739ms step_avg:51.77ms
step:1522/2035 train_time:78826ms step_avg:51.79ms
step:1523/2035 train_time:78914ms step_avg:51.81ms
step:1524/2035 train_time:79001ms step_avg:51.84ms
step:1525/2035 train_time:79091ms step_avg:51.86ms
step:1526/2035 train_time:79179ms step_avg:51.89ms
step:1527/2035 train_time:79268ms step_avg:51.91ms
step:1528/2035 train_time:79354ms step_avg:51.93ms
step:1529/2035 train_time:79442ms step_avg:51.96ms
step:1530/2035 train_time:79529ms step_avg:51.98ms
step:1531/2035 train_time:79616ms step_avg:52.00ms
step:1532/2035 train_time:79702ms step_avg:52.02ms
step:1533/2035 train_time:79790ms step_avg:52.05ms
step:1534/2035 train_time:79876ms step_avg:52.07ms
step:1535/2035 train_time:79965ms step_avg:52.09ms
step:1536/2035 train_time:80053ms step_avg:52.12ms
step:1537/2035 train_time:80142ms step_avg:52.14ms
step:1538/2035 train_time:80229ms step_avg:52.16ms
step:1539/2035 train_time:80318ms step_avg:52.19ms
step:1540/2035 train_time:80405ms step_avg:52.21ms
step:1541/2035 train_time:80493ms step_avg:52.23ms
step:1542/2035 train_time:80579ms step_avg:52.26ms
step:1543/2035 train_time:80667ms step_avg:52.28ms
step:1544/2035 train_time:80753ms step_avg:52.30ms
step:1545/2035 train_time:80842ms step_avg:52.32ms
step:1546/2035 train_time:80929ms step_avg:52.35ms
step:1547/2035 train_time:81019ms step_avg:52.37ms
step:1548/2035 train_time:81106ms step_avg:52.39ms
step:1549/2035 train_time:81195ms step_avg:52.42ms
step:1550/2035 train_time:81282ms step_avg:52.44ms
step:1551/2035 train_time:81370ms step_avg:52.46ms
step:1552/2035 train_time:81458ms step_avg:52.49ms
step:1553/2035 train_time:81545ms step_avg:52.51ms
step:1554/2035 train_time:81632ms step_avg:52.53ms
step:1555/2035 train_time:81720ms step_avg:52.55ms
step:1556/2035 train_time:81807ms step_avg:52.58ms
step:1557/2035 train_time:81894ms step_avg:52.60ms
step:1558/2035 train_time:81982ms step_avg:52.62ms
step:1559/2035 train_time:82071ms step_avg:52.64ms
step:1560/2035 train_time:82158ms step_avg:52.67ms
step:1561/2035 train_time:82246ms step_avg:52.69ms
step:1562/2035 train_time:82333ms step_avg:52.71ms
step:1563/2035 train_time:82422ms step_avg:52.73ms
step:1564/2035 train_time:82509ms step_avg:52.75ms
step:1565/2035 train_time:82597ms step_avg:52.78ms
step:1566/2035 train_time:82683ms step_avg:52.80ms
step:1567/2035 train_time:82772ms step_avg:52.82ms
step:1568/2035 train_time:82859ms step_avg:52.84ms
step:1569/2035 train_time:82948ms step_avg:52.87ms
step:1570/2035 train_time:83036ms step_avg:52.89ms
step:1571/2035 train_time:83123ms step_avg:52.91ms
step:1572/2035 train_time:83210ms step_avg:52.93ms
step:1573/2035 train_time:83298ms step_avg:52.95ms
step:1574/2035 train_time:83385ms step_avg:52.98ms
step:1575/2035 train_time:83474ms step_avg:53.00ms
step:1576/2035 train_time:83561ms step_avg:53.02ms
step:1577/2035 train_time:83649ms step_avg:53.04ms
step:1578/2035 train_time:83736ms step_avg:53.06ms
step:1579/2035 train_time:83824ms step_avg:53.09ms
step:1580/2035 train_time:83910ms step_avg:53.11ms
step:1581/2035 train_time:84000ms step_avg:53.13ms
step:1582/2035 train_time:84087ms step_avg:53.15ms
step:1583/2035 train_time:84175ms step_avg:53.17ms
step:1584/2035 train_time:84262ms step_avg:53.20ms
step:1585/2035 train_time:84350ms step_avg:53.22ms
step:1586/2035 train_time:84437ms step_avg:53.24ms
step:1587/2035 train_time:84525ms step_avg:53.26ms
step:1588/2035 train_time:84612ms step_avg:53.28ms
step:1589/2035 train_time:84700ms step_avg:53.30ms
step:1590/2035 train_time:84787ms step_avg:53.33ms
step:1591/2035 train_time:84875ms step_avg:53.35ms
step:1592/2035 train_time:84962ms step_avg:53.37ms
step:1593/2035 train_time:85051ms step_avg:53.39ms
step:1594/2035 train_time:85138ms step_avg:53.41ms
step:1595/2035 train_time:85227ms step_avg:53.43ms
step:1596/2035 train_time:85314ms step_avg:53.45ms
step:1597/2035 train_time:85403ms step_avg:53.48ms
step:1598/2035 train_time:85490ms step_avg:53.50ms
step:1599/2035 train_time:85579ms step_avg:53.52ms
step:1600/2035 train_time:85665ms step_avg:53.54ms
step:1601/2035 train_time:85753ms step_avg:53.56ms
step:1602/2035 train_time:85840ms step_avg:53.58ms
step:1603/2035 train_time:85929ms step_avg:53.60ms
step:1604/2035 train_time:86016ms step_avg:53.63ms
step:1605/2035 train_time:86104ms step_avg:53.65ms
step:1606/2035 train_time:86192ms step_avg:53.67ms
step:1607/2035 train_time:86280ms step_avg:53.69ms
step:1608/2035 train_time:86367ms step_avg:53.71ms
step:1609/2035 train_time:86456ms step_avg:53.73ms
step:1610/2035 train_time:86543ms step_avg:53.75ms
step:1611/2035 train_time:86632ms step_avg:53.78ms
step:1612/2035 train_time:86719ms step_avg:53.80ms
step:1613/2035 train_time:86806ms step_avg:53.82ms
step:1614/2035 train_time:86893ms step_avg:53.84ms
step:1615/2035 train_time:86982ms step_avg:53.86ms
step:1616/2035 train_time:87069ms step_avg:53.88ms
step:1617/2035 train_time:87158ms step_avg:53.90ms
step:1618/2035 train_time:87244ms step_avg:53.92ms
step:1619/2035 train_time:87333ms step_avg:53.94ms
step:1620/2035 train_time:87420ms step_avg:53.96ms
step:1621/2035 train_time:87508ms step_avg:53.98ms
step:1622/2035 train_time:87595ms step_avg:54.00ms
step:1623/2035 train_time:87684ms step_avg:54.03ms
step:1624/2035 train_time:87770ms step_avg:54.05ms
step:1625/2035 train_time:87859ms step_avg:54.07ms
step:1626/2035 train_time:87947ms step_avg:54.09ms
step:1627/2035 train_time:88035ms step_avg:54.11ms
step:1628/2035 train_time:88121ms step_avg:54.13ms
step:1629/2035 train_time:88210ms step_avg:54.15ms
step:1630/2035 train_time:88297ms step_avg:54.17ms
step:1631/2035 train_time:88385ms step_avg:54.19ms
step:1632/2035 train_time:88472ms step_avg:54.21ms
step:1633/2035 train_time:88560ms step_avg:54.23ms
step:1634/2035 train_time:88647ms step_avg:54.25ms
step:1635/2035 train_time:88736ms step_avg:54.27ms
step:1636/2035 train_time:88822ms step_avg:54.29ms
step:1637/2035 train_time:88911ms step_avg:54.31ms
step:1638/2035 train_time:88998ms step_avg:54.33ms
step:1639/2035 train_time:89086ms step_avg:54.35ms
step:1640/2035 train_time:89174ms step_avg:54.37ms
step:1641/2035 train_time:89263ms step_avg:54.40ms
step:1642/2035 train_time:89350ms step_avg:54.42ms
step:1643/2035 train_time:89437ms step_avg:54.44ms
step:1644/2035 train_time:89523ms step_avg:54.45ms
step:1645/2035 train_time:89612ms step_avg:54.48ms
step:1646/2035 train_time:89699ms step_avg:54.50ms
step:1647/2035 train_time:89787ms step_avg:54.52ms
step:1648/2035 train_time:89874ms step_avg:54.54ms
step:1649/2035 train_time:89962ms step_avg:54.56ms
step:1650/2035 train_time:90050ms step_avg:54.58ms
step:1651/2035 train_time:90139ms step_avg:54.60ms
step:1652/2035 train_time:90225ms step_avg:54.62ms
step:1653/2035 train_time:90314ms step_avg:54.64ms
step:1654/2035 train_time:90401ms step_avg:54.66ms
step:1655/2035 train_time:90489ms step_avg:54.68ms
step:1656/2035 train_time:90576ms step_avg:54.70ms
step:1657/2035 train_time:90665ms step_avg:54.72ms
step:1658/2035 train_time:90751ms step_avg:54.74ms
step:1659/2035 train_time:90840ms step_avg:54.76ms
step:1660/2035 train_time:90927ms step_avg:54.78ms
step:1661/2035 train_time:91015ms step_avg:54.80ms
step:1662/2035 train_time:91101ms step_avg:54.81ms
step:1663/2035 train_time:91190ms step_avg:54.83ms
step:1664/2035 train_time:91277ms step_avg:54.85ms
step:1665/2035 train_time:91366ms step_avg:54.87ms
step:1666/2035 train_time:91453ms step_avg:54.89ms
step:1667/2035 train_time:91541ms step_avg:54.91ms
step:1668/2035 train_time:91628ms step_avg:54.93ms
step:1669/2035 train_time:91716ms step_avg:54.95ms
step:1670/2035 train_time:91803ms step_avg:54.97ms
step:1671/2035 train_time:91891ms step_avg:54.99ms
step:1672/2035 train_time:91978ms step_avg:55.01ms
step:1673/2035 train_time:92067ms step_avg:55.03ms
step:1674/2035 train_time:92153ms step_avg:55.05ms
step:1675/2035 train_time:92242ms step_avg:55.07ms
step:1676/2035 train_time:92329ms step_avg:55.09ms
step:1677/2035 train_time:92417ms step_avg:55.11ms
step:1678/2035 train_time:92503ms step_avg:55.13ms
step:1679/2035 train_time:92591ms step_avg:55.15ms
step:1680/2035 train_time:92679ms step_avg:55.17ms
step:1681/2035 train_time:92767ms step_avg:55.19ms
step:1682/2035 train_time:92854ms step_avg:55.20ms
step:1683/2035 train_time:92943ms step_avg:55.22ms
step:1684/2035 train_time:93029ms step_avg:55.24ms
step:1685/2035 train_time:93118ms step_avg:55.26ms
step:1686/2035 train_time:93204ms step_avg:55.28ms
step:1687/2035 train_time:93293ms step_avg:55.30ms
step:1688/2035 train_time:93380ms step_avg:55.32ms
step:1689/2035 train_time:93468ms step_avg:55.34ms
step:1690/2035 train_time:93554ms step_avg:55.36ms
step:1691/2035 train_time:93642ms step_avg:55.38ms
step:1692/2035 train_time:93730ms step_avg:55.40ms
step:1693/2035 train_time:93819ms step_avg:55.42ms
step:1694/2035 train_time:93906ms step_avg:55.43ms
step:1695/2035 train_time:93994ms step_avg:55.45ms
step:1696/2035 train_time:94081ms step_avg:55.47ms
step:1697/2035 train_time:94169ms step_avg:55.49ms
step:1698/2035 train_time:94256ms step_avg:55.51ms
step:1699/2035 train_time:94344ms step_avg:55.53ms
step:1700/2035 train_time:94431ms step_avg:55.55ms
step:1701/2035 train_time:94519ms step_avg:55.57ms
step:1702/2035 train_time:94606ms step_avg:55.58ms
step:1703/2035 train_time:94695ms step_avg:55.60ms
step:1704/2035 train_time:94782ms step_avg:55.62ms
step:1705/2035 train_time:94870ms step_avg:55.64ms
step:1706/2035 train_time:94957ms step_avg:55.66ms
step:1707/2035 train_time:95045ms step_avg:55.68ms
step:1708/2035 train_time:95132ms step_avg:55.70ms
step:1709/2035 train_time:95220ms step_avg:55.72ms
step:1710/2035 train_time:95307ms step_avg:55.74ms
step:1711/2035 train_time:95396ms step_avg:55.75ms
step:1712/2035 train_time:95483ms step_avg:55.77ms
step:1713/2035 train_time:95571ms step_avg:55.79ms
step:1714/2035 train_time:95657ms step_avg:55.81ms
step:1715/2035 train_time:95746ms step_avg:55.83ms
step:1716/2035 train_time:95832ms step_avg:55.85ms
step:1717/2035 train_time:95920ms step_avg:55.87ms
step:1718/2035 train_time:96007ms step_avg:55.88ms
step:1719/2035 train_time:96096ms step_avg:55.90ms
step:1720/2035 train_time:96183ms step_avg:55.92ms
step:1721/2035 train_time:96272ms step_avg:55.94ms
step:1722/2035 train_time:96359ms step_avg:55.96ms
step:1723/2035 train_time:96447ms step_avg:55.98ms
step:1724/2035 train_time:96534ms step_avg:55.99ms
step:1725/2035 train_time:96622ms step_avg:56.01ms
step:1726/2035 train_time:96709ms step_avg:56.03ms
step:1727/2035 train_time:96798ms step_avg:56.05ms
step:1728/2035 train_time:96884ms step_avg:56.07ms
step:1729/2035 train_time:96974ms step_avg:56.09ms
step:1730/2035 train_time:97061ms step_avg:56.10ms
step:1731/2035 train_time:97150ms step_avg:56.12ms
step:1732/2035 train_time:97236ms step_avg:56.14ms
step:1733/2035 train_time:97324ms step_avg:56.16ms
step:1734/2035 train_time:97412ms step_avg:56.18ms
step:1735/2035 train_time:97501ms step_avg:56.20ms
step:1736/2035 train_time:97588ms step_avg:56.21ms
step:1737/2035 train_time:97677ms step_avg:56.23ms
step:1738/2035 train_time:97764ms step_avg:56.25ms
step:1739/2035 train_time:97852ms step_avg:56.27ms
step:1740/2035 train_time:97940ms step_avg:56.29ms
step:1741/2035 train_time:98028ms step_avg:56.31ms
step:1742/2035 train_time:98115ms step_avg:56.32ms
step:1743/2035 train_time:98203ms step_avg:56.34ms
step:1744/2035 train_time:98291ms step_avg:56.36ms
step:1745/2035 train_time:98379ms step_avg:56.38ms
step:1746/2035 train_time:98466ms step_avg:56.39ms
step:1747/2035 train_time:98554ms step_avg:56.41ms
step:1748/2035 train_time:98642ms step_avg:56.43ms
step:1749/2035 train_time:98730ms step_avg:56.45ms
step:1750/2035 train_time:98817ms step_avg:56.47ms
step:1750/2035 val_loss:3.3558 train_time:98906ms step_avg:56.52ms
step:1751/2035 train_time:98927ms step_avg:56.50ms
step:1752/2035 train_time:98995ms step_avg:56.50ms
step:1753/2035 train_time:99083ms step_avg:56.52ms
step:1754/2035 train_time:99170ms step_avg:56.54ms
step:1755/2035 train_time:99258ms step_avg:56.56ms
step:1756/2035 train_time:99345ms step_avg:56.57ms
step:1757/2035 train_time:99433ms step_avg:56.59ms
step:1758/2035 train_time:99520ms step_avg:56.61ms
step:1759/2035 train_time:99607ms step_avg:56.63ms
step:1760/2035 train_time:99693ms step_avg:56.64ms
step:1761/2035 train_time:99781ms step_avg:56.66ms
step:1762/2035 train_time:99870ms step_avg:56.68ms
step:1763/2035 train_time:99960ms step_avg:56.70ms
step:1764/2035 train_time:100048ms step_avg:56.72ms
step:1765/2035 train_time:100137ms step_avg:56.73ms
step:1766/2035 train_time:100225ms step_avg:56.75ms
step:1767/2035 train_time:100313ms step_avg:56.77ms
step:1768/2035 train_time:100400ms step_avg:56.79ms
step:1769/2035 train_time:100487ms step_avg:56.80ms
step:1770/2035 train_time:100573ms step_avg:56.82ms
step:1771/2035 train_time:100661ms step_avg:56.84ms
step:1772/2035 train_time:100747ms step_avg:56.86ms
step:1773/2035 train_time:100836ms step_avg:56.87ms
step:1774/2035 train_time:100924ms step_avg:56.89ms
step:1775/2035 train_time:101012ms step_avg:56.91ms
step:1776/2035 train_time:101100ms step_avg:56.93ms
step:1777/2035 train_time:101188ms step_avg:56.94ms
step:1778/2035 train_time:101275ms step_avg:56.96ms
step:1779/2035 train_time:101364ms step_avg:56.98ms
step:1780/2035 train_time:101450ms step_avg:56.99ms
step:1781/2035 train_time:101538ms step_avg:57.01ms
step:1782/2035 train_time:101625ms step_avg:57.03ms
step:1783/2035 train_time:101713ms step_avg:57.05ms
step:1784/2035 train_time:101801ms step_avg:57.06ms
step:1785/2035 train_time:101889ms step_avg:57.08ms
step:1786/2035 train_time:101977ms step_avg:57.10ms
step:1787/2035 train_time:102066ms step_avg:57.12ms
step:1788/2035 train_time:102153ms step_avg:57.13ms
step:1789/2035 train_time:102241ms step_avg:57.15ms
step:1790/2035 train_time:102328ms step_avg:57.17ms
step:1791/2035 train_time:102416ms step_avg:57.18ms
step:1792/2035 train_time:102502ms step_avg:57.20ms
step:1793/2035 train_time:102590ms step_avg:57.22ms
step:1794/2035 train_time:102677ms step_avg:57.23ms
step:1795/2035 train_time:102764ms step_avg:57.25ms
step:1796/2035 train_time:102852ms step_avg:57.27ms
step:1797/2035 train_time:102940ms step_avg:57.28ms
step:1798/2035 train_time:103027ms step_avg:57.30ms
step:1799/2035 train_time:103117ms step_avg:57.32ms
step:1800/2035 train_time:103203ms step_avg:57.34ms
step:1801/2035 train_time:103293ms step_avg:57.35ms
step:1802/2035 train_time:103380ms step_avg:57.37ms
step:1803/2035 train_time:103468ms step_avg:57.39ms
step:1804/2035 train_time:103554ms step_avg:57.40ms
step:1805/2035 train_time:103644ms step_avg:57.42ms
step:1806/2035 train_time:103730ms step_avg:57.44ms
step:1807/2035 train_time:103818ms step_avg:57.45ms
step:1808/2035 train_time:103905ms step_avg:57.47ms
step:1809/2035 train_time:103993ms step_avg:57.49ms
step:1810/2035 train_time:104081ms step_avg:57.50ms
step:1811/2035 train_time:104170ms step_avg:57.52ms
step:1812/2035 train_time:104257ms step_avg:57.54ms
step:1813/2035 train_time:104346ms step_avg:57.55ms
step:1814/2035 train_time:104432ms step_avg:57.57ms
step:1815/2035 train_time:104520ms step_avg:57.59ms
step:1816/2035 train_time:104607ms step_avg:57.60ms
step:1817/2035 train_time:104696ms step_avg:57.62ms
step:1818/2035 train_time:104783ms step_avg:57.64ms
step:1819/2035 train_time:104871ms step_avg:57.65ms
step:1820/2035 train_time:104958ms step_avg:57.67ms
step:1821/2035 train_time:105047ms step_avg:57.69ms
step:1822/2035 train_time:105134ms step_avg:57.70ms
step:1823/2035 train_time:105222ms step_avg:57.72ms
step:1824/2035 train_time:105310ms step_avg:57.74ms
step:1825/2035 train_time:105398ms step_avg:57.75ms
step:1826/2035 train_time:105484ms step_avg:57.77ms
step:1827/2035 train_time:105573ms step_avg:57.78ms
step:1828/2035 train_time:105660ms step_avg:57.80ms
step:1829/2035 train_time:105749ms step_avg:57.82ms
step:1830/2035 train_time:105835ms step_avg:57.83ms
step:1831/2035 train_time:105924ms step_avg:57.85ms
step:1832/2035 train_time:106011ms step_avg:57.87ms
step:1833/2035 train_time:106100ms step_avg:57.88ms
step:1834/2035 train_time:106187ms step_avg:57.90ms
step:1835/2035 train_time:106276ms step_avg:57.92ms
step:1836/2035 train_time:106363ms step_avg:57.93ms
step:1837/2035 train_time:106452ms step_avg:57.95ms
step:1838/2035 train_time:106538ms step_avg:57.96ms
step:1839/2035 train_time:106626ms step_avg:57.98ms
step:1840/2035 train_time:106712ms step_avg:58.00ms
step:1841/2035 train_time:106801ms step_avg:58.01ms
step:1842/2035 train_time:106887ms step_avg:58.03ms
step:1843/2035 train_time:106975ms step_avg:58.04ms
step:1844/2035 train_time:107063ms step_avg:58.06ms
step:1845/2035 train_time:107150ms step_avg:58.08ms
step:1846/2035 train_time:107237ms step_avg:58.09ms
step:1847/2035 train_time:107326ms step_avg:58.11ms
step:1848/2035 train_time:107413ms step_avg:58.12ms
step:1849/2035 train_time:107502ms step_avg:58.14ms
step:1850/2035 train_time:107589ms step_avg:58.16ms
step:1851/2035 train_time:107678ms step_avg:58.17ms
step:1852/2035 train_time:107764ms step_avg:58.19ms
step:1853/2035 train_time:107853ms step_avg:58.20ms
step:1854/2035 train_time:107941ms step_avg:58.22ms
step:1855/2035 train_time:108029ms step_avg:58.24ms
step:1856/2035 train_time:108116ms step_avg:58.25ms
step:1857/2035 train_time:108204ms step_avg:58.27ms
step:1858/2035 train_time:108292ms step_avg:58.28ms
step:1859/2035 train_time:108380ms step_avg:58.30ms
step:1860/2035 train_time:108467ms step_avg:58.32ms
step:1861/2035 train_time:108555ms step_avg:58.33ms
step:1862/2035 train_time:108642ms step_avg:58.35ms
step:1863/2035 train_time:108731ms step_avg:58.36ms
step:1864/2035 train_time:108817ms step_avg:58.38ms
step:1865/2035 train_time:108906ms step_avg:58.39ms
step:1866/2035 train_time:108993ms step_avg:58.41ms
step:1867/2035 train_time:109081ms step_avg:58.43ms
step:1868/2035 train_time:109168ms step_avg:58.44ms
step:1869/2035 train_time:109256ms step_avg:58.46ms
step:1870/2035 train_time:109343ms step_avg:58.47ms
step:1871/2035 train_time:109432ms step_avg:58.49ms
step:1872/2035 train_time:109519ms step_avg:58.50ms
step:1873/2035 train_time:109607ms step_avg:58.52ms
step:1874/2035 train_time:109694ms step_avg:58.53ms
step:1875/2035 train_time:109783ms step_avg:58.55ms
step:1876/2035 train_time:109870ms step_avg:58.57ms
step:1877/2035 train_time:109959ms step_avg:58.58ms
step:1878/2035 train_time:110045ms step_avg:58.60ms
step:1879/2035 train_time:110133ms step_avg:58.61ms
step:1880/2035 train_time:110220ms step_avg:58.63ms
step:1881/2035 train_time:110309ms step_avg:58.64ms
step:1882/2035 train_time:110395ms step_avg:58.66ms
step:1883/2035 train_time:110484ms step_avg:58.67ms
step:1884/2035 train_time:110571ms step_avg:58.69ms
step:1885/2035 train_time:110659ms step_avg:58.71ms
step:1886/2035 train_time:110745ms step_avg:58.72ms
step:1887/2035 train_time:110834ms step_avg:58.74ms
step:1888/2035 train_time:110921ms step_avg:58.75ms
step:1889/2035 train_time:111009ms step_avg:58.77ms
step:1890/2035 train_time:111097ms step_avg:58.78ms
step:1891/2035 train_time:111185ms step_avg:58.80ms
step:1892/2035 train_time:111273ms step_avg:58.81ms
step:1893/2035 train_time:111360ms step_avg:58.83ms
step:1894/2035 train_time:111447ms step_avg:58.84ms
step:1895/2035 train_time:111535ms step_avg:58.86ms
step:1896/2035 train_time:111622ms step_avg:58.87ms
step:1897/2035 train_time:111711ms step_avg:58.89ms
step:1898/2035 train_time:111799ms step_avg:58.90ms
step:1899/2035 train_time:111886ms step_avg:58.92ms
step:1900/2035 train_time:111973ms step_avg:58.93ms
step:1901/2035 train_time:112061ms step_avg:58.95ms
step:1902/2035 train_time:112148ms step_avg:58.96ms
step:1903/2035 train_time:112236ms step_avg:58.98ms
step:1904/2035 train_time:112323ms step_avg:58.99ms
step:1905/2035 train_time:112411ms step_avg:59.01ms
step:1906/2035 train_time:112498ms step_avg:59.02ms
step:1907/2035 train_time:112586ms step_avg:59.04ms
step:1908/2035 train_time:112673ms step_avg:59.05ms
step:1909/2035 train_time:112762ms step_avg:59.07ms
step:1910/2035 train_time:112849ms step_avg:59.08ms
step:1911/2035 train_time:112936ms step_avg:59.10ms
step:1912/2035 train_time:113023ms step_avg:59.11ms
step:1913/2035 train_time:113112ms step_avg:59.13ms
step:1914/2035 train_time:113198ms step_avg:59.14ms
step:1915/2035 train_time:113286ms step_avg:59.16ms
step:1916/2035 train_time:113373ms step_avg:59.17ms
step:1917/2035 train_time:113462ms step_avg:59.19ms
step:1918/2035 train_time:113549ms step_avg:59.20ms
step:1919/2035 train_time:113639ms step_avg:59.22ms
step:1920/2035 train_time:113725ms step_avg:59.23ms
step:1921/2035 train_time:113814ms step_avg:59.25ms
step:1922/2035 train_time:113900ms step_avg:59.26ms
step:1923/2035 train_time:113988ms step_avg:59.28ms
step:1924/2035 train_time:114075ms step_avg:59.29ms
step:1925/2035 train_time:114164ms step_avg:59.31ms
step:1926/2035 train_time:114251ms step_avg:59.32ms
step:1927/2035 train_time:114338ms step_avg:59.33ms
step:1928/2035 train_time:114425ms step_avg:59.35ms
step:1929/2035 train_time:114513ms step_avg:59.36ms
step:1930/2035 train_time:114600ms step_avg:59.38ms
step:1931/2035 train_time:114688ms step_avg:59.39ms
step:1932/2035 train_time:114775ms step_avg:59.41ms
step:1933/2035 train_time:114864ms step_avg:59.42ms
step:1934/2035 train_time:114951ms step_avg:59.44ms
step:1935/2035 train_time:115040ms step_avg:59.45ms
step:1936/2035 train_time:115126ms step_avg:59.47ms
step:1937/2035 train_time:115215ms step_avg:59.48ms
step:1938/2035 train_time:115301ms step_avg:59.49ms
step:1939/2035 train_time:115389ms step_avg:59.51ms
step:1940/2035 train_time:115477ms step_avg:59.52ms
step:1941/2035 train_time:115565ms step_avg:59.54ms
step:1942/2035 train_time:115653ms step_avg:59.55ms
step:1943/2035 train_time:115741ms step_avg:59.57ms
step:1944/2035 train_time:115828ms step_avg:59.58ms
step:1945/2035 train_time:115915ms step_avg:59.60ms
step:1946/2035 train_time:116003ms step_avg:59.61ms
step:1947/2035 train_time:116092ms step_avg:59.63ms
step:1948/2035 train_time:116179ms step_avg:59.64ms
step:1949/2035 train_time:116267ms step_avg:59.65ms
step:1950/2035 train_time:116355ms step_avg:59.67ms
step:1951/2035 train_time:116444ms step_avg:59.68ms
step:1952/2035 train_time:116531ms step_avg:59.70ms
step:1953/2035 train_time:116620ms step_avg:59.71ms
step:1954/2035 train_time:116706ms step_avg:59.73ms
step:1955/2035 train_time:116794ms step_avg:59.74ms
step:1956/2035 train_time:116881ms step_avg:59.75ms
step:1957/2035 train_time:116969ms step_avg:59.77ms
step:1958/2035 train_time:117057ms step_avg:59.78ms
step:1959/2035 train_time:117145ms step_avg:59.80ms
step:1960/2035 train_time:117232ms step_avg:59.81ms
step:1961/2035 train_time:117320ms step_avg:59.83ms
step:1962/2035 train_time:117408ms step_avg:59.84ms
step:1963/2035 train_time:117496ms step_avg:59.86ms
step:1964/2035 train_time:117583ms step_avg:59.87ms
step:1965/2035 train_time:117671ms step_avg:59.88ms
step:1966/2035 train_time:117758ms step_avg:59.90ms
step:1967/2035 train_time:117845ms step_avg:59.91ms
step:1968/2035 train_time:117933ms step_avg:59.93ms
step:1969/2035 train_time:118022ms step_avg:59.94ms
step:1970/2035 train_time:118109ms step_avg:59.95ms
step:1971/2035 train_time:118197ms step_avg:59.97ms
step:1972/2035 train_time:118284ms step_avg:59.98ms
step:1973/2035 train_time:118372ms step_avg:60.00ms
step:1974/2035 train_time:118461ms step_avg:60.01ms
step:1975/2035 train_time:118549ms step_avg:60.03ms
step:1976/2035 train_time:118636ms step_avg:60.04ms
step:1977/2035 train_time:118724ms step_avg:60.05ms
step:1978/2035 train_time:118811ms step_avg:60.07ms
step:1979/2035 train_time:118900ms step_avg:60.08ms
step:1980/2035 train_time:118987ms step_avg:60.09ms
step:1981/2035 train_time:119074ms step_avg:60.11ms
step:1982/2035 train_time:119161ms step_avg:60.12ms
step:1983/2035 train_time:119250ms step_avg:60.14ms
step:1984/2035 train_time:119338ms step_avg:60.15ms
step:1985/2035 train_time:119426ms step_avg:60.16ms
step:1986/2035 train_time:119514ms step_avg:60.18ms
step:1987/2035 train_time:119602ms step_avg:60.19ms
step:1988/2035 train_time:119689ms step_avg:60.21ms
step:1989/2035 train_time:119777ms step_avg:60.22ms
step:1990/2035 train_time:119863ms step_avg:60.23ms
step:1991/2035 train_time:119952ms step_avg:60.25ms
step:1992/2035 train_time:120039ms step_avg:60.26ms
step:1993/2035 train_time:120127ms step_avg:60.27ms
step:1994/2035 train_time:120214ms step_avg:60.29ms
step:1995/2035 train_time:120302ms step_avg:60.30ms
step:1996/2035 train_time:120390ms step_avg:60.32ms
step:1997/2035 train_time:120479ms step_avg:60.33ms
step:1998/2035 train_time:120565ms step_avg:60.34ms
step:1999/2035 train_time:120654ms step_avg:60.36ms
step:2000/2035 train_time:120742ms step_avg:60.37ms
step:2000/2035 val_loss:3.2837 train_time:120831ms step_avg:60.42ms
step:2001/2035 train_time:120852ms step_avg:60.40ms
step:2002/2035 train_time:120918ms step_avg:60.40ms
step:2003/2035 train_time:121010ms step_avg:60.41ms
step:2004/2035 train_time:121098ms step_avg:60.43ms
step:2005/2035 train_time:121186ms step_avg:60.44ms
step:2006/2035 train_time:121273ms step_avg:60.45ms
step:2007/2035 train_time:121360ms step_avg:60.47ms
step:2008/2035 train_time:121447ms step_avg:60.48ms
step:2009/2035 train_time:121536ms step_avg:60.50ms
step:2010/2035 train_time:121623ms step_avg:60.51ms
step:2011/2035 train_time:121712ms step_avg:60.52ms
step:2012/2035 train_time:121800ms step_avg:60.54ms
step:2013/2035 train_time:121891ms step_avg:60.55ms
step:2014/2035 train_time:121979ms step_avg:60.57ms
step:2015/2035 train_time:122069ms step_avg:60.58ms
step:2016/2035 train_time:122156ms step_avg:60.59ms
step:2017/2035 train_time:122244ms step_avg:60.61ms
step:2018/2035 train_time:122330ms step_avg:60.62ms
step:2019/2035 train_time:122419ms step_avg:60.63ms
step:2020/2035 train_time:122506ms step_avg:60.65ms
step:2021/2035 train_time:122594ms step_avg:60.66ms
step:2022/2035 train_time:122681ms step_avg:60.67ms
step:2023/2035 train_time:122769ms step_avg:60.69ms
step:2024/2035 train_time:122857ms step_avg:60.70ms
step:2025/2035 train_time:122947ms step_avg:60.71ms
step:2026/2035 train_time:123035ms step_avg:60.73ms
step:2027/2035 train_time:123124ms step_avg:60.74ms
step:2028/2035 train_time:123211ms step_avg:60.75ms
step:2029/2035 train_time:123299ms step_avg:60.77ms
step:2030/2035 train_time:123386ms step_avg:60.78ms
step:2031/2035 train_time:123473ms step_avg:60.79ms
step:2032/2035 train_time:123560ms step_avg:60.81ms
step:2033/2035 train_time:123648ms step_avg:60.82ms
step:2034/2035 train_time:123735ms step_avg:60.83ms
step:2035/2035 train_time:123824ms step_avg:60.85ms
step:2035/2035 val_loss:3.2763 train_time:123914ms step_avg:60.89ms
peak memory allocated: 29512 MiB reserved: 38536 MiB

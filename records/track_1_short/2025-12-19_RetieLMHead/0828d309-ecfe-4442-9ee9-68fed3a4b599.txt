import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from collections import defaultdict
from itertools import accumulate
from pathlib import Path
import gc

os.environ["PYTORCH_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64


# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng

@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor, split_baddbmm: bool = False):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    # Select batched vs unbatched
    if split_baddbmm:
        BX_matmul = torch.bmm if X.ndim > 2 else torch.mm
    else:
        aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A

        # Referencing X twice causes pytorch to make a defensive copy,
        # resulting in a cudaMemcpyAsync in baddbmm.
        # For large matrices (i.e., the mlp weights), it's faster to split
        # the operation into two kernels to avoid this.
        if split_baddbmm:
            BX_matmul(B, X, out=C)  # C = B @ X
            C.add_(X, alpha=a)      # C = C + a*X  (in-place, X only read)
        else:
            aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X

        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X


# -----------------------------------------------------------------------------
# Compiled helpers for NorMuon by @chrisjmccormick

@torch.compile(dynamic=False, fullgraph=True)
def cautious_wd_and_update_inplace(p, v, wd_tensor, lr_tensor):
    """Cautious weight decay + parameter update. wd_tensor and lr_tensor are 0-D CPU tensors."""
    mask = (v * p) >= 0
    wd_factor = wd_tensor.to(p.dtype)
    lr_factor = lr_tensor.to(p.dtype)
    p.copy_(p - (p * mask * wd_factor * lr_factor) - (v * lr_factor))


@torch.compile(dynamic=False, fullgraph=True)
def apply_normuon_variance_reduction(v_chunk, second_momentum_buffer, beta2, red_dim):
    """NorMuon variance reduction. Algebraically fuses the normalization steps to minimize memory ops."""
    v_mean = v_chunk.float().square().mean(dim=red_dim, keepdim=True)
    red_dim_size = v_chunk.size(red_dim)
    v_norm_sq = v_mean.sum(dim=(-2, -1), keepdim=True).mul_(red_dim_size)
    v_norm = v_norm_sq.sqrt_()
    second_momentum_buffer.lerp_(v_mean.to(dtype=second_momentum_buffer.dtype), 1 - beta2)
    step_size = second_momentum_buffer.clamp_min(1e-10).rsqrt_()
    scaled_sq_sum = (v_mean * red_dim_size) * step_size.float().square()
    v_norm_new = scaled_sq_sum.sum(dim=(-2, -1), keepdim=True).sqrt_()
    final_scale = step_size * (v_norm / v_norm_new.clamp_min_(1e-10))
    return v_chunk.mul_(final_scale.type_as(v_chunk))


# -----------------------------------------------------------------------------
# NorMuon optimizer

class NorMuon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).

    Differences from standard Muon:
    - Newton-Shulz is replaced with Polar Express for the orthogonalization step
    - NorMuon adds a low-rank variance estimator similar to Adafactor. https://arxiv.org/pdf/2510.05491
    - small 1D parameters handled here instead of in Adam
    - Cautious weight decay, a gated version of decoupled weight decay
    - Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as
    needed on the forward pass. This enables attn and mlp weights to be contained within the same
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn.
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, beta2=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, beta2=beta2)
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def reset(self):
        # expose a reset for clearing buffers
        for group in self.param_groups:
            group["momentum_buffer"].zero_()
            group["second_momentum_buffer"].zero_()

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per module.
        """
        groups = defaultdict(list)
        for param in params:
            groups[param.label].append(param)

        param_groups = []
        for module_name, group_params in groups.items():
            chunk_size = (len(group_params) + self.world_size - 1) // self.world_size
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))

        return param_groups

    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn
        and mlp params when a param group is split across GPUs.
        """
        module_group_order = ['smear_gate', 'attn_gate', 'attn', 'mlp']
        params_list = list(params)
        params_list.sort(key=lambda x: module_group_order.index(x.label))

        idx = 0
        group_sizes = [1, 10, 16, 16]
        assert len(params_list) == sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            chunk_size = (size + self.world_size - 1) // self.world_size
            group_params = params_list[idx: idx + size]
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))
            idx += size

        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient distributed step by @YouJiacheng, @KonstantinWilleke, @alexrgilbert,
        # @adricarda, @tuttyfrutyee, @vdlad, @ryanyang0, @vagrawal, @varunneal, @chrisjmccormick
        rank = dist.get_rank()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            stacked_grads = torch.empty(
                (padded_num_params, *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device
            )
            for i, p in enumerate(params):
                stacked_grads[i].copy_(p.grad, non_blocking=True)
            if len(params) < padded_num_params:
                stacked_grads[len(params):].zero_()

            grad_chunk = torch.empty_like(stacked_grads[:chunk_size])

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(dict(grad_chunk=grad_chunk, reduce_future=reduce_future))

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = group["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            start_idx = rank * chunk_size
            module_idx = start_idx if start_idx < len(params) else 0

            num_params = min(chunk_size, max(0, len(params) - start_idx))  # num params for this rank

            if "momentum_buffer" not in group:
                group["momentum_buffer"]  = torch.zeros_like(grad_chunk[:num_params])
            momentum_buffer = group["momentum_buffer"]
            # Apply momentum update to the persistent momentum buffer in-place
            momentum_buffer.lerp_(grad_chunk[:num_params], 1 - group["momentum"])
            updated_grads = grad_chunk[:num_params].lerp_(momentum_buffer, group["momentum"])

            grad_shape = updated_grads.shape
            if params[module_idx].label == 'attn':

                for p in params[module_idx:module_idx + num_params]:
                    assert p.label == 'attn'

                updated_grads = updated_grads.view(4 * grad_shape[0], grad_shape[1] // 4, grad_shape[2])

            ref_param = params[module_idx]
            param_shape = ref_param.shape

            # The below shape-based heuristic assumes that matrices have their input along the
            # row dimension and their output along the columns. Gates are an exception.
            is_gate = ref_param.label in ['smear_gate', 'attn_gate']

            if "second_momentum_buffer" not in group:
                if is_gate:
                    group["second_momentum_buffer"] = torch.zeros_like(updated_grads[..., :, :1])
                else:
                    group["second_momentum_buffer"] = (torch.zeros_like(updated_grads[..., :, :1])
                        if param_shape[-2] >= param_shape[-1] else torch.zeros_like(updated_grads[..., :1, :])
                    )
            second_momentum_buffer = group["second_momentum_buffer"]

            if "param_lr_cpu" not in group:
                # Define multipliers for ALL params in this group (global, not per-shard)
                lr_mults = []
                wd_mults = []
                for p in params:
                    # Increase learning rate for modules with larger inputs than outputs.
                    # This shape check also assumes rows=input, columns=output, so take care
                    # when changing memory layouts. @chrisjmccormick
                    shape = p.shape
                    if len(shape) >= 2:
                        shape_mult = max(1.0, shape[-2] / shape[-1]) ** 0.5
                    else:
                        shape_mult = 1.0
                    lr_mults.append(shape_mult * getattr(p, "lr_mul", 1.0))
                    wd_mults.append(getattr(p, "wd_mul", 1.0))
                # Define as cpu tensors to enable Inductor constant folding
                group["param_lr_cpu"] = torch.tensor(lr_mults, dtype=torch.float32, device="cpu")
                group["param_wd_cpu"] = torch.tensor(wd_mults, dtype=torch.float32, device="cpu")

            eff_lr_all = group["param_lr_cpu"] * group["lr"]
            eff_wd_all = group["param_wd_cpu"] * group["weight_decay"] * group["lr"]

            # Slice the portion corresponding to this rank's shard
            eff_lr_cpu = eff_lr_all[module_idx:module_idx + num_params]
            eff_wd_cpu = eff_wd_all[module_idx:module_idx + num_params]

            # Compute zeropower for the entire chunk in a single, batched call.
            if num_params == 0:
                v_chunk = updated_grads
            else:
                v_chunk = polar_express(updated_grads, split_baddbmm=(ref_param.label == 'mlp'))

            # Note that the head orientation in O is transposed relative to QKV, so red_dim
            # is 'incorrect' for O. However, correcting this showed no improvement. @chrisjmccormick
            red_dim = -1 if (is_gate or param_shape[-2] >= param_shape[-1]) else -2

            v_chunk = apply_normuon_variance_reduction(
                v_chunk, second_momentum_buffer, group["beta2"], red_dim
            )

            v_chunk = v_chunk.view(grad_shape)

            # # "Cautious" weight decay (https://arxiv.org/abs/2510.12402)
            updated_params = torch.empty_like(grad_chunk)
            if num_params > 0:
                # Work on a stacked copy to avoid touching original params
                param_chunk = torch.stack(params[module_idx:module_idx + num_params])

                for local_idx in range(num_params):
                    cautious_wd_and_update_inplace(
                        param_chunk[local_idx],
                        v_chunk[local_idx],
                        eff_wd_cpu[local_idx],
                        eff_lr_cpu[local_idx],
                    )
            else:
                param_chunk = torch.zeros_like(v_chunk)

            updated_params[:num_params].copy_(param_chunk)
            if num_params < chunk_size:
                updated_params[num_params:].zero_()

            stacked_params = torch.empty(
                (padded_num_params, *param_shape),
                dtype=updated_params.dtype,
                device=updated_params.device,
            )

            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_params, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        # Group by label, with explicit ordering for execution control.
        label_order = ['lm_head', 'value_embed', 'scalars']
        params_by_label = defaultdict(list)
        for p in params:
            params_by_label[getattr(p, 'label', None)].append(p)
        param_groups = []
        for label in label_order:
            if label in params_by_label:
                param_groups.append(dict(params=params_by_label[label]))
        # include any unlabeled params at the end (processed last)
        if None in params_by_label:
            param_groups.append(dict(params=params_by_label[None]))
        super().__init__(param_groups, defaults)
        # init state
        for p in params:
            chunk_size = p.size(0) // self.world_size
            exp_avg = torch.zeros_like(p[:chunk_size], dtype=torch.bfloat16, device=p[0].device)
            exp_avg_sq = torch.zeros_like(exp_avg)
            self.state[p] = dict(step=0, exp_avg=exp_avg, exp_avg_sq=exp_avg_sq)
        # DistributedAdam implementation by @vagrawal, @akash5474

        self.should_sync = False

        self._reduce_scatter_hooks = []
        self._reduce_scatter_futures = {}
        self.register_backward_hooks()

    def register_backward_hooks(self):
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                hook = param.register_post_accumulate_grad_hook(self._sync_gradient)
                self._reduce_scatter_hooks.append(hook)

    @torch.no_grad()
    def _sync_gradient(self, param):
        if not self.should_sync:
            return

        grad = param.grad
        rank_size = grad.shape[0] // self.world_size
        grad_slice = torch.empty_like(grad[:rank_size])
        self._reduce_scatter_futures[param] = (
            dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future(),
            grad_slice
        )

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        all_gather_futures: list[torch.Future] = []

        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            for param in group['params']:
                if param not in self._reduce_scatter_futures:
                    continue

                fut, g_slice = self._reduce_scatter_futures[param]
                fut.wait()

                rank_size = param.shape[0] // self.world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]

                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (bias2 ** 0.5 / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                # cautious weight decay
                mask = (update * p_slice) >= 0
                # lr as weight decay schedule
                eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                update.addcmul_(p_slice, mask, value=eff_weight_decay * lr)

                p_slice.add_(other=update, alpha=-1.0)

                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())

        self._reduce_scatter_futures.clear()
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        with torch.no_grad():
            self.weight.zero_()  # @Grad62304977 and others

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))


# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float
    key_shift: bool

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKVO weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # Simplified layout by @chrisjmccormick
        self.qkvo_w = nn.Parameter(torch.empty(self.dim * 4, self.hdim))
        # label all modules for explicit optimizer grouping
        self.qkvo_w.label = 'attn'

        with torch.no_grad():
            self.qkvo_w[:self.dim * 3].uniform_(-bound, bound)  # init QKV weights
            self.qkvo_w[self.dim * 3:].zero_()  # init O weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        self.attn_gate.weight.label = 'attn_gate'

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas, key_shift = attn_args.ve, attn_args.sa_lambdas, attn_args.key_shift
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, sa_lambdas[0] * self.qkvo_w[:self.dim * 3].type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if key_shift:
            # shift keys forward for the stationary head dims. Enables 1-layer induction.
            k[:, 1:, :, self.head_dim//4:self.head_dim//2] = k[:, :-1, :, self.head_dim//4:self.head_dim//2]
            k[:, 1:, :, self.head_dim//4+self.head_dim//2:] = k[:, :-1, :, self.head_dim//4+self.head_dim//2:]
        if ve is not None:
            v = v + ve.view_as(v) # @ KoszarskyB & @Grad62304977

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens,
                                                        max_seqlen_q=max_len, max_seqlen_k=max_len,
                                                        causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, sa_lambdas[1] * self.qkvo_w[self.dim * 3:].type_as(y))  # sa_lambdas[1] pre-multiplied to O @shenberg
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # Transposed layout to match attention weights
        self.c_fc = nn.Parameter(torch.empty(hdim, dim))
        self.c_proj = nn.Parameter(torch.empty(hdim, dim))
        # label all modules for explicit optimizer grouping
        self.c_fc.label = 'mlp'
        self.c_proj.label = 'mlp'
        self.c_proj.lr_mul = 2.

        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.T.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.6 (the 7th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx != 6 else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim)

    def forward(self, x: Tensor, attn_args: AttnArgs):
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        self.num_layers = num_layers
        vocab_size = next_multiple_of_n(vocab_size, n=128)

        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.label = 'smear_gate'

        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        for embed in self.value_embeds:
            nn.init.zeros_(embed.weight)
        for ve in self.value_embeds:
            ve.weight.label = 'value_embed'
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)

        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=100/448, w_s=1.5/448, grad_s=0.75/448)
        nn.init.normal_(self.lm_head.weight, mean=0, std=0.005)
        self.lm_head.weight.label = 'lm_head'
        # Add learnable skip connection weights for decoder layers
        pad = (-num_layers * 4 - 3) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    1.1 * torch.ones(num_layers),  # resid lambdas. 1.1 init such that layer i weight is i^(num_layers-i).
                    0 * torch.ones(num_layers),  # x0 lambdas
                    *[torch.tensor([0.5, 1.0]) for _ in range(num_layers)],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    -1.5 * torch.ones(1),  # skip_lambda -> σ(-1.5) ≈ 0.18
                    torch.ones(pad),
                ]
            )
        )

        self.scalars.label = 'scalars'
        # set learning rates
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.scalars.lr_mul = 5.0
        self.scalars.wd_mul = 0.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        # set configs
        skip_connections = []
        skip_in = [3] # long attention window on layer 3
        skip_out = [6] # no attn op on layer 6
        x_backout = None
        backout_layer = 7

        # set lambdas
        resid_lambdas = self.scalars[: 1 * self.num_layers]
        x0_lambdas = self.scalars[1 * self.num_layers: 2 * self.num_layers]
        sa_lambdas = self.scalars[2 * self.num_layers: 4 * self.num_layers].view(-1, 2)
        smear_lambda = self.scalars[4 * self.num_layers]
        backout_lambda = self.scalars[4 * self.num_layers+1]
        skip_lambda = self.scalars[4 * self.num_layers+2]

        # set block masks and key shift
        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == self.num_layers
        key_shift = [b==long_bm for b in bm_sizes] # apply key shift to long windows

        # weight-tied: use lm_head.weight for embedding lookup
        x = F.embedding(input_seq, self.lm_head.weight)
        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [ve[1], ve[2]] + [None] * (self.num_layers - 5) + [ve[0], ve[1], ve[2]]
        assert len(ve) == self.num_layers

        # smear token embed forward 1 position @classiclarryd
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        for i in range(self.num_layers):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale,
                key_shift = key_shift[i]
            )
            if i in skip_out:
                gate = torch.sigmoid(skip_lambda)  # in (0, 1)
                x = x + gate * skip_connections.pop()
            if i == 0:
                x = (resid_lambdas[0] + x0_lambdas[0]) * x
            else:
                x = resid_lambdas[i] * x + x0_lambdas[i] * x0
            x = self.blocks[i](x, attn_args)
            if i in skip_in:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 7 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()

    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        # Cast to int32 on CPU before transfer to avoid dtype conversion during .to()
        _inputs = _inputs.to(dtype=torch.int32)
        _targets = _targets.to(dtype=torch.int64)
        _cum_lengths = _cum_lengths.to(dtype=torch.int32)

        new_params = yield (
            _inputs.to(device="cuda", non_blocking=True),
            _targets.to(device="cuda", non_blocking=True),
            _cum_lengths.to(device="cuda", non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * new_grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens // new_grad_accum_steps
            max_seq_len = new_max_seq_len


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # batch sizes
    train_bs_schedule: tuple = (8 * 2048 * 8, 16 * 2048 * 8, 24 * 2048 * 8)
    train_bs_extension: int = 24 * 2048 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 1995  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: float = 0.55  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_final: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=11,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=args.val_batch_size // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.weight.data = m.weight.data.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    embed_params + scalar_params + head_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.005,
)
optimizer2 = NorMuon(hidden_matrix_params + gate_params, lr=0.023, momentum=0.95, beta2=0.95, weight_decay=1.2)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: tied to batch size schedule, with cooldown at the end.
def get_lr(step: int):
    if step > args.num_scheduled_iterations:
        return 0.1
    lr_max = 1.0
    x = step / args.num_scheduled_iterations
    if x > 1/3:
       lr_max = 1.51  # (16/8)**0.6
    if x > 2/3:
        lr_max = 1.93  # (24/8)**0.6
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = lr_max * w + (1 - w) * 0.1
        return lr
    return lr_max

def get_ws(step: int):
    # set short window size to half of long window size
    # Higher ws on "extension" steps
    if step >= args.num_scheduled_iterations:
        return args.ws_final // 2, args.ws_final
    x = step / args.num_scheduled_iterations
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]


def get_bs(step: int):
    if step >= args.num_scheduled_iterations:
        return args.train_bs_extension
    x = step / args.num_scheduled_iterations
    bs_idx = int(len(args.train_bs_schedule) * x)
    return args.train_bs_schedule[bs_idx]


def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)
        # disable sync in the next training step for the adam optimizer
        optimizers[0].should_sync = False

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 10
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_bs_schedule[0], args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
ws_schedule = list(args.ws_schedule) + [args.ws_final]
bs_schedule = list(args.train_bs_schedule) + [args.train_bs_extension]
ws_long = ws_schedule[0]
model.train()
model.yarn.reset()
assert len(ws_schedule) == len(bs_schedule), "This warmup assumes len(ws_schedule) == len(bs_schedule)"

for idx in range(len(ws_schedule)):
    send_args = None
    if idx != 0:
        new_ws_long = ws_schedule[idx]
        model.yarn.apply(ws_long, new_ws_long)
        ws_long = new_ws_long
        send_args = (bs_schedule[idx], args.train_max_seq_len, grad_accum_steps)
    for step in range(warmup_steps):
        inputs, targets, cum_seqlens = train_loader.send(send_args)
        if step % 2 == 1:
            optimizers[0].should_sync = True
        model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
        if step % 2 == 0:
            optimizers[1].step()
            optimizers[1].zero_grad(set_to_none=True)
        else:
            for opt in optimizers:
                opt.step()
            model.zero_grad(set_to_none=True)
            optimizers[0].should_sync = False

model.zero_grad(set_to_none=True)
optimizers[0].should_sync = False
model.eval()

# warm up validation too
val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
val_loss = 0
with torch.no_grad():
    for step in range(val_steps):
        inputs, targets, cum_seqlens = next(val_loader)
        ws_idx = step % len(ws_schedule)
        if ws_idx==0:
            model.yarn.reset()
            ws_long = ws_schedule[0]
        else:
            new_ws_long = ws_schedule[ws_idx]
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
        val_loss += model(inputs, targets, cum_seqlens, ws_long // 2, ws_long)

del val_loader, val_loss
model.train()
model.yarn.reset() # rotary buffer is not stored in state_dict
optimizer2.reset() # muon momentum buffers not in state dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

step_batch_size = args.train_bs_schedule[0]
train_loader = distributed_data_generator(args.train_files, step_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)

gc.collect()

training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    new_step_batch_size = get_bs(step)
    send_args = (new_step_batch_size, args.train_max_seq_len, grad_accum_steps) if new_step_batch_size != step_batch_size else None
    step_batch_size = new_step_batch_size
    for idx in range(grad_accum_steps):
        # enable gradient sync for the DistAdam optimizer on the last iteration before we step it
        if idx == grad_accum_steps - 1 and step % 2 == 1:
            optimizers[0].should_sync = True

        inputs, targets, cum_seqlens = train_loader.send(send_args)
        (model(inputs, targets, cum_seqlens, ws_short, ws_long) / grad_accum_steps).backward()
    step_optimizers(step, optimizers, model)

    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.10.12 (main, Nov  4 2025, 08:48:33) [GCC 11.4.0]
Running PyTorch 2.10.0.dev20251204+cu126 compiled for CUDA 12.6
Running Triton version 3.6.0
Sat Dec 20 00:17:39 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:19:00.0 Off |                    0 |
| N/A   29C    P0            120W /  700W |    5858MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:3B:00.0 Off |                    0 |
| N/A   23C    P0            113W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:4C:00.0 Off |                    0 |
| N/A   27C    P0            119W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:5D:00.0 Off |                    0 |
| N/A   27C    P0            122W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:9B:00.0 Off |                    0 |
| N/A   29C    P0            122W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:BB:00.0 Off |                    0 |
| N/A   22C    P0            112W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:CB:00.0 Off |                    0 |
| N/A   28C    P0            121W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:DB:00.0 Off |                    0 |
| N/A   23C    P0            116W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A    301103      C   /home/ubuntu/.venv/bin/python3               1510MiB |
|    0   N/A  N/A    301104      C   /home/ubuntu/.venv/bin/python3                614MiB |
|    0   N/A  N/A    301105      C   /home/ubuntu/.venv/bin/python3                614MiB |
|    0   N/A  N/A    301106      C   /home/ubuntu/.venv/bin/python3                614MiB |
|    0   N/A  N/A    301107      C   /home/ubuntu/.venv/bin/python3                614MiB |
|    0   N/A  N/A    301108      C   /home/ubuntu/.venv/bin/python3                614MiB |
|    0   N/A  N/A    301109      C   /home/ubuntu/.venv/bin/python3                614MiB |
|    0   N/A  N/A    301110      C   /home/ubuntu/.venv/bin/python3                614MiB |
|    1   N/A  N/A    301104      C   /home/ubuntu/.venv/bin/python3               1510MiB |
|    2   N/A  N/A    301105      C   /home/ubuntu/.venv/bin/python3               1510MiB |
|    3   N/A  N/A    301106      C   /home/ubuntu/.venv/bin/python3               1510MiB |
|    4   N/A  N/A    301107      C   /home/ubuntu/.venv/bin/python3               1510MiB |
|    5   N/A  N/A    301108      C   /home/ubuntu/.venv/bin/python3               1510MiB |
|    6   N/A  N/A    301109      C   /home/ubuntu/.venv/bin/python3               1510MiB |
|    7   N/A  N/A    301110      C   /home/ubuntu/.venv/bin/python3               1510MiB |
+-----------------------------------------------------------------------------------------+

====================================================================================================
step:0/2035 val_loss:10.8369 train_time:0ms step_avg:0.03ms
step:1/2035 train_time:79ms step_avg:79.34ms
step:2/2035 train_time:102ms step_avg:51.03ms
step:3/2035 train_time:124ms step_avg:41.25ms
step:4/2035 train_time:156ms step_avg:39.10ms
step:5/2035 train_time:189ms step_avg:37.83ms
step:6/2035 train_time:408ms step_avg:67.93ms
step:7/2035 train_time:429ms step_avg:61.22ms
step:8/2035 train_time:462ms step_avg:57.73ms
step:9/2035 train_time:494ms step_avg:54.93ms
step:10/2035 train_time:527ms step_avg:52.74ms
step:11/2035 train_time:560ms step_avg:50.94ms
step:12/2035 train_time:594ms step_avg:49.46ms
step:13/2035 train_time:626ms step_avg:48.19ms
step:14/2035 train_time:660ms step_avg:47.11ms
step:15/2035 train_time:692ms step_avg:46.16ms
step:16/2035 train_time:726ms step_avg:45.36ms
step:17/2035 train_time:759ms step_avg:44.63ms
step:18/2035 train_time:792ms step_avg:43.99ms
step:19/2035 train_time:825ms step_avg:43.41ms
step:20/2035 train_time:858ms step_avg:42.90ms
step:21/2035 train_time:891ms step_avg:42.42ms
step:22/2035 train_time:924ms step_avg:42.00ms
step:23/2035 train_time:957ms step_avg:41.61ms
step:24/2035 train_time:990ms step_avg:41.26ms
step:25/2035 train_time:1023ms step_avg:40.92ms
step:26/2035 train_time:1056ms step_avg:40.62ms
step:27/2035 train_time:1089ms step_avg:40.33ms
step:28/2035 train_time:1122ms step_avg:40.08ms
step:29/2035 train_time:1155ms step_avg:39.83ms
step:30/2035 train_time:1188ms step_avg:39.60ms
step:31/2035 train_time:1221ms step_avg:39.38ms
step:32/2035 train_time:1254ms step_avg:39.19ms
step:33/2035 train_time:1287ms step_avg:39.00ms
step:34/2035 train_time:1320ms step_avg:38.82ms
step:35/2035 train_time:1354ms step_avg:38.69ms
step:36/2035 train_time:1387ms step_avg:38.53ms
step:37/2035 train_time:1421ms step_avg:38.41ms
step:38/2035 train_time:1454ms step_avg:38.27ms
step:39/2035 train_time:1488ms step_avg:38.15ms
step:40/2035 train_time:1521ms step_avg:38.02ms
step:41/2035 train_time:1554ms step_avg:37.90ms
step:42/2035 train_time:1587ms step_avg:37.79ms
step:43/2035 train_time:1620ms step_avg:37.68ms
step:44/2035 train_time:1654ms step_avg:37.58ms
step:45/2035 train_time:1687ms step_avg:37.49ms
step:46/2035 train_time:1720ms step_avg:37.39ms
step:47/2035 train_time:1753ms step_avg:37.30ms
step:48/2035 train_time:1786ms step_avg:37.21ms
step:49/2035 train_time:1819ms step_avg:37.13ms
step:50/2035 train_time:1853ms step_avg:37.05ms
step:51/2035 train_time:1886ms step_avg:36.98ms
step:52/2035 train_time:1919ms step_avg:36.90ms
step:53/2035 train_time:1952ms step_avg:36.83ms
step:54/2035 train_time:1985ms step_avg:36.76ms
step:55/2035 train_time:2018ms step_avg:36.69ms
step:56/2035 train_time:2051ms step_avg:36.63ms
step:57/2035 train_time:2084ms step_avg:36.56ms
step:58/2035 train_time:2117ms step_avg:36.51ms
step:59/2035 train_time:2150ms step_avg:36.45ms
step:60/2035 train_time:2183ms step_avg:36.39ms
step:61/2035 train_time:2217ms step_avg:36.34ms
step:62/2035 train_time:2250ms step_avg:36.28ms
step:63/2035 train_time:2283ms step_avg:36.24ms
step:64/2035 train_time:2316ms step_avg:36.19ms
step:65/2035 train_time:2349ms step_avg:36.14ms
step:66/2035 train_time:2382ms step_avg:36.09ms
step:67/2035 train_time:2416ms step_avg:36.06ms
step:68/2035 train_time:2449ms step_avg:36.01ms
step:69/2035 train_time:2482ms step_avg:35.97ms
step:70/2035 train_time:2515ms step_avg:35.93ms
step:71/2035 train_time:2548ms step_avg:35.89ms
step:72/2035 train_time:2581ms step_avg:35.85ms
step:73/2035 train_time:2614ms step_avg:35.81ms
step:74/2035 train_time:2647ms step_avg:35.77ms
step:75/2035 train_time:2681ms step_avg:35.74ms
step:76/2035 train_time:2714ms step_avg:35.71ms
step:77/2035 train_time:2747ms step_avg:35.67ms
step:78/2035 train_time:2780ms step_avg:35.64ms
step:79/2035 train_time:2813ms step_avg:35.60ms
step:80/2035 train_time:2846ms step_avg:35.57ms
step:81/2035 train_time:2879ms step_avg:35.54ms
step:82/2035 train_time:2912ms step_avg:35.51ms
step:83/2035 train_time:2945ms step_avg:35.48ms
step:84/2035 train_time:2978ms step_avg:35.45ms
step:85/2035 train_time:3011ms step_avg:35.42ms
step:86/2035 train_time:3044ms step_avg:35.39ms
step:87/2035 train_time:3077ms step_avg:35.37ms
step:88/2035 train_time:3110ms step_avg:35.34ms
step:89/2035 train_time:3143ms step_avg:35.31ms
step:90/2035 train_time:3175ms step_avg:35.28ms
step:91/2035 train_time:3209ms step_avg:35.26ms
step:92/2035 train_time:3242ms step_avg:35.24ms
step:93/2035 train_time:3275ms step_avg:35.21ms
step:94/2035 train_time:3308ms step_avg:35.19ms
step:95/2035 train_time:3341ms step_avg:35.16ms
step:96/2035 train_time:3374ms step_avg:35.14ms
step:97/2035 train_time:3407ms step_avg:35.12ms
step:98/2035 train_time:3440ms step_avg:35.10ms
step:99/2035 train_time:3473ms step_avg:35.08ms
step:100/2035 train_time:3506ms step_avg:35.06ms
step:101/2035 train_time:3539ms step_avg:35.04ms
step:102/2035 train_time:3572ms step_avg:35.02ms
step:103/2035 train_time:3605ms step_avg:35.00ms
step:104/2035 train_time:3638ms step_avg:34.98ms
step:105/2035 train_time:3671ms step_avg:34.96ms
step:106/2035 train_time:3704ms step_avg:34.94ms
step:107/2035 train_time:3737ms step_avg:34.93ms
step:108/2035 train_time:3770ms step_avg:34.91ms
step:109/2035 train_time:3803ms step_avg:34.89ms
step:110/2035 train_time:3836ms step_avg:34.88ms
step:111/2035 train_time:3869ms step_avg:34.86ms
step:112/2035 train_time:3902ms step_avg:34.84ms
step:113/2035 train_time:3935ms step_avg:34.82ms
step:114/2035 train_time:3968ms step_avg:34.81ms
step:115/2035 train_time:4001ms step_avg:34.79ms
step:116/2035 train_time:4034ms step_avg:34.78ms
step:117/2035 train_time:4067ms step_avg:34.76ms
step:118/2035 train_time:4100ms step_avg:34.75ms
step:119/2035 train_time:4133ms step_avg:34.73ms
step:120/2035 train_time:4166ms step_avg:34.72ms
step:121/2035 train_time:4199ms step_avg:34.71ms
step:122/2035 train_time:4232ms step_avg:34.69ms
step:123/2035 train_time:4265ms step_avg:34.68ms
step:124/2035 train_time:4298ms step_avg:34.66ms
step:125/2035 train_time:4331ms step_avg:34.65ms
step:126/2035 train_time:4364ms step_avg:34.64ms
step:127/2035 train_time:4397ms step_avg:34.62ms
step:128/2035 train_time:4430ms step_avg:34.61ms
step:129/2035 train_time:4463ms step_avg:34.60ms
step:130/2035 train_time:4496ms step_avg:34.59ms
step:131/2035 train_time:4529ms step_avg:34.57ms
step:132/2035 train_time:4562ms step_avg:34.56ms
step:133/2035 train_time:4595ms step_avg:34.55ms
step:134/2035 train_time:4628ms step_avg:34.54ms
step:135/2035 train_time:4661ms step_avg:34.53ms
step:136/2035 train_time:4694ms step_avg:34.52ms
step:137/2035 train_time:4727ms step_avg:34.50ms
step:138/2035 train_time:4760ms step_avg:34.49ms
step:139/2035 train_time:4793ms step_avg:34.48ms
step:140/2035 train_time:4826ms step_avg:34.47ms
step:141/2035 train_time:4859ms step_avg:34.46ms
step:142/2035 train_time:4892ms step_avg:34.45ms
step:143/2035 train_time:4925ms step_avg:34.44ms
step:144/2035 train_time:4958ms step_avg:34.43ms
step:145/2035 train_time:4991ms step_avg:34.42ms
step:146/2035 train_time:5024ms step_avg:34.41ms
step:147/2035 train_time:5057ms step_avg:34.40ms
step:148/2035 train_time:5090ms step_avg:34.39ms
step:149/2035 train_time:5123ms step_avg:34.38ms
step:150/2035 train_time:5156ms step_avg:34.37ms
step:151/2035 train_time:5189ms step_avg:34.36ms
step:152/2035 train_time:5222ms step_avg:34.35ms
step:153/2035 train_time:5255ms step_avg:34.35ms
step:154/2035 train_time:5288ms step_avg:34.34ms
step:155/2035 train_time:5321ms step_avg:34.33ms
step:156/2035 train_time:5354ms step_avg:34.32ms
step:157/2035 train_time:5387ms step_avg:34.31ms
step:158/2035 train_time:5419ms step_avg:34.30ms
step:159/2035 train_time:5452ms step_avg:34.29ms
step:160/2035 train_time:5485ms step_avg:34.28ms
step:161/2035 train_time:5518ms step_avg:34.27ms
step:162/2035 train_time:5551ms step_avg:34.27ms
step:163/2035 train_time:5584ms step_avg:34.26ms
step:164/2035 train_time:5617ms step_avg:34.25ms
step:165/2035 train_time:5650ms step_avg:34.24ms
step:166/2035 train_time:5683ms step_avg:34.23ms
step:167/2035 train_time:5716ms step_avg:34.23ms
step:168/2035 train_time:5749ms step_avg:34.22ms
step:169/2035 train_time:5782ms step_avg:34.21ms
step:170/2035 train_time:5815ms step_avg:34.20ms
step:171/2035 train_time:5848ms step_avg:34.20ms
step:172/2035 train_time:5881ms step_avg:34.19ms
step:173/2035 train_time:5914ms step_avg:34.18ms
step:174/2035 train_time:5947ms step_avg:34.18ms
step:175/2035 train_time:5980ms step_avg:34.17ms
step:176/2035 train_time:6013ms step_avg:34.16ms
step:177/2035 train_time:6046ms step_avg:34.16ms
step:178/2035 train_time:6079ms step_avg:34.15ms
step:179/2035 train_time:6112ms step_avg:34.15ms
step:180/2035 train_time:6145ms step_avg:34.14ms
step:181/2035 train_time:6178ms step_avg:34.13ms
step:182/2035 train_time:6211ms step_avg:34.13ms
step:183/2035 train_time:6244ms step_avg:34.12ms
step:184/2035 train_time:6277ms step_avg:34.11ms
step:185/2035 train_time:6310ms step_avg:34.11ms
step:186/2035 train_time:6343ms step_avg:34.10ms
step:187/2035 train_time:6376ms step_avg:34.09ms
step:188/2035 train_time:6409ms step_avg:34.09ms
step:189/2035 train_time:6442ms step_avg:34.08ms
step:190/2035 train_time:6475ms step_avg:34.08ms
step:191/2035 train_time:6508ms step_avg:34.07ms
step:192/2035 train_time:6540ms step_avg:34.06ms
step:193/2035 train_time:6573ms step_avg:34.06ms
step:194/2035 train_time:6606ms step_avg:34.05ms
step:195/2035 train_time:6639ms step_avg:34.05ms
step:196/2035 train_time:6672ms step_avg:34.04ms
step:197/2035 train_time:6705ms step_avg:34.03ms
step:198/2035 train_time:6738ms step_avg:34.03ms
step:199/2035 train_time:6770ms step_avg:34.02ms
step:200/2035 train_time:6803ms step_avg:34.02ms
step:201/2035 train_time:6836ms step_avg:34.01ms
step:202/2035 train_time:6869ms step_avg:34.01ms
step:203/2035 train_time:6902ms step_avg:34.00ms
step:204/2035 train_time:6935ms step_avg:33.99ms
step:205/2035 train_time:6968ms step_avg:33.99ms
step:206/2035 train_time:7001ms step_avg:33.98ms
step:207/2035 train_time:7034ms step_avg:33.98ms
step:208/2035 train_time:7067ms step_avg:33.98ms
step:209/2035 train_time:7100ms step_avg:33.97ms
step:210/2035 train_time:7133ms step_avg:33.97ms
step:211/2035 train_time:7166ms step_avg:33.96ms
step:212/2035 train_time:7199ms step_avg:33.96ms
step:213/2035 train_time:7232ms step_avg:33.95ms
step:214/2035 train_time:7265ms step_avg:33.95ms
step:215/2035 train_time:7298ms step_avg:33.94ms
step:216/2035 train_time:7331ms step_avg:33.94ms
step:217/2035 train_time:7364ms step_avg:33.94ms
step:218/2035 train_time:7397ms step_avg:33.93ms
step:219/2035 train_time:7430ms step_avg:33.93ms
step:220/2035 train_time:7463ms step_avg:33.92ms
step:221/2035 train_time:7496ms step_avg:33.92ms
step:222/2035 train_time:7529ms step_avg:33.91ms
step:223/2035 train_time:7562ms step_avg:33.91ms
step:224/2035 train_time:7594ms step_avg:33.90ms
step:225/2035 train_time:7628ms step_avg:33.90ms
step:226/2035 train_time:7661ms step_avg:33.90ms
step:227/2035 train_time:7693ms step_avg:33.89ms
step:228/2035 train_time:7726ms step_avg:33.89ms
step:229/2035 train_time:7759ms step_avg:33.88ms
step:230/2035 train_time:7792ms step_avg:33.88ms
step:231/2035 train_time:7825ms step_avg:33.88ms
step:232/2035 train_time:7858ms step_avg:33.87ms
step:233/2035 train_time:7891ms step_avg:33.87ms
step:234/2035 train_time:7924ms step_avg:33.86ms
step:235/2035 train_time:7957ms step_avg:33.86ms
step:236/2035 train_time:7990ms step_avg:33.85ms
step:237/2035 train_time:8022ms step_avg:33.85ms
step:238/2035 train_time:8056ms step_avg:33.85ms
step:239/2035 train_time:8088ms step_avg:33.84ms
step:240/2035 train_time:8121ms step_avg:33.84ms
step:241/2035 train_time:8154ms step_avg:33.83ms
step:242/2035 train_time:8187ms step_avg:33.83ms
step:243/2035 train_time:8220ms step_avg:33.83ms
step:244/2035 train_time:8253ms step_avg:33.82ms
step:245/2035 train_time:8286ms step_avg:33.82ms
step:246/2035 train_time:8319ms step_avg:33.82ms
step:247/2035 train_time:8351ms step_avg:33.81ms
step:248/2035 train_time:8384ms step_avg:33.81ms
step:249/2035 train_time:8417ms step_avg:33.80ms
step:250/2035 train_time:8450ms step_avg:33.80ms
step:250/2035 val_loss:4.2657 train_time:8486ms step_avg:33.94ms
step:251/2035 train_time:8506ms step_avg:33.89ms
step:252/2035 train_time:8525ms step_avg:33.83ms
step:253/2035 train_time:8553ms step_avg:33.81ms
step:254/2035 train_time:8586ms step_avg:33.80ms
step:255/2035 train_time:8620ms step_avg:33.80ms
step:256/2035 train_time:8654ms step_avg:33.80ms
step:257/2035 train_time:8688ms step_avg:33.81ms
step:258/2035 train_time:8721ms step_avg:33.80ms
step:259/2035 train_time:8755ms step_avg:33.80ms
step:260/2035 train_time:8788ms step_avg:33.80ms
step:261/2035 train_time:8821ms step_avg:33.80ms
step:262/2035 train_time:8854ms step_avg:33.79ms
step:263/2035 train_time:8887ms step_avg:33.79ms
step:264/2035 train_time:8920ms step_avg:33.79ms
step:265/2035 train_time:8953ms step_avg:33.78ms
step:266/2035 train_time:8986ms step_avg:33.78ms
step:267/2035 train_time:9018ms step_avg:33.78ms
step:268/2035 train_time:9052ms step_avg:33.77ms
step:269/2035 train_time:9084ms step_avg:33.77ms
step:270/2035 train_time:9117ms step_avg:33.77ms
step:271/2035 train_time:9150ms step_avg:33.76ms
step:272/2035 train_time:9183ms step_avg:33.76ms
step:273/2035 train_time:9215ms step_avg:33.75ms
step:274/2035 train_time:9248ms step_avg:33.75ms
step:275/2035 train_time:9281ms step_avg:33.75ms
step:276/2035 train_time:9313ms step_avg:33.74ms
step:277/2035 train_time:9346ms step_avg:33.74ms
step:278/2035 train_time:9379ms step_avg:33.74ms
step:279/2035 train_time:9412ms step_avg:33.73ms
step:280/2035 train_time:9445ms step_avg:33.73ms
step:281/2035 train_time:9477ms step_avg:33.73ms
step:282/2035 train_time:9510ms step_avg:33.72ms
step:283/2035 train_time:9544ms step_avg:33.72ms
step:284/2035 train_time:9577ms step_avg:33.72ms
step:285/2035 train_time:9610ms step_avg:33.72ms
step:286/2035 train_time:9643ms step_avg:33.72ms
step:287/2035 train_time:9677ms step_avg:33.72ms
step:288/2035 train_time:9710ms step_avg:33.71ms
step:289/2035 train_time:9743ms step_avg:33.71ms
step:290/2035 train_time:9776ms step_avg:33.71ms
step:291/2035 train_time:9809ms step_avg:33.71ms
step:292/2035 train_time:9842ms step_avg:33.70ms
step:293/2035 train_time:9875ms step_avg:33.70ms
step:294/2035 train_time:9908ms step_avg:33.70ms
step:295/2035 train_time:9941ms step_avg:33.70ms
step:296/2035 train_time:9974ms step_avg:33.70ms
step:297/2035 train_time:10008ms step_avg:33.70ms
step:298/2035 train_time:10041ms step_avg:33.69ms
step:299/2035 train_time:10073ms step_avg:33.69ms
step:300/2035 train_time:10106ms step_avg:33.69ms
step:301/2035 train_time:10139ms step_avg:33.68ms
step:302/2035 train_time:10172ms step_avg:33.68ms
step:303/2035 train_time:10205ms step_avg:33.68ms
step:304/2035 train_time:10237ms step_avg:33.68ms
step:305/2035 train_time:10271ms step_avg:33.67ms
step:306/2035 train_time:10304ms step_avg:33.67ms
step:307/2035 train_time:10336ms step_avg:33.67ms
step:308/2035 train_time:10369ms step_avg:33.67ms
step:309/2035 train_time:10402ms step_avg:33.66ms
step:310/2035 train_time:10435ms step_avg:33.66ms
step:311/2035 train_time:10468ms step_avg:33.66ms
step:312/2035 train_time:10500ms step_avg:33.66ms
step:313/2035 train_time:10534ms step_avg:33.65ms
step:314/2035 train_time:10566ms step_avg:33.65ms
step:315/2035 train_time:10600ms step_avg:33.65ms
step:316/2035 train_time:10633ms step_avg:33.65ms
step:317/2035 train_time:10666ms step_avg:33.65ms
step:318/2035 train_time:10699ms step_avg:33.64ms
step:319/2035 train_time:10732ms step_avg:33.64ms
step:320/2035 train_time:10765ms step_avg:33.64ms
step:321/2035 train_time:10798ms step_avg:33.64ms
step:322/2035 train_time:10831ms step_avg:33.64ms
step:323/2035 train_time:10864ms step_avg:33.63ms
step:324/2035 train_time:10897ms step_avg:33.63ms
step:325/2035 train_time:10930ms step_avg:33.63ms
step:326/2035 train_time:10963ms step_avg:33.63ms
step:327/2035 train_time:10996ms step_avg:33.63ms
step:328/2035 train_time:11028ms step_avg:33.62ms
step:329/2035 train_time:11062ms step_avg:33.62ms
step:330/2035 train_time:11094ms step_avg:33.62ms
step:331/2035 train_time:11128ms step_avg:33.62ms
step:332/2035 train_time:11161ms step_avg:33.62ms
step:333/2035 train_time:11194ms step_avg:33.61ms
step:334/2035 train_time:11227ms step_avg:33.61ms
step:335/2035 train_time:11259ms step_avg:33.61ms
step:336/2035 train_time:11292ms step_avg:33.61ms
step:337/2035 train_time:11325ms step_avg:33.61ms
step:338/2035 train_time:11358ms step_avg:33.60ms
step:339/2035 train_time:11391ms step_avg:33.60ms
step:340/2035 train_time:11424ms step_avg:33.60ms
step:341/2035 train_time:11457ms step_avg:33.60ms
step:342/2035 train_time:11490ms step_avg:33.60ms
step:343/2035 train_time:11523ms step_avg:33.59ms
step:344/2035 train_time:11556ms step_avg:33.59ms
step:345/2035 train_time:11588ms step_avg:33.59ms
step:346/2035 train_time:11621ms step_avg:33.59ms
step:347/2035 train_time:11654ms step_avg:33.59ms
step:348/2035 train_time:11687ms step_avg:33.58ms
step:349/2035 train_time:11720ms step_avg:33.58ms
step:350/2035 train_time:11753ms step_avg:33.58ms
step:351/2035 train_time:11786ms step_avg:33.58ms
step:352/2035 train_time:11819ms step_avg:33.58ms
step:353/2035 train_time:11852ms step_avg:33.57ms
step:354/2035 train_time:11885ms step_avg:33.57ms
step:355/2035 train_time:11918ms step_avg:33.57ms
step:356/2035 train_time:11951ms step_avg:33.57ms
step:357/2035 train_time:11984ms step_avg:33.57ms
step:358/2035 train_time:12017ms step_avg:33.57ms
step:359/2035 train_time:12050ms step_avg:33.57ms
step:360/2035 train_time:12083ms step_avg:33.56ms
step:361/2035 train_time:12116ms step_avg:33.56ms
step:362/2035 train_time:12149ms step_avg:33.56ms
step:363/2035 train_time:12182ms step_avg:33.56ms
step:364/2035 train_time:12214ms step_avg:33.56ms
step:365/2035 train_time:12247ms step_avg:33.55ms
step:366/2035 train_time:12280ms step_avg:33.55ms
step:367/2035 train_time:12313ms step_avg:33.55ms
step:368/2035 train_time:12346ms step_avg:33.55ms
step:369/2035 train_time:12379ms step_avg:33.55ms
step:370/2035 train_time:12412ms step_avg:33.54ms
step:371/2035 train_time:12444ms step_avg:33.54ms
step:372/2035 train_time:12477ms step_avg:33.54ms
step:373/2035 train_time:12510ms step_avg:33.54ms
step:374/2035 train_time:12544ms step_avg:33.54ms
step:375/2035 train_time:12577ms step_avg:33.54ms
step:376/2035 train_time:12609ms step_avg:33.54ms
step:377/2035 train_time:12642ms step_avg:33.53ms
step:378/2035 train_time:12675ms step_avg:33.53ms
step:379/2035 train_time:12708ms step_avg:33.53ms
step:380/2035 train_time:12741ms step_avg:33.53ms
step:381/2035 train_time:12774ms step_avg:33.53ms
step:382/2035 train_time:12807ms step_avg:33.53ms
step:383/2035 train_time:12840ms step_avg:33.53ms
step:384/2035 train_time:12873ms step_avg:33.52ms
step:385/2035 train_time:12906ms step_avg:33.52ms
step:386/2035 train_time:12939ms step_avg:33.52ms
step:387/2035 train_time:12972ms step_avg:33.52ms
step:388/2035 train_time:13005ms step_avg:33.52ms
step:389/2035 train_time:13038ms step_avg:33.52ms
step:390/2035 train_time:13071ms step_avg:33.51ms
step:391/2035 train_time:13104ms step_avg:33.51ms
step:392/2035 train_time:13136ms step_avg:33.51ms
step:393/2035 train_time:13170ms step_avg:33.51ms
step:394/2035 train_time:13203ms step_avg:33.51ms
step:395/2035 train_time:13236ms step_avg:33.51ms
step:396/2035 train_time:13268ms step_avg:33.51ms
step:397/2035 train_time:13302ms step_avg:33.51ms
step:398/2035 train_time:13334ms step_avg:33.50ms
step:399/2035 train_time:13368ms step_avg:33.50ms
step:400/2035 train_time:13401ms step_avg:33.50ms
step:401/2035 train_time:13433ms step_avg:33.50ms
step:402/2035 train_time:13466ms step_avg:33.50ms
step:403/2035 train_time:13499ms step_avg:33.50ms
step:404/2035 train_time:13532ms step_avg:33.50ms
step:405/2035 train_time:13565ms step_avg:33.49ms
step:406/2035 train_time:13598ms step_avg:33.49ms
step:407/2035 train_time:13630ms step_avg:33.49ms
step:408/2035 train_time:13663ms step_avg:33.49ms
step:409/2035 train_time:13696ms step_avg:33.49ms
step:410/2035 train_time:13729ms step_avg:33.49ms
step:411/2035 train_time:13762ms step_avg:33.48ms
step:412/2035 train_time:13795ms step_avg:33.48ms
step:413/2035 train_time:13828ms step_avg:33.48ms
step:414/2035 train_time:13861ms step_avg:33.48ms
step:415/2035 train_time:13894ms step_avg:33.48ms
step:416/2035 train_time:13927ms step_avg:33.48ms
step:417/2035 train_time:13960ms step_avg:33.48ms
step:418/2035 train_time:13993ms step_avg:33.48ms
step:419/2035 train_time:14026ms step_avg:33.48ms
step:420/2035 train_time:14059ms step_avg:33.47ms
step:421/2035 train_time:14092ms step_avg:33.47ms
step:422/2035 train_time:14125ms step_avg:33.47ms
step:423/2035 train_time:14158ms step_avg:33.47ms
step:424/2035 train_time:14191ms step_avg:33.47ms
step:425/2035 train_time:14224ms step_avg:33.47ms
step:426/2035 train_time:14256ms step_avg:33.47ms
step:427/2035 train_time:14289ms step_avg:33.46ms
step:428/2035 train_time:14322ms step_avg:33.46ms
step:429/2035 train_time:14355ms step_avg:33.46ms
step:430/2035 train_time:14388ms step_avg:33.46ms
step:431/2035 train_time:14421ms step_avg:33.46ms
step:432/2035 train_time:14454ms step_avg:33.46ms
step:433/2035 train_time:14487ms step_avg:33.46ms
step:434/2035 train_time:14519ms step_avg:33.45ms
step:435/2035 train_time:14553ms step_avg:33.46ms
step:436/2035 train_time:14586ms step_avg:33.45ms
step:437/2035 train_time:14619ms step_avg:33.45ms
step:438/2035 train_time:14651ms step_avg:33.45ms
step:439/2035 train_time:14684ms step_avg:33.45ms
step:440/2035 train_time:14717ms step_avg:33.45ms
step:441/2035 train_time:14750ms step_avg:33.45ms
step:442/2035 train_time:14783ms step_avg:33.45ms
step:443/2035 train_time:14816ms step_avg:33.44ms
step:444/2035 train_time:14849ms step_avg:33.44ms
step:445/2035 train_time:14882ms step_avg:33.44ms
step:446/2035 train_time:14915ms step_avg:33.44ms
step:447/2035 train_time:14948ms step_avg:33.44ms
step:448/2035 train_time:14981ms step_avg:33.44ms
step:449/2035 train_time:15014ms step_avg:33.44ms
step:450/2035 train_time:15047ms step_avg:33.44ms
step:451/2035 train_time:15080ms step_avg:33.44ms
step:452/2035 train_time:15113ms step_avg:33.44ms
step:453/2035 train_time:15146ms step_avg:33.43ms
step:454/2035 train_time:15179ms step_avg:33.43ms
step:455/2035 train_time:15212ms step_avg:33.43ms
step:456/2035 train_time:15244ms step_avg:33.43ms
step:457/2035 train_time:15277ms step_avg:33.43ms
step:458/2035 train_time:15310ms step_avg:33.43ms
step:459/2035 train_time:15343ms step_avg:33.43ms
step:460/2035 train_time:15376ms step_avg:33.43ms
step:461/2035 train_time:15409ms step_avg:33.43ms
step:462/2035 train_time:15442ms step_avg:33.42ms
step:463/2035 train_time:15475ms step_avg:33.42ms
step:464/2035 train_time:15508ms step_avg:33.42ms
step:465/2035 train_time:15541ms step_avg:33.42ms
step:466/2035 train_time:15574ms step_avg:33.42ms
step:467/2035 train_time:15607ms step_avg:33.42ms
step:468/2035 train_time:15640ms step_avg:33.42ms
step:469/2035 train_time:15673ms step_avg:33.42ms
step:470/2035 train_time:15705ms step_avg:33.42ms
step:471/2035 train_time:15738ms step_avg:33.41ms
step:472/2035 train_time:15771ms step_avg:33.41ms
step:473/2035 train_time:15804ms step_avg:33.41ms
step:474/2035 train_time:15837ms step_avg:33.41ms
step:475/2035 train_time:15870ms step_avg:33.41ms
step:476/2035 train_time:15903ms step_avg:33.41ms
step:477/2035 train_time:15936ms step_avg:33.41ms
step:478/2035 train_time:15969ms step_avg:33.41ms
step:479/2035 train_time:16002ms step_avg:33.41ms
step:480/2035 train_time:16034ms step_avg:33.41ms
step:481/2035 train_time:16067ms step_avg:33.40ms
step:482/2035 train_time:16100ms step_avg:33.40ms
step:483/2035 train_time:16133ms step_avg:33.40ms
step:484/2035 train_time:16166ms step_avg:33.40ms
step:485/2035 train_time:16199ms step_avg:33.40ms
step:486/2035 train_time:16232ms step_avg:33.40ms
step:487/2035 train_time:16265ms step_avg:33.40ms
step:488/2035 train_time:16298ms step_avg:33.40ms
step:489/2035 train_time:16331ms step_avg:33.40ms
step:490/2035 train_time:16364ms step_avg:33.40ms
step:491/2035 train_time:16397ms step_avg:33.39ms
step:492/2035 train_time:16430ms step_avg:33.39ms
step:493/2035 train_time:16462ms step_avg:33.39ms
step:494/2035 train_time:16495ms step_avg:33.39ms
step:495/2035 train_time:16528ms step_avg:33.39ms
step:496/2035 train_time:16561ms step_avg:33.39ms
step:497/2035 train_time:16594ms step_avg:33.39ms
step:498/2035 train_time:16627ms step_avg:33.39ms
step:499/2035 train_time:16660ms step_avg:33.39ms
step:500/2035 train_time:16693ms step_avg:33.39ms
step:500/2035 val_loss:4.0017 train_time:16728ms step_avg:33.46ms
step:501/2035 train_time:16749ms step_avg:33.43ms
step:502/2035 train_time:16769ms step_avg:33.40ms
step:503/2035 train_time:16797ms step_avg:33.39ms
step:504/2035 train_time:16830ms step_avg:33.39ms
step:505/2035 train_time:16866ms step_avg:33.40ms
step:506/2035 train_time:16899ms step_avg:33.40ms
step:507/2035 train_time:16933ms step_avg:33.40ms
step:508/2035 train_time:16966ms step_avg:33.40ms
step:509/2035 train_time:17000ms step_avg:33.40ms
step:510/2035 train_time:17033ms step_avg:33.40ms
step:511/2035 train_time:17066ms step_avg:33.40ms
step:512/2035 train_time:17099ms step_avg:33.40ms
step:513/2035 train_time:17131ms step_avg:33.39ms
step:514/2035 train_time:17164ms step_avg:33.39ms
step:515/2035 train_time:17197ms step_avg:33.39ms
step:516/2035 train_time:17230ms step_avg:33.39ms
step:517/2035 train_time:17263ms step_avg:33.39ms
step:518/2035 train_time:17296ms step_avg:33.39ms
step:519/2035 train_time:17328ms step_avg:33.39ms
step:520/2035 train_time:17361ms step_avg:33.39ms
step:521/2035 train_time:17394ms step_avg:33.39ms
step:522/2035 train_time:17427ms step_avg:33.38ms
step:523/2035 train_time:17460ms step_avg:33.38ms
step:524/2035 train_time:17493ms step_avg:33.38ms
step:525/2035 train_time:17525ms step_avg:33.38ms
step:526/2035 train_time:17558ms step_avg:33.38ms
step:527/2035 train_time:17591ms step_avg:33.38ms
step:528/2035 train_time:17623ms step_avg:33.38ms
step:529/2035 train_time:17656ms step_avg:33.38ms
step:530/2035 train_time:17689ms step_avg:33.38ms
step:531/2035 train_time:17722ms step_avg:33.37ms
step:532/2035 train_time:17755ms step_avg:33.37ms
step:533/2035 train_time:17788ms step_avg:33.37ms
step:534/2035 train_time:17821ms step_avg:33.37ms
step:535/2035 train_time:17854ms step_avg:33.37ms
step:536/2035 train_time:17887ms step_avg:33.37ms
step:537/2035 train_time:17920ms step_avg:33.37ms
step:538/2035 train_time:17954ms step_avg:33.37ms
step:539/2035 train_time:17987ms step_avg:33.37ms
step:540/2035 train_time:18020ms step_avg:33.37ms
step:541/2035 train_time:18053ms step_avg:33.37ms
step:542/2035 train_time:18086ms step_avg:33.37ms
step:543/2035 train_time:18119ms step_avg:33.37ms
step:544/2035 train_time:18152ms step_avg:33.37ms
step:545/2035 train_time:18185ms step_avg:33.37ms
step:546/2035 train_time:18218ms step_avg:33.37ms
step:547/2035 train_time:18250ms step_avg:33.36ms
step:548/2035 train_time:18284ms step_avg:33.36ms
step:549/2035 train_time:18316ms step_avg:33.36ms
step:550/2035 train_time:18349ms step_avg:33.36ms
step:551/2035 train_time:18382ms step_avg:33.36ms
step:552/2035 train_time:18415ms step_avg:33.36ms
step:553/2035 train_time:18448ms step_avg:33.36ms
step:554/2035 train_time:18481ms step_avg:33.36ms
step:555/2035 train_time:18514ms step_avg:33.36ms
step:556/2035 train_time:18547ms step_avg:33.36ms
step:557/2035 train_time:18579ms step_avg:33.36ms
step:558/2035 train_time:18612ms step_avg:33.35ms
step:559/2035 train_time:18645ms step_avg:33.35ms
step:560/2035 train_time:18678ms step_avg:33.35ms
step:561/2035 train_time:18710ms step_avg:33.35ms
step:562/2035 train_time:18743ms step_avg:33.35ms
step:563/2035 train_time:18776ms step_avg:33.35ms
step:564/2035 train_time:18809ms step_avg:33.35ms
step:565/2035 train_time:18842ms step_avg:33.35ms
step:566/2035 train_time:18875ms step_avg:33.35ms
step:567/2035 train_time:18908ms step_avg:33.35ms
step:568/2035 train_time:18941ms step_avg:33.35ms
step:569/2035 train_time:18974ms step_avg:33.35ms
step:570/2035 train_time:19007ms step_avg:33.35ms
step:571/2035 train_time:19040ms step_avg:33.35ms
step:572/2035 train_time:19073ms step_avg:33.34ms
step:573/2035 train_time:19106ms step_avg:33.34ms
step:574/2035 train_time:19139ms step_avg:33.34ms
step:575/2035 train_time:19172ms step_avg:33.34ms
step:576/2035 train_time:19205ms step_avg:33.34ms
step:577/2035 train_time:19240ms step_avg:33.34ms
step:578/2035 train_time:19271ms step_avg:33.34ms
step:579/2035 train_time:19304ms step_avg:33.34ms
step:580/2035 train_time:19337ms step_avg:33.34ms
step:581/2035 train_time:19370ms step_avg:33.34ms
step:582/2035 train_time:19402ms step_avg:33.34ms
step:583/2035 train_time:19436ms step_avg:33.34ms
step:584/2035 train_time:19469ms step_avg:33.34ms
step:585/2035 train_time:19501ms step_avg:33.34ms
step:586/2035 train_time:19534ms step_avg:33.33ms
step:587/2035 train_time:19567ms step_avg:33.33ms
step:588/2035 train_time:19600ms step_avg:33.33ms
step:589/2035 train_time:19633ms step_avg:33.33ms
step:590/2035 train_time:19666ms step_avg:33.33ms
step:591/2035 train_time:19699ms step_avg:33.33ms
step:592/2035 train_time:19732ms step_avg:33.33ms
step:593/2035 train_time:19764ms step_avg:33.33ms
step:594/2035 train_time:19797ms step_avg:33.33ms
step:595/2035 train_time:19830ms step_avg:33.33ms
step:596/2035 train_time:19863ms step_avg:33.33ms
step:597/2035 train_time:19896ms step_avg:33.33ms
step:598/2035 train_time:19929ms step_avg:33.33ms
step:599/2035 train_time:19962ms step_avg:33.33ms
step:600/2035 train_time:19995ms step_avg:33.33ms
step:601/2035 train_time:20028ms step_avg:33.33ms
step:602/2035 train_time:20061ms step_avg:33.32ms
step:603/2035 train_time:20095ms step_avg:33.32ms
step:604/2035 train_time:20127ms step_avg:33.32ms
step:605/2035 train_time:20160ms step_avg:33.32ms
step:606/2035 train_time:20193ms step_avg:33.32ms
step:607/2035 train_time:20226ms step_avg:33.32ms
step:608/2035 train_time:20259ms step_avg:33.32ms
step:609/2035 train_time:20292ms step_avg:33.32ms
step:610/2035 train_time:20325ms step_avg:33.32ms
step:611/2035 train_time:20358ms step_avg:33.32ms
step:612/2035 train_time:20391ms step_avg:33.32ms
step:613/2035 train_time:20423ms step_avg:33.32ms
step:614/2035 train_time:20456ms step_avg:33.32ms
step:615/2035 train_time:20489ms step_avg:33.32ms
step:616/2035 train_time:20522ms step_avg:33.32ms
step:617/2035 train_time:20556ms step_avg:33.32ms
step:618/2035 train_time:20589ms step_avg:33.31ms
step:619/2035 train_time:20621ms step_avg:33.31ms
step:620/2035 train_time:20655ms step_avg:33.31ms
step:621/2035 train_time:20687ms step_avg:33.31ms
step:622/2035 train_time:20720ms step_avg:33.31ms
step:623/2035 train_time:20753ms step_avg:33.31ms
step:624/2035 train_time:20786ms step_avg:33.31ms
step:625/2035 train_time:20819ms step_avg:33.31ms
step:626/2035 train_time:20852ms step_avg:33.31ms
step:627/2035 train_time:20885ms step_avg:33.31ms
step:628/2035 train_time:20918ms step_avg:33.31ms
step:629/2035 train_time:20951ms step_avg:33.31ms
step:630/2035 train_time:20983ms step_avg:33.31ms
step:631/2035 train_time:21017ms step_avg:33.31ms
step:632/2035 train_time:21049ms step_avg:33.31ms
step:633/2035 train_time:21082ms step_avg:33.31ms
step:634/2035 train_time:21115ms step_avg:33.30ms
step:635/2035 train_time:21149ms step_avg:33.30ms
step:636/2035 train_time:21181ms step_avg:33.30ms
step:637/2035 train_time:21215ms step_avg:33.30ms
step:638/2035 train_time:21247ms step_avg:33.30ms
step:639/2035 train_time:21280ms step_avg:33.30ms
step:640/2035 train_time:21313ms step_avg:33.30ms
step:641/2035 train_time:21346ms step_avg:33.30ms
step:642/2035 train_time:21379ms step_avg:33.30ms
step:643/2035 train_time:21412ms step_avg:33.30ms
step:644/2035 train_time:21445ms step_avg:33.30ms
step:645/2035 train_time:21478ms step_avg:33.30ms
step:646/2035 train_time:21512ms step_avg:33.30ms
step:647/2035 train_time:21545ms step_avg:33.30ms
step:648/2035 train_time:21578ms step_avg:33.30ms
step:649/2035 train_time:21610ms step_avg:33.30ms
step:650/2035 train_time:21643ms step_avg:33.30ms
step:651/2035 train_time:21676ms step_avg:33.30ms
step:652/2035 train_time:21709ms step_avg:33.30ms
step:653/2035 train_time:21742ms step_avg:33.30ms
step:654/2035 train_time:21775ms step_avg:33.29ms
step:655/2035 train_time:21808ms step_avg:33.29ms
step:656/2035 train_time:21840ms step_avg:33.29ms
step:657/2035 train_time:21874ms step_avg:33.29ms
step:658/2035 train_time:21907ms step_avg:33.29ms
step:659/2035 train_time:21939ms step_avg:33.29ms
step:660/2035 train_time:21972ms step_avg:33.29ms
step:661/2035 train_time:22005ms step_avg:33.29ms
step:662/2035 train_time:22038ms step_avg:33.29ms
step:663/2035 train_time:22071ms step_avg:33.29ms
step:664/2035 train_time:22104ms step_avg:33.29ms
step:665/2035 train_time:22137ms step_avg:33.29ms
step:666/2035 train_time:22171ms step_avg:33.29ms
step:667/2035 train_time:22229ms step_avg:33.33ms
step:668/2035 train_time:22288ms step_avg:33.37ms
step:669/2035 train_time:22350ms step_avg:33.41ms
step:670/2035 train_time:22409ms step_avg:33.45ms
step:671/2035 train_time:22470ms step_avg:33.49ms
step:672/2035 train_time:22529ms step_avg:33.53ms
step:673/2035 train_time:22590ms step_avg:33.57ms
step:674/2035 train_time:22650ms step_avg:33.60ms
step:675/2035 train_time:22710ms step_avg:33.64ms
step:676/2035 train_time:22769ms step_avg:33.68ms
step:677/2035 train_time:22830ms step_avg:33.72ms
step:678/2035 train_time:22890ms step_avg:33.76ms
step:679/2035 train_time:22950ms step_avg:33.80ms
step:680/2035 train_time:23009ms step_avg:33.84ms
step:681/2035 train_time:23069ms step_avg:33.88ms
step:682/2035 train_time:23128ms step_avg:33.91ms
step:683/2035 train_time:23188ms step_avg:33.95ms
step:684/2035 train_time:23247ms step_avg:33.99ms
step:685/2035 train_time:23307ms step_avg:34.03ms
step:686/2035 train_time:23367ms step_avg:34.06ms
step:687/2035 train_time:23428ms step_avg:34.10ms
step:688/2035 train_time:23487ms step_avg:34.14ms
step:689/2035 train_time:23547ms step_avg:34.18ms
step:690/2035 train_time:23606ms step_avg:34.21ms
step:691/2035 train_time:23666ms step_avg:34.25ms
step:692/2035 train_time:23726ms step_avg:34.29ms
step:693/2035 train_time:23786ms step_avg:34.32ms
step:694/2035 train_time:23845ms step_avg:34.36ms
step:695/2035 train_time:23906ms step_avg:34.40ms
step:696/2035 train_time:23965ms step_avg:34.43ms
step:697/2035 train_time:24025ms step_avg:34.47ms
step:698/2035 train_time:24085ms step_avg:34.51ms
step:699/2035 train_time:24145ms step_avg:34.54ms
step:700/2035 train_time:24205ms step_avg:34.58ms
step:701/2035 train_time:24265ms step_avg:34.61ms
step:702/2035 train_time:24324ms step_avg:34.65ms
step:703/2035 train_time:24385ms step_avg:34.69ms
step:704/2035 train_time:24445ms step_avg:34.72ms
step:705/2035 train_time:24506ms step_avg:34.76ms
step:706/2035 train_time:24566ms step_avg:34.80ms
step:707/2035 train_time:24626ms step_avg:34.83ms
step:708/2035 train_time:24686ms step_avg:34.87ms
step:709/2035 train_time:24746ms step_avg:34.90ms
step:710/2035 train_time:24805ms step_avg:34.94ms
step:711/2035 train_time:24866ms step_avg:34.97ms
step:712/2035 train_time:24925ms step_avg:35.01ms
step:713/2035 train_time:24985ms step_avg:35.04ms
step:714/2035 train_time:25045ms step_avg:35.08ms
step:715/2035 train_time:25105ms step_avg:35.11ms
step:716/2035 train_time:25164ms step_avg:35.15ms
step:717/2035 train_time:25224ms step_avg:35.18ms
step:718/2035 train_time:25283ms step_avg:35.21ms
step:719/2035 train_time:25344ms step_avg:35.25ms
step:720/2035 train_time:25403ms step_avg:35.28ms
step:721/2035 train_time:25464ms step_avg:35.32ms
step:722/2035 train_time:25523ms step_avg:35.35ms
step:723/2035 train_time:25584ms step_avg:35.39ms
step:724/2035 train_time:25643ms step_avg:35.42ms
step:725/2035 train_time:25703ms step_avg:35.45ms
step:726/2035 train_time:25762ms step_avg:35.49ms
step:727/2035 train_time:25823ms step_avg:35.52ms
step:728/2035 train_time:25883ms step_avg:35.55ms
step:729/2035 train_time:25944ms step_avg:35.59ms
step:730/2035 train_time:26003ms step_avg:35.62ms
step:731/2035 train_time:26063ms step_avg:35.65ms
step:732/2035 train_time:26123ms step_avg:35.69ms
step:733/2035 train_time:26183ms step_avg:35.72ms
step:734/2035 train_time:26242ms step_avg:35.75ms
step:735/2035 train_time:26303ms step_avg:35.79ms
step:736/2035 train_time:26363ms step_avg:35.82ms
step:737/2035 train_time:26424ms step_avg:35.85ms
step:738/2035 train_time:26484ms step_avg:35.89ms
step:739/2035 train_time:26545ms step_avg:35.92ms
step:740/2035 train_time:26605ms step_avg:35.95ms
step:741/2035 train_time:26666ms step_avg:35.99ms
step:742/2035 train_time:26725ms step_avg:36.02ms
step:743/2035 train_time:26785ms step_avg:36.05ms
step:744/2035 train_time:26844ms step_avg:36.08ms
step:745/2035 train_time:26904ms step_avg:36.11ms
step:746/2035 train_time:26964ms step_avg:36.14ms
step:747/2035 train_time:27025ms step_avg:36.18ms
step:748/2035 train_time:27085ms step_avg:36.21ms
step:749/2035 train_time:27145ms step_avg:36.24ms
step:750/2035 train_time:27204ms step_avg:36.27ms
step:750/2035 val_loss:3.8428 train_time:27267ms step_avg:36.36ms
step:751/2035 train_time:27287ms step_avg:36.33ms
step:752/2035 train_time:27327ms step_avg:36.34ms
step:753/2035 train_time:27391ms step_avg:36.38ms
step:754/2035 train_time:27453ms step_avg:36.41ms
step:755/2035 train_time:27513ms step_avg:36.44ms
step:756/2035 train_time:27572ms step_avg:36.47ms
step:757/2035 train_time:27633ms step_avg:36.50ms
step:758/2035 train_time:27692ms step_avg:36.53ms
step:759/2035 train_time:27752ms step_avg:36.56ms
step:760/2035 train_time:27811ms step_avg:36.59ms
step:761/2035 train_time:27870ms step_avg:36.62ms
step:762/2035 train_time:27928ms step_avg:36.65ms
step:763/2035 train_time:27988ms step_avg:36.68ms
step:764/2035 train_time:28048ms step_avg:36.71ms
step:765/2035 train_time:28108ms step_avg:36.74ms
step:766/2035 train_time:28166ms step_avg:36.77ms
step:767/2035 train_time:28227ms step_avg:36.80ms
step:768/2035 train_time:28287ms step_avg:36.83ms
step:769/2035 train_time:28350ms step_avg:36.87ms
step:770/2035 train_time:28411ms step_avg:36.90ms
step:771/2035 train_time:28472ms step_avg:36.93ms
step:772/2035 train_time:28533ms step_avg:36.96ms
step:773/2035 train_time:28593ms step_avg:36.99ms
step:774/2035 train_time:28652ms step_avg:37.02ms
step:775/2035 train_time:28712ms step_avg:37.05ms
step:776/2035 train_time:28770ms step_avg:37.08ms
step:777/2035 train_time:28831ms step_avg:37.11ms
step:778/2035 train_time:28889ms step_avg:37.13ms
step:779/2035 train_time:28949ms step_avg:37.16ms
step:780/2035 train_time:29008ms step_avg:37.19ms
step:781/2035 train_time:29068ms step_avg:37.22ms
step:782/2035 train_time:29127ms step_avg:37.25ms
step:783/2035 train_time:29187ms step_avg:37.28ms
step:784/2035 train_time:29247ms step_avg:37.31ms
step:785/2035 train_time:29309ms step_avg:37.34ms
step:786/2035 train_time:29369ms step_avg:37.37ms
step:787/2035 train_time:29431ms step_avg:37.40ms
step:788/2035 train_time:29491ms step_avg:37.42ms
step:789/2035 train_time:29551ms step_avg:37.45ms
step:790/2035 train_time:29610ms step_avg:37.48ms
step:791/2035 train_time:29671ms step_avg:37.51ms
step:792/2035 train_time:29730ms step_avg:37.54ms
step:793/2035 train_time:29791ms step_avg:37.57ms
step:794/2035 train_time:29850ms step_avg:37.59ms
step:795/2035 train_time:29910ms step_avg:37.62ms
step:796/2035 train_time:29969ms step_avg:37.65ms
step:797/2035 train_time:30029ms step_avg:37.68ms
step:798/2035 train_time:30088ms step_avg:37.70ms
step:799/2035 train_time:30148ms step_avg:37.73ms
step:800/2035 train_time:30207ms step_avg:37.76ms
step:801/2035 train_time:30269ms step_avg:37.79ms
step:802/2035 train_time:30329ms step_avg:37.82ms
step:803/2035 train_time:30390ms step_avg:37.85ms
step:804/2035 train_time:30450ms step_avg:37.87ms
step:805/2035 train_time:30511ms step_avg:37.90ms
step:806/2035 train_time:30571ms step_avg:37.93ms
step:807/2035 train_time:30632ms step_avg:37.96ms
step:808/2035 train_time:30692ms step_avg:37.99ms
step:809/2035 train_time:30753ms step_avg:38.01ms
step:810/2035 train_time:30811ms step_avg:38.04ms
step:811/2035 train_time:30871ms step_avg:38.07ms
step:812/2035 train_time:30931ms step_avg:38.09ms
step:813/2035 train_time:30991ms step_avg:38.12ms
step:814/2035 train_time:31050ms step_avg:38.14ms
step:815/2035 train_time:31110ms step_avg:38.17ms
step:816/2035 train_time:31170ms step_avg:38.20ms
step:817/2035 train_time:31230ms step_avg:38.23ms
step:818/2035 train_time:31290ms step_avg:38.25ms
step:819/2035 train_time:31351ms step_avg:38.28ms
step:820/2035 train_time:31412ms step_avg:38.31ms
step:821/2035 train_time:31472ms step_avg:38.33ms
step:822/2035 train_time:31532ms step_avg:38.36ms
step:823/2035 train_time:31593ms step_avg:38.39ms
step:824/2035 train_time:31652ms step_avg:38.41ms
step:825/2035 train_time:31712ms step_avg:38.44ms
step:826/2035 train_time:31772ms step_avg:38.46ms
step:827/2035 train_time:31832ms step_avg:38.49ms
step:828/2035 train_time:31892ms step_avg:38.52ms
step:829/2035 train_time:31952ms step_avg:38.54ms
step:830/2035 train_time:32011ms step_avg:38.57ms
step:831/2035 train_time:32072ms step_avg:38.59ms
step:832/2035 train_time:32131ms step_avg:38.62ms
step:833/2035 train_time:32192ms step_avg:38.65ms
step:834/2035 train_time:32251ms step_avg:38.67ms
step:835/2035 train_time:32312ms step_avg:38.70ms
step:836/2035 train_time:32371ms step_avg:38.72ms
step:837/2035 train_time:32432ms step_avg:38.75ms
step:838/2035 train_time:32492ms step_avg:38.77ms
step:839/2035 train_time:32553ms step_avg:38.80ms
step:840/2035 train_time:32613ms step_avg:38.82ms
step:841/2035 train_time:32673ms step_avg:38.85ms
step:842/2035 train_time:32733ms step_avg:38.88ms
step:843/2035 train_time:32793ms step_avg:38.90ms
step:844/2035 train_time:32852ms step_avg:38.92ms
step:845/2035 train_time:32912ms step_avg:38.95ms
step:846/2035 train_time:32971ms step_avg:38.97ms
step:847/2035 train_time:33031ms step_avg:39.00ms
step:848/2035 train_time:33090ms step_avg:39.02ms
step:849/2035 train_time:33151ms step_avg:39.05ms
step:850/2035 train_time:33210ms step_avg:39.07ms
step:851/2035 train_time:33271ms step_avg:39.10ms
step:852/2035 train_time:33331ms step_avg:39.12ms
step:853/2035 train_time:33392ms step_avg:39.15ms
step:854/2035 train_time:33451ms step_avg:39.17ms
step:855/2035 train_time:33512ms step_avg:39.20ms
step:856/2035 train_time:33571ms step_avg:39.22ms
step:857/2035 train_time:33633ms step_avg:39.24ms
step:858/2035 train_time:33693ms step_avg:39.27ms
step:859/2035 train_time:33754ms step_avg:39.29ms
step:860/2035 train_time:33813ms step_avg:39.32ms
step:861/2035 train_time:33873ms step_avg:39.34ms
step:862/2035 train_time:33932ms step_avg:39.36ms
step:863/2035 train_time:33992ms step_avg:39.39ms
step:864/2035 train_time:34050ms step_avg:39.41ms
step:865/2035 train_time:34111ms step_avg:39.43ms
step:866/2035 train_time:34170ms step_avg:39.46ms
step:867/2035 train_time:34231ms step_avg:39.48ms
step:868/2035 train_time:34290ms step_avg:39.50ms
step:869/2035 train_time:34350ms step_avg:39.53ms
step:870/2035 train_time:34409ms step_avg:39.55ms
step:871/2035 train_time:34470ms step_avg:39.58ms
step:872/2035 train_time:34530ms step_avg:39.60ms
step:873/2035 train_time:34590ms step_avg:39.62ms
step:874/2035 train_time:34650ms step_avg:39.65ms
step:875/2035 train_time:34711ms step_avg:39.67ms
step:876/2035 train_time:34771ms step_avg:39.69ms
step:877/2035 train_time:34831ms step_avg:39.72ms
step:878/2035 train_time:34891ms step_avg:39.74ms
step:879/2035 train_time:34951ms step_avg:39.76ms
step:880/2035 train_time:35010ms step_avg:39.78ms
step:881/2035 train_time:35070ms step_avg:39.81ms
step:882/2035 train_time:35130ms step_avg:39.83ms
step:883/2035 train_time:35190ms step_avg:39.85ms
step:884/2035 train_time:35249ms step_avg:39.87ms
step:885/2035 train_time:35310ms step_avg:39.90ms
step:886/2035 train_time:35369ms step_avg:39.92ms
step:887/2035 train_time:35430ms step_avg:39.94ms
step:888/2035 train_time:35489ms step_avg:39.97ms
step:889/2035 train_time:35550ms step_avg:39.99ms
step:890/2035 train_time:35610ms step_avg:40.01ms
step:891/2035 train_time:35671ms step_avg:40.03ms
step:892/2035 train_time:35731ms step_avg:40.06ms
step:893/2035 train_time:35792ms step_avg:40.08ms
step:894/2035 train_time:35851ms step_avg:40.10ms
step:895/2035 train_time:35912ms step_avg:40.12ms
step:896/2035 train_time:35971ms step_avg:40.15ms
step:897/2035 train_time:36031ms step_avg:40.17ms
step:898/2035 train_time:36090ms step_avg:40.19ms
step:899/2035 train_time:36151ms step_avg:40.21ms
step:900/2035 train_time:36210ms step_avg:40.23ms
step:901/2035 train_time:36270ms step_avg:40.26ms
step:902/2035 train_time:36329ms step_avg:40.28ms
step:903/2035 train_time:36390ms step_avg:40.30ms
step:904/2035 train_time:36449ms step_avg:40.32ms
step:905/2035 train_time:36510ms step_avg:40.34ms
step:906/2035 train_time:36569ms step_avg:40.36ms
step:907/2035 train_time:36630ms step_avg:40.39ms
step:908/2035 train_time:36689ms step_avg:40.41ms
step:909/2035 train_time:36750ms step_avg:40.43ms
step:910/2035 train_time:36810ms step_avg:40.45ms
step:911/2035 train_time:36871ms step_avg:40.47ms
step:912/2035 train_time:36930ms step_avg:40.49ms
step:913/2035 train_time:36991ms step_avg:40.52ms
step:914/2035 train_time:37050ms step_avg:40.54ms
step:915/2035 train_time:37111ms step_avg:40.56ms
step:916/2035 train_time:37170ms step_avg:40.58ms
step:917/2035 train_time:37231ms step_avg:40.60ms
step:918/2035 train_time:37290ms step_avg:40.62ms
step:919/2035 train_time:37350ms step_avg:40.64ms
step:920/2035 train_time:37409ms step_avg:40.66ms
step:921/2035 train_time:37470ms step_avg:40.68ms
step:922/2035 train_time:37529ms step_avg:40.70ms
step:923/2035 train_time:37589ms step_avg:40.73ms
step:924/2035 train_time:37649ms step_avg:40.75ms
step:925/2035 train_time:37709ms step_avg:40.77ms
step:926/2035 train_time:37769ms step_avg:40.79ms
step:927/2035 train_time:37829ms step_avg:40.81ms
step:928/2035 train_time:37889ms step_avg:40.83ms
step:929/2035 train_time:37949ms step_avg:40.85ms
step:930/2035 train_time:38008ms step_avg:40.87ms
step:931/2035 train_time:38069ms step_avg:40.89ms
step:932/2035 train_time:38128ms step_avg:40.91ms
step:933/2035 train_time:38188ms step_avg:40.93ms
step:934/2035 train_time:38247ms step_avg:40.95ms
step:935/2035 train_time:38308ms step_avg:40.97ms
step:936/2035 train_time:38367ms step_avg:40.99ms
step:937/2035 train_time:38428ms step_avg:41.01ms
step:938/2035 train_time:38487ms step_avg:41.03ms
step:939/2035 train_time:38548ms step_avg:41.05ms
step:940/2035 train_time:38607ms step_avg:41.07ms
step:941/2035 train_time:38668ms step_avg:41.09ms
step:942/2035 train_time:38727ms step_avg:41.11ms
step:943/2035 train_time:38787ms step_avg:41.13ms
step:944/2035 train_time:38847ms step_avg:41.15ms
step:945/2035 train_time:38907ms step_avg:41.17ms
step:946/2035 train_time:38967ms step_avg:41.19ms
step:947/2035 train_time:39026ms step_avg:41.21ms
step:948/2035 train_time:39087ms step_avg:41.23ms
step:949/2035 train_time:39146ms step_avg:41.25ms
step:950/2035 train_time:39205ms step_avg:41.27ms
step:951/2035 train_time:39265ms step_avg:41.29ms
step:952/2035 train_time:39324ms step_avg:41.31ms
step:953/2035 train_time:39384ms step_avg:41.33ms
step:954/2035 train_time:39443ms step_avg:41.34ms
step:955/2035 train_time:39503ms step_avg:41.36ms
step:956/2035 train_time:39563ms step_avg:41.38ms
step:957/2035 train_time:39623ms step_avg:41.40ms
step:958/2035 train_time:39682ms step_avg:41.42ms
step:959/2035 train_time:39742ms step_avg:41.44ms
step:960/2035 train_time:39803ms step_avg:41.46ms
step:961/2035 train_time:39862ms step_avg:41.48ms
step:962/2035 train_time:39922ms step_avg:41.50ms
step:963/2035 train_time:39983ms step_avg:41.52ms
step:964/2035 train_time:40043ms step_avg:41.54ms
step:965/2035 train_time:40103ms step_avg:41.56ms
step:966/2035 train_time:40163ms step_avg:41.58ms
step:967/2035 train_time:40223ms step_avg:41.60ms
step:968/2035 train_time:40283ms step_avg:41.61ms
step:969/2035 train_time:40344ms step_avg:41.63ms
step:970/2035 train_time:40404ms step_avg:41.65ms
step:971/2035 train_time:40464ms step_avg:41.67ms
step:972/2035 train_time:40523ms step_avg:41.69ms
step:973/2035 train_time:40583ms step_avg:41.71ms
step:974/2035 train_time:40643ms step_avg:41.73ms
step:975/2035 train_time:40704ms step_avg:41.75ms
step:976/2035 train_time:40764ms step_avg:41.77ms
step:977/2035 train_time:40824ms step_avg:41.78ms
step:978/2035 train_time:40883ms step_avg:41.80ms
step:979/2035 train_time:40944ms step_avg:41.82ms
step:980/2035 train_time:41004ms step_avg:41.84ms
step:981/2035 train_time:41064ms step_avg:41.86ms
step:982/2035 train_time:41123ms step_avg:41.88ms
step:983/2035 train_time:41183ms step_avg:41.90ms
step:984/2035 train_time:41243ms step_avg:41.91ms
step:985/2035 train_time:41303ms step_avg:41.93ms
step:986/2035 train_time:41363ms step_avg:41.95ms
step:987/2035 train_time:41423ms step_avg:41.97ms
step:988/2035 train_time:41482ms step_avg:41.99ms
step:989/2035 train_time:41542ms step_avg:42.00ms
step:990/2035 train_time:41601ms step_avg:42.02ms
step:991/2035 train_time:41661ms step_avg:42.04ms
step:992/2035 train_time:41720ms step_avg:42.06ms
step:993/2035 train_time:41780ms step_avg:42.07ms
step:994/2035 train_time:41840ms step_avg:42.09ms
step:995/2035 train_time:41899ms step_avg:42.11ms
step:996/2035 train_time:41959ms step_avg:42.13ms
step:997/2035 train_time:42019ms step_avg:42.15ms
step:998/2035 train_time:42078ms step_avg:42.16ms
step:999/2035 train_time:42138ms step_avg:42.18ms
step:1000/2035 train_time:42197ms step_avg:42.20ms
step:1000/2035 val_loss:3.6846 train_time:42259ms step_avg:42.26ms
step:1001/2035 train_time:42280ms step_avg:42.24ms
step:1002/2035 train_time:42322ms step_avg:42.24ms
step:1003/2035 train_time:42384ms step_avg:42.26ms
step:1004/2035 train_time:42449ms step_avg:42.28ms
step:1005/2035 train_time:42511ms step_avg:42.30ms
step:1006/2035 train_time:42570ms step_avg:42.32ms
step:1007/2035 train_time:42630ms step_avg:42.33ms
step:1008/2035 train_time:42688ms step_avg:42.35ms
step:1009/2035 train_time:42748ms step_avg:42.37ms
step:1010/2035 train_time:42807ms step_avg:42.38ms
step:1011/2035 train_time:42866ms step_avg:42.40ms
step:1012/2035 train_time:42925ms step_avg:42.42ms
step:1013/2035 train_time:42985ms step_avg:42.43ms
step:1014/2035 train_time:43044ms step_avg:42.45ms
step:1015/2035 train_time:43104ms step_avg:42.47ms
step:1016/2035 train_time:43162ms step_avg:42.48ms
step:1017/2035 train_time:43223ms step_avg:42.50ms
step:1018/2035 train_time:43284ms step_avg:42.52ms
step:1019/2035 train_time:43346ms step_avg:42.54ms
step:1020/2035 train_time:43407ms step_avg:42.56ms
step:1021/2035 train_time:43468ms step_avg:42.57ms
step:1022/2035 train_time:43528ms step_avg:42.59ms
step:1023/2035 train_time:43589ms step_avg:42.61ms
step:1024/2035 train_time:43648ms step_avg:42.62ms
step:1025/2035 train_time:43708ms step_avg:42.64ms
step:1026/2035 train_time:43767ms step_avg:42.66ms
step:1027/2035 train_time:43828ms step_avg:42.68ms
step:1028/2035 train_time:43886ms step_avg:42.69ms
step:1029/2035 train_time:43946ms step_avg:42.71ms
step:1030/2035 train_time:44005ms step_avg:42.72ms
step:1031/2035 train_time:44064ms step_avg:42.74ms
step:1032/2035 train_time:44123ms step_avg:42.75ms
step:1033/2035 train_time:44183ms step_avg:42.77ms
step:1034/2035 train_time:44242ms step_avg:42.79ms
step:1035/2035 train_time:44304ms step_avg:42.81ms
step:1036/2035 train_time:44364ms step_avg:42.82ms
step:1037/2035 train_time:44425ms step_avg:42.84ms
step:1038/2035 train_time:44485ms step_avg:42.86ms
step:1039/2035 train_time:44546ms step_avg:42.87ms
step:1040/2035 train_time:44605ms step_avg:42.89ms
step:1041/2035 train_time:44666ms step_avg:42.91ms
step:1042/2035 train_time:44725ms step_avg:42.92ms
step:1043/2035 train_time:44786ms step_avg:42.94ms
step:1044/2035 train_time:44845ms step_avg:42.95ms
step:1045/2035 train_time:44905ms step_avg:42.97ms
step:1046/2035 train_time:44963ms step_avg:42.99ms
step:1047/2035 train_time:45024ms step_avg:43.00ms
step:1048/2035 train_time:45083ms step_avg:43.02ms
step:1049/2035 train_time:45142ms step_avg:43.03ms
step:1050/2035 train_time:45202ms step_avg:43.05ms
step:1051/2035 train_time:45263ms step_avg:43.07ms
step:1052/2035 train_time:45324ms step_avg:43.08ms
step:1053/2035 train_time:45385ms step_avg:43.10ms
step:1054/2035 train_time:45444ms step_avg:43.12ms
step:1055/2035 train_time:45506ms step_avg:43.13ms
step:1056/2035 train_time:45566ms step_avg:43.15ms
step:1057/2035 train_time:45627ms step_avg:43.17ms
step:1058/2035 train_time:45686ms step_avg:43.18ms
step:1059/2035 train_time:45747ms step_avg:43.20ms
step:1060/2035 train_time:45806ms step_avg:43.21ms
step:1061/2035 train_time:45866ms step_avg:43.23ms
step:1062/2035 train_time:45925ms step_avg:43.24ms
step:1063/2035 train_time:45986ms step_avg:43.26ms
step:1064/2035 train_time:46045ms step_avg:43.28ms
step:1065/2035 train_time:46106ms step_avg:43.29ms
step:1066/2035 train_time:46165ms step_avg:43.31ms
step:1067/2035 train_time:46225ms step_avg:43.32ms
step:1068/2035 train_time:46285ms step_avg:43.34ms
step:1069/2035 train_time:46346ms step_avg:43.35ms
step:1070/2035 train_time:46405ms step_avg:43.37ms
step:1071/2035 train_time:46466ms step_avg:43.39ms
step:1072/2035 train_time:46526ms step_avg:43.40ms
step:1073/2035 train_time:46587ms step_avg:43.42ms
step:1074/2035 train_time:46646ms step_avg:43.43ms
step:1075/2035 train_time:46707ms step_avg:43.45ms
step:1076/2035 train_time:46766ms step_avg:43.46ms
step:1077/2035 train_time:46827ms step_avg:43.48ms
step:1078/2035 train_time:46886ms step_avg:43.49ms
step:1079/2035 train_time:46946ms step_avg:43.51ms
step:1080/2035 train_time:47005ms step_avg:43.52ms
step:1081/2035 train_time:47065ms step_avg:43.54ms
step:1082/2035 train_time:47125ms step_avg:43.55ms
step:1083/2035 train_time:47185ms step_avg:43.57ms
step:1084/2035 train_time:47244ms step_avg:43.58ms
step:1085/2035 train_time:47305ms step_avg:43.60ms
step:1086/2035 train_time:47365ms step_avg:43.61ms
step:1087/2035 train_time:47426ms step_avg:43.63ms
step:1088/2035 train_time:47486ms step_avg:43.64ms
step:1089/2035 train_time:47546ms step_avg:43.66ms
step:1090/2035 train_time:47605ms step_avg:43.67ms
step:1091/2035 train_time:47665ms step_avg:43.69ms
step:1092/2035 train_time:47725ms step_avg:43.70ms
step:1093/2035 train_time:47786ms step_avg:43.72ms
step:1094/2035 train_time:47845ms step_avg:43.73ms
step:1095/2035 train_time:47906ms step_avg:43.75ms
step:1096/2035 train_time:47965ms step_avg:43.76ms
step:1097/2035 train_time:48025ms step_avg:43.78ms
step:1098/2035 train_time:48084ms step_avg:43.79ms
step:1099/2035 train_time:48145ms step_avg:43.81ms
step:1100/2035 train_time:48204ms step_avg:43.82ms
step:1101/2035 train_time:48264ms step_avg:43.84ms
step:1102/2035 train_time:48324ms step_avg:43.85ms
step:1103/2035 train_time:48385ms step_avg:43.87ms
step:1104/2035 train_time:48445ms step_avg:43.88ms
step:1105/2035 train_time:48506ms step_avg:43.90ms
step:1106/2035 train_time:48565ms step_avg:43.91ms
step:1107/2035 train_time:48625ms step_avg:43.93ms
step:1108/2035 train_time:48685ms step_avg:43.94ms
step:1109/2035 train_time:48744ms step_avg:43.95ms
step:1110/2035 train_time:48804ms step_avg:43.97ms
step:1111/2035 train_time:48865ms step_avg:43.98ms
step:1112/2035 train_time:48924ms step_avg:44.00ms
step:1113/2035 train_time:48985ms step_avg:44.01ms
step:1114/2035 train_time:49044ms step_avg:44.02ms
step:1115/2035 train_time:49104ms step_avg:44.04ms
step:1116/2035 train_time:49164ms step_avg:44.05ms
step:1117/2035 train_time:49224ms step_avg:44.07ms
step:1118/2035 train_time:49284ms step_avg:44.08ms
step:1119/2035 train_time:49344ms step_avg:44.10ms
step:1120/2035 train_time:49403ms step_avg:44.11ms
step:1121/2035 train_time:49464ms step_avg:44.13ms
step:1122/2035 train_time:49524ms step_avg:44.14ms
step:1123/2035 train_time:49585ms step_avg:44.15ms
step:1124/2035 train_time:49645ms step_avg:44.17ms
step:1125/2035 train_time:49706ms step_avg:44.18ms
step:1126/2035 train_time:49765ms step_avg:44.20ms
step:1127/2035 train_time:49826ms step_avg:44.21ms
step:1128/2035 train_time:49885ms step_avg:44.22ms
step:1129/2035 train_time:49945ms step_avg:44.24ms
step:1130/2035 train_time:50004ms step_avg:44.25ms
step:1131/2035 train_time:50064ms step_avg:44.27ms
step:1132/2035 train_time:50124ms step_avg:44.28ms
step:1133/2035 train_time:50185ms step_avg:44.29ms
step:1134/2035 train_time:50245ms step_avg:44.31ms
step:1135/2035 train_time:50306ms step_avg:44.32ms
step:1136/2035 train_time:50365ms step_avg:44.34ms
step:1137/2035 train_time:50427ms step_avg:44.35ms
step:1138/2035 train_time:50486ms step_avg:44.36ms
step:1139/2035 train_time:50547ms step_avg:44.38ms
step:1140/2035 train_time:50607ms step_avg:44.39ms
step:1141/2035 train_time:50667ms step_avg:44.41ms
step:1142/2035 train_time:50726ms step_avg:44.42ms
step:1143/2035 train_time:50787ms step_avg:44.43ms
step:1144/2035 train_time:50846ms step_avg:44.45ms
step:1145/2035 train_time:50907ms step_avg:44.46ms
step:1146/2035 train_time:50966ms step_avg:44.47ms
step:1147/2035 train_time:51026ms step_avg:44.49ms
step:1148/2035 train_time:51085ms step_avg:44.50ms
step:1149/2035 train_time:51146ms step_avg:44.51ms
step:1150/2035 train_time:51205ms step_avg:44.53ms
step:1151/2035 train_time:51265ms step_avg:44.54ms
step:1152/2035 train_time:51325ms step_avg:44.55ms
step:1153/2035 train_time:51386ms step_avg:44.57ms
step:1154/2035 train_time:51445ms step_avg:44.58ms
step:1155/2035 train_time:51506ms step_avg:44.59ms
step:1156/2035 train_time:51565ms step_avg:44.61ms
step:1157/2035 train_time:51626ms step_avg:44.62ms
step:1158/2035 train_time:51686ms step_avg:44.63ms
step:1159/2035 train_time:51746ms step_avg:44.65ms
step:1160/2035 train_time:51805ms step_avg:44.66ms
step:1161/2035 train_time:51865ms step_avg:44.67ms
step:1162/2035 train_time:51924ms step_avg:44.69ms
step:1163/2035 train_time:51985ms step_avg:44.70ms
step:1164/2035 train_time:52044ms step_avg:44.71ms
step:1165/2035 train_time:52105ms step_avg:44.73ms
step:1166/2035 train_time:52164ms step_avg:44.74ms
step:1167/2035 train_time:52225ms step_avg:44.75ms
step:1168/2035 train_time:52284ms step_avg:44.76ms
step:1169/2035 train_time:52344ms step_avg:44.78ms
step:1170/2035 train_time:52404ms step_avg:44.79ms
step:1171/2035 train_time:52465ms step_avg:44.80ms
step:1172/2035 train_time:52524ms step_avg:44.82ms
step:1173/2035 train_time:52586ms step_avg:44.83ms
step:1174/2035 train_time:52646ms step_avg:44.84ms
step:1175/2035 train_time:52706ms step_avg:44.86ms
step:1176/2035 train_time:52766ms step_avg:44.87ms
step:1177/2035 train_time:52826ms step_avg:44.88ms
step:1178/2035 train_time:52885ms step_avg:44.89ms
step:1179/2035 train_time:52946ms step_avg:44.91ms
step:1180/2035 train_time:53006ms step_avg:44.92ms
step:1181/2035 train_time:53067ms step_avg:44.93ms
step:1182/2035 train_time:53126ms step_avg:44.95ms
step:1183/2035 train_time:53187ms step_avg:44.96ms
step:1184/2035 train_time:53246ms step_avg:44.97ms
step:1185/2035 train_time:53307ms step_avg:44.98ms
step:1186/2035 train_time:53366ms step_avg:45.00ms
step:1187/2035 train_time:53426ms step_avg:45.01ms
step:1188/2035 train_time:53486ms step_avg:45.02ms
step:1189/2035 train_time:53547ms step_avg:45.04ms
step:1190/2035 train_time:53606ms step_avg:45.05ms
step:1191/2035 train_time:53667ms step_avg:45.06ms
step:1192/2035 train_time:53726ms step_avg:45.07ms
step:1193/2035 train_time:53786ms step_avg:45.09ms
step:1194/2035 train_time:53846ms step_avg:45.10ms
step:1195/2035 train_time:53907ms step_avg:45.11ms
step:1196/2035 train_time:53966ms step_avg:45.12ms
step:1197/2035 train_time:54027ms step_avg:45.13ms
step:1198/2035 train_time:54085ms step_avg:45.15ms
step:1199/2035 train_time:54146ms step_avg:45.16ms
step:1200/2035 train_time:54205ms step_avg:45.17ms
step:1201/2035 train_time:54266ms step_avg:45.18ms
step:1202/2035 train_time:54325ms step_avg:45.20ms
step:1203/2035 train_time:54386ms step_avg:45.21ms
step:1204/2035 train_time:54445ms step_avg:45.22ms
step:1205/2035 train_time:54506ms step_avg:45.23ms
step:1206/2035 train_time:54565ms step_avg:45.24ms
step:1207/2035 train_time:54625ms step_avg:45.26ms
step:1208/2035 train_time:54685ms step_avg:45.27ms
step:1209/2035 train_time:54746ms step_avg:45.28ms
step:1210/2035 train_time:54805ms step_avg:45.29ms
step:1211/2035 train_time:54865ms step_avg:45.31ms
step:1212/2035 train_time:54924ms step_avg:45.32ms
step:1213/2035 train_time:54985ms step_avg:45.33ms
step:1214/2035 train_time:55044ms step_avg:45.34ms
step:1215/2035 train_time:55106ms step_avg:45.35ms
step:1216/2035 train_time:55165ms step_avg:45.37ms
step:1217/2035 train_time:55225ms step_avg:45.38ms
step:1218/2035 train_time:55285ms step_avg:45.39ms
step:1219/2035 train_time:55346ms step_avg:45.40ms
step:1220/2035 train_time:55405ms step_avg:45.41ms
step:1221/2035 train_time:55465ms step_avg:45.43ms
step:1222/2035 train_time:55525ms step_avg:45.44ms
step:1223/2035 train_time:55585ms step_avg:45.45ms
step:1224/2035 train_time:55644ms step_avg:45.46ms
step:1225/2035 train_time:55704ms step_avg:45.47ms
step:1226/2035 train_time:55763ms step_avg:45.48ms
step:1227/2035 train_time:55825ms step_avg:45.50ms
step:1228/2035 train_time:55884ms step_avg:45.51ms
step:1229/2035 train_time:55945ms step_avg:45.52ms
step:1230/2035 train_time:56004ms step_avg:45.53ms
step:1231/2035 train_time:56065ms step_avg:45.54ms
step:1232/2035 train_time:56124ms step_avg:45.56ms
step:1233/2035 train_time:56185ms step_avg:45.57ms
step:1234/2035 train_time:56244ms step_avg:45.58ms
step:1235/2035 train_time:56305ms step_avg:45.59ms
step:1236/2035 train_time:56365ms step_avg:45.60ms
step:1237/2035 train_time:56426ms step_avg:45.61ms
step:1238/2035 train_time:56484ms step_avg:45.63ms
step:1239/2035 train_time:56545ms step_avg:45.64ms
step:1240/2035 train_time:56604ms step_avg:45.65ms
step:1241/2035 train_time:56665ms step_avg:45.66ms
step:1242/2035 train_time:56724ms step_avg:45.67ms
step:1243/2035 train_time:56785ms step_avg:45.68ms
step:1244/2035 train_time:56844ms step_avg:45.69ms
step:1245/2035 train_time:56905ms step_avg:45.71ms
step:1246/2035 train_time:56964ms step_avg:45.72ms
step:1247/2035 train_time:57025ms step_avg:45.73ms
step:1248/2035 train_time:57084ms step_avg:45.74ms
step:1249/2035 train_time:57145ms step_avg:45.75ms
step:1250/2035 train_time:57205ms step_avg:45.76ms
step:1250/2035 val_loss:3.5677 train_time:57268ms step_avg:45.81ms
step:1251/2035 train_time:57288ms step_avg:45.79ms
step:1252/2035 train_time:57328ms step_avg:45.79ms
step:1253/2035 train_time:57391ms step_avg:45.80ms
step:1254/2035 train_time:57453ms step_avg:45.82ms
step:1255/2035 train_time:57514ms step_avg:45.83ms
step:1256/2035 train_time:57573ms step_avg:45.84ms
step:1257/2035 train_time:57633ms step_avg:45.85ms
step:1258/2035 train_time:57692ms step_avg:45.86ms
step:1259/2035 train_time:57752ms step_avg:45.87ms
step:1260/2035 train_time:57810ms step_avg:45.88ms
step:1261/2035 train_time:57871ms step_avg:45.89ms
step:1262/2035 train_time:57930ms step_avg:45.90ms
step:1263/2035 train_time:57990ms step_avg:45.91ms
step:1264/2035 train_time:58049ms step_avg:45.92ms
step:1265/2035 train_time:58109ms step_avg:45.94ms
step:1266/2035 train_time:58169ms step_avg:45.95ms
step:1267/2035 train_time:58230ms step_avg:45.96ms
step:1268/2035 train_time:58290ms step_avg:45.97ms
step:1269/2035 train_time:58352ms step_avg:45.98ms
step:1270/2035 train_time:58412ms step_avg:45.99ms
step:1271/2035 train_time:58474ms step_avg:46.01ms
step:1272/2035 train_time:58533ms step_avg:46.02ms
step:1273/2035 train_time:58593ms step_avg:46.03ms
step:1274/2035 train_time:58652ms step_avg:46.04ms
step:1275/2035 train_time:58712ms step_avg:46.05ms
step:1276/2035 train_time:58771ms step_avg:46.06ms
step:1277/2035 train_time:58831ms step_avg:46.07ms
step:1278/2035 train_time:58890ms step_avg:46.08ms
step:1279/2035 train_time:58950ms step_avg:46.09ms
step:1280/2035 train_time:59009ms step_avg:46.10ms
step:1281/2035 train_time:59068ms step_avg:46.11ms
step:1282/2035 train_time:59127ms step_avg:46.12ms
step:1283/2035 train_time:59188ms step_avg:46.13ms
step:1284/2035 train_time:59247ms step_avg:46.14ms
step:1285/2035 train_time:59308ms step_avg:46.15ms
step:1286/2035 train_time:59368ms step_avg:46.16ms
step:1287/2035 train_time:59430ms step_avg:46.18ms
step:1288/2035 train_time:59489ms step_avg:46.19ms
step:1289/2035 train_time:59550ms step_avg:46.20ms
step:1290/2035 train_time:59609ms step_avg:46.21ms
step:1291/2035 train_time:59669ms step_avg:46.22ms
step:1292/2035 train_time:59728ms step_avg:46.23ms
step:1293/2035 train_time:59788ms step_avg:46.24ms
step:1294/2035 train_time:59847ms step_avg:46.25ms
step:1295/2035 train_time:59908ms step_avg:46.26ms
step:1296/2035 train_time:59967ms step_avg:46.27ms
step:1297/2035 train_time:60028ms step_avg:46.28ms
step:1298/2035 train_time:60086ms step_avg:46.29ms
step:1299/2035 train_time:60147ms step_avg:46.30ms
step:1300/2035 train_time:60206ms step_avg:46.31ms
step:1301/2035 train_time:60266ms step_avg:46.32ms
step:1302/2035 train_time:60327ms step_avg:46.33ms
step:1303/2035 train_time:60388ms step_avg:46.35ms
step:1304/2035 train_time:60447ms step_avg:46.36ms
step:1305/2035 train_time:60508ms step_avg:46.37ms
step:1306/2035 train_time:60567ms step_avg:46.38ms
step:1307/2035 train_time:60627ms step_avg:46.39ms
step:1308/2035 train_time:60687ms step_avg:46.40ms
step:1309/2035 train_time:60748ms step_avg:46.41ms
step:1310/2035 train_time:60806ms step_avg:46.42ms
step:1311/2035 train_time:60867ms step_avg:46.43ms
step:1312/2035 train_time:60927ms step_avg:46.44ms
step:1313/2035 train_time:60987ms step_avg:46.45ms
step:1314/2035 train_time:61046ms step_avg:46.46ms
step:1315/2035 train_time:61106ms step_avg:46.47ms
step:1316/2035 train_time:61165ms step_avg:46.48ms
step:1317/2035 train_time:61226ms step_avg:46.49ms
step:1318/2035 train_time:61285ms step_avg:46.50ms
step:1319/2035 train_time:61346ms step_avg:46.51ms
step:1320/2035 train_time:61405ms step_avg:46.52ms
step:1321/2035 train_time:61466ms step_avg:46.53ms
step:1322/2035 train_time:61527ms step_avg:46.54ms
step:1323/2035 train_time:61588ms step_avg:46.55ms
step:1324/2035 train_time:61647ms step_avg:46.56ms
step:1325/2035 train_time:61707ms step_avg:46.57ms
step:1326/2035 train_time:61766ms step_avg:46.58ms
step:1327/2035 train_time:61827ms step_avg:46.59ms
step:1328/2035 train_time:61886ms step_avg:46.60ms
step:1329/2035 train_time:61947ms step_avg:46.61ms
step:1330/2035 train_time:62006ms step_avg:46.62ms
step:1331/2035 train_time:62067ms step_avg:46.63ms
step:1332/2035 train_time:62154ms step_avg:46.66ms
step:1333/2035 train_time:62242ms step_avg:46.69ms
step:1334/2035 train_time:62329ms step_avg:46.72ms
step:1335/2035 train_time:62418ms step_avg:46.75ms
step:1336/2035 train_time:62504ms step_avg:46.78ms
step:1337/2035 train_time:62594ms step_avg:46.82ms
step:1338/2035 train_time:62680ms step_avg:46.85ms
step:1339/2035 train_time:62769ms step_avg:46.88ms
step:1340/2035 train_time:62856ms step_avg:46.91ms
step:1341/2035 train_time:62945ms step_avg:46.94ms
step:1342/2035 train_time:63031ms step_avg:46.97ms
step:1343/2035 train_time:63119ms step_avg:47.00ms
step:1344/2035 train_time:63205ms step_avg:47.03ms
step:1345/2035 train_time:63294ms step_avg:47.06ms
step:1346/2035 train_time:63381ms step_avg:47.09ms
step:1347/2035 train_time:63468ms step_avg:47.12ms
step:1348/2035 train_time:63555ms step_avg:47.15ms
step:1349/2035 train_time:63644ms step_avg:47.18ms
step:1350/2035 train_time:63731ms step_avg:47.21ms
step:1351/2035 train_time:63820ms step_avg:47.24ms
step:1352/2035 train_time:63907ms step_avg:47.27ms
step:1353/2035 train_time:63994ms step_avg:47.30ms
step:1354/2035 train_time:64081ms step_avg:47.33ms
step:1355/2035 train_time:64169ms step_avg:47.36ms
step:1356/2035 train_time:64255ms step_avg:47.39ms
step:1357/2035 train_time:64344ms step_avg:47.42ms
step:1358/2035 train_time:64431ms step_avg:47.45ms
step:1359/2035 train_time:64519ms step_avg:47.48ms
step:1360/2035 train_time:64606ms step_avg:47.50ms
step:1361/2035 train_time:64694ms step_avg:47.53ms
step:1362/2035 train_time:64781ms step_avg:47.56ms
step:1363/2035 train_time:64868ms step_avg:47.59ms
step:1364/2035 train_time:64954ms step_avg:47.62ms
step:1365/2035 train_time:65043ms step_avg:47.65ms
step:1366/2035 train_time:65129ms step_avg:47.68ms
step:1367/2035 train_time:65217ms step_avg:47.71ms
step:1368/2035 train_time:65306ms step_avg:47.74ms
step:1369/2035 train_time:65394ms step_avg:47.77ms
step:1370/2035 train_time:65482ms step_avg:47.80ms
step:1371/2035 train_time:65571ms step_avg:47.83ms
step:1372/2035 train_time:65657ms step_avg:47.86ms
step:1373/2035 train_time:65745ms step_avg:47.88ms
step:1374/2035 train_time:65832ms step_avg:47.91ms
step:1375/2035 train_time:65920ms step_avg:47.94ms
step:1376/2035 train_time:66007ms step_avg:47.97ms
step:1377/2035 train_time:66094ms step_avg:48.00ms
step:1378/2035 train_time:66181ms step_avg:48.03ms
step:1379/2035 train_time:66270ms step_avg:48.06ms
step:1380/2035 train_time:66357ms step_avg:48.08ms
step:1381/2035 train_time:66446ms step_avg:48.11ms
step:1382/2035 train_time:66533ms step_avg:48.14ms
step:1383/2035 train_time:66620ms step_avg:48.17ms
step:1384/2035 train_time:66706ms step_avg:48.20ms
step:1385/2035 train_time:66794ms step_avg:48.23ms
step:1386/2035 train_time:66881ms step_avg:48.25ms
step:1387/2035 train_time:66969ms step_avg:48.28ms
step:1388/2035 train_time:67056ms step_avg:48.31ms
step:1389/2035 train_time:67145ms step_avg:48.34ms
step:1390/2035 train_time:67232ms step_avg:48.37ms
step:1391/2035 train_time:67320ms step_avg:48.40ms
step:1392/2035 train_time:67408ms step_avg:48.42ms
step:1393/2035 train_time:67496ms step_avg:48.45ms
step:1394/2035 train_time:67582ms step_avg:48.48ms
step:1395/2035 train_time:67670ms step_avg:48.51ms
step:1396/2035 train_time:67757ms step_avg:48.54ms
step:1397/2035 train_time:67845ms step_avg:48.56ms
step:1398/2035 train_time:67931ms step_avg:48.59ms
step:1399/2035 train_time:68019ms step_avg:48.62ms
step:1400/2035 train_time:68106ms step_avg:48.65ms
step:1401/2035 train_time:68194ms step_avg:48.68ms
step:1402/2035 train_time:68281ms step_avg:48.70ms
step:1403/2035 train_time:68370ms step_avg:48.73ms
step:1404/2035 train_time:68456ms step_avg:48.76ms
step:1405/2035 train_time:68545ms step_avg:48.79ms
step:1406/2035 train_time:68631ms step_avg:48.81ms
step:1407/2035 train_time:68720ms step_avg:48.84ms
step:1408/2035 train_time:68806ms step_avg:48.87ms
step:1409/2035 train_time:68895ms step_avg:48.90ms
step:1410/2035 train_time:68981ms step_avg:48.92ms
step:1411/2035 train_time:69069ms step_avg:48.95ms
step:1412/2035 train_time:69155ms step_avg:48.98ms
step:1413/2035 train_time:69244ms step_avg:49.00ms
step:1414/2035 train_time:69332ms step_avg:49.03ms
step:1415/2035 train_time:69420ms step_avg:49.06ms
step:1416/2035 train_time:69506ms step_avg:49.09ms
step:1417/2035 train_time:69594ms step_avg:49.11ms
step:1418/2035 train_time:69681ms step_avg:49.14ms
step:1419/2035 train_time:69770ms step_avg:49.17ms
step:1420/2035 train_time:69857ms step_avg:49.19ms
step:1421/2035 train_time:69945ms step_avg:49.22ms
step:1422/2035 train_time:70033ms step_avg:49.25ms
step:1423/2035 train_time:70120ms step_avg:49.28ms
step:1424/2035 train_time:70207ms step_avg:49.30ms
step:1425/2035 train_time:70296ms step_avg:49.33ms
step:1426/2035 train_time:70384ms step_avg:49.36ms
step:1427/2035 train_time:70472ms step_avg:49.38ms
step:1428/2035 train_time:70557ms step_avg:49.41ms
step:1429/2035 train_time:70646ms step_avg:49.44ms
step:1430/2035 train_time:70733ms step_avg:49.46ms
step:1431/2035 train_time:70820ms step_avg:49.49ms
step:1432/2035 train_time:70908ms step_avg:49.52ms
step:1433/2035 train_time:70995ms step_avg:49.54ms
step:1434/2035 train_time:71082ms step_avg:49.57ms
step:1435/2035 train_time:71170ms step_avg:49.60ms
step:1436/2035 train_time:71256ms step_avg:49.62ms
step:1437/2035 train_time:71346ms step_avg:49.65ms
step:1438/2035 train_time:71433ms step_avg:49.68ms
step:1439/2035 train_time:71522ms step_avg:49.70ms
step:1440/2035 train_time:71609ms step_avg:49.73ms
step:1441/2035 train_time:71697ms step_avg:49.75ms
step:1442/2035 train_time:71783ms step_avg:49.78ms
step:1443/2035 train_time:71872ms step_avg:49.81ms
step:1444/2035 train_time:71958ms step_avg:49.83ms
step:1445/2035 train_time:72046ms step_avg:49.86ms
step:1446/2035 train_time:72133ms step_avg:49.88ms
step:1447/2035 train_time:72221ms step_avg:49.91ms
step:1448/2035 train_time:72309ms step_avg:49.94ms
step:1449/2035 train_time:72397ms step_avg:49.96ms
step:1450/2035 train_time:72485ms step_avg:49.99ms
step:1451/2035 train_time:72573ms step_avg:50.02ms
step:1452/2035 train_time:72660ms step_avg:50.04ms
step:1453/2035 train_time:72748ms step_avg:50.07ms
step:1454/2035 train_time:72835ms step_avg:50.09ms
step:1455/2035 train_time:72924ms step_avg:50.12ms
step:1456/2035 train_time:73010ms step_avg:50.14ms
step:1457/2035 train_time:73098ms step_avg:50.17ms
step:1458/2035 train_time:73184ms step_avg:50.19ms
step:1459/2035 train_time:73272ms step_avg:50.22ms
step:1460/2035 train_time:73358ms step_avg:50.25ms
step:1461/2035 train_time:73448ms step_avg:50.27ms
step:1462/2035 train_time:73534ms step_avg:50.30ms
step:1463/2035 train_time:73622ms step_avg:50.32ms
step:1464/2035 train_time:73709ms step_avg:50.35ms
step:1465/2035 train_time:73797ms step_avg:50.37ms
step:1466/2035 train_time:73884ms step_avg:50.40ms
step:1467/2035 train_time:73972ms step_avg:50.42ms
step:1468/2035 train_time:74060ms step_avg:50.45ms
step:1469/2035 train_time:74148ms step_avg:50.48ms
step:1470/2035 train_time:74235ms step_avg:50.50ms
step:1471/2035 train_time:74323ms step_avg:50.53ms
step:1472/2035 train_time:74410ms step_avg:50.55ms
step:1473/2035 train_time:74498ms step_avg:50.58ms
step:1474/2035 train_time:74585ms step_avg:50.60ms
step:1475/2035 train_time:74674ms step_avg:50.63ms
step:1476/2035 train_time:74761ms step_avg:50.65ms
step:1477/2035 train_time:74849ms step_avg:50.68ms
step:1478/2035 train_time:74936ms step_avg:50.70ms
step:1479/2035 train_time:75024ms step_avg:50.73ms
step:1480/2035 train_time:75111ms step_avg:50.75ms
step:1481/2035 train_time:75200ms step_avg:50.78ms
step:1482/2035 train_time:75287ms step_avg:50.80ms
step:1483/2035 train_time:75375ms step_avg:50.83ms
step:1484/2035 train_time:75461ms step_avg:50.85ms
step:1485/2035 train_time:75550ms step_avg:50.88ms
step:1486/2035 train_time:75637ms step_avg:50.90ms
step:1487/2035 train_time:75726ms step_avg:50.93ms
step:1488/2035 train_time:75813ms step_avg:50.95ms
step:1489/2035 train_time:75901ms step_avg:50.97ms
step:1490/2035 train_time:75988ms step_avg:51.00ms
step:1491/2035 train_time:76076ms step_avg:51.02ms
step:1492/2035 train_time:76163ms step_avg:51.05ms
step:1493/2035 train_time:76251ms step_avg:51.07ms
step:1494/2035 train_time:76338ms step_avg:51.10ms
step:1495/2035 train_time:76426ms step_avg:51.12ms
step:1496/2035 train_time:76513ms step_avg:51.14ms
step:1497/2035 train_time:76601ms step_avg:51.17ms
step:1498/2035 train_time:76689ms step_avg:51.19ms
step:1499/2035 train_time:76776ms step_avg:51.22ms
step:1500/2035 train_time:76862ms step_avg:51.24ms
step:1500/2035 val_loss:3.4545 train_time:76952ms step_avg:51.30ms
step:1501/2035 train_time:76973ms step_avg:51.28ms
step:1502/2035 train_time:77044ms step_avg:51.29ms
step:1503/2035 train_time:77135ms step_avg:51.32ms
step:1504/2035 train_time:77222ms step_avg:51.34ms
step:1505/2035 train_time:77310ms step_avg:51.37ms
step:1506/2035 train_time:77395ms step_avg:51.39ms
step:1507/2035 train_time:77482ms step_avg:51.41ms
step:1508/2035 train_time:77568ms step_avg:51.44ms
step:1509/2035 train_time:77655ms step_avg:51.46ms
step:1510/2035 train_time:77741ms step_avg:51.48ms
step:1511/2035 train_time:77828ms step_avg:51.51ms
step:1512/2035 train_time:77917ms step_avg:51.53ms
step:1513/2035 train_time:78007ms step_avg:51.56ms
step:1514/2035 train_time:78096ms step_avg:51.58ms
step:1515/2035 train_time:78187ms step_avg:51.61ms
step:1516/2035 train_time:78275ms step_avg:51.63ms
step:1517/2035 train_time:78363ms step_avg:51.66ms
step:1518/2035 train_time:78449ms step_avg:51.68ms
step:1519/2035 train_time:78537ms step_avg:51.70ms
step:1520/2035 train_time:78623ms step_avg:51.73ms
step:1521/2035 train_time:78710ms step_avg:51.75ms
step:1522/2035 train_time:78796ms step_avg:51.77ms
step:1523/2035 train_time:78884ms step_avg:51.80ms
step:1524/2035 train_time:78972ms step_avg:51.82ms
step:1525/2035 train_time:79061ms step_avg:51.84ms
step:1526/2035 train_time:79150ms step_avg:51.87ms
step:1527/2035 train_time:79239ms step_avg:51.89ms
step:1528/2035 train_time:79326ms step_avg:51.91ms
step:1529/2035 train_time:79414ms step_avg:51.94ms
step:1530/2035 train_time:79501ms step_avg:51.96ms
step:1531/2035 train_time:79588ms step_avg:51.98ms
step:1532/2035 train_time:79675ms step_avg:52.01ms
step:1533/2035 train_time:79762ms step_avg:52.03ms
step:1534/2035 train_time:79848ms step_avg:52.05ms
step:1535/2035 train_time:79937ms step_avg:52.08ms
step:1536/2035 train_time:80025ms step_avg:52.10ms
step:1537/2035 train_time:80114ms step_avg:52.12ms
step:1538/2035 train_time:80201ms step_avg:52.15ms
step:1539/2035 train_time:80290ms step_avg:52.17ms
step:1540/2035 train_time:80377ms step_avg:52.19ms
step:1541/2035 train_time:80465ms step_avg:52.22ms
step:1542/2035 train_time:80551ms step_avg:52.24ms
step:1543/2035 train_time:80639ms step_avg:52.26ms
step:1544/2035 train_time:80725ms step_avg:52.28ms
step:1545/2035 train_time:80813ms step_avg:52.31ms
step:1546/2035 train_time:80900ms step_avg:52.33ms
step:1547/2035 train_time:80990ms step_avg:52.35ms
step:1548/2035 train_time:81077ms step_avg:52.38ms
step:1549/2035 train_time:81165ms step_avg:52.40ms
step:1550/2035 train_time:81252ms step_avg:52.42ms
step:1551/2035 train_time:81341ms step_avg:52.44ms
step:1552/2035 train_time:81428ms step_avg:52.47ms
step:1553/2035 train_time:81515ms step_avg:52.49ms
step:1554/2035 train_time:81602ms step_avg:52.51ms
step:1555/2035 train_time:81690ms step_avg:52.53ms
step:1556/2035 train_time:81777ms step_avg:52.56ms
step:1557/2035 train_time:81866ms step_avg:52.58ms
step:1558/2035 train_time:81953ms step_avg:52.60ms
step:1559/2035 train_time:82041ms step_avg:52.62ms
step:1560/2035 train_time:82128ms step_avg:52.65ms
step:1561/2035 train_time:82216ms step_avg:52.67ms
step:1562/2035 train_time:82303ms step_avg:52.69ms
step:1563/2035 train_time:82392ms step_avg:52.71ms
step:1564/2035 train_time:82478ms step_avg:52.74ms
step:1565/2035 train_time:82566ms step_avg:52.76ms
step:1566/2035 train_time:82653ms step_avg:52.78ms
step:1567/2035 train_time:82740ms step_avg:52.80ms
step:1568/2035 train_time:82827ms step_avg:52.82ms
step:1569/2035 train_time:82916ms step_avg:52.85ms
step:1570/2035 train_time:83003ms step_avg:52.87ms
step:1571/2035 train_time:83091ms step_avg:52.89ms
step:1572/2035 train_time:83178ms step_avg:52.91ms
step:1573/2035 train_time:83266ms step_avg:52.93ms
step:1574/2035 train_time:83353ms step_avg:52.96ms
step:1575/2035 train_time:83442ms step_avg:52.98ms
step:1576/2035 train_time:83529ms step_avg:53.00ms
step:1577/2035 train_time:83617ms step_avg:53.02ms
step:1578/2035 train_time:83704ms step_avg:53.04ms
step:1579/2035 train_time:83791ms step_avg:53.07ms
step:1580/2035 train_time:83878ms step_avg:53.09ms
step:1581/2035 train_time:83967ms step_avg:53.11ms
step:1582/2035 train_time:84053ms step_avg:53.13ms
step:1583/2035 train_time:84142ms step_avg:53.15ms
step:1584/2035 train_time:84229ms step_avg:53.17ms
step:1585/2035 train_time:84317ms step_avg:53.20ms
step:1586/2035 train_time:84403ms step_avg:53.22ms
step:1587/2035 train_time:84492ms step_avg:53.24ms
step:1588/2035 train_time:84579ms step_avg:53.26ms
step:1589/2035 train_time:84667ms step_avg:53.28ms
step:1590/2035 train_time:84754ms step_avg:53.30ms
step:1591/2035 train_time:84843ms step_avg:53.33ms
step:1592/2035 train_time:84929ms step_avg:53.35ms
step:1593/2035 train_time:85018ms step_avg:53.37ms
step:1594/2035 train_time:85106ms step_avg:53.39ms
step:1595/2035 train_time:85195ms step_avg:53.41ms
step:1596/2035 train_time:85282ms step_avg:53.43ms
step:1597/2035 train_time:85370ms step_avg:53.46ms
step:1598/2035 train_time:85456ms step_avg:53.48ms
step:1599/2035 train_time:85545ms step_avg:53.50ms
step:1600/2035 train_time:85632ms step_avg:53.52ms
step:1601/2035 train_time:85720ms step_avg:53.54ms
step:1602/2035 train_time:85807ms step_avg:53.56ms
step:1603/2035 train_time:85895ms step_avg:53.58ms
step:1604/2035 train_time:85982ms step_avg:53.60ms
step:1605/2035 train_time:86070ms step_avg:53.63ms
step:1606/2035 train_time:86158ms step_avg:53.65ms
step:1607/2035 train_time:86245ms step_avg:53.67ms
step:1608/2035 train_time:86332ms step_avg:53.69ms
step:1609/2035 train_time:86421ms step_avg:53.71ms
step:1610/2035 train_time:86508ms step_avg:53.73ms
step:1611/2035 train_time:86597ms step_avg:53.75ms
step:1612/2035 train_time:86684ms step_avg:53.77ms
step:1613/2035 train_time:86772ms step_avg:53.80ms
step:1614/2035 train_time:86858ms step_avg:53.82ms
step:1615/2035 train_time:86947ms step_avg:53.84ms
step:1616/2035 train_time:87034ms step_avg:53.86ms
step:1617/2035 train_time:87122ms step_avg:53.88ms
step:1618/2035 train_time:87209ms step_avg:53.90ms
step:1619/2035 train_time:87297ms step_avg:53.92ms
step:1620/2035 train_time:87383ms step_avg:53.94ms
step:1621/2035 train_time:87471ms step_avg:53.96ms
step:1622/2035 train_time:87559ms step_avg:53.98ms
step:1623/2035 train_time:87648ms step_avg:54.00ms
step:1624/2035 train_time:87736ms step_avg:54.02ms
step:1625/2035 train_time:87824ms step_avg:54.05ms
step:1626/2035 train_time:87910ms step_avg:54.07ms
step:1627/2035 train_time:87998ms step_avg:54.09ms
step:1628/2035 train_time:88086ms step_avg:54.11ms
step:1629/2035 train_time:88175ms step_avg:54.13ms
step:1630/2035 train_time:88262ms step_avg:54.15ms
step:1631/2035 train_time:88350ms step_avg:54.17ms
step:1632/2035 train_time:88437ms step_avg:54.19ms
step:1633/2035 train_time:88524ms step_avg:54.21ms
step:1634/2035 train_time:88611ms step_avg:54.23ms
step:1635/2035 train_time:88699ms step_avg:54.25ms
step:1636/2035 train_time:88787ms step_avg:54.27ms
step:1637/2035 train_time:88875ms step_avg:54.29ms
step:1638/2035 train_time:88961ms step_avg:54.31ms
step:1639/2035 train_time:89049ms step_avg:54.33ms
step:1640/2035 train_time:89137ms step_avg:54.35ms
step:1641/2035 train_time:89224ms step_avg:54.37ms
step:1642/2035 train_time:89311ms step_avg:54.39ms
step:1643/2035 train_time:89399ms step_avg:54.41ms
step:1644/2035 train_time:89486ms step_avg:54.43ms
step:1645/2035 train_time:89573ms step_avg:54.45ms
step:1646/2035 train_time:89661ms step_avg:54.47ms
step:1647/2035 train_time:89750ms step_avg:54.49ms
step:1648/2035 train_time:89837ms step_avg:54.51ms
step:1649/2035 train_time:89926ms step_avg:54.53ms
step:1650/2035 train_time:90012ms step_avg:54.55ms
step:1651/2035 train_time:90101ms step_avg:54.57ms
step:1652/2035 train_time:90188ms step_avg:54.59ms
step:1653/2035 train_time:90276ms step_avg:54.61ms
step:1654/2035 train_time:90363ms step_avg:54.63ms
step:1655/2035 train_time:90451ms step_avg:54.65ms
step:1656/2035 train_time:90538ms step_avg:54.67ms
step:1657/2035 train_time:90627ms step_avg:54.69ms
step:1658/2035 train_time:90714ms step_avg:54.71ms
step:1659/2035 train_time:90801ms step_avg:54.73ms
step:1660/2035 train_time:90889ms step_avg:54.75ms
step:1661/2035 train_time:90977ms step_avg:54.77ms
step:1662/2035 train_time:91064ms step_avg:54.79ms
step:1663/2035 train_time:91153ms step_avg:54.81ms
step:1664/2035 train_time:91240ms step_avg:54.83ms
step:1665/2035 train_time:91328ms step_avg:54.85ms
step:1666/2035 train_time:91415ms step_avg:54.87ms
step:1667/2035 train_time:91503ms step_avg:54.89ms
step:1668/2035 train_time:91590ms step_avg:54.91ms
step:1669/2035 train_time:91678ms step_avg:54.93ms
step:1670/2035 train_time:91765ms step_avg:54.95ms
step:1671/2035 train_time:91853ms step_avg:54.97ms
step:1672/2035 train_time:91940ms step_avg:54.99ms
step:1673/2035 train_time:92030ms step_avg:55.01ms
step:1674/2035 train_time:92117ms step_avg:55.03ms
step:1675/2035 train_time:92206ms step_avg:55.05ms
step:1676/2035 train_time:92293ms step_avg:55.07ms
step:1677/2035 train_time:92380ms step_avg:55.09ms
step:1678/2035 train_time:92467ms step_avg:55.11ms
step:1679/2035 train_time:92556ms step_avg:55.13ms
step:1680/2035 train_time:92643ms step_avg:55.14ms
step:1681/2035 train_time:92732ms step_avg:55.16ms
step:1682/2035 train_time:92819ms step_avg:55.18ms
step:1683/2035 train_time:92908ms step_avg:55.20ms
step:1684/2035 train_time:92995ms step_avg:55.22ms
step:1685/2035 train_time:93085ms step_avg:55.24ms
step:1686/2035 train_time:93171ms step_avg:55.26ms
step:1687/2035 train_time:93259ms step_avg:55.28ms
step:1688/2035 train_time:93346ms step_avg:55.30ms
step:1689/2035 train_time:93434ms step_avg:55.32ms
step:1690/2035 train_time:93521ms step_avg:55.34ms
step:1691/2035 train_time:93610ms step_avg:55.36ms
step:1692/2035 train_time:93697ms step_avg:55.38ms
step:1693/2035 train_time:93785ms step_avg:55.40ms
step:1694/2035 train_time:93872ms step_avg:55.41ms
step:1695/2035 train_time:93960ms step_avg:55.43ms
step:1696/2035 train_time:94047ms step_avg:55.45ms
step:1697/2035 train_time:94136ms step_avg:55.47ms
step:1698/2035 train_time:94223ms step_avg:55.49ms
step:1699/2035 train_time:94312ms step_avg:55.51ms
step:1700/2035 train_time:94399ms step_avg:55.53ms
step:1701/2035 train_time:94487ms step_avg:55.55ms
step:1702/2035 train_time:94574ms step_avg:55.57ms
step:1703/2035 train_time:94662ms step_avg:55.59ms
step:1704/2035 train_time:94749ms step_avg:55.60ms
step:1705/2035 train_time:94838ms step_avg:55.62ms
step:1706/2035 train_time:94925ms step_avg:55.64ms
step:1707/2035 train_time:95013ms step_avg:55.66ms
step:1708/2035 train_time:95100ms step_avg:55.68ms
step:1709/2035 train_time:95188ms step_avg:55.70ms
step:1710/2035 train_time:95275ms step_avg:55.72ms
step:1711/2035 train_time:95364ms step_avg:55.74ms
step:1712/2035 train_time:95451ms step_avg:55.75ms
step:1713/2035 train_time:95539ms step_avg:55.77ms
step:1714/2035 train_time:95627ms step_avg:55.79ms
step:1715/2035 train_time:95715ms step_avg:55.81ms
step:1716/2035 train_time:95803ms step_avg:55.83ms
step:1717/2035 train_time:95891ms step_avg:55.85ms
step:1718/2035 train_time:95978ms step_avg:55.87ms
step:1719/2035 train_time:96067ms step_avg:55.89ms
step:1720/2035 train_time:96153ms step_avg:55.90ms
step:1721/2035 train_time:96242ms step_avg:55.92ms
step:1722/2035 train_time:96328ms step_avg:55.94ms
step:1723/2035 train_time:96416ms step_avg:55.96ms
step:1724/2035 train_time:96503ms step_avg:55.98ms
step:1725/2035 train_time:96591ms step_avg:55.99ms
step:1726/2035 train_time:96679ms step_avg:56.01ms
step:1727/2035 train_time:96767ms step_avg:56.03ms
step:1728/2035 train_time:96853ms step_avg:56.05ms
step:1729/2035 train_time:96941ms step_avg:56.07ms
step:1730/2035 train_time:97028ms step_avg:56.09ms
step:1731/2035 train_time:97116ms step_avg:56.10ms
step:1732/2035 train_time:97204ms step_avg:56.12ms
step:1733/2035 train_time:97291ms step_avg:56.14ms
step:1734/2035 train_time:97378ms step_avg:56.16ms
step:1735/2035 train_time:97466ms step_avg:56.18ms
step:1736/2035 train_time:97553ms step_avg:56.19ms
step:1737/2035 train_time:97642ms step_avg:56.21ms
step:1738/2035 train_time:97729ms step_avg:56.23ms
step:1739/2035 train_time:97817ms step_avg:56.25ms
step:1740/2035 train_time:97905ms step_avg:56.27ms
step:1741/2035 train_time:97993ms step_avg:56.29ms
step:1742/2035 train_time:98080ms step_avg:56.30ms
step:1743/2035 train_time:98169ms step_avg:56.32ms
step:1744/2035 train_time:98256ms step_avg:56.34ms
step:1745/2035 train_time:98344ms step_avg:56.36ms
step:1746/2035 train_time:98431ms step_avg:56.38ms
step:1747/2035 train_time:98519ms step_avg:56.39ms
step:1748/2035 train_time:98606ms step_avg:56.41ms
step:1749/2035 train_time:98695ms step_avg:56.43ms
step:1750/2035 train_time:98782ms step_avg:56.45ms
step:1750/2035 val_loss:3.3569 train_time:98872ms step_avg:56.50ms
step:1751/2035 train_time:98893ms step_avg:56.48ms
step:1752/2035 train_time:98961ms step_avg:56.48ms
step:1753/2035 train_time:99055ms step_avg:56.51ms
step:1754/2035 train_time:99145ms step_avg:56.52ms
step:1755/2035 train_time:99232ms step_avg:56.54ms
step:1756/2035 train_time:99318ms step_avg:56.56ms
step:1757/2035 train_time:99404ms step_avg:56.58ms
step:1758/2035 train_time:99490ms step_avg:56.59ms
step:1759/2035 train_time:99576ms step_avg:56.61ms
step:1760/2035 train_time:99662ms step_avg:56.63ms
step:1761/2035 train_time:99749ms step_avg:56.64ms
step:1762/2035 train_time:99837ms step_avg:56.66ms
step:1763/2035 train_time:99926ms step_avg:56.68ms
step:1764/2035 train_time:100015ms step_avg:56.70ms
step:1765/2035 train_time:100105ms step_avg:56.72ms
step:1766/2035 train_time:100193ms step_avg:56.73ms
step:1767/2035 train_time:100281ms step_avg:56.75ms
step:1768/2035 train_time:100368ms step_avg:56.77ms
step:1769/2035 train_time:100455ms step_avg:56.79ms
step:1770/2035 train_time:100541ms step_avg:56.80ms
step:1771/2035 train_time:100629ms step_avg:56.82ms
step:1772/2035 train_time:100715ms step_avg:56.84ms
step:1773/2035 train_time:100803ms step_avg:56.85ms
step:1774/2035 train_time:100891ms step_avg:56.87ms
step:1775/2035 train_time:100980ms step_avg:56.89ms
step:1776/2035 train_time:101068ms step_avg:56.91ms
step:1777/2035 train_time:101156ms step_avg:56.93ms
step:1778/2035 train_time:101244ms step_avg:56.94ms
step:1779/2035 train_time:101331ms step_avg:56.96ms
step:1780/2035 train_time:101417ms step_avg:56.98ms
step:1781/2035 train_time:101505ms step_avg:56.99ms
step:1782/2035 train_time:101591ms step_avg:57.01ms
step:1783/2035 train_time:101678ms step_avg:57.03ms
step:1784/2035 train_time:101764ms step_avg:57.04ms
step:1785/2035 train_time:101853ms step_avg:57.06ms
step:1786/2035 train_time:101941ms step_avg:57.08ms
step:1787/2035 train_time:102032ms step_avg:57.10ms
step:1788/2035 train_time:102120ms step_avg:57.11ms
step:1789/2035 train_time:102209ms step_avg:57.13ms
step:1790/2035 train_time:102296ms step_avg:57.15ms
step:1791/2035 train_time:102384ms step_avg:57.17ms
step:1792/2035 train_time:102470ms step_avg:57.18ms
step:1793/2035 train_time:102558ms step_avg:57.20ms
step:1794/2035 train_time:102644ms step_avg:57.22ms
step:1795/2035 train_time:102731ms step_avg:57.23ms
step:1796/2035 train_time:102818ms step_avg:57.25ms
step:1797/2035 train_time:102908ms step_avg:57.27ms
step:1798/2035 train_time:102994ms step_avg:57.28ms
step:1799/2035 train_time:103083ms step_avg:57.30ms
step:1800/2035 train_time:103171ms step_avg:57.32ms
step:1801/2035 train_time:103260ms step_avg:57.33ms
step:1802/2035 train_time:103347ms step_avg:57.35ms
step:1803/2035 train_time:103435ms step_avg:57.37ms
step:1804/2035 train_time:103522ms step_avg:57.38ms
step:1805/2035 train_time:103610ms step_avg:57.40ms
step:1806/2035 train_time:103697ms step_avg:57.42ms
step:1807/2035 train_time:103785ms step_avg:57.44ms
step:1808/2035 train_time:103873ms step_avg:57.45ms
step:1809/2035 train_time:103960ms step_avg:57.47ms
step:1810/2035 train_time:104048ms step_avg:57.49ms
step:1811/2035 train_time:104137ms step_avg:57.50ms
step:1812/2035 train_time:104224ms step_avg:57.52ms
step:1813/2035 train_time:104312ms step_avg:57.54ms
step:1814/2035 train_time:104399ms step_avg:57.55ms
step:1815/2035 train_time:104488ms step_avg:57.57ms
step:1816/2035 train_time:104574ms step_avg:57.58ms
step:1817/2035 train_time:104662ms step_avg:57.60ms
step:1818/2035 train_time:104749ms step_avg:57.62ms
step:1819/2035 train_time:104837ms step_avg:57.63ms
step:1820/2035 train_time:104924ms step_avg:57.65ms
step:1821/2035 train_time:105012ms step_avg:57.67ms
step:1822/2035 train_time:105100ms step_avg:57.68ms
step:1823/2035 train_time:105188ms step_avg:57.70ms
step:1824/2035 train_time:105275ms step_avg:57.72ms
step:1825/2035 train_time:105363ms step_avg:57.73ms
step:1826/2035 train_time:105450ms step_avg:57.75ms
step:1827/2035 train_time:105538ms step_avg:57.77ms
step:1828/2035 train_time:105624ms step_avg:57.78ms
step:1829/2035 train_time:105712ms step_avg:57.80ms
step:1830/2035 train_time:105799ms step_avg:57.81ms
step:1831/2035 train_time:105888ms step_avg:57.83ms
step:1832/2035 train_time:105975ms step_avg:57.85ms
step:1833/2035 train_time:106064ms step_avg:57.86ms
step:1834/2035 train_time:106150ms step_avg:57.88ms
step:1835/2035 train_time:106240ms step_avg:57.90ms
step:1836/2035 train_time:106326ms step_avg:57.91ms
step:1837/2035 train_time:106414ms step_avg:57.93ms
step:1838/2035 train_time:106500ms step_avg:57.94ms
step:1839/2035 train_time:106588ms step_avg:57.96ms
step:1840/2035 train_time:106674ms step_avg:57.98ms
step:1841/2035 train_time:106762ms step_avg:57.99ms
step:1842/2035 train_time:106850ms step_avg:58.01ms
step:1843/2035 train_time:106937ms step_avg:58.02ms
step:1844/2035 train_time:107024ms step_avg:58.04ms
step:1845/2035 train_time:107112ms step_avg:58.06ms
step:1846/2035 train_time:107199ms step_avg:58.07ms
step:1847/2035 train_time:107288ms step_avg:58.09ms
step:1848/2035 train_time:107375ms step_avg:58.10ms
step:1849/2035 train_time:107463ms step_avg:58.12ms
step:1850/2035 train_time:107550ms step_avg:58.14ms
step:1851/2035 train_time:107638ms step_avg:58.15ms
step:1852/2035 train_time:107725ms step_avg:58.17ms
step:1853/2035 train_time:107813ms step_avg:58.18ms
step:1854/2035 train_time:107900ms step_avg:58.20ms
step:1855/2035 train_time:107989ms step_avg:58.21ms
step:1856/2035 train_time:108075ms step_avg:58.23ms
step:1857/2035 train_time:108163ms step_avg:58.25ms
step:1858/2035 train_time:108250ms step_avg:58.26ms
step:1859/2035 train_time:108338ms step_avg:58.28ms
step:1860/2035 train_time:108425ms step_avg:58.29ms
step:1861/2035 train_time:108513ms step_avg:58.31ms
step:1862/2035 train_time:108600ms step_avg:58.32ms
step:1863/2035 train_time:108689ms step_avg:58.34ms
step:1864/2035 train_time:108775ms step_avg:58.36ms
step:1865/2035 train_time:108863ms step_avg:58.37ms
step:1866/2035 train_time:108950ms step_avg:58.39ms
step:1867/2035 train_time:109038ms step_avg:58.40ms
step:1868/2035 train_time:109125ms step_avg:58.42ms
step:1869/2035 train_time:109214ms step_avg:58.43ms
step:1870/2035 train_time:109301ms step_avg:58.45ms
step:1871/2035 train_time:109389ms step_avg:58.47ms
step:1872/2035 train_time:109475ms step_avg:58.48ms
step:1873/2035 train_time:109564ms step_avg:58.50ms
step:1874/2035 train_time:109650ms step_avg:58.51ms
step:1875/2035 train_time:109737ms step_avg:58.53ms
step:1876/2035 train_time:109824ms step_avg:58.54ms
step:1877/2035 train_time:109911ms step_avg:58.56ms
step:1878/2035 train_time:109998ms step_avg:58.57ms
step:1879/2035 train_time:110086ms step_avg:58.59ms
step:1880/2035 train_time:110173ms step_avg:58.60ms
step:1881/2035 train_time:110261ms step_avg:58.62ms
step:1882/2035 train_time:110349ms step_avg:58.63ms
step:1883/2035 train_time:110437ms step_avg:58.65ms
step:1884/2035 train_time:110524ms step_avg:58.66ms
step:1885/2035 train_time:110612ms step_avg:58.68ms
step:1886/2035 train_time:110698ms step_avg:58.69ms
step:1887/2035 train_time:110787ms step_avg:58.71ms
step:1888/2035 train_time:110874ms step_avg:58.73ms
step:1889/2035 train_time:110962ms step_avg:58.74ms
step:1890/2035 train_time:111049ms step_avg:58.76ms
step:1891/2035 train_time:111137ms step_avg:58.77ms
step:1892/2035 train_time:111225ms step_avg:58.79ms
step:1893/2035 train_time:111314ms step_avg:58.80ms
step:1894/2035 train_time:111400ms step_avg:58.82ms
step:1895/2035 train_time:111488ms step_avg:58.83ms
step:1896/2035 train_time:111576ms step_avg:58.85ms
step:1897/2035 train_time:111664ms step_avg:58.86ms
step:1898/2035 train_time:111751ms step_avg:58.88ms
step:1899/2035 train_time:111839ms step_avg:58.89ms
step:1900/2035 train_time:111925ms step_avg:58.91ms
step:1901/2035 train_time:112013ms step_avg:58.92ms
step:1902/2035 train_time:112101ms step_avg:58.94ms
step:1903/2035 train_time:112189ms step_avg:58.95ms
step:1904/2035 train_time:112276ms step_avg:58.97ms
step:1905/2035 train_time:112364ms step_avg:58.98ms
step:1906/2035 train_time:112451ms step_avg:59.00ms
step:1907/2035 train_time:112539ms step_avg:59.01ms
step:1908/2035 train_time:112627ms step_avg:59.03ms
step:1909/2035 train_time:112715ms step_avg:59.04ms
step:1910/2035 train_time:112802ms step_avg:59.06ms
step:1911/2035 train_time:112892ms step_avg:59.07ms
step:1912/2035 train_time:112978ms step_avg:59.09ms
step:1913/2035 train_time:113066ms step_avg:59.10ms
step:1914/2035 train_time:113153ms step_avg:59.12ms
step:1915/2035 train_time:113242ms step_avg:59.13ms
step:1916/2035 train_time:113328ms step_avg:59.15ms
step:1917/2035 train_time:113416ms step_avg:59.16ms
step:1918/2035 train_time:113503ms step_avg:59.18ms
step:1919/2035 train_time:113592ms step_avg:59.19ms
step:1920/2035 train_time:113679ms step_avg:59.21ms
step:1921/2035 train_time:113766ms step_avg:59.22ms
step:1922/2035 train_time:113852ms step_avg:59.24ms
step:1923/2035 train_time:113941ms step_avg:59.25ms
step:1924/2035 train_time:114028ms step_avg:59.27ms
step:1925/2035 train_time:114116ms step_avg:59.28ms
step:1926/2035 train_time:114203ms step_avg:59.30ms
step:1927/2035 train_time:114291ms step_avg:59.31ms
step:1928/2035 train_time:114378ms step_avg:59.32ms
step:1929/2035 train_time:114466ms step_avg:59.34ms
step:1930/2035 train_time:114554ms step_avg:59.35ms
step:1931/2035 train_time:114643ms step_avg:59.37ms
step:1932/2035 train_time:114729ms step_avg:59.38ms
step:1933/2035 train_time:114817ms step_avg:59.40ms
step:1934/2035 train_time:114904ms step_avg:59.41ms
step:1935/2035 train_time:114993ms step_avg:59.43ms
step:1936/2035 train_time:115079ms step_avg:59.44ms
step:1937/2035 train_time:115168ms step_avg:59.46ms
step:1938/2035 train_time:115254ms step_avg:59.47ms
step:1939/2035 train_time:115343ms step_avg:59.49ms
step:1940/2035 train_time:115429ms step_avg:59.50ms
step:1941/2035 train_time:115517ms step_avg:59.51ms
step:1942/2035 train_time:115603ms step_avg:59.53ms
step:1943/2035 train_time:115693ms step_avg:59.54ms
step:1944/2035 train_time:115780ms step_avg:59.56ms
step:1945/2035 train_time:115869ms step_avg:59.57ms
step:1946/2035 train_time:115956ms step_avg:59.59ms
step:1947/2035 train_time:116044ms step_avg:59.60ms
step:1948/2035 train_time:116132ms step_avg:59.62ms
step:1949/2035 train_time:116220ms step_avg:59.63ms
step:1950/2035 train_time:116307ms step_avg:59.64ms
step:1951/2035 train_time:116395ms step_avg:59.66ms
step:1952/2035 train_time:116481ms step_avg:59.67ms
step:1953/2035 train_time:116570ms step_avg:59.69ms
step:1954/2035 train_time:116656ms step_avg:59.70ms
step:1955/2035 train_time:116745ms step_avg:59.72ms
step:1956/2035 train_time:116832ms step_avg:59.73ms
step:1957/2035 train_time:116919ms step_avg:59.74ms
step:1958/2035 train_time:117006ms step_avg:59.76ms
step:1959/2035 train_time:117095ms step_avg:59.77ms
step:1960/2035 train_time:117181ms step_avg:59.79ms
step:1961/2035 train_time:117270ms step_avg:59.80ms
step:1962/2035 train_time:117357ms step_avg:59.81ms
step:1963/2035 train_time:117445ms step_avg:59.83ms
step:1964/2035 train_time:117531ms step_avg:59.84ms
step:1965/2035 train_time:117620ms step_avg:59.86ms
step:1966/2035 train_time:117706ms step_avg:59.87ms
step:1967/2035 train_time:117794ms step_avg:59.89ms
step:1968/2035 train_time:117882ms step_avg:59.90ms
step:1969/2035 train_time:117969ms step_avg:59.91ms
step:1970/2035 train_time:118056ms step_avg:59.93ms
step:1971/2035 train_time:118144ms step_avg:59.94ms
step:1972/2035 train_time:118231ms step_avg:59.95ms
step:1973/2035 train_time:118320ms step_avg:59.97ms
step:1974/2035 train_time:118407ms step_avg:59.98ms
step:1975/2035 train_time:118495ms step_avg:60.00ms
step:1976/2035 train_time:118582ms step_avg:60.01ms
step:1977/2035 train_time:118669ms step_avg:60.02ms
step:1978/2035 train_time:118756ms step_avg:60.04ms
step:1979/2035 train_time:118844ms step_avg:60.05ms
step:1980/2035 train_time:118931ms step_avg:60.07ms
step:1981/2035 train_time:119020ms step_avg:60.08ms
step:1982/2035 train_time:119106ms step_avg:60.09ms
step:1983/2035 train_time:119194ms step_avg:60.11ms
step:1984/2035 train_time:119281ms step_avg:60.12ms
step:1985/2035 train_time:119370ms step_avg:60.14ms
step:1986/2035 train_time:119457ms step_avg:60.15ms
step:1987/2035 train_time:119545ms step_avg:60.16ms
step:1988/2035 train_time:119632ms step_avg:60.18ms
step:1989/2035 train_time:119720ms step_avg:60.19ms
step:1990/2035 train_time:119807ms step_avg:60.20ms
step:1991/2035 train_time:119895ms step_avg:60.22ms
step:1992/2035 train_time:119982ms step_avg:60.23ms
step:1993/2035 train_time:120071ms step_avg:60.25ms
step:1994/2035 train_time:120158ms step_avg:60.26ms
step:1995/2035 train_time:120246ms step_avg:60.27ms
step:1996/2035 train_time:120333ms step_avg:60.29ms
step:1997/2035 train_time:120421ms step_avg:60.30ms
step:1998/2035 train_time:120508ms step_avg:60.31ms
step:1999/2035 train_time:120596ms step_avg:60.33ms
step:2000/2035 train_time:120683ms step_avg:60.34ms
step:2000/2035 val_loss:3.2847 train_time:120774ms step_avg:60.39ms
step:2001/2035 train_time:120795ms step_avg:60.37ms
step:2002/2035 train_time:120863ms step_avg:60.37ms
step:2003/2035 train_time:120955ms step_avg:60.39ms
step:2004/2035 train_time:121042ms step_avg:60.40ms
step:2005/2035 train_time:121130ms step_avg:60.41ms
step:2006/2035 train_time:121216ms step_avg:60.43ms
step:2007/2035 train_time:121303ms step_avg:60.44ms
step:2008/2035 train_time:121390ms step_avg:60.45ms
step:2009/2035 train_time:121478ms step_avg:60.47ms
step:2010/2035 train_time:121565ms step_avg:60.48ms
step:2011/2035 train_time:121652ms step_avg:60.49ms
step:2012/2035 train_time:121742ms step_avg:60.51ms
step:2013/2035 train_time:121832ms step_avg:60.52ms
step:2014/2035 train_time:121920ms step_avg:60.54ms
step:2015/2035 train_time:122009ms step_avg:60.55ms
step:2016/2035 train_time:122096ms step_avg:60.56ms
step:2017/2035 train_time:122184ms step_avg:60.58ms
step:2018/2035 train_time:122270ms step_avg:60.59ms
step:2019/2035 train_time:122358ms step_avg:60.60ms
step:2020/2035 train_time:122445ms step_avg:60.62ms
step:2021/2035 train_time:122532ms step_avg:60.63ms
step:2022/2035 train_time:122619ms step_avg:60.64ms
step:2023/2035 train_time:122708ms step_avg:60.66ms
step:2024/2035 train_time:122797ms step_avg:60.67ms
step:2025/2035 train_time:122886ms step_avg:60.68ms
step:2026/2035 train_time:122975ms step_avg:60.70ms
step:2027/2035 train_time:123063ms step_avg:60.71ms
step:2028/2035 train_time:123150ms step_avg:60.73ms
step:2029/2035 train_time:123238ms step_avg:60.74ms
step:2030/2035 train_time:123324ms step_avg:60.75ms
step:2031/2035 train_time:123412ms step_avg:60.76ms
step:2032/2035 train_time:123500ms step_avg:60.78ms
step:2033/2035 train_time:123587ms step_avg:60.79ms
step:2034/2035 train_time:123675ms step_avg:60.80ms
step:2035/2035 train_time:123765ms step_avg:60.82ms
step:2035/2035 val_loss:3.2777 train_time:123853ms step_avg:60.86ms
peak memory allocated: 29958 MiB reserved: 38816 MiB

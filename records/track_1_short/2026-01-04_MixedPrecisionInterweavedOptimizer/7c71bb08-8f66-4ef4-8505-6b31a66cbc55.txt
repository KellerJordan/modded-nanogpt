import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from collections import defaultdict
from itertools import accumulate
from pathlib import Path
import gc

os.environ["PYTORCH_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device=f"cuda:{os.environ['LOCAL_RANK']}", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng

@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor, split_baddbmm: bool = False):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    # Select batched vs unbatched
    if split_baddbmm:
        BX_matmul = torch.bmm if X.ndim > 2 else torch.mm
    else:
        aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A

        # Referencing X twice causes pytorch to make a defensive copy,
        # resulting in a cudaMemcpyAsync in baddbmm.
        # For large matrices (i.e., the mlp weights), it's faster to split
        # the operation into two kernels to avoid this.
        if split_baddbmm:
            BX_matmul(B, X, out=C)  # C = B @ X
            C.add_(X, alpha=a)      # C = C + a*X  (in-place, X only read)
        else:
            aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X

        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X


# -----------------------------------------------------------------------------
# Compiled helpers for NorMuon by @chrisjmccormick

@torch.compile(dynamic=False, fullgraph=True)
def cautious_wd_and_update_inplace(p, mantissa, grad, wd_tensor, lr_tensor):
    """
    Cautious weight decay + parameter update. wd_tensor and lr_tensor are 0-D CPU tensors.
    Mantissa is tracked to enable higher precision updates on bfloat16 parameters.
    bfloat16 format: 1 sign bit + 8 exponent bits + 7 mantissa bits = 16 bits total
    float32 format: 1 sign bit + 8 exponent bits + 23 mantissa bits = 32 bits total
    """
    assert p.dtype == mantissa.dtype == torch.uint16
    grad = grad.float()
    wd_factor = wd_tensor.to(torch.float32)
    lr_factor = lr_tensor.to(torch.float32)
    p_precise_raw = (p.to(torch.uint32) << 16) | mantissa.to(torch.uint32)
    p_precise = p_precise_raw.view(torch.float32)
    mask = (grad * p_precise) >= 0
    p_precise.copy_(p_precise - (p_precise * mask * wd_factor * lr_factor) - (grad * lr_factor))
    p.copy_((p_precise_raw >> 16).to(torch.uint16))
    mantissa.copy_(p_precise_raw.to(torch.uint16))

@torch.compile(dynamic=False, fullgraph=True)
def apply_normuon_variance_reduction(v_chunk, second_momentum_buffer, beta2, red_dim):
    """NorMuon variance reduction. Algebraically fuses the normalization steps to minimize memory ops."""
    v_mean = v_chunk.float().square().mean(dim=red_dim, keepdim=True)
    red_dim_size = v_chunk.size(red_dim)
    v_norm_sq = v_mean.sum(dim=(-2, -1), keepdim=True).mul_(red_dim_size)
    v_norm = v_norm_sq.sqrt_()
    second_momentum_buffer.lerp_(v_mean.to(dtype=second_momentum_buffer.dtype), 1 - beta2)
    step_size = second_momentum_buffer.clamp_min(1e-10).rsqrt_()
    scaled_sq_sum = (v_mean * red_dim_size) * step_size.float().square()
    v_norm_new = scaled_sq_sum.sum(dim=(-2, -1), keepdim=True).sqrt_()
    final_scale = step_size * (v_norm / v_norm_new.clamp_min_(1e-10))
    return v_chunk.mul_(final_scale.type_as(v_chunk))


# -----------------------------------------------------------------------------
# NorMuon optimizer

class NorMuon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).

    Differences from standard Muon:
    - Newton-Shulz is replaced with Polar Express for the orthogonalization step
    - NorMuon adds a low-rank variance estimator similar to Adafactor. https://arxiv.org/pdf/2510.05491
    - small 1D parameters handled here instead of in Adam
    - Cautious weight decay, a gated version of decoupled weight decay
    - Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as
    needed on the forward pass. This enables attn and mlp weights to be contained within the same
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn.
    The scheduling is:
        1. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        2. reduce scatter attn/mlp round 2 (16 mlp params)
        3. wait on step 1, then compute update of 1 and schedule all gather
        4. wait on step 2, then compute update of 2 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        5. wait for each all gather to complete and update params
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, beta2=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, beta2=beta2)
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def reset(self):
        # expose a reset for clearing buffers
        for group in self.param_groups:
            if "momentum_buffer" in group:
                group["momentum_buffer"].zero_()
                group["mantissa"].zero_()
                group["second_momentum_buffer"].zero_()

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per module.
        """
        groups = defaultdict(list)
        for param in params:
            groups[param.label].append(param)

        param_groups = []
        for module_name, group_params in groups.items():
            chunk_size = (len(group_params) + self.world_size - 1) // self.world_size
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))

        return param_groups

    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn
        and mlp params when a param group is split across GPUs.
        """
        params_list = list(params)
        module_group_order = ['attn', 'mlp']
        group_sizes = [16, 16]  # 10 attn + 6 mlp, then 16 mlp
        params_list.sort(key=lambda x: module_group_order.index(x.label))

        idx = 0
        assert len(params_list) == sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            chunk_size = (size + self.world_size - 1) // self.world_size
            group_params = params_list[idx: idx + size]
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))
            idx += size

        return param_groups

    def step(self):
        self.step_p1()
        self.step_p2()
        self.step_p3()
        
    @torch.no_grad()
    def step_p1(self):
        """
        Part 1: Launch distributed reduce_scatter operations for parameter groups.
        """
        rank = dist.get_rank()
        self.group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            stacked_grads = torch.empty(
                (padded_num_params, *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device
            )
            for i, p in enumerate(params):
                stacked_grads[i].copy_(p.grad, non_blocking=True)
            if len(params) < padded_num_params:
                stacked_grads[len(params):].zero_()

            grad_chunk = torch.empty_like(stacked_grads[:chunk_size])

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            self.group_infos.append(dict(grad_chunk=grad_chunk, reduce_future=reduce_future))
    
    @torch.no_grad()
    def step_p2(self):
        """
        Part 2: Compute gradient updates and launch all_gather operations
        Wait for all gather to complete for all parameter groups except final one
        """
        # Efficient distributed step by @YouJiacheng, @KonstantinWilleke, @alexrgilbert,
        # @adricarda, @tuttyfrutyee, @vdlad, @ryanyang0, @vagrawal, @varunneal, @chrisjmccormick
        rank = dist.get_rank()
        group_infos = self.group_infos
        
        self.all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = group["params"]
            grad_chunk = info["grad_chunk"].float()
            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            start_idx = rank * chunk_size
            module_idx = start_idx if start_idx < len(params) else 0

            num_params = min(chunk_size, max(0, len(params) - start_idx))  # num params for this rank

            if "momentum_buffer" not in group:
                group["momentum_buffer"]  = torch.zeros_like(grad_chunk[:num_params], dtype=torch.float32)
                
            momentum_buffer = group["momentum_buffer"]
            # Apply momentum update to the persistent momentum buffer in-place
            momentum_buffer.lerp_(grad_chunk[:num_params], 1 - group["momentum"])
            updated_grads = grad_chunk[:num_params].lerp_(momentum_buffer, group["momentum"])

            grad_shape = updated_grads.shape
            if params[module_idx].label == 'attn':
                for p in params[module_idx:module_idx + num_params]:
                    assert p.label == 'attn'
                updated_grads = updated_grads.view(4 * grad_shape[0], grad_shape[1] // 4, grad_shape[2])

            ref_param = params[module_idx]
            param_shape = ref_param.shape

            if "second_momentum_buffer" not in group:
                group["second_momentum_buffer"] = (torch.zeros_like(updated_grads[..., :, :1], dtype=torch.float32)
                    if param_shape[-2] >= param_shape[-1] else torch.zeros_like(updated_grads[..., :1, :])
                )
            second_momentum_buffer = group["second_momentum_buffer"]

            if "param_lr_cpu" not in group:
                # Define multipliers for ALL params in this group (global, not per-shard)
                lr_mults = []
                wd_mults = []
                for p in params:
                    # Increase learning rate for modules with larger inputs than outputs.
                    # This shape check also assumes rows=input, columns=output, so take care
                    # when changing memory layouts. @chrisjmccormick
                    shape = p.shape
                    if len(shape) >= 2:
                        shape_mult = max(1.0, shape[-2] / shape[-1]) ** 0.5
                    else:
                        shape_mult = 1.0
                    lr_mults.append(shape_mult * getattr(p, "lr_mul", 1.0))
                    wd_mults.append(getattr(p, "wd_mul", 1.0))
                # Define as cpu tensors to enable Inductor constant folding
                group["param_lr_cpu"] = torch.tensor(lr_mults, dtype=torch.float32, device="cpu")
                group["param_wd_cpu"] = torch.tensor(wd_mults, dtype=torch.float32, device="cpu")

            eff_lr_all = group["param_lr_cpu"] * group["lr"]
            eff_wd_all = group["param_wd_cpu"] * group["weight_decay"] * group["lr"]

            # Slice the portion corresponding to this rank's shard
            eff_lr_cpu = eff_lr_all[module_idx:module_idx + num_params]
            eff_wd_cpu = eff_wd_all[module_idx:module_idx + num_params]

            # Compute zeropower for the entire chunk in a single, batched call.
            if num_params == 0:
                v_chunk = updated_grads
            else:
                v_chunk = polar_express(updated_grads, split_baddbmm=(ref_param.label == 'mlp'))

            # Note that the head orientation in O is transposed relative to QKV, so red_dim
            # is 'incorrect' for O. However, correcting this showed no improvement. @chrisjmccormick
            red_dim = -1 if param_shape[-2] >= param_shape[-1] else -2

            v_chunk = apply_normuon_variance_reduction(
                v_chunk, second_momentum_buffer, group["beta2"], red_dim
            )

            v_chunk = v_chunk.view(grad_shape)

            # # "Cautious" weight decay (https://arxiv.org/abs/2510.12402)
            updated_params = torch.empty_like(grad_chunk, dtype=torch.bfloat16)
            if num_params > 0:
                # Work on a stacked copy to avoid touching original params
                param_chunk = torch.stack(params[module_idx:module_idx + num_params])

                if "mantissa" not in group:
                    group["mantissa"] = torch.zeros_like(param_chunk, dtype=torch.uint16)
                mantissa = group["mantissa"]

                for local_idx in range(num_params):
                    cautious_wd_and_update_inplace(
                        param_chunk[local_idx].view(torch.uint16),
                        mantissa[local_idx],
                        v_chunk[local_idx],
                        eff_wd_cpu[local_idx],
                        eff_lr_cpu[local_idx]
                    )
            else:
                param_chunk = torch.zeros_like(v_chunk)

            updated_params[:num_params].copy_(param_chunk)
            if num_params < chunk_size:
                updated_params[num_params:].zero_()

            stacked_params = torch.empty(
                (padded_num_params, *param_shape),
                dtype=updated_params.dtype,
                device=updated_params.device,
            )

            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_params, async_op=True
            ).get_future()

            self.all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete for all except final and copy results back into original parameter tensors.
        for info in self.all_gather_infos[:-1]:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)

    @torch.no_grad()
    def step_p3(self):
        """
        Part 3: Wait for final all gather to complete and copy results back into original parameter tensors
        """
        info = self.all_gather_infos[-1]
        info["gather_future"].wait()
        stacked_params = info["stacked_params"]
        orig_params = info["orig_params"]

        unstacked_params = torch.unbind(stacked_params)
        for i, p in enumerate(orig_params):
            p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, label_order: list[str], betas: list[list[float]], lr: float = 1e-3, eps: float = 1e-8, weight_decay: float = 0.01):
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        defaults = dict(lr=lr, eps=eps, weight_decay=weight_decay)
        params = list(params)
        # Group by label, with explicit ordering for execution control.
        params_by_label = defaultdict(list)
        for p in params:
            params_by_label[getattr(p, 'label', None)].append(p)
        param_groups = []
        for idx, label in enumerate(label_order):
            if label in params_by_label:
                param_groups.append(dict(params=params_by_label[label], betas=betas[idx]))
        # include any unlabeled params at the end (processed last)
        if None in params_by_label:
            param_groups.append(dict(params=params_by_label[None]))
        super().__init__(param_groups, defaults)
        # init state: small params (numel < 1024) use full-sized state, others use sharded
        for p in params:
            chunk = p if p.numel() < 1024 else p[:p.size(0) // self.world_size]
            exp_avg = torch.zeros_like(chunk, dtype=torch.float32, device=p.device)
            self.state[p] = dict(step=0, exp_avg=exp_avg, exp_avg_sq=torch.zeros_like(exp_avg))

        # tag the final param for optimizer pipelining, run all gather after muon copy
        param_groups[-1]['params'][-1].is_final_param = True
        
        # DistributedAdam implementation by @vagrawal, @akash5474
        self.should_sync = False
        self._reduce_scatter_hooks = []
        self._reduce_scatter_futures = {}
        # 0-D CPU tensors to avoid recompilation in _update_step
        self._step_size_t = torch.tensor(0.0, dtype=torch.float32, device="cpu")
        self._eff_wd_t = torch.tensor(0.0, dtype=torch.float32, device="cpu")
        self.register_backward_hooks()

    def register_backward_hooks(self):
        for group in self.param_groups:
            for param in group["params"]:
                self._reduce_scatter_hooks.append(param.register_post_accumulate_grad_hook(self._sync_gradient))

    def load_state_dict(self, state_dict):
        """Override to preserve optimizer state dtypes (avoid BFloat16->Float32 cast that causes recompilation)."""
        # Save original state dtypes before loading
        original_dtypes = {}
        for p, s in self.state.items():
            original_dtypes[p] = {k: v.dtype for k, v in s.items() if isinstance(v, torch.Tensor)}
        
        # Call parent load_state_dict (which may cast dtypes to match param dtype)
        super().load_state_dict(state_dict)
        
        # Restore original dtypes
        for p, s in self.state.items():
            if p in original_dtypes:
                for k, v in s.items():
                    if isinstance(v, torch.Tensor) and k in original_dtypes[p]:
                        if v.dtype != original_dtypes[p][k]:
                            s[k] = v.to(original_dtypes[p][k])

    @torch.no_grad()
    def _sync_gradient(self, param):
        if not self.should_sync:
            return

        grad = param.grad
        if param.numel() < 1024:
            # Small params: use all_reduce (no scatter/gather needed)
            self._reduce_scatter_futures[param] = (
                dist.all_reduce(grad, op=dist.ReduceOp.AVG, async_op=True).get_future(),
                grad
            )
        else:
            rank_size = grad.shape[0] // self.world_size
            if grad is not None:
                grad_slice = torch.empty_like(grad[:rank_size])
                self._reduce_scatter_futures[param] = (
                    dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future(),
                    grad_slice
                )

    def copy_lm_to_embed(self):
        # run at 2/3 of training
        lm_head = self.param_groups[0]['params'][0]
        embed = self.param_groups[-2]['params'][0]
        lm_head_state = self.state[lm_head]
        embed_state = self.state[embed]
        embed_state['step'] = lm_head_state['step']
        embed_state['exp_avg'] = lm_head_state['exp_avg'].clone()
        embed_state['exp_avg_sq'] = lm_head_state['exp_avg_sq'].clone()
        embed.data.copy_(lm_head.data)

    @staticmethod
    @torch.compile(dynamic=False, fullgraph=True)
    def _update_step(p_slice, g_slice, exp_avg, exp_avg_sq, beta1, beta2, eps, step_size_t, eff_wd_t):
        """Compiled Adam update step. step_size_t and eff_wd_t are 0-D CPU tensors to avoid recompilation."""
        exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)  # exp_avg = beta1 * exp_avg + (1 - beta1) * g_slice
        exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)  # exp_avg_sq = beta2 * exp_avg_sq + (1 - beta2) * g_slice^2
        # compute step
        update = exp_avg.div(exp_avg_sq.sqrt().add_(eps)).mul_(step_size_t)  # update = (exp_avg / (sqrt(exp_avg_sq) + eps)) * step_size
        # cautious weight decay
        mask = (update * p_slice) > 0
        update.addcmul_(p_slice, mask, value=eff_wd_t)  # update += eff_wd_t * p_slice * mask
        p_slice.add_(other=update, alpha=-1.0)  # p_slice -= update

    @torch.no_grad()
    def step(self, muon_opt):
        muon_opt.step_p1()
        rank = dist.get_rank()
        all_gather_futures: list[torch.Future] = []

        last_param = None
        last_p_slice = None
        for group in self.param_groups:      
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            for param in group['params']:
                if param not in self._reduce_scatter_futures:
                    continue

                fut, g_slice = self._reduce_scatter_futures[param]
                fut.wait()

                is_small = param.numel() < 1024
                if is_small:
                    # Small params: g_slice is actually full grad, p_slice is full param
                    p_slice = param
                else:
                    rank_size = param.shape[0] // self.world_size
                    p_slice = param[rank * rank_size:(rank + 1) * rank_size]

                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]
                state["step"] += 1
                t = state["step"]
                
                # Pre-compute changing values as 0-D CPU tensors to avoid recompilation.
                # `.fill_(value)` is the same as "= value", but doesn't change the tensor object.
                bias1, bias2 = 1 - beta1 ** t, 1 - beta2 ** t
                self._step_size_t.fill_(lr * (bias2 ** 0.5 / bias1))
                self._eff_wd_t.fill_(lr * lr * wd * getattr(param, "wd_mul", 1.0)) # `lr` included twice to serve as weight decay schedule.

                DistAdam._update_step(p_slice, g_slice, state["exp_avg"], state["exp_avg_sq"],
                                      beta1, beta2, eps, self._step_size_t, self._eff_wd_t)

                if not is_small:
                    if getattr(param, "is_final_param", False):
                        last_param = param
                        last_p_slice = p_slice
                    else:
                        all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())
        self._reduce_scatter_futures.clear()

        muon_opt.step_p2()
        torch.futures.collect_all(all_gather_futures).wait()

        if last_param is not None:
            last_all_gather_future = dist.all_gather_into_tensor(last_param, last_p_slice, async_op=True).get_future()
        muon_opt.step_p3()
        torch.futures.collect_all([last_all_gather_future]).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        with torch.no_grad():
            self.weight.zero_()  # @Grad62304977 and others

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))


# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float
    key_offset: bool
    attn_gate_w: torch.Tensor
    ve_gate_w: torch.Tensor

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = self.dim ** -0.5
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKVO weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # Simplified layout by @chrisjmccormick
        self.qkvo_w = nn.Parameter(torch.empty(self.dim * 4, self.hdim, dtype=torch.bfloat16))
        # label all modules for explicit optimizer grouping
        self.qkvo_w.label = 'attn'

        with torch.no_grad():
            self.qkvo_w[:self.dim * 3].uniform_(-bound, bound)  # init QKV weights
            self.qkvo_w[self.dim * 3:].zero_()  # init O weights to zero

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas, key_offset = attn_args.ve, attn_args.sa_lambdas, attn_args.key_offset
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size
        # sparse gated attention to enable context based no-op by @classiclarryd
        # only include gates on layers with value embeds used on forward pass
        attn_gate_w, ve_gate_w = attn_args.attn_gate_w, attn_args.ve_gate_w

        q, k, v = F.linear(x, sa_lambdas[0] * self.qkvo_w[:self.dim * 3].type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if key_offset:
            # shift keys forward for the stationary head dims. Enables 1-layer induction.
            k[:, 1:, :, self.head_dim // 4:self.head_dim // 2] = k[:, :-1, :, self.head_dim // 4:self.head_dim // 2]
            k[:, 1:, :, 3 * self.head_dim // 4:] = k[:, :-1, :, 3 * self.head_dim // 4:]
        if ve is not None:
            ve_gate_out = 2 * torch.sigmoid(F.linear(x[..., :12], ve_gate_w)).view(B, T, self.num_heads, 1)
            v = v + ve_gate_out * ve.view_as(v) # @ KoszarskyB & @Grad62304977

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens,
                                                        max_seqlen_q=max_len, max_seqlen_k=max_len,
                                                        causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(F.linear(x[..., :12], attn_gate_w)).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, sa_lambdas[1] * self.qkvo_w[self.dim * 3:].type_as(y))  # sa_lambdas[1] pre-multiplied to O @shenberg
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # Transposed layout to match attention weights
        self.c_fc = nn.Parameter(torch.empty(hdim, dim, dtype=torch.bfloat16))
        self.c_proj = nn.Parameter(torch.empty(hdim, dim, dtype=torch.bfloat16))
        # label all modules for explicit optimizer grouping
        self.c_fc.label = 'mlp'
        self.c_proj.label = 'mlp'
        self.c_proj.lr_mul = 2.

        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.T.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.6 (the 7th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads, layer_idx) if layer_idx != 6 else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim)

    def forward(self, x: Tensor, attn_args: AttnArgs):
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

@dataclass
class ForwardScheduleConfig:
    mtp_weights: torch.Tensor
    ws_short: int
    ws_long: int

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        self.num_layers = num_layers
        vocab_size = next_multiple_of_n(vocab_size, n=128)

        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.label = 'smear_gate'
        self.smear_gate.weight.lr_mul = 0.01
        self.smear_gate.weight.wd_mul = 0.0

        self.skip_gate = CastedLinear(12, 1)
        self.skip_gate.weight.label = 'skip_gate'
        self.skip_gate.weight.lr_mul = 0.05
        self.skip_gate.weight.wd_mul = 0.0

        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        for embed in self.value_embeds:
            nn.init.zeros_(embed.weight)
        for ve in self.value_embeds:
            ve.weight.label = 'value_embed'
        
        # parameter banks for attention and value embedding gate weights
        self.attn_gate_bank = nn.Parameter(torch.zeros(10, num_heads, 12)) # 10 layers
        self.attn_gate_bank.label = 'attn_gate_bank'
        self.ve_gate_bank = nn.Parameter(torch.zeros(5, num_heads, 12)) # 5 layers
        self.ve_gate_bank.label = 've_gate_bank'

        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)

        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=100/448, w_s=1.6/448, grad_s=0.75/448)
        nn.init.normal_(self.lm_head.weight, mean=0, std=0.005)
        self.lm_head.weight.label = 'lm_head'

        self.embed = nn.Embedding(vocab_size, model_dim)
        self.embed.weight.label = 'embed'

        # x0_lambdas separated out for different optimizer treatment (no beta smoothing)
        self.x0_lambdas = nn.Parameter(torch.zeros(num_layers))
        self.x0_lambdas.label = 'x0_lambdas'
        self.x0_lambdas.lr_mul = 5.0
        self.x0_lambdas.wd_mul = 0.0

        pad = (-num_layers * 3 - 3) % dist.get_world_size()  # updated: 3*num_layers instead of 4*
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    1.1 * torch.ones(num_layers),  # resid lambdas. 1.1 init such that layer i weight is i^(num_layers-i).
                    *[torch.tensor([0.5, 1.0]) for _ in range(num_layers)],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    -1.5 * torch.ones(1),  # skip_lambda -> σ(-1.5) ≈ 0.18
                    torch.ones(pad),
                ]
            )
        )

        self.scalars.label = 'scalars'
        # set learning rates
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
            param.wd_mul = 5.
        for param in self.embed.parameters():
            param.wd_mul = 150.
        for param in self.lm_head.parameters():
            param.wd_mul = 150.
        self.scalars.lr_mul = 5.0
        self.scalars.wd_mul = 0.0

        self.split_embed = False

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, schedule_cfg: ForwardScheduleConfig):
        assert input_seq.ndim == 1

        # unpack schedule_cfg
        mtp_weights, ws_short, ws_long = schedule_cfg.mtp_weights, schedule_cfg.ws_short, schedule_cfg.ws_long

        # set configs
        skip_connections = []
        skip_in = [3] # long attention window on layer 3
        skip_out = [6] # no attn op on layer 6
        x_backout = None
        backout_layer = 7

        # set lambdas
        resid_lambdas = self.scalars[: 1 * self.num_layers]
        x0_lambdas = self.x0_lambdas
        sa_lambdas = self.scalars[1 * self.num_layers: 3 * self.num_layers].view(-1, 2)
        smear_lambda = self.scalars[3 * self.num_layers]
        backout_lambda = self.scalars[3 * self.num_layers+1]
        skip_lambda = self.scalars[3 * self.num_layers+2]

        # set block masks and key shift
        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == self.num_layers
        key_offset = [b==long_bm for b in bm_sizes] # apply partial key offset to long windows

        # weight-tied: use lm_head.weight for embedding lookup (or separate embed after split)
        if self.split_embed:
            x = self.embed(input_seq)
        else:
            x = F.embedding(input_seq, self.lm_head.weight)
        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [ve[1], ve[2]] + [None] * (self.num_layers - 5) + [ve[0], ve[1], ve[2]]
        assert len(ve) == self.num_layers

        # smear token embed forward 1 position @classiclarryd
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # unbind gate banks to avoid select_backwards kernel
        ag = [w.bfloat16() for w in self.attn_gate_bank.unbind(0)] 
        veg = [w.bfloat16() for w in self.ve_gate_bank.unbind(0)]
        attn_gates = ag[:6] + [None] + ag[6:]
        ve_gates = [veg[0], veg[1]] + [None] * (self.num_layers - 5) + [veg[2], veg[3], veg[4]]
        assert len(attn_gates) == self.num_layers
        assert len(ve_gates) == self.num_layers

        for i in range(self.num_layers):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale,
                key_offset=key_offset[i],
                attn_gate_w=attn_gates[i],
                ve_gate_w=ve_gates[i]
            )
            if i in skip_out:
                skip_gate_out = torch.sigmoid(skip_lambda) * 2 * torch.sigmoid(self.skip_gate(x0[..., :self.skip_gate.weight.size(-1)]))
                x = x + skip_gate_out * skip_connections.pop()
            if i == 0:
                x = (resid_lambdas[0] + x0_lambdas[0]) * x
            else:
                x = resid_lambdas[i] * x + x0_lambdas[i] * x0
            x = self.blocks[i](x, attn_args)
            if i in skip_in:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 7 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15
        # @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1). @classiclarryd updated to 23*sigmoid((logits+5)/7.5)
        logits = 23 * torch.sigmoid((logits + 5) / 7.5)
        logits_for_loss = logits.float() if not self.training else logits

        n_predict = mtp_weights.size(0) if mtp_weights is not None else 1
        if self.training and n_predict > 1:
            # Multi-token prediction: take loss of the weighted average of next n_predict tokens
            logits_flat = logits_for_loss.view(-1, logits_for_loss.size(-1))
            idx = F.pad(target_seq, (0, n_predict - 1)).unfold(0, n_predict, 1)  # [T, n_predict] of shifted targets
            target_logits = logits_flat.gather(1, idx)
            cross_entropy = torch.logsumexp(logits_flat, dim=-1).unsqueeze(1) - target_logits
            for k in range(1, n_predict):  # zero out preds past end of sequence
                cross_entropy[-k:, k] = 0
            loss = (cross_entropy * mtp_weights).sum()
        elif self.training:
            loss = F.cross_entropy(logits_for_loss.view(-1, logits_for_loss.size(-1)), target_seq, reduction="sum")
        else:
            loss = F.cross_entropy(logits_for_loss.view(-1, logits_for_loss.size(-1)), target_seq, reduction="mean")
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()

    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        # Cast to int32 on CPU before transfer to avoid dtype conversion during .to()
        _inputs = _inputs.to(dtype=torch.int32)
        _targets = _targets.to(dtype=torch.int64)
        _cum_lengths = _cum_lengths.to(dtype=torch.int32)

        new_params = yield (
            _inputs.to(device="cuda", non_blocking=True),
            _targets.to(device="cuda", non_blocking=True),
            _cum_lengths.to(device="cuda", non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * new_grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens // new_grad_accum_steps
            max_seq_len = new_max_seq_len

# -----------------------------------------------------------------------------
# Training Management
 
def get_bs(step: int):
    if step >= args.num_scheduled_iterations:
        return args.train_bs_extension
    x = step / args.num_scheduled_iterations
    bs_idx = int(len(args.train_bs_schedule) * x)
    return args.train_bs_schedule[bs_idx]

def get_ws(step: int):
    # set short window size to half of long window size
    # Higher ws on "extension" steps
    if step >= args.num_scheduled_iterations:
        return args.ws_final // 2, args.ws_final
    x = step / args.num_scheduled_iterations
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

# learning rate schedule: tied to batch size schedule, with cooldown at the end.
def get_lr(step: int):
    if step > args.num_scheduled_iterations:
        return 0.1
    lr_max = 1.0
    x = step / args.num_scheduled_iterations
    if x > 1/3:
       lr_max = 1.52  # (16/8)**0.6
    if x > 2/3:
        lr_max = 1.73  # (24/8)**0.5
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = lr_max * w + (1 - w) * 0.1
        return lr
    return lr_max

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

class TrainingManager():
    """
    Manages three optimizers for Adam embed/lm_head, Adam scalars, and Muon weight matrices.
    Notable Features:
        1. Scalars are given higher momentum terms to smooth learning @ChrisJMcCormick
        2. Scalar weights are temporarily frozen during batch size or window size updates @ChrisJMcCormick
        3. Adam optimizers are only stepped on odd steps @classiclarryd
        4. Adam optimizers have hooks to start gradient communication during backwards pass @akash5474
        5. Muon has a linear momentum warmup and cooldown schedule
        6. Learning rates follow a linear decay schedule
        7. Embed/lm_head weights and optimizer state splits at 2/3 of training @classiclarryd

    Manages model architecture, data, and target that changes during training
    Notable Features:
        1. Multi Token Prediction schedule of [1, 0.5, 0.25->0] -> [1, 0.5->0] -> [1] @varunneal
        2. Sliding Attention window schedule of [1,3] -> [3,7] -> [5,11] -> [6,13]
        3. YaRN updates to RoPE on window changes
        4. Split embed and lm head at 2/3 of training
        5. Batch size schedule of 8 -> 16 -> 24
        6. Post training extension of long windows from 13 to 20
    """
    def __init__(self, model):
        self.mtp_weights_schedule = self._build_mtp_schedule()
        self.model = model
        adam_betas = {
            'lm_head': [0.5, 0.95],
            'smear_gate': [0.9, 0.99],
            'attn_gate_bank': [0.9, 0.99],
            've_gate_bank': [0.9, 0.99],
            'skip_gate': [0.9, 0.99],
            'x0_lambdas': [0.65, 0.95],
            'scalars': [0.9, 0.99],
            'embed': [0.5, 0.95],
            'value_embed': [0.75, 0.95]
        }
        adam_labels = list(adam_betas.keys())
        adam_beta_values = list(adam_betas.values())
        muon_labels = ['attn', 'mlp']
        adam_params = [p for p in model.parameters() if getattr(p, 'label', None) in adam_labels]
        muon_params = [p for p in model.parameters() if getattr(p, 'label', None) in muon_labels]
        assert set(getattr(p, 'label', None) for p in model.parameters()) == set(adam_labels + muon_labels), "All params must have label"

        self.adam_opt = DistAdam(adam_params, adam_labels, adam_beta_values, lr=0.008, eps=1e-10, weight_decay=0.005)
        self.muon_opt = NorMuon(muon_params, lr=0.023, momentum=0.95, beta2=0.95, weight_decay=1.2)
        self.optimizers = [self.adam_opt, self.muon_opt]

        # split after odd number step
        self.split_step = math.ceil(args.split_embed_frac * args.num_scheduled_iterations) | 1

        # set defaults
        for opt in self.optimizers:
            opt.odd_step_only = False 
            opt.should_sync = True
            for group in opt.param_groups:
                group["initial_lr"] = group["lr"]

        # on even steps, only step Muon params
        self.adam_opt.odd_step_only = True

        self.reset()

    def _build_mtp_schedule(self):
        # Precompute MTP weights for all steps to avoid tensor allocation during training
        # Schedule: [1, 0.5, 0.25->0] -> [1, 0.5->0] -> [1]
        mtp_weights_schedule = []
        for s in range(args.num_iterations + 1):
            x = s / args.num_scheduled_iterations
            if x < 1/3:
                w = [1.0, 0.5, 0.25 * (1 - 3*x)]
            elif x < 2/3:
                w = [1.0, 0.5 * (1 - (3*x - 1))]
            else:
                w = [1.0]
            mtp_weights_schedule.append(torch.tensor(w, device=device))
        return mtp_weights_schedule

    def apply_final_ws_ext(self):
        self.ws_long = args.ws_validate_post_yarn_ext

    def get_forward_args(self):
        return ForwardScheduleConfig(
            mtp_weights = self.mtp_weights,
            ws_short = self.ws_short,
            ws_long = self.ws_long
        )
    
    def _is_active_step(self, opt, step: int):
        return (opt.odd_step_only and step%2==1) or not opt.odd_step_only

    def get_transition_steps(self):
        transition_steps = []
        ws_short, ws_long = get_ws(0)
        for step in range(1, args.num_iterations):
            ws_short, new_ws_long = get_ws(step)
            if new_ws_long != ws_long:
                transition_steps.append(step)
                ws_long = new_ws_long
        return transition_steps

    def advance_schedule(self, step: int):
        self.ws_short, new_ws_long = get_ws(step)
        if new_ws_long != self.ws_long:
            self.model.yarn.apply(self.ws_long, new_ws_long)

        new_batch_size = get_bs(step)
        if new_batch_size != self.batch_size:
            self.train_loader_send_args = (new_batch_size, args.train_max_seq_len, grad_accum_steps)
        else:
            self.train_loader_send_args = None

        self.ws_long = new_ws_long
        self.mtp_weights = self.mtp_weights_schedule[step]
    
    def step_optimizers(self, step: int):                
        step_lr = get_lr(step)
        muon_momentum = get_muon_momentum(step)
        for group in self.muon_opt.param_groups:
            group["momentum"] = muon_momentum

        for opt in self.optimizers:
            for group in opt.param_groups:
                group["lr"] = group["initial_lr"] * step_lr
                
        if self._is_active_step(self.adam_opt, step):
            # adam will interleave calls to muon step
            self.adam_opt.step(self.muon_opt)
            self.model.zero_grad(set_to_none=True)
            self.adam_opt.should_sync = False
        else:
            self.muon_opt.step()
            self.muon_opt.zero_grad(set_to_none=True)
            
        if step == self.split_step:
            self.adam_opt.copy_lm_to_embed()
            self.model.split_embed = True
    
    def activate_hooks(self, step: int):
        for opt in self.optimizers:
            if self._is_active_step(opt, step):
                opt.should_sync = True

    def reset(self, state=None):
        if state is not None:
            for opt, opt_state in zip(self.optimizers, state):
                opt.should_sync = False
                opt.load_state_dict(opt_state)

        # muon momentum buffers not in state dict
        self.muon_opt.reset()
        self.model.split_embed = False

        self.ws_short, self.ws_long = get_ws(0)
        self.batch_size = get_bs(0)
        self.model.yarn.reset()

    def get_state(self):
        return [copy.deepcopy(opt.state_dict()) for opt in self.optimizers]

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # batch sizes
    train_bs_schedule: tuple = (8 * 2048 * 8, 16 * 2048 * 8, 24 * 2048 * 8)
    train_bs_extension: int = 24 * 2048 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 1800  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: float = 0.50  # fraction of num_scheduled_iterations spent cooling down the learning rate
    split_embed_frac: float = 2/3  # fraction of training when embeddings split from lm_head
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_final: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=11,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=args.val_batch_size // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.weight.data = m.weight.data.bfloat16()
model.attn_gate_bank.data = model.attn_gate_bank.data.bfloat16()
model.ve_gate_bank.data = model.ve_gate_bank.data.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)
training_manager = TrainingManager(model)

########################################
#            Warmup kernels            #
########################################
print0("Compiling model and warming up kernels (~7 minutes on first execution)", console=True)
# Warmup the training kernels, then re-initialize the state so we aren't cheating
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=training_manager.get_state()) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_bs_schedule[0], args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)

transition_steps = training_manager.get_transition_steps()
# first few steps plus transitions
warmup_steps = sorted({0, 1, 2} | set(s + offset for s in transition_steps for offset in [-1, 0, 1] if s + offset >= 0)) 
print0(f"Sampling steps {warmup_steps} for warmup", console=True)
for step in warmup_steps:
    training_manager.advance_schedule(step)
    model.eval()
    with torch.no_grad():
        inputs, targets, cum_seqlens = next(val_loader)
        model(inputs, targets, cum_seqlens, training_manager.get_forward_args())
    model.train()
    for idx in range(grad_accum_steps):
        # enable gradient sync for the DistAdam optimizers on the last iteration before we step them
        if idx == grad_accum_steps - 1:
            training_manager.activate_hooks(step)
        send_args = training_manager.train_loader_send_args
        inputs, targets, cum_seqlens = train_loader.send(send_args)
        (model(inputs, targets, cum_seqlens, training_manager.get_forward_args()) / grad_accum_steps).backward()
    training_manager.step_optimizers(step)
print0("Resetting Model", console=True)
model.zero_grad(set_to_none=True)
model.load_state_dict(initial_state["model"])
training_manager.reset(initial_state["optimizers"])
del val_loader, train_loader, initial_state
model.train()

########################################
#        Training and validation       #
########################################
train_loader = distributed_data_generator(args.train_files, args.train_bs_schedule[0], args.train_max_seq_len, grad_accum_steps=grad_accum_steps)

gc.collect()

training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    training_manager.advance_schedule(step)
    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            training_manager.apply_final_ws_ext()
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, training_manager.get_forward_args())
        val_loss /= val_steps
        del val_loader
        dist.reduce(val_loss, 0, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for idx in range(grad_accum_steps):
        # enable gradient sync for the DistAdam optimizers on the last iteration before we step them
        if idx == grad_accum_steps - 1:
            training_manager.activate_hooks(step)
        send_args = training_manager.train_loader_send_args
        inputs, targets, cum_seqlens = train_loader.send(send_args)
        (model(inputs, targets, cum_seqlens, training_manager.get_forward_args()) / grad_accum_steps).backward()
    training_manager.step_optimizers(step)

    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.10.12 (main, Nov  4 2025, 08:48:33) [GCC 11.4.0]
Running PyTorch 2.10.0.dev20251210+cu126 compiled for CUDA 12.6
Running Triton version 3.6.0
Mon Jan  5 08:02:51 2026       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:19:00.0 Off |                    0 |
| N/A   43C    P0            129W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:3B:00.0 Off |                    0 |
| N/A   35C    P0            130W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:4C:00.0 Off |                    0 |
| N/A   33C    P0            127W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:5D:00.0 Off |                    0 |
| N/A   42C    P0            130W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:9B:00.0 Off |                    0 |
| N/A   44C    P0            131W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:BB:00.0 Off |                    0 |
| N/A   35C    P0            125W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:CB:00.0 Off |                    0 |
| N/A   41C    P0            128W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:DB:00.0 Off |                    0 |
| N/A   32C    P0            125W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A     54487      C   /usr/bin/python                                 0MiB |
|    1   N/A  N/A     54488      C   /usr/bin/python                                 0MiB |
|    2   N/A  N/A     54489      C   /usr/bin/python                                 0MiB |
|    3   N/A  N/A     54490      C   /usr/bin/python                                 0MiB |
|    4   N/A  N/A     54491      C   /usr/bin/python                                 0MiB |
|    5   N/A  N/A     54492      C   /usr/bin/python                                 0MiB |
|    6   N/A  N/A     54493      C   /usr/bin/python                                 0MiB |
|    7   N/A  N/A     54494      C   /usr/bin/python                                 0MiB |
+-----------------------------------------------------------------------------------------+

====================================================================================================
Compiling model and warming up kernels (~7 minutes on first execution)
Sampling steps [0, 1, 2, 599, 600, 601, 1199, 1200, 1201, 1799, 1800, 1801] for warmup
Resetting Model
step:0/1840 val_loss:10.8310 train_time:0ms step_avg:0.04ms
step:1/1840 train_time:71ms step_avg:70.70ms
step:2/1840 train_time:94ms step_avg:46.92ms
step:3/1840 train_time:114ms step_avg:37.95ms
step:4/1840 train_time:138ms step_avg:34.55ms
step:5/1840 train_time:170ms step_avg:34.01ms
step:6/1840 train_time:258ms step_avg:43.03ms
step:7/1840 train_time:276ms step_avg:39.45ms
step:8/1840 train_time:298ms step_avg:37.21ms
step:9/1840 train_time:330ms step_avg:36.62ms
step:10/1840 train_time:364ms step_avg:36.38ms
step:11/1840 train_time:396ms step_avg:35.97ms
step:12/1840 train_time:430ms step_avg:35.81ms
step:13/1840 train_time:462ms step_avg:35.53ms
step:14/1840 train_time:496ms step_avg:35.43ms
step:15/1840 train_time:528ms step_avg:35.21ms
step:16/1840 train_time:562ms step_avg:35.15ms
step:17/1840 train_time:594ms step_avg:34.97ms
step:18/1840 train_time:629ms step_avg:34.92ms
step:19/1840 train_time:660ms step_avg:34.76ms
step:20/1840 train_time:694ms step_avg:34.72ms
step:21/1840 train_time:727ms step_avg:34.60ms
step:22/1840 train_time:761ms step_avg:34.59ms
step:23/1840 train_time:793ms step_avg:34.47ms
step:24/1840 train_time:827ms step_avg:34.46ms
step:25/1840 train_time:859ms step_avg:34.36ms
step:26/1840 train_time:893ms step_avg:34.34ms
step:27/1840 train_time:925ms step_avg:34.27ms
step:28/1840 train_time:960ms step_avg:34.28ms
step:29/1840 train_time:992ms step_avg:34.20ms
step:30/1840 train_time:1026ms step_avg:34.20ms
step:31/1840 train_time:1058ms step_avg:34.14ms
step:32/1840 train_time:1093ms step_avg:34.14ms
step:33/1840 train_time:1125ms step_avg:34.10ms
step:34/1840 train_time:1161ms step_avg:34.15ms
step:35/1840 train_time:1194ms step_avg:34.11ms
step:36/1840 train_time:1228ms step_avg:34.12ms
step:37/1840 train_time:1261ms step_avg:34.09ms
step:38/1840 train_time:1296ms step_avg:34.10ms
step:39/1840 train_time:1328ms step_avg:34.06ms
step:40/1840 train_time:1363ms step_avg:34.07ms
step:41/1840 train_time:1395ms step_avg:34.03ms
step:42/1840 train_time:1430ms step_avg:34.04ms
step:43/1840 train_time:1462ms step_avg:33.99ms
step:44/1840 train_time:1496ms step_avg:34.00ms
step:45/1840 train_time:1528ms step_avg:33.96ms
step:46/1840 train_time:1563ms step_avg:33.97ms
step:47/1840 train_time:1595ms step_avg:33.93ms
step:48/1840 train_time:1629ms step_avg:33.94ms
step:49/1840 train_time:1661ms step_avg:33.90ms
step:50/1840 train_time:1695ms step_avg:33.90ms
step:51/1840 train_time:1727ms step_avg:33.86ms
step:52/1840 train_time:1762ms step_avg:33.88ms
step:53/1840 train_time:1794ms step_avg:33.84ms
step:54/1840 train_time:1828ms step_avg:33.85ms
step:55/1840 train_time:1860ms step_avg:33.82ms
step:56/1840 train_time:1894ms step_avg:33.82ms
step:57/1840 train_time:1926ms step_avg:33.79ms
step:58/1840 train_time:1960ms step_avg:33.80ms
step:59/1840 train_time:1992ms step_avg:33.77ms
step:60/1840 train_time:2027ms step_avg:33.78ms
step:61/1840 train_time:2059ms step_avg:33.76ms
step:62/1840 train_time:2094ms step_avg:33.77ms
step:63/1840 train_time:2126ms step_avg:33.75ms
step:64/1840 train_time:2161ms step_avg:33.76ms
step:65/1840 train_time:2193ms step_avg:33.74ms
step:66/1840 train_time:2228ms step_avg:33.76ms
step:67/1840 train_time:2260ms step_avg:33.73ms
step:68/1840 train_time:2294ms step_avg:33.74ms
step:69/1840 train_time:2327ms step_avg:33.72ms
step:70/1840 train_time:2361ms step_avg:33.73ms
step:71/1840 train_time:2393ms step_avg:33.71ms
step:72/1840 train_time:2428ms step_avg:33.72ms
step:73/1840 train_time:2460ms step_avg:33.70ms
step:74/1840 train_time:2494ms step_avg:33.71ms
step:75/1840 train_time:2527ms step_avg:33.69ms
step:76/1840 train_time:2562ms step_avg:33.71ms
step:77/1840 train_time:2594ms step_avg:33.69ms
step:78/1840 train_time:2628ms step_avg:33.69ms
step:79/1840 train_time:2660ms step_avg:33.67ms
step:80/1840 train_time:2694ms step_avg:33.68ms
step:81/1840 train_time:2727ms step_avg:33.66ms
step:82/1840 train_time:2761ms step_avg:33.67ms
step:83/1840 train_time:2793ms step_avg:33.65ms
step:84/1840 train_time:2827ms step_avg:33.66ms
step:85/1840 train_time:2859ms step_avg:33.64ms
step:86/1840 train_time:2894ms step_avg:33.65ms
step:87/1840 train_time:2926ms step_avg:33.63ms
step:88/1840 train_time:2960ms step_avg:33.64ms
step:89/1840 train_time:2992ms step_avg:33.62ms
step:90/1840 train_time:3027ms step_avg:33.63ms
step:91/1840 train_time:3059ms step_avg:33.61ms
step:92/1840 train_time:3093ms step_avg:33.62ms
step:93/1840 train_time:3125ms step_avg:33.61ms
step:94/1840 train_time:3160ms step_avg:33.62ms
step:95/1840 train_time:3192ms step_avg:33.60ms
step:96/1840 train_time:3226ms step_avg:33.61ms
step:97/1840 train_time:3259ms step_avg:33.60ms
step:98/1840 train_time:3293ms step_avg:33.60ms
step:99/1840 train_time:3325ms step_avg:33.59ms
step:100/1840 train_time:3360ms step_avg:33.60ms
step:101/1840 train_time:3392ms step_avg:33.58ms
step:102/1840 train_time:3426ms step_avg:33.59ms
step:103/1840 train_time:3459ms step_avg:33.58ms
step:104/1840 train_time:3493ms step_avg:33.59ms
step:105/1840 train_time:3525ms step_avg:33.57ms
step:106/1840 train_time:3560ms step_avg:33.58ms
step:107/1840 train_time:3592ms step_avg:33.57ms
step:108/1840 train_time:3626ms step_avg:33.57ms
step:109/1840 train_time:3658ms step_avg:33.56ms
step:110/1840 train_time:3692ms step_avg:33.57ms
step:111/1840 train_time:3724ms step_avg:33.55ms
step:112/1840 train_time:3759ms step_avg:33.56ms
step:113/1840 train_time:3791ms step_avg:33.55ms
step:114/1840 train_time:3826ms step_avg:33.56ms
step:115/1840 train_time:3858ms step_avg:33.55ms
step:116/1840 train_time:3893ms step_avg:33.56ms
step:117/1840 train_time:3925ms step_avg:33.54ms
step:118/1840 train_time:3959ms step_avg:33.55ms
step:119/1840 train_time:3991ms step_avg:33.54ms
step:120/1840 train_time:4025ms step_avg:33.54ms
step:121/1840 train_time:4058ms step_avg:33.53ms
step:122/1840 train_time:4092ms step_avg:33.54ms
step:123/1840 train_time:4124ms step_avg:33.53ms
step:124/1840 train_time:4158ms step_avg:33.54ms
step:125/1840 train_time:4191ms step_avg:33.52ms
step:126/1840 train_time:4225ms step_avg:33.53ms
step:127/1840 train_time:4257ms step_avg:33.52ms
step:128/1840 train_time:4292ms step_avg:33.53ms
step:129/1840 train_time:4324ms step_avg:33.52ms
step:130/1840 train_time:4358ms step_avg:33.52ms
step:131/1840 train_time:4390ms step_avg:33.51ms
step:132/1840 train_time:4425ms step_avg:33.52ms
step:133/1840 train_time:4457ms step_avg:33.51ms
step:134/1840 train_time:4492ms step_avg:33.52ms
step:135/1840 train_time:4524ms step_avg:33.51ms
step:136/1840 train_time:4558ms step_avg:33.52ms
step:137/1840 train_time:4591ms step_avg:33.51ms
step:138/1840 train_time:4625ms step_avg:33.51ms
step:139/1840 train_time:4657ms step_avg:33.50ms
step:140/1840 train_time:4691ms step_avg:33.51ms
step:141/1840 train_time:4723ms step_avg:33.50ms
step:142/1840 train_time:4758ms step_avg:33.50ms
step:143/1840 train_time:4790ms step_avg:33.49ms
step:144/1840 train_time:4824ms step_avg:33.50ms
step:145/1840 train_time:4856ms step_avg:33.49ms
step:146/1840 train_time:4891ms step_avg:33.50ms
step:147/1840 train_time:4923ms step_avg:33.49ms
step:148/1840 train_time:4957ms step_avg:33.49ms
step:149/1840 train_time:4989ms step_avg:33.48ms
step:150/1840 train_time:5023ms step_avg:33.49ms
step:151/1840 train_time:5056ms step_avg:33.48ms
step:152/1840 train_time:5090ms step_avg:33.49ms
step:153/1840 train_time:5122ms step_avg:33.48ms
step:154/1840 train_time:5156ms step_avg:33.48ms
step:155/1840 train_time:5188ms step_avg:33.47ms
step:156/1840 train_time:5223ms step_avg:33.48ms
step:157/1840 train_time:5255ms step_avg:33.47ms
step:158/1840 train_time:5289ms step_avg:33.47ms
step:159/1840 train_time:5321ms step_avg:33.46ms
step:160/1840 train_time:5355ms step_avg:33.47ms
step:161/1840 train_time:5387ms step_avg:33.46ms
step:162/1840 train_time:5422ms step_avg:33.47ms
step:163/1840 train_time:5454ms step_avg:33.46ms
step:164/1840 train_time:5488ms step_avg:33.46ms
step:165/1840 train_time:5520ms step_avg:33.45ms
step:166/1840 train_time:5554ms step_avg:33.46ms
step:167/1840 train_time:5587ms step_avg:33.45ms
step:168/1840 train_time:5621ms step_avg:33.46ms
step:169/1840 train_time:5653ms step_avg:33.45ms
step:170/1840 train_time:5688ms step_avg:33.46ms
step:171/1840 train_time:5720ms step_avg:33.45ms
step:172/1840 train_time:5754ms step_avg:33.45ms
step:173/1840 train_time:5786ms step_avg:33.45ms
step:174/1840 train_time:5821ms step_avg:33.45ms
step:175/1840 train_time:5853ms step_avg:33.44ms
step:176/1840 train_time:5887ms step_avg:33.45ms
step:177/1840 train_time:5919ms step_avg:33.44ms
step:178/1840 train_time:5953ms step_avg:33.44ms
step:179/1840 train_time:5985ms step_avg:33.44ms
step:180/1840 train_time:6020ms step_avg:33.44ms
step:181/1840 train_time:6052ms step_avg:33.44ms
step:182/1840 train_time:6086ms step_avg:33.44ms
step:183/1840 train_time:6119ms step_avg:33.43ms
step:184/1840 train_time:6153ms step_avg:33.44ms
step:185/1840 train_time:6185ms step_avg:33.43ms
step:186/1840 train_time:6219ms step_avg:33.44ms
step:187/1840 train_time:6251ms step_avg:33.43ms
step:188/1840 train_time:6286ms step_avg:33.44ms
step:189/1840 train_time:6318ms step_avg:33.43ms
step:190/1840 train_time:6352ms step_avg:33.43ms
step:191/1840 train_time:6385ms step_avg:33.43ms
step:192/1840 train_time:6419ms step_avg:33.43ms
step:193/1840 train_time:6451ms step_avg:33.42ms
step:194/1840 train_time:6485ms step_avg:33.43ms
step:195/1840 train_time:6518ms step_avg:33.42ms
step:196/1840 train_time:6552ms step_avg:33.43ms
step:197/1840 train_time:6584ms step_avg:33.42ms
step:198/1840 train_time:6618ms step_avg:33.42ms
step:199/1840 train_time:6650ms step_avg:33.42ms
step:200/1840 train_time:6684ms step_avg:33.42ms
step:201/1840 train_time:6717ms step_avg:33.42ms
step:202/1840 train_time:6751ms step_avg:33.42ms
step:203/1840 train_time:6783ms step_avg:33.41ms
step:204/1840 train_time:6817ms step_avg:33.42ms
step:205/1840 train_time:6849ms step_avg:33.41ms
step:206/1840 train_time:6884ms step_avg:33.42ms
step:207/1840 train_time:6916ms step_avg:33.41ms
step:208/1840 train_time:6950ms step_avg:33.42ms
step:209/1840 train_time:6983ms step_avg:33.41ms
step:210/1840 train_time:7017ms step_avg:33.41ms
step:211/1840 train_time:7049ms step_avg:33.41ms
step:212/1840 train_time:7084ms step_avg:33.41ms
step:213/1840 train_time:7116ms step_avg:33.41ms
step:214/1840 train_time:7150ms step_avg:33.41ms
step:215/1840 train_time:7182ms step_avg:33.40ms
step:216/1840 train_time:7216ms step_avg:33.41ms
step:217/1840 train_time:7248ms step_avg:33.40ms
step:218/1840 train_time:7283ms step_avg:33.41ms
step:219/1840 train_time:7315ms step_avg:33.40ms
step:220/1840 train_time:7350ms step_avg:33.41ms
step:221/1840 train_time:7382ms step_avg:33.40ms
step:222/1840 train_time:7416ms step_avg:33.41ms
step:223/1840 train_time:7448ms step_avg:33.40ms
step:224/1840 train_time:7482ms step_avg:33.40ms
step:225/1840 train_time:7514ms step_avg:33.40ms
step:226/1840 train_time:7549ms step_avg:33.40ms
step:227/1840 train_time:7581ms step_avg:33.40ms
step:228/1840 train_time:7615ms step_avg:33.40ms
step:229/1840 train_time:7647ms step_avg:33.39ms
step:230/1840 train_time:7681ms step_avg:33.40ms
step:231/1840 train_time:7713ms step_avg:33.39ms
step:232/1840 train_time:7748ms step_avg:33.40ms
step:233/1840 train_time:7780ms step_avg:33.39ms
step:234/1840 train_time:7814ms step_avg:33.39ms
step:235/1840 train_time:7846ms step_avg:33.39ms
step:236/1840 train_time:7880ms step_avg:33.39ms
step:237/1840 train_time:7912ms step_avg:33.38ms
step:238/1840 train_time:7947ms step_avg:33.39ms
step:239/1840 train_time:7979ms step_avg:33.38ms
step:240/1840 train_time:8013ms step_avg:33.39ms
step:241/1840 train_time:8045ms step_avg:33.38ms
step:242/1840 train_time:8079ms step_avg:33.39ms
step:243/1840 train_time:8111ms step_avg:33.38ms
step:244/1840 train_time:8146ms step_avg:33.38ms
step:245/1840 train_time:8178ms step_avg:33.38ms
step:246/1840 train_time:8212ms step_avg:33.38ms
step:247/1840 train_time:8244ms step_avg:33.38ms
step:248/1840 train_time:8278ms step_avg:33.38ms
step:249/1840 train_time:8310ms step_avg:33.38ms
step:250/1840 train_time:8345ms step_avg:33.38ms
step:250/1840 val_loss:4.6101 train_time:8387ms step_avg:33.55ms
step:251/1840 train_time:8405ms step_avg:33.49ms
step:252/1840 train_time:8424ms step_avg:33.43ms
step:253/1840 train_time:8446ms step_avg:33.38ms
step:254/1840 train_time:8481ms step_avg:33.39ms
step:255/1840 train_time:8513ms step_avg:33.38ms
step:256/1840 train_time:8548ms step_avg:33.39ms
step:257/1840 train_time:8580ms step_avg:33.39ms
step:258/1840 train_time:8615ms step_avg:33.39ms
step:259/1840 train_time:8647ms step_avg:33.39ms
step:260/1840 train_time:8681ms step_avg:33.39ms
step:261/1840 train_time:8713ms step_avg:33.38ms
step:262/1840 train_time:8747ms step_avg:33.39ms
step:263/1840 train_time:8779ms step_avg:33.38ms
step:264/1840 train_time:8813ms step_avg:33.38ms
step:265/1840 train_time:8845ms step_avg:33.38ms
step:266/1840 train_time:8879ms step_avg:33.38ms
step:267/1840 train_time:8911ms step_avg:33.38ms
step:268/1840 train_time:8945ms step_avg:33.38ms
step:269/1840 train_time:8977ms step_avg:33.37ms
step:270/1840 train_time:9011ms step_avg:33.37ms
step:271/1840 train_time:9043ms step_avg:33.37ms
step:272/1840 train_time:9077ms step_avg:33.37ms
step:273/1840 train_time:9109ms step_avg:33.37ms
step:274/1840 train_time:9143ms step_avg:33.37ms
step:275/1840 train_time:9175ms step_avg:33.36ms
step:276/1840 train_time:9209ms step_avg:33.37ms
step:277/1840 train_time:9241ms step_avg:33.36ms
step:278/1840 train_time:9275ms step_avg:33.36ms
step:279/1840 train_time:9307ms step_avg:33.36ms
step:280/1840 train_time:9342ms step_avg:33.36ms
step:281/1840 train_time:9374ms step_avg:33.36ms
step:282/1840 train_time:9409ms step_avg:33.37ms
step:283/1840 train_time:9441ms step_avg:33.36ms
step:284/1840 train_time:9476ms step_avg:33.37ms
step:285/1840 train_time:9508ms step_avg:33.36ms
step:286/1840 train_time:9543ms step_avg:33.37ms
step:287/1840 train_time:9575ms step_avg:33.36ms
step:288/1840 train_time:9610ms step_avg:33.37ms
step:289/1840 train_time:9642ms step_avg:33.36ms
step:290/1840 train_time:9676ms step_avg:33.37ms
step:291/1840 train_time:9708ms step_avg:33.36ms
step:292/1840 train_time:9743ms step_avg:33.37ms
step:293/1840 train_time:9775ms step_avg:33.36ms
step:294/1840 train_time:9809ms step_avg:33.37ms
step:295/1840 train_time:9842ms step_avg:33.36ms
step:296/1840 train_time:9876ms step_avg:33.37ms
step:297/1840 train_time:9908ms step_avg:33.36ms
step:298/1840 train_time:9943ms step_avg:33.36ms
step:299/1840 train_time:9975ms step_avg:33.36ms
step:300/1840 train_time:10009ms step_avg:33.36ms
step:301/1840 train_time:10041ms step_avg:33.36ms
step:302/1840 train_time:10075ms step_avg:33.36ms
step:303/1840 train_time:10107ms step_avg:33.36ms
step:304/1840 train_time:10141ms step_avg:33.36ms
step:305/1840 train_time:10173ms step_avg:33.36ms
step:306/1840 train_time:10207ms step_avg:33.36ms
step:307/1840 train_time:10240ms step_avg:33.35ms
step:308/1840 train_time:10274ms step_avg:33.36ms
step:309/1840 train_time:10306ms step_avg:33.35ms
step:310/1840 train_time:10341ms step_avg:33.36ms
step:311/1840 train_time:10373ms step_avg:33.35ms
step:312/1840 train_time:10408ms step_avg:33.36ms
step:313/1840 train_time:10440ms step_avg:33.35ms
step:314/1840 train_time:10474ms step_avg:33.36ms
step:315/1840 train_time:10507ms step_avg:33.35ms
step:316/1840 train_time:10541ms step_avg:33.36ms
step:317/1840 train_time:10573ms step_avg:33.35ms
step:318/1840 train_time:10608ms step_avg:33.36ms
step:319/1840 train_time:10640ms step_avg:33.35ms
step:320/1840 train_time:10674ms step_avg:33.36ms
step:321/1840 train_time:10706ms step_avg:33.35ms
step:322/1840 train_time:10741ms step_avg:33.36ms
step:323/1840 train_time:10772ms step_avg:33.35ms
step:324/1840 train_time:10807ms step_avg:33.36ms
step:325/1840 train_time:10839ms step_avg:33.35ms
step:326/1840 train_time:10873ms step_avg:33.35ms
step:327/1840 train_time:10905ms step_avg:33.35ms
step:328/1840 train_time:10940ms step_avg:33.35ms
step:329/1840 train_time:10971ms step_avg:33.35ms
step:330/1840 train_time:11005ms step_avg:33.35ms
step:331/1840 train_time:11037ms step_avg:33.34ms
step:332/1840 train_time:11072ms step_avg:33.35ms
step:333/1840 train_time:11103ms step_avg:33.34ms
step:334/1840 train_time:11137ms step_avg:33.35ms
step:335/1840 train_time:11170ms step_avg:33.34ms
step:336/1840 train_time:11204ms step_avg:33.34ms
step:337/1840 train_time:11236ms step_avg:33.34ms
step:338/1840 train_time:11270ms step_avg:33.34ms
step:339/1840 train_time:11302ms step_avg:33.34ms
step:340/1840 train_time:11336ms step_avg:33.34ms
step:341/1840 train_time:11368ms step_avg:33.34ms
step:342/1840 train_time:11403ms step_avg:33.34ms
step:343/1840 train_time:11435ms step_avg:33.34ms
step:344/1840 train_time:11469ms step_avg:33.34ms
step:345/1840 train_time:11501ms step_avg:33.34ms
step:346/1840 train_time:11535ms step_avg:33.34ms
step:347/1840 train_time:11568ms step_avg:33.34ms
step:348/1840 train_time:11602ms step_avg:33.34ms
step:349/1840 train_time:11635ms step_avg:33.34ms
step:350/1840 train_time:11669ms step_avg:33.34ms
step:351/1840 train_time:11701ms step_avg:33.34ms
step:352/1840 train_time:11736ms step_avg:33.34ms
step:353/1840 train_time:11768ms step_avg:33.34ms
step:354/1840 train_time:11802ms step_avg:33.34ms
step:355/1840 train_time:11834ms step_avg:33.34ms
step:356/1840 train_time:11868ms step_avg:33.34ms
step:357/1840 train_time:11901ms step_avg:33.34ms
step:358/1840 train_time:11935ms step_avg:33.34ms
step:359/1840 train_time:11967ms step_avg:33.33ms
step:360/1840 train_time:12001ms step_avg:33.34ms
step:361/1840 train_time:12033ms step_avg:33.33ms
step:362/1840 train_time:12068ms step_avg:33.34ms
step:363/1840 train_time:12100ms step_avg:33.33ms
step:364/1840 train_time:12134ms step_avg:33.33ms
step:365/1840 train_time:12166ms step_avg:33.33ms
step:366/1840 train_time:12200ms step_avg:33.33ms
step:367/1840 train_time:12232ms step_avg:33.33ms
step:368/1840 train_time:12266ms step_avg:33.33ms
step:369/1840 train_time:12298ms step_avg:33.33ms
step:370/1840 train_time:12333ms step_avg:33.33ms
step:371/1840 train_time:12365ms step_avg:33.33ms
step:372/1840 train_time:12399ms step_avg:33.33ms
step:373/1840 train_time:12431ms step_avg:33.33ms
step:374/1840 train_time:12465ms step_avg:33.33ms
step:375/1840 train_time:12497ms step_avg:33.33ms
step:376/1840 train_time:12532ms step_avg:33.33ms
step:377/1840 train_time:12564ms step_avg:33.33ms
step:378/1840 train_time:12598ms step_avg:33.33ms
step:379/1840 train_time:12630ms step_avg:33.33ms
step:380/1840 train_time:12665ms step_avg:33.33ms
step:381/1840 train_time:12697ms step_avg:33.32ms
step:382/1840 train_time:12731ms step_avg:33.33ms
step:383/1840 train_time:12763ms step_avg:33.32ms
step:384/1840 train_time:12797ms step_avg:33.33ms
step:385/1840 train_time:12829ms step_avg:33.32ms
step:386/1840 train_time:12864ms step_avg:33.33ms
step:387/1840 train_time:12896ms step_avg:33.32ms
step:388/1840 train_time:12930ms step_avg:33.32ms
step:389/1840 train_time:12962ms step_avg:33.32ms
step:390/1840 train_time:12996ms step_avg:33.32ms
step:391/1840 train_time:13028ms step_avg:33.32ms
step:392/1840 train_time:13063ms step_avg:33.32ms
step:393/1840 train_time:13095ms step_avg:33.32ms
step:394/1840 train_time:13129ms step_avg:33.32ms
step:395/1840 train_time:13161ms step_avg:33.32ms
step:396/1840 train_time:13195ms step_avg:33.32ms
step:397/1840 train_time:13227ms step_avg:33.32ms
step:398/1840 train_time:13262ms step_avg:33.32ms
step:399/1840 train_time:13294ms step_avg:33.32ms
step:400/1840 train_time:13328ms step_avg:33.32ms
step:401/1840 train_time:13361ms step_avg:33.32ms
step:402/1840 train_time:13395ms step_avg:33.32ms
step:403/1840 train_time:13427ms step_avg:33.32ms
step:404/1840 train_time:13462ms step_avg:33.32ms
step:405/1840 train_time:13494ms step_avg:33.32ms
step:406/1840 train_time:13528ms step_avg:33.32ms
step:407/1840 train_time:13561ms step_avg:33.32ms
step:408/1840 train_time:13595ms step_avg:33.32ms
step:409/1840 train_time:13627ms step_avg:33.32ms
step:410/1840 train_time:13661ms step_avg:33.32ms
step:411/1840 train_time:13694ms step_avg:33.32ms
step:412/1840 train_time:13728ms step_avg:33.32ms
step:413/1840 train_time:13760ms step_avg:33.32ms
step:414/1840 train_time:13795ms step_avg:33.32ms
step:415/1840 train_time:13827ms step_avg:33.32ms
step:416/1840 train_time:13861ms step_avg:33.32ms
step:417/1840 train_time:13893ms step_avg:33.32ms
step:418/1840 train_time:13927ms step_avg:33.32ms
step:419/1840 train_time:13959ms step_avg:33.32ms
step:420/1840 train_time:13994ms step_avg:33.32ms
step:421/1840 train_time:14026ms step_avg:33.32ms
step:422/1840 train_time:14060ms step_avg:33.32ms
step:423/1840 train_time:14092ms step_avg:33.31ms
step:424/1840 train_time:14126ms step_avg:33.32ms
step:425/1840 train_time:14158ms step_avg:33.31ms
step:426/1840 train_time:14192ms step_avg:33.32ms
step:427/1840 train_time:14224ms step_avg:33.31ms
step:428/1840 train_time:14258ms step_avg:33.31ms
step:429/1840 train_time:14291ms step_avg:33.31ms
step:430/1840 train_time:14325ms step_avg:33.31ms
step:431/1840 train_time:14356ms step_avg:33.31ms
step:432/1840 train_time:14391ms step_avg:33.31ms
step:433/1840 train_time:14423ms step_avg:33.31ms
step:434/1840 train_time:14457ms step_avg:33.31ms
step:435/1840 train_time:14490ms step_avg:33.31ms
step:436/1840 train_time:14524ms step_avg:33.31ms
step:437/1840 train_time:14556ms step_avg:33.31ms
step:438/1840 train_time:14590ms step_avg:33.31ms
step:439/1840 train_time:14623ms step_avg:33.31ms
step:440/1840 train_time:14657ms step_avg:33.31ms
step:441/1840 train_time:14689ms step_avg:33.31ms
step:442/1840 train_time:14723ms step_avg:33.31ms
step:443/1840 train_time:14755ms step_avg:33.31ms
step:444/1840 train_time:14790ms step_avg:33.31ms
step:445/1840 train_time:14822ms step_avg:33.31ms
step:446/1840 train_time:14856ms step_avg:33.31ms
step:447/1840 train_time:14888ms step_avg:33.31ms
step:448/1840 train_time:14923ms step_avg:33.31ms
step:449/1840 train_time:14955ms step_avg:33.31ms
step:450/1840 train_time:14990ms step_avg:33.31ms
step:451/1840 train_time:15022ms step_avg:33.31ms
step:452/1840 train_time:15056ms step_avg:33.31ms
step:453/1840 train_time:15089ms step_avg:33.31ms
step:454/1840 train_time:15123ms step_avg:33.31ms
step:455/1840 train_time:15155ms step_avg:33.31ms
step:456/1840 train_time:15190ms step_avg:33.31ms
step:457/1840 train_time:15222ms step_avg:33.31ms
step:458/1840 train_time:15256ms step_avg:33.31ms
step:459/1840 train_time:15288ms step_avg:33.31ms
step:460/1840 train_time:15323ms step_avg:33.31ms
step:461/1840 train_time:15355ms step_avg:33.31ms
step:462/1840 train_time:15389ms step_avg:33.31ms
step:463/1840 train_time:15421ms step_avg:33.31ms
step:464/1840 train_time:15455ms step_avg:33.31ms
step:465/1840 train_time:15488ms step_avg:33.31ms
step:466/1840 train_time:15522ms step_avg:33.31ms
step:467/1840 train_time:15554ms step_avg:33.31ms
step:468/1840 train_time:15588ms step_avg:33.31ms
step:469/1840 train_time:15620ms step_avg:33.31ms
step:470/1840 train_time:15654ms step_avg:33.31ms
step:471/1840 train_time:15686ms step_avg:33.30ms
step:472/1840 train_time:15721ms step_avg:33.31ms
step:473/1840 train_time:15753ms step_avg:33.30ms
step:474/1840 train_time:15787ms step_avg:33.31ms
step:475/1840 train_time:15819ms step_avg:33.30ms
step:476/1840 train_time:15854ms step_avg:33.31ms
step:477/1840 train_time:15886ms step_avg:33.30ms
step:478/1840 train_time:15920ms step_avg:33.31ms
step:479/1840 train_time:15952ms step_avg:33.30ms
step:480/1840 train_time:15987ms step_avg:33.31ms
step:481/1840 train_time:16019ms step_avg:33.30ms
step:482/1840 train_time:16053ms step_avg:33.31ms
step:483/1840 train_time:16085ms step_avg:33.30ms
step:484/1840 train_time:16120ms step_avg:33.31ms
step:485/1840 train_time:16152ms step_avg:33.30ms
step:486/1840 train_time:16186ms step_avg:33.30ms
step:487/1840 train_time:16218ms step_avg:33.30ms
step:488/1840 train_time:16252ms step_avg:33.30ms
step:489/1840 train_time:16285ms step_avg:33.30ms
step:490/1840 train_time:16319ms step_avg:33.30ms
step:491/1840 train_time:16351ms step_avg:33.30ms
step:492/1840 train_time:16386ms step_avg:33.30ms
step:493/1840 train_time:16417ms step_avg:33.30ms
step:494/1840 train_time:16452ms step_avg:33.30ms
step:495/1840 train_time:16484ms step_avg:33.30ms
step:496/1840 train_time:16518ms step_avg:33.30ms
step:497/1840 train_time:16550ms step_avg:33.30ms
step:498/1840 train_time:16585ms step_avg:33.30ms
step:499/1840 train_time:16617ms step_avg:33.30ms
step:500/1840 train_time:16652ms step_avg:33.30ms
step:500/1840 val_loss:4.2809 train_time:16694ms step_avg:33.39ms
step:501/1840 train_time:16712ms step_avg:33.36ms
step:502/1840 train_time:16731ms step_avg:33.33ms
step:503/1840 train_time:16752ms step_avg:33.30ms
step:504/1840 train_time:16787ms step_avg:33.31ms
step:505/1840 train_time:16820ms step_avg:33.31ms
step:506/1840 train_time:16856ms step_avg:33.31ms
step:507/1840 train_time:16888ms step_avg:33.31ms
step:508/1840 train_time:16923ms step_avg:33.31ms
step:509/1840 train_time:16955ms step_avg:33.31ms
step:510/1840 train_time:16989ms step_avg:33.31ms
step:511/1840 train_time:17022ms step_avg:33.31ms
step:512/1840 train_time:17056ms step_avg:33.31ms
step:513/1840 train_time:17088ms step_avg:33.31ms
step:514/1840 train_time:17122ms step_avg:33.31ms
step:515/1840 train_time:17154ms step_avg:33.31ms
step:516/1840 train_time:17188ms step_avg:33.31ms
step:517/1840 train_time:17220ms step_avg:33.31ms
step:518/1840 train_time:17254ms step_avg:33.31ms
step:519/1840 train_time:17286ms step_avg:33.31ms
step:520/1840 train_time:17320ms step_avg:33.31ms
step:521/1840 train_time:17352ms step_avg:33.31ms
step:522/1840 train_time:17386ms step_avg:33.31ms
step:523/1840 train_time:17418ms step_avg:33.30ms
step:524/1840 train_time:17453ms step_avg:33.31ms
step:525/1840 train_time:17484ms step_avg:33.30ms
step:526/1840 train_time:17518ms step_avg:33.30ms
step:527/1840 train_time:17550ms step_avg:33.30ms
step:528/1840 train_time:17584ms step_avg:33.30ms
step:529/1840 train_time:17617ms step_avg:33.30ms
step:530/1840 train_time:17651ms step_avg:33.30ms
step:531/1840 train_time:17684ms step_avg:33.30ms
step:532/1840 train_time:17718ms step_avg:33.30ms
step:533/1840 train_time:17751ms step_avg:33.30ms
step:534/1840 train_time:17785ms step_avg:33.31ms
step:535/1840 train_time:17818ms step_avg:33.30ms
step:536/1840 train_time:17853ms step_avg:33.31ms
step:537/1840 train_time:17885ms step_avg:33.31ms
step:538/1840 train_time:17920ms step_avg:33.31ms
step:539/1840 train_time:17952ms step_avg:33.31ms
step:540/1840 train_time:17989ms step_avg:33.31ms
step:541/1840 train_time:18019ms step_avg:33.31ms
step:542/1840 train_time:18053ms step_avg:33.31ms
step:543/1840 train_time:18085ms step_avg:33.31ms
step:544/1840 train_time:18119ms step_avg:33.31ms
step:545/1840 train_time:18152ms step_avg:33.31ms
step:546/1840 train_time:18186ms step_avg:33.31ms
step:547/1840 train_time:18218ms step_avg:33.30ms
step:548/1840 train_time:18252ms step_avg:33.31ms
step:549/1840 train_time:18284ms step_avg:33.30ms
step:550/1840 train_time:18318ms step_avg:33.31ms
step:551/1840 train_time:18350ms step_avg:33.30ms
step:552/1840 train_time:18384ms step_avg:33.31ms
step:553/1840 train_time:18416ms step_avg:33.30ms
step:554/1840 train_time:18450ms step_avg:33.30ms
step:555/1840 train_time:18482ms step_avg:33.30ms
step:556/1840 train_time:18517ms step_avg:33.30ms
step:557/1840 train_time:18548ms step_avg:33.30ms
step:558/1840 train_time:18582ms step_avg:33.30ms
step:559/1840 train_time:18615ms step_avg:33.30ms
step:560/1840 train_time:18649ms step_avg:33.30ms
step:561/1840 train_time:18681ms step_avg:33.30ms
step:562/1840 train_time:18716ms step_avg:33.30ms
step:563/1840 train_time:18748ms step_avg:33.30ms
step:564/1840 train_time:18783ms step_avg:33.30ms
step:565/1840 train_time:18815ms step_avg:33.30ms
step:566/1840 train_time:18850ms step_avg:33.30ms
step:567/1840 train_time:18882ms step_avg:33.30ms
step:568/1840 train_time:18916ms step_avg:33.30ms
step:569/1840 train_time:18949ms step_avg:33.30ms
step:570/1840 train_time:18983ms step_avg:33.30ms
step:571/1840 train_time:19015ms step_avg:33.30ms
step:572/1840 train_time:19049ms step_avg:33.30ms
step:573/1840 train_time:19082ms step_avg:33.30ms
step:574/1840 train_time:19116ms step_avg:33.30ms
step:575/1840 train_time:19148ms step_avg:33.30ms
step:576/1840 train_time:19182ms step_avg:33.30ms
step:577/1840 train_time:19214ms step_avg:33.30ms
step:578/1840 train_time:19249ms step_avg:33.30ms
step:579/1840 train_time:19281ms step_avg:33.30ms
step:580/1840 train_time:19315ms step_avg:33.30ms
step:581/1840 train_time:19347ms step_avg:33.30ms
step:582/1840 train_time:19381ms step_avg:33.30ms
step:583/1840 train_time:19413ms step_avg:33.30ms
step:584/1840 train_time:19447ms step_avg:33.30ms
step:585/1840 train_time:19479ms step_avg:33.30ms
step:586/1840 train_time:19514ms step_avg:33.30ms
step:587/1840 train_time:19545ms step_avg:33.30ms
step:588/1840 train_time:19580ms step_avg:33.30ms
step:589/1840 train_time:19612ms step_avg:33.30ms
step:590/1840 train_time:19647ms step_avg:33.30ms
step:591/1840 train_time:19679ms step_avg:33.30ms
step:592/1840 train_time:19713ms step_avg:33.30ms
step:593/1840 train_time:19745ms step_avg:33.30ms
step:594/1840 train_time:19780ms step_avg:33.30ms
step:595/1840 train_time:19812ms step_avg:33.30ms
step:596/1840 train_time:19846ms step_avg:33.30ms
step:597/1840 train_time:19878ms step_avg:33.30ms
step:598/1840 train_time:19913ms step_avg:33.30ms
step:599/1840 train_time:19945ms step_avg:33.30ms
step:600/1840 train_time:19980ms step_avg:33.30ms
step:601/1840 train_time:20014ms step_avg:33.30ms
step:602/1840 train_time:20072ms step_avg:33.34ms
step:603/1840 train_time:20131ms step_avg:33.38ms
step:604/1840 train_time:20193ms step_avg:33.43ms
step:605/1840 train_time:20252ms step_avg:33.47ms
step:606/1840 train_time:20315ms step_avg:33.52ms
step:607/1840 train_time:20375ms step_avg:33.57ms
step:608/1840 train_time:20437ms step_avg:33.61ms
step:609/1840 train_time:20497ms step_avg:33.66ms
step:610/1840 train_time:20559ms step_avg:33.70ms
step:611/1840 train_time:20619ms step_avg:33.75ms
step:612/1840 train_time:20681ms step_avg:33.79ms
step:613/1840 train_time:20741ms step_avg:33.84ms
step:614/1840 train_time:20803ms step_avg:33.88ms
step:615/1840 train_time:20863ms step_avg:33.92ms
step:616/1840 train_time:20925ms step_avg:33.97ms
step:617/1840 train_time:20985ms step_avg:34.01ms
step:618/1840 train_time:21047ms step_avg:34.06ms
step:619/1840 train_time:21107ms step_avg:34.10ms
step:620/1840 train_time:21169ms step_avg:34.14ms
step:621/1840 train_time:21228ms step_avg:34.18ms
step:622/1840 train_time:21290ms step_avg:34.23ms
step:623/1840 train_time:21349ms step_avg:34.27ms
step:624/1840 train_time:21412ms step_avg:34.31ms
step:625/1840 train_time:21471ms step_avg:34.35ms
step:626/1840 train_time:21533ms step_avg:34.40ms
step:627/1840 train_time:21593ms step_avg:34.44ms
step:628/1840 train_time:21655ms step_avg:34.48ms
step:629/1840 train_time:21717ms step_avg:34.53ms
step:630/1840 train_time:21780ms step_avg:34.57ms
step:631/1840 train_time:21840ms step_avg:34.61ms
step:632/1840 train_time:21903ms step_avg:34.66ms
step:633/1840 train_time:21963ms step_avg:34.70ms
step:634/1840 train_time:22025ms step_avg:34.74ms
step:635/1840 train_time:22086ms step_avg:34.78ms
step:636/1840 train_time:22148ms step_avg:34.82ms
step:637/1840 train_time:22208ms step_avg:34.86ms
step:638/1840 train_time:22269ms step_avg:34.90ms
step:639/1840 train_time:22329ms step_avg:34.94ms
step:640/1840 train_time:22391ms step_avg:34.99ms
step:641/1840 train_time:22450ms step_avg:35.02ms
step:642/1840 train_time:22512ms step_avg:35.07ms
step:643/1840 train_time:22572ms step_avg:35.10ms
step:644/1840 train_time:22634ms step_avg:35.15ms
step:645/1840 train_time:22694ms step_avg:35.18ms
step:646/1840 train_time:22757ms step_avg:35.23ms
step:647/1840 train_time:22817ms step_avg:35.27ms
step:648/1840 train_time:22880ms step_avg:35.31ms
step:649/1840 train_time:22939ms step_avg:35.35ms
step:650/1840 train_time:23004ms step_avg:35.39ms
step:651/1840 train_time:23064ms step_avg:35.43ms
step:652/1840 train_time:23126ms step_avg:35.47ms
step:653/1840 train_time:23186ms step_avg:35.51ms
step:654/1840 train_time:23247ms step_avg:35.55ms
step:655/1840 train_time:23308ms step_avg:35.58ms
step:656/1840 train_time:23370ms step_avg:35.62ms
step:657/1840 train_time:23430ms step_avg:35.66ms
step:658/1840 train_time:23491ms step_avg:35.70ms
step:659/1840 train_time:23550ms step_avg:35.74ms
step:660/1840 train_time:23613ms step_avg:35.78ms
step:661/1840 train_time:23672ms step_avg:35.81ms
step:662/1840 train_time:23735ms step_avg:35.85ms
step:663/1840 train_time:23794ms step_avg:35.89ms
step:664/1840 train_time:23857ms step_avg:35.93ms
step:665/1840 train_time:23918ms step_avg:35.97ms
step:666/1840 train_time:23982ms step_avg:36.01ms
step:667/1840 train_time:24042ms step_avg:36.05ms
step:668/1840 train_time:24105ms step_avg:36.09ms
step:669/1840 train_time:24166ms step_avg:36.12ms
step:670/1840 train_time:24227ms step_avg:36.16ms
step:671/1840 train_time:24287ms step_avg:36.19ms
step:672/1840 train_time:24349ms step_avg:36.23ms
step:673/1840 train_time:24408ms step_avg:36.27ms
step:674/1840 train_time:24470ms step_avg:36.31ms
step:675/1840 train_time:24530ms step_avg:36.34ms
step:676/1840 train_time:24591ms step_avg:36.38ms
step:677/1840 train_time:24651ms step_avg:36.41ms
step:678/1840 train_time:24713ms step_avg:36.45ms
step:679/1840 train_time:24774ms step_avg:36.49ms
step:680/1840 train_time:24836ms step_avg:36.52ms
step:681/1840 train_time:24896ms step_avg:36.56ms
step:682/1840 train_time:24959ms step_avg:36.60ms
step:683/1840 train_time:25020ms step_avg:36.63ms
step:684/1840 train_time:25083ms step_avg:36.67ms
step:685/1840 train_time:25144ms step_avg:36.71ms
step:686/1840 train_time:25206ms step_avg:36.74ms
step:687/1840 train_time:25266ms step_avg:36.78ms
step:688/1840 train_time:25327ms step_avg:36.81ms
step:689/1840 train_time:25387ms step_avg:36.85ms
step:690/1840 train_time:25449ms step_avg:36.88ms
step:691/1840 train_time:25509ms step_avg:36.92ms
step:692/1840 train_time:25571ms step_avg:36.95ms
step:693/1840 train_time:25631ms step_avg:36.99ms
step:694/1840 train_time:25693ms step_avg:37.02ms
step:695/1840 train_time:25752ms step_avg:37.05ms
step:696/1840 train_time:25815ms step_avg:37.09ms
step:697/1840 train_time:25875ms step_avg:37.12ms
step:698/1840 train_time:25937ms step_avg:37.16ms
step:699/1840 train_time:25998ms step_avg:37.19ms
step:700/1840 train_time:26061ms step_avg:37.23ms
step:701/1840 train_time:26122ms step_avg:37.26ms
step:702/1840 train_time:26185ms step_avg:37.30ms
step:703/1840 train_time:26244ms step_avg:37.33ms
step:704/1840 train_time:26306ms step_avg:37.37ms
step:705/1840 train_time:26366ms step_avg:37.40ms
step:706/1840 train_time:26429ms step_avg:37.43ms
step:707/1840 train_time:26488ms step_avg:37.46ms
step:708/1840 train_time:26550ms step_avg:37.50ms
step:709/1840 train_time:26609ms step_avg:37.53ms
step:710/1840 train_time:26672ms step_avg:37.57ms
step:711/1840 train_time:26731ms step_avg:37.60ms
step:712/1840 train_time:26793ms step_avg:37.63ms
step:713/1840 train_time:26853ms step_avg:37.66ms
step:714/1840 train_time:26915ms step_avg:37.70ms
step:715/1840 train_time:26975ms step_avg:37.73ms
step:716/1840 train_time:27037ms step_avg:37.76ms
step:717/1840 train_time:27098ms step_avg:37.79ms
step:718/1840 train_time:27162ms step_avg:37.83ms
step:719/1840 train_time:27222ms step_avg:37.86ms
step:720/1840 train_time:27284ms step_avg:37.90ms
step:721/1840 train_time:27345ms step_avg:37.93ms
step:722/1840 train_time:27407ms step_avg:37.96ms
step:723/1840 train_time:27467ms step_avg:37.99ms
step:724/1840 train_time:27529ms step_avg:38.02ms
step:725/1840 train_time:27588ms step_avg:38.05ms
step:726/1840 train_time:27650ms step_avg:38.08ms
step:727/1840 train_time:27710ms step_avg:38.12ms
step:728/1840 train_time:27771ms step_avg:38.15ms
step:729/1840 train_time:27831ms step_avg:38.18ms
step:730/1840 train_time:27893ms step_avg:38.21ms
step:731/1840 train_time:27952ms step_avg:38.24ms
step:732/1840 train_time:28015ms step_avg:38.27ms
step:733/1840 train_time:28075ms step_avg:38.30ms
step:734/1840 train_time:28139ms step_avg:38.34ms
step:735/1840 train_time:28200ms step_avg:38.37ms
step:736/1840 train_time:28264ms step_avg:38.40ms
step:737/1840 train_time:28324ms step_avg:38.43ms
step:738/1840 train_time:28386ms step_avg:38.46ms
step:739/1840 train_time:28446ms step_avg:38.49ms
step:740/1840 train_time:28508ms step_avg:38.52ms
step:741/1840 train_time:28568ms step_avg:38.55ms
step:742/1840 train_time:28631ms step_avg:38.59ms
step:743/1840 train_time:28690ms step_avg:38.61ms
step:744/1840 train_time:28751ms step_avg:38.64ms
step:745/1840 train_time:28811ms step_avg:38.67ms
step:746/1840 train_time:28873ms step_avg:38.70ms
step:747/1840 train_time:28932ms step_avg:38.73ms
step:748/1840 train_time:28995ms step_avg:38.76ms
step:749/1840 train_time:29055ms step_avg:38.79ms
step:750/1840 train_time:29118ms step_avg:38.82ms
step:750/1840 val_loss:4.0204 train_time:29190ms step_avg:38.92ms
step:751/1840 train_time:29213ms step_avg:38.90ms
step:752/1840 train_time:29242ms step_avg:38.89ms
step:753/1840 train_time:29302ms step_avg:38.91ms
step:754/1840 train_time:29366ms step_avg:38.95ms
step:755/1840 train_time:29426ms step_avg:38.97ms
step:756/1840 train_time:29488ms step_avg:39.00ms
step:757/1840 train_time:29548ms step_avg:39.03ms
step:758/1840 train_time:29609ms step_avg:39.06ms
step:759/1840 train_time:29668ms step_avg:39.09ms
step:760/1840 train_time:29731ms step_avg:39.12ms
step:761/1840 train_time:29790ms step_avg:39.15ms
step:762/1840 train_time:29852ms step_avg:39.18ms
step:763/1840 train_time:29910ms step_avg:39.20ms
step:764/1840 train_time:29972ms step_avg:39.23ms
step:765/1840 train_time:30032ms step_avg:39.26ms
step:766/1840 train_time:30094ms step_avg:39.29ms
step:767/1840 train_time:30155ms step_avg:39.32ms
step:768/1840 train_time:30219ms step_avg:39.35ms
step:769/1840 train_time:30280ms step_avg:39.38ms
step:770/1840 train_time:30342ms step_avg:39.40ms
step:771/1840 train_time:30402ms step_avg:39.43ms
step:772/1840 train_time:30464ms step_avg:39.46ms
step:773/1840 train_time:30523ms step_avg:39.49ms
step:774/1840 train_time:30585ms step_avg:39.52ms
step:775/1840 train_time:30644ms step_avg:39.54ms
step:776/1840 train_time:30706ms step_avg:39.57ms
step:777/1840 train_time:30765ms step_avg:39.59ms
step:778/1840 train_time:30827ms step_avg:39.62ms
step:779/1840 train_time:30886ms step_avg:39.65ms
step:780/1840 train_time:30948ms step_avg:39.68ms
step:781/1840 train_time:31007ms step_avg:39.70ms
step:782/1840 train_time:31069ms step_avg:39.73ms
step:783/1840 train_time:31130ms step_avg:39.76ms
step:784/1840 train_time:31194ms step_avg:39.79ms
step:785/1840 train_time:31255ms step_avg:39.82ms
step:786/1840 train_time:31319ms step_avg:39.85ms
step:787/1840 train_time:31379ms step_avg:39.87ms
step:788/1840 train_time:31442ms step_avg:39.90ms
step:789/1840 train_time:31501ms step_avg:39.93ms
step:790/1840 train_time:31563ms step_avg:39.95ms
step:791/1840 train_time:31623ms step_avg:39.98ms
step:792/1840 train_time:31684ms step_avg:40.01ms
step:793/1840 train_time:31743ms step_avg:40.03ms
step:794/1840 train_time:31806ms step_avg:40.06ms
step:795/1840 train_time:31865ms step_avg:40.08ms
step:796/1840 train_time:31927ms step_avg:40.11ms
step:797/1840 train_time:31986ms step_avg:40.13ms
step:798/1840 train_time:32048ms step_avg:40.16ms
step:799/1840 train_time:32108ms step_avg:40.19ms
step:800/1840 train_time:32172ms step_avg:40.21ms
step:801/1840 train_time:32232ms step_avg:40.24ms
step:802/1840 train_time:32294ms step_avg:40.27ms
step:803/1840 train_time:32354ms step_avg:40.29ms
step:804/1840 train_time:32419ms step_avg:40.32ms
step:805/1840 train_time:32479ms step_avg:40.35ms
step:806/1840 train_time:32541ms step_avg:40.37ms
step:807/1840 train_time:32600ms step_avg:40.40ms
step:808/1840 train_time:32662ms step_avg:40.42ms
step:809/1840 train_time:32721ms step_avg:40.45ms
step:810/1840 train_time:32783ms step_avg:40.47ms
step:811/1840 train_time:32843ms step_avg:40.50ms
step:812/1840 train_time:32905ms step_avg:40.52ms
step:813/1840 train_time:32964ms step_avg:40.55ms
step:814/1840 train_time:33027ms step_avg:40.57ms
step:815/1840 train_time:33086ms step_avg:40.60ms
step:816/1840 train_time:33148ms step_avg:40.62ms
step:817/1840 train_time:33208ms step_avg:40.65ms
step:818/1840 train_time:33271ms step_avg:40.67ms
step:819/1840 train_time:33331ms step_avg:40.70ms
step:820/1840 train_time:33394ms step_avg:40.72ms
step:821/1840 train_time:33455ms step_avg:40.75ms
step:822/1840 train_time:33518ms step_avg:40.78ms
step:823/1840 train_time:33579ms step_avg:40.80ms
step:824/1840 train_time:33641ms step_avg:40.83ms
step:825/1840 train_time:33701ms step_avg:40.85ms
step:826/1840 train_time:33762ms step_avg:40.87ms
step:827/1840 train_time:33822ms step_avg:40.90ms
step:828/1840 train_time:33884ms step_avg:40.92ms
step:829/1840 train_time:33944ms step_avg:40.95ms
step:830/1840 train_time:34007ms step_avg:40.97ms
step:831/1840 train_time:34066ms step_avg:40.99ms
step:832/1840 train_time:34128ms step_avg:41.02ms
step:833/1840 train_time:34187ms step_avg:41.04ms
step:834/1840 train_time:34249ms step_avg:41.07ms
step:835/1840 train_time:34309ms step_avg:41.09ms
step:836/1840 train_time:34373ms step_avg:41.12ms
step:837/1840 train_time:34433ms step_avg:41.14ms
step:838/1840 train_time:34497ms step_avg:41.17ms
step:839/1840 train_time:34557ms step_avg:41.19ms
step:840/1840 train_time:34620ms step_avg:41.21ms
step:841/1840 train_time:34679ms step_avg:41.24ms
step:842/1840 train_time:34741ms step_avg:41.26ms
step:843/1840 train_time:34801ms step_avg:41.28ms
step:844/1840 train_time:34863ms step_avg:41.31ms
step:845/1840 train_time:34923ms step_avg:41.33ms
step:846/1840 train_time:34986ms step_avg:41.35ms
step:847/1840 train_time:35045ms step_avg:41.38ms
step:848/1840 train_time:35106ms step_avg:41.40ms
step:849/1840 train_time:35166ms step_avg:41.42ms
step:850/1840 train_time:35229ms step_avg:41.45ms
step:851/1840 train_time:35287ms step_avg:41.47ms
step:852/1840 train_time:35350ms step_avg:41.49ms
step:853/1840 train_time:35410ms step_avg:41.51ms
step:854/1840 train_time:35473ms step_avg:41.54ms
step:855/1840 train_time:35533ms step_avg:41.56ms
step:856/1840 train_time:35597ms step_avg:41.59ms
step:857/1840 train_time:35658ms step_avg:41.61ms
step:858/1840 train_time:35721ms step_avg:41.63ms
step:859/1840 train_time:35781ms step_avg:41.65ms
step:860/1840 train_time:35842ms step_avg:41.68ms
step:861/1840 train_time:35903ms step_avg:41.70ms
step:862/1840 train_time:35965ms step_avg:41.72ms
step:863/1840 train_time:36025ms step_avg:41.74ms
step:864/1840 train_time:36087ms step_avg:41.77ms
step:865/1840 train_time:36146ms step_avg:41.79ms
step:866/1840 train_time:36208ms step_avg:41.81ms
step:867/1840 train_time:36267ms step_avg:41.83ms
step:868/1840 train_time:36330ms step_avg:41.85ms
step:869/1840 train_time:36390ms step_avg:41.88ms
step:870/1840 train_time:36452ms step_avg:41.90ms
step:871/1840 train_time:36512ms step_avg:41.92ms
step:872/1840 train_time:36575ms step_avg:41.94ms
step:873/1840 train_time:36636ms step_avg:41.97ms
step:874/1840 train_time:36700ms step_avg:41.99ms
step:875/1840 train_time:36760ms step_avg:42.01ms
step:876/1840 train_time:36822ms step_avg:42.03ms
step:877/1840 train_time:36881ms step_avg:42.05ms
step:878/1840 train_time:36943ms step_avg:42.08ms
step:879/1840 train_time:37002ms step_avg:42.10ms
step:880/1840 train_time:37065ms step_avg:42.12ms
step:881/1840 train_time:37124ms step_avg:42.14ms
step:882/1840 train_time:37187ms step_avg:42.16ms
step:883/1840 train_time:37246ms step_avg:42.18ms
step:884/1840 train_time:37308ms step_avg:42.20ms
step:885/1840 train_time:37368ms step_avg:42.22ms
step:886/1840 train_time:37431ms step_avg:42.25ms
step:887/1840 train_time:37490ms step_avg:42.27ms
step:888/1840 train_time:37553ms step_avg:42.29ms
step:889/1840 train_time:37613ms step_avg:42.31ms
step:890/1840 train_time:37676ms step_avg:42.33ms
step:891/1840 train_time:37736ms step_avg:42.35ms
step:892/1840 train_time:37800ms step_avg:42.38ms
step:893/1840 train_time:37860ms step_avg:42.40ms
step:894/1840 train_time:37922ms step_avg:42.42ms
step:895/1840 train_time:37982ms step_avg:42.44ms
step:896/1840 train_time:38044ms step_avg:42.46ms
step:897/1840 train_time:38104ms step_avg:42.48ms
step:898/1840 train_time:38166ms step_avg:42.50ms
step:899/1840 train_time:38226ms step_avg:42.52ms
step:900/1840 train_time:38288ms step_avg:42.54ms
step:901/1840 train_time:38347ms step_avg:42.56ms
step:902/1840 train_time:38409ms step_avg:42.58ms
step:903/1840 train_time:38469ms step_avg:42.60ms
step:904/1840 train_time:38532ms step_avg:42.62ms
step:905/1840 train_time:38592ms step_avg:42.64ms
step:906/1840 train_time:38655ms step_avg:42.67ms
step:907/1840 train_time:38715ms step_avg:42.68ms
step:908/1840 train_time:38778ms step_avg:42.71ms
step:909/1840 train_time:38839ms step_avg:42.73ms
step:910/1840 train_time:38902ms step_avg:42.75ms
step:911/1840 train_time:38962ms step_avg:42.77ms
step:912/1840 train_time:39024ms step_avg:42.79ms
step:913/1840 train_time:39085ms step_avg:42.81ms
step:914/1840 train_time:39147ms step_avg:42.83ms
step:915/1840 train_time:39207ms step_avg:42.85ms
step:916/1840 train_time:39268ms step_avg:42.87ms
step:917/1840 train_time:39327ms step_avg:42.89ms
step:918/1840 train_time:39389ms step_avg:42.91ms
step:919/1840 train_time:39449ms step_avg:42.93ms
step:920/1840 train_time:39511ms step_avg:42.95ms
step:921/1840 train_time:39571ms step_avg:42.96ms
step:922/1840 train_time:39633ms step_avg:42.99ms
step:923/1840 train_time:39693ms step_avg:43.00ms
step:924/1840 train_time:39757ms step_avg:43.03ms
step:925/1840 train_time:39817ms step_avg:43.05ms
step:926/1840 train_time:39879ms step_avg:43.07ms
step:927/1840 train_time:39940ms step_avg:43.09ms
step:928/1840 train_time:40003ms step_avg:43.11ms
step:929/1840 train_time:40063ms step_avg:43.12ms
step:930/1840 train_time:40126ms step_avg:43.15ms
step:931/1840 train_time:40185ms step_avg:43.16ms
step:932/1840 train_time:40247ms step_avg:43.18ms
step:933/1840 train_time:40307ms step_avg:43.20ms
step:934/1840 train_time:40369ms step_avg:43.22ms
step:935/1840 train_time:40428ms step_avg:43.24ms
step:936/1840 train_time:40490ms step_avg:43.26ms
step:937/1840 train_time:40549ms step_avg:43.28ms
step:938/1840 train_time:40612ms step_avg:43.30ms
step:939/1840 train_time:40672ms step_avg:43.31ms
step:940/1840 train_time:40734ms step_avg:43.33ms
step:941/1840 train_time:40795ms step_avg:43.35ms
step:942/1840 train_time:40858ms step_avg:43.37ms
step:943/1840 train_time:40919ms step_avg:43.39ms
step:944/1840 train_time:40981ms step_avg:43.41ms
step:945/1840 train_time:41041ms step_avg:43.43ms
step:946/1840 train_time:41103ms step_avg:43.45ms
step:947/1840 train_time:41164ms step_avg:43.47ms
step:948/1840 train_time:41226ms step_avg:43.49ms
step:949/1840 train_time:41285ms step_avg:43.50ms
step:950/1840 train_time:41347ms step_avg:43.52ms
step:951/1840 train_time:41406ms step_avg:43.54ms
step:952/1840 train_time:41468ms step_avg:43.56ms
step:953/1840 train_time:41528ms step_avg:43.58ms
step:954/1840 train_time:41590ms step_avg:43.60ms
step:955/1840 train_time:41650ms step_avg:43.61ms
step:956/1840 train_time:41713ms step_avg:43.63ms
step:957/1840 train_time:41775ms step_avg:43.65ms
step:958/1840 train_time:41838ms step_avg:43.67ms
step:959/1840 train_time:41898ms step_avg:43.69ms
step:960/1840 train_time:41961ms step_avg:43.71ms
step:961/1840 train_time:42021ms step_avg:43.73ms
step:962/1840 train_time:42083ms step_avg:43.75ms
step:963/1840 train_time:42142ms step_avg:43.76ms
step:964/1840 train_time:42204ms step_avg:43.78ms
step:965/1840 train_time:42264ms step_avg:43.80ms
step:966/1840 train_time:42327ms step_avg:43.82ms
step:967/1840 train_time:42386ms step_avg:43.83ms
step:968/1840 train_time:42447ms step_avg:43.85ms
step:969/1840 train_time:42507ms step_avg:43.87ms
step:970/1840 train_time:42569ms step_avg:43.89ms
step:971/1840 train_time:42628ms step_avg:43.90ms
step:972/1840 train_time:42691ms step_avg:43.92ms
step:973/1840 train_time:42751ms step_avg:43.94ms
step:974/1840 train_time:42814ms step_avg:43.96ms
step:975/1840 train_time:42875ms step_avg:43.97ms
step:976/1840 train_time:42939ms step_avg:43.99ms
step:977/1840 train_time:43000ms step_avg:44.01ms
step:978/1840 train_time:43062ms step_avg:44.03ms
step:979/1840 train_time:43122ms step_avg:44.05ms
step:980/1840 train_time:43184ms step_avg:44.07ms
step:981/1840 train_time:43243ms step_avg:44.08ms
step:982/1840 train_time:43306ms step_avg:44.10ms
step:983/1840 train_time:43365ms step_avg:44.11ms
step:984/1840 train_time:43427ms step_avg:44.13ms
step:985/1840 train_time:43486ms step_avg:44.15ms
step:986/1840 train_time:43548ms step_avg:44.17ms
step:987/1840 train_time:43608ms step_avg:44.18ms
step:988/1840 train_time:43670ms step_avg:44.20ms
step:989/1840 train_time:43729ms step_avg:44.22ms
step:990/1840 train_time:43793ms step_avg:44.23ms
step:991/1840 train_time:43853ms step_avg:44.25ms
step:992/1840 train_time:43917ms step_avg:44.27ms
step:993/1840 train_time:43977ms step_avg:44.29ms
step:994/1840 train_time:44040ms step_avg:44.31ms
step:995/1840 train_time:44100ms step_avg:44.32ms
step:996/1840 train_time:44163ms step_avg:44.34ms
step:997/1840 train_time:44223ms step_avg:44.36ms
step:998/1840 train_time:44284ms step_avg:44.37ms
step:999/1840 train_time:44344ms step_avg:44.39ms
step:1000/1840 train_time:44406ms step_avg:44.41ms
step:1000/1840 val_loss:3.7789 train_time:44477ms step_avg:44.48ms
step:1001/1840 train_time:44496ms step_avg:44.45ms
step:1002/1840 train_time:44529ms step_avg:44.44ms
step:1003/1840 train_time:44591ms step_avg:44.46ms
step:1004/1840 train_time:44655ms step_avg:44.48ms
step:1005/1840 train_time:44715ms step_avg:44.49ms
step:1006/1840 train_time:44777ms step_avg:44.51ms
step:1007/1840 train_time:44836ms step_avg:44.52ms
step:1008/1840 train_time:44898ms step_avg:44.54ms
step:1009/1840 train_time:44957ms step_avg:44.56ms
step:1010/1840 train_time:45017ms step_avg:44.57ms
step:1011/1840 train_time:45076ms step_avg:44.59ms
step:1012/1840 train_time:45138ms step_avg:44.60ms
step:1013/1840 train_time:45196ms step_avg:44.62ms
step:1014/1840 train_time:45258ms step_avg:44.63ms
step:1015/1840 train_time:45317ms step_avg:44.65ms
step:1016/1840 train_time:45379ms step_avg:44.66ms
step:1017/1840 train_time:45441ms step_avg:44.68ms
step:1018/1840 train_time:45505ms step_avg:44.70ms
step:1019/1840 train_time:45567ms step_avg:44.72ms
step:1020/1840 train_time:45630ms step_avg:44.73ms
step:1021/1840 train_time:45690ms step_avg:44.75ms
step:1022/1840 train_time:45753ms step_avg:44.77ms
step:1023/1840 train_time:45812ms step_avg:44.78ms
step:1024/1840 train_time:45874ms step_avg:44.80ms
step:1025/1840 train_time:45934ms step_avg:44.81ms
step:1026/1840 train_time:45995ms step_avg:44.83ms
step:1027/1840 train_time:46055ms step_avg:44.84ms
step:1028/1840 train_time:46116ms step_avg:44.86ms
step:1029/1840 train_time:46175ms step_avg:44.87ms
step:1030/1840 train_time:46237ms step_avg:44.89ms
step:1031/1840 train_time:46297ms step_avg:44.90ms
step:1032/1840 train_time:46359ms step_avg:44.92ms
step:1033/1840 train_time:46420ms step_avg:44.94ms
step:1034/1840 train_time:46482ms step_avg:44.95ms
step:1035/1840 train_time:46542ms step_avg:44.97ms
step:1036/1840 train_time:46605ms step_avg:44.99ms
step:1037/1840 train_time:46666ms step_avg:45.00ms
step:1038/1840 train_time:46728ms step_avg:45.02ms
step:1039/1840 train_time:46788ms step_avg:45.03ms
step:1040/1840 train_time:46850ms step_avg:45.05ms
step:1041/1840 train_time:46910ms step_avg:45.06ms
step:1042/1840 train_time:46972ms step_avg:45.08ms
step:1043/1840 train_time:47032ms step_avg:45.09ms
step:1044/1840 train_time:47094ms step_avg:45.11ms
step:1045/1840 train_time:47154ms step_avg:45.12ms
step:1046/1840 train_time:47216ms step_avg:45.14ms
step:1047/1840 train_time:47275ms step_avg:45.15ms
step:1048/1840 train_time:47337ms step_avg:45.17ms
step:1049/1840 train_time:47397ms step_avg:45.18ms
step:1050/1840 train_time:47460ms step_avg:45.20ms
step:1051/1840 train_time:47519ms step_avg:45.21ms
step:1052/1840 train_time:47582ms step_avg:45.23ms
step:1053/1840 train_time:47642ms step_avg:45.24ms
step:1054/1840 train_time:47704ms step_avg:45.26ms
step:1055/1840 train_time:47764ms step_avg:45.27ms
step:1056/1840 train_time:47826ms step_avg:45.29ms
step:1057/1840 train_time:47887ms step_avg:45.30ms
step:1058/1840 train_time:47949ms step_avg:45.32ms
step:1059/1840 train_time:48009ms step_avg:45.33ms
step:1060/1840 train_time:48072ms step_avg:45.35ms
step:1061/1840 train_time:48131ms step_avg:45.36ms
step:1062/1840 train_time:48194ms step_avg:45.38ms
step:1063/1840 train_time:48254ms step_avg:45.39ms
step:1064/1840 train_time:48316ms step_avg:45.41ms
step:1065/1840 train_time:48376ms step_avg:45.42ms
step:1066/1840 train_time:48439ms step_avg:45.44ms
step:1067/1840 train_time:48498ms step_avg:45.45ms
step:1068/1840 train_time:48560ms step_avg:45.47ms
step:1069/1840 train_time:48620ms step_avg:45.48ms
step:1070/1840 train_time:48682ms step_avg:45.50ms
step:1071/1840 train_time:48742ms step_avg:45.51ms
step:1072/1840 train_time:48804ms step_avg:45.53ms
step:1073/1840 train_time:48864ms step_avg:45.54ms
step:1074/1840 train_time:48927ms step_avg:45.56ms
step:1075/1840 train_time:48987ms step_avg:45.57ms
step:1076/1840 train_time:49050ms step_avg:45.59ms
step:1077/1840 train_time:49110ms step_avg:45.60ms
step:1078/1840 train_time:49172ms step_avg:45.61ms
step:1079/1840 train_time:49232ms step_avg:45.63ms
step:1080/1840 train_time:49295ms step_avg:45.64ms
step:1081/1840 train_time:49355ms step_avg:45.66ms
step:1082/1840 train_time:49417ms step_avg:45.67ms
step:1083/1840 train_time:49477ms step_avg:45.69ms
step:1084/1840 train_time:49539ms step_avg:45.70ms
step:1085/1840 train_time:49599ms step_avg:45.71ms
step:1086/1840 train_time:49661ms step_avg:45.73ms
step:1087/1840 train_time:49721ms step_avg:45.74ms
step:1088/1840 train_time:49782ms step_avg:45.76ms
step:1089/1840 train_time:49842ms step_avg:45.77ms
step:1090/1840 train_time:49905ms step_avg:45.78ms
step:1091/1840 train_time:49965ms step_avg:45.80ms
step:1092/1840 train_time:50027ms step_avg:45.81ms
step:1093/1840 train_time:50087ms step_avg:45.83ms
step:1094/1840 train_time:50150ms step_avg:45.84ms
step:1095/1840 train_time:50211ms step_avg:45.86ms
step:1096/1840 train_time:50274ms step_avg:45.87ms
step:1097/1840 train_time:50334ms step_avg:45.88ms
step:1098/1840 train_time:50396ms step_avg:45.90ms
step:1099/1840 train_time:50456ms step_avg:45.91ms
step:1100/1840 train_time:50518ms step_avg:45.93ms
step:1101/1840 train_time:50578ms step_avg:45.94ms
step:1102/1840 train_time:50640ms step_avg:45.95ms
step:1103/1840 train_time:50700ms step_avg:45.97ms
step:1104/1840 train_time:50763ms step_avg:45.98ms
step:1105/1840 train_time:50821ms step_avg:45.99ms
step:1106/1840 train_time:50884ms step_avg:46.01ms
step:1107/1840 train_time:50943ms step_avg:46.02ms
step:1108/1840 train_time:51005ms step_avg:46.03ms
step:1109/1840 train_time:51065ms step_avg:46.05ms
step:1110/1840 train_time:51127ms step_avg:46.06ms
step:1111/1840 train_time:51187ms step_avg:46.07ms
step:1112/1840 train_time:51250ms step_avg:46.09ms
step:1113/1840 train_time:51311ms step_avg:46.10ms
step:1114/1840 train_time:51374ms step_avg:46.12ms
step:1115/1840 train_time:51434ms step_avg:46.13ms
step:1116/1840 train_time:51496ms step_avg:46.14ms
step:1117/1840 train_time:51556ms step_avg:46.16ms
step:1118/1840 train_time:51619ms step_avg:46.17ms
step:1119/1840 train_time:51679ms step_avg:46.18ms
step:1120/1840 train_time:51741ms step_avg:46.20ms
step:1121/1840 train_time:51800ms step_avg:46.21ms
step:1122/1840 train_time:51863ms step_avg:46.22ms
step:1123/1840 train_time:51922ms step_avg:46.24ms
step:1124/1840 train_time:51985ms step_avg:46.25ms
step:1125/1840 train_time:52044ms step_avg:46.26ms
step:1126/1840 train_time:52106ms step_avg:46.28ms
step:1127/1840 train_time:52166ms step_avg:46.29ms
step:1128/1840 train_time:52229ms step_avg:46.30ms
step:1129/1840 train_time:52289ms step_avg:46.31ms
step:1130/1840 train_time:52353ms step_avg:46.33ms
step:1131/1840 train_time:52413ms step_avg:46.34ms
step:1132/1840 train_time:52475ms step_avg:46.36ms
step:1133/1840 train_time:52536ms step_avg:46.37ms
step:1134/1840 train_time:52598ms step_avg:46.38ms
step:1135/1840 train_time:52659ms step_avg:46.40ms
step:1136/1840 train_time:52721ms step_avg:46.41ms
step:1137/1840 train_time:52781ms step_avg:46.42ms
step:1138/1840 train_time:52842ms step_avg:46.43ms
step:1139/1840 train_time:52902ms step_avg:46.45ms
step:1140/1840 train_time:52964ms step_avg:46.46ms
step:1141/1840 train_time:53022ms step_avg:46.47ms
step:1142/1840 train_time:53085ms step_avg:46.48ms
step:1143/1840 train_time:53145ms step_avg:46.50ms
step:1144/1840 train_time:53207ms step_avg:46.51ms
step:1145/1840 train_time:53267ms step_avg:46.52ms
step:1146/1840 train_time:53330ms step_avg:46.54ms
step:1147/1840 train_time:53390ms step_avg:46.55ms
step:1148/1840 train_time:53453ms step_avg:46.56ms
step:1149/1840 train_time:53513ms step_avg:46.57ms
step:1150/1840 train_time:53576ms step_avg:46.59ms
step:1151/1840 train_time:53637ms step_avg:46.60ms
step:1152/1840 train_time:53699ms step_avg:46.61ms
step:1153/1840 train_time:53758ms step_avg:46.62ms
step:1154/1840 train_time:53821ms step_avg:46.64ms
step:1155/1840 train_time:53881ms step_avg:46.65ms
step:1156/1840 train_time:53942ms step_avg:46.66ms
step:1157/1840 train_time:54002ms step_avg:46.67ms
step:1158/1840 train_time:54064ms step_avg:46.69ms
step:1159/1840 train_time:54124ms step_avg:46.70ms
step:1160/1840 train_time:54186ms step_avg:46.71ms
step:1161/1840 train_time:54246ms step_avg:46.72ms
step:1162/1840 train_time:54308ms step_avg:46.74ms
step:1163/1840 train_time:54368ms step_avg:46.75ms
step:1164/1840 train_time:54431ms step_avg:46.76ms
step:1165/1840 train_time:54491ms step_avg:46.77ms
step:1166/1840 train_time:54554ms step_avg:46.79ms
step:1167/1840 train_time:54615ms step_avg:46.80ms
step:1168/1840 train_time:54677ms step_avg:46.81ms
step:1169/1840 train_time:54738ms step_avg:46.82ms
step:1170/1840 train_time:54800ms step_avg:46.84ms
step:1171/1840 train_time:54859ms step_avg:46.85ms
step:1172/1840 train_time:54922ms step_avg:46.86ms
step:1173/1840 train_time:54982ms step_avg:46.87ms
step:1174/1840 train_time:55043ms step_avg:46.88ms
step:1175/1840 train_time:55103ms step_avg:46.90ms
step:1176/1840 train_time:55165ms step_avg:46.91ms
step:1177/1840 train_time:55224ms step_avg:46.92ms
step:1178/1840 train_time:55287ms step_avg:46.93ms
step:1179/1840 train_time:55346ms step_avg:46.94ms
step:1180/1840 train_time:55409ms step_avg:46.96ms
step:1181/1840 train_time:55470ms step_avg:46.97ms
step:1182/1840 train_time:55533ms step_avg:46.98ms
step:1183/1840 train_time:55594ms step_avg:46.99ms
step:1184/1840 train_time:55656ms step_avg:47.01ms
step:1185/1840 train_time:55716ms step_avg:47.02ms
step:1186/1840 train_time:55779ms step_avg:47.03ms
step:1187/1840 train_time:55839ms step_avg:47.04ms
step:1188/1840 train_time:55901ms step_avg:47.05ms
step:1189/1840 train_time:55961ms step_avg:47.07ms
step:1190/1840 train_time:56022ms step_avg:47.08ms
step:1191/1840 train_time:56082ms step_avg:47.09ms
step:1192/1840 train_time:56144ms step_avg:47.10ms
step:1193/1840 train_time:56203ms step_avg:47.11ms
step:1194/1840 train_time:56266ms step_avg:47.12ms
step:1195/1840 train_time:56325ms step_avg:47.13ms
step:1196/1840 train_time:56387ms step_avg:47.15ms
step:1197/1840 train_time:56448ms step_avg:47.16ms
step:1198/1840 train_time:56511ms step_avg:47.17ms
step:1199/1840 train_time:56571ms step_avg:47.18ms
step:1200/1840 train_time:56634ms step_avg:47.20ms
step:1201/1840 train_time:56696ms step_avg:47.21ms
step:1202/1840 train_time:56783ms step_avg:47.24ms
step:1203/1840 train_time:56870ms step_avg:47.27ms
step:1204/1840 train_time:56959ms step_avg:47.31ms
step:1205/1840 train_time:57046ms step_avg:47.34ms
step:1206/1840 train_time:57134ms step_avg:47.37ms
step:1207/1840 train_time:57221ms step_avg:47.41ms
step:1208/1840 train_time:57308ms step_avg:47.44ms
step:1209/1840 train_time:57395ms step_avg:47.47ms
step:1210/1840 train_time:57485ms step_avg:47.51ms
step:1211/1840 train_time:57572ms step_avg:47.54ms
step:1212/1840 train_time:57661ms step_avg:47.58ms
step:1213/1840 train_time:57749ms step_avg:47.61ms
step:1214/1840 train_time:57838ms step_avg:47.64ms
step:1215/1840 train_time:57926ms step_avg:47.68ms
step:1216/1840 train_time:58015ms step_avg:47.71ms
step:1217/1840 train_time:58102ms step_avg:47.74ms
step:1218/1840 train_time:58190ms step_avg:47.78ms
step:1219/1840 train_time:58276ms step_avg:47.81ms
step:1220/1840 train_time:58365ms step_avg:47.84ms
step:1221/1840 train_time:58451ms step_avg:47.87ms
step:1222/1840 train_time:58539ms step_avg:47.90ms
step:1223/1840 train_time:58625ms step_avg:47.94ms
step:1224/1840 train_time:58714ms step_avg:47.97ms
step:1225/1840 train_time:58801ms step_avg:48.00ms
step:1226/1840 train_time:58891ms step_avg:48.03ms
step:1227/1840 train_time:58977ms step_avg:48.07ms
step:1228/1840 train_time:59066ms step_avg:48.10ms
step:1229/1840 train_time:59152ms step_avg:48.13ms
step:1230/1840 train_time:59240ms step_avg:48.16ms
step:1231/1840 train_time:59327ms step_avg:48.19ms
step:1232/1840 train_time:59416ms step_avg:48.23ms
step:1233/1840 train_time:59503ms step_avg:48.26ms
step:1234/1840 train_time:59591ms step_avg:48.29ms
step:1235/1840 train_time:59677ms step_avg:48.32ms
step:1236/1840 train_time:59767ms step_avg:48.36ms
step:1237/1840 train_time:59853ms step_avg:48.39ms
step:1238/1840 train_time:59942ms step_avg:48.42ms
step:1239/1840 train_time:60029ms step_avg:48.45ms
step:1240/1840 train_time:60118ms step_avg:48.48ms
step:1241/1840 train_time:60205ms step_avg:48.51ms
step:1242/1840 train_time:60292ms step_avg:48.54ms
step:1243/1840 train_time:60378ms step_avg:48.57ms
step:1244/1840 train_time:60467ms step_avg:48.61ms
step:1245/1840 train_time:60553ms step_avg:48.64ms
step:1246/1840 train_time:60642ms step_avg:48.67ms
step:1247/1840 train_time:60729ms step_avg:48.70ms
step:1248/1840 train_time:60817ms step_avg:48.73ms
step:1249/1840 train_time:60904ms step_avg:48.76ms
step:1250/1840 train_time:60992ms step_avg:48.79ms
step:1250/1840 val_loss:3.5338 train_time:61092ms step_avg:48.87ms
step:1251/1840 train_time:61113ms step_avg:48.85ms
step:1252/1840 train_time:61169ms step_avg:48.86ms
step:1253/1840 train_time:61259ms step_avg:48.89ms
step:1254/1840 train_time:61352ms step_avg:48.92ms
step:1255/1840 train_time:61437ms step_avg:48.95ms
step:1256/1840 train_time:61527ms step_avg:48.99ms
step:1257/1840 train_time:61612ms step_avg:49.02ms
step:1258/1840 train_time:61700ms step_avg:49.05ms
step:1259/1840 train_time:61785ms step_avg:49.07ms
step:1260/1840 train_time:61873ms step_avg:49.11ms
step:1261/1840 train_time:61958ms step_avg:49.13ms
step:1262/1840 train_time:62047ms step_avg:49.17ms
step:1263/1840 train_time:62136ms step_avg:49.20ms
step:1264/1840 train_time:62227ms step_avg:49.23ms
step:1265/1840 train_time:62316ms step_avg:49.26ms
step:1266/1840 train_time:62406ms step_avg:49.29ms
step:1267/1840 train_time:62492ms step_avg:49.32ms
step:1268/1840 train_time:62580ms step_avg:49.35ms
step:1269/1840 train_time:62665ms step_avg:49.38ms
step:1270/1840 train_time:62754ms step_avg:49.41ms
step:1271/1840 train_time:62839ms step_avg:49.44ms
step:1272/1840 train_time:62926ms step_avg:49.47ms
step:1273/1840 train_time:63014ms step_avg:49.50ms
step:1274/1840 train_time:63103ms step_avg:49.53ms
step:1275/1840 train_time:63193ms step_avg:49.56ms
step:1276/1840 train_time:63281ms step_avg:49.59ms
step:1277/1840 train_time:63367ms step_avg:49.62ms
step:1278/1840 train_time:63456ms step_avg:49.65ms
step:1279/1840 train_time:63542ms step_avg:49.68ms
step:1280/1840 train_time:63631ms step_avg:49.71ms
step:1281/1840 train_time:63716ms step_avg:49.74ms
step:1282/1840 train_time:63803ms step_avg:49.77ms
step:1283/1840 train_time:63890ms step_avg:49.80ms
step:1284/1840 train_time:63978ms step_avg:49.83ms
step:1285/1840 train_time:64064ms step_avg:49.85ms
step:1286/1840 train_time:64154ms step_avg:49.89ms
step:1287/1840 train_time:64240ms step_avg:49.91ms
step:1288/1840 train_time:64330ms step_avg:49.95ms
step:1289/1840 train_time:64417ms step_avg:49.97ms
step:1290/1840 train_time:64505ms step_avg:50.00ms
step:1291/1840 train_time:64592ms step_avg:50.03ms
step:1292/1840 train_time:64679ms step_avg:50.06ms
step:1293/1840 train_time:64766ms step_avg:50.09ms
step:1294/1840 train_time:64854ms step_avg:50.12ms
step:1295/1840 train_time:64939ms step_avg:50.15ms
step:1296/1840 train_time:65029ms step_avg:50.18ms
step:1297/1840 train_time:65116ms step_avg:50.20ms
step:1298/1840 train_time:65204ms step_avg:50.23ms
step:1299/1840 train_time:65291ms step_avg:50.26ms
step:1300/1840 train_time:65380ms step_avg:50.29ms
step:1301/1840 train_time:65467ms step_avg:50.32ms
step:1302/1840 train_time:65556ms step_avg:50.35ms
step:1303/1840 train_time:65642ms step_avg:50.38ms
step:1304/1840 train_time:65732ms step_avg:50.41ms
step:1305/1840 train_time:65818ms step_avg:50.44ms
step:1306/1840 train_time:65907ms step_avg:50.46ms
step:1307/1840 train_time:65993ms step_avg:50.49ms
step:1308/1840 train_time:66082ms step_avg:50.52ms
step:1309/1840 train_time:66168ms step_avg:50.55ms
step:1310/1840 train_time:66256ms step_avg:50.58ms
step:1311/1840 train_time:66344ms step_avg:50.61ms
step:1312/1840 train_time:66433ms step_avg:50.64ms
step:1313/1840 train_time:66519ms step_avg:50.66ms
step:1314/1840 train_time:66609ms step_avg:50.69ms
step:1315/1840 train_time:66695ms step_avg:50.72ms
step:1316/1840 train_time:66782ms step_avg:50.75ms
step:1317/1840 train_time:66868ms step_avg:50.77ms
step:1318/1840 train_time:66956ms step_avg:50.80ms
step:1319/1840 train_time:67043ms step_avg:50.83ms
step:1320/1840 train_time:67133ms step_avg:50.86ms
step:1321/1840 train_time:67219ms step_avg:50.88ms
step:1322/1840 train_time:67309ms step_avg:50.91ms
step:1323/1840 train_time:67394ms step_avg:50.94ms
step:1324/1840 train_time:67482ms step_avg:50.97ms
step:1325/1840 train_time:67570ms step_avg:51.00ms
step:1326/1840 train_time:67658ms step_avg:51.02ms
step:1327/1840 train_time:67745ms step_avg:51.05ms
step:1328/1840 train_time:67833ms step_avg:51.08ms
step:1329/1840 train_time:67918ms step_avg:51.10ms
step:1330/1840 train_time:68007ms step_avg:51.13ms
step:1331/1840 train_time:68094ms step_avg:51.16ms
step:1332/1840 train_time:68183ms step_avg:51.19ms
step:1333/1840 train_time:68269ms step_avg:51.21ms
step:1334/1840 train_time:68358ms step_avg:51.24ms
step:1335/1840 train_time:68444ms step_avg:51.27ms
step:1336/1840 train_time:68534ms step_avg:51.30ms
step:1337/1840 train_time:68619ms step_avg:51.32ms
step:1338/1840 train_time:68709ms step_avg:51.35ms
step:1339/1840 train_time:68794ms step_avg:51.38ms
step:1340/1840 train_time:68883ms step_avg:51.40ms
step:1341/1840 train_time:68968ms step_avg:51.43ms
step:1342/1840 train_time:69056ms step_avg:51.46ms
step:1343/1840 train_time:69142ms step_avg:51.48ms
step:1344/1840 train_time:69233ms step_avg:51.51ms
step:1345/1840 train_time:69319ms step_avg:51.54ms
step:1346/1840 train_time:69409ms step_avg:51.57ms
step:1347/1840 train_time:69496ms step_avg:51.59ms
step:1348/1840 train_time:69584ms step_avg:51.62ms
step:1349/1840 train_time:69669ms step_avg:51.65ms
step:1350/1840 train_time:69759ms step_avg:51.67ms
step:1351/1840 train_time:69846ms step_avg:51.70ms
step:1352/1840 train_time:69935ms step_avg:51.73ms
step:1353/1840 train_time:70021ms step_avg:51.75ms
step:1354/1840 train_time:70110ms step_avg:51.78ms
step:1355/1840 train_time:70196ms step_avg:51.81ms
step:1356/1840 train_time:70286ms step_avg:51.83ms
step:1357/1840 train_time:70372ms step_avg:51.86ms
step:1358/1840 train_time:70461ms step_avg:51.89ms
step:1359/1840 train_time:70547ms step_avg:51.91ms
step:1360/1840 train_time:70637ms step_avg:51.94ms
step:1361/1840 train_time:70724ms step_avg:51.96ms
step:1362/1840 train_time:70812ms step_avg:51.99ms
step:1363/1840 train_time:70898ms step_avg:52.02ms
step:1364/1840 train_time:70986ms step_avg:52.04ms
step:1365/1840 train_time:71073ms step_avg:52.07ms
step:1366/1840 train_time:71163ms step_avg:52.10ms
step:1367/1840 train_time:71249ms step_avg:52.12ms
step:1368/1840 train_time:71338ms step_avg:52.15ms
step:1369/1840 train_time:71423ms step_avg:52.17ms
step:1370/1840 train_time:71513ms step_avg:52.20ms
step:1371/1840 train_time:71599ms step_avg:52.22ms
step:1372/1840 train_time:71688ms step_avg:52.25ms
step:1373/1840 train_time:71773ms step_avg:52.27ms
step:1374/1840 train_time:71862ms step_avg:52.30ms
step:1375/1840 train_time:71949ms step_avg:52.33ms
step:1376/1840 train_time:72038ms step_avg:52.35ms
step:1377/1840 train_time:72126ms step_avg:52.38ms
step:1378/1840 train_time:72216ms step_avg:52.41ms
step:1379/1840 train_time:72303ms step_avg:52.43ms
step:1380/1840 train_time:72392ms step_avg:52.46ms
step:1381/1840 train_time:72478ms step_avg:52.48ms
step:1382/1840 train_time:72565ms step_avg:52.51ms
step:1383/1840 train_time:72653ms step_avg:52.53ms
step:1384/1840 train_time:72742ms step_avg:52.56ms
step:1385/1840 train_time:72827ms step_avg:52.58ms
step:1386/1840 train_time:72917ms step_avg:52.61ms
step:1387/1840 train_time:73004ms step_avg:52.63ms
step:1388/1840 train_time:73092ms step_avg:52.66ms
step:1389/1840 train_time:73179ms step_avg:52.68ms
step:1390/1840 train_time:73267ms step_avg:52.71ms
step:1391/1840 train_time:73354ms step_avg:52.73ms
step:1392/1840 train_time:73444ms step_avg:52.76ms
step:1393/1840 train_time:73530ms step_avg:52.79ms
step:1394/1840 train_time:73618ms step_avg:52.81ms
step:1395/1840 train_time:73704ms step_avg:52.83ms
step:1396/1840 train_time:73794ms step_avg:52.86ms
step:1397/1840 train_time:73880ms step_avg:52.88ms
step:1398/1840 train_time:73968ms step_avg:52.91ms
step:1399/1840 train_time:74054ms step_avg:52.93ms
step:1400/1840 train_time:74142ms step_avg:52.96ms
step:1401/1840 train_time:74229ms step_avg:52.98ms
step:1402/1840 train_time:74317ms step_avg:53.01ms
step:1403/1840 train_time:74403ms step_avg:53.03ms
step:1404/1840 train_time:74492ms step_avg:53.06ms
step:1405/1840 train_time:74578ms step_avg:53.08ms
step:1406/1840 train_time:74667ms step_avg:53.11ms
step:1407/1840 train_time:74754ms step_avg:53.13ms
step:1408/1840 train_time:74842ms step_avg:53.15ms
step:1409/1840 train_time:74929ms step_avg:53.18ms
step:1410/1840 train_time:75017ms step_avg:53.20ms
step:1411/1840 train_time:75103ms step_avg:53.23ms
step:1412/1840 train_time:75192ms step_avg:53.25ms
step:1413/1840 train_time:75277ms step_avg:53.27ms
step:1414/1840 train_time:75366ms step_avg:53.30ms
step:1415/1840 train_time:75452ms step_avg:53.32ms
step:1416/1840 train_time:75540ms step_avg:53.35ms
step:1417/1840 train_time:75627ms step_avg:53.37ms
step:1418/1840 train_time:75717ms step_avg:53.40ms
step:1419/1840 train_time:75804ms step_avg:53.42ms
step:1420/1840 train_time:75893ms step_avg:53.45ms
step:1421/1840 train_time:75979ms step_avg:53.47ms
step:1422/1840 train_time:76067ms step_avg:53.49ms
step:1423/1840 train_time:76155ms step_avg:53.52ms
step:1424/1840 train_time:76243ms step_avg:53.54ms
step:1425/1840 train_time:76329ms step_avg:53.56ms
step:1426/1840 train_time:76418ms step_avg:53.59ms
step:1427/1840 train_time:76503ms step_avg:53.61ms
step:1428/1840 train_time:76593ms step_avg:53.64ms
step:1429/1840 train_time:76679ms step_avg:53.66ms
step:1430/1840 train_time:76768ms step_avg:53.68ms
step:1431/1840 train_time:76855ms step_avg:53.71ms
step:1432/1840 train_time:76944ms step_avg:53.73ms
step:1433/1840 train_time:77030ms step_avg:53.75ms
step:1434/1840 train_time:77118ms step_avg:53.78ms
step:1435/1840 train_time:77205ms step_avg:53.80ms
step:1436/1840 train_time:77295ms step_avg:53.83ms
step:1437/1840 train_time:77380ms step_avg:53.85ms
step:1438/1840 train_time:77470ms step_avg:53.87ms
step:1439/1840 train_time:77556ms step_avg:53.90ms
step:1440/1840 train_time:77645ms step_avg:53.92ms
step:1441/1840 train_time:77732ms step_avg:53.94ms
step:1442/1840 train_time:77821ms step_avg:53.97ms
step:1443/1840 train_time:77907ms step_avg:53.99ms
step:1444/1840 train_time:77995ms step_avg:54.01ms
step:1445/1840 train_time:78081ms step_avg:54.04ms
step:1446/1840 train_time:78170ms step_avg:54.06ms
step:1447/1840 train_time:78256ms step_avg:54.08ms
step:1448/1840 train_time:78344ms step_avg:54.11ms
step:1449/1840 train_time:78431ms step_avg:54.13ms
step:1450/1840 train_time:78521ms step_avg:54.15ms
step:1451/1840 train_time:78607ms step_avg:54.17ms
step:1452/1840 train_time:78696ms step_avg:54.20ms
step:1453/1840 train_time:78783ms step_avg:54.22ms
step:1454/1840 train_time:78872ms step_avg:54.24ms
step:1455/1840 train_time:78957ms step_avg:54.27ms
step:1456/1840 train_time:79046ms step_avg:54.29ms
step:1457/1840 train_time:79132ms step_avg:54.31ms
step:1458/1840 train_time:79221ms step_avg:54.34ms
step:1459/1840 train_time:79307ms step_avg:54.36ms
step:1460/1840 train_time:79395ms step_avg:54.38ms
step:1461/1840 train_time:79480ms step_avg:54.40ms
step:1462/1840 train_time:79569ms step_avg:54.42ms
step:1463/1840 train_time:79655ms step_avg:54.45ms
step:1464/1840 train_time:79745ms step_avg:54.47ms
step:1465/1840 train_time:79832ms step_avg:54.49ms
step:1466/1840 train_time:79919ms step_avg:54.52ms
step:1467/1840 train_time:80007ms step_avg:54.54ms
step:1468/1840 train_time:80096ms step_avg:54.56ms
step:1469/1840 train_time:80184ms step_avg:54.58ms
step:1470/1840 train_time:80273ms step_avg:54.61ms
step:1471/1840 train_time:80358ms step_avg:54.63ms
step:1472/1840 train_time:80447ms step_avg:54.65ms
step:1473/1840 train_time:80534ms step_avg:54.67ms
step:1474/1840 train_time:80623ms step_avg:54.70ms
step:1475/1840 train_time:80709ms step_avg:54.72ms
step:1476/1840 train_time:80796ms step_avg:54.74ms
step:1477/1840 train_time:80883ms step_avg:54.76ms
step:1478/1840 train_time:80971ms step_avg:54.78ms
step:1479/1840 train_time:81057ms step_avg:54.81ms
step:1480/1840 train_time:81146ms step_avg:54.83ms
step:1481/1840 train_time:81233ms step_avg:54.85ms
step:1482/1840 train_time:81321ms step_avg:54.87ms
step:1483/1840 train_time:81408ms step_avg:54.89ms
step:1484/1840 train_time:81496ms step_avg:54.92ms
step:1485/1840 train_time:81583ms step_avg:54.94ms
step:1486/1840 train_time:81673ms step_avg:54.96ms
step:1487/1840 train_time:81759ms step_avg:54.98ms
step:1488/1840 train_time:81847ms step_avg:55.00ms
step:1489/1840 train_time:81934ms step_avg:55.03ms
step:1490/1840 train_time:82022ms step_avg:55.05ms
step:1491/1840 train_time:82108ms step_avg:55.07ms
step:1492/1840 train_time:82197ms step_avg:55.09ms
step:1493/1840 train_time:82284ms step_avg:55.11ms
step:1494/1840 train_time:82373ms step_avg:55.14ms
step:1495/1840 train_time:82459ms step_avg:55.16ms
step:1496/1840 train_time:82548ms step_avg:55.18ms
step:1497/1840 train_time:82634ms step_avg:55.20ms
step:1498/1840 train_time:82723ms step_avg:55.22ms
step:1499/1840 train_time:82808ms step_avg:55.24ms
step:1500/1840 train_time:82897ms step_avg:55.26ms
step:1500/1840 val_loss:3.4025 train_time:82997ms step_avg:55.33ms
step:1501/1840 train_time:83017ms step_avg:55.31ms
step:1502/1840 train_time:83074ms step_avg:55.31ms
step:1503/1840 train_time:83163ms step_avg:55.33ms
step:1504/1840 train_time:83251ms step_avg:55.35ms
step:1505/1840 train_time:83337ms step_avg:55.37ms
step:1506/1840 train_time:83425ms step_avg:55.39ms
step:1507/1840 train_time:83510ms step_avg:55.41ms
step:1508/1840 train_time:83598ms step_avg:55.44ms
step:1509/1840 train_time:83682ms step_avg:55.46ms
step:1510/1840 train_time:83770ms step_avg:55.48ms
step:1511/1840 train_time:83856ms step_avg:55.50ms
step:1512/1840 train_time:83947ms step_avg:55.52ms
step:1513/1840 train_time:84036ms step_avg:55.54ms
step:1514/1840 train_time:84127ms step_avg:55.57ms
step:1515/1840 train_time:84214ms step_avg:55.59ms
step:1516/1840 train_time:84302ms step_avg:55.61ms
step:1517/1840 train_time:84387ms step_avg:55.63ms
step:1518/1840 train_time:84475ms step_avg:55.65ms
step:1519/1840 train_time:84559ms step_avg:55.67ms
step:1520/1840 train_time:84649ms step_avg:55.69ms
step:1521/1840 train_time:84735ms step_avg:55.71ms
step:1522/1840 train_time:84823ms step_avg:55.73ms
step:1523/1840 train_time:84910ms step_avg:55.75ms
step:1524/1840 train_time:84999ms step_avg:55.77ms
step:1525/1840 train_time:85088ms step_avg:55.80ms
step:1526/1840 train_time:85178ms step_avg:55.82ms
step:1527/1840 train_time:85264ms step_avg:55.84ms
step:1528/1840 train_time:85352ms step_avg:55.86ms
step:1529/1840 train_time:85437ms step_avg:55.88ms
step:1530/1840 train_time:85525ms step_avg:55.90ms
step:1531/1840 train_time:85611ms step_avg:55.92ms
step:1532/1840 train_time:85699ms step_avg:55.94ms
step:1533/1840 train_time:85784ms step_avg:55.96ms
step:1534/1840 train_time:85875ms step_avg:55.98ms
step:1535/1840 train_time:85961ms step_avg:56.00ms
step:1536/1840 train_time:86051ms step_avg:56.02ms
step:1537/1840 train_time:86138ms step_avg:56.04ms
step:1538/1840 train_time:86227ms step_avg:56.06ms
step:1539/1840 train_time:86314ms step_avg:56.08ms
step:1540/1840 train_time:86403ms step_avg:56.11ms
step:1541/1840 train_time:86489ms step_avg:56.12ms
step:1542/1840 train_time:86577ms step_avg:56.15ms
step:1543/1840 train_time:86661ms step_avg:56.16ms
step:1544/1840 train_time:86752ms step_avg:56.19ms
step:1545/1840 train_time:86839ms step_avg:56.21ms
step:1546/1840 train_time:86928ms step_avg:56.23ms
step:1547/1840 train_time:87015ms step_avg:56.25ms
step:1548/1840 train_time:87104ms step_avg:56.27ms
step:1549/1840 train_time:87189ms step_avg:56.29ms
step:1550/1840 train_time:87278ms step_avg:56.31ms
step:1551/1840 train_time:87364ms step_avg:56.33ms
step:1552/1840 train_time:87453ms step_avg:56.35ms
step:1553/1840 train_time:87539ms step_avg:56.37ms
step:1554/1840 train_time:87627ms step_avg:56.39ms
step:1555/1840 train_time:87713ms step_avg:56.41ms
step:1556/1840 train_time:87801ms step_avg:56.43ms
step:1557/1840 train_time:87886ms step_avg:56.45ms
step:1558/1840 train_time:87977ms step_avg:56.47ms
step:1559/1840 train_time:88063ms step_avg:56.49ms
step:1560/1840 train_time:88152ms step_avg:56.51ms
step:1561/1840 train_time:88237ms step_avg:56.53ms
step:1562/1840 train_time:88328ms step_avg:56.55ms
step:1563/1840 train_time:88414ms step_avg:56.57ms
step:1564/1840 train_time:88502ms step_avg:56.59ms
step:1565/1840 train_time:88587ms step_avg:56.61ms
step:1566/1840 train_time:88676ms step_avg:56.63ms
step:1567/1840 train_time:88761ms step_avg:56.64ms
step:1568/1840 train_time:88850ms step_avg:56.66ms
step:1569/1840 train_time:88938ms step_avg:56.68ms
step:1570/1840 train_time:89026ms step_avg:56.70ms
step:1571/1840 train_time:89113ms step_avg:56.72ms
step:1572/1840 train_time:89201ms step_avg:56.74ms
step:1573/1840 train_time:89287ms step_avg:56.76ms
step:1574/1840 train_time:89376ms step_avg:56.78ms
step:1575/1840 train_time:89461ms step_avg:56.80ms
step:1576/1840 train_time:89551ms step_avg:56.82ms
step:1577/1840 train_time:89637ms step_avg:56.84ms
step:1578/1840 train_time:89726ms step_avg:56.86ms
step:1579/1840 train_time:89812ms step_avg:56.88ms
step:1580/1840 train_time:89901ms step_avg:56.90ms
step:1581/1840 train_time:89987ms step_avg:56.92ms
step:1582/1840 train_time:90076ms step_avg:56.94ms
step:1583/1840 train_time:90163ms step_avg:56.96ms
step:1584/1840 train_time:90252ms step_avg:56.98ms
step:1585/1840 train_time:90338ms step_avg:57.00ms
step:1586/1840 train_time:90427ms step_avg:57.02ms
step:1587/1840 train_time:90512ms step_avg:57.03ms
step:1588/1840 train_time:90600ms step_avg:57.05ms
step:1589/1840 train_time:90686ms step_avg:57.07ms
step:1590/1840 train_time:90776ms step_avg:57.09ms
step:1591/1840 train_time:90863ms step_avg:57.11ms
step:1592/1840 train_time:90951ms step_avg:57.13ms
step:1593/1840 train_time:91037ms step_avg:57.15ms
step:1594/1840 train_time:91127ms step_avg:57.17ms
step:1595/1840 train_time:91212ms step_avg:57.19ms
step:1596/1840 train_time:91300ms step_avg:57.21ms
step:1597/1840 train_time:91386ms step_avg:57.22ms
step:1598/1840 train_time:91475ms step_avg:57.24ms
step:1599/1840 train_time:91560ms step_avg:57.26ms
step:1600/1840 train_time:91650ms step_avg:57.28ms
step:1601/1840 train_time:91737ms step_avg:57.30ms
step:1602/1840 train_time:91825ms step_avg:57.32ms
step:1603/1840 train_time:91912ms step_avg:57.34ms
step:1604/1840 train_time:92001ms step_avg:57.36ms
step:1605/1840 train_time:92087ms step_avg:57.38ms
step:1606/1840 train_time:92177ms step_avg:57.40ms
step:1607/1840 train_time:92263ms step_avg:57.41ms
step:1608/1840 train_time:92352ms step_avg:57.43ms
step:1609/1840 train_time:92437ms step_avg:57.45ms
step:1610/1840 train_time:92525ms step_avg:57.47ms
step:1611/1840 train_time:92611ms step_avg:57.49ms
step:1612/1840 train_time:92700ms step_avg:57.51ms
step:1613/1840 train_time:92786ms step_avg:57.52ms
step:1614/1840 train_time:92877ms step_avg:57.54ms
step:1615/1840 train_time:92962ms step_avg:57.56ms
step:1616/1840 train_time:93052ms step_avg:57.58ms
step:1617/1840 train_time:93138ms step_avg:57.60ms
step:1618/1840 train_time:93227ms step_avg:57.62ms
step:1619/1840 train_time:93312ms step_avg:57.64ms
step:1620/1840 train_time:93400ms step_avg:57.65ms
step:1621/1840 train_time:93487ms step_avg:57.67ms
step:1622/1840 train_time:93576ms step_avg:57.69ms
step:1623/1840 train_time:93661ms step_avg:57.71ms
step:1624/1840 train_time:93749ms step_avg:57.73ms
step:1625/1840 train_time:93838ms step_avg:57.75ms
step:1626/1840 train_time:93926ms step_avg:57.77ms
step:1627/1840 train_time:94013ms step_avg:57.78ms
step:1628/1840 train_time:94102ms step_avg:57.80ms
step:1629/1840 train_time:94188ms step_avg:57.82ms
step:1630/1840 train_time:94277ms step_avg:57.84ms
step:1631/1840 train_time:94363ms step_avg:57.86ms
step:1632/1840 train_time:94453ms step_avg:57.88ms
step:1633/1840 train_time:94538ms step_avg:57.89ms
step:1634/1840 train_time:94625ms step_avg:57.91ms
step:1635/1840 train_time:94713ms step_avg:57.93ms
step:1636/1840 train_time:94802ms step_avg:57.95ms
step:1637/1840 train_time:94887ms step_avg:57.96ms
step:1638/1840 train_time:94977ms step_avg:57.98ms
step:1639/1840 train_time:95062ms step_avg:58.00ms
step:1640/1840 train_time:95152ms step_avg:58.02ms
step:1641/1840 train_time:95238ms step_avg:58.04ms
step:1642/1840 train_time:95326ms step_avg:58.05ms
step:1643/1840 train_time:95413ms step_avg:58.07ms
step:1644/1840 train_time:95501ms step_avg:58.09ms
step:1645/1840 train_time:95586ms step_avg:58.11ms
step:1646/1840 train_time:95675ms step_avg:58.13ms
step:1647/1840 train_time:95760ms step_avg:58.14ms
step:1648/1840 train_time:95850ms step_avg:58.16ms
step:1649/1840 train_time:95937ms step_avg:58.18ms
step:1650/1840 train_time:96025ms step_avg:58.20ms
step:1651/1840 train_time:96112ms step_avg:58.21ms
step:1652/1840 train_time:96200ms step_avg:58.23ms
step:1653/1840 train_time:96286ms step_avg:58.25ms
step:1654/1840 train_time:96376ms step_avg:58.27ms
step:1655/1840 train_time:96461ms step_avg:58.28ms
step:1656/1840 train_time:96550ms step_avg:58.30ms
step:1657/1840 train_time:96636ms step_avg:58.32ms
step:1658/1840 train_time:96724ms step_avg:58.34ms
step:1659/1840 train_time:96812ms step_avg:58.36ms
step:1660/1840 train_time:96901ms step_avg:58.37ms
step:1661/1840 train_time:96988ms step_avg:58.39ms
step:1662/1840 train_time:97078ms step_avg:58.41ms
step:1663/1840 train_time:97164ms step_avg:58.43ms
step:1664/1840 train_time:97252ms step_avg:58.44ms
step:1665/1840 train_time:97338ms step_avg:58.46ms
step:1666/1840 train_time:97427ms step_avg:58.48ms
step:1667/1840 train_time:97513ms step_avg:58.50ms
step:1668/1840 train_time:97602ms step_avg:58.51ms
step:1669/1840 train_time:97688ms step_avg:58.53ms
step:1670/1840 train_time:97777ms step_avg:58.55ms
step:1671/1840 train_time:97863ms step_avg:58.57ms
step:1672/1840 train_time:97953ms step_avg:58.58ms
step:1673/1840 train_time:98038ms step_avg:58.60ms
step:1674/1840 train_time:98128ms step_avg:58.62ms
step:1675/1840 train_time:98214ms step_avg:58.64ms
step:1676/1840 train_time:98302ms step_avg:58.65ms
step:1677/1840 train_time:98388ms step_avg:58.67ms
step:1678/1840 train_time:98476ms step_avg:58.69ms
step:1679/1840 train_time:98562ms step_avg:58.70ms
step:1680/1840 train_time:98652ms step_avg:58.72ms
step:1681/1840 train_time:98738ms step_avg:58.74ms
step:1682/1840 train_time:98826ms step_avg:58.75ms
step:1683/1840 train_time:98912ms step_avg:58.77ms
step:1684/1840 train_time:99000ms step_avg:58.79ms
step:1685/1840 train_time:99086ms step_avg:58.81ms
step:1686/1840 train_time:99177ms step_avg:58.82ms
step:1687/1840 train_time:99262ms step_avg:58.84ms
step:1688/1840 train_time:99352ms step_avg:58.86ms
step:1689/1840 train_time:99438ms step_avg:58.87ms
step:1690/1840 train_time:99528ms step_avg:58.89ms
step:1691/1840 train_time:99615ms step_avg:58.91ms
step:1692/1840 train_time:99703ms step_avg:58.93ms
step:1693/1840 train_time:99789ms step_avg:58.94ms
step:1694/1840 train_time:99878ms step_avg:58.96ms
step:1695/1840 train_time:99963ms step_avg:58.98ms
step:1696/1840 train_time:100052ms step_avg:58.99ms
step:1697/1840 train_time:100138ms step_avg:59.01ms
step:1698/1840 train_time:100227ms step_avg:59.03ms
step:1699/1840 train_time:100314ms step_avg:59.04ms
step:1700/1840 train_time:100402ms step_avg:59.06ms
step:1701/1840 train_time:100487ms step_avg:59.08ms
step:1702/1840 train_time:100576ms step_avg:59.09ms
step:1703/1840 train_time:100661ms step_avg:59.11ms
step:1704/1840 train_time:100751ms step_avg:59.13ms
step:1705/1840 train_time:100838ms step_avg:59.14ms
step:1706/1840 train_time:100926ms step_avg:59.16ms
step:1707/1840 train_time:101013ms step_avg:59.18ms
step:1708/1840 train_time:101101ms step_avg:59.19ms
step:1709/1840 train_time:101186ms step_avg:59.21ms
step:1710/1840 train_time:101276ms step_avg:59.23ms
step:1711/1840 train_time:101362ms step_avg:59.24ms
step:1712/1840 train_time:101450ms step_avg:59.26ms
step:1713/1840 train_time:101537ms step_avg:59.27ms
step:1714/1840 train_time:101626ms step_avg:59.29ms
step:1715/1840 train_time:101712ms step_avg:59.31ms
step:1716/1840 train_time:101800ms step_avg:59.32ms
step:1717/1840 train_time:101885ms step_avg:59.34ms
step:1718/1840 train_time:101977ms step_avg:59.36ms
step:1719/1840 train_time:102062ms step_avg:59.37ms
step:1720/1840 train_time:102150ms step_avg:59.39ms
step:1721/1840 train_time:102238ms step_avg:59.41ms
step:1722/1840 train_time:102327ms step_avg:59.42ms
step:1723/1840 train_time:102413ms step_avg:59.44ms
step:1724/1840 train_time:102500ms step_avg:59.46ms
step:1725/1840 train_time:102586ms step_avg:59.47ms
step:1726/1840 train_time:102676ms step_avg:59.49ms
step:1727/1840 train_time:102761ms step_avg:59.50ms
step:1728/1840 train_time:102852ms step_avg:59.52ms
step:1729/1840 train_time:102938ms step_avg:59.54ms
step:1730/1840 train_time:103027ms step_avg:59.55ms
step:1731/1840 train_time:103114ms step_avg:59.57ms
step:1732/1840 train_time:103202ms step_avg:59.59ms
step:1733/1840 train_time:103288ms step_avg:59.60ms
step:1734/1840 train_time:103377ms step_avg:59.62ms
step:1735/1840 train_time:103462ms step_avg:59.63ms
step:1736/1840 train_time:103552ms step_avg:59.65ms
step:1737/1840 train_time:103637ms step_avg:59.66ms
step:1738/1840 train_time:103727ms step_avg:59.68ms
step:1739/1840 train_time:103813ms step_avg:59.70ms
step:1740/1840 train_time:103901ms step_avg:59.71ms
step:1741/1840 train_time:103988ms step_avg:59.73ms
step:1742/1840 train_time:104076ms step_avg:59.75ms
step:1743/1840 train_time:104161ms step_avg:59.76ms
step:1744/1840 train_time:104249ms step_avg:59.78ms
step:1745/1840 train_time:104337ms step_avg:59.79ms
step:1746/1840 train_time:104425ms step_avg:59.81ms
step:1747/1840 train_time:104511ms step_avg:59.82ms
step:1748/1840 train_time:104600ms step_avg:59.84ms
step:1749/1840 train_time:104686ms step_avg:59.85ms
step:1750/1840 train_time:104776ms step_avg:59.87ms
step:1750/1840 val_loss:3.3044 train_time:104876ms step_avg:59.93ms
step:1751/1840 train_time:104896ms step_avg:59.91ms
step:1752/1840 train_time:104953ms step_avg:59.90ms
step:1753/1840 train_time:105045ms step_avg:59.92ms
step:1754/1840 train_time:105134ms step_avg:59.94ms
step:1755/1840 train_time:105221ms step_avg:59.95ms
step:1756/1840 train_time:105308ms step_avg:59.97ms
step:1757/1840 train_time:105393ms step_avg:59.98ms
step:1758/1840 train_time:105481ms step_avg:60.00ms
step:1759/1840 train_time:105566ms step_avg:60.01ms
step:1760/1840 train_time:105654ms step_avg:60.03ms
step:1761/1840 train_time:105740ms step_avg:60.05ms
step:1762/1840 train_time:105830ms step_avg:60.06ms
step:1763/1840 train_time:105918ms step_avg:60.08ms
step:1764/1840 train_time:106010ms step_avg:60.10ms
step:1765/1840 train_time:106099ms step_avg:60.11ms
step:1766/1840 train_time:106187ms step_avg:60.13ms
step:1767/1840 train_time:106273ms step_avg:60.14ms
step:1768/1840 train_time:106362ms step_avg:60.16ms
step:1769/1840 train_time:106446ms step_avg:60.17ms
step:1770/1840 train_time:106534ms step_avg:60.19ms
step:1771/1840 train_time:106620ms step_avg:60.20ms
step:1772/1840 train_time:106707ms step_avg:60.22ms
step:1773/1840 train_time:106793ms step_avg:60.23ms
step:1774/1840 train_time:106884ms step_avg:60.25ms
step:1775/1840 train_time:106971ms step_avg:60.27ms
step:1776/1840 train_time:107061ms step_avg:60.28ms
step:1777/1840 train_time:107148ms step_avg:60.30ms
step:1778/1840 train_time:107236ms step_avg:60.31ms
step:1779/1840 train_time:107323ms step_avg:60.33ms
step:1780/1840 train_time:107410ms step_avg:60.34ms
step:1781/1840 train_time:107496ms step_avg:60.36ms
step:1782/1840 train_time:107585ms step_avg:60.37ms
step:1783/1840 train_time:107670ms step_avg:60.39ms
step:1784/1840 train_time:107758ms step_avg:60.40ms
step:1785/1840 train_time:107845ms step_avg:60.42ms
step:1786/1840 train_time:107935ms step_avg:60.43ms
step:1787/1840 train_time:108023ms step_avg:60.45ms
step:1788/1840 train_time:108111ms step_avg:60.46ms
step:1789/1840 train_time:108198ms step_avg:60.48ms
step:1790/1840 train_time:108287ms step_avg:60.50ms
step:1791/1840 train_time:108372ms step_avg:60.51ms
step:1792/1840 train_time:108461ms step_avg:60.53ms
step:1793/1840 train_time:108547ms step_avg:60.54ms
step:1794/1840 train_time:108635ms step_avg:60.55ms
step:1795/1840 train_time:108721ms step_avg:60.57ms
step:1796/1840 train_time:108810ms step_avg:60.58ms
step:1797/1840 train_time:108898ms step_avg:60.60ms
step:1798/1840 train_time:108987ms step_avg:60.62ms
step:1799/1840 train_time:109074ms step_avg:60.63ms
step:1800/1840 train_time:109165ms step_avg:60.65ms
step:1801/1840 train_time:109252ms step_avg:60.66ms
step:1802/1840 train_time:109339ms step_avg:60.68ms
step:1803/1840 train_time:109426ms step_avg:60.69ms
step:1804/1840 train_time:109514ms step_avg:60.71ms
step:1805/1840 train_time:109600ms step_avg:60.72ms
step:1806/1840 train_time:109689ms step_avg:60.74ms
step:1807/1840 train_time:109775ms step_avg:60.75ms
step:1808/1840 train_time:109865ms step_avg:60.77ms
step:1809/1840 train_time:109951ms step_avg:60.78ms
step:1810/1840 train_time:110041ms step_avg:60.80ms
step:1811/1840 train_time:110128ms step_avg:60.81ms
step:1812/1840 train_time:110217ms step_avg:60.83ms
step:1813/1840 train_time:110304ms step_avg:60.84ms
step:1814/1840 train_time:110393ms step_avg:60.86ms
step:1815/1840 train_time:110479ms step_avg:60.87ms
step:1816/1840 train_time:110567ms step_avg:60.89ms
step:1817/1840 train_time:110653ms step_avg:60.90ms
step:1818/1840 train_time:110742ms step_avg:60.91ms
step:1819/1840 train_time:110828ms step_avg:60.93ms
step:1820/1840 train_time:110918ms step_avg:60.94ms
step:1821/1840 train_time:111005ms step_avg:60.96ms
step:1822/1840 train_time:111095ms step_avg:60.97ms
step:1823/1840 train_time:111181ms step_avg:60.99ms
step:1824/1840 train_time:111270ms step_avg:61.00ms
step:1825/1840 train_time:111356ms step_avg:61.02ms
step:1826/1840 train_time:111445ms step_avg:61.03ms
step:1827/1840 train_time:111531ms step_avg:61.05ms
step:1828/1840 train_time:111620ms step_avg:61.06ms
step:1829/1840 train_time:111706ms step_avg:61.07ms
step:1830/1840 train_time:111795ms step_avg:61.09ms
step:1831/1840 train_time:111882ms step_avg:61.10ms
step:1832/1840 train_time:111971ms step_avg:61.12ms
step:1833/1840 train_time:112057ms step_avg:61.13ms
step:1834/1840 train_time:112146ms step_avg:61.15ms
step:1835/1840 train_time:112234ms step_avg:61.16ms
step:1836/1840 train_time:112321ms step_avg:61.18ms
step:1837/1840 train_time:112408ms step_avg:61.19ms
step:1838/1840 train_time:112497ms step_avg:61.21ms
step:1839/1840 train_time:112585ms step_avg:61.22ms
step:1840/1840 train_time:112673ms step_avg:61.24ms
step:1840/1840 val_loss:3.2795 train_time:112774ms step_avg:61.29ms
peak memory allocated: 28507 MiB reserved: 44038 MiB

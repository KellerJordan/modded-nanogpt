import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from collections import defaultdict
from itertools import accumulate
from pathlib import Path
import gc

os.environ["PYTORCH_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device=f"cuda:{os.environ['LOCAL_RANK']}", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng

@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor, split_baddbmm: bool = False):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    # Select batched vs unbatched
    if split_baddbmm:
        BX_matmul = torch.bmm if X.ndim > 2 else torch.mm
    else:
        aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A

        # Referencing X twice causes pytorch to make a defensive copy,
        # resulting in a cudaMemcpyAsync in baddbmm.
        # For large matrices (i.e., the mlp weights), it's faster to split
        # the operation into two kernels to avoid this.
        if split_baddbmm:
            BX_matmul(B, X, out=C)  # C = B @ X
            C.add_(X, alpha=a)      # C = C + a*X  (in-place, X only read)
        else:
            aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X

        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X


# -----------------------------------------------------------------------------
# Compiled helpers for NorMuon by @chrisjmccormick

@torch.compile(dynamic=False, fullgraph=True)
def cautious_wd_and_update_inplace(p, mantissa, grad, wd_tensor, lr_tensor):
    """
    Cautious weight decay + parameter update. wd_tensor and lr_tensor are 0-D CPU tensors.
    Mantissa is tracked to enable higher precision updates on bfloat16 parameters.
    bfloat16 format: 1 sign bit + 8 exponent bits + 7 mantissa bits = 16 bits total
    float32 format: 1 sign bit + 8 exponent bits + 23 mantissa bits = 32 bits total
    """
    assert p.dtype == mantissa.dtype == torch.uint16
    grad = grad.float()
    wd_factor = wd_tensor.to(torch.float32)
    lr_factor = lr_tensor.to(torch.float32)
    p_precise_raw = (p.to(torch.uint32) << 16) | mantissa.to(torch.uint32)
    p_precise = p_precise_raw.view(torch.float32)
    mask = (grad * p_precise) >= 0
    p_precise.copy_(p_precise - (p_precise * mask * wd_factor * lr_factor) - (grad * lr_factor))
    p.copy_((p_precise_raw >> 16).to(torch.uint16))
    mantissa.copy_(p_precise_raw.to(torch.uint16))

@torch.compile(dynamic=False, fullgraph=True)
def apply_normuon_variance_reduction(v_chunk, second_momentum_buffer, beta2, red_dim):
    """NorMuon variance reduction. Algebraically fuses the normalization steps to minimize memory ops."""
    v_mean = v_chunk.float().square().mean(dim=red_dim, keepdim=True)
    red_dim_size = v_chunk.size(red_dim)
    v_norm_sq = v_mean.sum(dim=(-2, -1), keepdim=True).mul_(red_dim_size)
    v_norm = v_norm_sq.sqrt_()
    second_momentum_buffer.lerp_(v_mean.to(dtype=second_momentum_buffer.dtype), 1 - beta2)
    step_size = second_momentum_buffer.clamp_min(1e-10).rsqrt_()
    scaled_sq_sum = (v_mean * red_dim_size) * step_size.float().square()
    v_norm_new = scaled_sq_sum.sum(dim=(-2, -1), keepdim=True).sqrt_()
    final_scale = step_size * (v_norm / v_norm_new.clamp_min_(1e-10))
    return v_chunk.mul_(final_scale.type_as(v_chunk))


# -----------------------------------------------------------------------------
# NorMuon optimizer

class NorMuon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).

    Differences from standard Muon:
    - Newton-Shulz is replaced with Polar Express for the orthogonalization step
    - NorMuon adds a low-rank variance estimator similar to Adafactor. https://arxiv.org/pdf/2510.05491
    - small 1D parameters handled here instead of in Adam
    - Cautious weight decay, a gated version of decoupled weight decay
    - Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as
    needed on the forward pass. This enables attn and mlp weights to be contained within the same
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn.
    The scheduling is:
        1. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        2. reduce scatter attn/mlp round 2 (16 mlp params)
        3. wait on step 1, then compute update of 1 and schedule all gather
        4. wait on step 2, then compute update of 2 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        5. wait for each all gather to complete and update params
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, beta2=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, beta2=beta2)
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def reset(self):
        # expose a reset for clearing buffers
        for group in self.param_groups:
            if "momentum_buffer" in group:
                group["momentum_buffer"].zero_()
                group["mantissa"].zero_()
                group["second_momentum_buffer"].zero_()

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per module.
        """
        groups = defaultdict(list)
        for param in params:
            groups[param.label].append(param)

        param_groups = []
        for module_name, group_params in groups.items():
            chunk_size = (len(group_params) + self.world_size - 1) // self.world_size
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))

        return param_groups

    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn
        and mlp params when a param group is split across GPUs.
        """
        params_list = list(params)
        module_group_order = ['attn', 'mlp']
        group_sizes = [16, 16]  # 10 attn + 6 mlp, then 16 mlp
        params_list.sort(key=lambda x: module_group_order.index(x.label))

        idx = 0
        assert len(params_list) == sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            chunk_size = (size + self.world_size - 1) // self.world_size
            group_params = params_list[idx: idx + size]
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))
            idx += size

        return param_groups

    def step(self):
        self.step_p1()
        self.step_p2()
        self.step_p3()
        
    @torch.no_grad()
    def step_p1(self):
        """
        Part 1: Launch distributed reduce_scatter operations for parameter groups.
        """
        rank = dist.get_rank()
        self.group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            stacked_grads = torch.empty(
                (padded_num_params, *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device
            )
            for i, p in enumerate(params):
                stacked_grads[i].copy_(p.grad, non_blocking=True)
            if len(params) < padded_num_params:
                stacked_grads[len(params):].zero_()

            grad_chunk = torch.empty_like(stacked_grads[:chunk_size])

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            self.group_infos.append(dict(grad_chunk=grad_chunk, reduce_future=reduce_future))
    
    @torch.no_grad()
    def step_p2(self):
        """
        Part 2: Compute gradient updates and launch all_gather operations
        Wait for all gather to complete for all parameter groups except final one
        """
        # Efficient distributed step by @YouJiacheng, @KonstantinWilleke, @alexrgilbert,
        # @adricarda, @tuttyfrutyee, @vdlad, @ryanyang0, @vagrawal, @varunneal, @chrisjmccormick
        rank = dist.get_rank()
        group_infos = self.group_infos
        
        self.all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = group["params"]
            grad_chunk = info["grad_chunk"].float()
            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            start_idx = rank * chunk_size
            module_idx = start_idx if start_idx < len(params) else 0

            num_params = min(chunk_size, max(0, len(params) - start_idx))  # num params for this rank

            if "momentum_buffer" not in group:
                group["momentum_buffer"]  = torch.zeros_like(grad_chunk[:num_params], dtype=torch.float32)
                
            momentum_buffer = group["momentum_buffer"]
            # Apply momentum update to the persistent momentum buffer in-place
            momentum_buffer.lerp_(grad_chunk[:num_params], 1 - group["momentum"])
            updated_grads = grad_chunk[:num_params].lerp_(momentum_buffer, group["momentum"])

            grad_shape = updated_grads.shape
            if params[module_idx].label == 'attn':
                for p in params[module_idx:module_idx + num_params]:
                    assert p.label == 'attn'
                updated_grads = updated_grads.view(4 * grad_shape[0], grad_shape[1] // 4, grad_shape[2])

            ref_param = params[module_idx]
            param_shape = ref_param.shape

            if "second_momentum_buffer" not in group:
                group["second_momentum_buffer"] = (torch.zeros_like(updated_grads[..., :, :1], dtype=torch.float32)
                    if param_shape[-2] >= param_shape[-1] else torch.zeros_like(updated_grads[..., :1, :])
                )
            second_momentum_buffer = group["second_momentum_buffer"]

            if "param_lr_cpu" not in group:
                # Define multipliers for ALL params in this group (global, not per-shard)
                lr_mults = []
                wd_mults = []
                for p in params:
                    # Increase learning rate for modules with larger inputs than outputs.
                    # This shape check also assumes rows=input, columns=output, so take care
                    # when changing memory layouts. @chrisjmccormick
                    shape = p.shape
                    if len(shape) >= 2:
                        shape_mult = max(1.0, shape[-2] / shape[-1]) ** 0.5
                    else:
                        shape_mult = 1.0
                    lr_mults.append(shape_mult * getattr(p, "lr_mul", 1.0))
                    wd_mults.append(getattr(p, "wd_mul", 1.0))
                # Define as cpu tensors to enable Inductor constant folding
                group["param_lr_cpu"] = torch.tensor(lr_mults, dtype=torch.float32, device="cpu")
                group["param_wd_cpu"] = torch.tensor(wd_mults, dtype=torch.float32, device="cpu")

            eff_lr_all = group["param_lr_cpu"] * group["lr"]
            eff_wd_all = group["param_wd_cpu"] * group["weight_decay"] * group["lr"]

            # Slice the portion corresponding to this rank's shard
            eff_lr_cpu = eff_lr_all[module_idx:module_idx + num_params]
            eff_wd_cpu = eff_wd_all[module_idx:module_idx + num_params]

            # Compute zeropower for the entire chunk in a single, batched call.
            if num_params == 0:
                v_chunk = updated_grads
            else:
                v_chunk = polar_express(updated_grads, split_baddbmm=(ref_param.label == 'mlp'))

            # Note that the head orientation in O is transposed relative to QKV, so red_dim
            # is 'incorrect' for O. However, correcting this showed no improvement. @chrisjmccormick
            red_dim = -1 if param_shape[-2] >= param_shape[-1] else -2

            v_chunk = apply_normuon_variance_reduction(
                v_chunk, second_momentum_buffer, group["beta2"], red_dim
            )

            v_chunk = v_chunk.view(grad_shape)

            # # "Cautious" weight decay (https://arxiv.org/abs/2510.12402)
            updated_params = torch.empty_like(grad_chunk, dtype=torch.bfloat16)
            if num_params > 0:
                # Work on a stacked copy to avoid touching original params
                param_chunk = torch.stack(params[module_idx:module_idx + num_params])

                if "mantissa" not in group:
                    group["mantissa"] = torch.zeros_like(param_chunk, dtype=torch.uint16)
                mantissa = group["mantissa"]

                for local_idx in range(num_params):
                    cautious_wd_and_update_inplace(
                        param_chunk[local_idx].view(torch.uint16),
                        mantissa[local_idx],
                        v_chunk[local_idx],
                        eff_wd_cpu[local_idx],
                        eff_lr_cpu[local_idx]
                    )
            else:
                param_chunk = torch.zeros_like(v_chunk)

            updated_params[:num_params].copy_(param_chunk)
            if num_params < chunk_size:
                updated_params[num_params:].zero_()

            stacked_params = torch.empty(
                (padded_num_params, *param_shape),
                dtype=updated_params.dtype,
                device=updated_params.device,
            )

            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_params, async_op=True
            ).get_future()

            self.all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete for all except final and copy results back into original parameter tensors.
        for info in self.all_gather_infos[:-1]:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)

    @torch.no_grad()
    def step_p3(self):
        """
        Part 3: Wait for final all gather to complete and copy results back into original parameter tensors
        """
        info = self.all_gather_infos[-1]
        info["gather_future"].wait()
        stacked_params = info["stacked_params"]
        orig_params = info["orig_params"]

        unstacked_params = torch.unbind(stacked_params)
        for i, p in enumerate(orig_params):
            p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, label_order: list[str], betas: list[list[float]], lr: float = 1e-3, eps: float = 1e-8, weight_decay: float = 0.01):
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        defaults = dict(lr=lr, eps=eps, weight_decay=weight_decay)
        params = list(params)
        # Group by label, with explicit ordering for execution control.
        params_by_label = defaultdict(list)
        for p in params:
            params_by_label[getattr(p, 'label', None)].append(p)
        param_groups = []
        for idx, label in enumerate(label_order):
            if label in params_by_label:
                param_groups.append(dict(params=params_by_label[label], betas=betas[idx]))
        # include any unlabeled params at the end (processed last)
        if None in params_by_label:
            param_groups.append(dict(params=params_by_label[None]))
        super().__init__(param_groups, defaults)
        # init state: small params (numel < 1024) use full-sized state, others use sharded
        for p in params:
            chunk = p if p.numel() < 1024 else p[:p.size(0) // self.world_size]
            exp_avg = torch.zeros_like(chunk, dtype=torch.float32, device=p.device)
            self.state[p] = dict(step=0, exp_avg=exp_avg, exp_avg_sq=torch.zeros_like(exp_avg))

        # tag the final param for optimizer pipelining, run all gather after muon copy
        param_groups[-1]['params'][-1].is_final_param = True
        
        # DistributedAdam implementation by @vagrawal, @akash5474
        self.should_sync = False
        self._reduce_scatter_hooks = []
        self._reduce_scatter_futures = {}
        # 0-D CPU tensors to avoid recompilation in _update_step
        self._step_size_t = torch.tensor(0.0, dtype=torch.float32, device="cpu")
        self._eff_wd_t = torch.tensor(0.0, dtype=torch.float32, device="cpu")
        self.register_backward_hooks()

    def register_backward_hooks(self):
        for group in self.param_groups:
            for param in group["params"]:
                self._reduce_scatter_hooks.append(param.register_post_accumulate_grad_hook(self._sync_gradient))

    def load_state_dict(self, state_dict):
        """Override to preserve optimizer state dtypes (avoid BFloat16->Float32 cast that causes recompilation)."""
        # Save original state dtypes before loading
        original_dtypes = {}
        for p, s in self.state.items():
            original_dtypes[p] = {k: v.dtype for k, v in s.items() if isinstance(v, torch.Tensor)}
        
        # Call parent load_state_dict (which may cast dtypes to match param dtype)
        super().load_state_dict(state_dict)
        
        # Restore original dtypes
        for p, s in self.state.items():
            if p in original_dtypes:
                for k, v in s.items():
                    if isinstance(v, torch.Tensor) and k in original_dtypes[p]:
                        if v.dtype != original_dtypes[p][k]:
                            s[k] = v.to(original_dtypes[p][k])

    @torch.no_grad()
    def _sync_gradient(self, param):
        if not self.should_sync:
            return

        grad = param.grad
        if param.numel() < 1024:
            # Small params: use all_reduce (no scatter/gather needed)
            self._reduce_scatter_futures[param] = (
                dist.all_reduce(grad, op=dist.ReduceOp.AVG, async_op=True).get_future(),
                grad
            )
        else:
            rank_size = grad.shape[0] // self.world_size
            if grad is not None:
                grad_slice = torch.empty_like(grad[:rank_size])
                self._reduce_scatter_futures[param] = (
                    dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future(),
                    grad_slice
                )

    def copy_lm_to_embed(self):
        # run at 2/3 of training
        lm_head = self.param_groups[0]['params'][0]
        embed = self.param_groups[-2]['params'][0]
        lm_head_state = self.state[lm_head]
        embed_state = self.state[embed]
        embed_state['step'] = lm_head_state['step']
        embed_state['exp_avg'] = lm_head_state['exp_avg'].clone()
        embed_state['exp_avg_sq'] = lm_head_state['exp_avg_sq'].clone()
        embed.data.copy_(lm_head.data)

    @staticmethod
    @torch.compile(dynamic=False, fullgraph=True)
    def _update_step(p_slice, g_slice, exp_avg, exp_avg_sq, beta1, beta2, eps, step_size_t, eff_wd_t):
        """Compiled Adam update step. step_size_t and eff_wd_t are 0-D CPU tensors to avoid recompilation."""
        exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)  # exp_avg = beta1 * exp_avg + (1 - beta1) * g_slice
        exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)  # exp_avg_sq = beta2 * exp_avg_sq + (1 - beta2) * g_slice^2
        # compute step
        update = exp_avg.div(exp_avg_sq.sqrt().add_(eps)).mul_(step_size_t)  # update = (exp_avg / (sqrt(exp_avg_sq) + eps)) * step_size
        # cautious weight decay
        mask = (update * p_slice) > 0
        update.addcmul_(p_slice, mask, value=eff_wd_t)  # update += eff_wd_t * p_slice * mask
        p_slice.add_(other=update, alpha=-1.0)  # p_slice -= update

    @torch.no_grad()
    def step(self, muon_opt):
        muon_opt.step_p1()
        rank = dist.get_rank()
        all_gather_futures: list[torch.Future] = []

        last_param = None
        last_p_slice = None
        for group in self.param_groups:      
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            for param in group['params']:
                if param not in self._reduce_scatter_futures:
                    continue

                fut, g_slice = self._reduce_scatter_futures[param]
                fut.wait()

                is_small = param.numel() < 1024
                if is_small:
                    # Small params: g_slice is actually full grad, p_slice is full param
                    p_slice = param
                else:
                    rank_size = param.shape[0] // self.world_size
                    p_slice = param[rank * rank_size:(rank + 1) * rank_size]

                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]
                state["step"] += 1
                t = state["step"]
                
                # Pre-compute changing values as 0-D CPU tensors to avoid recompilation.
                # `.fill_(value)` is the same as "= value", but doesn't change the tensor object.
                bias1, bias2 = 1 - beta1 ** t, 1 - beta2 ** t
                self._step_size_t.fill_(lr * (bias2 ** 0.5 / bias1))
                self._eff_wd_t.fill_(lr * lr * wd * getattr(param, "wd_mul", 1.0)) # `lr` included twice to serve as weight decay schedule.

                DistAdam._update_step(p_slice, g_slice, state["exp_avg"], state["exp_avg_sq"],
                                      beta1, beta2, eps, self._step_size_t, self._eff_wd_t)

                if not is_small:
                    if getattr(param, "is_final_param", False):
                        last_param = param
                        last_p_slice = p_slice
                    else:
                        all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())
        self._reduce_scatter_futures.clear()

        muon_opt.step_p2()
        torch.futures.collect_all(all_gather_futures).wait()

        if last_param is not None:
            last_all_gather_future = dist.all_gather_into_tensor(last_param, last_p_slice, async_op=True).get_future()
        muon_opt.step_p3()
        torch.futures.collect_all([last_all_gather_future]).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        with torch.no_grad():
            self.weight.zero_()  # @Grad62304977 and others

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))


# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float
    key_offset: bool
    attn_gate_w: torch.Tensor
    ve_gate_w: torch.Tensor

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = self.dim ** -0.5
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKVO weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # Simplified layout by @chrisjmccormick
        self.qkvo_w = nn.Parameter(torch.empty(self.dim * 4, self.hdim, dtype=torch.bfloat16))
        # label all modules for explicit optimizer grouping
        self.qkvo_w.label = 'attn'

        with torch.no_grad():
            self.qkvo_w[:self.dim * 3].uniform_(-bound, bound)  # init QKV weights
            self.qkvo_w[self.dim * 3:].zero_()  # init O weights to zero

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas, key_offset = attn_args.ve, attn_args.sa_lambdas, attn_args.key_offset
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size
        # sparse gated attention to enable context based no-op by @classiclarryd
        # only include gates on layers with value embeds used on forward pass
        attn_gate_w, ve_gate_w = attn_args.attn_gate_w, attn_args.ve_gate_w

        q, k, v = F.linear(x, sa_lambdas[0] * self.qkvo_w[:self.dim * 3].type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if key_offset:
            # shift keys forward for the stationary head dims. Enables 1-layer induction.
            k[:, 1:, :, self.head_dim // 4:self.head_dim // 2] = k[:, :-1, :, self.head_dim // 4:self.head_dim // 2]
            k[:, 1:, :, 3 * self.head_dim // 4:] = k[:, :-1, :, 3 * self.head_dim // 4:]
        if ve is not None:
            ve_gate_out = 2 * torch.sigmoid(F.linear(x[..., :12], ve_gate_w)).view(B, T, self.num_heads, 1)
            v = v + ve_gate_out * ve.view_as(v) # @ KoszarskyB & @Grad62304977

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens,
                                                        max_seqlen_q=max_len, max_seqlen_k=max_len,
                                                        causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(F.linear(x[..., :12], attn_gate_w)).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, sa_lambdas[1] * self.qkvo_w[self.dim * 3:].type_as(y))  # sa_lambdas[1] pre-multiplied to O @shenberg
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # Transposed layout to match attention weights
        self.c_fc = nn.Parameter(torch.empty(hdim, dim, dtype=torch.bfloat16))
        self.c_proj = nn.Parameter(torch.empty(hdim, dim, dtype=torch.bfloat16))
        # label all modules for explicit optimizer grouping
        self.c_fc.label = 'mlp'
        self.c_proj.label = 'mlp'
        self.c_proj.lr_mul = 2.

        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.T.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.6 (the 7th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads, layer_idx) if layer_idx != 6 else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim)

    def forward(self, x: Tensor, attn_args: AttnArgs):
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

@dataclass
class ForwardScheduleConfig:
    mtp_weights: torch.Tensor
    ws_short: int
    ws_long: int

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        self.num_layers = num_layers
        vocab_size = next_multiple_of_n(vocab_size, n=128)

        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.label = 'smear_gate'
        self.smear_gate.weight.lr_mul = 0.01
        self.smear_gate.weight.wd_mul = 0.0

        self.skip_gate = CastedLinear(12, 1)
        self.skip_gate.weight.label = 'skip_gate'
        self.skip_gate.weight.lr_mul = 0.05
        self.skip_gate.weight.wd_mul = 0.0

        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        for embed in self.value_embeds:
            nn.init.zeros_(embed.weight)
        for ve in self.value_embeds:
            ve.weight.label = 'value_embed'
        
        # parameter banks for attention and value embedding gate weights
        self.attn_gate_bank = nn.Parameter(torch.zeros(10, num_heads, 12)) # 10 layers
        self.attn_gate_bank.label = 'attn_gate_bank'
        self.ve_gate_bank = nn.Parameter(torch.zeros(5, num_heads, 12)) # 5 layers
        self.ve_gate_bank.label = 've_gate_bank'

        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)

        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=100/448, w_s=1.6/448, grad_s=0.75/448)
        nn.init.normal_(self.lm_head.weight, mean=0, std=0.005)
        self.lm_head.weight.label = 'lm_head'

        self.embed = nn.Embedding(vocab_size, model_dim)
        self.embed.weight.label = 'embed'

        # x0_lambdas separated out for different optimizer treatment (no beta smoothing)
        self.x0_lambdas = nn.Parameter(torch.zeros(num_layers))
        self.x0_lambdas.label = 'x0_lambdas'
        self.x0_lambdas.lr_mul = 5.0
        self.x0_lambdas.wd_mul = 0.0

        pad = (-num_layers * 3 - 3) % dist.get_world_size()  # updated: 3*num_layers instead of 4*
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    1.1 * torch.ones(num_layers),  # resid lambdas. 1.1 init such that layer i weight is i^(num_layers-i).
                    *[torch.tensor([0.5, 1.0]) for _ in range(num_layers)],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    -1.5 * torch.ones(1),  # skip_lambda -> σ(-1.5) ≈ 0.18
                    torch.ones(pad),
                ]
            )
        )

        self.scalars.label = 'scalars'
        # set learning rates
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
            param.wd_mul = 5.
        for param in self.embed.parameters():
            param.wd_mul = 150.
        for param in self.lm_head.parameters():
            param.wd_mul = 150.
        self.scalars.lr_mul = 5.0
        self.scalars.wd_mul = 0.0

        self.split_embed = False

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, schedule_cfg: ForwardScheduleConfig):
        assert input_seq.ndim == 1

        # unpack schedule_cfg
        mtp_weights, ws_short, ws_long = schedule_cfg.mtp_weights, schedule_cfg.ws_short, schedule_cfg.ws_long

        # set configs
        skip_connections = []
        skip_in = [3] # long attention window on layer 3
        skip_out = [6] # no attn op on layer 6
        x_backout = None
        backout_layer = 7

        # set lambdas
        resid_lambdas = self.scalars[: 1 * self.num_layers]
        x0_lambdas = self.x0_lambdas
        sa_lambdas = self.scalars[1 * self.num_layers: 3 * self.num_layers].view(-1, 2)
        smear_lambda = self.scalars[3 * self.num_layers]
        backout_lambda = self.scalars[3 * self.num_layers+1]
        skip_lambda = self.scalars[3 * self.num_layers+2]

        # set block masks and key shift
        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == self.num_layers
        key_offset = [b==long_bm for b in bm_sizes] # apply partial key offset to long windows

        # weight-tied: use lm_head.weight for embedding lookup (or separate embed after split)
        if self.split_embed:
            x = self.embed(input_seq)
        else:
            x = F.embedding(input_seq, self.lm_head.weight)
        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [ve[1], ve[2]] + [None] * (self.num_layers - 5) + [ve[0], ve[1], ve[2]]
        assert len(ve) == self.num_layers

        # smear token embed forward 1 position @classiclarryd
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # unbind gate banks to avoid select_backwards kernel
        ag = [w.bfloat16() for w in self.attn_gate_bank.unbind(0)] 
        veg = [w.bfloat16() for w in self.ve_gate_bank.unbind(0)]
        attn_gates = ag[:6] + [None] + ag[6:]
        ve_gates = [veg[0], veg[1]] + [None] * (self.num_layers - 5) + [veg[2], veg[3], veg[4]]
        assert len(attn_gates) == self.num_layers
        assert len(ve_gates) == self.num_layers

        for i in range(self.num_layers):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale,
                key_offset=key_offset[i],
                attn_gate_w=attn_gates[i],
                ve_gate_w=ve_gates[i]
            )
            if i in skip_out:
                skip_gate_out = torch.sigmoid(skip_lambda) * 2 * torch.sigmoid(self.skip_gate(x0[..., :self.skip_gate.weight.size(-1)]))
                x = x + skip_gate_out * skip_connections.pop()
            if i == 0:
                x = (resid_lambdas[0] + x0_lambdas[0]) * x
            else:
                x = resid_lambdas[i] * x + x0_lambdas[i] * x0
            x = self.blocks[i](x, attn_args)
            if i in skip_in:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 7 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15
        # @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1). @classiclarryd updated to 23*sigmoid((logits+5)/7.5)
        logits = 23 * torch.sigmoid((logits + 5) / 7.5)
        logits_for_loss = logits.float() if not self.training else logits

        n_predict = mtp_weights.size(0) if mtp_weights is not None else 1
        if self.training and n_predict > 1:
            # Multi-token prediction: take loss of the weighted average of next n_predict tokens
            logits_flat = logits_for_loss.view(-1, logits_for_loss.size(-1))
            idx = F.pad(target_seq, (0, n_predict - 1)).unfold(0, n_predict, 1)  # [T, n_predict] of shifted targets
            target_logits = logits_flat.gather(1, idx)
            cross_entropy = torch.logsumexp(logits_flat, dim=-1).unsqueeze(1) - target_logits
            for k in range(1, n_predict):  # zero out preds past end of sequence
                cross_entropy[-k:, k] = 0
            loss = (cross_entropy * mtp_weights).sum()
        elif self.training:
            loss = F.cross_entropy(logits_for_loss.view(-1, logits_for_loss.size(-1)), target_seq, reduction="sum")
        else:
            loss = F.cross_entropy(logits_for_loss.view(-1, logits_for_loss.size(-1)), target_seq, reduction="mean")
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()

    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        # Cast to int32 on CPU before transfer to avoid dtype conversion during .to()
        _inputs = _inputs.to(dtype=torch.int32)
        _targets = _targets.to(dtype=torch.int64)
        _cum_lengths = _cum_lengths.to(dtype=torch.int32)

        new_params = yield (
            _inputs.to(device="cuda", non_blocking=True),
            _targets.to(device="cuda", non_blocking=True),
            _cum_lengths.to(device="cuda", non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * new_grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens // new_grad_accum_steps
            max_seq_len = new_max_seq_len

# -----------------------------------------------------------------------------
# Training Management
 
def get_bs(step: int):
    if step >= args.num_scheduled_iterations:
        return args.train_bs_extension
    x = step / args.num_scheduled_iterations
    bs_idx = int(len(args.train_bs_schedule) * x)
    return args.train_bs_schedule[bs_idx]

def get_ws(step: int):
    # set short window size to half of long window size
    # Higher ws on "extension" steps
    if step >= args.num_scheduled_iterations:
        return args.ws_final // 2, args.ws_final
    x = step / args.num_scheduled_iterations
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

# learning rate schedule: tied to batch size schedule, with cooldown at the end.
def get_lr(step: int):
    if step > args.num_scheduled_iterations:
        return 0.1
    lr_max = 1.0
    x = step / args.num_scheduled_iterations
    if x > 1/3:
       lr_max = 1.52  # (16/8)**0.6
    if x > 2/3:
        lr_max = 1.73  # (24/8)**0.5
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = lr_max * w + (1 - w) * 0.1
        return lr
    return lr_max

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

class TrainingManager():
    """
    Manages three optimizers for Adam embed/lm_head, Adam scalars, and Muon weight matrices.
    Notable Features:
        1. Scalars are given higher momentum terms to smooth learning @ChrisJMcCormick
        2. Scalar weights are temporarily frozen during batch size or window size updates @ChrisJMcCormick
        3. Adam optimizers are only stepped on odd steps @classiclarryd
        4. Adam optimizers have hooks to start gradient communication during backwards pass @akash5474
        5. Muon has a linear momentum warmup and cooldown schedule
        6. Learning rates follow a linear decay schedule
        7. Embed/lm_head weights and optimizer state splits at 2/3 of training @classiclarryd

    Manages model architecture, data, and target that changes during training
    Notable Features:
        1. Multi Token Prediction schedule of [1, 0.5, 0.25->0] -> [1, 0.5->0] -> [1] @varunneal
        2. Sliding Attention window schedule of [1,3] -> [3,7] -> [5,11] -> [6,13]
        3. YaRN updates to RoPE on window changes
        4. Split embed and lm head at 2/3 of training
        5. Batch size schedule of 8 -> 16 -> 24
        6. Post training extension of long windows from 13 to 20
    """
    def __init__(self, model):
        self.mtp_weights_schedule = self._build_mtp_schedule()
        self.model = model
        adam_betas = {
            'lm_head': [0.5, 0.95],
            'smear_gate': [0.9, 0.99],
            'attn_gate_bank': [0.9, 0.99],
            've_gate_bank': [0.9, 0.99],
            'skip_gate': [0.9, 0.99],
            'x0_lambdas': [0.65, 0.95],
            'scalars': [0.9, 0.99],
            'embed': [0.5, 0.95],
            'value_embed': [0.75, 0.95]
        }
        adam_labels = list(adam_betas.keys())
        adam_beta_values = list(adam_betas.values())
        muon_labels = ['attn', 'mlp']
        adam_params = [p for p in model.parameters() if getattr(p, 'label', None) in adam_labels]
        muon_params = [p for p in model.parameters() if getattr(p, 'label', None) in muon_labels]
        assert set(getattr(p, 'label', None) for p in model.parameters()) == set(adam_labels + muon_labels), "All params must have label"

        self.adam_opt = DistAdam(adam_params, adam_labels, adam_beta_values, lr=0.008, eps=1e-10, weight_decay=0.005)
        self.muon_opt = NorMuon(muon_params, lr=0.023, momentum=0.95, beta2=0.95, weight_decay=1.2)
        self.optimizers = [self.adam_opt, self.muon_opt]

        # split after odd number step
        self.split_step = math.ceil(args.split_embed_frac * args.num_scheduled_iterations) | 1

        # set defaults
        for opt in self.optimizers:
            opt.odd_step_only = False 
            opt.should_sync = True
            for group in opt.param_groups:
                group["initial_lr"] = group["lr"]

        # on even steps, only step Muon params
        self.adam_opt.odd_step_only = True

        self.reset()

    def _build_mtp_schedule(self):
        # Precompute MTP weights for all steps to avoid tensor allocation during training
        # Schedule: [1, 0.5, 0.25->0] -> [1, 0.5->0] -> [1]
        mtp_weights_schedule = []
        for s in range(args.num_iterations + 1):
            x = s / args.num_scheduled_iterations
            if x < 1/3:
                w = [1.0, 0.5, 0.25 * (1 - 3*x)]
            elif x < 2/3:
                w = [1.0, 0.5 * (1 - (3*x - 1))]
            else:
                w = [1.0]
            mtp_weights_schedule.append(torch.tensor(w, device=device))
        return mtp_weights_schedule

    def apply_final_ws_ext(self):
        self.ws_long = args.ws_validate_post_yarn_ext

    def get_forward_args(self):
        return ForwardScheduleConfig(
            mtp_weights = self.mtp_weights,
            ws_short = self.ws_short,
            ws_long = self.ws_long
        )
    
    def _is_active_step(self, opt, step: int):
        return (opt.odd_step_only and step%2==1) or not opt.odd_step_only

    def get_transition_steps(self):
        transition_steps = []
        ws_short, ws_long = get_ws(0)
        for step in range(1, args.num_iterations):
            ws_short, new_ws_long = get_ws(step)
            if new_ws_long != ws_long:
                transition_steps.append(step)
                ws_long = new_ws_long
        return transition_steps

    def advance_schedule(self, step: int):
        self.ws_short, new_ws_long = get_ws(step)
        if new_ws_long != self.ws_long:
            self.model.yarn.apply(self.ws_long, new_ws_long)

        new_batch_size = get_bs(step)
        if new_batch_size != self.batch_size:
            self.train_loader_send_args = (new_batch_size, args.train_max_seq_len, grad_accum_steps)
        else:
            self.train_loader_send_args = None

        self.ws_long = new_ws_long
        self.mtp_weights = self.mtp_weights_schedule[step]
    
    def step_optimizers(self, step: int):                
        step_lr = get_lr(step)
        muon_momentum = get_muon_momentum(step)
        for group in self.muon_opt.param_groups:
            group["momentum"] = muon_momentum

        for opt in self.optimizers:
            for group in opt.param_groups:
                group["lr"] = group["initial_lr"] * step_lr
                
        if self._is_active_step(self.adam_opt, step):
            # adam will interleave calls to muon step
            self.adam_opt.step(self.muon_opt)
            self.model.zero_grad(set_to_none=True)
            self.adam_opt.should_sync = False
        else:
            self.muon_opt.step()
            self.muon_opt.zero_grad(set_to_none=True)
            
        if step == self.split_step:
            self.adam_opt.copy_lm_to_embed()
            self.model.split_embed = True
    
    def activate_hooks(self, step: int):
        for opt in self.optimizers:
            if self._is_active_step(opt, step):
                opt.should_sync = True

    def reset(self, state=None):
        if state is not None:
            for opt, opt_state in zip(self.optimizers, state):
                opt.should_sync = False
                opt.load_state_dict(opt_state)

        # muon momentum buffers not in state dict
        self.muon_opt.reset()
        self.model.split_embed = False

        self.ws_short, self.ws_long = get_ws(0)
        self.batch_size = get_bs(0)
        self.model.yarn.reset()

    def get_state(self):
        return [copy.deepcopy(opt.state_dict()) for opt in self.optimizers]

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # batch sizes
    train_bs_schedule: tuple = (8 * 2048 * 8, 16 * 2048 * 8, 24 * 2048 * 8)
    train_bs_extension: int = 24 * 2048 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 1800  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: float = 0.50  # fraction of num_scheduled_iterations spent cooling down the learning rate
    split_embed_frac: float = 2/3  # fraction of training when embeddings split from lm_head
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_final: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=11,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=args.val_batch_size // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.weight.data = m.weight.data.bfloat16()
model.attn_gate_bank.data = model.attn_gate_bank.data.bfloat16()
model.ve_gate_bank.data = model.ve_gate_bank.data.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)
training_manager = TrainingManager(model)

########################################
#            Warmup kernels            #
########################################
print0("Compiling model and warming up kernels (~7 minutes on first execution)", console=True)
# Warmup the training kernels, then re-initialize the state so we aren't cheating
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=training_manager.get_state()) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_bs_schedule[0], args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)

transition_steps = training_manager.get_transition_steps()
# first few steps plus transitions
warmup_steps = sorted({0, 1, 2} | set(s + offset for s in transition_steps for offset in [-1, 0, 1] if s + offset >= 0)) 
print0(f"Sampling steps {warmup_steps} for warmup", console=True)
for step in warmup_steps:
    training_manager.advance_schedule(step)
    model.eval()
    with torch.no_grad():
        inputs, targets, cum_seqlens = next(val_loader)
        model(inputs, targets, cum_seqlens, training_manager.get_forward_args())
    model.train()
    for idx in range(grad_accum_steps):
        # enable gradient sync for the DistAdam optimizers on the last iteration before we step them
        if idx == grad_accum_steps - 1:
            training_manager.activate_hooks(step)
        send_args = training_manager.train_loader_send_args
        inputs, targets, cum_seqlens = train_loader.send(send_args)
        (model(inputs, targets, cum_seqlens, training_manager.get_forward_args()) / grad_accum_steps).backward()
    training_manager.step_optimizers(step)
print0("Resetting Model", console=True)
model.zero_grad(set_to_none=True)
model.load_state_dict(initial_state["model"])
training_manager.reset(initial_state["optimizers"])
del val_loader, train_loader, initial_state
model.train()

########################################
#        Training and validation       #
########################################
train_loader = distributed_data_generator(args.train_files, args.train_bs_schedule[0], args.train_max_seq_len, grad_accum_steps=grad_accum_steps)

gc.collect()

training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    training_manager.advance_schedule(step)
    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            training_manager.apply_final_ws_ext()
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, training_manager.get_forward_args())
        val_loss /= val_steps
        del val_loader
        dist.reduce(val_loss, 0, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for idx in range(grad_accum_steps):
        # enable gradient sync for the DistAdam optimizers on the last iteration before we step them
        if idx == grad_accum_steps - 1:
            training_manager.activate_hooks(step)
        send_args = training_manager.train_loader_send_args
        inputs, targets, cum_seqlens = train_loader.send(send_args)
        (model(inputs, targets, cum_seqlens, training_manager.get_forward_args()) / grad_accum_steps).backward()
    training_manager.step_optimizers(step)

    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.10.12 (main, Nov  4 2025, 08:48:33) [GCC 11.4.0]
Running PyTorch 2.10.0.dev20251210+cu126 compiled for CUDA 12.6
Running Triton version 3.6.0
Mon Jan  5 07:51:46 2026       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:19:00.0 Off |                    0 |
| N/A   42C    P0            128W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:3B:00.0 Off |                    0 |
| N/A   35C    P0            130W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:4C:00.0 Off |                    0 |
| N/A   33C    P0            127W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:5D:00.0 Off |                    0 |
| N/A   41C    P0            127W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:9B:00.0 Off |                    0 |
| N/A   43C    P0            133W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:BB:00.0 Off |                    0 |
| N/A   35C    P0            125W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:CB:00.0 Off |                    0 |
| N/A   41C    P0            127W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:DB:00.0 Off |                    0 |
| N/A   31C    P0            125W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A     48608      C   /usr/bin/python                                 0MiB |
|    1   N/A  N/A     48609      C   /usr/bin/python                                 0MiB |
|    2   N/A  N/A     48610      C   /usr/bin/python                                 0MiB |
|    3   N/A  N/A     48611      C   /usr/bin/python                                 0MiB |
|    4   N/A  N/A     48612      C   /usr/bin/python                                 0MiB |
|    5   N/A  N/A     48613      C   /usr/bin/python                                 0MiB |
|    6   N/A  N/A     48614      C   /usr/bin/python                                 0MiB |
|    7   N/A  N/A     48615      C   /usr/bin/python                                 0MiB |
+-----------------------------------------------------------------------------------------+

====================================================================================================
Compiling model and warming up kernels (~7 minutes on first execution)
Sampling steps [0, 1, 2, 599, 600, 601, 1199, 1200, 1201, 1799, 1800, 1801] for warmup
Resetting Model
step:0/1840 val_loss:10.8327 train_time:0ms step_avg:0.04ms
step:1/1840 train_time:84ms step_avg:83.95ms
step:2/1840 train_time:106ms step_avg:53.20ms
step:3/1840 train_time:126ms step_avg:41.88ms
step:4/1840 train_time:155ms step_avg:38.63ms
step:5/1840 train_time:187ms step_avg:37.30ms
step:6/1840 train_time:274ms step_avg:45.60ms
step:7/1840 train_time:291ms step_avg:41.64ms
step:8/1840 train_time:315ms step_avg:39.35ms
step:9/1840 train_time:347ms step_avg:38.51ms
step:10/1840 train_time:381ms step_avg:38.06ms
step:11/1840 train_time:413ms step_avg:37.52ms
step:12/1840 train_time:447ms step_avg:37.24ms
step:13/1840 train_time:479ms step_avg:36.83ms
step:14/1840 train_time:513ms step_avg:36.64ms
step:15/1840 train_time:545ms step_avg:36.34ms
step:16/1840 train_time:579ms step_avg:36.19ms
step:17/1840 train_time:611ms step_avg:35.95ms
step:18/1840 train_time:645ms step_avg:35.85ms
step:19/1840 train_time:677ms step_avg:35.64ms
step:20/1840 train_time:711ms step_avg:35.56ms
step:21/1840 train_time:743ms step_avg:35.39ms
step:22/1840 train_time:777ms step_avg:35.34ms
step:23/1840 train_time:809ms step_avg:35.19ms
step:24/1840 train_time:844ms step_avg:35.15ms
step:25/1840 train_time:875ms step_avg:35.02ms
step:26/1840 train_time:909ms step_avg:34.98ms
step:27/1840 train_time:941ms step_avg:34.87ms
step:28/1840 train_time:976ms step_avg:34.86ms
step:29/1840 train_time:1008ms step_avg:34.76ms
step:30/1840 train_time:1043ms step_avg:34.76ms
step:31/1840 train_time:1075ms step_avg:34.67ms
step:32/1840 train_time:1109ms step_avg:34.65ms
step:33/1840 train_time:1141ms step_avg:34.59ms
step:34/1840 train_time:1177ms step_avg:34.61ms
step:35/1840 train_time:1210ms step_avg:34.56ms
step:36/1840 train_time:1244ms step_avg:34.56ms
step:37/1840 train_time:1277ms step_avg:34.50ms
step:38/1840 train_time:1311ms step_avg:34.50ms
step:39/1840 train_time:1343ms step_avg:34.44ms
step:40/1840 train_time:1378ms step_avg:34.45ms
step:41/1840 train_time:1410ms step_avg:34.39ms
step:42/1840 train_time:1444ms step_avg:34.39ms
step:43/1840 train_time:1477ms step_avg:34.34ms
step:44/1840 train_time:1511ms step_avg:34.33ms
step:45/1840 train_time:1543ms step_avg:34.29ms
step:46/1840 train_time:1578ms step_avg:34.30ms
step:47/1840 train_time:1610ms step_avg:34.25ms
step:48/1840 train_time:1644ms step_avg:34.25ms
step:49/1840 train_time:1676ms step_avg:34.20ms
step:50/1840 train_time:1710ms step_avg:34.20ms
step:51/1840 train_time:1742ms step_avg:34.16ms
step:52/1840 train_time:1776ms step_avg:34.16ms
step:53/1840 train_time:1808ms step_avg:34.12ms
step:54/1840 train_time:1842ms step_avg:34.12ms
step:55/1840 train_time:1874ms step_avg:34.08ms
step:56/1840 train_time:1909ms step_avg:34.08ms
step:57/1840 train_time:1941ms step_avg:34.05ms
step:58/1840 train_time:1975ms step_avg:34.05ms
step:59/1840 train_time:2007ms step_avg:34.02ms
step:60/1840 train_time:2041ms step_avg:34.02ms
step:61/1840 train_time:2074ms step_avg:33.99ms
step:62/1840 train_time:2108ms step_avg:34.00ms
step:63/1840 train_time:2140ms step_avg:33.97ms
step:64/1840 train_time:2175ms step_avg:33.98ms
step:65/1840 train_time:2207ms step_avg:33.95ms
step:66/1840 train_time:2242ms step_avg:33.97ms
step:67/1840 train_time:2275ms step_avg:33.95ms
step:68/1840 train_time:2309ms step_avg:33.96ms
step:69/1840 train_time:2342ms step_avg:33.94ms
step:70/1840 train_time:2376ms step_avg:33.94ms
step:71/1840 train_time:2408ms step_avg:33.92ms
step:72/1840 train_time:2443ms step_avg:33.93ms
step:73/1840 train_time:2475ms step_avg:33.91ms
step:74/1840 train_time:2509ms step_avg:33.91ms
step:75/1840 train_time:2542ms step_avg:33.89ms
step:76/1840 train_time:2576ms step_avg:33.90ms
step:77/1840 train_time:2608ms step_avg:33.87ms
step:78/1840 train_time:2642ms step_avg:33.88ms
step:79/1840 train_time:2675ms step_avg:33.86ms
step:80/1840 train_time:2709ms step_avg:33.86ms
step:81/1840 train_time:2741ms step_avg:33.84ms
step:82/1840 train_time:2775ms step_avg:33.85ms
step:83/1840 train_time:2807ms step_avg:33.82ms
step:84/1840 train_time:2842ms step_avg:33.83ms
step:85/1840 train_time:2874ms step_avg:33.81ms
step:86/1840 train_time:2908ms step_avg:33.82ms
step:87/1840 train_time:2940ms step_avg:33.80ms
step:88/1840 train_time:2975ms step_avg:33.80ms
step:89/1840 train_time:3007ms step_avg:33.78ms
step:90/1840 train_time:3041ms step_avg:33.79ms
step:91/1840 train_time:3073ms step_avg:33.77ms
step:92/1840 train_time:3107ms step_avg:33.77ms
step:93/1840 train_time:3140ms step_avg:33.76ms
step:94/1840 train_time:3174ms step_avg:33.77ms
step:95/1840 train_time:3206ms step_avg:33.75ms
step:96/1840 train_time:3241ms step_avg:33.76ms
step:97/1840 train_time:3273ms step_avg:33.74ms
step:98/1840 train_time:3307ms step_avg:33.75ms
step:99/1840 train_time:3340ms step_avg:33.73ms
step:100/1840 train_time:3374ms step_avg:33.74ms
step:101/1840 train_time:3406ms step_avg:33.73ms
step:102/1840 train_time:3441ms step_avg:33.73ms
step:103/1840 train_time:3473ms step_avg:33.72ms
step:104/1840 train_time:3508ms step_avg:33.73ms
step:105/1840 train_time:3540ms step_avg:33.71ms
step:106/1840 train_time:3574ms step_avg:33.72ms
step:107/1840 train_time:3606ms step_avg:33.70ms
step:108/1840 train_time:3641ms step_avg:33.72ms
step:109/1840 train_time:3674ms step_avg:33.70ms
step:110/1840 train_time:3708ms step_avg:33.71ms
step:111/1840 train_time:3740ms step_avg:33.69ms
step:112/1840 train_time:3774ms step_avg:33.70ms
step:113/1840 train_time:3806ms step_avg:33.68ms
step:114/1840 train_time:3840ms step_avg:33.69ms
step:115/1840 train_time:3872ms step_avg:33.67ms
step:116/1840 train_time:3906ms step_avg:33.68ms
step:117/1840 train_time:3939ms step_avg:33.66ms
step:118/1840 train_time:3973ms step_avg:33.67ms
step:119/1840 train_time:4005ms step_avg:33.65ms
step:120/1840 train_time:4039ms step_avg:33.66ms
step:121/1840 train_time:4072ms step_avg:33.65ms
step:122/1840 train_time:4106ms step_avg:33.65ms
step:123/1840 train_time:4138ms step_avg:33.64ms
step:124/1840 train_time:4172ms step_avg:33.64ms
step:125/1840 train_time:4205ms step_avg:33.64ms
step:126/1840 train_time:4239ms step_avg:33.64ms
step:127/1840 train_time:4271ms step_avg:33.63ms
step:128/1840 train_time:4306ms step_avg:33.64ms
step:129/1840 train_time:4338ms step_avg:33.63ms
step:130/1840 train_time:4372ms step_avg:33.63ms
step:131/1840 train_time:4404ms step_avg:33.62ms
step:132/1840 train_time:4439ms step_avg:33.63ms
step:133/1840 train_time:4471ms step_avg:33.62ms
step:134/1840 train_time:4505ms step_avg:33.62ms
step:135/1840 train_time:4538ms step_avg:33.61ms
step:136/1840 train_time:4572ms step_avg:33.62ms
step:137/1840 train_time:4604ms step_avg:33.61ms
step:138/1840 train_time:4638ms step_avg:33.61ms
step:139/1840 train_time:4671ms step_avg:33.60ms
step:140/1840 train_time:4705ms step_avg:33.61ms
step:141/1840 train_time:4737ms step_avg:33.60ms
step:142/1840 train_time:4772ms step_avg:33.60ms
step:143/1840 train_time:4804ms step_avg:33.59ms
step:144/1840 train_time:4838ms step_avg:33.60ms
step:145/1840 train_time:4870ms step_avg:33.59ms
step:146/1840 train_time:4904ms step_avg:33.59ms
step:147/1840 train_time:4936ms step_avg:33.58ms
step:148/1840 train_time:4970ms step_avg:33.58ms
step:149/1840 train_time:5003ms step_avg:33.57ms
step:150/1840 train_time:5037ms step_avg:33.58ms
step:151/1840 train_time:5069ms step_avg:33.57ms
step:152/1840 train_time:5103ms step_avg:33.57ms
step:153/1840 train_time:5135ms step_avg:33.56ms
step:154/1840 train_time:5169ms step_avg:33.57ms
step:155/1840 train_time:5202ms step_avg:33.56ms
step:156/1840 train_time:5236ms step_avg:33.56ms
step:157/1840 train_time:5268ms step_avg:33.55ms
step:158/1840 train_time:5302ms step_avg:33.56ms
step:159/1840 train_time:5334ms step_avg:33.55ms
step:160/1840 train_time:5369ms step_avg:33.55ms
step:161/1840 train_time:5401ms step_avg:33.54ms
step:162/1840 train_time:5435ms step_avg:33.55ms
step:163/1840 train_time:5467ms step_avg:33.54ms
step:164/1840 train_time:5502ms step_avg:33.55ms
step:165/1840 train_time:5534ms step_avg:33.54ms
step:166/1840 train_time:5568ms step_avg:33.54ms
step:167/1840 train_time:5600ms step_avg:33.53ms
step:168/1840 train_time:5635ms step_avg:33.54ms
step:169/1840 train_time:5667ms step_avg:33.53ms
step:170/1840 train_time:5701ms step_avg:33.54ms
step:171/1840 train_time:5733ms step_avg:33.53ms
step:172/1840 train_time:5768ms step_avg:33.53ms
step:173/1840 train_time:5800ms step_avg:33.53ms
step:174/1840 train_time:5834ms step_avg:33.53ms
step:175/1840 train_time:5866ms step_avg:33.52ms
step:176/1840 train_time:5900ms step_avg:33.52ms
step:177/1840 train_time:5932ms step_avg:33.52ms
step:178/1840 train_time:5967ms step_avg:33.52ms
step:179/1840 train_time:5999ms step_avg:33.51ms
step:180/1840 train_time:6033ms step_avg:33.52ms
step:181/1840 train_time:6065ms step_avg:33.51ms
step:182/1840 train_time:6099ms step_avg:33.51ms
step:183/1840 train_time:6131ms step_avg:33.50ms
step:184/1840 train_time:6165ms step_avg:33.51ms
step:185/1840 train_time:6197ms step_avg:33.50ms
step:186/1840 train_time:6232ms step_avg:33.50ms
step:187/1840 train_time:6264ms step_avg:33.50ms
step:188/1840 train_time:6298ms step_avg:33.50ms
step:189/1840 train_time:6330ms step_avg:33.49ms
step:190/1840 train_time:6364ms step_avg:33.49ms
step:191/1840 train_time:6396ms step_avg:33.49ms
step:192/1840 train_time:6430ms step_avg:33.49ms
step:193/1840 train_time:6463ms step_avg:33.49ms
step:194/1840 train_time:6497ms step_avg:33.49ms
step:195/1840 train_time:6529ms step_avg:33.48ms
step:196/1840 train_time:6564ms step_avg:33.49ms
step:197/1840 train_time:6596ms step_avg:33.48ms
step:198/1840 train_time:6630ms step_avg:33.48ms
step:199/1840 train_time:6662ms step_avg:33.48ms
step:200/1840 train_time:6696ms step_avg:33.48ms
step:201/1840 train_time:6728ms step_avg:33.47ms
step:202/1840 train_time:6763ms step_avg:33.48ms
step:203/1840 train_time:6795ms step_avg:33.47ms
step:204/1840 train_time:6829ms step_avg:33.48ms
step:205/1840 train_time:6861ms step_avg:33.47ms
step:206/1840 train_time:6896ms step_avg:33.48ms
step:207/1840 train_time:6928ms step_avg:33.47ms
step:208/1840 train_time:6963ms step_avg:33.48ms
step:209/1840 train_time:6994ms step_avg:33.47ms
step:210/1840 train_time:7028ms step_avg:33.47ms
step:211/1840 train_time:7060ms step_avg:33.46ms
step:212/1840 train_time:7094ms step_avg:33.46ms
step:213/1840 train_time:7126ms step_avg:33.46ms
step:214/1840 train_time:7161ms step_avg:33.46ms
step:215/1840 train_time:7193ms step_avg:33.46ms
step:216/1840 train_time:7227ms step_avg:33.46ms
step:217/1840 train_time:7260ms step_avg:33.45ms
step:218/1840 train_time:7294ms step_avg:33.46ms
step:219/1840 train_time:7326ms step_avg:33.45ms
step:220/1840 train_time:7360ms step_avg:33.46ms
step:221/1840 train_time:7393ms step_avg:33.45ms
step:222/1840 train_time:7427ms step_avg:33.46ms
step:223/1840 train_time:7459ms step_avg:33.45ms
step:224/1840 train_time:7493ms step_avg:33.45ms
step:225/1840 train_time:7526ms step_avg:33.45ms
step:226/1840 train_time:7560ms step_avg:33.45ms
step:227/1840 train_time:7592ms step_avg:33.44ms
step:228/1840 train_time:7626ms step_avg:33.45ms
step:229/1840 train_time:7658ms step_avg:33.44ms
step:230/1840 train_time:7692ms step_avg:33.44ms
step:231/1840 train_time:7724ms step_avg:33.44ms
step:232/1840 train_time:7759ms step_avg:33.45ms
step:233/1840 train_time:7791ms step_avg:33.44ms
step:234/1840 train_time:7825ms step_avg:33.44ms
step:235/1840 train_time:7858ms step_avg:33.44ms
step:236/1840 train_time:7892ms step_avg:33.44ms
step:237/1840 train_time:7924ms step_avg:33.43ms
step:238/1840 train_time:7958ms step_avg:33.44ms
step:239/1840 train_time:7990ms step_avg:33.43ms
step:240/1840 train_time:8025ms step_avg:33.44ms
step:241/1840 train_time:8057ms step_avg:33.43ms
step:242/1840 train_time:8091ms step_avg:33.43ms
step:243/1840 train_time:8123ms step_avg:33.43ms
step:244/1840 train_time:8158ms step_avg:33.43ms
step:245/1840 train_time:8189ms step_avg:33.43ms
step:246/1840 train_time:8224ms step_avg:33.43ms
step:247/1840 train_time:8256ms step_avg:33.43ms
step:248/1840 train_time:8290ms step_avg:33.43ms
step:249/1840 train_time:8322ms step_avg:33.42ms
step:250/1840 train_time:8356ms step_avg:33.42ms
step:250/1840 val_loss:4.6048 train_time:8398ms step_avg:33.59ms
step:251/1840 train_time:8418ms step_avg:33.54ms
step:252/1840 train_time:8437ms step_avg:33.48ms
step:253/1840 train_time:8456ms step_avg:33.42ms
step:254/1840 train_time:8491ms step_avg:33.43ms
step:255/1840 train_time:8525ms step_avg:33.43ms
step:256/1840 train_time:8561ms step_avg:33.44ms
step:257/1840 train_time:8593ms step_avg:33.44ms
step:258/1840 train_time:8627ms step_avg:33.44ms
step:259/1840 train_time:8659ms step_avg:33.43ms
step:260/1840 train_time:8693ms step_avg:33.43ms
step:261/1840 train_time:8725ms step_avg:33.43ms
step:262/1840 train_time:8759ms step_avg:33.43ms
step:263/1840 train_time:8791ms step_avg:33.42ms
step:264/1840 train_time:8825ms step_avg:33.43ms
step:265/1840 train_time:8857ms step_avg:33.42ms
step:266/1840 train_time:8891ms step_avg:33.42ms
step:267/1840 train_time:8923ms step_avg:33.42ms
step:268/1840 train_time:8956ms step_avg:33.42ms
step:269/1840 train_time:8988ms step_avg:33.41ms
step:270/1840 train_time:9022ms step_avg:33.42ms
step:271/1840 train_time:9054ms step_avg:33.41ms
step:272/1840 train_time:9088ms step_avg:33.41ms
step:273/1840 train_time:9120ms step_avg:33.41ms
step:274/1840 train_time:9154ms step_avg:33.41ms
step:275/1840 train_time:9186ms step_avg:33.40ms
step:276/1840 train_time:9220ms step_avg:33.41ms
step:277/1840 train_time:9252ms step_avg:33.40ms
step:278/1840 train_time:9286ms step_avg:33.40ms
step:279/1840 train_time:9318ms step_avg:33.40ms
step:280/1840 train_time:9353ms step_avg:33.40ms
step:281/1840 train_time:9386ms step_avg:33.40ms
step:282/1840 train_time:9420ms step_avg:33.40ms
step:283/1840 train_time:9452ms step_avg:33.40ms
step:284/1840 train_time:9487ms step_avg:33.40ms
step:285/1840 train_time:9520ms step_avg:33.40ms
step:286/1840 train_time:9554ms step_avg:33.41ms
step:287/1840 train_time:9586ms step_avg:33.40ms
step:288/1840 train_time:9620ms step_avg:33.40ms
step:289/1840 train_time:9652ms step_avg:33.40ms
step:290/1840 train_time:9686ms step_avg:33.40ms
step:291/1840 train_time:9719ms step_avg:33.40ms
step:292/1840 train_time:9753ms step_avg:33.40ms
step:293/1840 train_time:9785ms step_avg:33.40ms
step:294/1840 train_time:9819ms step_avg:33.40ms
step:295/1840 train_time:9851ms step_avg:33.39ms
step:296/1840 train_time:9885ms step_avg:33.40ms
step:297/1840 train_time:9917ms step_avg:33.39ms
step:298/1840 train_time:9951ms step_avg:33.39ms
step:299/1840 train_time:9983ms step_avg:33.39ms
step:300/1840 train_time:10017ms step_avg:33.39ms
step:301/1840 train_time:10049ms step_avg:33.38ms
step:302/1840 train_time:10083ms step_avg:33.39ms
step:303/1840 train_time:10115ms step_avg:33.38ms
step:304/1840 train_time:10149ms step_avg:33.38ms
step:305/1840 train_time:10181ms step_avg:33.38ms
step:306/1840 train_time:10215ms step_avg:33.38ms
step:307/1840 train_time:10247ms step_avg:33.38ms
step:308/1840 train_time:10281ms step_avg:33.38ms
step:309/1840 train_time:10313ms step_avg:33.38ms
step:310/1840 train_time:10348ms step_avg:33.38ms
step:311/1840 train_time:10380ms step_avg:33.38ms
step:312/1840 train_time:10414ms step_avg:33.38ms
step:313/1840 train_time:10446ms step_avg:33.37ms
step:314/1840 train_time:10481ms step_avg:33.38ms
step:315/1840 train_time:10513ms step_avg:33.37ms
step:316/1840 train_time:10547ms step_avg:33.38ms
step:317/1840 train_time:10579ms step_avg:33.37ms
step:318/1840 train_time:10613ms step_avg:33.37ms
step:319/1840 train_time:10646ms step_avg:33.37ms
step:320/1840 train_time:10680ms step_avg:33.38ms
step:321/1840 train_time:10712ms step_avg:33.37ms
step:322/1840 train_time:10747ms step_avg:33.37ms
step:323/1840 train_time:10779ms step_avg:33.37ms
step:324/1840 train_time:10813ms step_avg:33.37ms
step:325/1840 train_time:10845ms step_avg:33.37ms
step:326/1840 train_time:10879ms step_avg:33.37ms
step:327/1840 train_time:10911ms step_avg:33.37ms
step:328/1840 train_time:10945ms step_avg:33.37ms
step:329/1840 train_time:10977ms step_avg:33.37ms
step:330/1840 train_time:11011ms step_avg:33.37ms
step:331/1840 train_time:11043ms step_avg:33.36ms
step:332/1840 train_time:11078ms step_avg:33.37ms
step:333/1840 train_time:11110ms step_avg:33.36ms
step:334/1840 train_time:11144ms step_avg:33.36ms
step:335/1840 train_time:11176ms step_avg:33.36ms
step:336/1840 train_time:11210ms step_avg:33.36ms
step:337/1840 train_time:11242ms step_avg:33.36ms
step:338/1840 train_time:11276ms step_avg:33.36ms
step:339/1840 train_time:11308ms step_avg:33.36ms
step:340/1840 train_time:11343ms step_avg:33.36ms
step:341/1840 train_time:11375ms step_avg:33.36ms
step:342/1840 train_time:11409ms step_avg:33.36ms
step:343/1840 train_time:11441ms step_avg:33.36ms
step:344/1840 train_time:11475ms step_avg:33.36ms
step:345/1840 train_time:11508ms step_avg:33.36ms
step:346/1840 train_time:11542ms step_avg:33.36ms
step:347/1840 train_time:11574ms step_avg:33.35ms
step:348/1840 train_time:11608ms step_avg:33.36ms
step:349/1840 train_time:11640ms step_avg:33.35ms
step:350/1840 train_time:11674ms step_avg:33.36ms
step:351/1840 train_time:11706ms step_avg:33.35ms
step:352/1840 train_time:11741ms step_avg:33.35ms
step:353/1840 train_time:11772ms step_avg:33.35ms
step:354/1840 train_time:11807ms step_avg:33.35ms
step:355/1840 train_time:11839ms step_avg:33.35ms
step:356/1840 train_time:11873ms step_avg:33.35ms
step:357/1840 train_time:11905ms step_avg:33.35ms
step:358/1840 train_time:11939ms step_avg:33.35ms
step:359/1840 train_time:11970ms step_avg:33.34ms
step:360/1840 train_time:12005ms step_avg:33.35ms
step:361/1840 train_time:12037ms step_avg:33.34ms
step:362/1840 train_time:12071ms step_avg:33.35ms
step:363/1840 train_time:12103ms step_avg:33.34ms
step:364/1840 train_time:12137ms step_avg:33.34ms
step:365/1840 train_time:12169ms step_avg:33.34ms
step:366/1840 train_time:12204ms step_avg:33.34ms
step:367/1840 train_time:12235ms step_avg:33.34ms
step:368/1840 train_time:12269ms step_avg:33.34ms
step:369/1840 train_time:12302ms step_avg:33.34ms
step:370/1840 train_time:12336ms step_avg:33.34ms
step:371/1840 train_time:12368ms step_avg:33.34ms
step:372/1840 train_time:12402ms step_avg:33.34ms
step:373/1840 train_time:12434ms step_avg:33.34ms
step:374/1840 train_time:12468ms step_avg:33.34ms
step:375/1840 train_time:12500ms step_avg:33.33ms
step:376/1840 train_time:12535ms step_avg:33.34ms
step:377/1840 train_time:12567ms step_avg:33.34ms
step:378/1840 train_time:12602ms step_avg:33.34ms
step:379/1840 train_time:12633ms step_avg:33.33ms
step:380/1840 train_time:12668ms step_avg:33.34ms
step:381/1840 train_time:12700ms step_avg:33.33ms
step:382/1840 train_time:12734ms step_avg:33.33ms
step:383/1840 train_time:12766ms step_avg:33.33ms
step:384/1840 train_time:12801ms step_avg:33.33ms
step:385/1840 train_time:12832ms step_avg:33.33ms
step:386/1840 train_time:12867ms step_avg:33.33ms
step:387/1840 train_time:12899ms step_avg:33.33ms
step:388/1840 train_time:12933ms step_avg:33.33ms
step:389/1840 train_time:12965ms step_avg:33.33ms
step:390/1840 train_time:12999ms step_avg:33.33ms
step:391/1840 train_time:13031ms step_avg:33.33ms
step:392/1840 train_time:13066ms step_avg:33.33ms
step:393/1840 train_time:13098ms step_avg:33.33ms
step:394/1840 train_time:13133ms step_avg:33.33ms
step:395/1840 train_time:13165ms step_avg:33.33ms
step:396/1840 train_time:13199ms step_avg:33.33ms
step:397/1840 train_time:13231ms step_avg:33.33ms
step:398/1840 train_time:13265ms step_avg:33.33ms
step:399/1840 train_time:13297ms step_avg:33.33ms
step:400/1840 train_time:13331ms step_avg:33.33ms
step:401/1840 train_time:13364ms step_avg:33.33ms
step:402/1840 train_time:13398ms step_avg:33.33ms
step:403/1840 train_time:13430ms step_avg:33.32ms
step:404/1840 train_time:13465ms step_avg:33.33ms
step:405/1840 train_time:13497ms step_avg:33.33ms
step:406/1840 train_time:13531ms step_avg:33.33ms
step:407/1840 train_time:13563ms step_avg:33.32ms
step:408/1840 train_time:13597ms step_avg:33.33ms
step:409/1840 train_time:13629ms step_avg:33.32ms
step:410/1840 train_time:13664ms step_avg:33.33ms
step:411/1840 train_time:13696ms step_avg:33.32ms
step:412/1840 train_time:13730ms step_avg:33.33ms
step:413/1840 train_time:13763ms step_avg:33.32ms
step:414/1840 train_time:13797ms step_avg:33.33ms
step:415/1840 train_time:13829ms step_avg:33.32ms
step:416/1840 train_time:13864ms step_avg:33.33ms
step:417/1840 train_time:13896ms step_avg:33.32ms
step:418/1840 train_time:13930ms step_avg:33.33ms
step:419/1840 train_time:13962ms step_avg:33.32ms
step:420/1840 train_time:13996ms step_avg:33.32ms
step:421/1840 train_time:14028ms step_avg:33.32ms
step:422/1840 train_time:14062ms step_avg:33.32ms
step:423/1840 train_time:14094ms step_avg:33.32ms
step:424/1840 train_time:14129ms step_avg:33.32ms
step:425/1840 train_time:14161ms step_avg:33.32ms
step:426/1840 train_time:14195ms step_avg:33.32ms
step:427/1840 train_time:14227ms step_avg:33.32ms
step:428/1840 train_time:14262ms step_avg:33.32ms
step:429/1840 train_time:14294ms step_avg:33.32ms
step:430/1840 train_time:14328ms step_avg:33.32ms
step:431/1840 train_time:14360ms step_avg:33.32ms
step:432/1840 train_time:14394ms step_avg:33.32ms
step:433/1840 train_time:14426ms step_avg:33.32ms
step:434/1840 train_time:14460ms step_avg:33.32ms
step:435/1840 train_time:14492ms step_avg:33.31ms
step:436/1840 train_time:14526ms step_avg:33.32ms
step:437/1840 train_time:14558ms step_avg:33.31ms
step:438/1840 train_time:14593ms step_avg:33.32ms
step:439/1840 train_time:14625ms step_avg:33.31ms
step:440/1840 train_time:14660ms step_avg:33.32ms
step:441/1840 train_time:14692ms step_avg:33.31ms
step:442/1840 train_time:14726ms step_avg:33.32ms
step:443/1840 train_time:14758ms step_avg:33.31ms
step:444/1840 train_time:14792ms step_avg:33.32ms
step:445/1840 train_time:14824ms step_avg:33.31ms
step:446/1840 train_time:14858ms step_avg:33.32ms
step:447/1840 train_time:14891ms step_avg:33.31ms
step:448/1840 train_time:14925ms step_avg:33.32ms
step:449/1840 train_time:14958ms step_avg:33.31ms
step:450/1840 train_time:14992ms step_avg:33.31ms
step:451/1840 train_time:15024ms step_avg:33.31ms
step:452/1840 train_time:15058ms step_avg:33.31ms
step:453/1840 train_time:15090ms step_avg:33.31ms
step:454/1840 train_time:15124ms step_avg:33.31ms
step:455/1840 train_time:15156ms step_avg:33.31ms
step:456/1840 train_time:15190ms step_avg:33.31ms
step:457/1840 train_time:15223ms step_avg:33.31ms
step:458/1840 train_time:15257ms step_avg:33.31ms
step:459/1840 train_time:15289ms step_avg:33.31ms
step:460/1840 train_time:15323ms step_avg:33.31ms
step:461/1840 train_time:15355ms step_avg:33.31ms
step:462/1840 train_time:15389ms step_avg:33.31ms
step:463/1840 train_time:15421ms step_avg:33.31ms
step:464/1840 train_time:15455ms step_avg:33.31ms
step:465/1840 train_time:15487ms step_avg:33.31ms
step:466/1840 train_time:15522ms step_avg:33.31ms
step:467/1840 train_time:15553ms step_avg:33.30ms
step:468/1840 train_time:15588ms step_avg:33.31ms
step:469/1840 train_time:15620ms step_avg:33.30ms
step:470/1840 train_time:15654ms step_avg:33.31ms
step:471/1840 train_time:15686ms step_avg:33.30ms
step:472/1840 train_time:15721ms step_avg:33.31ms
step:473/1840 train_time:15753ms step_avg:33.30ms
step:474/1840 train_time:15787ms step_avg:33.31ms
step:475/1840 train_time:15819ms step_avg:33.30ms
step:476/1840 train_time:15853ms step_avg:33.30ms
step:477/1840 train_time:15885ms step_avg:33.30ms
step:478/1840 train_time:15920ms step_avg:33.30ms
step:479/1840 train_time:15952ms step_avg:33.30ms
step:480/1840 train_time:15986ms step_avg:33.30ms
step:481/1840 train_time:16018ms step_avg:33.30ms
step:482/1840 train_time:16052ms step_avg:33.30ms
step:483/1840 train_time:16084ms step_avg:33.30ms
step:484/1840 train_time:16119ms step_avg:33.30ms
step:485/1840 train_time:16151ms step_avg:33.30ms
step:486/1840 train_time:16185ms step_avg:33.30ms
step:487/1840 train_time:16217ms step_avg:33.30ms
step:488/1840 train_time:16252ms step_avg:33.30ms
step:489/1840 train_time:16283ms step_avg:33.30ms
step:490/1840 train_time:16318ms step_avg:33.30ms
step:491/1840 train_time:16350ms step_avg:33.30ms
step:492/1840 train_time:16384ms step_avg:33.30ms
step:493/1840 train_time:16416ms step_avg:33.30ms
step:494/1840 train_time:16450ms step_avg:33.30ms
step:495/1840 train_time:16483ms step_avg:33.30ms
step:496/1840 train_time:16517ms step_avg:33.30ms
step:497/1840 train_time:16549ms step_avg:33.30ms
step:498/1840 train_time:16583ms step_avg:33.30ms
step:499/1840 train_time:16615ms step_avg:33.30ms
step:500/1840 train_time:16649ms step_avg:33.30ms
step:500/1840 val_loss:4.2909 train_time:16691ms step_avg:33.38ms
step:501/1840 train_time:16715ms step_avg:33.36ms
step:502/1840 train_time:16735ms step_avg:33.34ms
step:503/1840 train_time:16752ms step_avg:33.30ms
step:504/1840 train_time:16786ms step_avg:33.30ms
step:505/1840 train_time:16819ms step_avg:33.30ms
step:506/1840 train_time:16854ms step_avg:33.31ms
step:507/1840 train_time:16887ms step_avg:33.31ms
step:508/1840 train_time:16921ms step_avg:33.31ms
step:509/1840 train_time:16954ms step_avg:33.31ms
step:510/1840 train_time:16988ms step_avg:33.31ms
step:511/1840 train_time:17020ms step_avg:33.31ms
step:512/1840 train_time:17054ms step_avg:33.31ms
step:513/1840 train_time:17086ms step_avg:33.31ms
step:514/1840 train_time:17120ms step_avg:33.31ms
step:515/1840 train_time:17152ms step_avg:33.30ms
step:516/1840 train_time:17186ms step_avg:33.31ms
step:517/1840 train_time:17218ms step_avg:33.30ms
step:518/1840 train_time:17252ms step_avg:33.30ms
step:519/1840 train_time:17284ms step_avg:33.30ms
step:520/1840 train_time:17318ms step_avg:33.30ms
step:521/1840 train_time:17349ms step_avg:33.30ms
step:522/1840 train_time:17383ms step_avg:33.30ms
step:523/1840 train_time:17415ms step_avg:33.30ms
step:524/1840 train_time:17449ms step_avg:33.30ms
step:525/1840 train_time:17481ms step_avg:33.30ms
step:526/1840 train_time:17515ms step_avg:33.30ms
step:527/1840 train_time:17547ms step_avg:33.30ms
step:528/1840 train_time:17581ms step_avg:33.30ms
step:529/1840 train_time:17614ms step_avg:33.30ms
step:530/1840 train_time:17648ms step_avg:33.30ms
step:531/1840 train_time:17681ms step_avg:33.30ms
step:532/1840 train_time:17715ms step_avg:33.30ms
step:533/1840 train_time:17748ms step_avg:33.30ms
step:534/1840 train_time:17782ms step_avg:33.30ms
step:535/1840 train_time:17815ms step_avg:33.30ms
step:536/1840 train_time:17850ms step_avg:33.30ms
step:537/1840 train_time:17882ms step_avg:33.30ms
step:538/1840 train_time:17916ms step_avg:33.30ms
step:539/1840 train_time:17949ms step_avg:33.30ms
step:540/1840 train_time:17982ms step_avg:33.30ms
step:541/1840 train_time:18015ms step_avg:33.30ms
step:542/1840 train_time:18049ms step_avg:33.30ms
step:543/1840 train_time:18081ms step_avg:33.30ms
step:544/1840 train_time:18115ms step_avg:33.30ms
step:545/1840 train_time:18148ms step_avg:33.30ms
step:546/1840 train_time:18181ms step_avg:33.30ms
step:547/1840 train_time:18214ms step_avg:33.30ms
step:548/1840 train_time:18248ms step_avg:33.30ms
step:549/1840 train_time:18280ms step_avg:33.30ms
step:550/1840 train_time:18314ms step_avg:33.30ms
step:551/1840 train_time:18346ms step_avg:33.30ms
step:552/1840 train_time:18380ms step_avg:33.30ms
step:553/1840 train_time:18412ms step_avg:33.29ms
step:554/1840 train_time:18446ms step_avg:33.30ms
step:555/1840 train_time:18478ms step_avg:33.29ms
step:556/1840 train_time:18513ms step_avg:33.30ms
step:557/1840 train_time:18545ms step_avg:33.29ms
step:558/1840 train_time:18579ms step_avg:33.30ms
step:559/1840 train_time:18611ms step_avg:33.29ms
step:560/1840 train_time:18645ms step_avg:33.29ms
step:561/1840 train_time:18677ms step_avg:33.29ms
step:562/1840 train_time:18712ms step_avg:33.29ms
step:563/1840 train_time:18744ms step_avg:33.29ms
step:564/1840 train_time:18778ms step_avg:33.30ms
step:565/1840 train_time:18811ms step_avg:33.29ms
step:566/1840 train_time:18845ms step_avg:33.29ms
step:567/1840 train_time:18878ms step_avg:33.29ms
step:568/1840 train_time:18912ms step_avg:33.30ms
step:569/1840 train_time:18944ms step_avg:33.29ms
step:570/1840 train_time:18978ms step_avg:33.29ms
step:571/1840 train_time:19010ms step_avg:33.29ms
step:572/1840 train_time:19044ms step_avg:33.29ms
step:573/1840 train_time:19077ms step_avg:33.29ms
step:574/1840 train_time:19111ms step_avg:33.29ms
step:575/1840 train_time:19142ms step_avg:33.29ms
step:576/1840 train_time:19177ms step_avg:33.29ms
step:577/1840 train_time:19209ms step_avg:33.29ms
step:578/1840 train_time:19243ms step_avg:33.29ms
step:579/1840 train_time:19275ms step_avg:33.29ms
step:580/1840 train_time:19309ms step_avg:33.29ms
step:581/1840 train_time:19341ms step_avg:33.29ms
step:582/1840 train_time:19375ms step_avg:33.29ms
step:583/1840 train_time:19407ms step_avg:33.29ms
step:584/1840 train_time:19441ms step_avg:33.29ms
step:585/1840 train_time:19474ms step_avg:33.29ms
step:586/1840 train_time:19508ms step_avg:33.29ms
step:587/1840 train_time:19540ms step_avg:33.29ms
step:588/1840 train_time:19574ms step_avg:33.29ms
step:589/1840 train_time:19606ms step_avg:33.29ms
step:590/1840 train_time:19640ms step_avg:33.29ms
step:591/1840 train_time:19673ms step_avg:33.29ms
step:592/1840 train_time:19707ms step_avg:33.29ms
step:593/1840 train_time:19739ms step_avg:33.29ms
step:594/1840 train_time:19773ms step_avg:33.29ms
step:595/1840 train_time:19806ms step_avg:33.29ms
step:596/1840 train_time:19840ms step_avg:33.29ms
step:597/1840 train_time:19872ms step_avg:33.29ms
step:598/1840 train_time:19906ms step_avg:33.29ms
step:599/1840 train_time:19939ms step_avg:33.29ms
step:600/1840 train_time:19973ms step_avg:33.29ms
step:601/1840 train_time:20007ms step_avg:33.29ms
step:602/1840 train_time:20065ms step_avg:33.33ms
step:603/1840 train_time:20124ms step_avg:33.37ms
step:604/1840 train_time:20187ms step_avg:33.42ms
step:605/1840 train_time:20247ms step_avg:33.47ms
step:606/1840 train_time:20309ms step_avg:33.51ms
step:607/1840 train_time:20368ms step_avg:33.56ms
step:608/1840 train_time:20431ms step_avg:33.60ms
step:609/1840 train_time:20490ms step_avg:33.65ms
step:610/1840 train_time:20553ms step_avg:33.69ms
step:611/1840 train_time:20614ms step_avg:33.74ms
step:612/1840 train_time:20676ms step_avg:33.78ms
step:613/1840 train_time:20736ms step_avg:33.83ms
step:614/1840 train_time:20798ms step_avg:33.87ms
step:615/1840 train_time:20858ms step_avg:33.92ms
step:616/1840 train_time:20921ms step_avg:33.96ms
step:617/1840 train_time:20981ms step_avg:34.00ms
step:618/1840 train_time:21043ms step_avg:34.05ms
step:619/1840 train_time:21102ms step_avg:34.09ms
step:620/1840 train_time:21164ms step_avg:34.14ms
step:621/1840 train_time:21224ms step_avg:34.18ms
step:622/1840 train_time:21286ms step_avg:34.22ms
step:623/1840 train_time:21346ms step_avg:34.26ms
step:624/1840 train_time:21407ms step_avg:34.31ms
step:625/1840 train_time:21467ms step_avg:34.35ms
step:626/1840 train_time:21529ms step_avg:34.39ms
step:627/1840 train_time:21589ms step_avg:34.43ms
step:628/1840 train_time:21651ms step_avg:34.48ms
step:629/1840 train_time:21711ms step_avg:34.52ms
step:630/1840 train_time:21775ms step_avg:34.56ms
step:631/1840 train_time:21835ms step_avg:34.60ms
step:632/1840 train_time:21898ms step_avg:34.65ms
step:633/1840 train_time:21958ms step_avg:34.69ms
step:634/1840 train_time:22020ms step_avg:34.73ms
step:635/1840 train_time:22080ms step_avg:34.77ms
step:636/1840 train_time:22142ms step_avg:34.81ms
step:637/1840 train_time:22202ms step_avg:34.85ms
step:638/1840 train_time:22264ms step_avg:34.90ms
step:639/1840 train_time:22324ms step_avg:34.94ms
step:640/1840 train_time:22387ms step_avg:34.98ms
step:641/1840 train_time:22446ms step_avg:35.02ms
step:642/1840 train_time:22508ms step_avg:35.06ms
step:643/1840 train_time:22567ms step_avg:35.10ms
step:644/1840 train_time:22629ms step_avg:35.14ms
step:645/1840 train_time:22689ms step_avg:35.18ms
step:646/1840 train_time:22752ms step_avg:35.22ms
step:647/1840 train_time:22813ms step_avg:35.26ms
step:648/1840 train_time:22876ms step_avg:35.30ms
step:649/1840 train_time:22936ms step_avg:35.34ms
step:650/1840 train_time:22999ms step_avg:35.38ms
step:651/1840 train_time:23059ms step_avg:35.42ms
step:652/1840 train_time:23121ms step_avg:35.46ms
step:653/1840 train_time:23181ms step_avg:35.50ms
step:654/1840 train_time:23242ms step_avg:35.54ms
step:655/1840 train_time:23303ms step_avg:35.58ms
step:656/1840 train_time:23364ms step_avg:35.62ms
step:657/1840 train_time:23425ms step_avg:35.65ms
step:658/1840 train_time:23486ms step_avg:35.69ms
step:659/1840 train_time:23547ms step_avg:35.73ms
step:660/1840 train_time:23609ms step_avg:35.77ms
step:661/1840 train_time:23668ms step_avg:35.81ms
step:662/1840 train_time:23730ms step_avg:35.85ms
step:663/1840 train_time:23791ms step_avg:35.88ms
step:664/1840 train_time:23855ms step_avg:35.93ms
step:665/1840 train_time:23915ms step_avg:35.96ms
step:666/1840 train_time:23979ms step_avg:36.00ms
step:667/1840 train_time:24039ms step_avg:36.04ms
step:668/1840 train_time:24101ms step_avg:36.08ms
step:669/1840 train_time:24162ms step_avg:36.12ms
step:670/1840 train_time:24224ms step_avg:36.15ms
step:671/1840 train_time:24283ms step_avg:36.19ms
step:672/1840 train_time:24345ms step_avg:36.23ms
step:673/1840 train_time:24404ms step_avg:36.26ms
step:674/1840 train_time:24466ms step_avg:36.30ms
step:675/1840 train_time:24526ms step_avg:36.33ms
step:676/1840 train_time:24589ms step_avg:36.37ms
step:677/1840 train_time:24648ms step_avg:36.41ms
step:678/1840 train_time:24710ms step_avg:36.45ms
step:679/1840 train_time:24770ms step_avg:36.48ms
step:680/1840 train_time:24833ms step_avg:36.52ms
step:681/1840 train_time:24893ms step_avg:36.55ms
step:682/1840 train_time:24956ms step_avg:36.59ms
step:683/1840 train_time:25016ms step_avg:36.63ms
step:684/1840 train_time:25078ms step_avg:36.66ms
step:685/1840 train_time:25139ms step_avg:36.70ms
step:686/1840 train_time:25201ms step_avg:36.74ms
step:687/1840 train_time:25262ms step_avg:36.77ms
step:688/1840 train_time:25324ms step_avg:36.81ms
step:689/1840 train_time:25384ms step_avg:36.84ms
step:690/1840 train_time:25445ms step_avg:36.88ms
step:691/1840 train_time:25505ms step_avg:36.91ms
step:692/1840 train_time:25566ms step_avg:36.95ms
step:693/1840 train_time:25626ms step_avg:36.98ms
step:694/1840 train_time:25689ms step_avg:37.02ms
step:695/1840 train_time:25749ms step_avg:37.05ms
step:696/1840 train_time:25811ms step_avg:37.08ms
step:697/1840 train_time:25871ms step_avg:37.12ms
step:698/1840 train_time:25934ms step_avg:37.16ms
step:699/1840 train_time:25995ms step_avg:37.19ms
step:700/1840 train_time:26057ms step_avg:37.22ms
step:701/1840 train_time:26117ms step_avg:37.26ms
step:702/1840 train_time:26180ms step_avg:37.29ms
step:703/1840 train_time:26241ms step_avg:37.33ms
step:704/1840 train_time:26303ms step_avg:37.36ms
step:705/1840 train_time:26363ms step_avg:37.39ms
step:706/1840 train_time:26425ms step_avg:37.43ms
step:707/1840 train_time:26484ms step_avg:37.46ms
step:708/1840 train_time:26546ms step_avg:37.49ms
step:709/1840 train_time:26605ms step_avg:37.53ms
step:710/1840 train_time:26668ms step_avg:37.56ms
step:711/1840 train_time:26728ms step_avg:37.59ms
step:712/1840 train_time:26790ms step_avg:37.63ms
step:713/1840 train_time:26849ms step_avg:37.66ms
step:714/1840 train_time:26913ms step_avg:37.69ms
step:715/1840 train_time:26974ms step_avg:37.73ms
step:716/1840 train_time:27036ms step_avg:37.76ms
step:717/1840 train_time:27096ms step_avg:37.79ms
step:718/1840 train_time:27160ms step_avg:37.83ms
step:719/1840 train_time:27220ms step_avg:37.86ms
step:720/1840 train_time:27282ms step_avg:37.89ms
step:721/1840 train_time:27343ms step_avg:37.92ms
step:722/1840 train_time:27404ms step_avg:37.96ms
step:723/1840 train_time:27464ms step_avg:37.99ms
step:724/1840 train_time:27526ms step_avg:38.02ms
step:725/1840 train_time:27586ms step_avg:38.05ms
step:726/1840 train_time:27648ms step_avg:38.08ms
step:727/1840 train_time:27708ms step_avg:38.11ms
step:728/1840 train_time:27770ms step_avg:38.15ms
step:729/1840 train_time:27830ms step_avg:38.18ms
step:730/1840 train_time:27893ms step_avg:38.21ms
step:731/1840 train_time:27954ms step_avg:38.24ms
step:732/1840 train_time:28016ms step_avg:38.27ms
step:733/1840 train_time:28077ms step_avg:38.30ms
step:734/1840 train_time:28139ms step_avg:38.34ms
step:735/1840 train_time:28199ms step_avg:38.37ms
step:736/1840 train_time:28261ms step_avg:38.40ms
step:737/1840 train_time:28321ms step_avg:38.43ms
step:738/1840 train_time:28383ms step_avg:38.46ms
step:739/1840 train_time:28443ms step_avg:38.49ms
step:740/1840 train_time:28505ms step_avg:38.52ms
step:741/1840 train_time:28565ms step_avg:38.55ms
step:742/1840 train_time:28627ms step_avg:38.58ms
step:743/1840 train_time:28687ms step_avg:38.61ms
step:744/1840 train_time:28750ms step_avg:38.64ms
step:745/1840 train_time:28809ms step_avg:38.67ms
step:746/1840 train_time:28871ms step_avg:38.70ms
step:747/1840 train_time:28931ms step_avg:38.73ms
step:748/1840 train_time:28996ms step_avg:38.76ms
step:749/1840 train_time:29056ms step_avg:38.79ms
step:750/1840 train_time:29118ms step_avg:38.82ms
step:750/1840 val_loss:4.0231 train_time:29189ms step_avg:38.92ms
step:751/1840 train_time:29212ms step_avg:38.90ms
step:752/1840 train_time:29241ms step_avg:38.88ms
step:753/1840 train_time:29302ms step_avg:38.91ms
step:754/1840 train_time:29366ms step_avg:38.95ms
step:755/1840 train_time:29427ms step_avg:38.98ms
step:756/1840 train_time:29489ms step_avg:39.01ms
step:757/1840 train_time:29549ms step_avg:39.03ms
step:758/1840 train_time:29610ms step_avg:39.06ms
step:759/1840 train_time:29669ms step_avg:39.09ms
step:760/1840 train_time:29730ms step_avg:39.12ms
step:761/1840 train_time:29790ms step_avg:39.15ms
step:762/1840 train_time:29852ms step_avg:39.18ms
step:763/1840 train_time:29912ms step_avg:39.20ms
step:764/1840 train_time:29974ms step_avg:39.23ms
step:765/1840 train_time:30033ms step_avg:39.26ms
step:766/1840 train_time:30095ms step_avg:39.29ms
step:767/1840 train_time:30155ms step_avg:39.32ms
step:768/1840 train_time:30219ms step_avg:39.35ms
step:769/1840 train_time:30280ms step_avg:39.38ms
step:770/1840 train_time:30342ms step_avg:39.40ms
step:771/1840 train_time:30402ms step_avg:39.43ms
step:772/1840 train_time:30466ms step_avg:39.46ms
step:773/1840 train_time:30525ms step_avg:39.49ms
step:774/1840 train_time:30587ms step_avg:39.52ms
step:775/1840 train_time:30647ms step_avg:39.54ms
step:776/1840 train_time:30710ms step_avg:39.57ms
step:777/1840 train_time:30768ms step_avg:39.60ms
step:778/1840 train_time:30831ms step_avg:39.63ms
step:779/1840 train_time:30890ms step_avg:39.65ms
step:780/1840 train_time:30952ms step_avg:39.68ms
step:781/1840 train_time:31013ms step_avg:39.71ms
step:782/1840 train_time:31075ms step_avg:39.74ms
step:783/1840 train_time:31135ms step_avg:39.76ms
step:784/1840 train_time:31198ms step_avg:39.79ms
step:785/1840 train_time:31258ms step_avg:39.82ms
step:786/1840 train_time:31321ms step_avg:39.85ms
step:787/1840 train_time:31381ms step_avg:39.87ms
step:788/1840 train_time:31443ms step_avg:39.90ms
step:789/1840 train_time:31503ms step_avg:39.93ms
step:790/1840 train_time:31565ms step_avg:39.96ms
step:791/1840 train_time:31625ms step_avg:39.98ms
step:792/1840 train_time:31688ms step_avg:40.01ms
step:793/1840 train_time:31748ms step_avg:40.04ms
step:794/1840 train_time:31810ms step_avg:40.06ms
step:795/1840 train_time:31870ms step_avg:40.09ms
step:796/1840 train_time:31932ms step_avg:40.12ms
step:797/1840 train_time:31991ms step_avg:40.14ms
step:798/1840 train_time:32054ms step_avg:40.17ms
step:799/1840 train_time:32115ms step_avg:40.19ms
step:800/1840 train_time:32177ms step_avg:40.22ms
step:801/1840 train_time:32237ms step_avg:40.25ms
step:802/1840 train_time:32299ms step_avg:40.27ms
step:803/1840 train_time:32359ms step_avg:40.30ms
step:804/1840 train_time:32421ms step_avg:40.32ms
step:805/1840 train_time:32481ms step_avg:40.35ms
step:806/1840 train_time:32543ms step_avg:40.38ms
step:807/1840 train_time:32604ms step_avg:40.40ms
step:808/1840 train_time:32667ms step_avg:40.43ms
step:809/1840 train_time:32728ms step_avg:40.45ms
step:810/1840 train_time:32790ms step_avg:40.48ms
step:811/1840 train_time:32850ms step_avg:40.51ms
step:812/1840 train_time:32911ms step_avg:40.53ms
step:813/1840 train_time:32971ms step_avg:40.55ms
step:814/1840 train_time:33034ms step_avg:40.58ms
step:815/1840 train_time:33094ms step_avg:40.61ms
step:816/1840 train_time:33157ms step_avg:40.63ms
step:817/1840 train_time:33216ms step_avg:40.66ms
step:818/1840 train_time:33279ms step_avg:40.68ms
step:819/1840 train_time:33338ms step_avg:40.71ms
step:820/1840 train_time:33400ms step_avg:40.73ms
step:821/1840 train_time:33460ms step_avg:40.76ms
step:822/1840 train_time:33522ms step_avg:40.78ms
step:823/1840 train_time:33582ms step_avg:40.80ms
step:824/1840 train_time:33644ms step_avg:40.83ms
step:825/1840 train_time:33704ms step_avg:40.85ms
step:826/1840 train_time:33767ms step_avg:40.88ms
step:827/1840 train_time:33827ms step_avg:40.90ms
step:828/1840 train_time:33890ms step_avg:40.93ms
step:829/1840 train_time:33950ms step_avg:40.95ms
step:830/1840 train_time:34012ms step_avg:40.98ms
step:831/1840 train_time:34073ms step_avg:41.00ms
step:832/1840 train_time:34135ms step_avg:41.03ms
step:833/1840 train_time:34195ms step_avg:41.05ms
step:834/1840 train_time:34258ms step_avg:41.08ms
step:835/1840 train_time:34317ms step_avg:41.10ms
step:836/1840 train_time:34379ms step_avg:41.12ms
step:837/1840 train_time:34439ms step_avg:41.15ms
step:838/1840 train_time:34500ms step_avg:41.17ms
step:839/1840 train_time:34559ms step_avg:41.19ms
step:840/1840 train_time:34622ms step_avg:41.22ms
step:841/1840 train_time:34682ms step_avg:41.24ms
step:842/1840 train_time:34744ms step_avg:41.26ms
step:843/1840 train_time:34805ms step_avg:41.29ms
step:844/1840 train_time:34867ms step_avg:41.31ms
step:845/1840 train_time:34928ms step_avg:41.33ms
step:846/1840 train_time:34990ms step_avg:41.36ms
step:847/1840 train_time:35051ms step_avg:41.38ms
step:848/1840 train_time:35114ms step_avg:41.41ms
step:849/1840 train_time:35174ms step_avg:41.43ms
step:850/1840 train_time:35236ms step_avg:41.45ms
step:851/1840 train_time:35296ms step_avg:41.48ms
step:852/1840 train_time:35358ms step_avg:41.50ms
step:853/1840 train_time:35417ms step_avg:41.52ms
step:854/1840 train_time:35480ms step_avg:41.55ms
step:855/1840 train_time:35539ms step_avg:41.57ms
step:856/1840 train_time:35601ms step_avg:41.59ms
step:857/1840 train_time:35661ms step_avg:41.61ms
step:858/1840 train_time:35723ms step_avg:41.64ms
step:859/1840 train_time:35783ms step_avg:41.66ms
step:860/1840 train_time:35846ms step_avg:41.68ms
step:861/1840 train_time:35906ms step_avg:41.70ms
step:862/1840 train_time:35969ms step_avg:41.73ms
step:863/1840 train_time:36030ms step_avg:41.75ms
step:864/1840 train_time:36092ms step_avg:41.77ms
step:865/1840 train_time:36152ms step_avg:41.79ms
step:866/1840 train_time:36215ms step_avg:41.82ms
step:867/1840 train_time:36275ms step_avg:41.84ms
step:868/1840 train_time:36337ms step_avg:41.86ms
step:869/1840 train_time:36397ms step_avg:41.88ms
step:870/1840 train_time:36459ms step_avg:41.91ms
step:871/1840 train_time:36518ms step_avg:41.93ms
step:872/1840 train_time:36581ms step_avg:41.95ms
step:873/1840 train_time:36640ms step_avg:41.97ms
step:874/1840 train_time:36702ms step_avg:41.99ms
step:875/1840 train_time:36762ms step_avg:42.01ms
step:876/1840 train_time:36825ms step_avg:42.04ms
step:877/1840 train_time:36884ms step_avg:42.06ms
step:878/1840 train_time:36946ms step_avg:42.08ms
step:879/1840 train_time:37007ms step_avg:42.10ms
step:880/1840 train_time:37070ms step_avg:42.13ms
step:881/1840 train_time:37130ms step_avg:42.15ms
step:882/1840 train_time:37192ms step_avg:42.17ms
step:883/1840 train_time:37253ms step_avg:42.19ms
step:884/1840 train_time:37315ms step_avg:42.21ms
step:885/1840 train_time:37374ms step_avg:42.23ms
step:886/1840 train_time:37437ms step_avg:42.25ms
step:887/1840 train_time:37497ms step_avg:42.27ms
step:888/1840 train_time:37560ms step_avg:42.30ms
step:889/1840 train_time:37619ms step_avg:42.32ms
step:890/1840 train_time:37680ms step_avg:42.34ms
step:891/1840 train_time:37740ms step_avg:42.36ms
step:892/1840 train_time:37803ms step_avg:42.38ms
step:893/1840 train_time:37862ms step_avg:42.40ms
step:894/1840 train_time:37925ms step_avg:42.42ms
step:895/1840 train_time:37986ms step_avg:42.44ms
step:896/1840 train_time:38049ms step_avg:42.47ms
step:897/1840 train_time:38110ms step_avg:42.49ms
step:898/1840 train_time:38173ms step_avg:42.51ms
step:899/1840 train_time:38232ms step_avg:42.53ms
step:900/1840 train_time:38295ms step_avg:42.55ms
step:901/1840 train_time:38354ms step_avg:42.57ms
step:902/1840 train_time:38416ms step_avg:42.59ms
step:903/1840 train_time:38476ms step_avg:42.61ms
step:904/1840 train_time:38539ms step_avg:42.63ms
step:905/1840 train_time:38598ms step_avg:42.65ms
step:906/1840 train_time:38661ms step_avg:42.67ms
step:907/1840 train_time:38720ms step_avg:42.69ms
step:908/1840 train_time:38783ms step_avg:42.71ms
step:909/1840 train_time:38842ms step_avg:42.73ms
step:910/1840 train_time:38904ms step_avg:42.75ms
step:911/1840 train_time:38964ms step_avg:42.77ms
step:912/1840 train_time:39028ms step_avg:42.79ms
step:913/1840 train_time:39089ms step_avg:42.81ms
step:914/1840 train_time:39152ms step_avg:42.84ms
step:915/1840 train_time:39212ms step_avg:42.85ms
step:916/1840 train_time:39275ms step_avg:42.88ms
step:917/1840 train_time:39334ms step_avg:42.89ms
step:918/1840 train_time:39396ms step_avg:42.92ms
step:919/1840 train_time:39456ms step_avg:42.93ms
step:920/1840 train_time:39518ms step_avg:42.95ms
step:921/1840 train_time:39577ms step_avg:42.97ms
step:922/1840 train_time:39640ms step_avg:42.99ms
step:923/1840 train_time:39700ms step_avg:43.01ms
step:924/1840 train_time:39762ms step_avg:43.03ms
step:925/1840 train_time:39822ms step_avg:43.05ms
step:926/1840 train_time:39884ms step_avg:43.07ms
step:927/1840 train_time:39943ms step_avg:43.09ms
step:928/1840 train_time:40006ms step_avg:43.11ms
step:929/1840 train_time:40065ms step_avg:43.13ms
step:930/1840 train_time:40129ms step_avg:43.15ms
step:931/1840 train_time:40190ms step_avg:43.17ms
step:932/1840 train_time:40252ms step_avg:43.19ms
step:933/1840 train_time:40312ms step_avg:43.21ms
step:934/1840 train_time:40375ms step_avg:43.23ms
step:935/1840 train_time:40435ms step_avg:43.25ms
step:936/1840 train_time:40496ms step_avg:43.27ms
step:937/1840 train_time:40557ms step_avg:43.28ms
step:938/1840 train_time:40619ms step_avg:43.30ms
step:939/1840 train_time:40679ms step_avg:43.32ms
step:940/1840 train_time:40740ms step_avg:43.34ms
step:941/1840 train_time:40800ms step_avg:43.36ms
step:942/1840 train_time:40862ms step_avg:43.38ms
step:943/1840 train_time:40922ms step_avg:43.40ms
step:944/1840 train_time:40986ms step_avg:43.42ms
step:945/1840 train_time:41047ms step_avg:43.44ms
step:946/1840 train_time:41109ms step_avg:43.46ms
step:947/1840 train_time:41169ms step_avg:43.47ms
step:948/1840 train_time:41232ms step_avg:43.49ms
step:949/1840 train_time:41293ms step_avg:43.51ms
step:950/1840 train_time:41356ms step_avg:43.53ms
step:951/1840 train_time:41415ms step_avg:43.55ms
step:952/1840 train_time:41477ms step_avg:43.57ms
step:953/1840 train_time:41537ms step_avg:43.59ms
step:954/1840 train_time:41599ms step_avg:43.60ms
step:955/1840 train_time:41659ms step_avg:43.62ms
step:956/1840 train_time:41720ms step_avg:43.64ms
step:957/1840 train_time:41780ms step_avg:43.66ms
step:958/1840 train_time:41842ms step_avg:43.68ms
step:959/1840 train_time:41902ms step_avg:43.69ms
step:960/1840 train_time:41964ms step_avg:43.71ms
step:961/1840 train_time:42025ms step_avg:43.73ms
step:962/1840 train_time:42087ms step_avg:43.75ms
step:963/1840 train_time:42147ms step_avg:43.77ms
step:964/1840 train_time:42210ms step_avg:43.79ms
step:965/1840 train_time:42269ms step_avg:43.80ms
step:966/1840 train_time:42332ms step_avg:43.82ms
step:967/1840 train_time:42392ms step_avg:43.84ms
step:968/1840 train_time:42454ms step_avg:43.86ms
step:969/1840 train_time:42514ms step_avg:43.87ms
step:970/1840 train_time:42577ms step_avg:43.89ms
step:971/1840 train_time:42636ms step_avg:43.91ms
step:972/1840 train_time:42697ms step_avg:43.93ms
step:973/1840 train_time:42758ms step_avg:43.94ms
step:974/1840 train_time:42820ms step_avg:43.96ms
step:975/1840 train_time:42880ms step_avg:43.98ms
step:976/1840 train_time:42942ms step_avg:44.00ms
step:977/1840 train_time:43001ms step_avg:44.01ms
step:978/1840 train_time:43064ms step_avg:44.03ms
step:979/1840 train_time:43125ms step_avg:44.05ms
step:980/1840 train_time:43187ms step_avg:44.07ms
step:981/1840 train_time:43247ms step_avg:44.08ms
step:982/1840 train_time:43310ms step_avg:44.10ms
step:983/1840 train_time:43370ms step_avg:44.12ms
step:984/1840 train_time:43433ms step_avg:44.14ms
step:985/1840 train_time:43493ms step_avg:44.16ms
step:986/1840 train_time:43556ms step_avg:44.17ms
step:987/1840 train_time:43616ms step_avg:44.19ms
step:988/1840 train_time:43678ms step_avg:44.21ms
step:989/1840 train_time:43737ms step_avg:44.22ms
step:990/1840 train_time:43800ms step_avg:44.24ms
step:991/1840 train_time:43859ms step_avg:44.26ms
step:992/1840 train_time:43921ms step_avg:44.28ms
step:993/1840 train_time:43981ms step_avg:44.29ms
step:994/1840 train_time:44043ms step_avg:44.31ms
step:995/1840 train_time:44103ms step_avg:44.33ms
step:996/1840 train_time:44167ms step_avg:44.34ms
step:997/1840 train_time:44227ms step_avg:44.36ms
step:998/1840 train_time:44289ms step_avg:44.38ms
step:999/1840 train_time:44349ms step_avg:44.39ms
step:1000/1840 train_time:44412ms step_avg:44.41ms
step:1000/1840 val_loss:3.7690 train_time:44483ms step_avg:44.48ms
step:1001/1840 train_time:44503ms step_avg:44.46ms
step:1002/1840 train_time:44536ms step_avg:44.45ms
step:1003/1840 train_time:44597ms step_avg:44.46ms
step:1004/1840 train_time:44662ms step_avg:44.48ms
step:1005/1840 train_time:44721ms step_avg:44.50ms
step:1006/1840 train_time:44783ms step_avg:44.52ms
step:1007/1840 train_time:44843ms step_avg:44.53ms
step:1008/1840 train_time:44905ms step_avg:44.55ms
step:1009/1840 train_time:44964ms step_avg:44.56ms
step:1010/1840 train_time:45026ms step_avg:44.58ms
step:1011/1840 train_time:45087ms step_avg:44.60ms
step:1012/1840 train_time:45149ms step_avg:44.61ms
step:1013/1840 train_time:45208ms step_avg:44.63ms
step:1014/1840 train_time:45270ms step_avg:44.64ms
step:1015/1840 train_time:45329ms step_avg:44.66ms
step:1016/1840 train_time:45391ms step_avg:44.68ms
step:1017/1840 train_time:45452ms step_avg:44.69ms
step:1018/1840 train_time:45517ms step_avg:44.71ms
step:1019/1840 train_time:45578ms step_avg:44.73ms
step:1020/1840 train_time:45641ms step_avg:44.75ms
step:1021/1840 train_time:45702ms step_avg:44.76ms
step:1022/1840 train_time:45763ms step_avg:44.78ms
step:1023/1840 train_time:45823ms step_avg:44.79ms
step:1024/1840 train_time:45884ms step_avg:44.81ms
step:1025/1840 train_time:45943ms step_avg:44.82ms
step:1026/1840 train_time:46005ms step_avg:44.84ms
step:1027/1840 train_time:46065ms step_avg:44.85ms
step:1028/1840 train_time:46127ms step_avg:44.87ms
step:1029/1840 train_time:46187ms step_avg:44.88ms
step:1030/1840 train_time:46248ms step_avg:44.90ms
step:1031/1840 train_time:46308ms step_avg:44.92ms
step:1032/1840 train_time:46369ms step_avg:44.93ms
step:1033/1840 train_time:46429ms step_avg:44.95ms
step:1034/1840 train_time:46492ms step_avg:44.96ms
step:1035/1840 train_time:46553ms step_avg:44.98ms
step:1036/1840 train_time:46617ms step_avg:45.00ms
step:1037/1840 train_time:46677ms step_avg:45.01ms
step:1038/1840 train_time:46740ms step_avg:45.03ms
step:1039/1840 train_time:46800ms step_avg:45.04ms
step:1040/1840 train_time:46863ms step_avg:45.06ms
step:1041/1840 train_time:46923ms step_avg:45.07ms
step:1042/1840 train_time:46984ms step_avg:45.09ms
step:1043/1840 train_time:47044ms step_avg:45.10ms
step:1044/1840 train_time:47106ms step_avg:45.12ms
step:1045/1840 train_time:47166ms step_avg:45.14ms
step:1046/1840 train_time:47228ms step_avg:45.15ms
step:1047/1840 train_time:47288ms step_avg:45.17ms
step:1048/1840 train_time:47350ms step_avg:45.18ms
step:1049/1840 train_time:47410ms step_avg:45.20ms
step:1050/1840 train_time:47472ms step_avg:45.21ms
step:1051/1840 train_time:47533ms step_avg:45.23ms
step:1052/1840 train_time:47595ms step_avg:45.24ms
step:1053/1840 train_time:47655ms step_avg:45.26ms
step:1054/1840 train_time:47718ms step_avg:45.27ms
step:1055/1840 train_time:47778ms step_avg:45.29ms
step:1056/1840 train_time:47841ms step_avg:45.30ms
step:1057/1840 train_time:47901ms step_avg:45.32ms
step:1058/1840 train_time:47963ms step_avg:45.33ms
step:1059/1840 train_time:48023ms step_avg:45.35ms
step:1060/1840 train_time:48085ms step_avg:45.36ms
step:1061/1840 train_time:48144ms step_avg:45.38ms
step:1062/1840 train_time:48207ms step_avg:45.39ms
step:1063/1840 train_time:48267ms step_avg:45.41ms
step:1064/1840 train_time:48329ms step_avg:45.42ms
step:1065/1840 train_time:48389ms step_avg:45.44ms
step:1066/1840 train_time:48450ms step_avg:45.45ms
step:1067/1840 train_time:48510ms step_avg:45.46ms
step:1068/1840 train_time:48574ms step_avg:45.48ms
step:1069/1840 train_time:48633ms step_avg:45.49ms
step:1070/1840 train_time:48695ms step_avg:45.51ms
step:1071/1840 train_time:48755ms step_avg:45.52ms
step:1072/1840 train_time:48819ms step_avg:45.54ms
step:1073/1840 train_time:48879ms step_avg:45.55ms
step:1074/1840 train_time:48941ms step_avg:45.57ms
step:1075/1840 train_time:49001ms step_avg:45.58ms
step:1076/1840 train_time:49063ms step_avg:45.60ms
step:1077/1840 train_time:49124ms step_avg:45.61ms
step:1078/1840 train_time:49186ms step_avg:45.63ms
step:1079/1840 train_time:49245ms step_avg:45.64ms
step:1080/1840 train_time:49308ms step_avg:45.66ms
step:1081/1840 train_time:49368ms step_avg:45.67ms
step:1082/1840 train_time:49429ms step_avg:45.68ms
step:1083/1840 train_time:49489ms step_avg:45.70ms
step:1084/1840 train_time:49551ms step_avg:45.71ms
step:1085/1840 train_time:49612ms step_avg:45.72ms
step:1086/1840 train_time:49673ms step_avg:45.74ms
step:1087/1840 train_time:49732ms step_avg:45.75ms
step:1088/1840 train_time:49795ms step_avg:45.77ms
step:1089/1840 train_time:49855ms step_avg:45.78ms
step:1090/1840 train_time:49919ms step_avg:45.80ms
step:1091/1840 train_time:49979ms step_avg:45.81ms
step:1092/1840 train_time:50042ms step_avg:45.83ms
step:1093/1840 train_time:50103ms step_avg:45.84ms
step:1094/1840 train_time:50164ms step_avg:45.85ms
step:1095/1840 train_time:50224ms step_avg:45.87ms
step:1096/1840 train_time:50286ms step_avg:45.88ms
step:1097/1840 train_time:50346ms step_avg:45.89ms
step:1098/1840 train_time:50408ms step_avg:45.91ms
step:1099/1840 train_time:50468ms step_avg:45.92ms
step:1100/1840 train_time:50530ms step_avg:45.94ms
step:1101/1840 train_time:50590ms step_avg:45.95ms
step:1102/1840 train_time:50652ms step_avg:45.96ms
step:1103/1840 train_time:50713ms step_avg:45.98ms
step:1104/1840 train_time:50775ms step_avg:45.99ms
step:1105/1840 train_time:50836ms step_avg:46.01ms
step:1106/1840 train_time:50899ms step_avg:46.02ms
step:1107/1840 train_time:50960ms step_avg:46.03ms
step:1108/1840 train_time:51023ms step_avg:46.05ms
step:1109/1840 train_time:51082ms step_avg:46.06ms
step:1110/1840 train_time:51145ms step_avg:46.08ms
step:1111/1840 train_time:51204ms step_avg:46.09ms
step:1112/1840 train_time:51267ms step_avg:46.10ms
step:1113/1840 train_time:51326ms step_avg:46.12ms
step:1114/1840 train_time:51388ms step_avg:46.13ms
step:1115/1840 train_time:51448ms step_avg:46.14ms
step:1116/1840 train_time:51510ms step_avg:46.16ms
step:1117/1840 train_time:51569ms step_avg:46.17ms
step:1118/1840 train_time:51632ms step_avg:46.18ms
step:1119/1840 train_time:51692ms step_avg:46.20ms
step:1120/1840 train_time:51754ms step_avg:46.21ms
step:1121/1840 train_time:51813ms step_avg:46.22ms
step:1122/1840 train_time:51877ms step_avg:46.24ms
step:1123/1840 train_time:51937ms step_avg:46.25ms
step:1124/1840 train_time:51999ms step_avg:46.26ms
step:1125/1840 train_time:52060ms step_avg:46.28ms
step:1126/1840 train_time:52122ms step_avg:46.29ms
step:1127/1840 train_time:52183ms step_avg:46.30ms
step:1128/1840 train_time:52244ms step_avg:46.32ms
step:1129/1840 train_time:52304ms step_avg:46.33ms
step:1130/1840 train_time:52367ms step_avg:46.34ms
step:1131/1840 train_time:52427ms step_avg:46.35ms
step:1132/1840 train_time:52489ms step_avg:46.37ms
step:1133/1840 train_time:52549ms step_avg:46.38ms
step:1134/1840 train_time:52612ms step_avg:46.39ms
step:1135/1840 train_time:52672ms step_avg:46.41ms
step:1136/1840 train_time:52734ms step_avg:46.42ms
step:1137/1840 train_time:52794ms step_avg:46.43ms
step:1138/1840 train_time:52857ms step_avg:46.45ms
step:1139/1840 train_time:52917ms step_avg:46.46ms
step:1140/1840 train_time:52978ms step_avg:46.47ms
step:1141/1840 train_time:53038ms step_avg:46.48ms
step:1142/1840 train_time:53101ms step_avg:46.50ms
step:1143/1840 train_time:53161ms step_avg:46.51ms
step:1144/1840 train_time:53223ms step_avg:46.52ms
step:1145/1840 train_time:53284ms step_avg:46.54ms
step:1146/1840 train_time:53346ms step_avg:46.55ms
step:1147/1840 train_time:53406ms step_avg:46.56ms
step:1148/1840 train_time:53468ms step_avg:46.57ms
step:1149/1840 train_time:53527ms step_avg:46.59ms
step:1150/1840 train_time:53589ms step_avg:46.60ms
step:1151/1840 train_time:53650ms step_avg:46.61ms
step:1152/1840 train_time:53712ms step_avg:46.63ms
step:1153/1840 train_time:53771ms step_avg:46.64ms
step:1154/1840 train_time:53834ms step_avg:46.65ms
step:1155/1840 train_time:53894ms step_avg:46.66ms
step:1156/1840 train_time:53957ms step_avg:46.68ms
step:1157/1840 train_time:54017ms step_avg:46.69ms
step:1158/1840 train_time:54079ms step_avg:46.70ms
step:1159/1840 train_time:54140ms step_avg:46.71ms
step:1160/1840 train_time:54202ms step_avg:46.73ms
step:1161/1840 train_time:54262ms step_avg:46.74ms
step:1162/1840 train_time:54325ms step_avg:46.75ms
step:1163/1840 train_time:54385ms step_avg:46.76ms
step:1164/1840 train_time:54447ms step_avg:46.78ms
step:1165/1840 train_time:54507ms step_avg:46.79ms
step:1166/1840 train_time:54569ms step_avg:46.80ms
step:1167/1840 train_time:54628ms step_avg:46.81ms
step:1168/1840 train_time:54691ms step_avg:46.82ms
step:1169/1840 train_time:54750ms step_avg:46.83ms
step:1170/1840 train_time:54813ms step_avg:46.85ms
step:1171/1840 train_time:54872ms step_avg:46.86ms
step:1172/1840 train_time:54934ms step_avg:46.87ms
step:1173/1840 train_time:54994ms step_avg:46.88ms
step:1174/1840 train_time:55056ms step_avg:46.90ms
step:1175/1840 train_time:55116ms step_avg:46.91ms
step:1176/1840 train_time:55178ms step_avg:46.92ms
step:1177/1840 train_time:55239ms step_avg:46.93ms
step:1178/1840 train_time:55302ms step_avg:46.95ms
step:1179/1840 train_time:55362ms step_avg:46.96ms
step:1180/1840 train_time:55424ms step_avg:46.97ms
step:1181/1840 train_time:55484ms step_avg:46.98ms
step:1182/1840 train_time:55547ms step_avg:46.99ms
step:1183/1840 train_time:55607ms step_avg:47.01ms
step:1184/1840 train_time:55670ms step_avg:47.02ms
step:1185/1840 train_time:55729ms step_avg:47.03ms
step:1186/1840 train_time:55791ms step_avg:47.04ms
step:1187/1840 train_time:55851ms step_avg:47.05ms
step:1188/1840 train_time:55913ms step_avg:47.06ms
step:1189/1840 train_time:55973ms step_avg:47.08ms
step:1190/1840 train_time:56035ms step_avg:47.09ms
step:1191/1840 train_time:56095ms step_avg:47.10ms
step:1192/1840 train_time:56157ms step_avg:47.11ms
step:1193/1840 train_time:56218ms step_avg:47.12ms
step:1194/1840 train_time:56281ms step_avg:47.14ms
step:1195/1840 train_time:56341ms step_avg:47.15ms
step:1196/1840 train_time:56404ms step_avg:47.16ms
step:1197/1840 train_time:56464ms step_avg:47.17ms
step:1198/1840 train_time:56526ms step_avg:47.18ms
step:1199/1840 train_time:56587ms step_avg:47.20ms
step:1200/1840 train_time:56649ms step_avg:47.21ms
step:1201/1840 train_time:56710ms step_avg:47.22ms
step:1202/1840 train_time:56796ms step_avg:47.25ms
step:1203/1840 train_time:56882ms step_avg:47.28ms
step:1204/1840 train_time:56973ms step_avg:47.32ms
step:1205/1840 train_time:57060ms step_avg:47.35ms
step:1206/1840 train_time:57150ms step_avg:47.39ms
step:1207/1840 train_time:57236ms step_avg:47.42ms
step:1208/1840 train_time:57325ms step_avg:47.45ms
step:1209/1840 train_time:57413ms step_avg:47.49ms
step:1210/1840 train_time:57502ms step_avg:47.52ms
step:1211/1840 train_time:57592ms step_avg:47.56ms
step:1212/1840 train_time:57680ms step_avg:47.59ms
step:1213/1840 train_time:57767ms step_avg:47.62ms
step:1214/1840 train_time:57856ms step_avg:47.66ms
step:1215/1840 train_time:57941ms step_avg:47.69ms
step:1216/1840 train_time:58030ms step_avg:47.72ms
step:1217/1840 train_time:58116ms step_avg:47.75ms
step:1218/1840 train_time:58205ms step_avg:47.79ms
step:1219/1840 train_time:58291ms step_avg:47.82ms
step:1220/1840 train_time:58380ms step_avg:47.85ms
step:1221/1840 train_time:58468ms step_avg:47.89ms
step:1222/1840 train_time:58558ms step_avg:47.92ms
step:1223/1840 train_time:58644ms step_avg:47.95ms
step:1224/1840 train_time:58734ms step_avg:47.99ms
step:1225/1840 train_time:58819ms step_avg:48.02ms
step:1226/1840 train_time:58909ms step_avg:48.05ms
step:1227/1840 train_time:58995ms step_avg:48.08ms
step:1228/1840 train_time:59082ms step_avg:48.11ms
step:1229/1840 train_time:59169ms step_avg:48.14ms
step:1230/1840 train_time:59259ms step_avg:48.18ms
step:1231/1840 train_time:59346ms step_avg:48.21ms
step:1232/1840 train_time:59436ms step_avg:48.24ms
step:1233/1840 train_time:59522ms step_avg:48.27ms
step:1234/1840 train_time:59612ms step_avg:48.31ms
step:1235/1840 train_time:59700ms step_avg:48.34ms
step:1236/1840 train_time:59788ms step_avg:48.37ms
step:1237/1840 train_time:59875ms step_avg:48.40ms
step:1238/1840 train_time:59962ms step_avg:48.43ms
step:1239/1840 train_time:60049ms step_avg:48.47ms
step:1240/1840 train_time:60138ms step_avg:48.50ms
step:1241/1840 train_time:60223ms step_avg:48.53ms
step:1242/1840 train_time:60313ms step_avg:48.56ms
step:1243/1840 train_time:60399ms step_avg:48.59ms
step:1244/1840 train_time:60486ms step_avg:48.62ms
step:1245/1840 train_time:60575ms step_avg:48.65ms
step:1246/1840 train_time:60662ms step_avg:48.69ms
step:1247/1840 train_time:60749ms step_avg:48.72ms
step:1248/1840 train_time:60839ms step_avg:48.75ms
step:1249/1840 train_time:60925ms step_avg:48.78ms
step:1250/1840 train_time:61014ms step_avg:48.81ms
step:1250/1840 val_loss:3.5324 train_time:61113ms step_avg:48.89ms
step:1251/1840 train_time:61134ms step_avg:48.87ms
step:1252/1840 train_time:61194ms step_avg:48.88ms
step:1253/1840 train_time:61283ms step_avg:48.91ms
step:1254/1840 train_time:61371ms step_avg:48.94ms
step:1255/1840 train_time:61457ms step_avg:48.97ms
step:1256/1840 train_time:61547ms step_avg:49.00ms
step:1257/1840 train_time:61633ms step_avg:49.03ms
step:1258/1840 train_time:61721ms step_avg:49.06ms
step:1259/1840 train_time:61806ms step_avg:49.09ms
step:1260/1840 train_time:61895ms step_avg:49.12ms
step:1261/1840 train_time:61980ms step_avg:49.15ms
step:1262/1840 train_time:62070ms step_avg:49.18ms
step:1263/1840 train_time:62158ms step_avg:49.21ms
step:1264/1840 train_time:62249ms step_avg:49.25ms
step:1265/1840 train_time:62338ms step_avg:49.28ms
step:1266/1840 train_time:62429ms step_avg:49.31ms
step:1267/1840 train_time:62514ms step_avg:49.34ms
step:1268/1840 train_time:62604ms step_avg:49.37ms
step:1269/1840 train_time:62689ms step_avg:49.40ms
step:1270/1840 train_time:62778ms step_avg:49.43ms
step:1271/1840 train_time:62864ms step_avg:49.46ms
step:1272/1840 train_time:62951ms step_avg:49.49ms
step:1273/1840 train_time:63037ms step_avg:49.52ms
step:1274/1840 train_time:63127ms step_avg:49.55ms
step:1275/1840 train_time:63214ms step_avg:49.58ms
step:1276/1840 train_time:63305ms step_avg:49.61ms
step:1277/1840 train_time:63390ms step_avg:49.64ms
step:1278/1840 train_time:63480ms step_avg:49.67ms
step:1279/1840 train_time:63565ms step_avg:49.70ms
step:1280/1840 train_time:63653ms step_avg:49.73ms
step:1281/1840 train_time:63739ms step_avg:49.76ms
step:1282/1840 train_time:63826ms step_avg:49.79ms
step:1283/1840 train_time:63912ms step_avg:49.81ms
step:1284/1840 train_time:64001ms step_avg:49.85ms
step:1285/1840 train_time:64088ms step_avg:49.87ms
step:1286/1840 train_time:64178ms step_avg:49.91ms
step:1287/1840 train_time:64267ms step_avg:49.94ms
step:1288/1840 train_time:64355ms step_avg:49.97ms
step:1289/1840 train_time:64442ms step_avg:49.99ms
step:1290/1840 train_time:64530ms step_avg:50.02ms
step:1291/1840 train_time:64616ms step_avg:50.05ms
step:1292/1840 train_time:64705ms step_avg:50.08ms
step:1293/1840 train_time:64791ms step_avg:50.11ms
step:1294/1840 train_time:64879ms step_avg:50.14ms
step:1295/1840 train_time:64967ms step_avg:50.17ms
step:1296/1840 train_time:65056ms step_avg:50.20ms
step:1297/1840 train_time:65142ms step_avg:50.23ms
step:1298/1840 train_time:65230ms step_avg:50.25ms
step:1299/1840 train_time:65316ms step_avg:50.28ms
step:1300/1840 train_time:65407ms step_avg:50.31ms
step:1301/1840 train_time:65493ms step_avg:50.34ms
step:1302/1840 train_time:65582ms step_avg:50.37ms
step:1303/1840 train_time:65668ms step_avg:50.40ms
step:1304/1840 train_time:65755ms step_avg:50.43ms
step:1305/1840 train_time:65841ms step_avg:50.45ms
step:1306/1840 train_time:65929ms step_avg:50.48ms
step:1307/1840 train_time:66015ms step_avg:50.51ms
step:1308/1840 train_time:66105ms step_avg:50.54ms
step:1309/1840 train_time:66192ms step_avg:50.57ms
step:1310/1840 train_time:66280ms step_avg:50.60ms
step:1311/1840 train_time:66368ms step_avg:50.62ms
step:1312/1840 train_time:66456ms step_avg:50.65ms
step:1313/1840 train_time:66542ms step_avg:50.68ms
step:1314/1840 train_time:66630ms step_avg:50.71ms
step:1315/1840 train_time:66716ms step_avg:50.73ms
step:1316/1840 train_time:66806ms step_avg:50.76ms
step:1317/1840 train_time:66891ms step_avg:50.79ms
step:1318/1840 train_time:66979ms step_avg:50.82ms
step:1319/1840 train_time:67066ms step_avg:50.85ms
step:1320/1840 train_time:67156ms step_avg:50.88ms
step:1321/1840 train_time:67242ms step_avg:50.90ms
step:1322/1840 train_time:67330ms step_avg:50.93ms
step:1323/1840 train_time:67416ms step_avg:50.96ms
step:1324/1840 train_time:67506ms step_avg:50.99ms
step:1325/1840 train_time:67592ms step_avg:51.01ms
step:1326/1840 train_time:67682ms step_avg:51.04ms
step:1327/1840 train_time:67767ms step_avg:51.07ms
step:1328/1840 train_time:67855ms step_avg:51.10ms
step:1329/1840 train_time:67941ms step_avg:51.12ms
step:1330/1840 train_time:68029ms step_avg:51.15ms
step:1331/1840 train_time:68114ms step_avg:51.18ms
step:1332/1840 train_time:68205ms step_avg:51.21ms
step:1333/1840 train_time:68292ms step_avg:51.23ms
step:1334/1840 train_time:68382ms step_avg:51.26ms
step:1335/1840 train_time:68468ms step_avg:51.29ms
step:1336/1840 train_time:68556ms step_avg:51.31ms
step:1337/1840 train_time:68642ms step_avg:51.34ms
step:1338/1840 train_time:68731ms step_avg:51.37ms
step:1339/1840 train_time:68816ms step_avg:51.39ms
step:1340/1840 train_time:68906ms step_avg:51.42ms
step:1341/1840 train_time:68991ms step_avg:51.45ms
step:1342/1840 train_time:69080ms step_avg:51.48ms
step:1343/1840 train_time:69166ms step_avg:51.50ms
step:1344/1840 train_time:69255ms step_avg:51.53ms
step:1345/1840 train_time:69342ms step_avg:51.56ms
step:1346/1840 train_time:69431ms step_avg:51.58ms
step:1347/1840 train_time:69517ms step_avg:51.61ms
step:1348/1840 train_time:69607ms step_avg:51.64ms
step:1349/1840 train_time:69693ms step_avg:51.66ms
step:1350/1840 train_time:69781ms step_avg:51.69ms
step:1351/1840 train_time:69867ms step_avg:51.72ms
step:1352/1840 train_time:69955ms step_avg:51.74ms
step:1353/1840 train_time:70041ms step_avg:51.77ms
step:1354/1840 train_time:70129ms step_avg:51.79ms
step:1355/1840 train_time:70215ms step_avg:51.82ms
step:1356/1840 train_time:70306ms step_avg:51.85ms
step:1357/1840 train_time:70392ms step_avg:51.87ms
step:1358/1840 train_time:70480ms step_avg:51.90ms
step:1359/1840 train_time:70567ms step_avg:51.93ms
step:1360/1840 train_time:70656ms step_avg:51.95ms
step:1361/1840 train_time:70742ms step_avg:51.98ms
step:1362/1840 train_time:70830ms step_avg:52.00ms
step:1363/1840 train_time:70917ms step_avg:52.03ms
step:1364/1840 train_time:71005ms step_avg:52.06ms
step:1365/1840 train_time:71090ms step_avg:52.08ms
step:1366/1840 train_time:71180ms step_avg:52.11ms
step:1367/1840 train_time:71268ms step_avg:52.13ms
step:1368/1840 train_time:71357ms step_avg:52.16ms
step:1369/1840 train_time:71442ms step_avg:52.19ms
step:1370/1840 train_time:71529ms step_avg:52.21ms
step:1371/1840 train_time:71616ms step_avg:52.24ms
step:1372/1840 train_time:71706ms step_avg:52.26ms
step:1373/1840 train_time:71791ms step_avg:52.29ms
step:1374/1840 train_time:71880ms step_avg:52.31ms
step:1375/1840 train_time:71966ms step_avg:52.34ms
step:1376/1840 train_time:72056ms step_avg:52.37ms
step:1377/1840 train_time:72141ms step_avg:52.39ms
step:1378/1840 train_time:72230ms step_avg:52.42ms
step:1379/1840 train_time:72317ms step_avg:52.44ms
step:1380/1840 train_time:72407ms step_avg:52.47ms
step:1381/1840 train_time:72492ms step_avg:52.49ms
step:1382/1840 train_time:72581ms step_avg:52.52ms
step:1383/1840 train_time:72667ms step_avg:52.54ms
step:1384/1840 train_time:72755ms step_avg:52.57ms
step:1385/1840 train_time:72841ms step_avg:52.59ms
step:1386/1840 train_time:72930ms step_avg:52.62ms
step:1387/1840 train_time:73016ms step_avg:52.64ms
step:1388/1840 train_time:73105ms step_avg:52.67ms
step:1389/1840 train_time:73191ms step_avg:52.69ms
step:1390/1840 train_time:73280ms step_avg:52.72ms
step:1391/1840 train_time:73367ms step_avg:52.74ms
step:1392/1840 train_time:73456ms step_avg:52.77ms
step:1393/1840 train_time:73541ms step_avg:52.79ms
step:1394/1840 train_time:73629ms step_avg:52.82ms
step:1395/1840 train_time:73715ms step_avg:52.84ms
step:1396/1840 train_time:73806ms step_avg:52.87ms
step:1397/1840 train_time:73891ms step_avg:52.89ms
step:1398/1840 train_time:73980ms step_avg:52.92ms
step:1399/1840 train_time:74067ms step_avg:52.94ms
step:1400/1840 train_time:74155ms step_avg:52.97ms
step:1401/1840 train_time:74240ms step_avg:52.99ms
step:1402/1840 train_time:74329ms step_avg:53.02ms
step:1403/1840 train_time:74415ms step_avg:53.04ms
step:1404/1840 train_time:74506ms step_avg:53.07ms
step:1405/1840 train_time:74592ms step_avg:53.09ms
step:1406/1840 train_time:74681ms step_avg:53.12ms
step:1407/1840 train_time:74767ms step_avg:53.14ms
step:1408/1840 train_time:74856ms step_avg:53.16ms
step:1409/1840 train_time:74942ms step_avg:53.19ms
step:1410/1840 train_time:75029ms step_avg:53.21ms
step:1411/1840 train_time:75116ms step_avg:53.24ms
step:1412/1840 train_time:75206ms step_avg:53.26ms
step:1413/1840 train_time:75291ms step_avg:53.28ms
step:1414/1840 train_time:75380ms step_avg:53.31ms
step:1415/1840 train_time:75467ms step_avg:53.33ms
step:1416/1840 train_time:75556ms step_avg:53.36ms
step:1417/1840 train_time:75642ms step_avg:53.38ms
step:1418/1840 train_time:75729ms step_avg:53.41ms
step:1419/1840 train_time:75816ms step_avg:53.43ms
step:1420/1840 train_time:75906ms step_avg:53.46ms
step:1421/1840 train_time:75992ms step_avg:53.48ms
step:1422/1840 train_time:76081ms step_avg:53.50ms
step:1423/1840 train_time:76167ms step_avg:53.53ms
step:1424/1840 train_time:76255ms step_avg:53.55ms
step:1425/1840 train_time:76342ms step_avg:53.57ms
step:1426/1840 train_time:76429ms step_avg:53.60ms
step:1427/1840 train_time:76515ms step_avg:53.62ms
step:1428/1840 train_time:76606ms step_avg:53.65ms
step:1429/1840 train_time:76692ms step_avg:53.67ms
step:1430/1840 train_time:76781ms step_avg:53.69ms
step:1431/1840 train_time:76868ms step_avg:53.72ms
step:1432/1840 train_time:76955ms step_avg:53.74ms
step:1433/1840 train_time:77042ms step_avg:53.76ms
step:1434/1840 train_time:77130ms step_avg:53.79ms
step:1435/1840 train_time:77215ms step_avg:53.81ms
step:1436/1840 train_time:77306ms step_avg:53.83ms
step:1437/1840 train_time:77392ms step_avg:53.86ms
step:1438/1840 train_time:77482ms step_avg:53.88ms
step:1439/1840 train_time:77568ms step_avg:53.90ms
step:1440/1840 train_time:77656ms step_avg:53.93ms
step:1441/1840 train_time:77743ms step_avg:53.95ms
step:1442/1840 train_time:77831ms step_avg:53.97ms
step:1443/1840 train_time:77917ms step_avg:54.00ms
step:1444/1840 train_time:78007ms step_avg:54.02ms
step:1445/1840 train_time:78093ms step_avg:54.04ms
step:1446/1840 train_time:78181ms step_avg:54.07ms
step:1447/1840 train_time:78268ms step_avg:54.09ms
step:1448/1840 train_time:78355ms step_avg:54.11ms
step:1449/1840 train_time:78440ms step_avg:54.13ms
step:1450/1840 train_time:78531ms step_avg:54.16ms
step:1451/1840 train_time:78617ms step_avg:54.18ms
step:1452/1840 train_time:78707ms step_avg:54.21ms
step:1453/1840 train_time:78793ms step_avg:54.23ms
step:1454/1840 train_time:78883ms step_avg:54.25ms
step:1455/1840 train_time:78968ms step_avg:54.27ms
step:1456/1840 train_time:79057ms step_avg:54.30ms
step:1457/1840 train_time:79145ms step_avg:54.32ms
step:1458/1840 train_time:79234ms step_avg:54.34ms
step:1459/1840 train_time:79321ms step_avg:54.37ms
step:1460/1840 train_time:79409ms step_avg:54.39ms
step:1461/1840 train_time:79494ms step_avg:54.41ms
step:1462/1840 train_time:79584ms step_avg:54.44ms
step:1463/1840 train_time:79670ms step_avg:54.46ms
step:1464/1840 train_time:79760ms step_avg:54.48ms
step:1465/1840 train_time:79846ms step_avg:54.50ms
step:1466/1840 train_time:79935ms step_avg:54.53ms
step:1467/1840 train_time:80020ms step_avg:54.55ms
step:1468/1840 train_time:80108ms step_avg:54.57ms
step:1469/1840 train_time:80195ms step_avg:54.59ms
step:1470/1840 train_time:80285ms step_avg:54.62ms
step:1471/1840 train_time:80372ms step_avg:54.64ms
step:1472/1840 train_time:80460ms step_avg:54.66ms
step:1473/1840 train_time:80545ms step_avg:54.68ms
step:1474/1840 train_time:80635ms step_avg:54.70ms
step:1475/1840 train_time:80720ms step_avg:54.73ms
step:1476/1840 train_time:80809ms step_avg:54.75ms
step:1477/1840 train_time:80895ms step_avg:54.77ms
step:1478/1840 train_time:80984ms step_avg:54.79ms
step:1479/1840 train_time:81070ms step_avg:54.81ms
step:1480/1840 train_time:81159ms step_avg:54.84ms
step:1481/1840 train_time:81246ms step_avg:54.86ms
step:1482/1840 train_time:81335ms step_avg:54.88ms
step:1483/1840 train_time:81421ms step_avg:54.90ms
step:1484/1840 train_time:81509ms step_avg:54.93ms
step:1485/1840 train_time:81596ms step_avg:54.95ms
step:1486/1840 train_time:81686ms step_avg:54.97ms
step:1487/1840 train_time:81772ms step_avg:54.99ms
step:1488/1840 train_time:81861ms step_avg:55.01ms
step:1489/1840 train_time:81946ms step_avg:55.03ms
step:1490/1840 train_time:82034ms step_avg:55.06ms
step:1491/1840 train_time:82121ms step_avg:55.08ms
step:1492/1840 train_time:82210ms step_avg:55.10ms
step:1493/1840 train_time:82296ms step_avg:55.12ms
step:1494/1840 train_time:82386ms step_avg:55.14ms
step:1495/1840 train_time:82471ms step_avg:55.16ms
step:1496/1840 train_time:82560ms step_avg:55.19ms
step:1497/1840 train_time:82646ms step_avg:55.21ms
step:1498/1840 train_time:82735ms step_avg:55.23ms
step:1499/1840 train_time:82820ms step_avg:55.25ms
step:1500/1840 train_time:82908ms step_avg:55.27ms
step:1500/1840 val_loss:3.4010 train_time:83011ms step_avg:55.34ms
step:1501/1840 train_time:83034ms step_avg:55.32ms
step:1502/1840 train_time:83090ms step_avg:55.32ms
step:1503/1840 train_time:83181ms step_avg:55.34ms
step:1504/1840 train_time:83269ms step_avg:55.36ms
step:1505/1840 train_time:83354ms step_avg:55.38ms
step:1506/1840 train_time:83441ms step_avg:55.41ms
step:1507/1840 train_time:83527ms step_avg:55.43ms
step:1508/1840 train_time:83613ms step_avg:55.45ms
step:1509/1840 train_time:83698ms step_avg:55.47ms
step:1510/1840 train_time:83787ms step_avg:55.49ms
step:1511/1840 train_time:83872ms step_avg:55.51ms
step:1512/1840 train_time:83962ms step_avg:55.53ms
step:1513/1840 train_time:84052ms step_avg:55.55ms
step:1514/1840 train_time:84143ms step_avg:55.58ms
step:1515/1840 train_time:84231ms step_avg:55.60ms
step:1516/1840 train_time:84318ms step_avg:55.62ms
step:1517/1840 train_time:84404ms step_avg:55.64ms
step:1518/1840 train_time:84491ms step_avg:55.66ms
step:1519/1840 train_time:84577ms step_avg:55.68ms
step:1520/1840 train_time:84664ms step_avg:55.70ms
step:1521/1840 train_time:84750ms step_avg:55.72ms
step:1522/1840 train_time:84838ms step_avg:55.74ms
step:1523/1840 train_time:84926ms step_avg:55.76ms
step:1524/1840 train_time:85015ms step_avg:55.78ms
step:1525/1840 train_time:85103ms step_avg:55.81ms
step:1526/1840 train_time:85194ms step_avg:55.83ms
step:1527/1840 train_time:85280ms step_avg:55.85ms
step:1528/1840 train_time:85370ms step_avg:55.87ms
step:1529/1840 train_time:85455ms step_avg:55.89ms
step:1530/1840 train_time:85543ms step_avg:55.91ms
step:1531/1840 train_time:85628ms step_avg:55.93ms
step:1532/1840 train_time:85716ms step_avg:55.95ms
step:1533/1840 train_time:85801ms step_avg:55.97ms
step:1534/1840 train_time:85891ms step_avg:55.99ms
step:1535/1840 train_time:85977ms step_avg:56.01ms
step:1536/1840 train_time:86069ms step_avg:56.03ms
step:1537/1840 train_time:86155ms step_avg:56.05ms
step:1538/1840 train_time:86244ms step_avg:56.08ms
step:1539/1840 train_time:86331ms step_avg:56.10ms
step:1540/1840 train_time:86419ms step_avg:56.12ms
step:1541/1840 train_time:86505ms step_avg:56.14ms
step:1542/1840 train_time:86594ms step_avg:56.16ms
step:1543/1840 train_time:86679ms step_avg:56.18ms
step:1544/1840 train_time:86769ms step_avg:56.20ms
step:1545/1840 train_time:86854ms step_avg:56.22ms
step:1546/1840 train_time:86942ms step_avg:56.24ms
step:1547/1840 train_time:87030ms step_avg:56.26ms
step:1548/1840 train_time:87119ms step_avg:56.28ms
step:1549/1840 train_time:87206ms step_avg:56.30ms
step:1550/1840 train_time:87294ms step_avg:56.32ms
step:1551/1840 train_time:87380ms step_avg:56.34ms
step:1552/1840 train_time:87470ms step_avg:56.36ms
step:1553/1840 train_time:87555ms step_avg:56.38ms
step:1554/1840 train_time:87643ms step_avg:56.40ms
step:1555/1840 train_time:87729ms step_avg:56.42ms
step:1556/1840 train_time:87817ms step_avg:56.44ms
step:1557/1840 train_time:87904ms step_avg:56.46ms
step:1558/1840 train_time:87992ms step_avg:56.48ms
step:1559/1840 train_time:88079ms step_avg:56.50ms
step:1560/1840 train_time:88170ms step_avg:56.52ms
step:1561/1840 train_time:88258ms step_avg:56.54ms
step:1562/1840 train_time:88348ms step_avg:56.56ms
step:1563/1840 train_time:88433ms step_avg:56.58ms
step:1564/1840 train_time:88523ms step_avg:56.60ms
step:1565/1840 train_time:88609ms step_avg:56.62ms
step:1566/1840 train_time:88696ms step_avg:56.64ms
step:1567/1840 train_time:88782ms step_avg:56.66ms
step:1568/1840 train_time:88870ms step_avg:56.68ms
step:1569/1840 train_time:88956ms step_avg:56.70ms
step:1570/1840 train_time:89044ms step_avg:56.72ms
step:1571/1840 train_time:89131ms step_avg:56.74ms
step:1572/1840 train_time:89220ms step_avg:56.76ms
step:1573/1840 train_time:89307ms step_avg:56.78ms
step:1574/1840 train_time:89395ms step_avg:56.80ms
step:1575/1840 train_time:89482ms step_avg:56.81ms
step:1576/1840 train_time:89571ms step_avg:56.83ms
step:1577/1840 train_time:89656ms step_avg:56.85ms
step:1578/1840 train_time:89745ms step_avg:56.87ms
step:1579/1840 train_time:89831ms step_avg:56.89ms
step:1580/1840 train_time:89920ms step_avg:56.91ms
step:1581/1840 train_time:90006ms step_avg:56.93ms
step:1582/1840 train_time:90094ms step_avg:56.95ms
step:1583/1840 train_time:90182ms step_avg:56.97ms
step:1584/1840 train_time:90271ms step_avg:56.99ms
step:1585/1840 train_time:90357ms step_avg:57.01ms
step:1586/1840 train_time:90446ms step_avg:57.03ms
step:1587/1840 train_time:90531ms step_avg:57.05ms
step:1588/1840 train_time:90620ms step_avg:57.07ms
step:1589/1840 train_time:90706ms step_avg:57.08ms
step:1590/1840 train_time:90795ms step_avg:57.10ms
step:1591/1840 train_time:90881ms step_avg:57.12ms
step:1592/1840 train_time:90970ms step_avg:57.14ms
step:1593/1840 train_time:91056ms step_avg:57.16ms
step:1594/1840 train_time:91144ms step_avg:57.18ms
step:1595/1840 train_time:91231ms step_avg:57.20ms
step:1596/1840 train_time:91320ms step_avg:57.22ms
step:1597/1840 train_time:91407ms step_avg:57.24ms
step:1598/1840 train_time:91495ms step_avg:57.26ms
step:1599/1840 train_time:91581ms step_avg:57.27ms
step:1600/1840 train_time:91670ms step_avg:57.29ms
step:1601/1840 train_time:91755ms step_avg:57.31ms
step:1602/1840 train_time:91844ms step_avg:57.33ms
step:1603/1840 train_time:91931ms step_avg:57.35ms
step:1604/1840 train_time:92020ms step_avg:57.37ms
step:1605/1840 train_time:92105ms step_avg:57.39ms
step:1606/1840 train_time:92194ms step_avg:57.41ms
step:1607/1840 train_time:92280ms step_avg:57.42ms
step:1608/1840 train_time:92370ms step_avg:57.44ms
step:1609/1840 train_time:92457ms step_avg:57.46ms
step:1610/1840 train_time:92547ms step_avg:57.48ms
step:1611/1840 train_time:92632ms step_avg:57.50ms
step:1612/1840 train_time:92722ms step_avg:57.52ms
step:1613/1840 train_time:92808ms step_avg:57.54ms
step:1614/1840 train_time:92896ms step_avg:57.56ms
step:1615/1840 train_time:92983ms step_avg:57.57ms
step:1616/1840 train_time:93071ms step_avg:57.59ms
step:1617/1840 train_time:93158ms step_avg:57.61ms
step:1618/1840 train_time:93249ms step_avg:57.63ms
step:1619/1840 train_time:93334ms step_avg:57.65ms
step:1620/1840 train_time:93422ms step_avg:57.67ms
step:1621/1840 train_time:93509ms step_avg:57.69ms
step:1622/1840 train_time:93596ms step_avg:57.70ms
step:1623/1840 train_time:93683ms step_avg:57.72ms
step:1624/1840 train_time:93771ms step_avg:57.74ms
step:1625/1840 train_time:93857ms step_avg:57.76ms
step:1626/1840 train_time:93946ms step_avg:57.78ms
step:1627/1840 train_time:94032ms step_avg:57.79ms
step:1628/1840 train_time:94121ms step_avg:57.81ms
step:1629/1840 train_time:94208ms step_avg:57.83ms
step:1630/1840 train_time:94296ms step_avg:57.85ms
step:1631/1840 train_time:94382ms step_avg:57.87ms
step:1632/1840 train_time:94471ms step_avg:57.89ms
step:1633/1840 train_time:94556ms step_avg:57.90ms
step:1634/1840 train_time:94644ms step_avg:57.92ms
step:1635/1840 train_time:94732ms step_avg:57.94ms
step:1636/1840 train_time:94820ms step_avg:57.96ms
step:1637/1840 train_time:94906ms step_avg:57.98ms
step:1638/1840 train_time:94995ms step_avg:57.99ms
step:1639/1840 train_time:95081ms step_avg:58.01ms
step:1640/1840 train_time:95169ms step_avg:58.03ms
step:1641/1840 train_time:95255ms step_avg:58.05ms
step:1642/1840 train_time:95345ms step_avg:58.07ms
step:1643/1840 train_time:95432ms step_avg:58.08ms
step:1644/1840 train_time:95521ms step_avg:58.10ms
step:1645/1840 train_time:95608ms step_avg:58.12ms
step:1646/1840 train_time:95696ms step_avg:58.14ms
step:1647/1840 train_time:95782ms step_avg:58.16ms
step:1648/1840 train_time:95871ms step_avg:58.17ms
step:1649/1840 train_time:95955ms step_avg:58.19ms
step:1650/1840 train_time:96045ms step_avg:58.21ms
step:1651/1840 train_time:96132ms step_avg:58.23ms
step:1652/1840 train_time:96220ms step_avg:58.24ms
step:1653/1840 train_time:96307ms step_avg:58.26ms
step:1654/1840 train_time:96395ms step_avg:58.28ms
step:1655/1840 train_time:96481ms step_avg:58.30ms
step:1656/1840 train_time:96570ms step_avg:58.32ms
step:1657/1840 train_time:96656ms step_avg:58.33ms
step:1658/1840 train_time:96745ms step_avg:58.35ms
step:1659/1840 train_time:96831ms step_avg:58.37ms
step:1660/1840 train_time:96919ms step_avg:58.39ms
step:1661/1840 train_time:97005ms step_avg:58.40ms
step:1662/1840 train_time:97094ms step_avg:58.42ms
step:1663/1840 train_time:97180ms step_avg:58.44ms
step:1664/1840 train_time:97269ms step_avg:58.46ms
step:1665/1840 train_time:97355ms step_avg:58.47ms
step:1666/1840 train_time:97444ms step_avg:58.49ms
step:1667/1840 train_time:97531ms step_avg:58.51ms
step:1668/1840 train_time:97619ms step_avg:58.52ms
step:1669/1840 train_time:97705ms step_avg:58.54ms
step:1670/1840 train_time:97793ms step_avg:58.56ms
step:1671/1840 train_time:97879ms step_avg:58.58ms
step:1672/1840 train_time:97968ms step_avg:58.59ms
step:1673/1840 train_time:98054ms step_avg:58.61ms
step:1674/1840 train_time:98143ms step_avg:58.63ms
step:1675/1840 train_time:98231ms step_avg:58.65ms
step:1676/1840 train_time:98320ms step_avg:58.66ms
step:1677/1840 train_time:98407ms step_avg:58.68ms
step:1678/1840 train_time:98496ms step_avg:58.70ms
step:1679/1840 train_time:98583ms step_avg:58.72ms
step:1680/1840 train_time:98672ms step_avg:58.73ms
step:1681/1840 train_time:98758ms step_avg:58.75ms
step:1682/1840 train_time:98847ms step_avg:58.77ms
step:1683/1840 train_time:98932ms step_avg:58.78ms
step:1684/1840 train_time:99022ms step_avg:58.80ms
step:1685/1840 train_time:99109ms step_avg:58.82ms
step:1686/1840 train_time:99198ms step_avg:58.84ms
step:1687/1840 train_time:99284ms step_avg:58.85ms
step:1688/1840 train_time:99372ms step_avg:58.87ms
step:1689/1840 train_time:99459ms step_avg:58.89ms
step:1690/1840 train_time:99549ms step_avg:58.90ms
step:1691/1840 train_time:99634ms step_avg:58.92ms
step:1692/1840 train_time:99723ms step_avg:58.94ms
step:1693/1840 train_time:99809ms step_avg:58.95ms
step:1694/1840 train_time:99897ms step_avg:58.97ms
step:1695/1840 train_time:99983ms step_avg:58.99ms
step:1696/1840 train_time:100072ms step_avg:59.00ms
step:1697/1840 train_time:100158ms step_avg:59.02ms
step:1698/1840 train_time:100249ms step_avg:59.04ms
step:1699/1840 train_time:100335ms step_avg:59.06ms
step:1700/1840 train_time:100425ms step_avg:59.07ms
step:1701/1840 train_time:100511ms step_avg:59.09ms
step:1702/1840 train_time:100600ms step_avg:59.11ms
step:1703/1840 train_time:100686ms step_avg:59.12ms
step:1704/1840 train_time:100774ms step_avg:59.14ms
step:1705/1840 train_time:100860ms step_avg:59.16ms
step:1706/1840 train_time:100949ms step_avg:59.17ms
step:1707/1840 train_time:101035ms step_avg:59.19ms
step:1708/1840 train_time:101124ms step_avg:59.21ms
step:1709/1840 train_time:101211ms step_avg:59.22ms
step:1710/1840 train_time:101300ms step_avg:59.24ms
step:1711/1840 train_time:101387ms step_avg:59.26ms
step:1712/1840 train_time:101474ms step_avg:59.27ms
step:1713/1840 train_time:101560ms step_avg:59.29ms
step:1714/1840 train_time:101650ms step_avg:59.31ms
step:1715/1840 train_time:101736ms step_avg:59.32ms
step:1716/1840 train_time:101825ms step_avg:59.34ms
step:1717/1840 train_time:101910ms step_avg:59.35ms
step:1718/1840 train_time:101999ms step_avg:59.37ms
step:1719/1840 train_time:102085ms step_avg:59.39ms
step:1720/1840 train_time:102173ms step_avg:59.40ms
step:1721/1840 train_time:102261ms step_avg:59.42ms
step:1722/1840 train_time:102351ms step_avg:59.44ms
step:1723/1840 train_time:102436ms step_avg:59.45ms
step:1724/1840 train_time:102527ms step_avg:59.47ms
step:1725/1840 train_time:102613ms step_avg:59.49ms
step:1726/1840 train_time:102702ms step_avg:59.50ms
step:1727/1840 train_time:102788ms step_avg:59.52ms
step:1728/1840 train_time:102876ms step_avg:59.53ms
step:1729/1840 train_time:102962ms step_avg:59.55ms
step:1730/1840 train_time:103050ms step_avg:59.57ms
step:1731/1840 train_time:103136ms step_avg:59.58ms
step:1732/1840 train_time:103226ms step_avg:59.60ms
step:1733/1840 train_time:103312ms step_avg:59.61ms
step:1734/1840 train_time:103401ms step_avg:59.63ms
step:1735/1840 train_time:103489ms step_avg:59.65ms
step:1736/1840 train_time:103577ms step_avg:59.66ms
step:1737/1840 train_time:103663ms step_avg:59.68ms
step:1738/1840 train_time:103751ms step_avg:59.70ms
step:1739/1840 train_time:103838ms step_avg:59.71ms
step:1740/1840 train_time:103928ms step_avg:59.73ms
step:1741/1840 train_time:104014ms step_avg:59.74ms
step:1742/1840 train_time:104103ms step_avg:59.76ms
step:1743/1840 train_time:104191ms step_avg:59.78ms
step:1744/1840 train_time:104278ms step_avg:59.79ms
step:1745/1840 train_time:104365ms step_avg:59.81ms
step:1746/1840 train_time:104454ms step_avg:59.82ms
step:1747/1840 train_time:104539ms step_avg:59.84ms
step:1748/1840 train_time:104629ms step_avg:59.86ms
step:1749/1840 train_time:104714ms step_avg:59.87ms
step:1750/1840 train_time:104803ms step_avg:59.89ms
step:1750/1840 val_loss:3.3035 train_time:104904ms step_avg:59.95ms
step:1751/1840 train_time:104926ms step_avg:59.92ms
step:1752/1840 train_time:104981ms step_avg:59.92ms
step:1753/1840 train_time:105070ms step_avg:59.94ms
step:1754/1840 train_time:105160ms step_avg:59.95ms
step:1755/1840 train_time:105248ms step_avg:59.97ms
step:1756/1840 train_time:105337ms step_avg:59.99ms
step:1757/1840 train_time:105422ms step_avg:60.00ms
step:1758/1840 train_time:105510ms step_avg:60.02ms
step:1759/1840 train_time:105595ms step_avg:60.03ms
step:1760/1840 train_time:105682ms step_avg:60.05ms
step:1761/1840 train_time:105768ms step_avg:60.06ms
step:1762/1840 train_time:105857ms step_avg:60.08ms
step:1763/1840 train_time:105945ms step_avg:60.09ms
step:1764/1840 train_time:106037ms step_avg:60.11ms
step:1765/1840 train_time:106125ms step_avg:60.13ms
step:1766/1840 train_time:106216ms step_avg:60.14ms
step:1767/1840 train_time:106302ms step_avg:60.16ms
step:1768/1840 train_time:106391ms step_avg:60.18ms
step:1769/1840 train_time:106477ms step_avg:60.19ms
step:1770/1840 train_time:106564ms step_avg:60.21ms
step:1771/1840 train_time:106650ms step_avg:60.22ms
step:1772/1840 train_time:106737ms step_avg:60.24ms
step:1773/1840 train_time:106823ms step_avg:60.25ms
step:1774/1840 train_time:106913ms step_avg:60.27ms
step:1775/1840 train_time:106999ms step_avg:60.28ms
step:1776/1840 train_time:107090ms step_avg:60.30ms
step:1777/1840 train_time:107177ms step_avg:60.31ms
step:1778/1840 train_time:107264ms step_avg:60.33ms
step:1779/1840 train_time:107351ms step_avg:60.34ms
step:1780/1840 train_time:107438ms step_avg:60.36ms
step:1781/1840 train_time:107524ms step_avg:60.37ms
step:1782/1840 train_time:107613ms step_avg:60.39ms
step:1783/1840 train_time:107699ms step_avg:60.40ms
step:1784/1840 train_time:107787ms step_avg:60.42ms
step:1785/1840 train_time:107873ms step_avg:60.43ms
step:1786/1840 train_time:107963ms step_avg:60.45ms
step:1787/1840 train_time:108053ms step_avg:60.47ms
step:1788/1840 train_time:108141ms step_avg:60.48ms
step:1789/1840 train_time:108228ms step_avg:60.50ms
step:1790/1840 train_time:108317ms step_avg:60.51ms
step:1791/1840 train_time:108402ms step_avg:60.53ms
step:1792/1840 train_time:108492ms step_avg:60.54ms
step:1793/1840 train_time:108577ms step_avg:60.56ms
step:1794/1840 train_time:108664ms step_avg:60.57ms
step:1795/1840 train_time:108750ms step_avg:60.59ms
step:1796/1840 train_time:108838ms step_avg:60.60ms
step:1797/1840 train_time:108925ms step_avg:60.62ms
step:1798/1840 train_time:109017ms step_avg:60.63ms
step:1799/1840 train_time:109103ms step_avg:60.65ms
step:1800/1840 train_time:109192ms step_avg:60.66ms
step:1801/1840 train_time:109279ms step_avg:60.68ms
step:1802/1840 train_time:109366ms step_avg:60.69ms
step:1803/1840 train_time:109454ms step_avg:60.71ms
step:1804/1840 train_time:109542ms step_avg:60.72ms
step:1805/1840 train_time:109630ms step_avg:60.74ms
step:1806/1840 train_time:109718ms step_avg:60.75ms
step:1807/1840 train_time:109803ms step_avg:60.77ms
step:1808/1840 train_time:109894ms step_avg:60.78ms
step:1809/1840 train_time:109981ms step_avg:60.80ms
step:1810/1840 train_time:110069ms step_avg:60.81ms
step:1811/1840 train_time:110157ms step_avg:60.83ms
step:1812/1840 train_time:110245ms step_avg:60.84ms
step:1813/1840 train_time:110331ms step_avg:60.86ms
step:1814/1840 train_time:110420ms step_avg:60.87ms
step:1815/1840 train_time:110506ms step_avg:60.88ms
step:1816/1840 train_time:110595ms step_avg:60.90ms
step:1817/1840 train_time:110680ms step_avg:60.91ms
step:1818/1840 train_time:110770ms step_avg:60.93ms
step:1819/1840 train_time:110857ms step_avg:60.94ms
step:1820/1840 train_time:110946ms step_avg:60.96ms
step:1821/1840 train_time:111033ms step_avg:60.97ms
step:1822/1840 train_time:111122ms step_avg:60.99ms
step:1823/1840 train_time:111208ms step_avg:61.00ms
step:1824/1840 train_time:111299ms step_avg:61.02ms
step:1825/1840 train_time:111386ms step_avg:61.03ms
step:1826/1840 train_time:111476ms step_avg:61.05ms
step:1827/1840 train_time:111563ms step_avg:61.06ms
step:1828/1840 train_time:111651ms step_avg:61.08ms
step:1829/1840 train_time:111737ms step_avg:61.09ms
step:1830/1840 train_time:111827ms step_avg:61.11ms
step:1831/1840 train_time:111914ms step_avg:61.12ms
step:1832/1840 train_time:112002ms step_avg:61.14ms
step:1833/1840 train_time:112089ms step_avg:61.15ms
step:1834/1840 train_time:112177ms step_avg:61.17ms
step:1835/1840 train_time:112263ms step_avg:61.18ms
step:1836/1840 train_time:112353ms step_avg:61.19ms
step:1837/1840 train_time:112438ms step_avg:61.21ms
step:1838/1840 train_time:112528ms step_avg:61.22ms
step:1839/1840 train_time:112616ms step_avg:61.24ms
step:1840/1840 train_time:112704ms step_avg:61.25ms
step:1840/1840 val_loss:3.2785 train_time:112805ms step_avg:61.31ms
peak memory allocated: 28507 MiB reserved: 44038 MiB

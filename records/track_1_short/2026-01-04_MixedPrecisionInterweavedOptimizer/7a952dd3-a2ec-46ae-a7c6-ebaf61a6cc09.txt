import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from collections import defaultdict
from itertools import accumulate
from pathlib import Path
import gc

os.environ["PYTORCH_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device=f"cuda:{os.environ['LOCAL_RANK']}", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng

@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor, split_baddbmm: bool = False):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    # Select batched vs unbatched
    if split_baddbmm:
        BX_matmul = torch.bmm if X.ndim > 2 else torch.mm
    else:
        aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A

        # Referencing X twice causes pytorch to make a defensive copy,
        # resulting in a cudaMemcpyAsync in baddbmm.
        # For large matrices (i.e., the mlp weights), it's faster to split
        # the operation into two kernels to avoid this.
        if split_baddbmm:
            BX_matmul(B, X, out=C)  # C = B @ X
            C.add_(X, alpha=a)      # C = C + a*X  (in-place, X only read)
        else:
            aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X

        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X


# -----------------------------------------------------------------------------
# Compiled helpers for NorMuon by @chrisjmccormick

@torch.compile(dynamic=False, fullgraph=True)
def cautious_wd_and_update_inplace(p, mantissa, grad, wd_tensor, lr_tensor):
    """
    Cautious weight decay + parameter update. wd_tensor and lr_tensor are 0-D CPU tensors.
    Mantissa is tracked to enable higher precision updates on bfloat16 parameters.
    bfloat16 format: 1 sign bit + 8 exponent bits + 7 mantissa bits = 16 bits total
    float32 format: 1 sign bit + 8 exponent bits + 23 mantissa bits = 32 bits total
    """
    assert p.dtype == mantissa.dtype == torch.uint16
    grad = grad.float()
    wd_factor = wd_tensor.to(torch.float32)
    lr_factor = lr_tensor.to(torch.float32)
    p_precise_raw = (p.to(torch.uint32) << 16) | mantissa.to(torch.uint32)
    p_precise = p_precise_raw.view(torch.float32)
    mask = (grad * p_precise) >= 0
    p_precise.copy_(p_precise - (p_precise * mask * wd_factor * lr_factor) - (grad * lr_factor))
    p.copy_((p_precise_raw >> 16).to(torch.uint16))
    mantissa.copy_(p_precise_raw.to(torch.uint16))

@torch.compile(dynamic=False, fullgraph=True)
def apply_normuon_variance_reduction(v_chunk, second_momentum_buffer, beta2, red_dim):
    """NorMuon variance reduction. Algebraically fuses the normalization steps to minimize memory ops."""
    v_mean = v_chunk.float().square().mean(dim=red_dim, keepdim=True)
    red_dim_size = v_chunk.size(red_dim)
    v_norm_sq = v_mean.sum(dim=(-2, -1), keepdim=True).mul_(red_dim_size)
    v_norm = v_norm_sq.sqrt_()
    second_momentum_buffer.lerp_(v_mean.to(dtype=second_momentum_buffer.dtype), 1 - beta2)
    step_size = second_momentum_buffer.clamp_min(1e-10).rsqrt_()
    scaled_sq_sum = (v_mean * red_dim_size) * step_size.float().square()
    v_norm_new = scaled_sq_sum.sum(dim=(-2, -1), keepdim=True).sqrt_()
    final_scale = step_size * (v_norm / v_norm_new.clamp_min_(1e-10))
    return v_chunk.mul_(final_scale.type_as(v_chunk))


# -----------------------------------------------------------------------------
# NorMuon optimizer

class NorMuon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).

    Differences from standard Muon:
    - Newton-Shulz is replaced with Polar Express for the orthogonalization step
    - NorMuon adds a low-rank variance estimator similar to Adafactor. https://arxiv.org/pdf/2510.05491
    - small 1D parameters handled here instead of in Adam
    - Cautious weight decay, a gated version of decoupled weight decay
    - Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as
    needed on the forward pass. This enables attn and mlp weights to be contained within the same
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn.
    The scheduling is:
        1. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        2. reduce scatter attn/mlp round 2 (16 mlp params)
        3. wait on step 1, then compute update of 1 and schedule all gather
        4. wait on step 2, then compute update of 2 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        5. wait for each all gather to complete and update params
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, beta2=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, beta2=beta2)
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def reset(self):
        # expose a reset for clearing buffers
        for group in self.param_groups:
            if "momentum_buffer" in group:
                group["momentum_buffer"].zero_()
                group["mantissa"].zero_()
                group["second_momentum_buffer"].zero_()

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per module.
        """
        groups = defaultdict(list)
        for param in params:
            groups[param.label].append(param)

        param_groups = []
        for module_name, group_params in groups.items():
            chunk_size = (len(group_params) + self.world_size - 1) // self.world_size
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))

        return param_groups

    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn
        and mlp params when a param group is split across GPUs.
        """
        params_list = list(params)
        module_group_order = ['attn', 'mlp']
        group_sizes = [16, 16]  # 10 attn + 6 mlp, then 16 mlp
        params_list.sort(key=lambda x: module_group_order.index(x.label))

        idx = 0
        assert len(params_list) == sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            chunk_size = (size + self.world_size - 1) // self.world_size
            group_params = params_list[idx: idx + size]
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))
            idx += size

        return param_groups

    def step(self):
        self.step_p1()
        self.step_p2()
        self.step_p3()
        
    @torch.no_grad()
    def step_p1(self):
        """
        Part 1: Launch distributed reduce_scatter operations for parameter groups.
        """
        rank = dist.get_rank()
        self.group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            stacked_grads = torch.empty(
                (padded_num_params, *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device
            )
            for i, p in enumerate(params):
                stacked_grads[i].copy_(p.grad, non_blocking=True)
            if len(params) < padded_num_params:
                stacked_grads[len(params):].zero_()

            grad_chunk = torch.empty_like(stacked_grads[:chunk_size])

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            self.group_infos.append(dict(grad_chunk=grad_chunk, reduce_future=reduce_future))
    
    @torch.no_grad()
    def step_p2(self):
        """
        Part 2: Compute gradient updates and launch all_gather operations
        Wait for all gather to complete for all parameter groups except final one
        """
        # Efficient distributed step by @YouJiacheng, @KonstantinWilleke, @alexrgilbert,
        # @adricarda, @tuttyfrutyee, @vdlad, @ryanyang0, @vagrawal, @varunneal, @chrisjmccormick
        rank = dist.get_rank()
        group_infos = self.group_infos
        
        self.all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = group["params"]
            grad_chunk = info["grad_chunk"].float()
            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            start_idx = rank * chunk_size
            module_idx = start_idx if start_idx < len(params) else 0

            num_params = min(chunk_size, max(0, len(params) - start_idx))  # num params for this rank

            if "momentum_buffer" not in group:
                group["momentum_buffer"]  = torch.zeros_like(grad_chunk[:num_params], dtype=torch.float32)
                
            momentum_buffer = group["momentum_buffer"]
            # Apply momentum update to the persistent momentum buffer in-place
            momentum_buffer.lerp_(grad_chunk[:num_params], 1 - group["momentum"])
            updated_grads = grad_chunk[:num_params].lerp_(momentum_buffer, group["momentum"])

            grad_shape = updated_grads.shape
            if params[module_idx].label == 'attn':
                for p in params[module_idx:module_idx + num_params]:
                    assert p.label == 'attn'
                updated_grads = updated_grads.view(4 * grad_shape[0], grad_shape[1] // 4, grad_shape[2])

            ref_param = params[module_idx]
            param_shape = ref_param.shape

            if "second_momentum_buffer" not in group:
                group["second_momentum_buffer"] = (torch.zeros_like(updated_grads[..., :, :1], dtype=torch.float32)
                    if param_shape[-2] >= param_shape[-1] else torch.zeros_like(updated_grads[..., :1, :])
                )
            second_momentum_buffer = group["second_momentum_buffer"]

            if "param_lr_cpu" not in group:
                # Define multipliers for ALL params in this group (global, not per-shard)
                lr_mults = []
                wd_mults = []
                for p in params:
                    # Increase learning rate for modules with larger inputs than outputs.
                    # This shape check also assumes rows=input, columns=output, so take care
                    # when changing memory layouts. @chrisjmccormick
                    shape = p.shape
                    if len(shape) >= 2:
                        shape_mult = max(1.0, shape[-2] / shape[-1]) ** 0.5
                    else:
                        shape_mult = 1.0
                    lr_mults.append(shape_mult * getattr(p, "lr_mul", 1.0))
                    wd_mults.append(getattr(p, "wd_mul", 1.0))
                # Define as cpu tensors to enable Inductor constant folding
                group["param_lr_cpu"] = torch.tensor(lr_mults, dtype=torch.float32, device="cpu")
                group["param_wd_cpu"] = torch.tensor(wd_mults, dtype=torch.float32, device="cpu")

            eff_lr_all = group["param_lr_cpu"] * group["lr"]
            eff_wd_all = group["param_wd_cpu"] * group["weight_decay"] * group["lr"]

            # Slice the portion corresponding to this rank's shard
            eff_lr_cpu = eff_lr_all[module_idx:module_idx + num_params]
            eff_wd_cpu = eff_wd_all[module_idx:module_idx + num_params]

            # Compute zeropower for the entire chunk in a single, batched call.
            if num_params == 0:
                v_chunk = updated_grads
            else:
                v_chunk = polar_express(updated_grads, split_baddbmm=(ref_param.label == 'mlp'))

            # Note that the head orientation in O is transposed relative to QKV, so red_dim
            # is 'incorrect' for O. However, correcting this showed no improvement. @chrisjmccormick
            red_dim = -1 if param_shape[-2] >= param_shape[-1] else -2

            v_chunk = apply_normuon_variance_reduction(
                v_chunk, second_momentum_buffer, group["beta2"], red_dim
            )

            v_chunk = v_chunk.view(grad_shape)

            # # "Cautious" weight decay (https://arxiv.org/abs/2510.12402)
            updated_params = torch.empty_like(grad_chunk, dtype=torch.bfloat16)
            if num_params > 0:
                # Work on a stacked copy to avoid touching original params
                param_chunk = torch.stack(params[module_idx:module_idx + num_params])

                if "mantissa" not in group:
                    group["mantissa"] = torch.zeros_like(param_chunk, dtype=torch.uint16)
                mantissa = group["mantissa"]

                for local_idx in range(num_params):
                    cautious_wd_and_update_inplace(
                        param_chunk[local_idx].view(torch.uint16),
                        mantissa[local_idx],
                        v_chunk[local_idx],
                        eff_wd_cpu[local_idx],
                        eff_lr_cpu[local_idx]
                    )
            else:
                param_chunk = torch.zeros_like(v_chunk)

            updated_params[:num_params].copy_(param_chunk)
            if num_params < chunk_size:
                updated_params[num_params:].zero_()

            stacked_params = torch.empty(
                (padded_num_params, *param_shape),
                dtype=updated_params.dtype,
                device=updated_params.device,
            )

            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_params, async_op=True
            ).get_future()

            self.all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete for all except final and copy results back into original parameter tensors.
        for info in self.all_gather_infos[:-1]:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)

    @torch.no_grad()
    def step_p3(self):
        """
        Part 3: Wait for final all gather to complete and copy results back into original parameter tensors
        """
        info = self.all_gather_infos[-1]
        info["gather_future"].wait()
        stacked_params = info["stacked_params"]
        orig_params = info["orig_params"]

        unstacked_params = torch.unbind(stacked_params)
        for i, p in enumerate(orig_params):
            p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, label_order: list[str], betas: list[list[float]], lr: float = 1e-3, eps: float = 1e-8, weight_decay: float = 0.01):
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        defaults = dict(lr=lr, eps=eps, weight_decay=weight_decay)
        params = list(params)
        # Group by label, with explicit ordering for execution control.
        params_by_label = defaultdict(list)
        for p in params:
            params_by_label[getattr(p, 'label', None)].append(p)
        param_groups = []
        for idx, label in enumerate(label_order):
            if label in params_by_label:
                param_groups.append(dict(params=params_by_label[label], betas=betas[idx]))
        # include any unlabeled params at the end (processed last)
        if None in params_by_label:
            param_groups.append(dict(params=params_by_label[None]))
        super().__init__(param_groups, defaults)
        # init state: small params (numel < 1024) use full-sized state, others use sharded
        for p in params:
            chunk = p if p.numel() < 1024 else p[:p.size(0) // self.world_size]
            exp_avg = torch.zeros_like(chunk, dtype=torch.float32, device=p.device)
            self.state[p] = dict(step=0, exp_avg=exp_avg, exp_avg_sq=torch.zeros_like(exp_avg))

        # tag the final param for optimizer pipelining, run all gather after muon copy
        param_groups[-1]['params'][-1].is_final_param = True
        
        # DistributedAdam implementation by @vagrawal, @akash5474
        self.should_sync = False
        self._reduce_scatter_hooks = []
        self._reduce_scatter_futures = {}
        # 0-D CPU tensors to avoid recompilation in _update_step
        self._step_size_t = torch.tensor(0.0, dtype=torch.float32, device="cpu")
        self._eff_wd_t = torch.tensor(0.0, dtype=torch.float32, device="cpu")
        self.register_backward_hooks()

    def register_backward_hooks(self):
        for group in self.param_groups:
            for param in group["params"]:
                self._reduce_scatter_hooks.append(param.register_post_accumulate_grad_hook(self._sync_gradient))

    def load_state_dict(self, state_dict):
        """Override to preserve optimizer state dtypes (avoid BFloat16->Float32 cast that causes recompilation)."""
        # Save original state dtypes before loading
        original_dtypes = {}
        for p, s in self.state.items():
            original_dtypes[p] = {k: v.dtype for k, v in s.items() if isinstance(v, torch.Tensor)}
        
        # Call parent load_state_dict (which may cast dtypes to match param dtype)
        super().load_state_dict(state_dict)
        
        # Restore original dtypes
        for p, s in self.state.items():
            if p in original_dtypes:
                for k, v in s.items():
                    if isinstance(v, torch.Tensor) and k in original_dtypes[p]:
                        if v.dtype != original_dtypes[p][k]:
                            s[k] = v.to(original_dtypes[p][k])

    @torch.no_grad()
    def _sync_gradient(self, param):
        if not self.should_sync:
            return

        grad = param.grad
        if param.numel() < 1024:
            # Small params: use all_reduce (no scatter/gather needed)
            self._reduce_scatter_futures[param] = (
                dist.all_reduce(grad, op=dist.ReduceOp.AVG, async_op=True).get_future(),
                grad
            )
        else:
            rank_size = grad.shape[0] // self.world_size
            if grad is not None:
                grad_slice = torch.empty_like(grad[:rank_size])
                self._reduce_scatter_futures[param] = (
                    dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future(),
                    grad_slice
                )

    def copy_lm_to_embed(self):
        # run at 2/3 of training
        lm_head = self.param_groups[0]['params'][0]
        embed = self.param_groups[-2]['params'][0]
        lm_head_state = self.state[lm_head]
        embed_state = self.state[embed]
        embed_state['step'] = lm_head_state['step']
        embed_state['exp_avg'] = lm_head_state['exp_avg'].clone()
        embed_state['exp_avg_sq'] = lm_head_state['exp_avg_sq'].clone()
        embed.data.copy_(lm_head.data)

    @staticmethod
    @torch.compile(dynamic=False, fullgraph=True)
    def _update_step(p_slice, g_slice, exp_avg, exp_avg_sq, beta1, beta2, eps, step_size_t, eff_wd_t):
        """Compiled Adam update step. step_size_t and eff_wd_t are 0-D CPU tensors to avoid recompilation."""
        exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)  # exp_avg = beta1 * exp_avg + (1 - beta1) * g_slice
        exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)  # exp_avg_sq = beta2 * exp_avg_sq + (1 - beta2) * g_slice^2
        # compute step
        update = exp_avg.div(exp_avg_sq.sqrt().add_(eps)).mul_(step_size_t)  # update = (exp_avg / (sqrt(exp_avg_sq) + eps)) * step_size
        # cautious weight decay
        mask = (update * p_slice) > 0
        update.addcmul_(p_slice, mask, value=eff_wd_t)  # update += eff_wd_t * p_slice * mask
        p_slice.add_(other=update, alpha=-1.0)  # p_slice -= update

    @torch.no_grad()
    def step(self, muon_opt):
        muon_opt.step_p1()
        rank = dist.get_rank()
        all_gather_futures: list[torch.Future] = []

        last_param = None
        last_p_slice = None
        for group in self.param_groups:      
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            for param in group['params']:
                if param not in self._reduce_scatter_futures:
                    continue

                fut, g_slice = self._reduce_scatter_futures[param]
                fut.wait()

                is_small = param.numel() < 1024
                if is_small:
                    # Small params: g_slice is actually full grad, p_slice is full param
                    p_slice = param
                else:
                    rank_size = param.shape[0] // self.world_size
                    p_slice = param[rank * rank_size:(rank + 1) * rank_size]

                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]
                state["step"] += 1
                t = state["step"]
                
                # Pre-compute changing values as 0-D CPU tensors to avoid recompilation.
                # `.fill_(value)` is the same as "= value", but doesn't change the tensor object.
                bias1, bias2 = 1 - beta1 ** t, 1 - beta2 ** t
                self._step_size_t.fill_(lr * (bias2 ** 0.5 / bias1))
                self._eff_wd_t.fill_(lr * lr * wd * getattr(param, "wd_mul", 1.0)) # `lr` included twice to serve as weight decay schedule.

                DistAdam._update_step(p_slice, g_slice, state["exp_avg"], state["exp_avg_sq"],
                                      beta1, beta2, eps, self._step_size_t, self._eff_wd_t)

                if not is_small:
                    if getattr(param, "is_final_param", False):
                        last_param = param
                        last_p_slice = p_slice
                    else:
                        all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())
        self._reduce_scatter_futures.clear()

        muon_opt.step_p2()
        torch.futures.collect_all(all_gather_futures).wait()

        if last_param is not None:
            last_all_gather_future = dist.all_gather_into_tensor(last_param, last_p_slice, async_op=True).get_future()
        muon_opt.step_p3()
        torch.futures.collect_all([last_all_gather_future]).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        with torch.no_grad():
            self.weight.zero_()  # @Grad62304977 and others

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))


# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float
    key_offset: bool
    attn_gate_w: torch.Tensor
    ve_gate_w: torch.Tensor

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = self.dim ** -0.5
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKVO weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # Simplified layout by @chrisjmccormick
        self.qkvo_w = nn.Parameter(torch.empty(self.dim * 4, self.hdim, dtype=torch.bfloat16))
        # label all modules for explicit optimizer grouping
        self.qkvo_w.label = 'attn'

        with torch.no_grad():
            self.qkvo_w[:self.dim * 3].uniform_(-bound, bound)  # init QKV weights
            self.qkvo_w[self.dim * 3:].zero_()  # init O weights to zero

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas, key_offset = attn_args.ve, attn_args.sa_lambdas, attn_args.key_offset
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size
        # sparse gated attention to enable context based no-op by @classiclarryd
        # only include gates on layers with value embeds used on forward pass
        attn_gate_w, ve_gate_w = attn_args.attn_gate_w, attn_args.ve_gate_w

        q, k, v = F.linear(x, sa_lambdas[0] * self.qkvo_w[:self.dim * 3].type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if key_offset:
            # shift keys forward for the stationary head dims. Enables 1-layer induction.
            k[:, 1:, :, self.head_dim // 4:self.head_dim // 2] = k[:, :-1, :, self.head_dim // 4:self.head_dim // 2]
            k[:, 1:, :, 3 * self.head_dim // 4:] = k[:, :-1, :, 3 * self.head_dim // 4:]
        if ve is not None:
            ve_gate_out = 2 * torch.sigmoid(F.linear(x[..., :12], ve_gate_w)).view(B, T, self.num_heads, 1)
            v = v + ve_gate_out * ve.view_as(v) # @ KoszarskyB & @Grad62304977

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens,
                                                        max_seqlen_q=max_len, max_seqlen_k=max_len,
                                                        causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(F.linear(x[..., :12], attn_gate_w)).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, sa_lambdas[1] * self.qkvo_w[self.dim * 3:].type_as(y))  # sa_lambdas[1] pre-multiplied to O @shenberg
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # Transposed layout to match attention weights
        self.c_fc = nn.Parameter(torch.empty(hdim, dim, dtype=torch.bfloat16))
        self.c_proj = nn.Parameter(torch.empty(hdim, dim, dtype=torch.bfloat16))
        # label all modules for explicit optimizer grouping
        self.c_fc.label = 'mlp'
        self.c_proj.label = 'mlp'
        self.c_proj.lr_mul = 2.

        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.T.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.6 (the 7th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads, layer_idx) if layer_idx != 6 else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim)

    def forward(self, x: Tensor, attn_args: AttnArgs):
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

@dataclass
class ForwardScheduleConfig:
    mtp_weights: torch.Tensor
    ws_short: int
    ws_long: int

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        self.num_layers = num_layers
        vocab_size = next_multiple_of_n(vocab_size, n=128)

        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.label = 'smear_gate'
        self.smear_gate.weight.lr_mul = 0.01
        self.smear_gate.weight.wd_mul = 0.0

        self.skip_gate = CastedLinear(12, 1)
        self.skip_gate.weight.label = 'skip_gate'
        self.skip_gate.weight.lr_mul = 0.05
        self.skip_gate.weight.wd_mul = 0.0

        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        for embed in self.value_embeds:
            nn.init.zeros_(embed.weight)
        for ve in self.value_embeds:
            ve.weight.label = 'value_embed'
        
        # parameter banks for attention and value embedding gate weights
        self.attn_gate_bank = nn.Parameter(torch.zeros(10, num_heads, 12)) # 10 layers
        self.attn_gate_bank.label = 'attn_gate_bank'
        self.ve_gate_bank = nn.Parameter(torch.zeros(5, num_heads, 12)) # 5 layers
        self.ve_gate_bank.label = 've_gate_bank'

        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)

        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=100/448, w_s=1.6/448, grad_s=0.75/448)
        nn.init.normal_(self.lm_head.weight, mean=0, std=0.005)
        self.lm_head.weight.label = 'lm_head'

        self.embed = nn.Embedding(vocab_size, model_dim)
        self.embed.weight.label = 'embed'

        # x0_lambdas separated out for different optimizer treatment (no beta smoothing)
        self.x0_lambdas = nn.Parameter(torch.zeros(num_layers))
        self.x0_lambdas.label = 'x0_lambdas'
        self.x0_lambdas.lr_mul = 5.0
        self.x0_lambdas.wd_mul = 0.0

        pad = (-num_layers * 3 - 3) % dist.get_world_size()  # updated: 3*num_layers instead of 4*
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    1.1 * torch.ones(num_layers),  # resid lambdas. 1.1 init such that layer i weight is i^(num_layers-i).
                    *[torch.tensor([0.5, 1.0]) for _ in range(num_layers)],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    -1.5 * torch.ones(1),  # skip_lambda -> σ(-1.5) ≈ 0.18
                    torch.ones(pad),
                ]
            )
        )

        self.scalars.label = 'scalars'
        # set learning rates
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
            param.wd_mul = 5.
        for param in self.embed.parameters():
            param.wd_mul = 150.
        for param in self.lm_head.parameters():
            param.wd_mul = 150.
        self.scalars.lr_mul = 5.0
        self.scalars.wd_mul = 0.0

        self.split_embed = False

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, schedule_cfg: ForwardScheduleConfig):
        assert input_seq.ndim == 1

        # unpack schedule_cfg
        mtp_weights, ws_short, ws_long = schedule_cfg.mtp_weights, schedule_cfg.ws_short, schedule_cfg.ws_long

        # set configs
        skip_connections = []
        skip_in = [3] # long attention window on layer 3
        skip_out = [6] # no attn op on layer 6
        x_backout = None
        backout_layer = 7

        # set lambdas
        resid_lambdas = self.scalars[: 1 * self.num_layers]
        x0_lambdas = self.x0_lambdas
        sa_lambdas = self.scalars[1 * self.num_layers: 3 * self.num_layers].view(-1, 2)
        smear_lambda = self.scalars[3 * self.num_layers]
        backout_lambda = self.scalars[3 * self.num_layers+1]
        skip_lambda = self.scalars[3 * self.num_layers+2]

        # set block masks and key shift
        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == self.num_layers
        key_offset = [b==long_bm for b in bm_sizes] # apply partial key offset to long windows

        # weight-tied: use lm_head.weight for embedding lookup (or separate embed after split)
        if self.split_embed:
            x = self.embed(input_seq)
        else:
            x = F.embedding(input_seq, self.lm_head.weight)
        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [ve[1], ve[2]] + [None] * (self.num_layers - 5) + [ve[0], ve[1], ve[2]]
        assert len(ve) == self.num_layers

        # smear token embed forward 1 position @classiclarryd
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # unbind gate banks to avoid select_backwards kernel
        ag = [w.bfloat16() for w in self.attn_gate_bank.unbind(0)] 
        veg = [w.bfloat16() for w in self.ve_gate_bank.unbind(0)]
        attn_gates = ag[:6] + [None] + ag[6:]
        ve_gates = [veg[0], veg[1]] + [None] * (self.num_layers - 5) + [veg[2], veg[3], veg[4]]
        assert len(attn_gates) == self.num_layers
        assert len(ve_gates) == self.num_layers

        for i in range(self.num_layers):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale,
                key_offset=key_offset[i],
                attn_gate_w=attn_gates[i],
                ve_gate_w=ve_gates[i]
            )
            if i in skip_out:
                skip_gate_out = torch.sigmoid(skip_lambda) * 2 * torch.sigmoid(self.skip_gate(x0[..., :self.skip_gate.weight.size(-1)]))
                x = x + skip_gate_out * skip_connections.pop()
            if i == 0:
                x = (resid_lambdas[0] + x0_lambdas[0]) * x
            else:
                x = resid_lambdas[i] * x + x0_lambdas[i] * x0
            x = self.blocks[i](x, attn_args)
            if i in skip_in:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 7 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15
        # @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1). @classiclarryd updated to 23*sigmoid((logits+5)/7.5)
        logits = 23 * torch.sigmoid((logits + 5) / 7.5)
        logits_for_loss = logits.float() if not self.training else logits

        n_predict = mtp_weights.size(0) if mtp_weights is not None else 1
        if self.training and n_predict > 1:
            # Multi-token prediction: take loss of the weighted average of next n_predict tokens
            logits_flat = logits_for_loss.view(-1, logits_for_loss.size(-1))
            idx = F.pad(target_seq, (0, n_predict - 1)).unfold(0, n_predict, 1)  # [T, n_predict] of shifted targets
            target_logits = logits_flat.gather(1, idx)
            cross_entropy = torch.logsumexp(logits_flat, dim=-1).unsqueeze(1) - target_logits
            for k in range(1, n_predict):  # zero out preds past end of sequence
                cross_entropy[-k:, k] = 0
            loss = (cross_entropy * mtp_weights).sum()
        elif self.training:
            loss = F.cross_entropy(logits_for_loss.view(-1, logits_for_loss.size(-1)), target_seq, reduction="sum")
        else:
            loss = F.cross_entropy(logits_for_loss.view(-1, logits_for_loss.size(-1)), target_seq, reduction="mean")
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()

    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        # Cast to int32 on CPU before transfer to avoid dtype conversion during .to()
        _inputs = _inputs.to(dtype=torch.int32)
        _targets = _targets.to(dtype=torch.int64)
        _cum_lengths = _cum_lengths.to(dtype=torch.int32)

        new_params = yield (
            _inputs.to(device="cuda", non_blocking=True),
            _targets.to(device="cuda", non_blocking=True),
            _cum_lengths.to(device="cuda", non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * new_grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens // new_grad_accum_steps
            max_seq_len = new_max_seq_len

# -----------------------------------------------------------------------------
# Training Management
 
def get_bs(step: int):
    if step >= args.num_scheduled_iterations:
        return args.train_bs_extension
    x = step / args.num_scheduled_iterations
    bs_idx = int(len(args.train_bs_schedule) * x)
    return args.train_bs_schedule[bs_idx]

def get_ws(step: int):
    # set short window size to half of long window size
    # Higher ws on "extension" steps
    if step >= args.num_scheduled_iterations:
        return args.ws_final // 2, args.ws_final
    x = step / args.num_scheduled_iterations
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

# learning rate schedule: tied to batch size schedule, with cooldown at the end.
def get_lr(step: int):
    if step > args.num_scheduled_iterations:
        return 0.1
    lr_max = 1.0
    x = step / args.num_scheduled_iterations
    if x > 1/3:
       lr_max = 1.52  # (16/8)**0.6
    if x > 2/3:
        lr_max = 1.73  # (24/8)**0.5
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = lr_max * w + (1 - w) * 0.1
        return lr
    return lr_max

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

class TrainingManager():
    """
    Manages three optimizers for Adam embed/lm_head, Adam scalars, and Muon weight matrices.
    Notable Features:
        1. Scalars are given higher momentum terms to smooth learning @ChrisJMcCormick
        2. Scalar weights are temporarily frozen during batch size or window size updates @ChrisJMcCormick
        3. Adam optimizers are only stepped on odd steps @classiclarryd
        4. Adam optimizers have hooks to start gradient communication during backwards pass @akash5474
        5. Muon has a linear momentum warmup and cooldown schedule
        6. Learning rates follow a linear decay schedule
        7. Embed/lm_head weights and optimizer state splits at 2/3 of training @classiclarryd

    Manages model architecture, data, and target that changes during training
    Notable Features:
        1. Multi Token Prediction schedule of [1, 0.5, 0.25->0] -> [1, 0.5->0] -> [1] @varunneal
        2. Sliding Attention window schedule of [1,3] -> [3,7] -> [5,11] -> [6,13]
        3. YaRN updates to RoPE on window changes
        4. Split embed and lm head at 2/3 of training
        5. Batch size schedule of 8 -> 16 -> 24
        6. Post training extension of long windows from 13 to 20
    """
    def __init__(self, model):
        self.mtp_weights_schedule = self._build_mtp_schedule()
        self.model = model
        adam_betas = {
            'lm_head': [0.5, 0.95],
            'smear_gate': [0.9, 0.99],
            'attn_gate_bank': [0.9, 0.99],
            've_gate_bank': [0.9, 0.99],
            'skip_gate': [0.9, 0.99],
            'x0_lambdas': [0.65, 0.95],
            'scalars': [0.9, 0.99],
            'embed': [0.5, 0.95],
            'value_embed': [0.75, 0.95]
        }
        adam_labels = list(adam_betas.keys())
        adam_beta_values = list(adam_betas.values())
        muon_labels = ['attn', 'mlp']
        adam_params = [p for p in model.parameters() if getattr(p, 'label', None) in adam_labels]
        muon_params = [p for p in model.parameters() if getattr(p, 'label', None) in muon_labels]
        assert set(getattr(p, 'label', None) for p in model.parameters()) == set(adam_labels + muon_labels), "All params must have label"

        self.adam_opt = DistAdam(adam_params, adam_labels, adam_beta_values, lr=0.008, eps=1e-10, weight_decay=0.005)
        self.muon_opt = NorMuon(muon_params, lr=0.023, momentum=0.95, beta2=0.95, weight_decay=1.2)
        self.optimizers = [self.adam_opt, self.muon_opt]

        # split after odd number step
        self.split_step = math.ceil(args.split_embed_frac * args.num_scheduled_iterations) | 1

        # set defaults
        for opt in self.optimizers:
            opt.odd_step_only = False 
            opt.should_sync = True
            for group in opt.param_groups:
                group["initial_lr"] = group["lr"]

        # on even steps, only step Muon params
        self.adam_opt.odd_step_only = True

        self.reset()

    def _build_mtp_schedule(self):
        # Precompute MTP weights for all steps to avoid tensor allocation during training
        # Schedule: [1, 0.5, 0.25->0] -> [1, 0.5->0] -> [1]
        mtp_weights_schedule = []
        for s in range(args.num_iterations + 1):
            x = s / args.num_scheduled_iterations
            if x < 1/3:
                w = [1.0, 0.5, 0.25 * (1 - 3*x)]
            elif x < 2/3:
                w = [1.0, 0.5 * (1 - (3*x - 1))]
            else:
                w = [1.0]
            mtp_weights_schedule.append(torch.tensor(w, device=device))
        return mtp_weights_schedule

    def apply_final_ws_ext(self):
        self.ws_long = args.ws_validate_post_yarn_ext

    def get_forward_args(self):
        return ForwardScheduleConfig(
            mtp_weights = self.mtp_weights,
            ws_short = self.ws_short,
            ws_long = self.ws_long
        )
    
    def _is_active_step(self, opt, step: int):
        return (opt.odd_step_only and step%2==1) or not opt.odd_step_only

    def get_transition_steps(self):
        transition_steps = []
        ws_short, ws_long = get_ws(0)
        for step in range(1, args.num_iterations):
            ws_short, new_ws_long = get_ws(step)
            if new_ws_long != ws_long:
                transition_steps.append(step)
                ws_long = new_ws_long
        return transition_steps

    def advance_schedule(self, step: int):
        self.ws_short, new_ws_long = get_ws(step)
        if new_ws_long != self.ws_long:
            self.model.yarn.apply(self.ws_long, new_ws_long)

        new_batch_size = get_bs(step)
        if new_batch_size != self.batch_size:
            self.train_loader_send_args = (new_batch_size, args.train_max_seq_len, grad_accum_steps)
        else:
            self.train_loader_send_args = None

        self.ws_long = new_ws_long
        self.mtp_weights = self.mtp_weights_schedule[step]
    
    def step_optimizers(self, step: int):                
        step_lr = get_lr(step)
        muon_momentum = get_muon_momentum(step)
        for group in self.muon_opt.param_groups:
            group["momentum"] = muon_momentum

        for opt in self.optimizers:
            for group in opt.param_groups:
                group["lr"] = group["initial_lr"] * step_lr
                
        if self._is_active_step(self.adam_opt, step):
            # adam will interleave calls to muon step
            self.adam_opt.step(self.muon_opt)
            self.model.zero_grad(set_to_none=True)
            self.adam_opt.should_sync = False
        else:
            self.muon_opt.step()
            self.muon_opt.zero_grad(set_to_none=True)
            
        if step == self.split_step:
            self.adam_opt.copy_lm_to_embed()
            self.model.split_embed = True
    
    def activate_hooks(self, step: int):
        for opt in self.optimizers:
            if self._is_active_step(opt, step):
                opt.should_sync = True

    def reset(self, state=None):
        if state is not None:
            for opt, opt_state in zip(self.optimizers, state):
                opt.should_sync = False
                opt.load_state_dict(opt_state)

        # muon momentum buffers not in state dict
        self.muon_opt.reset()
        self.model.split_embed = False

        self.ws_short, self.ws_long = get_ws(0)
        self.batch_size = get_bs(0)
        self.model.yarn.reset()

    def get_state(self):
        return [copy.deepcopy(opt.state_dict()) for opt in self.optimizers]

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # batch sizes
    train_bs_schedule: tuple = (8 * 2048 * 8, 16 * 2048 * 8, 24 * 2048 * 8)
    train_bs_extension: int = 24 * 2048 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 1800  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: float = 0.50  # fraction of num_scheduled_iterations spent cooling down the learning rate
    split_embed_frac: float = 2/3  # fraction of training when embeddings split from lm_head
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_final: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=11,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=args.val_batch_size // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.weight.data = m.weight.data.bfloat16()
model.attn_gate_bank.data = model.attn_gate_bank.data.bfloat16()
model.ve_gate_bank.data = model.ve_gate_bank.data.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)
training_manager = TrainingManager(model)

########################################
#            Warmup kernels            #
########################################
print0("Compiling model and warming up kernels (~7 minutes on first execution)", console=True)
# Warmup the training kernels, then re-initialize the state so we aren't cheating
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=training_manager.get_state()) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_bs_schedule[0], args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)

transition_steps = training_manager.get_transition_steps()
# first few steps plus transitions
warmup_steps = sorted({0, 1, 2} | set(s + offset for s in transition_steps for offset in [-1, 0, 1] if s + offset >= 0)) 
print0(f"Sampling steps {warmup_steps} for warmup", console=True)
for step in warmup_steps:
    training_manager.advance_schedule(step)
    model.eval()
    with torch.no_grad():
        inputs, targets, cum_seqlens = next(val_loader)
        model(inputs, targets, cum_seqlens, training_manager.get_forward_args())
    model.train()
    for idx in range(grad_accum_steps):
        # enable gradient sync for the DistAdam optimizers on the last iteration before we step them
        if idx == grad_accum_steps - 1:
            training_manager.activate_hooks(step)
        send_args = training_manager.train_loader_send_args
        inputs, targets, cum_seqlens = train_loader.send(send_args)
        (model(inputs, targets, cum_seqlens, training_manager.get_forward_args()) / grad_accum_steps).backward()
    training_manager.step_optimizers(step)
print0("Resetting Model", console=True)
model.zero_grad(set_to_none=True)
model.load_state_dict(initial_state["model"])
training_manager.reset(initial_state["optimizers"])
del val_loader, train_loader, initial_state
model.train()

########################################
#        Training and validation       #
########################################
train_loader = distributed_data_generator(args.train_files, args.train_bs_schedule[0], args.train_max_seq_len, grad_accum_steps=grad_accum_steps)

gc.collect()

training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    training_manager.advance_schedule(step)
    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            training_manager.apply_final_ws_ext()
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, training_manager.get_forward_args())
        val_loss /= val_steps
        del val_loader
        dist.reduce(val_loss, 0, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for idx in range(grad_accum_steps):
        # enable gradient sync for the DistAdam optimizers on the last iteration before we step them
        if idx == grad_accum_steps - 1:
            training_manager.activate_hooks(step)
        send_args = training_manager.train_loader_send_args
        inputs, targets, cum_seqlens = train_loader.send(send_args)
        (model(inputs, targets, cum_seqlens, training_manager.get_forward_args()) / grad_accum_steps).backward()
    training_manager.step_optimizers(step)

    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.10.12 (main, Nov  4 2025, 08:48:33) [GCC 11.4.0]
Running PyTorch 2.10.0.dev20251210+cu126 compiled for CUDA 12.6
Running Triton version 3.6.0
Mon Jan  5 07:59:09 2026       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:19:00.0 Off |                    0 |
| N/A   42C    P0            127W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:3B:00.0 Off |                    0 |
| N/A   35C    P0            131W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:4C:00.0 Off |                    0 |
| N/A   33C    P0            127W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:5D:00.0 Off |                    0 |
| N/A   42C    P0            130W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:9B:00.0 Off |                    0 |
| N/A   44C    P0            133W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:BB:00.0 Off |                    0 |
| N/A   35C    P0            126W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:CB:00.0 Off |                    0 |
| N/A   41C    P0            128W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:DB:00.0 Off |                    0 |
| N/A   32C    P0            125W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A     52536      C   /usr/bin/python                                 0MiB |
|    1   N/A  N/A     52537      C   /usr/bin/python                                 0MiB |
|    2   N/A  N/A     52538      C   /usr/bin/python                                 0MiB |
|    3   N/A  N/A     52539      C   /usr/bin/python                                 0MiB |
|    4   N/A  N/A     52540      C   /usr/bin/python                                 0MiB |
|    5   N/A  N/A     52541      C   /usr/bin/python                                 0MiB |
|    6   N/A  N/A     52542      C   /usr/bin/python                                 0MiB |
|    7   N/A  N/A     52543      C   /usr/bin/python                                 0MiB |
+-----------------------------------------------------------------------------------------+

====================================================================================================
Compiling model and warming up kernels (~7 minutes on first execution)
Sampling steps [0, 1, 2, 599, 600, 601, 1199, 1200, 1201, 1799, 1800, 1801] for warmup
Resetting Model
step:0/1840 val_loss:10.8294 train_time:0ms step_avg:0.04ms
step:1/1840 train_time:69ms step_avg:68.56ms
step:2/1840 train_time:90ms step_avg:45.20ms
step:3/1840 train_time:109ms step_avg:36.27ms
step:4/1840 train_time:139ms step_avg:34.66ms
step:5/1840 train_time:170ms step_avg:34.08ms
step:6/1840 train_time:256ms step_avg:42.60ms
step:7/1840 train_time:272ms step_avg:38.93ms
step:8/1840 train_time:325ms step_avg:40.58ms
step:9/1840 train_time:356ms step_avg:39.61ms
step:10/1840 train_time:390ms step_avg:39.04ms
step:11/1840 train_time:422ms step_avg:38.39ms
step:12/1840 train_time:456ms step_avg:38.03ms
step:13/1840 train_time:488ms step_avg:37.57ms
step:14/1840 train_time:522ms step_avg:37.31ms
step:15/1840 train_time:555ms step_avg:36.97ms
step:16/1840 train_time:589ms step_avg:36.80ms
step:17/1840 train_time:621ms step_avg:36.52ms
step:18/1840 train_time:655ms step_avg:36.38ms
step:19/1840 train_time:687ms step_avg:36.14ms
step:20/1840 train_time:721ms step_avg:36.04ms
step:21/1840 train_time:753ms step_avg:35.85ms
step:22/1840 train_time:787ms step_avg:35.78ms
step:23/1840 train_time:819ms step_avg:35.61ms
step:24/1840 train_time:853ms step_avg:35.54ms
step:25/1840 train_time:885ms step_avg:35.40ms
step:26/1840 train_time:919ms step_avg:35.36ms
step:27/1840 train_time:951ms step_avg:35.23ms
step:28/1840 train_time:985ms step_avg:35.19ms
step:29/1840 train_time:1017ms step_avg:35.08ms
step:30/1840 train_time:1051ms step_avg:35.05ms
step:31/1840 train_time:1083ms step_avg:34.95ms
step:32/1840 train_time:1118ms step_avg:34.93ms
step:33/1840 train_time:1150ms step_avg:34.84ms
step:34/1840 train_time:1185ms step_avg:34.86ms
step:35/1840 train_time:1219ms step_avg:34.82ms
step:36/1840 train_time:1254ms step_avg:34.83ms
step:37/1840 train_time:1287ms step_avg:34.77ms
step:38/1840 train_time:1321ms step_avg:34.77ms
step:39/1840 train_time:1353ms step_avg:34.70ms
step:40/1840 train_time:1388ms step_avg:34.70ms
step:41/1840 train_time:1420ms step_avg:34.64ms
step:42/1840 train_time:1454ms step_avg:34.63ms
step:43/1840 train_time:1487ms step_avg:34.57ms
step:44/1840 train_time:1521ms step_avg:34.57ms
step:45/1840 train_time:1553ms step_avg:34.51ms
step:46/1840 train_time:1588ms step_avg:34.51ms
step:47/1840 train_time:1620ms step_avg:34.46ms
step:48/1840 train_time:1654ms step_avg:34.45ms
step:49/1840 train_time:1686ms step_avg:34.40ms
step:50/1840 train_time:1720ms step_avg:34.40ms
step:51/1840 train_time:1752ms step_avg:34.36ms
step:52/1840 train_time:1786ms step_avg:34.36ms
step:53/1840 train_time:1819ms step_avg:34.31ms
step:54/1840 train_time:1853ms step_avg:34.31ms
step:55/1840 train_time:1885ms step_avg:34.27ms
step:56/1840 train_time:1919ms step_avg:34.27ms
step:57/1840 train_time:1951ms step_avg:34.23ms
step:58/1840 train_time:1985ms step_avg:34.23ms
step:59/1840 train_time:2017ms step_avg:34.19ms
step:60/1840 train_time:2051ms step_avg:34.19ms
step:61/1840 train_time:2083ms step_avg:34.15ms
step:62/1840 train_time:2117ms step_avg:34.15ms
step:63/1840 train_time:2149ms step_avg:34.12ms
step:64/1840 train_time:2184ms step_avg:34.13ms
step:65/1840 train_time:2217ms step_avg:34.11ms
step:66/1840 train_time:2251ms step_avg:34.11ms
step:67/1840 train_time:2283ms step_avg:34.08ms
step:68/1840 train_time:2318ms step_avg:34.09ms
step:69/1840 train_time:2350ms step_avg:34.05ms
step:70/1840 train_time:2384ms step_avg:34.06ms
step:71/1840 train_time:2417ms step_avg:34.04ms
step:72/1840 train_time:2451ms step_avg:34.04ms
step:73/1840 train_time:2483ms step_avg:34.02ms
step:74/1840 train_time:2518ms step_avg:34.03ms
step:75/1840 train_time:2550ms step_avg:34.00ms
step:76/1840 train_time:2584ms step_avg:34.00ms
step:77/1840 train_time:2616ms step_avg:33.98ms
step:78/1840 train_time:2651ms step_avg:33.98ms
step:79/1840 train_time:2683ms step_avg:33.96ms
step:80/1840 train_time:2717ms step_avg:33.97ms
step:81/1840 train_time:2749ms step_avg:33.94ms
step:82/1840 train_time:2784ms step_avg:33.95ms
step:83/1840 train_time:2816ms step_avg:33.92ms
step:84/1840 train_time:2850ms step_avg:33.93ms
step:85/1840 train_time:2882ms step_avg:33.91ms
step:86/1840 train_time:2916ms step_avg:33.91ms
step:87/1840 train_time:2948ms step_avg:33.89ms
step:88/1840 train_time:2982ms step_avg:33.89ms
step:89/1840 train_time:3014ms step_avg:33.87ms
step:90/1840 train_time:3048ms step_avg:33.87ms
step:91/1840 train_time:3081ms step_avg:33.86ms
step:92/1840 train_time:3115ms step_avg:33.86ms
step:93/1840 train_time:3147ms step_avg:33.84ms
step:94/1840 train_time:3181ms step_avg:33.84ms
step:95/1840 train_time:3214ms step_avg:33.83ms
step:96/1840 train_time:3248ms step_avg:33.83ms
step:97/1840 train_time:3280ms step_avg:33.82ms
step:98/1840 train_time:3314ms step_avg:33.82ms
step:99/1840 train_time:3346ms step_avg:33.80ms
step:100/1840 train_time:3381ms step_avg:33.81ms
step:101/1840 train_time:3413ms step_avg:33.79ms
step:102/1840 train_time:3448ms step_avg:33.80ms
step:103/1840 train_time:3480ms step_avg:33.78ms
step:104/1840 train_time:3514ms step_avg:33.79ms
step:105/1840 train_time:3546ms step_avg:33.78ms
step:106/1840 train_time:3581ms step_avg:33.78ms
step:107/1840 train_time:3613ms step_avg:33.76ms
step:108/1840 train_time:3647ms step_avg:33.77ms
step:109/1840 train_time:3679ms step_avg:33.76ms
step:110/1840 train_time:3714ms step_avg:33.76ms
step:111/1840 train_time:3746ms step_avg:33.75ms
step:112/1840 train_time:3780ms step_avg:33.75ms
step:113/1840 train_time:3812ms step_avg:33.73ms
step:114/1840 train_time:3846ms step_avg:33.74ms
step:115/1840 train_time:3878ms step_avg:33.72ms
step:116/1840 train_time:3912ms step_avg:33.73ms
step:117/1840 train_time:3944ms step_avg:33.71ms
step:118/1840 train_time:3979ms step_avg:33.72ms
step:119/1840 train_time:4011ms step_avg:33.70ms
step:120/1840 train_time:4045ms step_avg:33.71ms
step:121/1840 train_time:4077ms step_avg:33.70ms
step:122/1840 train_time:4112ms step_avg:33.70ms
step:123/1840 train_time:4144ms step_avg:33.69ms
step:124/1840 train_time:4178ms step_avg:33.69ms
step:125/1840 train_time:4210ms step_avg:33.68ms
step:126/1840 train_time:4244ms step_avg:33.68ms
step:127/1840 train_time:4277ms step_avg:33.67ms
step:128/1840 train_time:4311ms step_avg:33.68ms
step:129/1840 train_time:4343ms step_avg:33.67ms
step:130/1840 train_time:4377ms step_avg:33.67ms
step:131/1840 train_time:4409ms step_avg:33.66ms
step:132/1840 train_time:4444ms step_avg:33.66ms
step:133/1840 train_time:4476ms step_avg:33.65ms
step:134/1840 train_time:4510ms step_avg:33.66ms
step:135/1840 train_time:4542ms step_avg:33.65ms
step:136/1840 train_time:4577ms step_avg:33.65ms
step:137/1840 train_time:4609ms step_avg:33.64ms
step:138/1840 train_time:4643ms step_avg:33.65ms
step:139/1840 train_time:4675ms step_avg:33.63ms
step:140/1840 train_time:4709ms step_avg:33.64ms
step:141/1840 train_time:4742ms step_avg:33.63ms
step:142/1840 train_time:4776ms step_avg:33.63ms
step:143/1840 train_time:4808ms step_avg:33.62ms
step:144/1840 train_time:4842ms step_avg:33.63ms
step:145/1840 train_time:4875ms step_avg:33.62ms
step:146/1840 train_time:4909ms step_avg:33.62ms
step:147/1840 train_time:4941ms step_avg:33.61ms
step:148/1840 train_time:4975ms step_avg:33.62ms
step:149/1840 train_time:5008ms step_avg:33.61ms
step:150/1840 train_time:5042ms step_avg:33.61ms
step:151/1840 train_time:5074ms step_avg:33.60ms
step:152/1840 train_time:5108ms step_avg:33.61ms
step:153/1840 train_time:5140ms step_avg:33.60ms
step:154/1840 train_time:5175ms step_avg:33.60ms
step:155/1840 train_time:5207ms step_avg:33.59ms
step:156/1840 train_time:5241ms step_avg:33.60ms
step:157/1840 train_time:5273ms step_avg:33.59ms
step:158/1840 train_time:5307ms step_avg:33.59ms
step:159/1840 train_time:5340ms step_avg:33.58ms
step:160/1840 train_time:5374ms step_avg:33.59ms
step:161/1840 train_time:5406ms step_avg:33.58ms
step:162/1840 train_time:5440ms step_avg:33.58ms
step:163/1840 train_time:5472ms step_avg:33.57ms
step:164/1840 train_time:5507ms step_avg:33.58ms
step:165/1840 train_time:5539ms step_avg:33.57ms
step:166/1840 train_time:5573ms step_avg:33.57ms
step:167/1840 train_time:5605ms step_avg:33.56ms
step:168/1840 train_time:5639ms step_avg:33.57ms
step:169/1840 train_time:5671ms step_avg:33.56ms
step:170/1840 train_time:5705ms step_avg:33.56ms
step:171/1840 train_time:5737ms step_avg:33.55ms
step:172/1840 train_time:5771ms step_avg:33.55ms
step:173/1840 train_time:5803ms step_avg:33.54ms
step:174/1840 train_time:5837ms step_avg:33.55ms
step:175/1840 train_time:5869ms step_avg:33.54ms
step:176/1840 train_time:5903ms step_avg:33.54ms
step:177/1840 train_time:5936ms step_avg:33.53ms
step:178/1840 train_time:5970ms step_avg:33.54ms
step:179/1840 train_time:6002ms step_avg:33.53ms
step:180/1840 train_time:6036ms step_avg:33.54ms
step:181/1840 train_time:6069ms step_avg:33.53ms
step:182/1840 train_time:6103ms step_avg:33.53ms
step:183/1840 train_time:6135ms step_avg:33.53ms
step:184/1840 train_time:6170ms step_avg:33.53ms
step:185/1840 train_time:6202ms step_avg:33.52ms
step:186/1840 train_time:6236ms step_avg:33.53ms
step:187/1840 train_time:6269ms step_avg:33.52ms
step:188/1840 train_time:6303ms step_avg:33.53ms
step:189/1840 train_time:6335ms step_avg:33.52ms
step:190/1840 train_time:6369ms step_avg:33.52ms
step:191/1840 train_time:6402ms step_avg:33.52ms
step:192/1840 train_time:6436ms step_avg:33.52ms
step:193/1840 train_time:6468ms step_avg:33.51ms
step:194/1840 train_time:6502ms step_avg:33.52ms
step:195/1840 train_time:6534ms step_avg:33.51ms
step:196/1840 train_time:6569ms step_avg:33.51ms
step:197/1840 train_time:6601ms step_avg:33.51ms
step:198/1840 train_time:6635ms step_avg:33.51ms
step:199/1840 train_time:6667ms step_avg:33.50ms
step:200/1840 train_time:6702ms step_avg:33.51ms
step:201/1840 train_time:6734ms step_avg:33.50ms
step:202/1840 train_time:6768ms step_avg:33.51ms
step:203/1840 train_time:6800ms step_avg:33.50ms
step:204/1840 train_time:6834ms step_avg:33.50ms
step:205/1840 train_time:6866ms step_avg:33.49ms
step:206/1840 train_time:6901ms step_avg:33.50ms
step:207/1840 train_time:6933ms step_avg:33.49ms
step:208/1840 train_time:6967ms step_avg:33.50ms
step:209/1840 train_time:6999ms step_avg:33.49ms
step:210/1840 train_time:7033ms step_avg:33.49ms
step:211/1840 train_time:7065ms step_avg:33.49ms
step:212/1840 train_time:7100ms step_avg:33.49ms
step:213/1840 train_time:7132ms step_avg:33.48ms
step:214/1840 train_time:7166ms step_avg:33.48ms
step:215/1840 train_time:7198ms step_avg:33.48ms
step:216/1840 train_time:7232ms step_avg:33.48ms
step:217/1840 train_time:7264ms step_avg:33.48ms
step:218/1840 train_time:7298ms step_avg:33.48ms
step:219/1840 train_time:7330ms step_avg:33.47ms
step:220/1840 train_time:7365ms step_avg:33.48ms
step:221/1840 train_time:7397ms step_avg:33.47ms
step:222/1840 train_time:7431ms step_avg:33.48ms
step:223/1840 train_time:7464ms step_avg:33.47ms
step:224/1840 train_time:7498ms step_avg:33.47ms
step:225/1840 train_time:7530ms step_avg:33.47ms
step:226/1840 train_time:7564ms step_avg:33.47ms
step:227/1840 train_time:7596ms step_avg:33.46ms
step:228/1840 train_time:7630ms step_avg:33.46ms
step:229/1840 train_time:7663ms step_avg:33.46ms
step:230/1840 train_time:7697ms step_avg:33.47ms
step:231/1840 train_time:7729ms step_avg:33.46ms
step:232/1840 train_time:7763ms step_avg:33.46ms
step:233/1840 train_time:7795ms step_avg:33.46ms
step:234/1840 train_time:7829ms step_avg:33.46ms
step:235/1840 train_time:7862ms step_avg:33.45ms
step:236/1840 train_time:7896ms step_avg:33.46ms
step:237/1840 train_time:7928ms step_avg:33.45ms
step:238/1840 train_time:7962ms step_avg:33.45ms
step:239/1840 train_time:7994ms step_avg:33.45ms
step:240/1840 train_time:8029ms step_avg:33.45ms
step:241/1840 train_time:8061ms step_avg:33.45ms
step:242/1840 train_time:8095ms step_avg:33.45ms
step:243/1840 train_time:8127ms step_avg:33.44ms
step:244/1840 train_time:8161ms step_avg:33.45ms
step:245/1840 train_time:8194ms step_avg:33.44ms
step:246/1840 train_time:8228ms step_avg:33.45ms
step:247/1840 train_time:8260ms step_avg:33.44ms
step:248/1840 train_time:8294ms step_avg:33.44ms
step:249/1840 train_time:8326ms step_avg:33.44ms
step:250/1840 train_time:8361ms step_avg:33.44ms
step:250/1840 val_loss:4.6133 train_time:8403ms step_avg:33.61ms
step:251/1840 train_time:8421ms step_avg:33.55ms
step:252/1840 train_time:8439ms step_avg:33.49ms
step:253/1840 train_time:8463ms step_avg:33.45ms
step:254/1840 train_time:8497ms step_avg:33.45ms
step:255/1840 train_time:8531ms step_avg:33.45ms
step:256/1840 train_time:8566ms step_avg:33.46ms
step:257/1840 train_time:8599ms step_avg:33.46ms
step:258/1840 train_time:8634ms step_avg:33.47ms
step:259/1840 train_time:8667ms step_avg:33.46ms
step:260/1840 train_time:8701ms step_avg:33.47ms
step:261/1840 train_time:8733ms step_avg:33.46ms
step:262/1840 train_time:8767ms step_avg:33.46ms
step:263/1840 train_time:8799ms step_avg:33.46ms
step:264/1840 train_time:8833ms step_avg:33.46ms
step:265/1840 train_time:8865ms step_avg:33.45ms
step:266/1840 train_time:8899ms step_avg:33.46ms
step:267/1840 train_time:8931ms step_avg:33.45ms
step:268/1840 train_time:8965ms step_avg:33.45ms
step:269/1840 train_time:8997ms step_avg:33.45ms
step:270/1840 train_time:9031ms step_avg:33.45ms
step:271/1840 train_time:9063ms step_avg:33.44ms
step:272/1840 train_time:9097ms step_avg:33.44ms
step:273/1840 train_time:9129ms step_avg:33.44ms
step:274/1840 train_time:9163ms step_avg:33.44ms
step:275/1840 train_time:9195ms step_avg:33.44ms
step:276/1840 train_time:9229ms step_avg:33.44ms
step:277/1840 train_time:9261ms step_avg:33.43ms
step:278/1840 train_time:9295ms step_avg:33.44ms
step:279/1840 train_time:9327ms step_avg:33.43ms
step:280/1840 train_time:9362ms step_avg:33.43ms
step:281/1840 train_time:9394ms step_avg:33.43ms
step:282/1840 train_time:9429ms step_avg:33.43ms
step:283/1840 train_time:9461ms step_avg:33.43ms
step:284/1840 train_time:9496ms step_avg:33.44ms
step:285/1840 train_time:9529ms step_avg:33.43ms
step:286/1840 train_time:9563ms step_avg:33.44ms
step:287/1840 train_time:9596ms step_avg:33.44ms
step:288/1840 train_time:9630ms step_avg:33.44ms
step:289/1840 train_time:9662ms step_avg:33.43ms
step:290/1840 train_time:9697ms step_avg:33.44ms
step:291/1840 train_time:9729ms step_avg:33.43ms
step:292/1840 train_time:9763ms step_avg:33.43ms
step:293/1840 train_time:9795ms step_avg:33.43ms
step:294/1840 train_time:9830ms step_avg:33.43ms
step:295/1840 train_time:9862ms step_avg:33.43ms
step:296/1840 train_time:9896ms step_avg:33.43ms
step:297/1840 train_time:9928ms step_avg:33.43ms
step:298/1840 train_time:9962ms step_avg:33.43ms
step:299/1840 train_time:9994ms step_avg:33.42ms
step:300/1840 train_time:10028ms step_avg:33.43ms
step:301/1840 train_time:10060ms step_avg:33.42ms
step:302/1840 train_time:10094ms step_avg:33.42ms
step:303/1840 train_time:10125ms step_avg:33.42ms
step:304/1840 train_time:10159ms step_avg:33.42ms
step:305/1840 train_time:10192ms step_avg:33.42ms
step:306/1840 train_time:10226ms step_avg:33.42ms
step:307/1840 train_time:10258ms step_avg:33.41ms
step:308/1840 train_time:10292ms step_avg:33.42ms
step:309/1840 train_time:10324ms step_avg:33.41ms
step:310/1840 train_time:10358ms step_avg:33.41ms
step:311/1840 train_time:10391ms step_avg:33.41ms
step:312/1840 train_time:10426ms step_avg:33.42ms
step:313/1840 train_time:10457ms step_avg:33.41ms
step:314/1840 train_time:10492ms step_avg:33.41ms
step:315/1840 train_time:10525ms step_avg:33.41ms
step:316/1840 train_time:10559ms step_avg:33.41ms
step:317/1840 train_time:10591ms step_avg:33.41ms
step:318/1840 train_time:10626ms step_avg:33.41ms
step:319/1840 train_time:10658ms step_avg:33.41ms
step:320/1840 train_time:10693ms step_avg:33.41ms
step:321/1840 train_time:10725ms step_avg:33.41ms
step:322/1840 train_time:10759ms step_avg:33.41ms
step:323/1840 train_time:10791ms step_avg:33.41ms
step:324/1840 train_time:10826ms step_avg:33.41ms
step:325/1840 train_time:10858ms step_avg:33.41ms
step:326/1840 train_time:10892ms step_avg:33.41ms
step:327/1840 train_time:10924ms step_avg:33.41ms
step:328/1840 train_time:10959ms step_avg:33.41ms
step:329/1840 train_time:10991ms step_avg:33.41ms
step:330/1840 train_time:11025ms step_avg:33.41ms
step:331/1840 train_time:11057ms step_avg:33.40ms
step:332/1840 train_time:11091ms step_avg:33.41ms
step:333/1840 train_time:11123ms step_avg:33.40ms
step:334/1840 train_time:11157ms step_avg:33.41ms
step:335/1840 train_time:11189ms step_avg:33.40ms
step:336/1840 train_time:11224ms step_avg:33.40ms
step:337/1840 train_time:11256ms step_avg:33.40ms
step:338/1840 train_time:11290ms step_avg:33.40ms
step:339/1840 train_time:11322ms step_avg:33.40ms
step:340/1840 train_time:11356ms step_avg:33.40ms
step:341/1840 train_time:11388ms step_avg:33.40ms
step:342/1840 train_time:11423ms step_avg:33.40ms
step:343/1840 train_time:11455ms step_avg:33.40ms
step:344/1840 train_time:11489ms step_avg:33.40ms
step:345/1840 train_time:11521ms step_avg:33.39ms
step:346/1840 train_time:11555ms step_avg:33.40ms
step:347/1840 train_time:11587ms step_avg:33.39ms
step:348/1840 train_time:11621ms step_avg:33.40ms
step:349/1840 train_time:11654ms step_avg:33.39ms
step:350/1840 train_time:11688ms step_avg:33.39ms
step:351/1840 train_time:11720ms step_avg:33.39ms
step:352/1840 train_time:11754ms step_avg:33.39ms
step:353/1840 train_time:11786ms step_avg:33.39ms
step:354/1840 train_time:11820ms step_avg:33.39ms
step:355/1840 train_time:11852ms step_avg:33.39ms
step:356/1840 train_time:11887ms step_avg:33.39ms
step:357/1840 train_time:11919ms step_avg:33.39ms
step:358/1840 train_time:11953ms step_avg:33.39ms
step:359/1840 train_time:11985ms step_avg:33.38ms
step:360/1840 train_time:12019ms step_avg:33.39ms
step:361/1840 train_time:12051ms step_avg:33.38ms
step:362/1840 train_time:12085ms step_avg:33.38ms
step:363/1840 train_time:12117ms step_avg:33.38ms
step:364/1840 train_time:12152ms step_avg:33.38ms
step:365/1840 train_time:12184ms step_avg:33.38ms
step:366/1840 train_time:12218ms step_avg:33.38ms
step:367/1840 train_time:12250ms step_avg:33.38ms
step:368/1840 train_time:12284ms step_avg:33.38ms
step:369/1840 train_time:12316ms step_avg:33.38ms
step:370/1840 train_time:12350ms step_avg:33.38ms
step:371/1840 train_time:12382ms step_avg:33.38ms
step:372/1840 train_time:12417ms step_avg:33.38ms
step:373/1840 train_time:12449ms step_avg:33.37ms
step:374/1840 train_time:12483ms step_avg:33.38ms
step:375/1840 train_time:12515ms step_avg:33.37ms
step:376/1840 train_time:12549ms step_avg:33.38ms
step:377/1840 train_time:12581ms step_avg:33.37ms
step:378/1840 train_time:12616ms step_avg:33.38ms
step:379/1840 train_time:12648ms step_avg:33.37ms
step:380/1840 train_time:12682ms step_avg:33.37ms
step:381/1840 train_time:12714ms step_avg:33.37ms
step:382/1840 train_time:12749ms step_avg:33.37ms
step:383/1840 train_time:12781ms step_avg:33.37ms
step:384/1840 train_time:12815ms step_avg:33.37ms
step:385/1840 train_time:12847ms step_avg:33.37ms
step:386/1840 train_time:12881ms step_avg:33.37ms
step:387/1840 train_time:12913ms step_avg:33.37ms
step:388/1840 train_time:12948ms step_avg:33.37ms
step:389/1840 train_time:12979ms step_avg:33.37ms
step:390/1840 train_time:13014ms step_avg:33.37ms
step:391/1840 train_time:13046ms step_avg:33.36ms
step:392/1840 train_time:13080ms step_avg:33.37ms
step:393/1840 train_time:13112ms step_avg:33.36ms
step:394/1840 train_time:13146ms step_avg:33.37ms
step:395/1840 train_time:13178ms step_avg:33.36ms
step:396/1840 train_time:13212ms step_avg:33.36ms
step:397/1840 train_time:13244ms step_avg:33.36ms
step:398/1840 train_time:13279ms step_avg:33.36ms
step:399/1840 train_time:13311ms step_avg:33.36ms
step:400/1840 train_time:13345ms step_avg:33.36ms
step:401/1840 train_time:13377ms step_avg:33.36ms
step:402/1840 train_time:13412ms step_avg:33.36ms
step:403/1840 train_time:13444ms step_avg:33.36ms
step:404/1840 train_time:13478ms step_avg:33.36ms
step:405/1840 train_time:13510ms step_avg:33.36ms
step:406/1840 train_time:13544ms step_avg:33.36ms
step:407/1840 train_time:13576ms step_avg:33.36ms
step:408/1840 train_time:13610ms step_avg:33.36ms
step:409/1840 train_time:13643ms step_avg:33.36ms
step:410/1840 train_time:13677ms step_avg:33.36ms
step:411/1840 train_time:13709ms step_avg:33.35ms
step:412/1840 train_time:13743ms step_avg:33.36ms
step:413/1840 train_time:13776ms step_avg:33.35ms
step:414/1840 train_time:13810ms step_avg:33.36ms
step:415/1840 train_time:13842ms step_avg:33.36ms
step:416/1840 train_time:13877ms step_avg:33.36ms
step:417/1840 train_time:13908ms step_avg:33.35ms
step:418/1840 train_time:13943ms step_avg:33.36ms
step:419/1840 train_time:13975ms step_avg:33.35ms
step:420/1840 train_time:14010ms step_avg:33.36ms
step:421/1840 train_time:14042ms step_avg:33.35ms
step:422/1840 train_time:14077ms step_avg:33.36ms
step:423/1840 train_time:14109ms step_avg:33.35ms
step:424/1840 train_time:14143ms step_avg:33.36ms
step:425/1840 train_time:14175ms step_avg:33.35ms
step:426/1840 train_time:14209ms step_avg:33.36ms
step:427/1840 train_time:14241ms step_avg:33.35ms
step:428/1840 train_time:14276ms step_avg:33.35ms
step:429/1840 train_time:14308ms step_avg:33.35ms
step:430/1840 train_time:14342ms step_avg:33.35ms
step:431/1840 train_time:14374ms step_avg:33.35ms
step:432/1840 train_time:14408ms step_avg:33.35ms
step:433/1840 train_time:14440ms step_avg:33.35ms
step:434/1840 train_time:14474ms step_avg:33.35ms
step:435/1840 train_time:14506ms step_avg:33.35ms
step:436/1840 train_time:14541ms step_avg:33.35ms
step:437/1840 train_time:14573ms step_avg:33.35ms
step:438/1840 train_time:14607ms step_avg:33.35ms
step:439/1840 train_time:14639ms step_avg:33.35ms
step:440/1840 train_time:14673ms step_avg:33.35ms
step:441/1840 train_time:14705ms step_avg:33.35ms
step:442/1840 train_time:14739ms step_avg:33.35ms
step:443/1840 train_time:14772ms step_avg:33.35ms
step:444/1840 train_time:14806ms step_avg:33.35ms
step:445/1840 train_time:14838ms step_avg:33.34ms
step:446/1840 train_time:14873ms step_avg:33.35ms
step:447/1840 train_time:14905ms step_avg:33.34ms
step:448/1840 train_time:14939ms step_avg:33.35ms
step:449/1840 train_time:14971ms step_avg:33.34ms
step:450/1840 train_time:15006ms step_avg:33.35ms
step:451/1840 train_time:15038ms step_avg:33.34ms
step:452/1840 train_time:15072ms step_avg:33.35ms
step:453/1840 train_time:15104ms step_avg:33.34ms
step:454/1840 train_time:15138ms step_avg:33.34ms
step:455/1840 train_time:15170ms step_avg:33.34ms
step:456/1840 train_time:15205ms step_avg:33.34ms
step:457/1840 train_time:15237ms step_avg:33.34ms
step:458/1840 train_time:15271ms step_avg:33.34ms
step:459/1840 train_time:15303ms step_avg:33.34ms
step:460/1840 train_time:15337ms step_avg:33.34ms
step:461/1840 train_time:15369ms step_avg:33.34ms
step:462/1840 train_time:15403ms step_avg:33.34ms
step:463/1840 train_time:15436ms step_avg:33.34ms
step:464/1840 train_time:15470ms step_avg:33.34ms
step:465/1840 train_time:15502ms step_avg:33.34ms
step:466/1840 train_time:15537ms step_avg:33.34ms
step:467/1840 train_time:15569ms step_avg:33.34ms
step:468/1840 train_time:15603ms step_avg:33.34ms
step:469/1840 train_time:15635ms step_avg:33.34ms
step:470/1840 train_time:15669ms step_avg:33.34ms
step:471/1840 train_time:15701ms step_avg:33.34ms
step:472/1840 train_time:15736ms step_avg:33.34ms
step:473/1840 train_time:15768ms step_avg:33.34ms
step:474/1840 train_time:15802ms step_avg:33.34ms
step:475/1840 train_time:15834ms step_avg:33.33ms
step:476/1840 train_time:15868ms step_avg:33.34ms
step:477/1840 train_time:15900ms step_avg:33.33ms
step:478/1840 train_time:15934ms step_avg:33.33ms
step:479/1840 train_time:15967ms step_avg:33.33ms
step:480/1840 train_time:16001ms step_avg:33.34ms
step:481/1840 train_time:16033ms step_avg:33.33ms
step:482/1840 train_time:16068ms step_avg:33.34ms
step:483/1840 train_time:16100ms step_avg:33.33ms
step:484/1840 train_time:16134ms step_avg:33.33ms
step:485/1840 train_time:16166ms step_avg:33.33ms
step:486/1840 train_time:16200ms step_avg:33.33ms
step:487/1840 train_time:16232ms step_avg:33.33ms
step:488/1840 train_time:16267ms step_avg:33.33ms
step:489/1840 train_time:16298ms step_avg:33.33ms
step:490/1840 train_time:16333ms step_avg:33.33ms
step:491/1840 train_time:16365ms step_avg:33.33ms
step:492/1840 train_time:16400ms step_avg:33.33ms
step:493/1840 train_time:16432ms step_avg:33.33ms
step:494/1840 train_time:16466ms step_avg:33.33ms
step:495/1840 train_time:16498ms step_avg:33.33ms
step:496/1840 train_time:16532ms step_avg:33.33ms
step:497/1840 train_time:16564ms step_avg:33.33ms
step:498/1840 train_time:16598ms step_avg:33.33ms
step:499/1840 train_time:16630ms step_avg:33.33ms
step:500/1840 train_time:16665ms step_avg:33.33ms
step:500/1840 val_loss:4.2924 train_time:16706ms step_avg:33.41ms
step:501/1840 train_time:16724ms step_avg:33.38ms
step:502/1840 train_time:16742ms step_avg:33.35ms
step:503/1840 train_time:16766ms step_avg:33.33ms
step:504/1840 train_time:16800ms step_avg:33.33ms
step:505/1840 train_time:16834ms step_avg:33.34ms
step:506/1840 train_time:16870ms step_avg:33.34ms
step:507/1840 train_time:16903ms step_avg:33.34ms
step:508/1840 train_time:16937ms step_avg:33.34ms
step:509/1840 train_time:16969ms step_avg:33.34ms
step:510/1840 train_time:17003ms step_avg:33.34ms
step:511/1840 train_time:17035ms step_avg:33.34ms
step:512/1840 train_time:17069ms step_avg:33.34ms
step:513/1840 train_time:17101ms step_avg:33.34ms
step:514/1840 train_time:17135ms step_avg:33.34ms
step:515/1840 train_time:17167ms step_avg:33.33ms
step:516/1840 train_time:17201ms step_avg:33.33ms
step:517/1840 train_time:17233ms step_avg:33.33ms
step:518/1840 train_time:17267ms step_avg:33.33ms
step:519/1840 train_time:17298ms step_avg:33.33ms
step:520/1840 train_time:17333ms step_avg:33.33ms
step:521/1840 train_time:17365ms step_avg:33.33ms
step:522/1840 train_time:17398ms step_avg:33.33ms
step:523/1840 train_time:17430ms step_avg:33.33ms
step:524/1840 train_time:17464ms step_avg:33.33ms
step:525/1840 train_time:17496ms step_avg:33.33ms
step:526/1840 train_time:17530ms step_avg:33.33ms
step:527/1840 train_time:17562ms step_avg:33.32ms
step:528/1840 train_time:17596ms step_avg:33.33ms
step:529/1840 train_time:17629ms step_avg:33.32ms
step:530/1840 train_time:17663ms step_avg:33.33ms
step:531/1840 train_time:17696ms step_avg:33.33ms
step:532/1840 train_time:17730ms step_avg:33.33ms
step:533/1840 train_time:17763ms step_avg:33.33ms
step:534/1840 train_time:17797ms step_avg:33.33ms
step:535/1840 train_time:17830ms step_avg:33.33ms
step:536/1840 train_time:17865ms step_avg:33.33ms
step:537/1840 train_time:17897ms step_avg:33.33ms
step:538/1840 train_time:17931ms step_avg:33.33ms
step:539/1840 train_time:17963ms step_avg:33.33ms
step:540/1840 train_time:17998ms step_avg:33.33ms
step:541/1840 train_time:18030ms step_avg:33.33ms
step:542/1840 train_time:18064ms step_avg:33.33ms
step:543/1840 train_time:18096ms step_avg:33.33ms
step:544/1840 train_time:18130ms step_avg:33.33ms
step:545/1840 train_time:18162ms step_avg:33.32ms
step:546/1840 train_time:18196ms step_avg:33.33ms
step:547/1840 train_time:18228ms step_avg:33.32ms
step:548/1840 train_time:18262ms step_avg:33.32ms
step:549/1840 train_time:18294ms step_avg:33.32ms
step:550/1840 train_time:18328ms step_avg:33.32ms
step:551/1840 train_time:18360ms step_avg:33.32ms
step:552/1840 train_time:18394ms step_avg:33.32ms
step:553/1840 train_time:18426ms step_avg:33.32ms
step:554/1840 train_time:18460ms step_avg:33.32ms
step:555/1840 train_time:18492ms step_avg:33.32ms
step:556/1840 train_time:18526ms step_avg:33.32ms
step:557/1840 train_time:18558ms step_avg:33.32ms
step:558/1840 train_time:18592ms step_avg:33.32ms
step:559/1840 train_time:18624ms step_avg:33.32ms
step:560/1840 train_time:18658ms step_avg:33.32ms
step:561/1840 train_time:18691ms step_avg:33.32ms
step:562/1840 train_time:18725ms step_avg:33.32ms
step:563/1840 train_time:18758ms step_avg:33.32ms
step:564/1840 train_time:18792ms step_avg:33.32ms
step:565/1840 train_time:18824ms step_avg:33.32ms
step:566/1840 train_time:18859ms step_avg:33.32ms
step:567/1840 train_time:18892ms step_avg:33.32ms
step:568/1840 train_time:18926ms step_avg:33.32ms
step:569/1840 train_time:18959ms step_avg:33.32ms
step:570/1840 train_time:18993ms step_avg:33.32ms
step:571/1840 train_time:19026ms step_avg:33.32ms
step:572/1840 train_time:19060ms step_avg:33.32ms
step:573/1840 train_time:19092ms step_avg:33.32ms
step:574/1840 train_time:19126ms step_avg:33.32ms
step:575/1840 train_time:19158ms step_avg:33.32ms
step:576/1840 train_time:19193ms step_avg:33.32ms
step:577/1840 train_time:19225ms step_avg:33.32ms
step:578/1840 train_time:19259ms step_avg:33.32ms
step:579/1840 train_time:19291ms step_avg:33.32ms
step:580/1840 train_time:19325ms step_avg:33.32ms
step:581/1840 train_time:19357ms step_avg:33.32ms
step:582/1840 train_time:19392ms step_avg:33.32ms
step:583/1840 train_time:19424ms step_avg:33.32ms
step:584/1840 train_time:19458ms step_avg:33.32ms
step:585/1840 train_time:19490ms step_avg:33.32ms
step:586/1840 train_time:19524ms step_avg:33.32ms
step:587/1840 train_time:19556ms step_avg:33.31ms
step:588/1840 train_time:19590ms step_avg:33.32ms
step:589/1840 train_time:19622ms step_avg:33.31ms
step:590/1840 train_time:19656ms step_avg:33.32ms
step:591/1840 train_time:19688ms step_avg:33.31ms
step:592/1840 train_time:19723ms step_avg:33.32ms
step:593/1840 train_time:19755ms step_avg:33.31ms
step:594/1840 train_time:19790ms step_avg:33.32ms
step:595/1840 train_time:19822ms step_avg:33.31ms
step:596/1840 train_time:19856ms step_avg:33.32ms
step:597/1840 train_time:19889ms step_avg:33.31ms
step:598/1840 train_time:19923ms step_avg:33.32ms
step:599/1840 train_time:19955ms step_avg:33.31ms
step:600/1840 train_time:19990ms step_avg:33.32ms
step:601/1840 train_time:20024ms step_avg:33.32ms
step:602/1840 train_time:20082ms step_avg:33.36ms
step:603/1840 train_time:20142ms step_avg:33.40ms
step:604/1840 train_time:20203ms step_avg:33.45ms
step:605/1840 train_time:20263ms step_avg:33.49ms
step:606/1840 train_time:20324ms step_avg:33.54ms
step:607/1840 train_time:20383ms step_avg:33.58ms
step:608/1840 train_time:20445ms step_avg:33.63ms
step:609/1840 train_time:20505ms step_avg:33.67ms
step:610/1840 train_time:20567ms step_avg:33.72ms
step:611/1840 train_time:20627ms step_avg:33.76ms
step:612/1840 train_time:20690ms step_avg:33.81ms
step:613/1840 train_time:20750ms step_avg:33.85ms
step:614/1840 train_time:20813ms step_avg:33.90ms
step:615/1840 train_time:20873ms step_avg:33.94ms
step:616/1840 train_time:20936ms step_avg:33.99ms
step:617/1840 train_time:20996ms step_avg:34.03ms
step:618/1840 train_time:21058ms step_avg:34.08ms
step:619/1840 train_time:21118ms step_avg:34.12ms
step:620/1840 train_time:21180ms step_avg:34.16ms
step:621/1840 train_time:21240ms step_avg:34.20ms
step:622/1840 train_time:21302ms step_avg:34.25ms
step:623/1840 train_time:21361ms step_avg:34.29ms
step:624/1840 train_time:21423ms step_avg:34.33ms
step:625/1840 train_time:21482ms step_avg:34.37ms
step:626/1840 train_time:21545ms step_avg:34.42ms
step:627/1840 train_time:21604ms step_avg:34.46ms
step:628/1840 train_time:21667ms step_avg:34.50ms
step:629/1840 train_time:21727ms step_avg:34.54ms
step:630/1840 train_time:21790ms step_avg:34.59ms
step:631/1840 train_time:21850ms step_avg:34.63ms
step:632/1840 train_time:21912ms step_avg:34.67ms
step:633/1840 train_time:21972ms step_avg:34.71ms
step:634/1840 train_time:22035ms step_avg:34.76ms
step:635/1840 train_time:22095ms step_avg:34.80ms
step:636/1840 train_time:22158ms step_avg:34.84ms
step:637/1840 train_time:22219ms step_avg:34.88ms
step:638/1840 train_time:22281ms step_avg:34.92ms
step:639/1840 train_time:22340ms step_avg:34.96ms
step:640/1840 train_time:22402ms step_avg:35.00ms
step:641/1840 train_time:22461ms step_avg:35.04ms
step:642/1840 train_time:22523ms step_avg:35.08ms
step:643/1840 train_time:22583ms step_avg:35.12ms
step:644/1840 train_time:22645ms step_avg:35.16ms
step:645/1840 train_time:22704ms step_avg:35.20ms
step:646/1840 train_time:22767ms step_avg:35.24ms
step:647/1840 train_time:22827ms step_avg:35.28ms
step:648/1840 train_time:22890ms step_avg:35.32ms
step:649/1840 train_time:22951ms step_avg:35.36ms
step:650/1840 train_time:23014ms step_avg:35.41ms
step:651/1840 train_time:23074ms step_avg:35.44ms
step:652/1840 train_time:23137ms step_avg:35.49ms
step:653/1840 train_time:23198ms step_avg:35.52ms
step:654/1840 train_time:23260ms step_avg:35.57ms
step:655/1840 train_time:23319ms step_avg:35.60ms
step:656/1840 train_time:23381ms step_avg:35.64ms
step:657/1840 train_time:23441ms step_avg:35.68ms
step:658/1840 train_time:23502ms step_avg:35.72ms
step:659/1840 train_time:23562ms step_avg:35.75ms
step:660/1840 train_time:23624ms step_avg:35.79ms
step:661/1840 train_time:23683ms step_avg:35.83ms
step:662/1840 train_time:23747ms step_avg:35.87ms
step:663/1840 train_time:23806ms step_avg:35.91ms
step:664/1840 train_time:23868ms step_avg:35.95ms
step:665/1840 train_time:23929ms step_avg:35.98ms
step:666/1840 train_time:23991ms step_avg:36.02ms
step:667/1840 train_time:24052ms step_avg:36.06ms
step:668/1840 train_time:24115ms step_avg:36.10ms
step:669/1840 train_time:24176ms step_avg:36.14ms
step:670/1840 train_time:24239ms step_avg:36.18ms
step:671/1840 train_time:24299ms step_avg:36.21ms
step:672/1840 train_time:24361ms step_avg:36.25ms
step:673/1840 train_time:24420ms step_avg:36.29ms
step:674/1840 train_time:24482ms step_avg:36.32ms
step:675/1840 train_time:24542ms step_avg:36.36ms
step:676/1840 train_time:24604ms step_avg:36.40ms
step:677/1840 train_time:24663ms step_avg:36.43ms
step:678/1840 train_time:24726ms step_avg:36.47ms
step:679/1840 train_time:24785ms step_avg:36.50ms
step:680/1840 train_time:24847ms step_avg:36.54ms
step:681/1840 train_time:24906ms step_avg:36.57ms
step:682/1840 train_time:24969ms step_avg:36.61ms
step:683/1840 train_time:25029ms step_avg:36.65ms
step:684/1840 train_time:25092ms step_avg:36.68ms
step:685/1840 train_time:25153ms step_avg:36.72ms
step:686/1840 train_time:25216ms step_avg:36.76ms
step:687/1840 train_time:25277ms step_avg:36.79ms
step:688/1840 train_time:25338ms step_avg:36.83ms
step:689/1840 train_time:25398ms step_avg:36.86ms
step:690/1840 train_time:25460ms step_avg:36.90ms
step:691/1840 train_time:25521ms step_avg:36.93ms
step:692/1840 train_time:25583ms step_avg:36.97ms
step:693/1840 train_time:25643ms step_avg:37.00ms
step:694/1840 train_time:25704ms step_avg:37.04ms
step:695/1840 train_time:25763ms step_avg:37.07ms
step:696/1840 train_time:25826ms step_avg:37.11ms
step:697/1840 train_time:25886ms step_avg:37.14ms
step:698/1840 train_time:25949ms step_avg:37.18ms
step:699/1840 train_time:26008ms step_avg:37.21ms
step:700/1840 train_time:26070ms step_avg:37.24ms
step:701/1840 train_time:26130ms step_avg:37.28ms
step:702/1840 train_time:26193ms step_avg:37.31ms
step:703/1840 train_time:26252ms step_avg:37.34ms
step:704/1840 train_time:26315ms step_avg:37.38ms
step:705/1840 train_time:26375ms step_avg:37.41ms
step:706/1840 train_time:26439ms step_avg:37.45ms
step:707/1840 train_time:26499ms step_avg:37.48ms
step:708/1840 train_time:26562ms step_avg:37.52ms
step:709/1840 train_time:26621ms step_avg:37.55ms
step:710/1840 train_time:26684ms step_avg:37.58ms
step:711/1840 train_time:26743ms step_avg:37.61ms
step:712/1840 train_time:26805ms step_avg:37.65ms
step:713/1840 train_time:26864ms step_avg:37.68ms
step:714/1840 train_time:26926ms step_avg:37.71ms
step:715/1840 train_time:26986ms step_avg:37.74ms
step:716/1840 train_time:27049ms step_avg:37.78ms
step:717/1840 train_time:27109ms step_avg:37.81ms
step:718/1840 train_time:27171ms step_avg:37.84ms
step:719/1840 train_time:27232ms step_avg:37.87ms
step:720/1840 train_time:27295ms step_avg:37.91ms
step:721/1840 train_time:27355ms step_avg:37.94ms
step:722/1840 train_time:27417ms step_avg:37.97ms
step:723/1840 train_time:27478ms step_avg:38.01ms
step:724/1840 train_time:27541ms step_avg:38.04ms
step:725/1840 train_time:27601ms step_avg:38.07ms
step:726/1840 train_time:27663ms step_avg:38.10ms
step:727/1840 train_time:27722ms step_avg:38.13ms
step:728/1840 train_time:27783ms step_avg:38.16ms
step:729/1840 train_time:27844ms step_avg:38.20ms
step:730/1840 train_time:27906ms step_avg:38.23ms
step:731/1840 train_time:27965ms step_avg:38.26ms
step:732/1840 train_time:28027ms step_avg:38.29ms
step:733/1840 train_time:28086ms step_avg:38.32ms
step:734/1840 train_time:28149ms step_avg:38.35ms
step:735/1840 train_time:28210ms step_avg:38.38ms
step:736/1840 train_time:28272ms step_avg:38.41ms
step:737/1840 train_time:28333ms step_avg:38.44ms
step:738/1840 train_time:28397ms step_avg:38.48ms
step:739/1840 train_time:28458ms step_avg:38.51ms
step:740/1840 train_time:28520ms step_avg:38.54ms
step:741/1840 train_time:28580ms step_avg:38.57ms
step:742/1840 train_time:28642ms step_avg:38.60ms
step:743/1840 train_time:28701ms step_avg:38.63ms
step:744/1840 train_time:28763ms step_avg:38.66ms
step:745/1840 train_time:28822ms step_avg:38.69ms
step:746/1840 train_time:28884ms step_avg:38.72ms
step:747/1840 train_time:28944ms step_avg:38.75ms
step:748/1840 train_time:29006ms step_avg:38.78ms
step:749/1840 train_time:29065ms step_avg:38.81ms
step:750/1840 train_time:29127ms step_avg:38.84ms
step:750/1840 val_loss:4.0171 train_time:29200ms step_avg:38.93ms
step:751/1840 train_time:29220ms step_avg:38.91ms
step:752/1840 train_time:29252ms step_avg:38.90ms
step:753/1840 train_time:29313ms step_avg:38.93ms
step:754/1840 train_time:29375ms step_avg:38.96ms
step:755/1840 train_time:29437ms step_avg:38.99ms
step:756/1840 train_time:29500ms step_avg:39.02ms
step:757/1840 train_time:29559ms step_avg:39.05ms
step:758/1840 train_time:29621ms step_avg:39.08ms
step:759/1840 train_time:29681ms step_avg:39.10ms
step:760/1840 train_time:29743ms step_avg:39.14ms
step:761/1840 train_time:29802ms step_avg:39.16ms
step:762/1840 train_time:29863ms step_avg:39.19ms
step:763/1840 train_time:29923ms step_avg:39.22ms
step:764/1840 train_time:29986ms step_avg:39.25ms
step:765/1840 train_time:30046ms step_avg:39.28ms
step:766/1840 train_time:30109ms step_avg:39.31ms
step:767/1840 train_time:30170ms step_avg:39.33ms
step:768/1840 train_time:30233ms step_avg:39.37ms
step:769/1840 train_time:30294ms step_avg:39.39ms
step:770/1840 train_time:30356ms step_avg:39.42ms
step:771/1840 train_time:30415ms step_avg:39.45ms
step:772/1840 train_time:30478ms step_avg:39.48ms
step:773/1840 train_time:30537ms step_avg:39.51ms
step:774/1840 train_time:30599ms step_avg:39.53ms
step:775/1840 train_time:30659ms step_avg:39.56ms
step:776/1840 train_time:30722ms step_avg:39.59ms
step:777/1840 train_time:30781ms step_avg:39.62ms
step:778/1840 train_time:30843ms step_avg:39.64ms
step:779/1840 train_time:30903ms step_avg:39.67ms
step:780/1840 train_time:30966ms step_avg:39.70ms
step:781/1840 train_time:31026ms step_avg:39.73ms
step:782/1840 train_time:31088ms step_avg:39.76ms
step:783/1840 train_time:31149ms step_avg:39.78ms
step:784/1840 train_time:31211ms step_avg:39.81ms
step:785/1840 train_time:31272ms step_avg:39.84ms
step:786/1840 train_time:31335ms step_avg:39.87ms
step:787/1840 train_time:31395ms step_avg:39.89ms
step:788/1840 train_time:31456ms step_avg:39.92ms
step:789/1840 train_time:31516ms step_avg:39.94ms
step:790/1840 train_time:31577ms step_avg:39.97ms
step:791/1840 train_time:31636ms step_avg:40.00ms
step:792/1840 train_time:31699ms step_avg:40.02ms
step:793/1840 train_time:31759ms step_avg:40.05ms
step:794/1840 train_time:31821ms step_avg:40.08ms
step:795/1840 train_time:31881ms step_avg:40.10ms
step:796/1840 train_time:31943ms step_avg:40.13ms
step:797/1840 train_time:32003ms step_avg:40.15ms
step:798/1840 train_time:32066ms step_avg:40.18ms
step:799/1840 train_time:32127ms step_avg:40.21ms
step:800/1840 train_time:32190ms step_avg:40.24ms
step:801/1840 train_time:32250ms step_avg:40.26ms
step:802/1840 train_time:32313ms step_avg:40.29ms
step:803/1840 train_time:32372ms step_avg:40.31ms
step:804/1840 train_time:32435ms step_avg:40.34ms
step:805/1840 train_time:32495ms step_avg:40.37ms
step:806/1840 train_time:32556ms step_avg:40.39ms
step:807/1840 train_time:32616ms step_avg:40.42ms
step:808/1840 train_time:32678ms step_avg:40.44ms
step:809/1840 train_time:32737ms step_avg:40.47ms
step:810/1840 train_time:32799ms step_avg:40.49ms
step:811/1840 train_time:32858ms step_avg:40.52ms
step:812/1840 train_time:32920ms step_avg:40.54ms
step:813/1840 train_time:32981ms step_avg:40.57ms
step:814/1840 train_time:33044ms step_avg:40.59ms
step:815/1840 train_time:33105ms step_avg:40.62ms
step:816/1840 train_time:33168ms step_avg:40.65ms
step:817/1840 train_time:33228ms step_avg:40.67ms
step:818/1840 train_time:33290ms step_avg:40.70ms
step:819/1840 train_time:33351ms step_avg:40.72ms
step:820/1840 train_time:33413ms step_avg:40.75ms
step:821/1840 train_time:33474ms step_avg:40.77ms
step:822/1840 train_time:33535ms step_avg:40.80ms
step:823/1840 train_time:33595ms step_avg:40.82ms
step:824/1840 train_time:33657ms step_avg:40.85ms
step:825/1840 train_time:33716ms step_avg:40.87ms
step:826/1840 train_time:33779ms step_avg:40.89ms
step:827/1840 train_time:33837ms step_avg:40.92ms
step:828/1840 train_time:33900ms step_avg:40.94ms
step:829/1840 train_time:33959ms step_avg:40.96ms
step:830/1840 train_time:34022ms step_avg:40.99ms
step:831/1840 train_time:34084ms step_avg:41.02ms
step:832/1840 train_time:34147ms step_avg:41.04ms
step:833/1840 train_time:34208ms step_avg:41.07ms
step:834/1840 train_time:34270ms step_avg:41.09ms
step:835/1840 train_time:34330ms step_avg:41.11ms
step:836/1840 train_time:34392ms step_avg:41.14ms
step:837/1840 train_time:34452ms step_avg:41.16ms
step:838/1840 train_time:34514ms step_avg:41.19ms
step:839/1840 train_time:34573ms step_avg:41.21ms
step:840/1840 train_time:34635ms step_avg:41.23ms
step:841/1840 train_time:34695ms step_avg:41.25ms
step:842/1840 train_time:34756ms step_avg:41.28ms
step:843/1840 train_time:34815ms step_avg:41.30ms
step:844/1840 train_time:34877ms step_avg:41.32ms
step:845/1840 train_time:34937ms step_avg:41.35ms
step:846/1840 train_time:35000ms step_avg:41.37ms
step:847/1840 train_time:35060ms step_avg:41.39ms
step:848/1840 train_time:35123ms step_avg:41.42ms
step:849/1840 train_time:35184ms step_avg:41.44ms
step:850/1840 train_time:35247ms step_avg:41.47ms
step:851/1840 train_time:35307ms step_avg:41.49ms
step:852/1840 train_time:35369ms step_avg:41.51ms
step:853/1840 train_time:35430ms step_avg:41.54ms
step:854/1840 train_time:35492ms step_avg:41.56ms
step:855/1840 train_time:35552ms step_avg:41.58ms
step:856/1840 train_time:35613ms step_avg:41.60ms
step:857/1840 train_time:35673ms step_avg:41.63ms
step:858/1840 train_time:35736ms step_avg:41.65ms
step:859/1840 train_time:35795ms step_avg:41.67ms
step:860/1840 train_time:35857ms step_avg:41.69ms
step:861/1840 train_time:35917ms step_avg:41.72ms
step:862/1840 train_time:35979ms step_avg:41.74ms
step:863/1840 train_time:36038ms step_avg:41.76ms
step:864/1840 train_time:36102ms step_avg:41.78ms
step:865/1840 train_time:36162ms step_avg:41.81ms
step:866/1840 train_time:36225ms step_avg:41.83ms
step:867/1840 train_time:36285ms step_avg:41.85ms
step:868/1840 train_time:36348ms step_avg:41.88ms
step:869/1840 train_time:36409ms step_avg:41.90ms
step:870/1840 train_time:36472ms step_avg:41.92ms
step:871/1840 train_time:36531ms step_avg:41.94ms
step:872/1840 train_time:36593ms step_avg:41.96ms
step:873/1840 train_time:36653ms step_avg:41.98ms
step:874/1840 train_time:36715ms step_avg:42.01ms
step:875/1840 train_time:36774ms step_avg:42.03ms
step:876/1840 train_time:36836ms step_avg:42.05ms
step:877/1840 train_time:36895ms step_avg:42.07ms
step:878/1840 train_time:36957ms step_avg:42.09ms
step:879/1840 train_time:37017ms step_avg:42.11ms
step:880/1840 train_time:37079ms step_avg:42.14ms
step:881/1840 train_time:37140ms step_avg:42.16ms
step:882/1840 train_time:37203ms step_avg:42.18ms
step:883/1840 train_time:37263ms step_avg:42.20ms
step:884/1840 train_time:37325ms step_avg:42.22ms
step:885/1840 train_time:37387ms step_avg:42.25ms
step:886/1840 train_time:37449ms step_avg:42.27ms
step:887/1840 train_time:37509ms step_avg:42.29ms
step:888/1840 train_time:37572ms step_avg:42.31ms
step:889/1840 train_time:37632ms step_avg:42.33ms
step:890/1840 train_time:37693ms step_avg:42.35ms
step:891/1840 train_time:37753ms step_avg:42.37ms
step:892/1840 train_time:37815ms step_avg:42.39ms
step:893/1840 train_time:37874ms step_avg:42.41ms
step:894/1840 train_time:37936ms step_avg:42.43ms
step:895/1840 train_time:37995ms step_avg:42.45ms
step:896/1840 train_time:38058ms step_avg:42.48ms
step:897/1840 train_time:38118ms step_avg:42.49ms
step:898/1840 train_time:38181ms step_avg:42.52ms
step:899/1840 train_time:38239ms step_avg:42.54ms
step:900/1840 train_time:38302ms step_avg:42.56ms
step:901/1840 train_time:38364ms step_avg:42.58ms
step:902/1840 train_time:38426ms step_avg:42.60ms
step:903/1840 train_time:38486ms step_avg:42.62ms
step:904/1840 train_time:38549ms step_avg:42.64ms
step:905/1840 train_time:38609ms step_avg:42.66ms
step:906/1840 train_time:38671ms step_avg:42.68ms
step:907/1840 train_time:38731ms step_avg:42.70ms
step:908/1840 train_time:38793ms step_avg:42.72ms
step:909/1840 train_time:38852ms step_avg:42.74ms
step:910/1840 train_time:38915ms step_avg:42.76ms
step:911/1840 train_time:38974ms step_avg:42.78ms
step:912/1840 train_time:39035ms step_avg:42.80ms
step:913/1840 train_time:39094ms step_avg:42.82ms
step:914/1840 train_time:39158ms step_avg:42.84ms
step:915/1840 train_time:39217ms step_avg:42.86ms
step:916/1840 train_time:39280ms step_avg:42.88ms
step:917/1840 train_time:39340ms step_avg:42.90ms
step:918/1840 train_time:39404ms step_avg:42.92ms
step:919/1840 train_time:39465ms step_avg:42.94ms
step:920/1840 train_time:39528ms step_avg:42.97ms
step:921/1840 train_time:39589ms step_avg:42.98ms
step:922/1840 train_time:39652ms step_avg:43.01ms
step:923/1840 train_time:39712ms step_avg:43.02ms
step:924/1840 train_time:39774ms step_avg:43.04ms
step:925/1840 train_time:39833ms step_avg:43.06ms
step:926/1840 train_time:39895ms step_avg:43.08ms
step:927/1840 train_time:39954ms step_avg:43.10ms
step:928/1840 train_time:40016ms step_avg:43.12ms
step:929/1840 train_time:40076ms step_avg:43.14ms
step:930/1840 train_time:40138ms step_avg:43.16ms
step:931/1840 train_time:40197ms step_avg:43.18ms
step:932/1840 train_time:40260ms step_avg:43.20ms
step:933/1840 train_time:40321ms step_avg:43.22ms
step:934/1840 train_time:40383ms step_avg:43.24ms
step:935/1840 train_time:40443ms step_avg:43.25ms
step:936/1840 train_time:40507ms step_avg:43.28ms
step:937/1840 train_time:40568ms step_avg:43.30ms
step:938/1840 train_time:40630ms step_avg:43.32ms
step:939/1840 train_time:40690ms step_avg:43.33ms
step:940/1840 train_time:40753ms step_avg:43.35ms
step:941/1840 train_time:40813ms step_avg:43.37ms
step:942/1840 train_time:40875ms step_avg:43.39ms
step:943/1840 train_time:40934ms step_avg:43.41ms
step:944/1840 train_time:40996ms step_avg:43.43ms
step:945/1840 train_time:41056ms step_avg:43.45ms
step:946/1840 train_time:41117ms step_avg:43.46ms
step:947/1840 train_time:41177ms step_avg:43.48ms
step:948/1840 train_time:41240ms step_avg:43.50ms
step:949/1840 train_time:41299ms step_avg:43.52ms
step:950/1840 train_time:41362ms step_avg:43.54ms
step:951/1840 train_time:41422ms step_avg:43.56ms
step:952/1840 train_time:41486ms step_avg:43.58ms
step:953/1840 train_time:41547ms step_avg:43.60ms
step:954/1840 train_time:41609ms step_avg:43.62ms
step:955/1840 train_time:41670ms step_avg:43.63ms
step:956/1840 train_time:41732ms step_avg:43.65ms
step:957/1840 train_time:41791ms step_avg:43.67ms
step:958/1840 train_time:41854ms step_avg:43.69ms
step:959/1840 train_time:41913ms step_avg:43.70ms
step:960/1840 train_time:41975ms step_avg:43.72ms
step:961/1840 train_time:42035ms step_avg:43.74ms
step:962/1840 train_time:42097ms step_avg:43.76ms
step:963/1840 train_time:42156ms step_avg:43.78ms
step:964/1840 train_time:42219ms step_avg:43.80ms
step:965/1840 train_time:42278ms step_avg:43.81ms
step:966/1840 train_time:42341ms step_avg:43.83ms
step:967/1840 train_time:42402ms step_avg:43.85ms
step:968/1840 train_time:42465ms step_avg:43.87ms
step:969/1840 train_time:42525ms step_avg:43.89ms
step:970/1840 train_time:42588ms step_avg:43.91ms
step:971/1840 train_time:42648ms step_avg:43.92ms
step:972/1840 train_time:42710ms step_avg:43.94ms
step:973/1840 train_time:42771ms step_avg:43.96ms
step:974/1840 train_time:42833ms step_avg:43.98ms
step:975/1840 train_time:42892ms step_avg:43.99ms
step:976/1840 train_time:42954ms step_avg:44.01ms
step:977/1840 train_time:43014ms step_avg:44.03ms
step:978/1840 train_time:43075ms step_avg:44.04ms
step:979/1840 train_time:43136ms step_avg:44.06ms
step:980/1840 train_time:43197ms step_avg:44.08ms
step:981/1840 train_time:43257ms step_avg:44.09ms
step:982/1840 train_time:43319ms step_avg:44.11ms
step:983/1840 train_time:43379ms step_avg:44.13ms
step:984/1840 train_time:43442ms step_avg:44.15ms
step:985/1840 train_time:43503ms step_avg:44.17ms
step:986/1840 train_time:43565ms step_avg:44.18ms
step:987/1840 train_time:43626ms step_avg:44.20ms
step:988/1840 train_time:43689ms step_avg:44.22ms
step:989/1840 train_time:43748ms step_avg:44.23ms
step:990/1840 train_time:43810ms step_avg:44.25ms
step:991/1840 train_time:43870ms step_avg:44.27ms
step:992/1840 train_time:43932ms step_avg:44.29ms
step:993/1840 train_time:43992ms step_avg:44.30ms
step:994/1840 train_time:44054ms step_avg:44.32ms
step:995/1840 train_time:44114ms step_avg:44.34ms
step:996/1840 train_time:44176ms step_avg:44.35ms
step:997/1840 train_time:44235ms step_avg:44.37ms
step:998/1840 train_time:44297ms step_avg:44.39ms
step:999/1840 train_time:44357ms step_avg:44.40ms
step:1000/1840 train_time:44421ms step_avg:44.42ms
step:1000/1840 val_loss:3.7666 train_time:44493ms step_avg:44.49ms
step:1001/1840 train_time:44511ms step_avg:44.47ms
step:1002/1840 train_time:44544ms step_avg:44.45ms
step:1003/1840 train_time:44604ms step_avg:44.47ms
step:1004/1840 train_time:44668ms step_avg:44.49ms
step:1005/1840 train_time:44728ms step_avg:44.51ms
step:1006/1840 train_time:44791ms step_avg:44.52ms
step:1007/1840 train_time:44851ms step_avg:44.54ms
step:1008/1840 train_time:44913ms step_avg:44.56ms
step:1009/1840 train_time:44972ms step_avg:44.57ms
step:1010/1840 train_time:45034ms step_avg:44.59ms
step:1011/1840 train_time:45094ms step_avg:44.60ms
step:1012/1840 train_time:45156ms step_avg:44.62ms
step:1013/1840 train_time:45216ms step_avg:44.64ms
step:1014/1840 train_time:45278ms step_avg:44.65ms
step:1015/1840 train_time:45337ms step_avg:44.67ms
step:1016/1840 train_time:45399ms step_avg:44.68ms
step:1017/1840 train_time:45460ms step_avg:44.70ms
step:1018/1840 train_time:45523ms step_avg:44.72ms
step:1019/1840 train_time:45585ms step_avg:44.73ms
step:1020/1840 train_time:45647ms step_avg:44.75ms
step:1021/1840 train_time:45707ms step_avg:44.77ms
step:1022/1840 train_time:45769ms step_avg:44.78ms
step:1023/1840 train_time:45828ms step_avg:44.80ms
step:1024/1840 train_time:45890ms step_avg:44.81ms
step:1025/1840 train_time:45950ms step_avg:44.83ms
step:1026/1840 train_time:46013ms step_avg:44.85ms
step:1027/1840 train_time:46072ms step_avg:44.86ms
step:1028/1840 train_time:46134ms step_avg:44.88ms
step:1029/1840 train_time:46194ms step_avg:44.89ms
step:1030/1840 train_time:46256ms step_avg:44.91ms
step:1031/1840 train_time:46316ms step_avg:44.92ms
step:1032/1840 train_time:46378ms step_avg:44.94ms
step:1033/1840 train_time:46439ms step_avg:44.96ms
step:1034/1840 train_time:46502ms step_avg:44.97ms
step:1035/1840 train_time:46562ms step_avg:44.99ms
step:1036/1840 train_time:46625ms step_avg:45.00ms
step:1037/1840 train_time:46685ms step_avg:45.02ms
step:1038/1840 train_time:46747ms step_avg:45.04ms
step:1039/1840 train_time:46807ms step_avg:45.05ms
step:1040/1840 train_time:46869ms step_avg:45.07ms
step:1041/1840 train_time:46930ms step_avg:45.08ms
step:1042/1840 train_time:46992ms step_avg:45.10ms
step:1043/1840 train_time:47051ms step_avg:45.11ms
step:1044/1840 train_time:47113ms step_avg:45.13ms
step:1045/1840 train_time:47173ms step_avg:45.14ms
step:1046/1840 train_time:47235ms step_avg:45.16ms
step:1047/1840 train_time:47296ms step_avg:45.17ms
step:1048/1840 train_time:47359ms step_avg:45.19ms
step:1049/1840 train_time:47419ms step_avg:45.20ms
step:1050/1840 train_time:47481ms step_avg:45.22ms
step:1051/1840 train_time:47541ms step_avg:45.23ms
step:1052/1840 train_time:47603ms step_avg:45.25ms
step:1053/1840 train_time:47663ms step_avg:45.26ms
step:1054/1840 train_time:47726ms step_avg:45.28ms
step:1055/1840 train_time:47785ms step_avg:45.29ms
step:1056/1840 train_time:47847ms step_avg:45.31ms
step:1057/1840 train_time:47907ms step_avg:45.32ms
step:1058/1840 train_time:47969ms step_avg:45.34ms
step:1059/1840 train_time:48030ms step_avg:45.35ms
step:1060/1840 train_time:48091ms step_avg:45.37ms
step:1061/1840 train_time:48151ms step_avg:45.38ms
step:1062/1840 train_time:48213ms step_avg:45.40ms
step:1063/1840 train_time:48273ms step_avg:45.41ms
step:1064/1840 train_time:48336ms step_avg:45.43ms
step:1065/1840 train_time:48396ms step_avg:45.44ms
step:1066/1840 train_time:48459ms step_avg:45.46ms
step:1067/1840 train_time:48519ms step_avg:45.47ms
step:1068/1840 train_time:48581ms step_avg:45.49ms
step:1069/1840 train_time:48641ms step_avg:45.50ms
step:1070/1840 train_time:48703ms step_avg:45.52ms
step:1071/1840 train_time:48762ms step_avg:45.53ms
step:1072/1840 train_time:48825ms step_avg:45.55ms
step:1073/1840 train_time:48884ms step_avg:45.56ms
step:1074/1840 train_time:48946ms step_avg:45.57ms
step:1075/1840 train_time:49006ms step_avg:45.59ms
step:1076/1840 train_time:49068ms step_avg:45.60ms
step:1077/1840 train_time:49129ms step_avg:45.62ms
step:1078/1840 train_time:49192ms step_avg:45.63ms
step:1079/1840 train_time:49252ms step_avg:45.65ms
step:1080/1840 train_time:49315ms step_avg:45.66ms
step:1081/1840 train_time:49376ms step_avg:45.68ms
step:1082/1840 train_time:49438ms step_avg:45.69ms
step:1083/1840 train_time:49498ms step_avg:45.70ms
step:1084/1840 train_time:49560ms step_avg:45.72ms
step:1085/1840 train_time:49620ms step_avg:45.73ms
step:1086/1840 train_time:49682ms step_avg:45.75ms
step:1087/1840 train_time:49742ms step_avg:45.76ms
step:1088/1840 train_time:49803ms step_avg:45.77ms
step:1089/1840 train_time:49863ms step_avg:45.79ms
step:1090/1840 train_time:49924ms step_avg:45.80ms
step:1091/1840 train_time:49984ms step_avg:45.81ms
step:1092/1840 train_time:50047ms step_avg:45.83ms
step:1093/1840 train_time:50107ms step_avg:45.84ms
step:1094/1840 train_time:50169ms step_avg:45.86ms
step:1095/1840 train_time:50231ms step_avg:45.87ms
step:1096/1840 train_time:50293ms step_avg:45.89ms
step:1097/1840 train_time:50354ms step_avg:45.90ms
step:1098/1840 train_time:50417ms step_avg:45.92ms
step:1099/1840 train_time:50477ms step_avg:45.93ms
step:1100/1840 train_time:50539ms step_avg:45.94ms
step:1101/1840 train_time:50599ms step_avg:45.96ms
step:1102/1840 train_time:50661ms step_avg:45.97ms
step:1103/1840 train_time:50720ms step_avg:45.98ms
step:1104/1840 train_time:50782ms step_avg:46.00ms
step:1105/1840 train_time:50842ms step_avg:46.01ms
step:1106/1840 train_time:50904ms step_avg:46.03ms
step:1107/1840 train_time:50963ms step_avg:46.04ms
step:1108/1840 train_time:51026ms step_avg:46.05ms
step:1109/1840 train_time:51085ms step_avg:46.06ms
step:1110/1840 train_time:51148ms step_avg:46.08ms
step:1111/1840 train_time:51208ms step_avg:46.09ms
step:1112/1840 train_time:51273ms step_avg:46.11ms
step:1113/1840 train_time:51333ms step_avg:46.12ms
step:1114/1840 train_time:51396ms step_avg:46.14ms
step:1115/1840 train_time:51457ms step_avg:46.15ms
step:1116/1840 train_time:51519ms step_avg:46.16ms
step:1117/1840 train_time:51580ms step_avg:46.18ms
step:1118/1840 train_time:51641ms step_avg:46.19ms
step:1119/1840 train_time:51702ms step_avg:46.20ms
step:1120/1840 train_time:51763ms step_avg:46.22ms
step:1121/1840 train_time:51823ms step_avg:46.23ms
step:1122/1840 train_time:51884ms step_avg:46.24ms
step:1123/1840 train_time:51944ms step_avg:46.25ms
step:1124/1840 train_time:52006ms step_avg:46.27ms
step:1125/1840 train_time:52065ms step_avg:46.28ms
step:1126/1840 train_time:52127ms step_avg:46.29ms
step:1127/1840 train_time:52188ms step_avg:46.31ms
step:1128/1840 train_time:52251ms step_avg:46.32ms
step:1129/1840 train_time:52311ms step_avg:46.33ms
step:1130/1840 train_time:52375ms step_avg:46.35ms
step:1131/1840 train_time:52435ms step_avg:46.36ms
step:1132/1840 train_time:52498ms step_avg:46.38ms
step:1133/1840 train_time:52558ms step_avg:46.39ms
step:1134/1840 train_time:52619ms step_avg:46.40ms
step:1135/1840 train_time:52679ms step_avg:46.41ms
step:1136/1840 train_time:52741ms step_avg:46.43ms
step:1137/1840 train_time:52800ms step_avg:46.44ms
step:1138/1840 train_time:52862ms step_avg:46.45ms
step:1139/1840 train_time:52921ms step_avg:46.46ms
step:1140/1840 train_time:52983ms step_avg:46.48ms
step:1141/1840 train_time:53042ms step_avg:46.49ms
step:1142/1840 train_time:53106ms step_avg:46.50ms
step:1143/1840 train_time:53165ms step_avg:46.51ms
step:1144/1840 train_time:53227ms step_avg:46.53ms
step:1145/1840 train_time:53288ms step_avg:46.54ms
step:1146/1840 train_time:53352ms step_avg:46.55ms
step:1147/1840 train_time:53412ms step_avg:46.57ms
step:1148/1840 train_time:53475ms step_avg:46.58ms
step:1149/1840 train_time:53536ms step_avg:46.59ms
step:1150/1840 train_time:53598ms step_avg:46.61ms
step:1151/1840 train_time:53659ms step_avg:46.62ms
step:1152/1840 train_time:53720ms step_avg:46.63ms
step:1153/1840 train_time:53779ms step_avg:46.64ms
step:1154/1840 train_time:53842ms step_avg:46.66ms
step:1155/1840 train_time:53901ms step_avg:46.67ms
step:1156/1840 train_time:53963ms step_avg:46.68ms
step:1157/1840 train_time:54022ms step_avg:46.69ms
step:1158/1840 train_time:54084ms step_avg:46.70ms
step:1159/1840 train_time:54144ms step_avg:46.72ms
step:1160/1840 train_time:54207ms step_avg:46.73ms
step:1161/1840 train_time:54266ms step_avg:46.74ms
step:1162/1840 train_time:54330ms step_avg:46.76ms
step:1163/1840 train_time:54390ms step_avg:46.77ms
step:1164/1840 train_time:54453ms step_avg:46.78ms
step:1165/1840 train_time:54513ms step_avg:46.79ms
step:1166/1840 train_time:54575ms step_avg:46.81ms
step:1167/1840 train_time:54635ms step_avg:46.82ms
step:1168/1840 train_time:54698ms step_avg:46.83ms
step:1169/1840 train_time:54758ms step_avg:46.84ms
step:1170/1840 train_time:54820ms step_avg:46.85ms
step:1171/1840 train_time:54879ms step_avg:46.87ms
step:1172/1840 train_time:54942ms step_avg:46.88ms
step:1173/1840 train_time:55001ms step_avg:46.89ms
step:1174/1840 train_time:55063ms step_avg:46.90ms
step:1175/1840 train_time:55123ms step_avg:46.91ms
step:1176/1840 train_time:55186ms step_avg:46.93ms
step:1177/1840 train_time:55245ms step_avg:46.94ms
step:1178/1840 train_time:55308ms step_avg:46.95ms
step:1179/1840 train_time:55367ms step_avg:46.96ms
step:1180/1840 train_time:55430ms step_avg:46.97ms
step:1181/1840 train_time:55491ms step_avg:46.99ms
step:1182/1840 train_time:55554ms step_avg:47.00ms
step:1183/1840 train_time:55614ms step_avg:47.01ms
step:1184/1840 train_time:55676ms step_avg:47.02ms
step:1185/1840 train_time:55735ms step_avg:47.03ms
step:1186/1840 train_time:55798ms step_avg:47.05ms
step:1187/1840 train_time:55858ms step_avg:47.06ms
step:1188/1840 train_time:55921ms step_avg:47.07ms
step:1189/1840 train_time:55981ms step_avg:47.08ms
step:1190/1840 train_time:56043ms step_avg:47.09ms
step:1191/1840 train_time:56102ms step_avg:47.10ms
step:1192/1840 train_time:56164ms step_avg:47.12ms
step:1193/1840 train_time:56222ms step_avg:47.13ms
step:1194/1840 train_time:56285ms step_avg:47.14ms
step:1195/1840 train_time:56345ms step_avg:47.15ms
step:1196/1840 train_time:56407ms step_avg:47.16ms
step:1197/1840 train_time:56467ms step_avg:47.17ms
step:1198/1840 train_time:56530ms step_avg:47.19ms
step:1199/1840 train_time:56590ms step_avg:47.20ms
step:1200/1840 train_time:56653ms step_avg:47.21ms
step:1201/1840 train_time:56714ms step_avg:47.22ms
step:1202/1840 train_time:56800ms step_avg:47.25ms
step:1203/1840 train_time:56889ms step_avg:47.29ms
step:1204/1840 train_time:56979ms step_avg:47.32ms
step:1205/1840 train_time:57066ms step_avg:47.36ms
step:1206/1840 train_time:57154ms step_avg:47.39ms
step:1207/1840 train_time:57241ms step_avg:47.42ms
step:1208/1840 train_time:57328ms step_avg:47.46ms
step:1209/1840 train_time:57415ms step_avg:47.49ms
step:1210/1840 train_time:57504ms step_avg:47.52ms
step:1211/1840 train_time:57590ms step_avg:47.56ms
step:1212/1840 train_time:57680ms step_avg:47.59ms
step:1213/1840 train_time:57769ms step_avg:47.62ms
step:1214/1840 train_time:57859ms step_avg:47.66ms
step:1215/1840 train_time:57947ms step_avg:47.69ms
step:1216/1840 train_time:58036ms step_avg:47.73ms
step:1217/1840 train_time:58122ms step_avg:47.76ms
step:1218/1840 train_time:58210ms step_avg:47.79ms
step:1219/1840 train_time:58296ms step_avg:47.82ms
step:1220/1840 train_time:58383ms step_avg:47.86ms
step:1221/1840 train_time:58469ms step_avg:47.89ms
step:1222/1840 train_time:58559ms step_avg:47.92ms
step:1223/1840 train_time:58646ms step_avg:47.95ms
step:1224/1840 train_time:58734ms step_avg:47.99ms
step:1225/1840 train_time:58822ms step_avg:48.02ms
step:1226/1840 train_time:58911ms step_avg:48.05ms
step:1227/1840 train_time:58998ms step_avg:48.08ms
step:1228/1840 train_time:59087ms step_avg:48.12ms
step:1229/1840 train_time:59172ms step_avg:48.15ms
step:1230/1840 train_time:59259ms step_avg:48.18ms
step:1231/1840 train_time:59346ms step_avg:48.21ms
step:1232/1840 train_time:59434ms step_avg:48.24ms
step:1233/1840 train_time:59521ms step_avg:48.27ms
step:1234/1840 train_time:59609ms step_avg:48.31ms
step:1235/1840 train_time:59697ms step_avg:48.34ms
step:1236/1840 train_time:59787ms step_avg:48.37ms
step:1237/1840 train_time:59874ms step_avg:48.40ms
step:1238/1840 train_time:59963ms step_avg:48.44ms
step:1239/1840 train_time:60050ms step_avg:48.47ms
step:1240/1840 train_time:60140ms step_avg:48.50ms
step:1241/1840 train_time:60226ms step_avg:48.53ms
step:1242/1840 train_time:60314ms step_avg:48.56ms
step:1243/1840 train_time:60400ms step_avg:48.59ms
step:1244/1840 train_time:60489ms step_avg:48.62ms
step:1245/1840 train_time:60574ms step_avg:48.65ms
step:1246/1840 train_time:60665ms step_avg:48.69ms
step:1247/1840 train_time:60750ms step_avg:48.72ms
step:1248/1840 train_time:60839ms step_avg:48.75ms
step:1249/1840 train_time:60928ms step_avg:48.78ms
step:1250/1840 train_time:61016ms step_avg:48.81ms
step:1250/1840 val_loss:3.5341 train_time:61117ms step_avg:48.89ms
step:1251/1840 train_time:61136ms step_avg:48.87ms
step:1252/1840 train_time:61192ms step_avg:48.88ms
step:1253/1840 train_time:61284ms step_avg:48.91ms
step:1254/1840 train_time:61374ms step_avg:48.94ms
step:1255/1840 train_time:61459ms step_avg:48.97ms
step:1256/1840 train_time:61546ms step_avg:49.00ms
step:1257/1840 train_time:61631ms step_avg:49.03ms
step:1258/1840 train_time:61719ms step_avg:49.06ms
step:1259/1840 train_time:61804ms step_avg:49.09ms
step:1260/1840 train_time:61892ms step_avg:49.12ms
step:1261/1840 train_time:61978ms step_avg:49.15ms
step:1262/1840 train_time:62068ms step_avg:49.18ms
step:1263/1840 train_time:62157ms step_avg:49.21ms
step:1264/1840 train_time:62250ms step_avg:49.25ms
step:1265/1840 train_time:62339ms step_avg:49.28ms
step:1266/1840 train_time:62427ms step_avg:49.31ms
step:1267/1840 train_time:62511ms step_avg:49.34ms
step:1268/1840 train_time:62599ms step_avg:49.37ms
step:1269/1840 train_time:62685ms step_avg:49.40ms
step:1270/1840 train_time:62774ms step_avg:49.43ms
step:1271/1840 train_time:62859ms step_avg:49.46ms
step:1272/1840 train_time:62947ms step_avg:49.49ms
step:1273/1840 train_time:63033ms step_avg:49.52ms
step:1274/1840 train_time:63125ms step_avg:49.55ms
step:1275/1840 train_time:63212ms step_avg:49.58ms
step:1276/1840 train_time:63303ms step_avg:49.61ms
step:1277/1840 train_time:63388ms step_avg:49.64ms
step:1278/1840 train_time:63478ms step_avg:49.67ms
step:1279/1840 train_time:63563ms step_avg:49.70ms
step:1280/1840 train_time:63652ms step_avg:49.73ms
step:1281/1840 train_time:63738ms step_avg:49.76ms
step:1282/1840 train_time:63826ms step_avg:49.79ms
step:1283/1840 train_time:63911ms step_avg:49.81ms
step:1284/1840 train_time:63999ms step_avg:49.84ms
step:1285/1840 train_time:64087ms step_avg:49.87ms
step:1286/1840 train_time:64178ms step_avg:49.90ms
step:1287/1840 train_time:64266ms step_avg:49.93ms
step:1288/1840 train_time:64355ms step_avg:49.96ms
step:1289/1840 train_time:64441ms step_avg:49.99ms
step:1290/1840 train_time:64529ms step_avg:50.02ms
step:1291/1840 train_time:64614ms step_avg:50.05ms
step:1292/1840 train_time:64703ms step_avg:50.08ms
step:1293/1840 train_time:64788ms step_avg:50.11ms
step:1294/1840 train_time:64877ms step_avg:50.14ms
step:1295/1840 train_time:64964ms step_avg:50.17ms
step:1296/1840 train_time:65052ms step_avg:50.19ms
step:1297/1840 train_time:65138ms step_avg:50.22ms
step:1298/1840 train_time:65227ms step_avg:50.25ms
step:1299/1840 train_time:65313ms step_avg:50.28ms
step:1300/1840 train_time:65402ms step_avg:50.31ms
step:1301/1840 train_time:65488ms step_avg:50.34ms
step:1302/1840 train_time:65576ms step_avg:50.37ms
step:1303/1840 train_time:65662ms step_avg:50.39ms
step:1304/1840 train_time:65750ms step_avg:50.42ms
step:1305/1840 train_time:65837ms step_avg:50.45ms
step:1306/1840 train_time:65926ms step_avg:50.48ms
step:1307/1840 train_time:66013ms step_avg:50.51ms
step:1308/1840 train_time:66102ms step_avg:50.54ms
step:1309/1840 train_time:66189ms step_avg:50.56ms
step:1310/1840 train_time:66278ms step_avg:50.59ms
step:1311/1840 train_time:66364ms step_avg:50.62ms
step:1312/1840 train_time:66453ms step_avg:50.65ms
step:1313/1840 train_time:66540ms step_avg:50.68ms
step:1314/1840 train_time:66627ms step_avg:50.71ms
step:1315/1840 train_time:66713ms step_avg:50.73ms
step:1316/1840 train_time:66802ms step_avg:50.76ms
step:1317/1840 train_time:66887ms step_avg:50.79ms
step:1318/1840 train_time:66977ms step_avg:50.82ms
step:1319/1840 train_time:67065ms step_avg:50.85ms
step:1320/1840 train_time:67154ms step_avg:50.87ms
step:1321/1840 train_time:67241ms step_avg:50.90ms
step:1322/1840 train_time:67329ms step_avg:50.93ms
step:1323/1840 train_time:67416ms step_avg:50.96ms
step:1324/1840 train_time:67505ms step_avg:50.99ms
step:1325/1840 train_time:67590ms step_avg:51.01ms
step:1326/1840 train_time:67679ms step_avg:51.04ms
step:1327/1840 train_time:67766ms step_avg:51.07ms
step:1328/1840 train_time:67854ms step_avg:51.09ms
step:1329/1840 train_time:67941ms step_avg:51.12ms
step:1330/1840 train_time:68029ms step_avg:51.15ms
step:1331/1840 train_time:68117ms step_avg:51.18ms
step:1332/1840 train_time:68207ms step_avg:51.21ms
step:1333/1840 train_time:68292ms step_avg:51.23ms
step:1334/1840 train_time:68381ms step_avg:51.26ms
step:1335/1840 train_time:68467ms step_avg:51.29ms
step:1336/1840 train_time:68555ms step_avg:51.31ms
step:1337/1840 train_time:68642ms step_avg:51.34ms
step:1338/1840 train_time:68731ms step_avg:51.37ms
step:1339/1840 train_time:68818ms step_avg:51.39ms
step:1340/1840 train_time:68907ms step_avg:51.42ms
step:1341/1840 train_time:68993ms step_avg:51.45ms
step:1342/1840 train_time:69083ms step_avg:51.48ms
step:1343/1840 train_time:69168ms step_avg:51.50ms
step:1344/1840 train_time:69257ms step_avg:51.53ms
step:1345/1840 train_time:69344ms step_avg:51.56ms
step:1346/1840 train_time:69433ms step_avg:51.58ms
step:1347/1840 train_time:69518ms step_avg:51.61ms
step:1348/1840 train_time:69606ms step_avg:51.64ms
step:1349/1840 train_time:69692ms step_avg:51.66ms
step:1350/1840 train_time:69783ms step_avg:51.69ms
step:1351/1840 train_time:69869ms step_avg:51.72ms
step:1352/1840 train_time:69957ms step_avg:51.74ms
step:1353/1840 train_time:70045ms step_avg:51.77ms
step:1354/1840 train_time:70132ms step_avg:51.80ms
step:1355/1840 train_time:70219ms step_avg:51.82ms
step:1356/1840 train_time:70307ms step_avg:51.85ms
step:1357/1840 train_time:70393ms step_avg:51.87ms
step:1358/1840 train_time:70482ms step_avg:51.90ms
step:1359/1840 train_time:70568ms step_avg:51.93ms
step:1360/1840 train_time:70656ms step_avg:51.95ms
step:1361/1840 train_time:70744ms step_avg:51.98ms
step:1362/1840 train_time:70833ms step_avg:52.01ms
step:1363/1840 train_time:70919ms step_avg:52.03ms
step:1364/1840 train_time:71007ms step_avg:52.06ms
step:1365/1840 train_time:71094ms step_avg:52.08ms
step:1366/1840 train_time:71184ms step_avg:52.11ms
step:1367/1840 train_time:71270ms step_avg:52.14ms
step:1368/1840 train_time:71359ms step_avg:52.16ms
step:1369/1840 train_time:71445ms step_avg:52.19ms
step:1370/1840 train_time:71533ms step_avg:52.21ms
step:1371/1840 train_time:71619ms step_avg:52.24ms
step:1372/1840 train_time:71707ms step_avg:52.26ms
step:1373/1840 train_time:71793ms step_avg:52.29ms
step:1374/1840 train_time:71882ms step_avg:52.32ms
step:1375/1840 train_time:71968ms step_avg:52.34ms
step:1376/1840 train_time:72058ms step_avg:52.37ms
step:1377/1840 train_time:72145ms step_avg:52.39ms
step:1378/1840 train_time:72233ms step_avg:52.42ms
step:1379/1840 train_time:72319ms step_avg:52.44ms
step:1380/1840 train_time:72407ms step_avg:52.47ms
step:1381/1840 train_time:72492ms step_avg:52.49ms
step:1382/1840 train_time:72581ms step_avg:52.52ms
step:1383/1840 train_time:72668ms step_avg:52.54ms
step:1384/1840 train_time:72757ms step_avg:52.57ms
step:1385/1840 train_time:72844ms step_avg:52.59ms
step:1386/1840 train_time:72932ms step_avg:52.62ms
step:1387/1840 train_time:73018ms step_avg:52.64ms
step:1388/1840 train_time:73107ms step_avg:52.67ms
step:1389/1840 train_time:73192ms step_avg:52.69ms
step:1390/1840 train_time:73282ms step_avg:52.72ms
step:1391/1840 train_time:73367ms step_avg:52.74ms
step:1392/1840 train_time:73457ms step_avg:52.77ms
step:1393/1840 train_time:73544ms step_avg:52.80ms
step:1394/1840 train_time:73633ms step_avg:52.82ms
step:1395/1840 train_time:73719ms step_avg:52.85ms
step:1396/1840 train_time:73807ms step_avg:52.87ms
step:1397/1840 train_time:73893ms step_avg:52.89ms
step:1398/1840 train_time:73982ms step_avg:52.92ms
step:1399/1840 train_time:74068ms step_avg:52.94ms
step:1400/1840 train_time:74157ms step_avg:52.97ms
step:1401/1840 train_time:74245ms step_avg:52.99ms
step:1402/1840 train_time:74333ms step_avg:53.02ms
step:1403/1840 train_time:74420ms step_avg:53.04ms
step:1404/1840 train_time:74508ms step_avg:53.07ms
step:1405/1840 train_time:74594ms step_avg:53.09ms
step:1406/1840 train_time:74684ms step_avg:53.12ms
step:1407/1840 train_time:74769ms step_avg:53.14ms
step:1408/1840 train_time:74859ms step_avg:53.17ms
step:1409/1840 train_time:74946ms step_avg:53.19ms
step:1410/1840 train_time:75035ms step_avg:53.22ms
step:1411/1840 train_time:75121ms step_avg:53.24ms
step:1412/1840 train_time:75208ms step_avg:53.26ms
step:1413/1840 train_time:75295ms step_avg:53.29ms
step:1414/1840 train_time:75385ms step_avg:53.31ms
step:1415/1840 train_time:75470ms step_avg:53.34ms
step:1416/1840 train_time:75559ms step_avg:53.36ms
step:1417/1840 train_time:75645ms step_avg:53.38ms
step:1418/1840 train_time:75735ms step_avg:53.41ms
step:1419/1840 train_time:75821ms step_avg:53.43ms
step:1420/1840 train_time:75909ms step_avg:53.46ms
step:1421/1840 train_time:75996ms step_avg:53.48ms
step:1422/1840 train_time:76085ms step_avg:53.51ms
step:1423/1840 train_time:76171ms step_avg:53.53ms
step:1424/1840 train_time:76260ms step_avg:53.55ms
step:1425/1840 train_time:76346ms step_avg:53.58ms
step:1426/1840 train_time:76435ms step_avg:53.60ms
step:1427/1840 train_time:76520ms step_avg:53.62ms
step:1428/1840 train_time:76608ms step_avg:53.65ms
step:1429/1840 train_time:76695ms step_avg:53.67ms
step:1430/1840 train_time:76784ms step_avg:53.70ms
step:1431/1840 train_time:76870ms step_avg:53.72ms
step:1432/1840 train_time:76960ms step_avg:53.74ms
step:1433/1840 train_time:77046ms step_avg:53.77ms
step:1434/1840 train_time:77135ms step_avg:53.79ms
step:1435/1840 train_time:77222ms step_avg:53.81ms
step:1436/1840 train_time:77310ms step_avg:53.84ms
step:1437/1840 train_time:77396ms step_avg:53.86ms
step:1438/1840 train_time:77486ms step_avg:53.88ms
step:1439/1840 train_time:77571ms step_avg:53.91ms
step:1440/1840 train_time:77661ms step_avg:53.93ms
step:1441/1840 train_time:77746ms step_avg:53.95ms
step:1442/1840 train_time:77834ms step_avg:53.98ms
step:1443/1840 train_time:77921ms step_avg:54.00ms
step:1444/1840 train_time:78008ms step_avg:54.02ms
step:1445/1840 train_time:78094ms step_avg:54.04ms
step:1446/1840 train_time:78184ms step_avg:54.07ms
step:1447/1840 train_time:78270ms step_avg:54.09ms
step:1448/1840 train_time:78359ms step_avg:54.11ms
step:1449/1840 train_time:78445ms step_avg:54.14ms
step:1450/1840 train_time:78534ms step_avg:54.16ms
step:1451/1840 train_time:78620ms step_avg:54.18ms
step:1452/1840 train_time:78708ms step_avg:54.21ms
step:1453/1840 train_time:78795ms step_avg:54.23ms
step:1454/1840 train_time:78884ms step_avg:54.25ms
step:1455/1840 train_time:78970ms step_avg:54.27ms
step:1456/1840 train_time:79059ms step_avg:54.30ms
step:1457/1840 train_time:79146ms step_avg:54.32ms
step:1458/1840 train_time:79236ms step_avg:54.35ms
step:1459/1840 train_time:79323ms step_avg:54.37ms
step:1460/1840 train_time:79410ms step_avg:54.39ms
step:1461/1840 train_time:79496ms step_avg:54.41ms
step:1462/1840 train_time:79585ms step_avg:54.44ms
step:1463/1840 train_time:79671ms step_avg:54.46ms
step:1464/1840 train_time:79760ms step_avg:54.48ms
step:1465/1840 train_time:79846ms step_avg:54.50ms
step:1466/1840 train_time:79935ms step_avg:54.53ms
step:1467/1840 train_time:80021ms step_avg:54.55ms
step:1468/1840 train_time:80109ms step_avg:54.57ms
step:1469/1840 train_time:80195ms step_avg:54.59ms
step:1470/1840 train_time:80285ms step_avg:54.62ms
step:1471/1840 train_time:80370ms step_avg:54.64ms
step:1472/1840 train_time:80461ms step_avg:54.66ms
step:1473/1840 train_time:80547ms step_avg:54.68ms
step:1474/1840 train_time:80635ms step_avg:54.70ms
step:1475/1840 train_time:80721ms step_avg:54.73ms
step:1476/1840 train_time:80809ms step_avg:54.75ms
step:1477/1840 train_time:80894ms step_avg:54.77ms
step:1478/1840 train_time:80985ms step_avg:54.79ms
step:1479/1840 train_time:81069ms step_avg:54.81ms
step:1480/1840 train_time:81159ms step_avg:54.84ms
step:1481/1840 train_time:81246ms step_avg:54.86ms
step:1482/1840 train_time:81335ms step_avg:54.88ms
step:1483/1840 train_time:81422ms step_avg:54.90ms
step:1484/1840 train_time:81510ms step_avg:54.93ms
step:1485/1840 train_time:81596ms step_avg:54.95ms
step:1486/1840 train_time:81685ms step_avg:54.97ms
step:1487/1840 train_time:81770ms step_avg:54.99ms
step:1488/1840 train_time:81860ms step_avg:55.01ms
step:1489/1840 train_time:81946ms step_avg:55.03ms
step:1490/1840 train_time:82035ms step_avg:55.06ms
step:1491/1840 train_time:82121ms step_avg:55.08ms
step:1492/1840 train_time:82210ms step_avg:55.10ms
step:1493/1840 train_time:82297ms step_avg:55.12ms
step:1494/1840 train_time:82386ms step_avg:55.14ms
step:1495/1840 train_time:82471ms step_avg:55.16ms
step:1496/1840 train_time:82560ms step_avg:55.19ms
step:1497/1840 train_time:82646ms step_avg:55.21ms
step:1498/1840 train_time:82735ms step_avg:55.23ms
step:1499/1840 train_time:82821ms step_avg:55.25ms
step:1500/1840 train_time:82909ms step_avg:55.27ms
step:1500/1840 val_loss:3.4007 train_time:83009ms step_avg:55.34ms
step:1501/1840 train_time:83028ms step_avg:55.31ms
step:1502/1840 train_time:83086ms step_avg:55.32ms
step:1503/1840 train_time:83181ms step_avg:55.34ms
step:1504/1840 train_time:83269ms step_avg:55.37ms
step:1505/1840 train_time:83354ms step_avg:55.38ms
step:1506/1840 train_time:83442ms step_avg:55.41ms
step:1507/1840 train_time:83527ms step_avg:55.43ms
step:1508/1840 train_time:83614ms step_avg:55.45ms
step:1509/1840 train_time:83700ms step_avg:55.47ms
step:1510/1840 train_time:83789ms step_avg:55.49ms
step:1511/1840 train_time:83873ms step_avg:55.51ms
step:1512/1840 train_time:83962ms step_avg:55.53ms
step:1513/1840 train_time:84052ms step_avg:55.55ms
step:1514/1840 train_time:84144ms step_avg:55.58ms
step:1515/1840 train_time:84232ms step_avg:55.60ms
step:1516/1840 train_time:84321ms step_avg:55.62ms
step:1517/1840 train_time:84406ms step_avg:55.64ms
step:1518/1840 train_time:84493ms step_avg:55.66ms
step:1519/1840 train_time:84579ms step_avg:55.68ms
step:1520/1840 train_time:84669ms step_avg:55.70ms
step:1521/1840 train_time:84753ms step_avg:55.72ms
step:1522/1840 train_time:84843ms step_avg:55.74ms
step:1523/1840 train_time:84929ms step_avg:55.76ms
step:1524/1840 train_time:85019ms step_avg:55.79ms
step:1525/1840 train_time:85107ms step_avg:55.81ms
step:1526/1840 train_time:85196ms step_avg:55.83ms
step:1527/1840 train_time:85283ms step_avg:55.85ms
step:1528/1840 train_time:85373ms step_avg:55.87ms
step:1529/1840 train_time:85459ms step_avg:55.89ms
step:1530/1840 train_time:85547ms step_avg:55.91ms
step:1531/1840 train_time:85632ms step_avg:55.93ms
step:1532/1840 train_time:85720ms step_avg:55.95ms
step:1533/1840 train_time:85807ms step_avg:55.97ms
step:1534/1840 train_time:85895ms step_avg:55.99ms
step:1535/1840 train_time:85983ms step_avg:56.01ms
step:1536/1840 train_time:86074ms step_avg:56.04ms
step:1537/1840 train_time:86160ms step_avg:56.06ms
step:1538/1840 train_time:86251ms step_avg:56.08ms
step:1539/1840 train_time:86336ms step_avg:56.10ms
step:1540/1840 train_time:86426ms step_avg:56.12ms
step:1541/1840 train_time:86512ms step_avg:56.14ms
step:1542/1840 train_time:86600ms step_avg:56.16ms
step:1543/1840 train_time:86685ms step_avg:56.18ms
step:1544/1840 train_time:86773ms step_avg:56.20ms
step:1545/1840 train_time:86859ms step_avg:56.22ms
step:1546/1840 train_time:86949ms step_avg:56.24ms
step:1547/1840 train_time:87034ms step_avg:56.26ms
step:1548/1840 train_time:87123ms step_avg:56.28ms
step:1549/1840 train_time:87212ms step_avg:56.30ms
step:1550/1840 train_time:87300ms step_avg:56.32ms
step:1551/1840 train_time:87386ms step_avg:56.34ms
step:1552/1840 train_time:87473ms step_avg:56.36ms
step:1553/1840 train_time:87560ms step_avg:56.38ms
step:1554/1840 train_time:87651ms step_avg:56.40ms
step:1555/1840 train_time:87736ms step_avg:56.42ms
step:1556/1840 train_time:87824ms step_avg:56.44ms
step:1557/1840 train_time:87910ms step_avg:56.46ms
step:1558/1840 train_time:87998ms step_avg:56.48ms
step:1559/1840 train_time:88085ms step_avg:56.50ms
step:1560/1840 train_time:88174ms step_avg:56.52ms
step:1561/1840 train_time:88261ms step_avg:56.54ms
step:1562/1840 train_time:88350ms step_avg:56.56ms
step:1563/1840 train_time:88435ms step_avg:56.58ms
step:1564/1840 train_time:88524ms step_avg:56.60ms
step:1565/1840 train_time:88611ms step_avg:56.62ms
step:1566/1840 train_time:88699ms step_avg:56.64ms
step:1567/1840 train_time:88784ms step_avg:56.66ms
step:1568/1840 train_time:88873ms step_avg:56.68ms
step:1569/1840 train_time:88958ms step_avg:56.70ms
step:1570/1840 train_time:89049ms step_avg:56.72ms
step:1571/1840 train_time:89136ms step_avg:56.74ms
step:1572/1840 train_time:89225ms step_avg:56.76ms
step:1573/1840 train_time:89312ms step_avg:56.78ms
step:1574/1840 train_time:89400ms step_avg:56.80ms
step:1575/1840 train_time:89487ms step_avg:56.82ms
step:1576/1840 train_time:89576ms step_avg:56.84ms
step:1577/1840 train_time:89661ms step_avg:56.86ms
step:1578/1840 train_time:89750ms step_avg:56.88ms
step:1579/1840 train_time:89835ms step_avg:56.89ms
step:1580/1840 train_time:89923ms step_avg:56.91ms
step:1581/1840 train_time:90010ms step_avg:56.93ms
step:1582/1840 train_time:90098ms step_avg:56.95ms
step:1583/1840 train_time:90185ms step_avg:56.97ms
step:1584/1840 train_time:90274ms step_avg:56.99ms
step:1585/1840 train_time:90359ms step_avg:57.01ms
step:1586/1840 train_time:90449ms step_avg:57.03ms
step:1587/1840 train_time:90535ms step_avg:57.05ms
step:1588/1840 train_time:90624ms step_avg:57.07ms
step:1589/1840 train_time:90710ms step_avg:57.09ms
step:1590/1840 train_time:90799ms step_avg:57.11ms
step:1591/1840 train_time:90884ms step_avg:57.12ms
step:1592/1840 train_time:90973ms step_avg:57.14ms
step:1593/1840 train_time:91059ms step_avg:57.16ms
step:1594/1840 train_time:91149ms step_avg:57.18ms
step:1595/1840 train_time:91236ms step_avg:57.20ms
step:1596/1840 train_time:91325ms step_avg:57.22ms
step:1597/1840 train_time:91412ms step_avg:57.24ms
step:1598/1840 train_time:91500ms step_avg:57.26ms
step:1599/1840 train_time:91587ms step_avg:57.28ms
step:1600/1840 train_time:91675ms step_avg:57.30ms
step:1601/1840 train_time:91761ms step_avg:57.31ms
step:1602/1840 train_time:91849ms step_avg:57.33ms
step:1603/1840 train_time:91935ms step_avg:57.35ms
step:1604/1840 train_time:92024ms step_avg:57.37ms
step:1605/1840 train_time:92111ms step_avg:57.39ms
step:1606/1840 train_time:92201ms step_avg:57.41ms
step:1607/1840 train_time:92286ms step_avg:57.43ms
step:1608/1840 train_time:92375ms step_avg:57.45ms
step:1609/1840 train_time:92461ms step_avg:57.46ms
step:1610/1840 train_time:92550ms step_avg:57.48ms
step:1611/1840 train_time:92637ms step_avg:57.50ms
step:1612/1840 train_time:92726ms step_avg:57.52ms
step:1613/1840 train_time:92811ms step_avg:57.54ms
step:1614/1840 train_time:92899ms step_avg:57.56ms
step:1615/1840 train_time:92985ms step_avg:57.58ms
step:1616/1840 train_time:93074ms step_avg:57.60ms
step:1617/1840 train_time:93161ms step_avg:57.61ms
step:1618/1840 train_time:93250ms step_avg:57.63ms
step:1619/1840 train_time:93336ms step_avg:57.65ms
step:1620/1840 train_time:93427ms step_avg:57.67ms
step:1621/1840 train_time:93513ms step_avg:57.69ms
step:1622/1840 train_time:93602ms step_avg:57.71ms
step:1623/1840 train_time:93689ms step_avg:57.73ms
step:1624/1840 train_time:93777ms step_avg:57.74ms
step:1625/1840 train_time:93862ms step_avg:57.76ms
step:1626/1840 train_time:93950ms step_avg:57.78ms
step:1627/1840 train_time:94036ms step_avg:57.80ms
step:1628/1840 train_time:94124ms step_avg:57.82ms
step:1629/1840 train_time:94212ms step_avg:57.83ms
step:1630/1840 train_time:94300ms step_avg:57.85ms
step:1631/1840 train_time:94387ms step_avg:57.87ms
step:1632/1840 train_time:94475ms step_avg:57.89ms
step:1633/1840 train_time:94562ms step_avg:57.91ms
step:1634/1840 train_time:94649ms step_avg:57.92ms
step:1635/1840 train_time:94736ms step_avg:57.94ms
step:1636/1840 train_time:94826ms step_avg:57.96ms
step:1637/1840 train_time:94912ms step_avg:57.98ms
step:1638/1840 train_time:95000ms step_avg:58.00ms
step:1639/1840 train_time:95086ms step_avg:58.01ms
step:1640/1840 train_time:95174ms step_avg:58.03ms
step:1641/1840 train_time:95260ms step_avg:58.05ms
step:1642/1840 train_time:95349ms step_avg:58.07ms
step:1643/1840 train_time:95436ms step_avg:58.09ms
step:1644/1840 train_time:95524ms step_avg:58.10ms
step:1645/1840 train_time:95611ms step_avg:58.12ms
step:1646/1840 train_time:95700ms step_avg:58.14ms
step:1647/1840 train_time:95785ms step_avg:58.16ms
step:1648/1840 train_time:95873ms step_avg:58.18ms
step:1649/1840 train_time:95959ms step_avg:58.19ms
step:1650/1840 train_time:96049ms step_avg:58.21ms
step:1651/1840 train_time:96135ms step_avg:58.23ms
step:1652/1840 train_time:96224ms step_avg:58.25ms
step:1653/1840 train_time:96312ms step_avg:58.26ms
step:1654/1840 train_time:96400ms step_avg:58.28ms
step:1655/1840 train_time:96486ms step_avg:58.30ms
step:1656/1840 train_time:96574ms step_avg:58.32ms
step:1657/1840 train_time:96661ms step_avg:58.33ms
step:1658/1840 train_time:96750ms step_avg:58.35ms
step:1659/1840 train_time:96836ms step_avg:58.37ms
step:1660/1840 train_time:96924ms step_avg:58.39ms
step:1661/1840 train_time:97010ms step_avg:58.40ms
step:1662/1840 train_time:97099ms step_avg:58.42ms
step:1663/1840 train_time:97185ms step_avg:58.44ms
step:1664/1840 train_time:97274ms step_avg:58.46ms
step:1665/1840 train_time:97359ms step_avg:58.47ms
step:1666/1840 train_time:97449ms step_avg:58.49ms
step:1667/1840 train_time:97535ms step_avg:58.51ms
step:1668/1840 train_time:97624ms step_avg:58.53ms
step:1669/1840 train_time:97710ms step_avg:58.54ms
step:1670/1840 train_time:97798ms step_avg:58.56ms
step:1671/1840 train_time:97885ms step_avg:58.58ms
step:1672/1840 train_time:97974ms step_avg:58.60ms
step:1673/1840 train_time:98059ms step_avg:58.61ms
step:1674/1840 train_time:98147ms step_avg:58.63ms
step:1675/1840 train_time:98234ms step_avg:58.65ms
step:1676/1840 train_time:98322ms step_avg:58.66ms
step:1677/1840 train_time:98410ms step_avg:58.68ms
step:1678/1840 train_time:98499ms step_avg:58.70ms
step:1679/1840 train_time:98584ms step_avg:58.72ms
step:1680/1840 train_time:98673ms step_avg:58.73ms
step:1681/1840 train_time:98759ms step_avg:58.75ms
step:1682/1840 train_time:98848ms step_avg:58.77ms
step:1683/1840 train_time:98934ms step_avg:58.78ms
step:1684/1840 train_time:99022ms step_avg:58.80ms
step:1685/1840 train_time:99110ms step_avg:58.82ms
step:1686/1840 train_time:99198ms step_avg:58.84ms
step:1687/1840 train_time:99284ms step_avg:58.85ms
step:1688/1840 train_time:99373ms step_avg:58.87ms
step:1689/1840 train_time:99459ms step_avg:58.89ms
step:1690/1840 train_time:99548ms step_avg:58.90ms
step:1691/1840 train_time:99633ms step_avg:58.92ms
step:1692/1840 train_time:99721ms step_avg:58.94ms
step:1693/1840 train_time:99808ms step_avg:58.95ms
step:1694/1840 train_time:99896ms step_avg:58.97ms
step:1695/1840 train_time:99982ms step_avg:58.99ms
step:1696/1840 train_time:100071ms step_avg:59.00ms
step:1697/1840 train_time:100156ms step_avg:59.02ms
step:1698/1840 train_time:100247ms step_avg:59.04ms
step:1699/1840 train_time:100333ms step_avg:59.05ms
step:1700/1840 train_time:100421ms step_avg:59.07ms
step:1701/1840 train_time:100509ms step_avg:59.09ms
step:1702/1840 train_time:100596ms step_avg:59.10ms
step:1703/1840 train_time:100682ms step_avg:59.12ms
step:1704/1840 train_time:100772ms step_avg:59.14ms
step:1705/1840 train_time:100856ms step_avg:59.15ms
step:1706/1840 train_time:100948ms step_avg:59.17ms
step:1707/1840 train_time:101033ms step_avg:59.19ms
step:1708/1840 train_time:101121ms step_avg:59.20ms
step:1709/1840 train_time:101209ms step_avg:59.22ms
step:1710/1840 train_time:101297ms step_avg:59.24ms
step:1711/1840 train_time:101383ms step_avg:59.25ms
step:1712/1840 train_time:101472ms step_avg:59.27ms
step:1713/1840 train_time:101557ms step_avg:59.29ms
step:1714/1840 train_time:101647ms step_avg:59.30ms
step:1715/1840 train_time:101732ms step_avg:59.32ms
step:1716/1840 train_time:101820ms step_avg:59.34ms
step:1717/1840 train_time:101908ms step_avg:59.35ms
step:1718/1840 train_time:101995ms step_avg:59.37ms
step:1719/1840 train_time:102082ms step_avg:59.38ms
step:1720/1840 train_time:102171ms step_avg:59.40ms
step:1721/1840 train_time:102256ms step_avg:59.42ms
step:1722/1840 train_time:102345ms step_avg:59.43ms
step:1723/1840 train_time:102431ms step_avg:59.45ms
step:1724/1840 train_time:102519ms step_avg:59.47ms
step:1725/1840 train_time:102606ms step_avg:59.48ms
step:1726/1840 train_time:102695ms step_avg:59.50ms
step:1727/1840 train_time:102781ms step_avg:59.51ms
step:1728/1840 train_time:102871ms step_avg:59.53ms
step:1729/1840 train_time:102955ms step_avg:59.55ms
step:1730/1840 train_time:103045ms step_avg:59.56ms
step:1731/1840 train_time:103131ms step_avg:59.58ms
step:1732/1840 train_time:103220ms step_avg:59.60ms
step:1733/1840 train_time:103306ms step_avg:59.61ms
step:1734/1840 train_time:103394ms step_avg:59.63ms
step:1735/1840 train_time:103482ms step_avg:59.64ms
step:1736/1840 train_time:103571ms step_avg:59.66ms
step:1737/1840 train_time:103656ms step_avg:59.68ms
step:1738/1840 train_time:103746ms step_avg:59.69ms
step:1739/1840 train_time:103832ms step_avg:59.71ms
step:1740/1840 train_time:103920ms step_avg:59.72ms
step:1741/1840 train_time:104006ms step_avg:59.74ms
step:1742/1840 train_time:104094ms step_avg:59.76ms
step:1743/1840 train_time:104181ms step_avg:59.77ms
step:1744/1840 train_time:104270ms step_avg:59.79ms
step:1745/1840 train_time:104355ms step_avg:59.80ms
step:1746/1840 train_time:104446ms step_avg:59.82ms
step:1747/1840 train_time:104533ms step_avg:59.84ms
step:1748/1840 train_time:104621ms step_avg:59.85ms
step:1749/1840 train_time:104707ms step_avg:59.87ms
step:1750/1840 train_time:104794ms step_avg:59.88ms
step:1750/1840 val_loss:3.3028 train_time:104898ms step_avg:59.94ms
step:1751/1840 train_time:104916ms step_avg:59.92ms
step:1752/1840 train_time:104975ms step_avg:59.92ms
step:1753/1840 train_time:105066ms step_avg:59.94ms
step:1754/1840 train_time:105155ms step_avg:59.95ms
step:1755/1840 train_time:105240ms step_avg:59.97ms
step:1756/1840 train_time:105329ms step_avg:59.98ms
step:1757/1840 train_time:105414ms step_avg:60.00ms
step:1758/1840 train_time:105503ms step_avg:60.01ms
step:1759/1840 train_time:105587ms step_avg:60.03ms
step:1760/1840 train_time:105676ms step_avg:60.04ms
step:1761/1840 train_time:105761ms step_avg:60.06ms
step:1762/1840 train_time:105851ms step_avg:60.07ms
step:1763/1840 train_time:105939ms step_avg:60.09ms
step:1764/1840 train_time:106032ms step_avg:60.11ms
step:1765/1840 train_time:106118ms step_avg:60.12ms
step:1766/1840 train_time:106207ms step_avg:60.14ms
step:1767/1840 train_time:106293ms step_avg:60.15ms
step:1768/1840 train_time:106379ms step_avg:60.17ms
step:1769/1840 train_time:106466ms step_avg:60.18ms
step:1770/1840 train_time:106555ms step_avg:60.20ms
step:1771/1840 train_time:106639ms step_avg:60.21ms
step:1772/1840 train_time:106730ms step_avg:60.23ms
step:1773/1840 train_time:106816ms step_avg:60.25ms
step:1774/1840 train_time:106907ms step_avg:60.26ms
step:1775/1840 train_time:106996ms step_avg:60.28ms
step:1776/1840 train_time:107084ms step_avg:60.30ms
step:1777/1840 train_time:107172ms step_avg:60.31ms
step:1778/1840 train_time:107259ms step_avg:60.33ms
step:1779/1840 train_time:107344ms step_avg:60.34ms
step:1780/1840 train_time:107433ms step_avg:60.36ms
step:1781/1840 train_time:107518ms step_avg:60.37ms
step:1782/1840 train_time:107605ms step_avg:60.38ms
step:1783/1840 train_time:107692ms step_avg:60.40ms
step:1784/1840 train_time:107780ms step_avg:60.41ms
step:1785/1840 train_time:107867ms step_avg:60.43ms
step:1786/1840 train_time:107958ms step_avg:60.45ms
step:1787/1840 train_time:108045ms step_avg:60.46ms
step:1788/1840 train_time:108135ms step_avg:60.48ms
step:1789/1840 train_time:108221ms step_avg:60.49ms
step:1790/1840 train_time:108309ms step_avg:60.51ms
step:1791/1840 train_time:108396ms step_avg:60.52ms
step:1792/1840 train_time:108483ms step_avg:60.54ms
step:1793/1840 train_time:108569ms step_avg:60.55ms
step:1794/1840 train_time:108657ms step_avg:60.57ms
step:1795/1840 train_time:108744ms step_avg:60.58ms
step:1796/1840 train_time:108834ms step_avg:60.60ms
step:1797/1840 train_time:108920ms step_avg:60.61ms
step:1798/1840 train_time:109009ms step_avg:60.63ms
step:1799/1840 train_time:109097ms step_avg:60.64ms
step:1800/1840 train_time:109186ms step_avg:60.66ms
step:1801/1840 train_time:109273ms step_avg:60.67ms
step:1802/1840 train_time:109360ms step_avg:60.69ms
step:1803/1840 train_time:109446ms step_avg:60.70ms
step:1804/1840 train_time:109536ms step_avg:60.72ms
step:1805/1840 train_time:109623ms step_avg:60.73ms
step:1806/1840 train_time:109713ms step_avg:60.75ms
step:1807/1840 train_time:109799ms step_avg:60.76ms
step:1808/1840 train_time:109887ms step_avg:60.78ms
step:1809/1840 train_time:109977ms step_avg:60.79ms
step:1810/1840 train_time:110067ms step_avg:60.81ms
step:1811/1840 train_time:110154ms step_avg:60.82ms
step:1812/1840 train_time:110242ms step_avg:60.84ms
step:1813/1840 train_time:110328ms step_avg:60.85ms
step:1814/1840 train_time:110417ms step_avg:60.87ms
step:1815/1840 train_time:110501ms step_avg:60.88ms
step:1816/1840 train_time:110591ms step_avg:60.90ms
step:1817/1840 train_time:110676ms step_avg:60.91ms
step:1818/1840 train_time:110766ms step_avg:60.93ms
step:1819/1840 train_time:110852ms step_avg:60.94ms
step:1820/1840 train_time:110940ms step_avg:60.96ms
step:1821/1840 train_time:111028ms step_avg:60.97ms
step:1822/1840 train_time:111117ms step_avg:60.99ms
step:1823/1840 train_time:111204ms step_avg:61.00ms
step:1824/1840 train_time:111294ms step_avg:61.02ms
step:1825/1840 train_time:111379ms step_avg:61.03ms
step:1826/1840 train_time:111469ms step_avg:61.05ms
step:1827/1840 train_time:111556ms step_avg:61.06ms
step:1828/1840 train_time:111643ms step_avg:61.07ms
step:1829/1840 train_time:111729ms step_avg:61.09ms
step:1830/1840 train_time:111820ms step_avg:61.10ms
step:1831/1840 train_time:111908ms step_avg:61.12ms
step:1832/1840 train_time:111997ms step_avg:61.13ms
step:1833/1840 train_time:112084ms step_avg:61.15ms
step:1834/1840 train_time:112173ms step_avg:61.16ms
step:1835/1840 train_time:112260ms step_avg:61.18ms
step:1836/1840 train_time:112349ms step_avg:61.19ms
step:1837/1840 train_time:112436ms step_avg:61.21ms
step:1838/1840 train_time:112525ms step_avg:61.22ms
step:1839/1840 train_time:112612ms step_avg:61.24ms
step:1840/1840 train_time:112700ms step_avg:61.25ms
step:1840/1840 val_loss:3.2780 train_time:112802ms step_avg:61.31ms
peak memory allocated: 28507 MiB reserved: 43838 MiB

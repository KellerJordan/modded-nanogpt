import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from collections import defaultdict
from itertools import accumulate
from pathlib import Path
import gc

os.environ["PYTORCH_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device=f"cuda:{os.environ['LOCAL_RANK']}", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng

@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor, split_baddbmm: bool = False):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    # Select batched vs unbatched
    if split_baddbmm:
        BX_matmul = torch.bmm if X.ndim > 2 else torch.mm
    else:
        aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A

        # Referencing X twice causes pytorch to make a defensive copy,
        # resulting in a cudaMemcpyAsync in baddbmm.
        # For large matrices (i.e., the mlp weights), it's faster to split
        # the operation into two kernels to avoid this.
        if split_baddbmm:
            BX_matmul(B, X, out=C)  # C = B @ X
            C.add_(X, alpha=a)      # C = C + a*X  (in-place, X only read)
        else:
            aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X

        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X


# -----------------------------------------------------------------------------
# Compiled helpers for NorMuon by @chrisjmccormick

@torch.compile(dynamic=False, fullgraph=True)
def cautious_wd_and_update_inplace(p, mantissa, grad, wd_tensor, lr_tensor):
    """
    Cautious weight decay + parameter update. wd_tensor and lr_tensor are 0-D CPU tensors.
    Mantissa is tracked to enable higher precision updates on bfloat16 parameters.
    bfloat16 format: 1 sign bit + 8 exponent bits + 7 mantissa bits = 16 bits total
    float32 format: 1 sign bit + 8 exponent bits + 23 mantissa bits = 32 bits total
    """
    assert p.dtype == mantissa.dtype == torch.uint16
    grad = grad.float()
    wd_factor = wd_tensor.to(torch.float32)
    lr_factor = lr_tensor.to(torch.float32)
    p_precise_raw = (p.to(torch.uint32) << 16) | mantissa.to(torch.uint32)
    p_precise = p_precise_raw.view(torch.float32)
    mask = (grad * p_precise) >= 0
    p_precise.copy_(p_precise - (p_precise * mask * wd_factor * lr_factor) - (grad * lr_factor))
    p.copy_((p_precise_raw >> 16).to(torch.uint16))
    mantissa.copy_(p_precise_raw.to(torch.uint16))

@torch.compile(dynamic=False, fullgraph=True)
def apply_normuon_variance_reduction(v_chunk, second_momentum_buffer, beta2, red_dim):
    """NorMuon variance reduction. Algebraically fuses the normalization steps to minimize memory ops."""
    v_mean = v_chunk.float().square().mean(dim=red_dim, keepdim=True)
    red_dim_size = v_chunk.size(red_dim)
    v_norm_sq = v_mean.sum(dim=(-2, -1), keepdim=True).mul_(red_dim_size)
    v_norm = v_norm_sq.sqrt_()
    second_momentum_buffer.lerp_(v_mean.to(dtype=second_momentum_buffer.dtype), 1 - beta2)
    step_size = second_momentum_buffer.clamp_min(1e-10).rsqrt_()
    scaled_sq_sum = (v_mean * red_dim_size) * step_size.float().square()
    v_norm_new = scaled_sq_sum.sum(dim=(-2, -1), keepdim=True).sqrt_()
    final_scale = step_size * (v_norm / v_norm_new.clamp_min_(1e-10))
    return v_chunk.mul_(final_scale.type_as(v_chunk))


# -----------------------------------------------------------------------------
# NorMuon optimizer

class NorMuon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).

    Differences from standard Muon:
    - Newton-Shulz is replaced with Polar Express for the orthogonalization step
    - NorMuon adds a low-rank variance estimator similar to Adafactor. https://arxiv.org/pdf/2510.05491
    - small 1D parameters handled here instead of in Adam
    - Cautious weight decay, a gated version of decoupled weight decay
    - Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as
    needed on the forward pass. This enables attn and mlp weights to be contained within the same
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn.
    The scheduling is:
        1. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        2. reduce scatter attn/mlp round 2 (16 mlp params)
        3. wait on step 1, then compute update of 1 and schedule all gather
        4. wait on step 2, then compute update of 2 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        5. wait for each all gather to complete and update params
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, beta2=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, beta2=beta2)
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def reset(self):
        # expose a reset for clearing buffers
        for group in self.param_groups:
            if "momentum_buffer" in group:
                group["momentum_buffer"].zero_()
                group["mantissa"].zero_()
                group["second_momentum_buffer"].zero_()

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per module.
        """
        groups = defaultdict(list)
        for param in params:
            groups[param.label].append(param)

        param_groups = []
        for module_name, group_params in groups.items():
            chunk_size = (len(group_params) + self.world_size - 1) // self.world_size
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))

        return param_groups

    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn
        and mlp params when a param group is split across GPUs.
        """
        params_list = list(params)
        module_group_order = ['attn', 'mlp']
        group_sizes = [16, 16]  # 10 attn + 6 mlp, then 16 mlp
        params_list.sort(key=lambda x: module_group_order.index(x.label))

        idx = 0
        assert len(params_list) == sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            chunk_size = (size + self.world_size - 1) // self.world_size
            group_params = params_list[idx: idx + size]
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))
            idx += size

        return param_groups

    def step(self):
        self.step_p1()
        self.step_p2()
        self.step_p3()
        
    @torch.no_grad()
    def step_p1(self):
        """
        Part 1: Launch distributed reduce_scatter operations for parameter groups.
        """
        rank = dist.get_rank()
        self.group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            stacked_grads = torch.empty(
                (padded_num_params, *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device
            )
            for i, p in enumerate(params):
                stacked_grads[i].copy_(p.grad, non_blocking=True)
            if len(params) < padded_num_params:
                stacked_grads[len(params):].zero_()

            grad_chunk = torch.empty_like(stacked_grads[:chunk_size])

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            self.group_infos.append(dict(grad_chunk=grad_chunk, reduce_future=reduce_future))
    
    @torch.no_grad()
    def step_p2(self):
        """
        Part 2: Compute gradient updates and launch all_gather operations
        Wait for all gather to complete for all parameter groups except final one
        """
        # Efficient distributed step by @YouJiacheng, @KonstantinWilleke, @alexrgilbert,
        # @adricarda, @tuttyfrutyee, @vdlad, @ryanyang0, @vagrawal, @varunneal, @chrisjmccormick
        rank = dist.get_rank()
        group_infos = self.group_infos
        
        self.all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = group["params"]
            grad_chunk = info["grad_chunk"].float()
            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            start_idx = rank * chunk_size
            module_idx = start_idx if start_idx < len(params) else 0

            num_params = min(chunk_size, max(0, len(params) - start_idx))  # num params for this rank

            if "momentum_buffer" not in group:
                group["momentum_buffer"]  = torch.zeros_like(grad_chunk[:num_params], dtype=torch.float32)
                
            momentum_buffer = group["momentum_buffer"]
            # Apply momentum update to the persistent momentum buffer in-place
            momentum_buffer.lerp_(grad_chunk[:num_params], 1 - group["momentum"])
            updated_grads = grad_chunk[:num_params].lerp_(momentum_buffer, group["momentum"])

            grad_shape = updated_grads.shape
            if params[module_idx].label == 'attn':
                for p in params[module_idx:module_idx + num_params]:
                    assert p.label == 'attn'
                updated_grads = updated_grads.view(4 * grad_shape[0], grad_shape[1] // 4, grad_shape[2])

            ref_param = params[module_idx]
            param_shape = ref_param.shape

            if "second_momentum_buffer" not in group:
                group["second_momentum_buffer"] = (torch.zeros_like(updated_grads[..., :, :1], dtype=torch.float32)
                    if param_shape[-2] >= param_shape[-1] else torch.zeros_like(updated_grads[..., :1, :])
                )
            second_momentum_buffer = group["second_momentum_buffer"]

            if "param_lr_cpu" not in group:
                # Define multipliers for ALL params in this group (global, not per-shard)
                lr_mults = []
                wd_mults = []
                for p in params:
                    # Increase learning rate for modules with larger inputs than outputs.
                    # This shape check also assumes rows=input, columns=output, so take care
                    # when changing memory layouts. @chrisjmccormick
                    shape = p.shape
                    if len(shape) >= 2:
                        shape_mult = max(1.0, shape[-2] / shape[-1]) ** 0.5
                    else:
                        shape_mult = 1.0
                    lr_mults.append(shape_mult * getattr(p, "lr_mul", 1.0))
                    wd_mults.append(getattr(p, "wd_mul", 1.0))
                # Define as cpu tensors to enable Inductor constant folding
                group["param_lr_cpu"] = torch.tensor(lr_mults, dtype=torch.float32, device="cpu")
                group["param_wd_cpu"] = torch.tensor(wd_mults, dtype=torch.float32, device="cpu")

            eff_lr_all = group["param_lr_cpu"] * group["lr"]
            eff_wd_all = group["param_wd_cpu"] * group["weight_decay"] * group["lr"]

            # Slice the portion corresponding to this rank's shard
            eff_lr_cpu = eff_lr_all[module_idx:module_idx + num_params]
            eff_wd_cpu = eff_wd_all[module_idx:module_idx + num_params]

            # Compute zeropower for the entire chunk in a single, batched call.
            if num_params == 0:
                v_chunk = updated_grads
            else:
                v_chunk = polar_express(updated_grads, split_baddbmm=(ref_param.label == 'mlp'))

            # Note that the head orientation in O is transposed relative to QKV, so red_dim
            # is 'incorrect' for O. However, correcting this showed no improvement. @chrisjmccormick
            red_dim = -1 if param_shape[-2] >= param_shape[-1] else -2

            v_chunk = apply_normuon_variance_reduction(
                v_chunk, second_momentum_buffer, group["beta2"], red_dim
            )

            v_chunk = v_chunk.view(grad_shape)

            # # "Cautious" weight decay (https://arxiv.org/abs/2510.12402)
            updated_params = torch.empty_like(grad_chunk, dtype=torch.bfloat16)
            if num_params > 0:
                # Work on a stacked copy to avoid touching original params
                param_chunk = torch.stack(params[module_idx:module_idx + num_params])

                if "mantissa" not in group:
                    group["mantissa"] = torch.zeros_like(param_chunk, dtype=torch.uint16)
                mantissa = group["mantissa"]

                for local_idx in range(num_params):
                    cautious_wd_and_update_inplace(
                        param_chunk[local_idx].view(torch.uint16),
                        mantissa[local_idx],
                        v_chunk[local_idx],
                        eff_wd_cpu[local_idx],
                        eff_lr_cpu[local_idx]
                    )
            else:
                param_chunk = torch.zeros_like(v_chunk)

            updated_params[:num_params].copy_(param_chunk)
            if num_params < chunk_size:
                updated_params[num_params:].zero_()

            stacked_params = torch.empty(
                (padded_num_params, *param_shape),
                dtype=updated_params.dtype,
                device=updated_params.device,
            )

            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_params, async_op=True
            ).get_future()

            self.all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete for all except final and copy results back into original parameter tensors.
        for info in self.all_gather_infos[:-1]:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)

    @torch.no_grad()
    def step_p3(self):
        """
        Part 3: Wait for final all gather to complete and copy results back into original parameter tensors
        """
        info = self.all_gather_infos[-1]
        info["gather_future"].wait()
        stacked_params = info["stacked_params"]
        orig_params = info["orig_params"]

        unstacked_params = torch.unbind(stacked_params)
        for i, p in enumerate(orig_params):
            p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, label_order: list[str], betas: list[list[float]], lr: float = 1e-3, eps: float = 1e-8, weight_decay: float = 0.01):
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        defaults = dict(lr=lr, eps=eps, weight_decay=weight_decay)
        params = list(params)
        # Group by label, with explicit ordering for execution control.
        params_by_label = defaultdict(list)
        for p in params:
            params_by_label[getattr(p, 'label', None)].append(p)
        param_groups = []
        for idx, label in enumerate(label_order):
            if label in params_by_label:
                param_groups.append(dict(params=params_by_label[label], betas=betas[idx]))
        # include any unlabeled params at the end (processed last)
        if None in params_by_label:
            param_groups.append(dict(params=params_by_label[None]))
        super().__init__(param_groups, defaults)
        # init state: small params (numel < 1024) use full-sized state, others use sharded
        for p in params:
            chunk = p if p.numel() < 1024 else p[:p.size(0) // self.world_size]
            exp_avg = torch.zeros_like(chunk, dtype=torch.float32, device=p.device)
            self.state[p] = dict(step=0, exp_avg=exp_avg, exp_avg_sq=torch.zeros_like(exp_avg))

        # tag the final param for optimizer pipelining, run all gather after muon copy
        param_groups[-1]['params'][-1].is_final_param = True
        
        # DistributedAdam implementation by @vagrawal, @akash5474
        self.should_sync = False
        self._reduce_scatter_hooks = []
        self._reduce_scatter_futures = {}
        # 0-D CPU tensors to avoid recompilation in _update_step
        self._step_size_t = torch.tensor(0.0, dtype=torch.float32, device="cpu")
        self._eff_wd_t = torch.tensor(0.0, dtype=torch.float32, device="cpu")
        self.register_backward_hooks()

    def register_backward_hooks(self):
        for group in self.param_groups:
            for param in group["params"]:
                self._reduce_scatter_hooks.append(param.register_post_accumulate_grad_hook(self._sync_gradient))

    def load_state_dict(self, state_dict):
        """Override to preserve optimizer state dtypes (avoid BFloat16->Float32 cast that causes recompilation)."""
        # Save original state dtypes before loading
        original_dtypes = {}
        for p, s in self.state.items():
            original_dtypes[p] = {k: v.dtype for k, v in s.items() if isinstance(v, torch.Tensor)}
        
        # Call parent load_state_dict (which may cast dtypes to match param dtype)
        super().load_state_dict(state_dict)
        
        # Restore original dtypes
        for p, s in self.state.items():
            if p in original_dtypes:
                for k, v in s.items():
                    if isinstance(v, torch.Tensor) and k in original_dtypes[p]:
                        if v.dtype != original_dtypes[p][k]:
                            s[k] = v.to(original_dtypes[p][k])

    @torch.no_grad()
    def _sync_gradient(self, param):
        if not self.should_sync:
            return

        grad = param.grad
        if param.numel() < 1024:
            # Small params: use all_reduce (no scatter/gather needed)
            self._reduce_scatter_futures[param] = (
                dist.all_reduce(grad, op=dist.ReduceOp.AVG, async_op=True).get_future(),
                grad
            )
        else:
            rank_size = grad.shape[0] // self.world_size
            if grad is not None:
                grad_slice = torch.empty_like(grad[:rank_size])
                self._reduce_scatter_futures[param] = (
                    dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future(),
                    grad_slice
                )

    def copy_lm_to_embed(self):
        # run at 2/3 of training
        lm_head = self.param_groups[0]['params'][0]
        embed = self.param_groups[-2]['params'][0]
        lm_head_state = self.state[lm_head]
        embed_state = self.state[embed]
        embed_state['step'] = lm_head_state['step']
        embed_state['exp_avg'] = lm_head_state['exp_avg'].clone()
        embed_state['exp_avg_sq'] = lm_head_state['exp_avg_sq'].clone()
        embed.data.copy_(lm_head.data)

    @staticmethod
    @torch.compile(dynamic=False, fullgraph=True)
    def _update_step(p_slice, g_slice, exp_avg, exp_avg_sq, beta1, beta2, eps, step_size_t, eff_wd_t):
        """Compiled Adam update step. step_size_t and eff_wd_t are 0-D CPU tensors to avoid recompilation."""
        exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)  # exp_avg = beta1 * exp_avg + (1 - beta1) * g_slice
        exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)  # exp_avg_sq = beta2 * exp_avg_sq + (1 - beta2) * g_slice^2
        # compute step
        update = exp_avg.div(exp_avg_sq.sqrt().add_(eps)).mul_(step_size_t)  # update = (exp_avg / (sqrt(exp_avg_sq) + eps)) * step_size
        # cautious weight decay
        mask = (update * p_slice) > 0
        update.addcmul_(p_slice, mask, value=eff_wd_t)  # update += eff_wd_t * p_slice * mask
        p_slice.add_(other=update, alpha=-1.0)  # p_slice -= update

    @torch.no_grad()
    def step(self, muon_opt):
        muon_opt.step_p1()
        rank = dist.get_rank()
        all_gather_futures: list[torch.Future] = []

        last_param = None
        last_p_slice = None
        for group in self.param_groups:      
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            for param in group['params']:
                if param not in self._reduce_scatter_futures:
                    continue

                fut, g_slice = self._reduce_scatter_futures[param]
                fut.wait()

                is_small = param.numel() < 1024
                if is_small:
                    # Small params: g_slice is actually full grad, p_slice is full param
                    p_slice = param
                else:
                    rank_size = param.shape[0] // self.world_size
                    p_slice = param[rank * rank_size:(rank + 1) * rank_size]

                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]
                state["step"] += 1
                t = state["step"]
                
                # Pre-compute changing values as 0-D CPU tensors to avoid recompilation.
                # `.fill_(value)` is the same as "= value", but doesn't change the tensor object.
                bias1, bias2 = 1 - beta1 ** t, 1 - beta2 ** t
                self._step_size_t.fill_(lr * (bias2 ** 0.5 / bias1))
                self._eff_wd_t.fill_(lr * lr * wd * getattr(param, "wd_mul", 1.0)) # `lr` included twice to serve as weight decay schedule.

                DistAdam._update_step(p_slice, g_slice, state["exp_avg"], state["exp_avg_sq"],
                                      beta1, beta2, eps, self._step_size_t, self._eff_wd_t)

                if not is_small:
                    if getattr(param, "is_final_param", False):
                        last_param = param
                        last_p_slice = p_slice
                    else:
                        all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())
        self._reduce_scatter_futures.clear()

        muon_opt.step_p2()
        torch.futures.collect_all(all_gather_futures).wait()

        if last_param is not None:
            last_all_gather_future = dist.all_gather_into_tensor(last_param, last_p_slice, async_op=True).get_future()
        muon_opt.step_p3()
        torch.futures.collect_all([last_all_gather_future]).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        with torch.no_grad():
            self.weight.zero_()  # @Grad62304977 and others

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))


# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float
    key_offset: bool
    attn_gate_w: torch.Tensor
    ve_gate_w: torch.Tensor

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = self.dim ** -0.5
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKVO weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # Simplified layout by @chrisjmccormick
        self.qkvo_w = nn.Parameter(torch.empty(self.dim * 4, self.hdim, dtype=torch.bfloat16))
        # label all modules for explicit optimizer grouping
        self.qkvo_w.label = 'attn'

        with torch.no_grad():
            self.qkvo_w[:self.dim * 3].uniform_(-bound, bound)  # init QKV weights
            self.qkvo_w[self.dim * 3:].zero_()  # init O weights to zero

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas, key_offset = attn_args.ve, attn_args.sa_lambdas, attn_args.key_offset
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size
        # sparse gated attention to enable context based no-op by @classiclarryd
        # only include gates on layers with value embeds used on forward pass
        attn_gate_w, ve_gate_w = attn_args.attn_gate_w, attn_args.ve_gate_w

        q, k, v = F.linear(x, sa_lambdas[0] * self.qkvo_w[:self.dim * 3].type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if key_offset:
            # shift keys forward for the stationary head dims. Enables 1-layer induction.
            k[:, 1:, :, self.head_dim // 4:self.head_dim // 2] = k[:, :-1, :, self.head_dim // 4:self.head_dim // 2]
            k[:, 1:, :, 3 * self.head_dim // 4:] = k[:, :-1, :, 3 * self.head_dim // 4:]
        if ve is not None:
            ve_gate_out = 2 * torch.sigmoid(F.linear(x[..., :12], ve_gate_w)).view(B, T, self.num_heads, 1)
            v = v + ve_gate_out * ve.view_as(v) # @ KoszarskyB & @Grad62304977

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens,
                                                        max_seqlen_q=max_len, max_seqlen_k=max_len,
                                                        causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(F.linear(x[..., :12], attn_gate_w)).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, sa_lambdas[1] * self.qkvo_w[self.dim * 3:].type_as(y))  # sa_lambdas[1] pre-multiplied to O @shenberg
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # Transposed layout to match attention weights
        self.c_fc = nn.Parameter(torch.empty(hdim, dim, dtype=torch.bfloat16))
        self.c_proj = nn.Parameter(torch.empty(hdim, dim, dtype=torch.bfloat16))
        # label all modules for explicit optimizer grouping
        self.c_fc.label = 'mlp'
        self.c_proj.label = 'mlp'
        self.c_proj.lr_mul = 2.

        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.T.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.6 (the 7th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads, layer_idx) if layer_idx != 6 else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim)

    def forward(self, x: Tensor, attn_args: AttnArgs):
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

@dataclass
class ForwardScheduleConfig:
    mtp_weights: torch.Tensor
    ws_short: int
    ws_long: int

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        self.num_layers = num_layers
        vocab_size = next_multiple_of_n(vocab_size, n=128)

        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.label = 'smear_gate'
        self.smear_gate.weight.lr_mul = 0.01
        self.smear_gate.weight.wd_mul = 0.0

        self.skip_gate = CastedLinear(12, 1)
        self.skip_gate.weight.label = 'skip_gate'
        self.skip_gate.weight.lr_mul = 0.05
        self.skip_gate.weight.wd_mul = 0.0

        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        for embed in self.value_embeds:
            nn.init.zeros_(embed.weight)
        for ve in self.value_embeds:
            ve.weight.label = 'value_embed'
        
        # parameter banks for attention and value embedding gate weights
        self.attn_gate_bank = nn.Parameter(torch.zeros(10, num_heads, 12)) # 10 layers
        self.attn_gate_bank.label = 'attn_gate_bank'
        self.ve_gate_bank = nn.Parameter(torch.zeros(5, num_heads, 12)) # 5 layers
        self.ve_gate_bank.label = 've_gate_bank'

        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)

        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=100/448, w_s=1.6/448, grad_s=0.75/448)
        nn.init.normal_(self.lm_head.weight, mean=0, std=0.005)
        self.lm_head.weight.label = 'lm_head'

        self.embed = nn.Embedding(vocab_size, model_dim)
        self.embed.weight.label = 'embed'

        # x0_lambdas separated out for different optimizer treatment (no beta smoothing)
        self.x0_lambdas = nn.Parameter(torch.zeros(num_layers))
        self.x0_lambdas.label = 'x0_lambdas'
        self.x0_lambdas.lr_mul = 5.0
        self.x0_lambdas.wd_mul = 0.0

        pad = (-num_layers * 3 - 3) % dist.get_world_size()  # updated: 3*num_layers instead of 4*
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    1.1 * torch.ones(num_layers),  # resid lambdas. 1.1 init such that layer i weight is i^(num_layers-i).
                    *[torch.tensor([0.5, 1.0]) for _ in range(num_layers)],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    -1.5 * torch.ones(1),  # skip_lambda -> σ(-1.5) ≈ 0.18
                    torch.ones(pad),
                ]
            )
        )

        self.scalars.label = 'scalars'
        # set learning rates
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
            param.wd_mul = 5.
        for param in self.embed.parameters():
            param.wd_mul = 150.
        for param in self.lm_head.parameters():
            param.wd_mul = 150.
        self.scalars.lr_mul = 5.0
        self.scalars.wd_mul = 0.0

        self.split_embed = False

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, schedule_cfg: ForwardScheduleConfig):
        assert input_seq.ndim == 1

        # unpack schedule_cfg
        mtp_weights, ws_short, ws_long = schedule_cfg.mtp_weights, schedule_cfg.ws_short, schedule_cfg.ws_long

        # set configs
        skip_connections = []
        skip_in = [3] # long attention window on layer 3
        skip_out = [6] # no attn op on layer 6
        x_backout = None
        backout_layer = 7

        # set lambdas
        resid_lambdas = self.scalars[: 1 * self.num_layers]
        x0_lambdas = self.x0_lambdas
        sa_lambdas = self.scalars[1 * self.num_layers: 3 * self.num_layers].view(-1, 2)
        smear_lambda = self.scalars[3 * self.num_layers]
        backout_lambda = self.scalars[3 * self.num_layers+1]
        skip_lambda = self.scalars[3 * self.num_layers+2]

        # set block masks and key shift
        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == self.num_layers
        key_offset = [b==long_bm for b in bm_sizes] # apply partial key offset to long windows

        # weight-tied: use lm_head.weight for embedding lookup (or separate embed after split)
        if self.split_embed:
            x = self.embed(input_seq)
        else:
            x = F.embedding(input_seq, self.lm_head.weight)
        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [ve[1], ve[2]] + [None] * (self.num_layers - 5) + [ve[0], ve[1], ve[2]]
        assert len(ve) == self.num_layers

        # smear token embed forward 1 position @classiclarryd
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # unbind gate banks to avoid select_backwards kernel
        ag = [w.bfloat16() for w in self.attn_gate_bank.unbind(0)] 
        veg = [w.bfloat16() for w in self.ve_gate_bank.unbind(0)]
        attn_gates = ag[:6] + [None] + ag[6:]
        ve_gates = [veg[0], veg[1]] + [None] * (self.num_layers - 5) + [veg[2], veg[3], veg[4]]
        assert len(attn_gates) == self.num_layers
        assert len(ve_gates) == self.num_layers

        for i in range(self.num_layers):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale,
                key_offset=key_offset[i],
                attn_gate_w=attn_gates[i],
                ve_gate_w=ve_gates[i]
            )
            if i in skip_out:
                skip_gate_out = torch.sigmoid(skip_lambda) * 2 * torch.sigmoid(self.skip_gate(x0[..., :self.skip_gate.weight.size(-1)]))
                x = x + skip_gate_out * skip_connections.pop()
            if i == 0:
                x = (resid_lambdas[0] + x0_lambdas[0]) * x
            else:
                x = resid_lambdas[i] * x + x0_lambdas[i] * x0
            x = self.blocks[i](x, attn_args)
            if i in skip_in:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 7 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15
        # @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1). @classiclarryd updated to 23*sigmoid((logits+5)/7.5)
        logits = 23 * torch.sigmoid((logits + 5) / 7.5)
        logits_for_loss = logits.float() if not self.training else logits

        n_predict = mtp_weights.size(0) if mtp_weights is not None else 1
        if self.training and n_predict > 1:
            # Multi-token prediction: take loss of the weighted average of next n_predict tokens
            logits_flat = logits_for_loss.view(-1, logits_for_loss.size(-1))
            idx = F.pad(target_seq, (0, n_predict - 1)).unfold(0, n_predict, 1)  # [T, n_predict] of shifted targets
            target_logits = logits_flat.gather(1, idx)
            cross_entropy = torch.logsumexp(logits_flat, dim=-1).unsqueeze(1) - target_logits
            for k in range(1, n_predict):  # zero out preds past end of sequence
                cross_entropy[-k:, k] = 0
            loss = (cross_entropy * mtp_weights).sum()
        elif self.training:
            loss = F.cross_entropy(logits_for_loss.view(-1, logits_for_loss.size(-1)), target_seq, reduction="sum")
        else:
            loss = F.cross_entropy(logits_for_loss.view(-1, logits_for_loss.size(-1)), target_seq, reduction="mean")
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()

    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        # Cast to int32 on CPU before transfer to avoid dtype conversion during .to()
        _inputs = _inputs.to(dtype=torch.int32)
        _targets = _targets.to(dtype=torch.int64)
        _cum_lengths = _cum_lengths.to(dtype=torch.int32)

        new_params = yield (
            _inputs.to(device="cuda", non_blocking=True),
            _targets.to(device="cuda", non_blocking=True),
            _cum_lengths.to(device="cuda", non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * new_grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens // new_grad_accum_steps
            max_seq_len = new_max_seq_len

# -----------------------------------------------------------------------------
# Training Management
 
def get_bs(step: int):
    if step >= args.num_scheduled_iterations:
        return args.train_bs_extension
    x = step / args.num_scheduled_iterations
    bs_idx = int(len(args.train_bs_schedule) * x)
    return args.train_bs_schedule[bs_idx]

def get_ws(step: int):
    # set short window size to half of long window size
    # Higher ws on "extension" steps
    if step >= args.num_scheduled_iterations:
        return args.ws_final // 2, args.ws_final
    x = step / args.num_scheduled_iterations
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

# learning rate schedule: tied to batch size schedule, with cooldown at the end.
def get_lr(step: int):
    if step > args.num_scheduled_iterations:
        return 0.1
    lr_max = 1.0
    x = step / args.num_scheduled_iterations
    if x > 1/3:
       lr_max = 1.52  # (16/8)**0.6
    if x > 2/3:
        lr_max = 1.73  # (24/8)**0.5
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = lr_max * w + (1 - w) * 0.1
        return lr
    return lr_max

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

class TrainingManager():
    """
    Manages three optimizers for Adam embed/lm_head, Adam scalars, and Muon weight matrices.
    Notable Features:
        1. Scalars are given higher momentum terms to smooth learning @ChrisJMcCormick
        2. Scalar weights are temporarily frozen during batch size or window size updates @ChrisJMcCormick
        3. Adam optimizers are only stepped on odd steps @classiclarryd
        4. Adam optimizers have hooks to start gradient communication during backwards pass @akash5474
        5. Muon has a linear momentum warmup and cooldown schedule
        6. Learning rates follow a linear decay schedule
        7. Embed/lm_head weights and optimizer state splits at 2/3 of training @classiclarryd

    Manages model architecture, data, and target that changes during training
    Notable Features:
        1. Multi Token Prediction schedule of [1, 0.5, 0.25->0] -> [1, 0.5->0] -> [1] @varunneal
        2. Sliding Attention window schedule of [1,3] -> [3,7] -> [5,11] -> [6,13]
        3. YaRN updates to RoPE on window changes
        4. Split embed and lm head at 2/3 of training
        5. Batch size schedule of 8 -> 16 -> 24
        6. Post training extension of long windows from 13 to 20
    """
    def __init__(self, model):
        self.mtp_weights_schedule = self._build_mtp_schedule()
        self.model = model
        adam_betas = {
            'lm_head': [0.5, 0.95],
            'smear_gate': [0.9, 0.99],
            'attn_gate_bank': [0.9, 0.99],
            've_gate_bank': [0.9, 0.99],
            'skip_gate': [0.9, 0.99],
            'x0_lambdas': [0.65, 0.95],
            'scalars': [0.9, 0.99],
            'embed': [0.5, 0.95],
            'value_embed': [0.75, 0.95]
        }
        adam_labels = list(adam_betas.keys())
        adam_beta_values = list(adam_betas.values())
        muon_labels = ['attn', 'mlp']
        adam_params = [p for p in model.parameters() if getattr(p, 'label', None) in adam_labels]
        muon_params = [p for p in model.parameters() if getattr(p, 'label', None) in muon_labels]
        assert set(getattr(p, 'label', None) for p in model.parameters()) == set(adam_labels + muon_labels), "All params must have label"

        self.adam_opt = DistAdam(adam_params, adam_labels, adam_beta_values, lr=0.008, eps=1e-10, weight_decay=0.005)
        self.muon_opt = NorMuon(muon_params, lr=0.023, momentum=0.95, beta2=0.95, weight_decay=1.2)
        self.optimizers = [self.adam_opt, self.muon_opt]

        # split after odd number step
        self.split_step = math.ceil(args.split_embed_frac * args.num_scheduled_iterations) | 1

        # set defaults
        for opt in self.optimizers:
            opt.odd_step_only = False 
            opt.should_sync = True
            for group in opt.param_groups:
                group["initial_lr"] = group["lr"]

        # on even steps, only step Muon params
        self.adam_opt.odd_step_only = True

        self.reset()

    def _build_mtp_schedule(self):
        # Precompute MTP weights for all steps to avoid tensor allocation during training
        # Schedule: [1, 0.5, 0.25->0] -> [1, 0.5->0] -> [1]
        mtp_weights_schedule = []
        for s in range(args.num_iterations + 1):
            x = s / args.num_scheduled_iterations
            if x < 1/3:
                w = [1.0, 0.5, 0.25 * (1 - 3*x)]
            elif x < 2/3:
                w = [1.0, 0.5 * (1 - (3*x - 1))]
            else:
                w = [1.0]
            mtp_weights_schedule.append(torch.tensor(w, device=device))
        return mtp_weights_schedule

    def apply_final_ws_ext(self):
        self.ws_long = args.ws_validate_post_yarn_ext

    def get_forward_args(self):
        return ForwardScheduleConfig(
            mtp_weights = self.mtp_weights,
            ws_short = self.ws_short,
            ws_long = self.ws_long
        )
    
    def _is_active_step(self, opt, step: int):
        return (opt.odd_step_only and step%2==1) or not opt.odd_step_only

    def get_transition_steps(self):
        transition_steps = []
        ws_short, ws_long = get_ws(0)
        for step in range(1, args.num_iterations):
            ws_short, new_ws_long = get_ws(step)
            if new_ws_long != ws_long:
                transition_steps.append(step)
                ws_long = new_ws_long
        return transition_steps

    def advance_schedule(self, step: int):
        self.ws_short, new_ws_long = get_ws(step)
        if new_ws_long != self.ws_long:
            self.model.yarn.apply(self.ws_long, new_ws_long)

        new_batch_size = get_bs(step)
        if new_batch_size != self.batch_size:
            self.train_loader_send_args = (new_batch_size, args.train_max_seq_len, grad_accum_steps)
        else:
            self.train_loader_send_args = None

        self.ws_long = new_ws_long
        self.mtp_weights = self.mtp_weights_schedule[step]
    
    def step_optimizers(self, step: int):                
        step_lr = get_lr(step)
        muon_momentum = get_muon_momentum(step)
        for group in self.muon_opt.param_groups:
            group["momentum"] = muon_momentum

        for opt in self.optimizers:
            for group in opt.param_groups:
                group["lr"] = group["initial_lr"] * step_lr
                
        if self._is_active_step(self.adam_opt, step):
            # adam will interleave calls to muon step
            self.adam_opt.step(self.muon_opt)
            self.model.zero_grad(set_to_none=True)
            self.adam_opt.should_sync = False
        else:
            self.muon_opt.step()
            self.muon_opt.zero_grad(set_to_none=True)
            
        if step == self.split_step:
            self.adam_opt.copy_lm_to_embed()
            self.model.split_embed = True
    
    def activate_hooks(self, step: int):
        for opt in self.optimizers:
            if self._is_active_step(opt, step):
                opt.should_sync = True

    def reset(self, state=None):
        if state is not None:
            for opt, opt_state in zip(self.optimizers, state):
                opt.should_sync = False
                opt.load_state_dict(opt_state)

        # muon momentum buffers not in state dict
        self.muon_opt.reset()
        self.model.split_embed = False

        self.ws_short, self.ws_long = get_ws(0)
        self.batch_size = get_bs(0)
        self.model.yarn.reset()

    def get_state(self):
        return [copy.deepcopy(opt.state_dict()) for opt in self.optimizers]

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # batch sizes
    train_bs_schedule: tuple = (8 * 2048 * 8, 16 * 2048 * 8, 24 * 2048 * 8)
    train_bs_extension: int = 24 * 2048 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 1800  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: float = 0.50  # fraction of num_scheduled_iterations spent cooling down the learning rate
    split_embed_frac: float = 2/3  # fraction of training when embeddings split from lm_head
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_final: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=11,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=args.val_batch_size // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.weight.data = m.weight.data.bfloat16()
model.attn_gate_bank.data = model.attn_gate_bank.data.bfloat16()
model.ve_gate_bank.data = model.ve_gate_bank.data.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)
training_manager = TrainingManager(model)

########################################
#            Warmup kernels            #
########################################
print0("Compiling model and warming up kernels (~7 minutes on first execution)", console=True)
# Warmup the training kernels, then re-initialize the state so we aren't cheating
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=training_manager.get_state()) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_bs_schedule[0], args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)

transition_steps = training_manager.get_transition_steps()
# first few steps plus transitions
warmup_steps = sorted({0, 1, 2} | set(s + offset for s in transition_steps for offset in [-1, 0, 1] if s + offset >= 0)) 
print0(f"Sampling steps {warmup_steps} for warmup", console=True)
for step in warmup_steps:
    training_manager.advance_schedule(step)
    model.eval()
    with torch.no_grad():
        inputs, targets, cum_seqlens = next(val_loader)
        model(inputs, targets, cum_seqlens, training_manager.get_forward_args())
    model.train()
    for idx in range(grad_accum_steps):
        # enable gradient sync for the DistAdam optimizers on the last iteration before we step them
        if idx == grad_accum_steps - 1:
            training_manager.activate_hooks(step)
        send_args = training_manager.train_loader_send_args
        inputs, targets, cum_seqlens = train_loader.send(send_args)
        (model(inputs, targets, cum_seqlens, training_manager.get_forward_args()) / grad_accum_steps).backward()
    training_manager.step_optimizers(step)
print0("Resetting Model", console=True)
model.zero_grad(set_to_none=True)
model.load_state_dict(initial_state["model"])
training_manager.reset(initial_state["optimizers"])
del val_loader, train_loader, initial_state
model.train()

########################################
#        Training and validation       #
########################################
train_loader = distributed_data_generator(args.train_files, args.train_bs_schedule[0], args.train_max_seq_len, grad_accum_steps=grad_accum_steps)

gc.collect()

training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    training_manager.advance_schedule(step)
    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            training_manager.apply_final_ws_ext()
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, training_manager.get_forward_args())
        val_loss /= val_steps
        del val_loader
        dist.reduce(val_loss, 0, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for idx in range(grad_accum_steps):
        # enable gradient sync for the DistAdam optimizers on the last iteration before we step them
        if idx == grad_accum_steps - 1:
            training_manager.activate_hooks(step)
        send_args = training_manager.train_loader_send_args
        inputs, targets, cum_seqlens = train_loader.send(send_args)
        (model(inputs, targets, cum_seqlens, training_manager.get_forward_args()) / grad_accum_steps).backward()
    training_manager.step_optimizers(step)

    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.10.12 (main, Nov  4 2025, 08:48:33) [GCC 11.4.0]
Running PyTorch 2.10.0.dev20251210+cu126 compiled for CUDA 12.6
Running Triton version 3.6.0
Mon Jan  5 07:48:04 2026       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:19:00.0 Off |                    0 |
| N/A   42C    P0            128W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:3B:00.0 Off |                    0 |
| N/A   34C    P0            130W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:4C:00.0 Off |                    0 |
| N/A   32C    P0            128W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:5D:00.0 Off |                    0 |
| N/A   41C    P0            129W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:9B:00.0 Off |                    0 |
| N/A   43C    P0            132W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:BB:00.0 Off |                    0 |
| N/A   35C    P0            124W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:CB:00.0 Off |                    0 |
| N/A   41C    P0            126W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:DB:00.0 Off |                    0 |
| N/A   31C    P0            125W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A     46652      C   /usr/bin/python                                 0MiB |
|    1   N/A  N/A     46653      C   /usr/bin/python                                 0MiB |
|    2   N/A  N/A     46654      C   /usr/bin/python                                 0MiB |
|    3   N/A  N/A     46655      C   /usr/bin/python                                 0MiB |
|    4   N/A  N/A     46656      C   /usr/bin/python                                 0MiB |
|    5   N/A  N/A     46657      C   /usr/bin/python                                 0MiB |
|    6   N/A  N/A     46658      C   /usr/bin/python                                 0MiB |
|    7   N/A  N/A     46659      C   /usr/bin/python                                 0MiB |
+-----------------------------------------------------------------------------------------+

====================================================================================================
Compiling model and warming up kernels (~7 minutes on first execution)
Sampling steps [0, 1, 2, 599, 600, 601, 1199, 1200, 1201, 1799, 1800, 1801] for warmup
Resetting Model
step:0/1840 val_loss:10.8324 train_time:0ms step_avg:0.04ms
step:1/1840 train_time:92ms step_avg:91.78ms
step:2/1840 train_time:114ms step_avg:56.82ms
step:3/1840 train_time:133ms step_avg:44.23ms
step:4/1840 train_time:160ms step_avg:40.00ms
step:5/1840 train_time:192ms step_avg:38.36ms
step:6/1840 train_time:278ms step_avg:46.33ms
step:7/1840 train_time:295ms step_avg:42.17ms
step:8/1840 train_time:431ms step_avg:53.93ms
step:9/1840 train_time:463ms step_avg:51.47ms
step:10/1840 train_time:497ms step_avg:49.71ms
step:11/1840 train_time:529ms step_avg:48.11ms
step:12/1840 train_time:563ms step_avg:46.94ms
step:13/1840 train_time:595ms step_avg:45.78ms
step:14/1840 train_time:629ms step_avg:44.95ms
step:15/1840 train_time:662ms step_avg:44.10ms
step:16/1840 train_time:696ms step_avg:43.48ms
step:17/1840 train_time:728ms step_avg:42.82ms
step:18/1840 train_time:762ms step_avg:42.34ms
step:19/1840 train_time:794ms step_avg:41.79ms
step:20/1840 train_time:828ms step_avg:41.41ms
step:21/1840 train_time:860ms step_avg:40.97ms
step:22/1840 train_time:895ms step_avg:40.67ms
step:23/1840 train_time:927ms step_avg:40.29ms
step:24/1840 train_time:961ms step_avg:40.05ms
step:25/1840 train_time:993ms step_avg:39.72ms
step:26/1840 train_time:1027ms step_avg:39.51ms
step:27/1840 train_time:1059ms step_avg:39.23ms
step:28/1840 train_time:1093ms step_avg:39.05ms
step:29/1840 train_time:1125ms step_avg:38.80ms
step:30/1840 train_time:1160ms step_avg:38.65ms
step:31/1840 train_time:1192ms step_avg:38.44ms
step:32/1840 train_time:1226ms step_avg:38.32ms
step:33/1840 train_time:1258ms step_avg:38.12ms
step:34/1840 train_time:1292ms step_avg:38.00ms
step:35/1840 train_time:1326ms step_avg:37.88ms
step:36/1840 train_time:1361ms step_avg:37.81ms
step:37/1840 train_time:1393ms step_avg:37.66ms
step:38/1840 train_time:1428ms step_avg:37.59ms
step:39/1840 train_time:1461ms step_avg:37.46ms
step:40/1840 train_time:1495ms step_avg:37.38ms
step:41/1840 train_time:1528ms step_avg:37.27ms
step:42/1840 train_time:1563ms step_avg:37.21ms
step:43/1840 train_time:1595ms step_avg:37.09ms
step:44/1840 train_time:1630ms step_avg:37.04ms
step:45/1840 train_time:1662ms step_avg:36.93ms
step:46/1840 train_time:1696ms step_avg:36.87ms
step:47/1840 train_time:1728ms step_avg:36.77ms
step:48/1840 train_time:1763ms step_avg:36.72ms
step:49/1840 train_time:1795ms step_avg:36.62ms
step:50/1840 train_time:1829ms step_avg:36.58ms
step:51/1840 train_time:1861ms step_avg:36.49ms
step:52/1840 train_time:1895ms step_avg:36.45ms
step:53/1840 train_time:1927ms step_avg:36.36ms
step:54/1840 train_time:1962ms step_avg:36.33ms
step:55/1840 train_time:1994ms step_avg:36.25ms
step:56/1840 train_time:2028ms step_avg:36.21ms
step:57/1840 train_time:2060ms step_avg:36.14ms
step:58/1840 train_time:2094ms step_avg:36.10ms
step:59/1840 train_time:2126ms step_avg:36.03ms
step:60/1840 train_time:2160ms step_avg:36.00ms
step:61/1840 train_time:2192ms step_avg:35.93ms
step:62/1840 train_time:2226ms step_avg:35.91ms
step:63/1840 train_time:2258ms step_avg:35.85ms
step:64/1840 train_time:2293ms step_avg:35.83ms
step:65/1840 train_time:2325ms step_avg:35.78ms
step:66/1840 train_time:2360ms step_avg:35.76ms
step:67/1840 train_time:2392ms step_avg:35.71ms
step:68/1840 train_time:2427ms step_avg:35.69ms
step:69/1840 train_time:2459ms step_avg:35.64ms
step:70/1840 train_time:2493ms step_avg:35.62ms
step:71/1840 train_time:2526ms step_avg:35.57ms
step:72/1840 train_time:2560ms step_avg:35.56ms
step:73/1840 train_time:2592ms step_avg:35.51ms
step:74/1840 train_time:2627ms step_avg:35.50ms
step:75/1840 train_time:2659ms step_avg:35.46ms
step:76/1840 train_time:2693ms step_avg:35.44ms
step:77/1840 train_time:2726ms step_avg:35.40ms
step:78/1840 train_time:2760ms step_avg:35.38ms
step:79/1840 train_time:2792ms step_avg:35.34ms
step:80/1840 train_time:2826ms step_avg:35.33ms
step:81/1840 train_time:2859ms step_avg:35.29ms
step:82/1840 train_time:2893ms step_avg:35.28ms
step:83/1840 train_time:2925ms step_avg:35.24ms
step:84/1840 train_time:2959ms step_avg:35.23ms
step:85/1840 train_time:2992ms step_avg:35.19ms
step:86/1840 train_time:3026ms step_avg:35.18ms
step:87/1840 train_time:3058ms step_avg:35.15ms
step:88/1840 train_time:3092ms step_avg:35.13ms
step:89/1840 train_time:3124ms step_avg:35.10ms
step:90/1840 train_time:3158ms step_avg:35.09ms
step:91/1840 train_time:3190ms step_avg:35.06ms
step:92/1840 train_time:3225ms step_avg:35.05ms
step:93/1840 train_time:3257ms step_avg:35.02ms
step:94/1840 train_time:3292ms step_avg:35.02ms
step:95/1840 train_time:3324ms step_avg:34.99ms
step:96/1840 train_time:3358ms step_avg:34.98ms
step:97/1840 train_time:3390ms step_avg:34.95ms
step:98/1840 train_time:3425ms step_avg:34.95ms
step:99/1840 train_time:3458ms step_avg:34.93ms
step:100/1840 train_time:3492ms step_avg:34.92ms
step:101/1840 train_time:3525ms step_avg:34.90ms
step:102/1840 train_time:3559ms step_avg:34.89ms
step:103/1840 train_time:3591ms step_avg:34.87ms
step:104/1840 train_time:3626ms step_avg:34.87ms
step:105/1840 train_time:3659ms step_avg:34.85ms
step:106/1840 train_time:3693ms step_avg:34.84ms
step:107/1840 train_time:3725ms step_avg:34.81ms
step:108/1840 train_time:3760ms step_avg:34.81ms
step:109/1840 train_time:3792ms step_avg:34.79ms
step:110/1840 train_time:3826ms step_avg:34.78ms
step:111/1840 train_time:3858ms step_avg:34.76ms
step:112/1840 train_time:3892ms step_avg:34.75ms
step:113/1840 train_time:3924ms step_avg:34.73ms
step:114/1840 train_time:3959ms step_avg:34.73ms
step:115/1840 train_time:3991ms step_avg:34.70ms
step:116/1840 train_time:4025ms step_avg:34.70ms
step:117/1840 train_time:4058ms step_avg:34.68ms
step:118/1840 train_time:4092ms step_avg:34.68ms
step:119/1840 train_time:4124ms step_avg:34.66ms
step:120/1840 train_time:4158ms step_avg:34.65ms
step:121/1840 train_time:4190ms step_avg:34.63ms
step:122/1840 train_time:4225ms step_avg:34.63ms
step:123/1840 train_time:4257ms step_avg:34.61ms
step:124/1840 train_time:4291ms step_avg:34.60ms
step:125/1840 train_time:4323ms step_avg:34.59ms
step:126/1840 train_time:4358ms step_avg:34.58ms
step:127/1840 train_time:4390ms step_avg:34.56ms
step:128/1840 train_time:4424ms step_avg:34.56ms
step:129/1840 train_time:4456ms step_avg:34.54ms
step:130/1840 train_time:4491ms step_avg:34.54ms
step:131/1840 train_time:4523ms step_avg:34.52ms
step:132/1840 train_time:4557ms step_avg:34.52ms
step:133/1840 train_time:4590ms step_avg:34.51ms
step:134/1840 train_time:4624ms step_avg:34.51ms
step:135/1840 train_time:4656ms step_avg:34.49ms
step:136/1840 train_time:4691ms step_avg:34.49ms
step:137/1840 train_time:4723ms step_avg:34.47ms
step:138/1840 train_time:4757ms step_avg:34.47ms
step:139/1840 train_time:4789ms step_avg:34.46ms
step:140/1840 train_time:4824ms step_avg:34.45ms
step:141/1840 train_time:4855ms step_avg:34.44ms
step:142/1840 train_time:4890ms step_avg:34.44ms
step:143/1840 train_time:4922ms step_avg:34.42ms
step:144/1840 train_time:4956ms step_avg:34.42ms
step:145/1840 train_time:4989ms step_avg:34.40ms
step:146/1840 train_time:5023ms step_avg:34.40ms
step:147/1840 train_time:5055ms step_avg:34.39ms
step:148/1840 train_time:5089ms step_avg:34.38ms
step:149/1840 train_time:5121ms step_avg:34.37ms
step:150/1840 train_time:5156ms step_avg:34.37ms
step:151/1840 train_time:5187ms step_avg:34.35ms
step:152/1840 train_time:5222ms step_avg:34.35ms
step:153/1840 train_time:5254ms step_avg:34.34ms
step:154/1840 train_time:5288ms step_avg:34.34ms
step:155/1840 train_time:5320ms step_avg:34.32ms
step:156/1840 train_time:5354ms step_avg:34.32ms
step:157/1840 train_time:5386ms step_avg:34.31ms
step:158/1840 train_time:5421ms step_avg:34.31ms
step:159/1840 train_time:5452ms step_avg:34.29ms
step:160/1840 train_time:5487ms step_avg:34.29ms
step:161/1840 train_time:5519ms step_avg:34.28ms
step:162/1840 train_time:5554ms step_avg:34.28ms
step:163/1840 train_time:5586ms step_avg:34.27ms
step:164/1840 train_time:5620ms step_avg:34.27ms
step:165/1840 train_time:5652ms step_avg:34.25ms
step:166/1840 train_time:5686ms step_avg:34.25ms
step:167/1840 train_time:5718ms step_avg:34.24ms
step:168/1840 train_time:5752ms step_avg:34.24ms
step:169/1840 train_time:5785ms step_avg:34.23ms
step:170/1840 train_time:5819ms step_avg:34.23ms
step:171/1840 train_time:5851ms step_avg:34.22ms
step:172/1840 train_time:5885ms step_avg:34.22ms
step:173/1840 train_time:5918ms step_avg:34.21ms
step:174/1840 train_time:5952ms step_avg:34.21ms
step:175/1840 train_time:5984ms step_avg:34.20ms
step:176/1840 train_time:6018ms step_avg:34.20ms
step:177/1840 train_time:6051ms step_avg:34.18ms
step:178/1840 train_time:6084ms step_avg:34.18ms
step:179/1840 train_time:6117ms step_avg:34.17ms
step:180/1840 train_time:6151ms step_avg:34.17ms
step:181/1840 train_time:6183ms step_avg:34.16ms
step:182/1840 train_time:6217ms step_avg:34.16ms
step:183/1840 train_time:6249ms step_avg:34.15ms
step:184/1840 train_time:6284ms step_avg:34.15ms
step:185/1840 train_time:6316ms step_avg:34.14ms
step:186/1840 train_time:6350ms step_avg:34.14ms
step:187/1840 train_time:6382ms step_avg:34.13ms
step:188/1840 train_time:6417ms step_avg:34.13ms
step:189/1840 train_time:6449ms step_avg:34.12ms
step:190/1840 train_time:6484ms step_avg:34.13ms
step:191/1840 train_time:6516ms step_avg:34.12ms
step:192/1840 train_time:6550ms step_avg:34.12ms
step:193/1840 train_time:6582ms step_avg:34.10ms
step:194/1840 train_time:6616ms step_avg:34.10ms
step:195/1840 train_time:6648ms step_avg:34.09ms
step:196/1840 train_time:6683ms step_avg:34.10ms
step:197/1840 train_time:6715ms step_avg:34.09ms
step:198/1840 train_time:6749ms step_avg:34.09ms
step:199/1840 train_time:6782ms step_avg:34.08ms
step:200/1840 train_time:6816ms step_avg:34.08ms
step:201/1840 train_time:6848ms step_avg:34.07ms
step:202/1840 train_time:6882ms step_avg:34.07ms
step:203/1840 train_time:6914ms step_avg:34.06ms
step:204/1840 train_time:6948ms step_avg:34.06ms
step:205/1840 train_time:6981ms step_avg:34.05ms
step:206/1840 train_time:7015ms step_avg:34.05ms
step:207/1840 train_time:7047ms step_avg:34.04ms
step:208/1840 train_time:7081ms step_avg:34.04ms
step:209/1840 train_time:7113ms step_avg:34.03ms
step:210/1840 train_time:7147ms step_avg:34.03ms
step:211/1840 train_time:7179ms step_avg:34.03ms
step:212/1840 train_time:7213ms step_avg:34.03ms
step:213/1840 train_time:7245ms step_avg:34.02ms
step:214/1840 train_time:7280ms step_avg:34.02ms
step:215/1840 train_time:7312ms step_avg:34.01ms
step:216/1840 train_time:7346ms step_avg:34.01ms
step:217/1840 train_time:7378ms step_avg:34.00ms
step:218/1840 train_time:7412ms step_avg:34.00ms
step:219/1840 train_time:7445ms step_avg:33.99ms
step:220/1840 train_time:7479ms step_avg:34.00ms
step:221/1840 train_time:7511ms step_avg:33.99ms
step:222/1840 train_time:7545ms step_avg:33.99ms
step:223/1840 train_time:7577ms step_avg:33.98ms
step:224/1840 train_time:7612ms step_avg:33.98ms
step:225/1840 train_time:7644ms step_avg:33.97ms
step:226/1840 train_time:7678ms step_avg:33.97ms
step:227/1840 train_time:7710ms step_avg:33.96ms
step:228/1840 train_time:7744ms step_avg:33.97ms
step:229/1840 train_time:7777ms step_avg:33.96ms
step:230/1840 train_time:7811ms step_avg:33.96ms
step:231/1840 train_time:7843ms step_avg:33.95ms
step:232/1840 train_time:7877ms step_avg:33.95ms
step:233/1840 train_time:7909ms step_avg:33.95ms
step:234/1840 train_time:7944ms step_avg:33.95ms
step:235/1840 train_time:7976ms step_avg:33.94ms
step:236/1840 train_time:8010ms step_avg:33.94ms
step:237/1840 train_time:8043ms step_avg:33.94ms
step:238/1840 train_time:8077ms step_avg:33.94ms
step:239/1840 train_time:8109ms step_avg:33.93ms
step:240/1840 train_time:8144ms step_avg:33.93ms
step:241/1840 train_time:8176ms step_avg:33.92ms
step:242/1840 train_time:8210ms step_avg:33.92ms
step:243/1840 train_time:8242ms step_avg:33.92ms
step:244/1840 train_time:8276ms step_avg:33.92ms
step:245/1840 train_time:8308ms step_avg:33.91ms
step:246/1840 train_time:8343ms step_avg:33.91ms
step:247/1840 train_time:8375ms step_avg:33.91ms
step:248/1840 train_time:8409ms step_avg:33.91ms
step:249/1840 train_time:8441ms step_avg:33.90ms
step:250/1840 train_time:8475ms step_avg:33.90ms
step:250/1840 val_loss:4.6172 train_time:8518ms step_avg:34.07ms
step:251/1840 train_time:8537ms step_avg:34.01ms
step:252/1840 train_time:8556ms step_avg:33.95ms
step:253/1840 train_time:8576ms step_avg:33.90ms
step:254/1840 train_time:8612ms step_avg:33.90ms
step:255/1840 train_time:8646ms step_avg:33.91ms
step:256/1840 train_time:8681ms step_avg:33.91ms
step:257/1840 train_time:8713ms step_avg:33.90ms
step:258/1840 train_time:8748ms step_avg:33.91ms
step:259/1840 train_time:8780ms step_avg:33.90ms
step:260/1840 train_time:8814ms step_avg:33.90ms
step:261/1840 train_time:8846ms step_avg:33.89ms
step:262/1840 train_time:8880ms step_avg:33.89ms
step:263/1840 train_time:8912ms step_avg:33.88ms
step:264/1840 train_time:8946ms step_avg:33.89ms
step:265/1840 train_time:8978ms step_avg:33.88ms
step:266/1840 train_time:9012ms step_avg:33.88ms
step:267/1840 train_time:9044ms step_avg:33.87ms
step:268/1840 train_time:9078ms step_avg:33.87ms
step:269/1840 train_time:9110ms step_avg:33.86ms
step:270/1840 train_time:9144ms step_avg:33.87ms
step:271/1840 train_time:9175ms step_avg:33.86ms
step:272/1840 train_time:9209ms step_avg:33.86ms
step:273/1840 train_time:9242ms step_avg:33.85ms
step:274/1840 train_time:9276ms step_avg:33.85ms
step:275/1840 train_time:9308ms step_avg:33.85ms
step:276/1840 train_time:9342ms step_avg:33.85ms
step:277/1840 train_time:9373ms step_avg:33.84ms
step:278/1840 train_time:9408ms step_avg:33.84ms
step:279/1840 train_time:9440ms step_avg:33.83ms
step:280/1840 train_time:9474ms step_avg:33.84ms
step:281/1840 train_time:9507ms step_avg:33.83ms
step:282/1840 train_time:9542ms step_avg:33.84ms
step:283/1840 train_time:9575ms step_avg:33.83ms
step:284/1840 train_time:9609ms step_avg:33.84ms
step:285/1840 train_time:9642ms step_avg:33.83ms
step:286/1840 train_time:9676ms step_avg:33.83ms
step:287/1840 train_time:9709ms step_avg:33.83ms
step:288/1840 train_time:9743ms step_avg:33.83ms
step:289/1840 train_time:9775ms step_avg:33.82ms
step:290/1840 train_time:9810ms step_avg:33.83ms
step:291/1840 train_time:9842ms step_avg:33.82ms
step:292/1840 train_time:9876ms step_avg:33.82ms
step:293/1840 train_time:9908ms step_avg:33.82ms
step:294/1840 train_time:9943ms step_avg:33.82ms
step:295/1840 train_time:9974ms step_avg:33.81ms
step:296/1840 train_time:10008ms step_avg:33.81ms
step:297/1840 train_time:10040ms step_avg:33.81ms
step:298/1840 train_time:10074ms step_avg:33.81ms
step:299/1840 train_time:10106ms step_avg:33.80ms
step:300/1840 train_time:10140ms step_avg:33.80ms
step:301/1840 train_time:10172ms step_avg:33.79ms
step:302/1840 train_time:10206ms step_avg:33.79ms
step:303/1840 train_time:10238ms step_avg:33.79ms
step:304/1840 train_time:10272ms step_avg:33.79ms
step:305/1840 train_time:10304ms step_avg:33.78ms
step:306/1840 train_time:10338ms step_avg:33.78ms
step:307/1840 train_time:10370ms step_avg:33.78ms
step:308/1840 train_time:10404ms step_avg:33.78ms
step:309/1840 train_time:10436ms step_avg:33.77ms
step:310/1840 train_time:10471ms step_avg:33.78ms
step:311/1840 train_time:10503ms step_avg:33.77ms
step:312/1840 train_time:10537ms step_avg:33.77ms
step:313/1840 train_time:10570ms step_avg:33.77ms
step:314/1840 train_time:10604ms step_avg:33.77ms
step:315/1840 train_time:10636ms step_avg:33.77ms
step:316/1840 train_time:10671ms step_avg:33.77ms
step:317/1840 train_time:10703ms step_avg:33.76ms
step:318/1840 train_time:10737ms step_avg:33.76ms
step:319/1840 train_time:10769ms step_avg:33.76ms
step:320/1840 train_time:10804ms step_avg:33.76ms
step:321/1840 train_time:10836ms step_avg:33.76ms
step:322/1840 train_time:10870ms step_avg:33.76ms
step:323/1840 train_time:10903ms step_avg:33.75ms
step:324/1840 train_time:10937ms step_avg:33.76ms
step:325/1840 train_time:10969ms step_avg:33.75ms
step:326/1840 train_time:11003ms step_avg:33.75ms
step:327/1840 train_time:11035ms step_avg:33.75ms
step:328/1840 train_time:11069ms step_avg:33.75ms
step:329/1840 train_time:11101ms step_avg:33.74ms
step:330/1840 train_time:11135ms step_avg:33.74ms
step:331/1840 train_time:11168ms step_avg:33.74ms
step:332/1840 train_time:11202ms step_avg:33.74ms
step:333/1840 train_time:11234ms step_avg:33.73ms
step:334/1840 train_time:11268ms step_avg:33.74ms
step:335/1840 train_time:11300ms step_avg:33.73ms
step:336/1840 train_time:11334ms step_avg:33.73ms
step:337/1840 train_time:11366ms step_avg:33.73ms
step:338/1840 train_time:11400ms step_avg:33.73ms
step:339/1840 train_time:11432ms step_avg:33.72ms
step:340/1840 train_time:11467ms step_avg:33.73ms
step:341/1840 train_time:11499ms step_avg:33.72ms
step:342/1840 train_time:11533ms step_avg:33.72ms
step:343/1840 train_time:11565ms step_avg:33.72ms
step:344/1840 train_time:11599ms step_avg:33.72ms
step:345/1840 train_time:11632ms step_avg:33.72ms
step:346/1840 train_time:11666ms step_avg:33.72ms
step:347/1840 train_time:11698ms step_avg:33.71ms
step:348/1840 train_time:11733ms step_avg:33.71ms
step:349/1840 train_time:11765ms step_avg:33.71ms
step:350/1840 train_time:11800ms step_avg:33.71ms
step:351/1840 train_time:11832ms step_avg:33.71ms
step:352/1840 train_time:11866ms step_avg:33.71ms
step:353/1840 train_time:11898ms step_avg:33.71ms
step:354/1840 train_time:11932ms step_avg:33.71ms
step:355/1840 train_time:11965ms step_avg:33.70ms
step:356/1840 train_time:11999ms step_avg:33.70ms
step:357/1840 train_time:12031ms step_avg:33.70ms
step:358/1840 train_time:12065ms step_avg:33.70ms
step:359/1840 train_time:12097ms step_avg:33.70ms
step:360/1840 train_time:12132ms step_avg:33.70ms
step:361/1840 train_time:12164ms step_avg:33.69ms
step:362/1840 train_time:12198ms step_avg:33.70ms
step:363/1840 train_time:12230ms step_avg:33.69ms
step:364/1840 train_time:12264ms step_avg:33.69ms
step:365/1840 train_time:12296ms step_avg:33.69ms
step:366/1840 train_time:12330ms step_avg:33.69ms
step:367/1840 train_time:12362ms step_avg:33.68ms
step:368/1840 train_time:12396ms step_avg:33.68ms
step:369/1840 train_time:12428ms step_avg:33.68ms
step:370/1840 train_time:12462ms step_avg:33.68ms
step:371/1840 train_time:12494ms step_avg:33.68ms
step:372/1840 train_time:12528ms step_avg:33.68ms
step:373/1840 train_time:12560ms step_avg:33.67ms
step:374/1840 train_time:12595ms step_avg:33.68ms
step:375/1840 train_time:12627ms step_avg:33.67ms
step:376/1840 train_time:12662ms step_avg:33.67ms
step:377/1840 train_time:12694ms step_avg:33.67ms
step:378/1840 train_time:12728ms step_avg:33.67ms
step:379/1840 train_time:12760ms step_avg:33.67ms
step:380/1840 train_time:12795ms step_avg:33.67ms
step:381/1840 train_time:12827ms step_avg:33.67ms
step:382/1840 train_time:12861ms step_avg:33.67ms
step:383/1840 train_time:12893ms step_avg:33.66ms
step:384/1840 train_time:12928ms step_avg:33.67ms
step:385/1840 train_time:12960ms step_avg:33.66ms
step:386/1840 train_time:12994ms step_avg:33.66ms
step:387/1840 train_time:13026ms step_avg:33.66ms
step:388/1840 train_time:13060ms step_avg:33.66ms
step:389/1840 train_time:13092ms step_avg:33.66ms
step:390/1840 train_time:13126ms step_avg:33.66ms
step:391/1840 train_time:13158ms step_avg:33.65ms
step:392/1840 train_time:13192ms step_avg:33.65ms
step:393/1840 train_time:13225ms step_avg:33.65ms
step:394/1840 train_time:13259ms step_avg:33.65ms
step:395/1840 train_time:13291ms step_avg:33.65ms
step:396/1840 train_time:13325ms step_avg:33.65ms
step:397/1840 train_time:13357ms step_avg:33.64ms
step:398/1840 train_time:13391ms step_avg:33.65ms
step:399/1840 train_time:13423ms step_avg:33.64ms
step:400/1840 train_time:13457ms step_avg:33.64ms
step:401/1840 train_time:13489ms step_avg:33.64ms
step:402/1840 train_time:13524ms step_avg:33.64ms
step:403/1840 train_time:13556ms step_avg:33.64ms
step:404/1840 train_time:13590ms step_avg:33.64ms
step:405/1840 train_time:13623ms step_avg:33.64ms
step:406/1840 train_time:13657ms step_avg:33.64ms
step:407/1840 train_time:13689ms step_avg:33.63ms
step:408/1840 train_time:13723ms step_avg:33.64ms
step:409/1840 train_time:13755ms step_avg:33.63ms
step:410/1840 train_time:13790ms step_avg:33.63ms
step:411/1840 train_time:13822ms step_avg:33.63ms
step:412/1840 train_time:13856ms step_avg:33.63ms
step:413/1840 train_time:13888ms step_avg:33.63ms
step:414/1840 train_time:13922ms step_avg:33.63ms
step:415/1840 train_time:13954ms step_avg:33.62ms
step:416/1840 train_time:13988ms step_avg:33.63ms
step:417/1840 train_time:14020ms step_avg:33.62ms
step:418/1840 train_time:14055ms step_avg:33.62ms
step:419/1840 train_time:14087ms step_avg:33.62ms
step:420/1840 train_time:14122ms step_avg:33.62ms
step:421/1840 train_time:14154ms step_avg:33.62ms
step:422/1840 train_time:14188ms step_avg:33.62ms
step:423/1840 train_time:14220ms step_avg:33.62ms
step:424/1840 train_time:14254ms step_avg:33.62ms
step:425/1840 train_time:14287ms step_avg:33.62ms
step:426/1840 train_time:14321ms step_avg:33.62ms
step:427/1840 train_time:14353ms step_avg:33.61ms
step:428/1840 train_time:14388ms step_avg:33.62ms
step:429/1840 train_time:14420ms step_avg:33.61ms
step:430/1840 train_time:14454ms step_avg:33.61ms
step:431/1840 train_time:14486ms step_avg:33.61ms
step:432/1840 train_time:14520ms step_avg:33.61ms
step:433/1840 train_time:14552ms step_avg:33.61ms
step:434/1840 train_time:14587ms step_avg:33.61ms
step:435/1840 train_time:14619ms step_avg:33.61ms
step:436/1840 train_time:14654ms step_avg:33.61ms
step:437/1840 train_time:14686ms step_avg:33.61ms
step:438/1840 train_time:14720ms step_avg:33.61ms
step:439/1840 train_time:14752ms step_avg:33.60ms
step:440/1840 train_time:14787ms step_avg:33.61ms
step:441/1840 train_time:14820ms step_avg:33.60ms
step:442/1840 train_time:14854ms step_avg:33.61ms
step:443/1840 train_time:14886ms step_avg:33.60ms
step:444/1840 train_time:14921ms step_avg:33.60ms
step:445/1840 train_time:14953ms step_avg:33.60ms
step:446/1840 train_time:14987ms step_avg:33.60ms
step:447/1840 train_time:15019ms step_avg:33.60ms
step:448/1840 train_time:15053ms step_avg:33.60ms
step:449/1840 train_time:15086ms step_avg:33.60ms
step:450/1840 train_time:15120ms step_avg:33.60ms
step:451/1840 train_time:15152ms step_avg:33.60ms
step:452/1840 train_time:15186ms step_avg:33.60ms
step:453/1840 train_time:15219ms step_avg:33.60ms
step:454/1840 train_time:15253ms step_avg:33.60ms
step:455/1840 train_time:15285ms step_avg:33.59ms
step:456/1840 train_time:15319ms step_avg:33.60ms
step:457/1840 train_time:15351ms step_avg:33.59ms
step:458/1840 train_time:15386ms step_avg:33.59ms
step:459/1840 train_time:15418ms step_avg:33.59ms
step:460/1840 train_time:15452ms step_avg:33.59ms
step:461/1840 train_time:15484ms step_avg:33.59ms
step:462/1840 train_time:15518ms step_avg:33.59ms
step:463/1840 train_time:15550ms step_avg:33.59ms
step:464/1840 train_time:15585ms step_avg:33.59ms
step:465/1840 train_time:15617ms step_avg:33.58ms
step:466/1840 train_time:15651ms step_avg:33.59ms
step:467/1840 train_time:15683ms step_avg:33.58ms
step:468/1840 train_time:15717ms step_avg:33.58ms
step:469/1840 train_time:15749ms step_avg:33.58ms
step:470/1840 train_time:15784ms step_avg:33.58ms
step:471/1840 train_time:15816ms step_avg:33.58ms
step:472/1840 train_time:15850ms step_avg:33.58ms
step:473/1840 train_time:15883ms step_avg:33.58ms
step:474/1840 train_time:15917ms step_avg:33.58ms
step:475/1840 train_time:15949ms step_avg:33.58ms
step:476/1840 train_time:15984ms step_avg:33.58ms
step:477/1840 train_time:16016ms step_avg:33.58ms
step:478/1840 train_time:16051ms step_avg:33.58ms
step:479/1840 train_time:16083ms step_avg:33.58ms
step:480/1840 train_time:16117ms step_avg:33.58ms
step:481/1840 train_time:16149ms step_avg:33.57ms
step:482/1840 train_time:16183ms step_avg:33.58ms
step:483/1840 train_time:16215ms step_avg:33.57ms
step:484/1840 train_time:16250ms step_avg:33.57ms
step:485/1840 train_time:16282ms step_avg:33.57ms
step:486/1840 train_time:16316ms step_avg:33.57ms
step:487/1840 train_time:16348ms step_avg:33.57ms
step:488/1840 train_time:16383ms step_avg:33.57ms
step:489/1840 train_time:16415ms step_avg:33.57ms
step:490/1840 train_time:16449ms step_avg:33.57ms
step:491/1840 train_time:16481ms step_avg:33.57ms
step:492/1840 train_time:16516ms step_avg:33.57ms
step:493/1840 train_time:16547ms step_avg:33.56ms
step:494/1840 train_time:16582ms step_avg:33.57ms
step:495/1840 train_time:16613ms step_avg:33.56ms
step:496/1840 train_time:16648ms step_avg:33.56ms
step:497/1840 train_time:16680ms step_avg:33.56ms
step:498/1840 train_time:16714ms step_avg:33.56ms
step:499/1840 train_time:16747ms step_avg:33.56ms
step:500/1840 train_time:16781ms step_avg:33.56ms
step:500/1840 val_loss:4.2832 train_time:16823ms step_avg:33.65ms
step:501/1840 train_time:16842ms step_avg:33.62ms
step:502/1840 train_time:16860ms step_avg:33.59ms
step:503/1840 train_time:16884ms step_avg:33.57ms
step:504/1840 train_time:16919ms step_avg:33.57ms
step:505/1840 train_time:16952ms step_avg:33.57ms
step:506/1840 train_time:16987ms step_avg:33.57ms
step:507/1840 train_time:17020ms step_avg:33.57ms
step:508/1840 train_time:17054ms step_avg:33.57ms
step:509/1840 train_time:17086ms step_avg:33.57ms
step:510/1840 train_time:17121ms step_avg:33.57ms
step:511/1840 train_time:17153ms step_avg:33.57ms
step:512/1840 train_time:17188ms step_avg:33.57ms
step:513/1840 train_time:17220ms step_avg:33.57ms
step:514/1840 train_time:17254ms step_avg:33.57ms
step:515/1840 train_time:17286ms step_avg:33.56ms
step:516/1840 train_time:17320ms step_avg:33.57ms
step:517/1840 train_time:17352ms step_avg:33.56ms
step:518/1840 train_time:17386ms step_avg:33.56ms
step:519/1840 train_time:17417ms step_avg:33.56ms
step:520/1840 train_time:17452ms step_avg:33.56ms
step:521/1840 train_time:17484ms step_avg:33.56ms
step:522/1840 train_time:17518ms step_avg:33.56ms
step:523/1840 train_time:17550ms step_avg:33.56ms
step:524/1840 train_time:17583ms step_avg:33.56ms
step:525/1840 train_time:17615ms step_avg:33.55ms
step:526/1840 train_time:17649ms step_avg:33.55ms
step:527/1840 train_time:17681ms step_avg:33.55ms
step:528/1840 train_time:17715ms step_avg:33.55ms
step:529/1840 train_time:17747ms step_avg:33.55ms
step:530/1840 train_time:17782ms step_avg:33.55ms
step:531/1840 train_time:17814ms step_avg:33.55ms
step:532/1840 train_time:17849ms step_avg:33.55ms
step:533/1840 train_time:17882ms step_avg:33.55ms
step:534/1840 train_time:17916ms step_avg:33.55ms
step:535/1840 train_time:17948ms step_avg:33.55ms
step:536/1840 train_time:17983ms step_avg:33.55ms
step:537/1840 train_time:18016ms step_avg:33.55ms
step:538/1840 train_time:18050ms step_avg:33.55ms
step:539/1840 train_time:18083ms step_avg:33.55ms
step:540/1840 train_time:18117ms step_avg:33.55ms
step:541/1840 train_time:18149ms step_avg:33.55ms
step:542/1840 train_time:18184ms step_avg:33.55ms
step:543/1840 train_time:18216ms step_avg:33.55ms
step:544/1840 train_time:18250ms step_avg:33.55ms
step:545/1840 train_time:18282ms step_avg:33.55ms
step:546/1840 train_time:18316ms step_avg:33.55ms
step:547/1840 train_time:18348ms step_avg:33.54ms
step:548/1840 train_time:18383ms step_avg:33.54ms
step:549/1840 train_time:18415ms step_avg:33.54ms
step:550/1840 train_time:18449ms step_avg:33.54ms
step:551/1840 train_time:18481ms step_avg:33.54ms
step:552/1840 train_time:18515ms step_avg:33.54ms
step:553/1840 train_time:18547ms step_avg:33.54ms
step:554/1840 train_time:18581ms step_avg:33.54ms
step:555/1840 train_time:18613ms step_avg:33.54ms
step:556/1840 train_time:18647ms step_avg:33.54ms
step:557/1840 train_time:18679ms step_avg:33.54ms
step:558/1840 train_time:18713ms step_avg:33.54ms
step:559/1840 train_time:18745ms step_avg:33.53ms
step:560/1840 train_time:18780ms step_avg:33.53ms
step:561/1840 train_time:18812ms step_avg:33.53ms
step:562/1840 train_time:18846ms step_avg:33.53ms
step:563/1840 train_time:18878ms step_avg:33.53ms
step:564/1840 train_time:18913ms step_avg:33.53ms
step:565/1840 train_time:18945ms step_avg:33.53ms
step:566/1840 train_time:18979ms step_avg:33.53ms
step:567/1840 train_time:19012ms step_avg:33.53ms
step:568/1840 train_time:19046ms step_avg:33.53ms
step:569/1840 train_time:19078ms step_avg:33.53ms
step:570/1840 train_time:19113ms step_avg:33.53ms
step:571/1840 train_time:19145ms step_avg:33.53ms
step:572/1840 train_time:19179ms step_avg:33.53ms
step:573/1840 train_time:19211ms step_avg:33.53ms
step:574/1840 train_time:19246ms step_avg:33.53ms
step:575/1840 train_time:19278ms step_avg:33.53ms
step:576/1840 train_time:19312ms step_avg:33.53ms
step:577/1840 train_time:19344ms step_avg:33.53ms
step:578/1840 train_time:19378ms step_avg:33.53ms
step:579/1840 train_time:19411ms step_avg:33.52ms
step:580/1840 train_time:19445ms step_avg:33.53ms
step:581/1840 train_time:19477ms step_avg:33.52ms
step:582/1840 train_time:19511ms step_avg:33.52ms
step:583/1840 train_time:19543ms step_avg:33.52ms
step:584/1840 train_time:19577ms step_avg:33.52ms
step:585/1840 train_time:19609ms step_avg:33.52ms
step:586/1840 train_time:19644ms step_avg:33.52ms
step:587/1840 train_time:19676ms step_avg:33.52ms
step:588/1840 train_time:19710ms step_avg:33.52ms
step:589/1840 train_time:19742ms step_avg:33.52ms
step:590/1840 train_time:19777ms step_avg:33.52ms
step:591/1840 train_time:19809ms step_avg:33.52ms
step:592/1840 train_time:19844ms step_avg:33.52ms
step:593/1840 train_time:19876ms step_avg:33.52ms
step:594/1840 train_time:19910ms step_avg:33.52ms
step:595/1840 train_time:19943ms step_avg:33.52ms
step:596/1840 train_time:19977ms step_avg:33.52ms
step:597/1840 train_time:20010ms step_avg:33.52ms
step:598/1840 train_time:20044ms step_avg:33.52ms
step:599/1840 train_time:20076ms step_avg:33.52ms
step:600/1840 train_time:20111ms step_avg:33.52ms
step:601/1840 train_time:20145ms step_avg:33.52ms
step:602/1840 train_time:20203ms step_avg:33.56ms
step:603/1840 train_time:20262ms step_avg:33.60ms
step:604/1840 train_time:20324ms step_avg:33.65ms
step:605/1840 train_time:20384ms step_avg:33.69ms
step:606/1840 train_time:20447ms step_avg:33.74ms
step:607/1840 train_time:20506ms step_avg:33.78ms
step:608/1840 train_time:20569ms step_avg:33.83ms
step:609/1840 train_time:20629ms step_avg:33.87ms
step:610/1840 train_time:20691ms step_avg:33.92ms
step:611/1840 train_time:20751ms step_avg:33.96ms
step:612/1840 train_time:20813ms step_avg:34.01ms
step:613/1840 train_time:20873ms step_avg:34.05ms
step:614/1840 train_time:20935ms step_avg:34.10ms
step:615/1840 train_time:20995ms step_avg:34.14ms
step:616/1840 train_time:21057ms step_avg:34.18ms
step:617/1840 train_time:21117ms step_avg:34.23ms
step:618/1840 train_time:21179ms step_avg:34.27ms
step:619/1840 train_time:21239ms step_avg:34.31ms
step:620/1840 train_time:21301ms step_avg:34.36ms
step:621/1840 train_time:21360ms step_avg:34.40ms
step:622/1840 train_time:21422ms step_avg:34.44ms
step:623/1840 train_time:21482ms step_avg:34.48ms
step:624/1840 train_time:21544ms step_avg:34.53ms
step:625/1840 train_time:21604ms step_avg:34.57ms
step:626/1840 train_time:21667ms step_avg:34.61ms
step:627/1840 train_time:21726ms step_avg:34.65ms
step:628/1840 train_time:21789ms step_avg:34.70ms
step:629/1840 train_time:21849ms step_avg:34.74ms
step:630/1840 train_time:21911ms step_avg:34.78ms
step:631/1840 train_time:21972ms step_avg:34.82ms
step:632/1840 train_time:22034ms step_avg:34.86ms
step:633/1840 train_time:22094ms step_avg:34.90ms
step:634/1840 train_time:22157ms step_avg:34.95ms
step:635/1840 train_time:22217ms step_avg:34.99ms
step:636/1840 train_time:22279ms step_avg:35.03ms
step:637/1840 train_time:22339ms step_avg:35.07ms
step:638/1840 train_time:22401ms step_avg:35.11ms
step:639/1840 train_time:22461ms step_avg:35.15ms
step:640/1840 train_time:22523ms step_avg:35.19ms
step:641/1840 train_time:22583ms step_avg:35.23ms
step:642/1840 train_time:22645ms step_avg:35.27ms
step:643/1840 train_time:22705ms step_avg:35.31ms
step:644/1840 train_time:22768ms step_avg:35.35ms
step:645/1840 train_time:22828ms step_avg:35.39ms
step:646/1840 train_time:22892ms step_avg:35.44ms
step:647/1840 train_time:22951ms step_avg:35.47ms
step:648/1840 train_time:23013ms step_avg:35.51ms
step:649/1840 train_time:23073ms step_avg:35.55ms
step:650/1840 train_time:23136ms step_avg:35.59ms
step:651/1840 train_time:23196ms step_avg:35.63ms
step:652/1840 train_time:23258ms step_avg:35.67ms
step:653/1840 train_time:23318ms step_avg:35.71ms
step:654/1840 train_time:23380ms step_avg:35.75ms
step:655/1840 train_time:23440ms step_avg:35.79ms
step:656/1840 train_time:23502ms step_avg:35.83ms
step:657/1840 train_time:23562ms step_avg:35.86ms
step:658/1840 train_time:23624ms step_avg:35.90ms
step:659/1840 train_time:23683ms step_avg:35.94ms
step:660/1840 train_time:23746ms step_avg:35.98ms
step:661/1840 train_time:23806ms step_avg:36.01ms
step:662/1840 train_time:23870ms step_avg:36.06ms
step:663/1840 train_time:23931ms step_avg:36.09ms
step:664/1840 train_time:23993ms step_avg:36.13ms
step:665/1840 train_time:24053ms step_avg:36.17ms
step:666/1840 train_time:24116ms step_avg:36.21ms
step:667/1840 train_time:24177ms step_avg:36.25ms
step:668/1840 train_time:24239ms step_avg:36.29ms
step:669/1840 train_time:24298ms step_avg:36.32ms
step:670/1840 train_time:24360ms step_avg:36.36ms
step:671/1840 train_time:24420ms step_avg:36.39ms
step:672/1840 train_time:24482ms step_avg:36.43ms
step:673/1840 train_time:24541ms step_avg:36.47ms
step:674/1840 train_time:24603ms step_avg:36.50ms
step:675/1840 train_time:24663ms step_avg:36.54ms
step:676/1840 train_time:24726ms step_avg:36.58ms
step:677/1840 train_time:24786ms step_avg:36.61ms
step:678/1840 train_time:24849ms step_avg:36.65ms
step:679/1840 train_time:24910ms step_avg:36.69ms
step:680/1840 train_time:24972ms step_avg:36.72ms
step:681/1840 train_time:25032ms step_avg:36.76ms
step:682/1840 train_time:25095ms step_avg:36.80ms
step:683/1840 train_time:25155ms step_avg:36.83ms
step:684/1840 train_time:25217ms step_avg:36.87ms
step:685/1840 train_time:25277ms step_avg:36.90ms
step:686/1840 train_time:25339ms step_avg:36.94ms
step:687/1840 train_time:25399ms step_avg:36.97ms
step:688/1840 train_time:25461ms step_avg:37.01ms
step:689/1840 train_time:25521ms step_avg:37.04ms
step:690/1840 train_time:25582ms step_avg:37.08ms
step:691/1840 train_time:25642ms step_avg:37.11ms
step:692/1840 train_time:25704ms step_avg:37.15ms
step:693/1840 train_time:25764ms step_avg:37.18ms
step:694/1840 train_time:25827ms step_avg:37.21ms
step:695/1840 train_time:25887ms step_avg:37.25ms
step:696/1840 train_time:25949ms step_avg:37.28ms
step:697/1840 train_time:26010ms step_avg:37.32ms
step:698/1840 train_time:26073ms step_avg:37.35ms
step:699/1840 train_time:26133ms step_avg:37.39ms
step:700/1840 train_time:26196ms step_avg:37.42ms
step:701/1840 train_time:26257ms step_avg:37.46ms
step:702/1840 train_time:26319ms step_avg:37.49ms
step:703/1840 train_time:26378ms step_avg:37.52ms
step:704/1840 train_time:26441ms step_avg:37.56ms
step:705/1840 train_time:26501ms step_avg:37.59ms
step:706/1840 train_time:26563ms step_avg:37.62ms
step:707/1840 train_time:26622ms step_avg:37.65ms
step:708/1840 train_time:26684ms step_avg:37.69ms
step:709/1840 train_time:26744ms step_avg:37.72ms
step:710/1840 train_time:26806ms step_avg:37.76ms
step:711/1840 train_time:26866ms step_avg:37.79ms
step:712/1840 train_time:26929ms step_avg:37.82ms
step:713/1840 train_time:26989ms step_avg:37.85ms
step:714/1840 train_time:27052ms step_avg:37.89ms
step:715/1840 train_time:27111ms step_avg:37.92ms
step:716/1840 train_time:27175ms step_avg:37.95ms
step:717/1840 train_time:27235ms step_avg:37.98ms
step:718/1840 train_time:27297ms step_avg:38.02ms
step:719/1840 train_time:27357ms step_avg:38.05ms
step:720/1840 train_time:27419ms step_avg:38.08ms
step:721/1840 train_time:27480ms step_avg:38.11ms
step:722/1840 train_time:27542ms step_avg:38.15ms
step:723/1840 train_time:27602ms step_avg:38.18ms
step:724/1840 train_time:27664ms step_avg:38.21ms
step:725/1840 train_time:27723ms step_avg:38.24ms
step:726/1840 train_time:27785ms step_avg:38.27ms
step:727/1840 train_time:27845ms step_avg:38.30ms
step:728/1840 train_time:27907ms step_avg:38.33ms
step:729/1840 train_time:27967ms step_avg:38.36ms
step:730/1840 train_time:28030ms step_avg:38.40ms
step:731/1840 train_time:28090ms step_avg:38.43ms
step:732/1840 train_time:28153ms step_avg:38.46ms
step:733/1840 train_time:28213ms step_avg:38.49ms
step:734/1840 train_time:28275ms step_avg:38.52ms
step:735/1840 train_time:28335ms step_avg:38.55ms
step:736/1840 train_time:28398ms step_avg:38.58ms
step:737/1840 train_time:28458ms step_avg:38.61ms
step:738/1840 train_time:28521ms step_avg:38.65ms
step:739/1840 train_time:28580ms step_avg:38.67ms
step:740/1840 train_time:28642ms step_avg:38.70ms
step:741/1840 train_time:28702ms step_avg:38.73ms
step:742/1840 train_time:28764ms step_avg:38.77ms
step:743/1840 train_time:28823ms step_avg:38.79ms
step:744/1840 train_time:28885ms step_avg:38.82ms
step:745/1840 train_time:28945ms step_avg:38.85ms
step:746/1840 train_time:29009ms step_avg:38.89ms
step:747/1840 train_time:29069ms step_avg:38.91ms
step:748/1840 train_time:29131ms step_avg:38.95ms
step:749/1840 train_time:29191ms step_avg:38.97ms
step:750/1840 train_time:29253ms step_avg:39.00ms
step:750/1840 val_loss:4.0232 train_time:29325ms step_avg:39.10ms
step:751/1840 train_time:29344ms step_avg:39.07ms
step:752/1840 train_time:29377ms step_avg:39.06ms
step:753/1840 train_time:29437ms step_avg:39.09ms
step:754/1840 train_time:29504ms step_avg:39.13ms
step:755/1840 train_time:29564ms step_avg:39.16ms
step:756/1840 train_time:29626ms step_avg:39.19ms
step:757/1840 train_time:29685ms step_avg:39.21ms
step:758/1840 train_time:29746ms step_avg:39.24ms
step:759/1840 train_time:29805ms step_avg:39.27ms
step:760/1840 train_time:29867ms step_avg:39.30ms
step:761/1840 train_time:29926ms step_avg:39.32ms
step:762/1840 train_time:29987ms step_avg:39.35ms
step:763/1840 train_time:30046ms step_avg:39.38ms
step:764/1840 train_time:30109ms step_avg:39.41ms
step:765/1840 train_time:30170ms step_avg:39.44ms
step:766/1840 train_time:30232ms step_avg:39.47ms
step:767/1840 train_time:30294ms step_avg:39.50ms
step:768/1840 train_time:30359ms step_avg:39.53ms
step:769/1840 train_time:30420ms step_avg:39.56ms
step:770/1840 train_time:30483ms step_avg:39.59ms
step:771/1840 train_time:30544ms step_avg:39.62ms
step:772/1840 train_time:30606ms step_avg:39.65ms
step:773/1840 train_time:30665ms step_avg:39.67ms
step:774/1840 train_time:30727ms step_avg:39.70ms
step:775/1840 train_time:30786ms step_avg:39.72ms
step:776/1840 train_time:30847ms step_avg:39.75ms
step:777/1840 train_time:30907ms step_avg:39.78ms
step:778/1840 train_time:30969ms step_avg:39.81ms
step:779/1840 train_time:31028ms step_avg:39.83ms
step:780/1840 train_time:31091ms step_avg:39.86ms
step:781/1840 train_time:31149ms step_avg:39.88ms
step:782/1840 train_time:31212ms step_avg:39.91ms
step:783/1840 train_time:31272ms step_avg:39.94ms
step:784/1840 train_time:31336ms step_avg:39.97ms
step:785/1840 train_time:31397ms step_avg:40.00ms
step:786/1840 train_time:31459ms step_avg:40.02ms
step:787/1840 train_time:31520ms step_avg:40.05ms
step:788/1840 train_time:31582ms step_avg:40.08ms
step:789/1840 train_time:31642ms step_avg:40.10ms
step:790/1840 train_time:31704ms step_avg:40.13ms
step:791/1840 train_time:31764ms step_avg:40.16ms
step:792/1840 train_time:31825ms step_avg:40.18ms
step:793/1840 train_time:31884ms step_avg:40.21ms
step:794/1840 train_time:31946ms step_avg:40.23ms
step:795/1840 train_time:32006ms step_avg:40.26ms
step:796/1840 train_time:32068ms step_avg:40.29ms
step:797/1840 train_time:32127ms step_avg:40.31ms
step:798/1840 train_time:32189ms step_avg:40.34ms
step:799/1840 train_time:32249ms step_avg:40.36ms
step:800/1840 train_time:32313ms step_avg:40.39ms
step:801/1840 train_time:32373ms step_avg:40.42ms
step:802/1840 train_time:32436ms step_avg:40.44ms
step:803/1840 train_time:32496ms step_avg:40.47ms
step:804/1840 train_time:32560ms step_avg:40.50ms
step:805/1840 train_time:32620ms step_avg:40.52ms
step:806/1840 train_time:32682ms step_avg:40.55ms
step:807/1840 train_time:32741ms step_avg:40.57ms
step:808/1840 train_time:32803ms step_avg:40.60ms
step:809/1840 train_time:32862ms step_avg:40.62ms
step:810/1840 train_time:32924ms step_avg:40.65ms
step:811/1840 train_time:32984ms step_avg:40.67ms
step:812/1840 train_time:33046ms step_avg:40.70ms
step:813/1840 train_time:33106ms step_avg:40.72ms
step:814/1840 train_time:33168ms step_avg:40.75ms
step:815/1840 train_time:33228ms step_avg:40.77ms
step:816/1840 train_time:33290ms step_avg:40.80ms
step:817/1840 train_time:33351ms step_avg:40.82ms
step:818/1840 train_time:33414ms step_avg:40.85ms
step:819/1840 train_time:33474ms step_avg:40.87ms
step:820/1840 train_time:33537ms step_avg:40.90ms
step:821/1840 train_time:33597ms step_avg:40.92ms
step:822/1840 train_time:33658ms step_avg:40.95ms
step:823/1840 train_time:33718ms step_avg:40.97ms
step:824/1840 train_time:33781ms step_avg:41.00ms
step:825/1840 train_time:33840ms step_avg:41.02ms
step:826/1840 train_time:33902ms step_avg:41.04ms
step:827/1840 train_time:33962ms step_avg:41.07ms
step:828/1840 train_time:34023ms step_avg:41.09ms
step:829/1840 train_time:34083ms step_avg:41.11ms
step:830/1840 train_time:34146ms step_avg:41.14ms
step:831/1840 train_time:34207ms step_avg:41.16ms
step:832/1840 train_time:34270ms step_avg:41.19ms
step:833/1840 train_time:34329ms step_avg:41.21ms
step:834/1840 train_time:34392ms step_avg:41.24ms
step:835/1840 train_time:34452ms step_avg:41.26ms
step:836/1840 train_time:34515ms step_avg:41.29ms
step:837/1840 train_time:34574ms step_avg:41.31ms
step:838/1840 train_time:34636ms step_avg:41.33ms
step:839/1840 train_time:34696ms step_avg:41.35ms
step:840/1840 train_time:34759ms step_avg:41.38ms
step:841/1840 train_time:34819ms step_avg:41.40ms
step:842/1840 train_time:34881ms step_avg:41.43ms
step:843/1840 train_time:34940ms step_avg:41.45ms
step:844/1840 train_time:35002ms step_avg:41.47ms
step:845/1840 train_time:35062ms step_avg:41.49ms
step:846/1840 train_time:35124ms step_avg:41.52ms
step:847/1840 train_time:35185ms step_avg:41.54ms
step:848/1840 train_time:35247ms step_avg:41.56ms
step:849/1840 train_time:35307ms step_avg:41.59ms
step:850/1840 train_time:35370ms step_avg:41.61ms
step:851/1840 train_time:35429ms step_avg:41.63ms
step:852/1840 train_time:35491ms step_avg:41.66ms
step:853/1840 train_time:35551ms step_avg:41.68ms
step:854/1840 train_time:35615ms step_avg:41.70ms
step:855/1840 train_time:35674ms step_avg:41.72ms
step:856/1840 train_time:35736ms step_avg:41.75ms
step:857/1840 train_time:35796ms step_avg:41.77ms
step:858/1840 train_time:35858ms step_avg:41.79ms
step:859/1840 train_time:35918ms step_avg:41.81ms
step:860/1840 train_time:35981ms step_avg:41.84ms
step:861/1840 train_time:36041ms step_avg:41.86ms
step:862/1840 train_time:36104ms step_avg:41.88ms
step:863/1840 train_time:36165ms step_avg:41.91ms
step:864/1840 train_time:36227ms step_avg:41.93ms
step:865/1840 train_time:36287ms step_avg:41.95ms
step:866/1840 train_time:36349ms step_avg:41.97ms
step:867/1840 train_time:36409ms step_avg:41.99ms
step:868/1840 train_time:36470ms step_avg:42.02ms
step:869/1840 train_time:36530ms step_avg:42.04ms
step:870/1840 train_time:36592ms step_avg:42.06ms
step:871/1840 train_time:36652ms step_avg:42.08ms
step:872/1840 train_time:36714ms step_avg:42.10ms
step:873/1840 train_time:36774ms step_avg:42.12ms
step:874/1840 train_time:36837ms step_avg:42.15ms
step:875/1840 train_time:36897ms step_avg:42.17ms
step:876/1840 train_time:36960ms step_avg:42.19ms
step:877/1840 train_time:37020ms step_avg:42.21ms
step:878/1840 train_time:37082ms step_avg:42.23ms
step:879/1840 train_time:37142ms step_avg:42.25ms
step:880/1840 train_time:37204ms step_avg:42.28ms
step:881/1840 train_time:37264ms step_avg:42.30ms
step:882/1840 train_time:37327ms step_avg:42.32ms
step:883/1840 train_time:37388ms step_avg:42.34ms
step:884/1840 train_time:37449ms step_avg:42.36ms
step:885/1840 train_time:37509ms step_avg:42.38ms
step:886/1840 train_time:37571ms step_avg:42.41ms
step:887/1840 train_time:37632ms step_avg:42.43ms
step:888/1840 train_time:37693ms step_avg:42.45ms
step:889/1840 train_time:37753ms step_avg:42.47ms
step:890/1840 train_time:37816ms step_avg:42.49ms
step:891/1840 train_time:37875ms step_avg:42.51ms
step:892/1840 train_time:37938ms step_avg:42.53ms
step:893/1840 train_time:37998ms step_avg:42.55ms
step:894/1840 train_time:38061ms step_avg:42.57ms
step:895/1840 train_time:38122ms step_avg:42.59ms
step:896/1840 train_time:38184ms step_avg:42.62ms
step:897/1840 train_time:38244ms step_avg:42.64ms
step:898/1840 train_time:38306ms step_avg:42.66ms
step:899/1840 train_time:38366ms step_avg:42.68ms
step:900/1840 train_time:38428ms step_avg:42.70ms
step:901/1840 train_time:38488ms step_avg:42.72ms
step:902/1840 train_time:38550ms step_avg:42.74ms
step:903/1840 train_time:38610ms step_avg:42.76ms
step:904/1840 train_time:38671ms step_avg:42.78ms
step:905/1840 train_time:38732ms step_avg:42.80ms
step:906/1840 train_time:38794ms step_avg:42.82ms
step:907/1840 train_time:38853ms step_avg:42.84ms
step:908/1840 train_time:38917ms step_avg:42.86ms
step:909/1840 train_time:38977ms step_avg:42.88ms
step:910/1840 train_time:39040ms step_avg:42.90ms
step:911/1840 train_time:39099ms step_avg:42.92ms
step:912/1840 train_time:39162ms step_avg:42.94ms
step:913/1840 train_time:39222ms step_avg:42.96ms
step:914/1840 train_time:39284ms step_avg:42.98ms
step:915/1840 train_time:39344ms step_avg:43.00ms
step:916/1840 train_time:39407ms step_avg:43.02ms
step:917/1840 train_time:39467ms step_avg:43.04ms
step:918/1840 train_time:39529ms step_avg:43.06ms
step:919/1840 train_time:39589ms step_avg:43.08ms
step:920/1840 train_time:39651ms step_avg:43.10ms
step:921/1840 train_time:39711ms step_avg:43.12ms
step:922/1840 train_time:39773ms step_avg:43.14ms
step:923/1840 train_time:39833ms step_avg:43.16ms
step:924/1840 train_time:39896ms step_avg:43.18ms
step:925/1840 train_time:39956ms step_avg:43.20ms
step:926/1840 train_time:40018ms step_avg:43.22ms
step:927/1840 train_time:40078ms step_avg:43.23ms
step:928/1840 train_time:40141ms step_avg:43.26ms
step:929/1840 train_time:40201ms step_avg:43.27ms
step:930/1840 train_time:40263ms step_avg:43.29ms
step:931/1840 train_time:40323ms step_avg:43.31ms
step:932/1840 train_time:40386ms step_avg:43.33ms
step:933/1840 train_time:40446ms step_avg:43.35ms
step:934/1840 train_time:40509ms step_avg:43.37ms
step:935/1840 train_time:40568ms step_avg:43.39ms
step:936/1840 train_time:40630ms step_avg:43.41ms
step:937/1840 train_time:40690ms step_avg:43.43ms
step:938/1840 train_time:40752ms step_avg:43.45ms
step:939/1840 train_time:40811ms step_avg:43.46ms
step:940/1840 train_time:40873ms step_avg:43.48ms
step:941/1840 train_time:40933ms step_avg:43.50ms
step:942/1840 train_time:40996ms step_avg:43.52ms
step:943/1840 train_time:41055ms step_avg:43.54ms
step:944/1840 train_time:41119ms step_avg:43.56ms
step:945/1840 train_time:41179ms step_avg:43.58ms
step:946/1840 train_time:41241ms step_avg:43.60ms
step:947/1840 train_time:41301ms step_avg:43.61ms
step:948/1840 train_time:41364ms step_avg:43.63ms
step:949/1840 train_time:41424ms step_avg:43.65ms
step:950/1840 train_time:41487ms step_avg:43.67ms
step:951/1840 train_time:41548ms step_avg:43.69ms
step:952/1840 train_time:41610ms step_avg:43.71ms
step:953/1840 train_time:41669ms step_avg:43.72ms
step:954/1840 train_time:41731ms step_avg:43.74ms
step:955/1840 train_time:41791ms step_avg:43.76ms
step:956/1840 train_time:41853ms step_avg:43.78ms
step:957/1840 train_time:41913ms step_avg:43.80ms
step:958/1840 train_time:41975ms step_avg:43.82ms
step:959/1840 train_time:42035ms step_avg:43.83ms
step:960/1840 train_time:42097ms step_avg:43.85ms
step:961/1840 train_time:42158ms step_avg:43.87ms
step:962/1840 train_time:42220ms step_avg:43.89ms
step:963/1840 train_time:42279ms step_avg:43.90ms
step:964/1840 train_time:42342ms step_avg:43.92ms
step:965/1840 train_time:42402ms step_avg:43.94ms
step:966/1840 train_time:42466ms step_avg:43.96ms
step:967/1840 train_time:42526ms step_avg:43.98ms
step:968/1840 train_time:42589ms step_avg:44.00ms
step:969/1840 train_time:42648ms step_avg:44.01ms
step:970/1840 train_time:42710ms step_avg:44.03ms
step:971/1840 train_time:42771ms step_avg:44.05ms
step:972/1840 train_time:42832ms step_avg:44.07ms
step:973/1840 train_time:42892ms step_avg:44.08ms
step:974/1840 train_time:42954ms step_avg:44.10ms
step:975/1840 train_time:43014ms step_avg:44.12ms
step:976/1840 train_time:43076ms step_avg:44.13ms
step:977/1840 train_time:43136ms step_avg:44.15ms
step:978/1840 train_time:43198ms step_avg:44.17ms
step:979/1840 train_time:43259ms step_avg:44.19ms
step:980/1840 train_time:43321ms step_avg:44.20ms
step:981/1840 train_time:43381ms step_avg:44.22ms
step:982/1840 train_time:43444ms step_avg:44.24ms
step:983/1840 train_time:43504ms step_avg:44.26ms
step:984/1840 train_time:43567ms step_avg:44.28ms
step:985/1840 train_time:43627ms step_avg:44.29ms
step:986/1840 train_time:43689ms step_avg:44.31ms
step:987/1840 train_time:43749ms step_avg:44.33ms
step:988/1840 train_time:43811ms step_avg:44.34ms
step:989/1840 train_time:43870ms step_avg:44.36ms
step:990/1840 train_time:43932ms step_avg:44.38ms
step:991/1840 train_time:43993ms step_avg:44.39ms
step:992/1840 train_time:44055ms step_avg:44.41ms
step:993/1840 train_time:44115ms step_avg:44.43ms
step:994/1840 train_time:44178ms step_avg:44.44ms
step:995/1840 train_time:44238ms step_avg:44.46ms
step:996/1840 train_time:44300ms step_avg:44.48ms
step:997/1840 train_time:44360ms step_avg:44.49ms
step:998/1840 train_time:44423ms step_avg:44.51ms
step:999/1840 train_time:44483ms step_avg:44.53ms
step:1000/1840 train_time:44545ms step_avg:44.54ms
step:1000/1840 val_loss:3.7747 train_time:44616ms step_avg:44.62ms
step:1001/1840 train_time:44635ms step_avg:44.59ms
step:1002/1840 train_time:44668ms step_avg:44.58ms
step:1003/1840 train_time:44730ms step_avg:44.60ms
step:1004/1840 train_time:44798ms step_avg:44.62ms
step:1005/1840 train_time:44857ms step_avg:44.63ms
step:1006/1840 train_time:44919ms step_avg:44.65ms
step:1007/1840 train_time:44978ms step_avg:44.67ms
step:1008/1840 train_time:45039ms step_avg:44.68ms
step:1009/1840 train_time:45099ms step_avg:44.70ms
step:1010/1840 train_time:45160ms step_avg:44.71ms
step:1011/1840 train_time:45219ms step_avg:44.73ms
step:1012/1840 train_time:45282ms step_avg:44.75ms
step:1013/1840 train_time:45341ms step_avg:44.76ms
step:1014/1840 train_time:45402ms step_avg:44.78ms
step:1015/1840 train_time:45462ms step_avg:44.79ms
step:1016/1840 train_time:45523ms step_avg:44.81ms
step:1017/1840 train_time:45585ms step_avg:44.82ms
step:1018/1840 train_time:45650ms step_avg:44.84ms
step:1019/1840 train_time:45712ms step_avg:44.86ms
step:1020/1840 train_time:45776ms step_avg:44.88ms
step:1021/1840 train_time:45835ms step_avg:44.89ms
step:1022/1840 train_time:45897ms step_avg:44.91ms
step:1023/1840 train_time:45957ms step_avg:44.92ms
step:1024/1840 train_time:46018ms step_avg:44.94ms
step:1025/1840 train_time:46077ms step_avg:44.95ms
step:1026/1840 train_time:46140ms step_avg:44.97ms
step:1027/1840 train_time:46199ms step_avg:44.98ms
step:1028/1840 train_time:46260ms step_avg:45.00ms
step:1029/1840 train_time:46320ms step_avg:45.01ms
step:1030/1840 train_time:46382ms step_avg:45.03ms
step:1031/1840 train_time:46441ms step_avg:45.05ms
step:1032/1840 train_time:46504ms step_avg:45.06ms
step:1033/1840 train_time:46564ms step_avg:45.08ms
step:1034/1840 train_time:46628ms step_avg:45.09ms
step:1035/1840 train_time:46689ms step_avg:45.11ms
step:1036/1840 train_time:46752ms step_avg:45.13ms
step:1037/1840 train_time:46813ms step_avg:45.14ms
step:1038/1840 train_time:46875ms step_avg:45.16ms
step:1039/1840 train_time:46935ms step_avg:45.17ms
step:1040/1840 train_time:46997ms step_avg:45.19ms
step:1041/1840 train_time:47055ms step_avg:45.20ms
step:1042/1840 train_time:47118ms step_avg:45.22ms
step:1043/1840 train_time:47176ms step_avg:45.23ms
step:1044/1840 train_time:47238ms step_avg:45.25ms
step:1045/1840 train_time:47298ms step_avg:45.26ms
step:1046/1840 train_time:47359ms step_avg:45.28ms
step:1047/1840 train_time:47419ms step_avg:45.29ms
step:1048/1840 train_time:47481ms step_avg:45.31ms
step:1049/1840 train_time:47541ms step_avg:45.32ms
step:1050/1840 train_time:47604ms step_avg:45.34ms
step:1051/1840 train_time:47664ms step_avg:45.35ms
step:1052/1840 train_time:47728ms step_avg:45.37ms
step:1053/1840 train_time:47789ms step_avg:45.38ms
step:1054/1840 train_time:47851ms step_avg:45.40ms
step:1055/1840 train_time:47912ms step_avg:45.41ms
step:1056/1840 train_time:47974ms step_avg:45.43ms
step:1057/1840 train_time:48034ms step_avg:45.44ms
step:1058/1840 train_time:48096ms step_avg:45.46ms
step:1059/1840 train_time:48155ms step_avg:45.47ms
step:1060/1840 train_time:48217ms step_avg:45.49ms
step:1061/1840 train_time:48276ms step_avg:45.50ms
step:1062/1840 train_time:48339ms step_avg:45.52ms
step:1063/1840 train_time:48398ms step_avg:45.53ms
step:1064/1840 train_time:48460ms step_avg:45.55ms
step:1065/1840 train_time:48520ms step_avg:45.56ms
step:1066/1840 train_time:48583ms step_avg:45.57ms
step:1067/1840 train_time:48643ms step_avg:45.59ms
step:1068/1840 train_time:48706ms step_avg:45.61ms
step:1069/1840 train_time:48767ms step_avg:45.62ms
step:1070/1840 train_time:48830ms step_avg:45.64ms
step:1071/1840 train_time:48890ms step_avg:45.65ms
step:1072/1840 train_time:48952ms step_avg:45.66ms
step:1073/1840 train_time:49013ms step_avg:45.68ms
step:1074/1840 train_time:49075ms step_avg:45.69ms
step:1075/1840 train_time:49134ms step_avg:45.71ms
step:1076/1840 train_time:49196ms step_avg:45.72ms
step:1077/1840 train_time:49256ms step_avg:45.73ms
step:1078/1840 train_time:49318ms step_avg:45.75ms
step:1079/1840 train_time:49377ms step_avg:45.76ms
step:1080/1840 train_time:49439ms step_avg:45.78ms
step:1081/1840 train_time:49498ms step_avg:45.79ms
step:1082/1840 train_time:49560ms step_avg:45.80ms
step:1083/1840 train_time:49621ms step_avg:45.82ms
step:1084/1840 train_time:49684ms step_avg:45.83ms
step:1085/1840 train_time:49745ms step_avg:45.85ms
step:1086/1840 train_time:49808ms step_avg:45.86ms
step:1087/1840 train_time:49867ms step_avg:45.88ms
step:1088/1840 train_time:49930ms step_avg:45.89ms
step:1089/1840 train_time:49991ms step_avg:45.91ms
step:1090/1840 train_time:50053ms step_avg:45.92ms
step:1091/1840 train_time:50113ms step_avg:45.93ms
step:1092/1840 train_time:50175ms step_avg:45.95ms
step:1093/1840 train_time:50234ms step_avg:45.96ms
step:1094/1840 train_time:50297ms step_avg:45.98ms
step:1095/1840 train_time:50357ms step_avg:45.99ms
step:1096/1840 train_time:50419ms step_avg:46.00ms
step:1097/1840 train_time:50477ms step_avg:46.01ms
step:1098/1840 train_time:50540ms step_avg:46.03ms
step:1099/1840 train_time:50599ms step_avg:46.04ms
step:1100/1840 train_time:50663ms step_avg:46.06ms
step:1101/1840 train_time:50723ms step_avg:46.07ms
step:1102/1840 train_time:50786ms step_avg:46.09ms
step:1103/1840 train_time:50846ms step_avg:46.10ms
step:1104/1840 train_time:50910ms step_avg:46.11ms
step:1105/1840 train_time:50970ms step_avg:46.13ms
step:1106/1840 train_time:51033ms step_avg:46.14ms
step:1107/1840 train_time:51093ms step_avg:46.15ms
step:1108/1840 train_time:51155ms step_avg:46.17ms
step:1109/1840 train_time:51214ms step_avg:46.18ms
step:1110/1840 train_time:51276ms step_avg:46.19ms
step:1111/1840 train_time:51335ms step_avg:46.21ms
step:1112/1840 train_time:51397ms step_avg:46.22ms
step:1113/1840 train_time:51458ms step_avg:46.23ms
step:1114/1840 train_time:51520ms step_avg:46.25ms
step:1115/1840 train_time:51580ms step_avg:46.26ms
step:1116/1840 train_time:51643ms step_avg:46.28ms
step:1117/1840 train_time:51703ms step_avg:46.29ms
step:1118/1840 train_time:51766ms step_avg:46.30ms
step:1119/1840 train_time:51827ms step_avg:46.32ms
step:1120/1840 train_time:51890ms step_avg:46.33ms
step:1121/1840 train_time:51949ms step_avg:46.34ms
step:1122/1840 train_time:52013ms step_avg:46.36ms
step:1123/1840 train_time:52073ms step_avg:46.37ms
step:1124/1840 train_time:52135ms step_avg:46.38ms
step:1125/1840 train_time:52194ms step_avg:46.39ms
step:1126/1840 train_time:52256ms step_avg:46.41ms
step:1127/1840 train_time:52315ms step_avg:46.42ms
step:1128/1840 train_time:52377ms step_avg:46.43ms
step:1129/1840 train_time:52436ms step_avg:46.44ms
step:1130/1840 train_time:52499ms step_avg:46.46ms
step:1131/1840 train_time:52559ms step_avg:46.47ms
step:1132/1840 train_time:52623ms step_avg:46.49ms
step:1133/1840 train_time:52683ms step_avg:46.50ms
step:1134/1840 train_time:52745ms step_avg:46.51ms
step:1135/1840 train_time:52806ms step_avg:46.52ms
step:1136/1840 train_time:52868ms step_avg:46.54ms
step:1137/1840 train_time:52928ms step_avg:46.55ms
step:1138/1840 train_time:52991ms step_avg:46.57ms
step:1139/1840 train_time:53051ms step_avg:46.58ms
step:1140/1840 train_time:53114ms step_avg:46.59ms
step:1141/1840 train_time:53174ms step_avg:46.60ms
step:1142/1840 train_time:53236ms step_avg:46.62ms
step:1143/1840 train_time:53296ms step_avg:46.63ms
step:1144/1840 train_time:53357ms step_avg:46.64ms
step:1145/1840 train_time:53417ms step_avg:46.65ms
step:1146/1840 train_time:53479ms step_avg:46.67ms
step:1147/1840 train_time:53538ms step_avg:46.68ms
step:1148/1840 train_time:53600ms step_avg:46.69ms
step:1149/1840 train_time:53660ms step_avg:46.70ms
step:1150/1840 train_time:53724ms step_avg:46.72ms
step:1151/1840 train_time:53784ms step_avg:46.73ms
step:1152/1840 train_time:53847ms step_avg:46.74ms
step:1153/1840 train_time:53908ms step_avg:46.75ms
step:1154/1840 train_time:53971ms step_avg:46.77ms
step:1155/1840 train_time:54030ms step_avg:46.78ms
step:1156/1840 train_time:54093ms step_avg:46.79ms
step:1157/1840 train_time:54153ms step_avg:46.80ms
step:1158/1840 train_time:54215ms step_avg:46.82ms
step:1159/1840 train_time:54275ms step_avg:46.83ms
step:1160/1840 train_time:54337ms step_avg:46.84ms
step:1161/1840 train_time:54397ms step_avg:46.85ms
step:1162/1840 train_time:54458ms step_avg:46.87ms
step:1163/1840 train_time:54519ms step_avg:46.88ms
step:1164/1840 train_time:54580ms step_avg:46.89ms
step:1165/1840 train_time:54640ms step_avg:46.90ms
step:1166/1840 train_time:54703ms step_avg:46.91ms
step:1167/1840 train_time:54763ms step_avg:46.93ms
step:1168/1840 train_time:54826ms step_avg:46.94ms
step:1169/1840 train_time:54886ms step_avg:46.95ms
step:1170/1840 train_time:54948ms step_avg:46.96ms
step:1171/1840 train_time:55008ms step_avg:46.98ms
step:1172/1840 train_time:55070ms step_avg:46.99ms
step:1173/1840 train_time:55130ms step_avg:47.00ms
step:1174/1840 train_time:55192ms step_avg:47.01ms
step:1175/1840 train_time:55253ms step_avg:47.02ms
step:1176/1840 train_time:55314ms step_avg:47.04ms
step:1177/1840 train_time:55375ms step_avg:47.05ms
step:1178/1840 train_time:55437ms step_avg:47.06ms
step:1179/1840 train_time:55497ms step_avg:47.07ms
step:1180/1840 train_time:55559ms step_avg:47.08ms
step:1181/1840 train_time:55619ms step_avg:47.09ms
step:1182/1840 train_time:55680ms step_avg:47.11ms
step:1183/1840 train_time:55741ms step_avg:47.12ms
step:1184/1840 train_time:55804ms step_avg:47.13ms
step:1185/1840 train_time:55864ms step_avg:47.14ms
step:1186/1840 train_time:55927ms step_avg:47.16ms
step:1187/1840 train_time:55987ms step_avg:47.17ms
step:1188/1840 train_time:56049ms step_avg:47.18ms
step:1189/1840 train_time:56109ms step_avg:47.19ms
step:1190/1840 train_time:56171ms step_avg:47.20ms
step:1191/1840 train_time:56231ms step_avg:47.21ms
step:1192/1840 train_time:56294ms step_avg:47.23ms
step:1193/1840 train_time:56354ms step_avg:47.24ms
step:1194/1840 train_time:56417ms step_avg:47.25ms
step:1195/1840 train_time:56476ms step_avg:47.26ms
step:1196/1840 train_time:56539ms step_avg:47.27ms
step:1197/1840 train_time:56598ms step_avg:47.28ms
step:1198/1840 train_time:56660ms step_avg:47.30ms
step:1199/1840 train_time:56720ms step_avg:47.31ms
step:1200/1840 train_time:56783ms step_avg:47.32ms
step:1201/1840 train_time:56844ms step_avg:47.33ms
step:1202/1840 train_time:56928ms step_avg:47.36ms
step:1203/1840 train_time:57017ms step_avg:47.40ms
step:1204/1840 train_time:57107ms step_avg:47.43ms
step:1205/1840 train_time:57196ms step_avg:47.47ms
step:1206/1840 train_time:57286ms step_avg:47.50ms
step:1207/1840 train_time:57373ms step_avg:47.53ms
step:1208/1840 train_time:57462ms step_avg:47.57ms
step:1209/1840 train_time:57548ms step_avg:47.60ms
step:1210/1840 train_time:57637ms step_avg:47.63ms
step:1211/1840 train_time:57723ms step_avg:47.67ms
step:1212/1840 train_time:57811ms step_avg:47.70ms
step:1213/1840 train_time:57898ms step_avg:47.73ms
step:1214/1840 train_time:57987ms step_avg:47.77ms
step:1215/1840 train_time:58074ms step_avg:47.80ms
step:1216/1840 train_time:58163ms step_avg:47.83ms
step:1217/1840 train_time:58251ms step_avg:47.86ms
step:1218/1840 train_time:58342ms step_avg:47.90ms
step:1219/1840 train_time:58427ms step_avg:47.93ms
step:1220/1840 train_time:58516ms step_avg:47.96ms
step:1221/1840 train_time:58603ms step_avg:48.00ms
step:1222/1840 train_time:58691ms step_avg:48.03ms
step:1223/1840 train_time:58776ms step_avg:48.06ms
step:1224/1840 train_time:58864ms step_avg:48.09ms
step:1225/1840 train_time:58952ms step_avg:48.12ms
step:1226/1840 train_time:59041ms step_avg:48.16ms
step:1227/1840 train_time:59127ms step_avg:48.19ms
step:1228/1840 train_time:59218ms step_avg:48.22ms
step:1229/1840 train_time:59304ms step_avg:48.25ms
step:1230/1840 train_time:59393ms step_avg:48.29ms
step:1231/1840 train_time:59481ms step_avg:48.32ms
step:1232/1840 train_time:59569ms step_avg:48.35ms
step:1233/1840 train_time:59655ms step_avg:48.38ms
step:1234/1840 train_time:59744ms step_avg:48.41ms
step:1235/1840 train_time:59829ms step_avg:48.44ms
step:1236/1840 train_time:59919ms step_avg:48.48ms
step:1237/1840 train_time:60004ms step_avg:48.51ms
step:1238/1840 train_time:60096ms step_avg:48.54ms
step:1239/1840 train_time:60182ms step_avg:48.57ms
step:1240/1840 train_time:60270ms step_avg:48.60ms
step:1241/1840 train_time:60355ms step_avg:48.63ms
step:1242/1840 train_time:60445ms step_avg:48.67ms
step:1243/1840 train_time:60531ms step_avg:48.70ms
step:1244/1840 train_time:60620ms step_avg:48.73ms
step:1245/1840 train_time:60706ms step_avg:48.76ms
step:1246/1840 train_time:60795ms step_avg:48.79ms
step:1247/1840 train_time:60882ms step_avg:48.82ms
step:1248/1840 train_time:60969ms step_avg:48.85ms
step:1249/1840 train_time:61056ms step_avg:48.88ms
step:1250/1840 train_time:61145ms step_avg:48.92ms
step:1250/1840 val_loss:3.5343 train_time:61246ms step_avg:49.00ms
step:1251/1840 train_time:61266ms step_avg:48.97ms
step:1252/1840 train_time:61322ms step_avg:48.98ms
step:1253/1840 train_time:61411ms step_avg:49.01ms
step:1254/1840 train_time:61502ms step_avg:49.04ms
step:1255/1840 train_time:61587ms step_avg:49.07ms
step:1256/1840 train_time:61676ms step_avg:49.11ms
step:1257/1840 train_time:61761ms step_avg:49.13ms
step:1258/1840 train_time:61848ms step_avg:49.16ms
step:1259/1840 train_time:61934ms step_avg:49.19ms
step:1260/1840 train_time:62023ms step_avg:49.22ms
step:1261/1840 train_time:62108ms step_avg:49.25ms
step:1262/1840 train_time:62199ms step_avg:49.29ms
step:1263/1840 train_time:62288ms step_avg:49.32ms
step:1264/1840 train_time:62381ms step_avg:49.35ms
step:1265/1840 train_time:62468ms step_avg:49.38ms
step:1266/1840 train_time:62558ms step_avg:49.41ms
step:1267/1840 train_time:62643ms step_avg:49.44ms
step:1268/1840 train_time:62732ms step_avg:49.47ms
step:1269/1840 train_time:62817ms step_avg:49.50ms
step:1270/1840 train_time:62905ms step_avg:49.53ms
step:1271/1840 train_time:62990ms step_avg:49.56ms
step:1272/1840 train_time:63079ms step_avg:49.59ms
step:1273/1840 train_time:63166ms step_avg:49.62ms
step:1274/1840 train_time:63257ms step_avg:49.65ms
step:1275/1840 train_time:63347ms step_avg:49.68ms
step:1276/1840 train_time:63436ms step_avg:49.71ms
step:1277/1840 train_time:63523ms step_avg:49.74ms
step:1278/1840 train_time:63611ms step_avg:49.77ms
step:1279/1840 train_time:63696ms step_avg:49.80ms
step:1280/1840 train_time:63785ms step_avg:49.83ms
step:1281/1840 train_time:63870ms step_avg:49.86ms
step:1282/1840 train_time:63958ms step_avg:49.89ms
step:1283/1840 train_time:64043ms step_avg:49.92ms
step:1284/1840 train_time:64132ms step_avg:49.95ms
step:1285/1840 train_time:64218ms step_avg:49.98ms
step:1286/1840 train_time:64308ms step_avg:50.01ms
step:1287/1840 train_time:64395ms step_avg:50.04ms
step:1288/1840 train_time:64485ms step_avg:50.07ms
step:1289/1840 train_time:64570ms step_avg:50.09ms
step:1290/1840 train_time:64659ms step_avg:50.12ms
step:1291/1840 train_time:64745ms step_avg:50.15ms
step:1292/1840 train_time:64833ms step_avg:50.18ms
step:1293/1840 train_time:64919ms step_avg:50.21ms
step:1294/1840 train_time:65007ms step_avg:50.24ms
step:1295/1840 train_time:65092ms step_avg:50.26ms
step:1296/1840 train_time:65184ms step_avg:50.30ms
step:1297/1840 train_time:65270ms step_avg:50.32ms
step:1298/1840 train_time:65361ms step_avg:50.36ms
step:1299/1840 train_time:65447ms step_avg:50.38ms
step:1300/1840 train_time:65536ms step_avg:50.41ms
step:1301/1840 train_time:65623ms step_avg:50.44ms
step:1302/1840 train_time:65712ms step_avg:50.47ms
step:1303/1840 train_time:65797ms step_avg:50.50ms
step:1304/1840 train_time:65885ms step_avg:50.53ms
step:1305/1840 train_time:65971ms step_avg:50.55ms
step:1306/1840 train_time:66060ms step_avg:50.58ms
step:1307/1840 train_time:66147ms step_avg:50.61ms
step:1308/1840 train_time:66236ms step_avg:50.64ms
step:1309/1840 train_time:66323ms step_avg:50.67ms
step:1310/1840 train_time:66410ms step_avg:50.69ms
step:1311/1840 train_time:66497ms step_avg:50.72ms
step:1312/1840 train_time:66584ms step_avg:50.75ms
step:1313/1840 train_time:66671ms step_avg:50.78ms
step:1314/1840 train_time:66760ms step_avg:50.81ms
step:1315/1840 train_time:66845ms step_avg:50.83ms
step:1316/1840 train_time:66935ms step_avg:50.86ms
step:1317/1840 train_time:67021ms step_avg:50.89ms
step:1318/1840 train_time:67108ms step_avg:50.92ms
step:1319/1840 train_time:67196ms step_avg:50.94ms
step:1320/1840 train_time:67287ms step_avg:50.97ms
step:1321/1840 train_time:67372ms step_avg:51.00ms
step:1322/1840 train_time:67461ms step_avg:51.03ms
step:1323/1840 train_time:67547ms step_avg:51.06ms
step:1324/1840 train_time:67635ms step_avg:51.08ms
step:1325/1840 train_time:67721ms step_avg:51.11ms
step:1326/1840 train_time:67808ms step_avg:51.14ms
step:1327/1840 train_time:67897ms step_avg:51.17ms
step:1328/1840 train_time:67986ms step_avg:51.19ms
step:1329/1840 train_time:68071ms step_avg:51.22ms
step:1330/1840 train_time:68161ms step_avg:51.25ms
step:1331/1840 train_time:68246ms step_avg:51.27ms
step:1332/1840 train_time:68337ms step_avg:51.30ms
step:1333/1840 train_time:68424ms step_avg:51.33ms
step:1334/1840 train_time:68512ms step_avg:51.36ms
step:1335/1840 train_time:68598ms step_avg:51.38ms
step:1336/1840 train_time:68686ms step_avg:51.41ms
step:1337/1840 train_time:68771ms step_avg:51.44ms
step:1338/1840 train_time:68862ms step_avg:51.47ms
step:1339/1840 train_time:68948ms step_avg:51.49ms
step:1340/1840 train_time:69037ms step_avg:51.52ms
step:1341/1840 train_time:69124ms step_avg:51.55ms
step:1342/1840 train_time:69212ms step_avg:51.57ms
step:1343/1840 train_time:69299ms step_avg:51.60ms
step:1344/1840 train_time:69388ms step_avg:51.63ms
step:1345/1840 train_time:69474ms step_avg:51.65ms
step:1346/1840 train_time:69564ms step_avg:51.68ms
step:1347/1840 train_time:69649ms step_avg:51.71ms
step:1348/1840 train_time:69738ms step_avg:51.73ms
step:1349/1840 train_time:69824ms step_avg:51.76ms
step:1350/1840 train_time:69915ms step_avg:51.79ms
step:1351/1840 train_time:70001ms step_avg:51.81ms
step:1352/1840 train_time:70088ms step_avg:51.84ms
step:1353/1840 train_time:70174ms step_avg:51.87ms
step:1354/1840 train_time:70264ms step_avg:51.89ms
step:1355/1840 train_time:70349ms step_avg:51.92ms
step:1356/1840 train_time:70439ms step_avg:51.95ms
step:1357/1840 train_time:70526ms step_avg:51.97ms
step:1358/1840 train_time:70616ms step_avg:52.00ms
step:1359/1840 train_time:70702ms step_avg:52.02ms
step:1360/1840 train_time:70791ms step_avg:52.05ms
step:1361/1840 train_time:70878ms step_avg:52.08ms
step:1362/1840 train_time:70967ms step_avg:52.10ms
step:1363/1840 train_time:71052ms step_avg:52.13ms
step:1364/1840 train_time:71142ms step_avg:52.16ms
step:1365/1840 train_time:71228ms step_avg:52.18ms
step:1366/1840 train_time:71317ms step_avg:52.21ms
step:1367/1840 train_time:71404ms step_avg:52.23ms
step:1368/1840 train_time:71493ms step_avg:52.26ms
step:1369/1840 train_time:71578ms step_avg:52.29ms
step:1370/1840 train_time:71667ms step_avg:52.31ms
step:1371/1840 train_time:71753ms step_avg:52.34ms
step:1372/1840 train_time:71841ms step_avg:52.36ms
step:1373/1840 train_time:71927ms step_avg:52.39ms
step:1374/1840 train_time:72016ms step_avg:52.41ms
step:1375/1840 train_time:72104ms step_avg:52.44ms
step:1376/1840 train_time:72192ms step_avg:52.47ms
step:1377/1840 train_time:72277ms step_avg:52.49ms
step:1378/1840 train_time:72366ms step_avg:52.52ms
step:1379/1840 train_time:72452ms step_avg:52.54ms
step:1380/1840 train_time:72541ms step_avg:52.57ms
step:1381/1840 train_time:72627ms step_avg:52.59ms
step:1382/1840 train_time:72716ms step_avg:52.62ms
step:1383/1840 train_time:72804ms step_avg:52.64ms
step:1384/1840 train_time:72892ms step_avg:52.67ms
step:1385/1840 train_time:72979ms step_avg:52.69ms
step:1386/1840 train_time:73068ms step_avg:52.72ms
step:1387/1840 train_time:73153ms step_avg:52.74ms
step:1388/1840 train_time:73244ms step_avg:52.77ms
step:1389/1840 train_time:73329ms step_avg:52.79ms
step:1390/1840 train_time:73417ms step_avg:52.82ms
step:1391/1840 train_time:73504ms step_avg:52.84ms
step:1392/1840 train_time:73593ms step_avg:52.87ms
step:1393/1840 train_time:73680ms step_avg:52.89ms
step:1394/1840 train_time:73768ms step_avg:52.92ms
step:1395/1840 train_time:73855ms step_avg:52.94ms
step:1396/1840 train_time:73944ms step_avg:52.97ms
step:1397/1840 train_time:74030ms step_avg:52.99ms
step:1398/1840 train_time:74117ms step_avg:53.02ms
step:1399/1840 train_time:74204ms step_avg:53.04ms
step:1400/1840 train_time:74293ms step_avg:53.07ms
step:1401/1840 train_time:74379ms step_avg:53.09ms
step:1402/1840 train_time:74467ms step_avg:53.11ms
step:1403/1840 train_time:74553ms step_avg:53.14ms
step:1404/1840 train_time:74642ms step_avg:53.16ms
step:1405/1840 train_time:74727ms step_avg:53.19ms
step:1406/1840 train_time:74816ms step_avg:53.21ms
step:1407/1840 train_time:74904ms step_avg:53.24ms
step:1408/1840 train_time:74993ms step_avg:53.26ms
step:1409/1840 train_time:75079ms step_avg:53.29ms
step:1410/1840 train_time:75167ms step_avg:53.31ms
step:1411/1840 train_time:75254ms step_avg:53.33ms
step:1412/1840 train_time:75343ms step_avg:53.36ms
step:1413/1840 train_time:75430ms step_avg:53.38ms
step:1414/1840 train_time:75518ms step_avg:53.41ms
step:1415/1840 train_time:75604ms step_avg:53.43ms
step:1416/1840 train_time:75691ms step_avg:53.45ms
step:1417/1840 train_time:75780ms step_avg:53.48ms
step:1418/1840 train_time:75868ms step_avg:53.50ms
step:1419/1840 train_time:75955ms step_avg:53.53ms
step:1420/1840 train_time:76045ms step_avg:53.55ms
step:1421/1840 train_time:76131ms step_avg:53.58ms
step:1422/1840 train_time:76220ms step_avg:53.60ms
step:1423/1840 train_time:76306ms step_avg:53.62ms
step:1424/1840 train_time:76394ms step_avg:53.65ms
step:1425/1840 train_time:76480ms step_avg:53.67ms
step:1426/1840 train_time:76568ms step_avg:53.69ms
step:1427/1840 train_time:76653ms step_avg:53.72ms
step:1428/1840 train_time:76744ms step_avg:53.74ms
step:1429/1840 train_time:76830ms step_avg:53.76ms
step:1430/1840 train_time:76918ms step_avg:53.79ms
step:1431/1840 train_time:77005ms step_avg:53.81ms
step:1432/1840 train_time:77094ms step_avg:53.84ms
step:1433/1840 train_time:77180ms step_avg:53.86ms
step:1434/1840 train_time:77268ms step_avg:53.88ms
step:1435/1840 train_time:77354ms step_avg:53.91ms
step:1436/1840 train_time:77443ms step_avg:53.93ms
step:1437/1840 train_time:77529ms step_avg:53.95ms
step:1438/1840 train_time:77618ms step_avg:53.98ms
step:1439/1840 train_time:77706ms step_avg:54.00ms
step:1440/1840 train_time:77795ms step_avg:54.02ms
step:1441/1840 train_time:77881ms step_avg:54.05ms
step:1442/1840 train_time:77968ms step_avg:54.07ms
step:1443/1840 train_time:78055ms step_avg:54.09ms
step:1444/1840 train_time:78145ms step_avg:54.12ms
step:1445/1840 train_time:78232ms step_avg:54.14ms
step:1446/1840 train_time:78320ms step_avg:54.16ms
step:1447/1840 train_time:78406ms step_avg:54.19ms
step:1448/1840 train_time:78495ms step_avg:54.21ms
step:1449/1840 train_time:78581ms step_avg:54.23ms
step:1450/1840 train_time:78670ms step_avg:54.26ms
step:1451/1840 train_time:78757ms step_avg:54.28ms
step:1452/1840 train_time:78845ms step_avg:54.30ms
step:1453/1840 train_time:78932ms step_avg:54.32ms
step:1454/1840 train_time:79020ms step_avg:54.35ms
step:1455/1840 train_time:79106ms step_avg:54.37ms
step:1456/1840 train_time:79195ms step_avg:54.39ms
step:1457/1840 train_time:79281ms step_avg:54.41ms
step:1458/1840 train_time:79369ms step_avg:54.44ms
step:1459/1840 train_time:79456ms step_avg:54.46ms
step:1460/1840 train_time:79545ms step_avg:54.48ms
step:1461/1840 train_time:79631ms step_avg:54.50ms
step:1462/1840 train_time:79719ms step_avg:54.53ms
step:1463/1840 train_time:79806ms step_avg:54.55ms
step:1464/1840 train_time:79894ms step_avg:54.57ms
step:1465/1840 train_time:79980ms step_avg:54.59ms
step:1466/1840 train_time:80068ms step_avg:54.62ms
step:1467/1840 train_time:80154ms step_avg:54.64ms
step:1468/1840 train_time:80243ms step_avg:54.66ms
step:1469/1840 train_time:80329ms step_avg:54.68ms
step:1470/1840 train_time:80418ms step_avg:54.71ms
step:1471/1840 train_time:80506ms step_avg:54.73ms
step:1472/1840 train_time:80594ms step_avg:54.75ms
step:1473/1840 train_time:80679ms step_avg:54.77ms
step:1474/1840 train_time:80767ms step_avg:54.79ms
step:1475/1840 train_time:80854ms step_avg:54.82ms
step:1476/1840 train_time:80944ms step_avg:54.84ms
step:1477/1840 train_time:81029ms step_avg:54.86ms
step:1478/1840 train_time:81118ms step_avg:54.88ms
step:1479/1840 train_time:81205ms step_avg:54.91ms
step:1480/1840 train_time:81293ms step_avg:54.93ms
step:1481/1840 train_time:81380ms step_avg:54.95ms
step:1482/1840 train_time:81468ms step_avg:54.97ms
step:1483/1840 train_time:81555ms step_avg:54.99ms
step:1484/1840 train_time:81644ms step_avg:55.02ms
step:1485/1840 train_time:81731ms step_avg:55.04ms
step:1486/1840 train_time:81821ms step_avg:55.06ms
step:1487/1840 train_time:81907ms step_avg:55.08ms
step:1488/1840 train_time:81997ms step_avg:55.11ms
step:1489/1840 train_time:82083ms step_avg:55.13ms
step:1490/1840 train_time:82170ms step_avg:55.15ms
step:1491/1840 train_time:82257ms step_avg:55.17ms
step:1492/1840 train_time:82346ms step_avg:55.19ms
step:1493/1840 train_time:82431ms step_avg:55.21ms
step:1494/1840 train_time:82520ms step_avg:55.23ms
step:1495/1840 train_time:82606ms step_avg:55.25ms
step:1496/1840 train_time:82696ms step_avg:55.28ms
step:1497/1840 train_time:82782ms step_avg:55.30ms
step:1498/1840 train_time:82870ms step_avg:55.32ms
step:1499/1840 train_time:82957ms step_avg:55.34ms
step:1500/1840 train_time:83046ms step_avg:55.36ms
step:1500/1840 val_loss:3.4028 train_time:83148ms step_avg:55.43ms
step:1501/1840 train_time:83169ms step_avg:55.41ms
step:1502/1840 train_time:83228ms step_avg:55.41ms
step:1503/1840 train_time:83317ms step_avg:55.43ms
step:1504/1840 train_time:83406ms step_avg:55.46ms
step:1505/1840 train_time:83492ms step_avg:55.48ms
step:1506/1840 train_time:83580ms step_avg:55.50ms
step:1507/1840 train_time:83665ms step_avg:55.52ms
step:1508/1840 train_time:83753ms step_avg:55.54ms
step:1509/1840 train_time:83838ms step_avg:55.56ms
step:1510/1840 train_time:83926ms step_avg:55.58ms
step:1511/1840 train_time:84011ms step_avg:55.60ms
step:1512/1840 train_time:84101ms step_avg:55.62ms
step:1513/1840 train_time:84191ms step_avg:55.65ms
step:1514/1840 train_time:84284ms step_avg:55.67ms
step:1515/1840 train_time:84371ms step_avg:55.69ms
step:1516/1840 train_time:84459ms step_avg:55.71ms
step:1517/1840 train_time:84546ms step_avg:55.73ms
step:1518/1840 train_time:84632ms step_avg:55.75ms
step:1519/1840 train_time:84718ms step_avg:55.77ms
step:1520/1840 train_time:84807ms step_avg:55.79ms
step:1521/1840 train_time:84892ms step_avg:55.81ms
step:1522/1840 train_time:84981ms step_avg:55.84ms
step:1523/1840 train_time:85068ms step_avg:55.86ms
step:1524/1840 train_time:85158ms step_avg:55.88ms
step:1525/1840 train_time:85246ms step_avg:55.90ms
step:1526/1840 train_time:85333ms step_avg:55.92ms
step:1527/1840 train_time:85422ms step_avg:55.94ms
step:1528/1840 train_time:85510ms step_avg:55.96ms
step:1529/1840 train_time:85595ms step_avg:55.98ms
step:1530/1840 train_time:85685ms step_avg:56.00ms
step:1531/1840 train_time:85769ms step_avg:56.02ms
step:1532/1840 train_time:85858ms step_avg:56.04ms
step:1533/1840 train_time:85943ms step_avg:56.06ms
step:1534/1840 train_time:86031ms step_avg:56.08ms
step:1535/1840 train_time:86121ms step_avg:56.10ms
step:1536/1840 train_time:86210ms step_avg:56.13ms
step:1537/1840 train_time:86296ms step_avg:56.15ms
step:1538/1840 train_time:86388ms step_avg:56.17ms
step:1539/1840 train_time:86473ms step_avg:56.19ms
step:1540/1840 train_time:86564ms step_avg:56.21ms
step:1541/1840 train_time:86649ms step_avg:56.23ms
step:1542/1840 train_time:86737ms step_avg:56.25ms
step:1543/1840 train_time:86823ms step_avg:56.27ms
step:1544/1840 train_time:86910ms step_avg:56.29ms
step:1545/1840 train_time:86997ms step_avg:56.31ms
step:1546/1840 train_time:87087ms step_avg:56.33ms
step:1547/1840 train_time:87173ms step_avg:56.35ms
step:1548/1840 train_time:87262ms step_avg:56.37ms
step:1549/1840 train_time:87349ms step_avg:56.39ms
step:1550/1840 train_time:87438ms step_avg:56.41ms
step:1551/1840 train_time:87525ms step_avg:56.43ms
step:1552/1840 train_time:87612ms step_avg:56.45ms
step:1553/1840 train_time:87697ms step_avg:56.47ms
step:1554/1840 train_time:87788ms step_avg:56.49ms
step:1555/1840 train_time:87872ms step_avg:56.51ms
step:1556/1840 train_time:87962ms step_avg:56.53ms
step:1557/1840 train_time:88048ms step_avg:56.55ms
step:1558/1840 train_time:88138ms step_avg:56.57ms
step:1559/1840 train_time:88225ms step_avg:56.59ms
step:1560/1840 train_time:88313ms step_avg:56.61ms
step:1561/1840 train_time:88400ms step_avg:56.63ms
step:1562/1840 train_time:88490ms step_avg:56.65ms
step:1563/1840 train_time:88574ms step_avg:56.67ms
step:1564/1840 train_time:88662ms step_avg:56.69ms
step:1565/1840 train_time:88747ms step_avg:56.71ms
step:1566/1840 train_time:88836ms step_avg:56.73ms
step:1567/1840 train_time:88921ms step_avg:56.75ms
step:1568/1840 train_time:89010ms step_avg:56.77ms
step:1569/1840 train_time:89097ms step_avg:56.79ms
step:1570/1840 train_time:89186ms step_avg:56.81ms
step:1571/1840 train_time:89271ms step_avg:56.82ms
step:1572/1840 train_time:89362ms step_avg:56.85ms
step:1573/1840 train_time:89449ms step_avg:56.87ms
step:1574/1840 train_time:89537ms step_avg:56.89ms
step:1575/1840 train_time:89622ms step_avg:56.90ms
step:1576/1840 train_time:89711ms step_avg:56.92ms
step:1577/1840 train_time:89797ms step_avg:56.94ms
step:1578/1840 train_time:89886ms step_avg:56.96ms
step:1579/1840 train_time:89971ms step_avg:56.98ms
step:1580/1840 train_time:90061ms step_avg:57.00ms
step:1581/1840 train_time:90147ms step_avg:57.02ms
step:1582/1840 train_time:90234ms step_avg:57.04ms
step:1583/1840 train_time:90323ms step_avg:57.06ms
step:1584/1840 train_time:90411ms step_avg:57.08ms
step:1585/1840 train_time:90498ms step_avg:57.10ms
step:1586/1840 train_time:90588ms step_avg:57.12ms
step:1587/1840 train_time:90673ms step_avg:57.13ms
step:1588/1840 train_time:90761ms step_avg:57.15ms
step:1589/1840 train_time:90847ms step_avg:57.17ms
step:1590/1840 train_time:90936ms step_avg:57.19ms
step:1591/1840 train_time:91023ms step_avg:57.21ms
step:1592/1840 train_time:91111ms step_avg:57.23ms
step:1593/1840 train_time:91197ms step_avg:57.25ms
step:1594/1840 train_time:91288ms step_avg:57.27ms
step:1595/1840 train_time:91373ms step_avg:57.29ms
step:1596/1840 train_time:91462ms step_avg:57.31ms
step:1597/1840 train_time:91548ms step_avg:57.32ms
step:1598/1840 train_time:91637ms step_avg:57.34ms
step:1599/1840 train_time:91723ms step_avg:57.36ms
step:1600/1840 train_time:91811ms step_avg:57.38ms
step:1601/1840 train_time:91897ms step_avg:57.40ms
step:1602/1840 train_time:91987ms step_avg:57.42ms
step:1603/1840 train_time:92073ms step_avg:57.44ms
step:1604/1840 train_time:92161ms step_avg:57.46ms
step:1605/1840 train_time:92247ms step_avg:57.47ms
step:1606/1840 train_time:92336ms step_avg:57.49ms
step:1607/1840 train_time:92423ms step_avg:57.51ms
step:1608/1840 train_time:92511ms step_avg:57.53ms
step:1609/1840 train_time:92597ms step_avg:57.55ms
step:1610/1840 train_time:92688ms step_avg:57.57ms
step:1611/1840 train_time:92773ms step_avg:57.59ms
step:1612/1840 train_time:92862ms step_avg:57.61ms
step:1613/1840 train_time:92948ms step_avg:57.62ms
step:1614/1840 train_time:93037ms step_avg:57.64ms
step:1615/1840 train_time:93125ms step_avg:57.66ms
step:1616/1840 train_time:93213ms step_avg:57.68ms
step:1617/1840 train_time:93299ms step_avg:57.70ms
step:1618/1840 train_time:93389ms step_avg:57.72ms
step:1619/1840 train_time:93475ms step_avg:57.74ms
step:1620/1840 train_time:93564ms step_avg:57.76ms
step:1621/1840 train_time:93650ms step_avg:57.77ms
step:1622/1840 train_time:93737ms step_avg:57.79ms
step:1623/1840 train_time:93825ms step_avg:57.81ms
step:1624/1840 train_time:93912ms step_avg:57.83ms
step:1625/1840 train_time:93999ms step_avg:57.85ms
step:1626/1840 train_time:94088ms step_avg:57.86ms
step:1627/1840 train_time:94175ms step_avg:57.88ms
step:1628/1840 train_time:94264ms step_avg:57.90ms
step:1629/1840 train_time:94349ms step_avg:57.92ms
step:1630/1840 train_time:94437ms step_avg:57.94ms
step:1631/1840 train_time:94524ms step_avg:57.95ms
step:1632/1840 train_time:94612ms step_avg:57.97ms
step:1633/1840 train_time:94698ms step_avg:57.99ms
step:1634/1840 train_time:94787ms step_avg:58.01ms
step:1635/1840 train_time:94872ms step_avg:58.03ms
step:1636/1840 train_time:94961ms step_avg:58.04ms
step:1637/1840 train_time:95048ms step_avg:58.06ms
step:1638/1840 train_time:95137ms step_avg:58.08ms
step:1639/1840 train_time:95224ms step_avg:58.10ms
step:1640/1840 train_time:95312ms step_avg:58.12ms
step:1641/1840 train_time:95398ms step_avg:58.13ms
step:1642/1840 train_time:95487ms step_avg:58.15ms
step:1643/1840 train_time:95572ms step_avg:58.17ms
step:1644/1840 train_time:95661ms step_avg:58.19ms
step:1645/1840 train_time:95748ms step_avg:58.21ms
step:1646/1840 train_time:95835ms step_avg:58.22ms
step:1647/1840 train_time:95922ms step_avg:58.24ms
step:1648/1840 train_time:96009ms step_avg:58.26ms
step:1649/1840 train_time:96096ms step_avg:58.28ms
step:1650/1840 train_time:96187ms step_avg:58.30ms
step:1651/1840 train_time:96272ms step_avg:58.31ms
step:1652/1840 train_time:96362ms step_avg:58.33ms
step:1653/1840 train_time:96449ms step_avg:58.35ms
step:1654/1840 train_time:96537ms step_avg:58.37ms
step:1655/1840 train_time:96622ms step_avg:58.38ms
step:1656/1840 train_time:96710ms step_avg:58.40ms
step:1657/1840 train_time:96797ms step_avg:58.42ms
step:1658/1840 train_time:96887ms step_avg:58.44ms
step:1659/1840 train_time:96972ms step_avg:58.45ms
step:1660/1840 train_time:97062ms step_avg:58.47ms
step:1661/1840 train_time:97148ms step_avg:58.49ms
step:1662/1840 train_time:97236ms step_avg:58.51ms
step:1663/1840 train_time:97323ms step_avg:58.52ms
step:1664/1840 train_time:97411ms step_avg:58.54ms
step:1665/1840 train_time:97497ms step_avg:58.56ms
step:1666/1840 train_time:97588ms step_avg:58.58ms
step:1667/1840 train_time:97672ms step_avg:58.59ms
step:1668/1840 train_time:97762ms step_avg:58.61ms
step:1669/1840 train_time:97848ms step_avg:58.63ms
step:1670/1840 train_time:97936ms step_avg:58.64ms
step:1671/1840 train_time:98023ms step_avg:58.66ms
step:1672/1840 train_time:98110ms step_avg:58.68ms
step:1673/1840 train_time:98197ms step_avg:58.69ms
step:1674/1840 train_time:98287ms step_avg:58.71ms
step:1675/1840 train_time:98373ms step_avg:58.73ms
step:1676/1840 train_time:98462ms step_avg:58.75ms
step:1677/1840 train_time:98548ms step_avg:58.76ms
step:1678/1840 train_time:98636ms step_avg:58.78ms
step:1679/1840 train_time:98723ms step_avg:58.80ms
step:1680/1840 train_time:98810ms step_avg:58.82ms
step:1681/1840 train_time:98895ms step_avg:58.83ms
step:1682/1840 train_time:98986ms step_avg:58.85ms
step:1683/1840 train_time:99072ms step_avg:58.87ms
step:1684/1840 train_time:99161ms step_avg:58.88ms
step:1685/1840 train_time:99248ms step_avg:58.90ms
step:1686/1840 train_time:99336ms step_avg:58.92ms
step:1687/1840 train_time:99422ms step_avg:58.93ms
step:1688/1840 train_time:99511ms step_avg:58.95ms
step:1689/1840 train_time:99598ms step_avg:58.97ms
step:1690/1840 train_time:99687ms step_avg:58.99ms
step:1691/1840 train_time:99772ms step_avg:59.00ms
step:1692/1840 train_time:99861ms step_avg:59.02ms
step:1693/1840 train_time:99948ms step_avg:59.04ms
step:1694/1840 train_time:100037ms step_avg:59.05ms
step:1695/1840 train_time:100122ms step_avg:59.07ms
step:1696/1840 train_time:100211ms step_avg:59.09ms
step:1697/1840 train_time:100298ms step_avg:59.10ms
step:1698/1840 train_time:100389ms step_avg:59.12ms
step:1699/1840 train_time:100474ms step_avg:59.14ms
step:1700/1840 train_time:100563ms step_avg:59.15ms
step:1701/1840 train_time:100649ms step_avg:59.17ms
step:1702/1840 train_time:100737ms step_avg:59.19ms
step:1703/1840 train_time:100823ms step_avg:59.20ms
step:1704/1840 train_time:100911ms step_avg:59.22ms
step:1705/1840 train_time:100997ms step_avg:59.24ms
step:1706/1840 train_time:101087ms step_avg:59.25ms
step:1707/1840 train_time:101172ms step_avg:59.27ms
step:1708/1840 train_time:101262ms step_avg:59.29ms
step:1709/1840 train_time:101349ms step_avg:59.30ms
step:1710/1840 train_time:101439ms step_avg:59.32ms
step:1711/1840 train_time:101524ms step_avg:59.34ms
step:1712/1840 train_time:101612ms step_avg:59.35ms
step:1713/1840 train_time:101697ms step_avg:59.37ms
step:1714/1840 train_time:101787ms step_avg:59.39ms
step:1715/1840 train_time:101873ms step_avg:59.40ms
step:1716/1840 train_time:101962ms step_avg:59.42ms
step:1717/1840 train_time:102048ms step_avg:59.43ms
step:1718/1840 train_time:102136ms step_avg:59.45ms
step:1719/1840 train_time:102223ms step_avg:59.47ms
step:1720/1840 train_time:102311ms step_avg:59.48ms
step:1721/1840 train_time:102397ms step_avg:59.50ms
step:1722/1840 train_time:102487ms step_avg:59.52ms
step:1723/1840 train_time:102573ms step_avg:59.53ms
step:1724/1840 train_time:102662ms step_avg:59.55ms
step:1725/1840 train_time:102748ms step_avg:59.56ms
step:1726/1840 train_time:102837ms step_avg:59.58ms
step:1727/1840 train_time:102923ms step_avg:59.60ms
step:1728/1840 train_time:103010ms step_avg:59.61ms
step:1729/1840 train_time:103096ms step_avg:59.63ms
step:1730/1840 train_time:103187ms step_avg:59.65ms
step:1731/1840 train_time:103272ms step_avg:59.66ms
step:1732/1840 train_time:103362ms step_avg:59.68ms
step:1733/1840 train_time:103448ms step_avg:59.69ms
step:1734/1840 train_time:103535ms step_avg:59.71ms
step:1735/1840 train_time:103622ms step_avg:59.72ms
step:1736/1840 train_time:103711ms step_avg:59.74ms
step:1737/1840 train_time:103796ms step_avg:59.76ms
step:1738/1840 train_time:103887ms step_avg:59.77ms
step:1739/1840 train_time:103972ms step_avg:59.79ms
step:1740/1840 train_time:104061ms step_avg:59.81ms
step:1741/1840 train_time:104147ms step_avg:59.82ms
step:1742/1840 train_time:104236ms step_avg:59.84ms
step:1743/1840 train_time:104322ms step_avg:59.85ms
step:1744/1840 train_time:104410ms step_avg:59.87ms
step:1745/1840 train_time:104496ms step_avg:59.88ms
step:1746/1840 train_time:104586ms step_avg:59.90ms
step:1747/1840 train_time:104670ms step_avg:59.91ms
step:1748/1840 train_time:104760ms step_avg:59.93ms
step:1749/1840 train_time:104846ms step_avg:59.95ms
step:1750/1840 train_time:104934ms step_avg:59.96ms
step:1750/1840 val_loss:3.3053 train_time:105035ms step_avg:60.02ms
step:1751/1840 train_time:105055ms step_avg:60.00ms
step:1752/1840 train_time:105113ms step_avg:60.00ms
step:1753/1840 train_time:105204ms step_avg:60.01ms
step:1754/1840 train_time:105293ms step_avg:60.03ms
step:1755/1840 train_time:105377ms step_avg:60.04ms
step:1756/1840 train_time:105465ms step_avg:60.06ms
step:1757/1840 train_time:105549ms step_avg:60.07ms
step:1758/1840 train_time:105636ms step_avg:60.09ms
step:1759/1840 train_time:105723ms step_avg:60.10ms
step:1760/1840 train_time:105810ms step_avg:60.12ms
step:1761/1840 train_time:105896ms step_avg:60.13ms
step:1762/1840 train_time:105987ms step_avg:60.15ms
step:1763/1840 train_time:106077ms step_avg:60.17ms
step:1764/1840 train_time:106169ms step_avg:60.19ms
step:1765/1840 train_time:106255ms step_avg:60.20ms
step:1766/1840 train_time:106346ms step_avg:60.22ms
step:1767/1840 train_time:106431ms step_avg:60.23ms
step:1768/1840 train_time:106519ms step_avg:60.25ms
step:1769/1840 train_time:106605ms step_avg:60.26ms
step:1770/1840 train_time:106692ms step_avg:60.28ms
step:1771/1840 train_time:106776ms step_avg:60.29ms
step:1772/1840 train_time:106864ms step_avg:60.31ms
step:1773/1840 train_time:106950ms step_avg:60.32ms
step:1774/1840 train_time:107043ms step_avg:60.34ms
step:1775/1840 train_time:107131ms step_avg:60.36ms
step:1776/1840 train_time:107220ms step_avg:60.37ms
step:1777/1840 train_time:107308ms step_avg:60.39ms
step:1778/1840 train_time:107396ms step_avg:60.40ms
step:1779/1840 train_time:107480ms step_avg:60.42ms
step:1780/1840 train_time:107568ms step_avg:60.43ms
step:1781/1840 train_time:107653ms step_avg:60.45ms
step:1782/1840 train_time:107742ms step_avg:60.46ms
step:1783/1840 train_time:107826ms step_avg:60.47ms
step:1784/1840 train_time:107915ms step_avg:60.49ms
step:1785/1840 train_time:108002ms step_avg:60.51ms
step:1786/1840 train_time:108091ms step_avg:60.52ms
step:1787/1840 train_time:108180ms step_avg:60.54ms
step:1788/1840 train_time:108269ms step_avg:60.55ms
step:1789/1840 train_time:108356ms step_avg:60.57ms
step:1790/1840 train_time:108445ms step_avg:60.58ms
step:1791/1840 train_time:108530ms step_avg:60.60ms
step:1792/1840 train_time:108618ms step_avg:60.61ms
step:1793/1840 train_time:108705ms step_avg:60.63ms
step:1794/1840 train_time:108792ms step_avg:60.64ms
step:1795/1840 train_time:108878ms step_avg:60.66ms
step:1796/1840 train_time:108967ms step_avg:60.67ms
step:1797/1840 train_time:109054ms step_avg:60.69ms
step:1798/1840 train_time:109144ms step_avg:60.70ms
step:1799/1840 train_time:109230ms step_avg:60.72ms
step:1800/1840 train_time:109320ms step_avg:60.73ms
step:1801/1840 train_time:109409ms step_avg:60.75ms
step:1802/1840 train_time:109495ms step_avg:60.76ms
step:1803/1840 train_time:109582ms step_avg:60.78ms
step:1804/1840 train_time:109670ms step_avg:60.79ms
step:1805/1840 train_time:109756ms step_avg:60.81ms
step:1806/1840 train_time:109846ms step_avg:60.82ms
step:1807/1840 train_time:109932ms step_avg:60.84ms
step:1808/1840 train_time:110020ms step_avg:60.85ms
step:1809/1840 train_time:110108ms step_avg:60.87ms
step:1810/1840 train_time:110197ms step_avg:60.88ms
step:1811/1840 train_time:110285ms step_avg:60.90ms
step:1812/1840 train_time:110375ms step_avg:60.91ms
step:1813/1840 train_time:110460ms step_avg:60.93ms
step:1814/1840 train_time:110548ms step_avg:60.94ms
step:1815/1840 train_time:110634ms step_avg:60.96ms
step:1816/1840 train_time:110725ms step_avg:60.97ms
step:1817/1840 train_time:110811ms step_avg:60.99ms
step:1818/1840 train_time:110900ms step_avg:61.00ms
step:1819/1840 train_time:110987ms step_avg:61.02ms
step:1820/1840 train_time:111077ms step_avg:61.03ms
step:1821/1840 train_time:111164ms step_avg:61.05ms
step:1822/1840 train_time:111252ms step_avg:61.06ms
step:1823/1840 train_time:111338ms step_avg:61.07ms
step:1824/1840 train_time:111428ms step_avg:61.09ms
step:1825/1840 train_time:111514ms step_avg:61.10ms
step:1826/1840 train_time:111604ms step_avg:61.12ms
step:1827/1840 train_time:111690ms step_avg:61.13ms
step:1828/1840 train_time:111779ms step_avg:61.15ms
step:1829/1840 train_time:111865ms step_avg:61.16ms
step:1830/1840 train_time:111954ms step_avg:61.18ms
step:1831/1840 train_time:112041ms step_avg:61.19ms
step:1832/1840 train_time:112130ms step_avg:61.21ms
step:1833/1840 train_time:112216ms step_avg:61.22ms
step:1834/1840 train_time:112306ms step_avg:61.24ms
step:1835/1840 train_time:112393ms step_avg:61.25ms
step:1836/1840 train_time:112481ms step_avg:61.26ms
step:1837/1840 train_time:112567ms step_avg:61.28ms
step:1838/1840 train_time:112656ms step_avg:61.29ms
step:1839/1840 train_time:112743ms step_avg:61.31ms
step:1840/1840 train_time:112831ms step_avg:61.32ms
step:1840/1840 val_loss:3.2802 train_time:112932ms step_avg:61.38ms
peak memory allocated: 28507 MiB reserved: 44038 MiB

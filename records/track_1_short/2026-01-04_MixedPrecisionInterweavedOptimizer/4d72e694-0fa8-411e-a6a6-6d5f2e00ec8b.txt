import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from collections import defaultdict
from itertools import accumulate
from pathlib import Path
import gc

os.environ["PYTORCH_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device=f"cuda:{os.environ['LOCAL_RANK']}", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng

@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor, split_baddbmm: bool = False):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    # Select batched vs unbatched
    if split_baddbmm:
        BX_matmul = torch.bmm if X.ndim > 2 else torch.mm
    else:
        aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A

        # Referencing X twice causes pytorch to make a defensive copy,
        # resulting in a cudaMemcpyAsync in baddbmm.
        # For large matrices (i.e., the mlp weights), it's faster to split
        # the operation into two kernels to avoid this.
        if split_baddbmm:
            BX_matmul(B, X, out=C)  # C = B @ X
            C.add_(X, alpha=a)      # C = C + a*X  (in-place, X only read)
        else:
            aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X

        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X


# -----------------------------------------------------------------------------
# Compiled helpers for NorMuon by @chrisjmccormick

@torch.compile(dynamic=False, fullgraph=True)
def cautious_wd_and_update_inplace(p, mantissa, grad, wd_tensor, lr_tensor):
    """
    Cautious weight decay + parameter update. wd_tensor and lr_tensor are 0-D CPU tensors.
    Mantissa is tracked to enable higher precision updates on bfloat16 parameters.
    bfloat16 format: 1 sign bit + 8 exponent bits + 7 mantissa bits = 16 bits total
    float32 format: 1 sign bit + 8 exponent bits + 23 mantissa bits = 32 bits total
    """
    assert p.dtype == mantissa.dtype == torch.uint16
    grad = grad.float()
    wd_factor = wd_tensor.to(torch.float32)
    lr_factor = lr_tensor.to(torch.float32)
    p_precise_raw = (p.to(torch.uint32) << 16) | mantissa.to(torch.uint32)
    p_precise = p_precise_raw.view(torch.float32)
    mask = (grad * p_precise) >= 0
    p_precise.copy_(p_precise - (p_precise * mask * wd_factor * lr_factor) - (grad * lr_factor))
    p.copy_((p_precise_raw >> 16).to(torch.uint16))
    mantissa.copy_(p_precise_raw.to(torch.uint16))

@torch.compile(dynamic=False, fullgraph=True)
def apply_normuon_variance_reduction(v_chunk, second_momentum_buffer, beta2, red_dim):
    """NorMuon variance reduction. Algebraically fuses the normalization steps to minimize memory ops."""
    v_mean = v_chunk.float().square().mean(dim=red_dim, keepdim=True)
    red_dim_size = v_chunk.size(red_dim)
    v_norm_sq = v_mean.sum(dim=(-2, -1), keepdim=True).mul_(red_dim_size)
    v_norm = v_norm_sq.sqrt_()
    second_momentum_buffer.lerp_(v_mean.to(dtype=second_momentum_buffer.dtype), 1 - beta2)
    step_size = second_momentum_buffer.clamp_min(1e-10).rsqrt_()
    scaled_sq_sum = (v_mean * red_dim_size) * step_size.float().square()
    v_norm_new = scaled_sq_sum.sum(dim=(-2, -1), keepdim=True).sqrt_()
    final_scale = step_size * (v_norm / v_norm_new.clamp_min_(1e-10))
    return v_chunk.mul_(final_scale.type_as(v_chunk))


# -----------------------------------------------------------------------------
# NorMuon optimizer

class NorMuon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).

    Differences from standard Muon:
    - Newton-Shulz is replaced with Polar Express for the orthogonalization step
    - NorMuon adds a low-rank variance estimator similar to Adafactor. https://arxiv.org/pdf/2510.05491
    - small 1D parameters handled here instead of in Adam
    - Cautious weight decay, a gated version of decoupled weight decay
    - Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as
    needed on the forward pass. This enables attn and mlp weights to be contained within the same
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn.
    The scheduling is:
        1. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        2. reduce scatter attn/mlp round 2 (16 mlp params)
        3. wait on step 1, then compute update of 1 and schedule all gather
        4. wait on step 2, then compute update of 2 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        5. wait for each all gather to complete and update params
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, beta2=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, beta2=beta2)
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def reset(self):
        # expose a reset for clearing buffers
        for group in self.param_groups:
            if "momentum_buffer" in group:
                group["momentum_buffer"].zero_()
                group["mantissa"].zero_()
                group["second_momentum_buffer"].zero_()

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per module.
        """
        groups = defaultdict(list)
        for param in params:
            groups[param.label].append(param)

        param_groups = []
        for module_name, group_params in groups.items():
            chunk_size = (len(group_params) + self.world_size - 1) // self.world_size
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))

        return param_groups

    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn
        and mlp params when a param group is split across GPUs.
        """
        params_list = list(params)
        module_group_order = ['attn', 'mlp']
        group_sizes = [16, 16]  # 10 attn + 6 mlp, then 16 mlp
        params_list.sort(key=lambda x: module_group_order.index(x.label))

        idx = 0
        assert len(params_list) == sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            chunk_size = (size + self.world_size - 1) // self.world_size
            group_params = params_list[idx: idx + size]
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))
            idx += size

        return param_groups

    def step(self):
        self.step_p1()
        self.step_p2()
        self.step_p3()
        
    @torch.no_grad()
    def step_p1(self):
        """
        Part 1: Launch distributed reduce_scatter operations for parameter groups.
        """
        rank = dist.get_rank()
        self.group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            stacked_grads = torch.empty(
                (padded_num_params, *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device
            )
            for i, p in enumerate(params):
                stacked_grads[i].copy_(p.grad, non_blocking=True)
            if len(params) < padded_num_params:
                stacked_grads[len(params):].zero_()

            grad_chunk = torch.empty_like(stacked_grads[:chunk_size])

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            self.group_infos.append(dict(grad_chunk=grad_chunk, reduce_future=reduce_future))
    
    @torch.no_grad()
    def step_p2(self):
        """
        Part 2: Compute gradient updates and launch all_gather operations
        Wait for all gather to complete for all parameter groups except final one
        """
        # Efficient distributed step by @YouJiacheng, @KonstantinWilleke, @alexrgilbert,
        # @adricarda, @tuttyfrutyee, @vdlad, @ryanyang0, @vagrawal, @varunneal, @chrisjmccormick
        rank = dist.get_rank()
        group_infos = self.group_infos
        
        self.all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = group["params"]
            grad_chunk = info["grad_chunk"].float()
            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            start_idx = rank * chunk_size
            module_idx = start_idx if start_idx < len(params) else 0

            num_params = min(chunk_size, max(0, len(params) - start_idx))  # num params for this rank

            if "momentum_buffer" not in group:
                group["momentum_buffer"]  = torch.zeros_like(grad_chunk[:num_params], dtype=torch.float32)
                
            momentum_buffer = group["momentum_buffer"]
            # Apply momentum update to the persistent momentum buffer in-place
            momentum_buffer.lerp_(grad_chunk[:num_params], 1 - group["momentum"])
            updated_grads = grad_chunk[:num_params].lerp_(momentum_buffer, group["momentum"])

            grad_shape = updated_grads.shape
            if params[module_idx].label == 'attn':
                for p in params[module_idx:module_idx + num_params]:
                    assert p.label == 'attn'
                updated_grads = updated_grads.view(4 * grad_shape[0], grad_shape[1] // 4, grad_shape[2])

            ref_param = params[module_idx]
            param_shape = ref_param.shape

            if "second_momentum_buffer" not in group:
                group["second_momentum_buffer"] = (torch.zeros_like(updated_grads[..., :, :1], dtype=torch.float32)
                    if param_shape[-2] >= param_shape[-1] else torch.zeros_like(updated_grads[..., :1, :])
                )
            second_momentum_buffer = group["second_momentum_buffer"]

            if "param_lr_cpu" not in group:
                # Define multipliers for ALL params in this group (global, not per-shard)
                lr_mults = []
                wd_mults = []
                for p in params:
                    # Increase learning rate for modules with larger inputs than outputs.
                    # This shape check also assumes rows=input, columns=output, so take care
                    # when changing memory layouts. @chrisjmccormick
                    shape = p.shape
                    if len(shape) >= 2:
                        shape_mult = max(1.0, shape[-2] / shape[-1]) ** 0.5
                    else:
                        shape_mult = 1.0
                    lr_mults.append(shape_mult * getattr(p, "lr_mul", 1.0))
                    wd_mults.append(getattr(p, "wd_mul", 1.0))
                # Define as cpu tensors to enable Inductor constant folding
                group["param_lr_cpu"] = torch.tensor(lr_mults, dtype=torch.float32, device="cpu")
                group["param_wd_cpu"] = torch.tensor(wd_mults, dtype=torch.float32, device="cpu")

            eff_lr_all = group["param_lr_cpu"] * group["lr"]
            eff_wd_all = group["param_wd_cpu"] * group["weight_decay"] * group["lr"]

            # Slice the portion corresponding to this rank's shard
            eff_lr_cpu = eff_lr_all[module_idx:module_idx + num_params]
            eff_wd_cpu = eff_wd_all[module_idx:module_idx + num_params]

            # Compute zeropower for the entire chunk in a single, batched call.
            if num_params == 0:
                v_chunk = updated_grads
            else:
                v_chunk = polar_express(updated_grads, split_baddbmm=(ref_param.label == 'mlp'))

            # Note that the head orientation in O is transposed relative to QKV, so red_dim
            # is 'incorrect' for O. However, correcting this showed no improvement. @chrisjmccormick
            red_dim = -1 if param_shape[-2] >= param_shape[-1] else -2

            v_chunk = apply_normuon_variance_reduction(
                v_chunk, second_momentum_buffer, group["beta2"], red_dim
            )

            v_chunk = v_chunk.view(grad_shape)

            # # "Cautious" weight decay (https://arxiv.org/abs/2510.12402)
            updated_params = torch.empty_like(grad_chunk, dtype=torch.bfloat16)
            if num_params > 0:
                # Work on a stacked copy to avoid touching original params
                param_chunk = torch.stack(params[module_idx:module_idx + num_params])

                if "mantissa" not in group:
                    group["mantissa"] = torch.zeros_like(param_chunk, dtype=torch.uint16)
                mantissa = group["mantissa"]

                for local_idx in range(num_params):
                    cautious_wd_and_update_inplace(
                        param_chunk[local_idx].view(torch.uint16),
                        mantissa[local_idx],
                        v_chunk[local_idx],
                        eff_wd_cpu[local_idx],
                        eff_lr_cpu[local_idx]
                    )
            else:
                param_chunk = torch.zeros_like(v_chunk)

            updated_params[:num_params].copy_(param_chunk)
            if num_params < chunk_size:
                updated_params[num_params:].zero_()

            stacked_params = torch.empty(
                (padded_num_params, *param_shape),
                dtype=updated_params.dtype,
                device=updated_params.device,
            )

            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_params, async_op=True
            ).get_future()

            self.all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete for all except final and copy results back into original parameter tensors.
        for info in self.all_gather_infos[:-1]:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)

    @torch.no_grad()
    def step_p3(self):
        """
        Part 3: Wait for final all gather to complete and copy results back into original parameter tensors
        """
        info = self.all_gather_infos[-1]
        info["gather_future"].wait()
        stacked_params = info["stacked_params"]
        orig_params = info["orig_params"]

        unstacked_params = torch.unbind(stacked_params)
        for i, p in enumerate(orig_params):
            p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, label_order: list[str], betas: list[list[float]], lr: float = 1e-3, eps: float = 1e-8, weight_decay: float = 0.01):
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        defaults = dict(lr=lr, eps=eps, weight_decay=weight_decay)
        params = list(params)
        # Group by label, with explicit ordering for execution control.
        params_by_label = defaultdict(list)
        for p in params:
            params_by_label[getattr(p, 'label', None)].append(p)
        param_groups = []
        for idx, label in enumerate(label_order):
            if label in params_by_label:
                param_groups.append(dict(params=params_by_label[label], betas=betas[idx]))
        # include any unlabeled params at the end (processed last)
        if None in params_by_label:
            param_groups.append(dict(params=params_by_label[None]))
        super().__init__(param_groups, defaults)
        # init state: small params (numel < 1024) use full-sized state, others use sharded
        for p in params:
            chunk = p if p.numel() < 1024 else p[:p.size(0) // self.world_size]
            exp_avg = torch.zeros_like(chunk, dtype=torch.float32, device=p.device)
            self.state[p] = dict(step=0, exp_avg=exp_avg, exp_avg_sq=torch.zeros_like(exp_avg))

        # tag the final param for optimizer pipelining, run all gather after muon copy
        param_groups[-1]['params'][-1].is_final_param = True
        
        # DistributedAdam implementation by @vagrawal, @akash5474
        self.should_sync = False
        self._reduce_scatter_hooks = []
        self._reduce_scatter_futures = {}
        # 0-D CPU tensors to avoid recompilation in _update_step
        self._step_size_t = torch.tensor(0.0, dtype=torch.float32, device="cpu")
        self._eff_wd_t = torch.tensor(0.0, dtype=torch.float32, device="cpu")
        self.register_backward_hooks()

    def register_backward_hooks(self):
        for group in self.param_groups:
            for param in group["params"]:
                self._reduce_scatter_hooks.append(param.register_post_accumulate_grad_hook(self._sync_gradient))

    def load_state_dict(self, state_dict):
        """Override to preserve optimizer state dtypes (avoid BFloat16->Float32 cast that causes recompilation)."""
        # Save original state dtypes before loading
        original_dtypes = {}
        for p, s in self.state.items():
            original_dtypes[p] = {k: v.dtype for k, v in s.items() if isinstance(v, torch.Tensor)}
        
        # Call parent load_state_dict (which may cast dtypes to match param dtype)
        super().load_state_dict(state_dict)
        
        # Restore original dtypes
        for p, s in self.state.items():
            if p in original_dtypes:
                for k, v in s.items():
                    if isinstance(v, torch.Tensor) and k in original_dtypes[p]:
                        if v.dtype != original_dtypes[p][k]:
                            s[k] = v.to(original_dtypes[p][k])

    @torch.no_grad()
    def _sync_gradient(self, param):
        if not self.should_sync:
            return

        grad = param.grad
        if param.numel() < 1024:
            # Small params: use all_reduce (no scatter/gather needed)
            self._reduce_scatter_futures[param] = (
                dist.all_reduce(grad, op=dist.ReduceOp.AVG, async_op=True).get_future(),
                grad
            )
        else:
            rank_size = grad.shape[0] // self.world_size
            if grad is not None:
                grad_slice = torch.empty_like(grad[:rank_size])
                self._reduce_scatter_futures[param] = (
                    dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future(),
                    grad_slice
                )

    def copy_lm_to_embed(self):
        # run at 2/3 of training
        lm_head = self.param_groups[0]['params'][0]
        embed = self.param_groups[-2]['params'][0]
        lm_head_state = self.state[lm_head]
        embed_state = self.state[embed]
        embed_state['step'] = lm_head_state['step']
        embed_state['exp_avg'] = lm_head_state['exp_avg'].clone()
        embed_state['exp_avg_sq'] = lm_head_state['exp_avg_sq'].clone()
        embed.data.copy_(lm_head.data)

    @staticmethod
    @torch.compile(dynamic=False, fullgraph=True)
    def _update_step(p_slice, g_slice, exp_avg, exp_avg_sq, beta1, beta2, eps, step_size_t, eff_wd_t):
        """Compiled Adam update step. step_size_t and eff_wd_t are 0-D CPU tensors to avoid recompilation."""
        exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)  # exp_avg = beta1 * exp_avg + (1 - beta1) * g_slice
        exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)  # exp_avg_sq = beta2 * exp_avg_sq + (1 - beta2) * g_slice^2
        # compute step
        update = exp_avg.div(exp_avg_sq.sqrt().add_(eps)).mul_(step_size_t)  # update = (exp_avg / (sqrt(exp_avg_sq) + eps)) * step_size
        # cautious weight decay
        mask = (update * p_slice) > 0
        update.addcmul_(p_slice, mask, value=eff_wd_t)  # update += eff_wd_t * p_slice * mask
        p_slice.add_(other=update, alpha=-1.0)  # p_slice -= update

    @torch.no_grad()
    def step(self, muon_opt):
        muon_opt.step_p1()
        rank = dist.get_rank()
        all_gather_futures: list[torch.Future] = []

        last_param = None
        last_p_slice = None
        for group in self.param_groups:      
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            for param in group['params']:
                if param not in self._reduce_scatter_futures:
                    continue

                fut, g_slice = self._reduce_scatter_futures[param]
                fut.wait()

                is_small = param.numel() < 1024
                if is_small:
                    # Small params: g_slice is actually full grad, p_slice is full param
                    p_slice = param
                else:
                    rank_size = param.shape[0] // self.world_size
                    p_slice = param[rank * rank_size:(rank + 1) * rank_size]

                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]
                state["step"] += 1
                t = state["step"]
                
                # Pre-compute changing values as 0-D CPU tensors to avoid recompilation.
                # `.fill_(value)` is the same as "= value", but doesn't change the tensor object.
                bias1, bias2 = 1 - beta1 ** t, 1 - beta2 ** t
                self._step_size_t.fill_(lr * (bias2 ** 0.5 / bias1))
                self._eff_wd_t.fill_(lr * lr * wd * getattr(param, "wd_mul", 1.0)) # `lr` included twice to serve as weight decay schedule.

                DistAdam._update_step(p_slice, g_slice, state["exp_avg"], state["exp_avg_sq"],
                                      beta1, beta2, eps, self._step_size_t, self._eff_wd_t)

                if not is_small:
                    if getattr(param, "is_final_param", False):
                        last_param = param
                        last_p_slice = p_slice
                    else:
                        all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())
        self._reduce_scatter_futures.clear()

        muon_opt.step_p2()
        torch.futures.collect_all(all_gather_futures).wait()

        if last_param is not None:
            last_all_gather_future = dist.all_gather_into_tensor(last_param, last_p_slice, async_op=True).get_future()
        muon_opt.step_p3()
        torch.futures.collect_all([last_all_gather_future]).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        with torch.no_grad():
            self.weight.zero_()  # @Grad62304977 and others

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))


# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float
    key_offset: bool
    attn_gate_w: torch.Tensor
    ve_gate_w: torch.Tensor

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = self.dim ** -0.5
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKVO weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # Simplified layout by @chrisjmccormick
        self.qkvo_w = nn.Parameter(torch.empty(self.dim * 4, self.hdim, dtype=torch.bfloat16))
        # label all modules for explicit optimizer grouping
        self.qkvo_w.label = 'attn'

        with torch.no_grad():
            self.qkvo_w[:self.dim * 3].uniform_(-bound, bound)  # init QKV weights
            self.qkvo_w[self.dim * 3:].zero_()  # init O weights to zero

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas, key_offset = attn_args.ve, attn_args.sa_lambdas, attn_args.key_offset
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size
        # sparse gated attention to enable context based no-op by @classiclarryd
        # only include gates on layers with value embeds used on forward pass
        attn_gate_w, ve_gate_w = attn_args.attn_gate_w, attn_args.ve_gate_w

        q, k, v = F.linear(x, sa_lambdas[0] * self.qkvo_w[:self.dim * 3].type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if key_offset:
            # shift keys forward for the stationary head dims. Enables 1-layer induction.
            k[:, 1:, :, self.head_dim // 4:self.head_dim // 2] = k[:, :-1, :, self.head_dim // 4:self.head_dim // 2]
            k[:, 1:, :, 3 * self.head_dim // 4:] = k[:, :-1, :, 3 * self.head_dim // 4:]
        if ve is not None:
            ve_gate_out = 2 * torch.sigmoid(F.linear(x[..., :12], ve_gate_w)).view(B, T, self.num_heads, 1)
            v = v + ve_gate_out * ve.view_as(v) # @ KoszarskyB & @Grad62304977

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens,
                                                        max_seqlen_q=max_len, max_seqlen_k=max_len,
                                                        causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(F.linear(x[..., :12], attn_gate_w)).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, sa_lambdas[1] * self.qkvo_w[self.dim * 3:].type_as(y))  # sa_lambdas[1] pre-multiplied to O @shenberg
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # Transposed layout to match attention weights
        self.c_fc = nn.Parameter(torch.empty(hdim, dim, dtype=torch.bfloat16))
        self.c_proj = nn.Parameter(torch.empty(hdim, dim, dtype=torch.bfloat16))
        # label all modules for explicit optimizer grouping
        self.c_fc.label = 'mlp'
        self.c_proj.label = 'mlp'
        self.c_proj.lr_mul = 2.

        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.T.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.6 (the 7th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads, layer_idx) if layer_idx != 6 else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim)

    def forward(self, x: Tensor, attn_args: AttnArgs):
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

@dataclass
class ForwardScheduleConfig:
    mtp_weights: torch.Tensor
    ws_short: int
    ws_long: int

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        self.num_layers = num_layers
        vocab_size = next_multiple_of_n(vocab_size, n=128)

        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.label = 'smear_gate'
        self.smear_gate.weight.lr_mul = 0.01
        self.smear_gate.weight.wd_mul = 0.0

        self.skip_gate = CastedLinear(12, 1)
        self.skip_gate.weight.label = 'skip_gate'
        self.skip_gate.weight.lr_mul = 0.05
        self.skip_gate.weight.wd_mul = 0.0

        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        for embed in self.value_embeds:
            nn.init.zeros_(embed.weight)
        for ve in self.value_embeds:
            ve.weight.label = 'value_embed'
        
        # parameter banks for attention and value embedding gate weights
        self.attn_gate_bank = nn.Parameter(torch.zeros(10, num_heads, 12)) # 10 layers
        self.attn_gate_bank.label = 'attn_gate_bank'
        self.ve_gate_bank = nn.Parameter(torch.zeros(5, num_heads, 12)) # 5 layers
        self.ve_gate_bank.label = 've_gate_bank'

        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)

        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=100/448, w_s=1.6/448, grad_s=0.75/448)
        nn.init.normal_(self.lm_head.weight, mean=0, std=0.005)
        self.lm_head.weight.label = 'lm_head'

        self.embed = nn.Embedding(vocab_size, model_dim)
        self.embed.weight.label = 'embed'

        # x0_lambdas separated out for different optimizer treatment (no beta smoothing)
        self.x0_lambdas = nn.Parameter(torch.zeros(num_layers))
        self.x0_lambdas.label = 'x0_lambdas'
        self.x0_lambdas.lr_mul = 5.0
        self.x0_lambdas.wd_mul = 0.0

        pad = (-num_layers * 3 - 3) % dist.get_world_size()  # updated: 3*num_layers instead of 4*
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    1.1 * torch.ones(num_layers),  # resid lambdas. 1.1 init such that layer i weight is i^(num_layers-i).
                    *[torch.tensor([0.5, 1.0]) for _ in range(num_layers)],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    -1.5 * torch.ones(1),  # skip_lambda -> σ(-1.5) ≈ 0.18
                    torch.ones(pad),
                ]
            )
        )

        self.scalars.label = 'scalars'
        # set learning rates
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
            param.wd_mul = 5.
        for param in self.embed.parameters():
            param.wd_mul = 150.
        for param in self.lm_head.parameters():
            param.wd_mul = 150.
        self.scalars.lr_mul = 5.0
        self.scalars.wd_mul = 0.0

        self.split_embed = False

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, schedule_cfg: ForwardScheduleConfig):
        assert input_seq.ndim == 1

        # unpack schedule_cfg
        mtp_weights, ws_short, ws_long = schedule_cfg.mtp_weights, schedule_cfg.ws_short, schedule_cfg.ws_long

        # set configs
        skip_connections = []
        skip_in = [3] # long attention window on layer 3
        skip_out = [6] # no attn op on layer 6
        x_backout = None
        backout_layer = 7

        # set lambdas
        resid_lambdas = self.scalars[: 1 * self.num_layers]
        x0_lambdas = self.x0_lambdas
        sa_lambdas = self.scalars[1 * self.num_layers: 3 * self.num_layers].view(-1, 2)
        smear_lambda = self.scalars[3 * self.num_layers]
        backout_lambda = self.scalars[3 * self.num_layers+1]
        skip_lambda = self.scalars[3 * self.num_layers+2]

        # set block masks and key shift
        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == self.num_layers
        key_offset = [b==long_bm for b in bm_sizes] # apply partial key offset to long windows

        # weight-tied: use lm_head.weight for embedding lookup (or separate embed after split)
        if self.split_embed:
            x = self.embed(input_seq)
        else:
            x = F.embedding(input_seq, self.lm_head.weight)
        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [ve[1], ve[2]] + [None] * (self.num_layers - 5) + [ve[0], ve[1], ve[2]]
        assert len(ve) == self.num_layers

        # smear token embed forward 1 position @classiclarryd
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # unbind gate banks to avoid select_backwards kernel
        ag = [w.bfloat16() for w in self.attn_gate_bank.unbind(0)] 
        veg = [w.bfloat16() for w in self.ve_gate_bank.unbind(0)]
        attn_gates = ag[:6] + [None] + ag[6:]
        ve_gates = [veg[0], veg[1]] + [None] * (self.num_layers - 5) + [veg[2], veg[3], veg[4]]
        assert len(attn_gates) == self.num_layers
        assert len(ve_gates) == self.num_layers

        for i in range(self.num_layers):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale,
                key_offset=key_offset[i],
                attn_gate_w=attn_gates[i],
                ve_gate_w=ve_gates[i]
            )
            if i in skip_out:
                skip_gate_out = torch.sigmoid(skip_lambda) * 2 * torch.sigmoid(self.skip_gate(x0[..., :self.skip_gate.weight.size(-1)]))
                x = x + skip_gate_out * skip_connections.pop()
            if i == 0:
                x = (resid_lambdas[0] + x0_lambdas[0]) * x
            else:
                x = resid_lambdas[i] * x + x0_lambdas[i] * x0
            x = self.blocks[i](x, attn_args)
            if i in skip_in:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 7 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15
        # @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1). @classiclarryd updated to 23*sigmoid((logits+5)/7.5)
        logits = 23 * torch.sigmoid((logits + 5) / 7.5)
        logits_for_loss = logits.float() if not self.training else logits

        n_predict = mtp_weights.size(0) if mtp_weights is not None else 1
        if self.training and n_predict > 1:
            # Multi-token prediction: take loss of the weighted average of next n_predict tokens
            logits_flat = logits_for_loss.view(-1, logits_for_loss.size(-1))
            idx = F.pad(target_seq, (0, n_predict - 1)).unfold(0, n_predict, 1)  # [T, n_predict] of shifted targets
            target_logits = logits_flat.gather(1, idx)
            cross_entropy = torch.logsumexp(logits_flat, dim=-1).unsqueeze(1) - target_logits
            for k in range(1, n_predict):  # zero out preds past end of sequence
                cross_entropy[-k:, k] = 0
            loss = (cross_entropy * mtp_weights).sum()
        elif self.training:
            loss = F.cross_entropy(logits_for_loss.view(-1, logits_for_loss.size(-1)), target_seq, reduction="sum")
        else:
            loss = F.cross_entropy(logits_for_loss.view(-1, logits_for_loss.size(-1)), target_seq, reduction="mean")
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()

    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        # Cast to int32 on CPU before transfer to avoid dtype conversion during .to()
        _inputs = _inputs.to(dtype=torch.int32)
        _targets = _targets.to(dtype=torch.int64)
        _cum_lengths = _cum_lengths.to(dtype=torch.int32)

        new_params = yield (
            _inputs.to(device="cuda", non_blocking=True),
            _targets.to(device="cuda", non_blocking=True),
            _cum_lengths.to(device="cuda", non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * new_grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens // new_grad_accum_steps
            max_seq_len = new_max_seq_len

# -----------------------------------------------------------------------------
# Training Management
 
def get_bs(step: int):
    if step >= args.num_scheduled_iterations:
        return args.train_bs_extension
    x = step / args.num_scheduled_iterations
    bs_idx = int(len(args.train_bs_schedule) * x)
    return args.train_bs_schedule[bs_idx]

def get_ws(step: int):
    # set short window size to half of long window size
    # Higher ws on "extension" steps
    if step >= args.num_scheduled_iterations:
        return args.ws_final // 2, args.ws_final
    x = step / args.num_scheduled_iterations
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

# learning rate schedule: tied to batch size schedule, with cooldown at the end.
def get_lr(step: int):
    if step > args.num_scheduled_iterations:
        return 0.1
    lr_max = 1.0
    x = step / args.num_scheduled_iterations
    if x > 1/3:
       lr_max = 1.52  # (16/8)**0.6
    if x > 2/3:
        lr_max = 1.73  # (24/8)**0.5
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = lr_max * w + (1 - w) * 0.1
        return lr
    return lr_max

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

class TrainingManager():
    """
    Manages three optimizers for Adam embed/lm_head, Adam scalars, and Muon weight matrices.
    Notable Features:
        1. Scalars are given higher momentum terms to smooth learning @ChrisJMcCormick
        2. Scalar weights are temporarily frozen during batch size or window size updates @ChrisJMcCormick
        3. Adam optimizers are only stepped on odd steps @classiclarryd
        4. Adam optimizers have hooks to start gradient communication during backwards pass @akash5474
        5. Muon has a linear momentum warmup and cooldown schedule
        6. Learning rates follow a linear decay schedule
        7. Embed/lm_head weights and optimizer state splits at 2/3 of training @classiclarryd

    Manages model architecture, data, and target that changes during training
    Notable Features:
        1. Multi Token Prediction schedule of [1, 0.5, 0.25->0] -> [1, 0.5->0] -> [1] @varunneal
        2. Sliding Attention window schedule of [1,3] -> [3,7] -> [5,11] -> [6,13]
        3. YaRN updates to RoPE on window changes
        4. Split embed and lm head at 2/3 of training
        5. Batch size schedule of 8 -> 16 -> 24
        6. Post training extension of long windows from 13 to 20
    """
    def __init__(self, model):
        self.mtp_weights_schedule = self._build_mtp_schedule()
        self.model = model
        adam_betas = {
            'lm_head': [0.5, 0.95],
            'smear_gate': [0.9, 0.99],
            'attn_gate_bank': [0.9, 0.99],
            've_gate_bank': [0.9, 0.99],
            'skip_gate': [0.9, 0.99],
            'x0_lambdas': [0.65, 0.95],
            'scalars': [0.9, 0.99],
            'embed': [0.5, 0.95],
            'value_embed': [0.75, 0.95]
        }
        adam_labels = list(adam_betas.keys())
        adam_beta_values = list(adam_betas.values())
        muon_labels = ['attn', 'mlp']
        adam_params = [p for p in model.parameters() if getattr(p, 'label', None) in adam_labels]
        muon_params = [p for p in model.parameters() if getattr(p, 'label', None) in muon_labels]
        assert set(getattr(p, 'label', None) for p in model.parameters()) == set(adam_labels + muon_labels), "All params must have label"

        self.adam_opt = DistAdam(adam_params, adam_labels, adam_beta_values, lr=0.008, eps=1e-10, weight_decay=0.005)
        self.muon_opt = NorMuon(muon_params, lr=0.023, momentum=0.95, beta2=0.95, weight_decay=1.2)
        self.optimizers = [self.adam_opt, self.muon_opt]

        # split after odd number step
        self.split_step = math.ceil(args.split_embed_frac * args.num_scheduled_iterations) | 1

        # set defaults
        for opt in self.optimizers:
            opt.odd_step_only = False 
            opt.should_sync = True
            for group in opt.param_groups:
                group["initial_lr"] = group["lr"]

        # on even steps, only step Muon params
        self.adam_opt.odd_step_only = True

        self.reset()

    def _build_mtp_schedule(self):
        # Precompute MTP weights for all steps to avoid tensor allocation during training
        # Schedule: [1, 0.5, 0.25->0] -> [1, 0.5->0] -> [1]
        mtp_weights_schedule = []
        for s in range(args.num_iterations + 1):
            x = s / args.num_scheduled_iterations
            if x < 1/3:
                w = [1.0, 0.5, 0.25 * (1 - 3*x)]
            elif x < 2/3:
                w = [1.0, 0.5 * (1 - (3*x - 1))]
            else:
                w = [1.0]
            mtp_weights_schedule.append(torch.tensor(w, device=device))
        return mtp_weights_schedule

    def apply_final_ws_ext(self):
        self.ws_long = args.ws_validate_post_yarn_ext

    def get_forward_args(self):
        return ForwardScheduleConfig(
            mtp_weights = self.mtp_weights,
            ws_short = self.ws_short,
            ws_long = self.ws_long
        )
    
    def _is_active_step(self, opt, step: int):
        return (opt.odd_step_only and step%2==1) or not opt.odd_step_only

    def get_transition_steps(self):
        transition_steps = []
        ws_short, ws_long = get_ws(0)
        for step in range(1, args.num_iterations):
            ws_short, new_ws_long = get_ws(step)
            if new_ws_long != ws_long:
                transition_steps.append(step)
                ws_long = new_ws_long
        return transition_steps

    def advance_schedule(self, step: int):
        self.ws_short, new_ws_long = get_ws(step)
        if new_ws_long != self.ws_long:
            self.model.yarn.apply(self.ws_long, new_ws_long)

        new_batch_size = get_bs(step)
        if new_batch_size != self.batch_size:
            self.train_loader_send_args = (new_batch_size, args.train_max_seq_len, grad_accum_steps)
        else:
            self.train_loader_send_args = None

        self.ws_long = new_ws_long
        self.mtp_weights = self.mtp_weights_schedule[step]
    
    def step_optimizers(self, step: int):                
        step_lr = get_lr(step)
        muon_momentum = get_muon_momentum(step)
        for group in self.muon_opt.param_groups:
            group["momentum"] = muon_momentum

        for opt in self.optimizers:
            for group in opt.param_groups:
                group["lr"] = group["initial_lr"] * step_lr
                
        if self._is_active_step(self.adam_opt, step):
            # adam will interleave calls to muon step
            self.adam_opt.step(self.muon_opt)
            self.model.zero_grad(set_to_none=True)
            self.adam_opt.should_sync = False
        else:
            self.muon_opt.step()
            self.muon_opt.zero_grad(set_to_none=True)
            
        if step == self.split_step:
            self.adam_opt.copy_lm_to_embed()
            self.model.split_embed = True
    
    def activate_hooks(self, step: int):
        for opt in self.optimizers:
            if self._is_active_step(opt, step):
                opt.should_sync = True

    def reset(self, state=None):
        if state is not None:
            for opt, opt_state in zip(self.optimizers, state):
                opt.should_sync = False
                opt.load_state_dict(opt_state)

        # muon momentum buffers not in state dict
        self.muon_opt.reset()
        self.model.split_embed = False

        self.ws_short, self.ws_long = get_ws(0)
        self.batch_size = get_bs(0)
        self.model.yarn.reset()

    def get_state(self):
        return [copy.deepcopy(opt.state_dict()) for opt in self.optimizers]

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # batch sizes
    train_bs_schedule: tuple = (8 * 2048 * 8, 16 * 2048 * 8, 24 * 2048 * 8)
    train_bs_extension: int = 24 * 2048 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 1800  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: float = 0.50  # fraction of num_scheduled_iterations spent cooling down the learning rate
    split_embed_frac: float = 2/3  # fraction of training when embeddings split from lm_head
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_final: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=11,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=args.val_batch_size // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.weight.data = m.weight.data.bfloat16()
model.attn_gate_bank.data = model.attn_gate_bank.data.bfloat16()
model.ve_gate_bank.data = model.ve_gate_bank.data.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)
training_manager = TrainingManager(model)

########################################
#            Warmup kernels            #
########################################
print0("Compiling model and warming up kernels (~7 minutes on first execution)", console=True)
# Warmup the training kernels, then re-initialize the state so we aren't cheating
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=training_manager.get_state()) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_bs_schedule[0], args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)

transition_steps = training_manager.get_transition_steps()
# first few steps plus transitions
warmup_steps = sorted({0, 1, 2} | set(s + offset for s in transition_steps for offset in [-1, 0, 1] if s + offset >= 0)) 
print0(f"Sampling steps {warmup_steps} for warmup", console=True)
for step in warmup_steps:
    training_manager.advance_schedule(step)
    model.eval()
    with torch.no_grad():
        inputs, targets, cum_seqlens = next(val_loader)
        model(inputs, targets, cum_seqlens, training_manager.get_forward_args())
    model.train()
    for idx in range(grad_accum_steps):
        # enable gradient sync for the DistAdam optimizers on the last iteration before we step them
        if idx == grad_accum_steps - 1:
            training_manager.activate_hooks(step)
        send_args = training_manager.train_loader_send_args
        inputs, targets, cum_seqlens = train_loader.send(send_args)
        (model(inputs, targets, cum_seqlens, training_manager.get_forward_args()) / grad_accum_steps).backward()
    training_manager.step_optimizers(step)
print0("Resetting Model", console=True)
model.zero_grad(set_to_none=True)
model.load_state_dict(initial_state["model"])
training_manager.reset(initial_state["optimizers"])
del val_loader, train_loader, initial_state
model.train()

########################################
#        Training and validation       #
########################################
train_loader = distributed_data_generator(args.train_files, args.train_bs_schedule[0], args.train_max_seq_len, grad_accum_steps=grad_accum_steps)

gc.collect()

training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    training_manager.advance_schedule(step)
    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            training_manager.apply_final_ws_ext()
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, training_manager.get_forward_args())
        val_loss /= val_steps
        del val_loader
        dist.reduce(val_loss, 0, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for idx in range(grad_accum_steps):
        # enable gradient sync for the DistAdam optimizers on the last iteration before we step them
        if idx == grad_accum_steps - 1:
            training_manager.activate_hooks(step)
        send_args = training_manager.train_loader_send_args
        inputs, targets, cum_seqlens = train_loader.send(send_args)
        (model(inputs, targets, cum_seqlens, training_manager.get_forward_args()) / grad_accum_steps).backward()
    training_manager.step_optimizers(step)

    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.10.12 (main, Nov  4 2025, 08:48:33) [GCC 11.4.0]
Running PyTorch 2.10.0.dev20251210+cu126 compiled for CUDA 12.6
Running Triton version 3.6.0
Mon Jan  5 07:55:28 2026       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:19:00.0 Off |                    0 |
| N/A   43C    P0            128W /  700W |    1520MiB /  81559MiB |      1%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:3B:00.0 Off |                    0 |
| N/A   35C    P0            131W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:4C:00.0 Off |                    0 |
| N/A   33C    P0            128W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:5D:00.0 Off |                    0 |
| N/A   42C    P0            130W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:9B:00.0 Off |                    0 |
| N/A   44C    P0            131W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:BB:00.0 Off |                    0 |
| N/A   35C    P0            125W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:CB:00.0 Off |                    0 |
| N/A   41C    P0            127W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:DB:00.0 Off |                    0 |
| N/A   32C    P0            125W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A     50579      C   /usr/bin/python                                 0MiB |
|    1   N/A  N/A     50580      C   /usr/bin/python                                 0MiB |
|    2   N/A  N/A     50581      C   /usr/bin/python                                 0MiB |
|    3   N/A  N/A     50582      C   /usr/bin/python                                 0MiB |
|    4   N/A  N/A     50583      C   /usr/bin/python                                 0MiB |
|    5   N/A  N/A     50584      C   /usr/bin/python                                 0MiB |
|    6   N/A  N/A     50585      C   /usr/bin/python                                 0MiB |
|    7   N/A  N/A     50586      C   /usr/bin/python                                 0MiB |
+-----------------------------------------------------------------------------------------+

====================================================================================================
Compiling model and warming up kernels (~7 minutes on first execution)
Sampling steps [0, 1, 2, 599, 600, 601, 1199, 1200, 1201, 1799, 1800, 1801] for warmup
Resetting Model
step:0/1840 val_loss:10.8275 train_time:0ms step_avg:0.03ms
step:1/1840 train_time:75ms step_avg:75.38ms
step:2/1840 train_time:97ms step_avg:48.55ms
step:3/1840 train_time:116ms step_avg:38.63ms
step:4/1840 train_time:147ms step_avg:36.67ms
step:5/1840 train_time:179ms step_avg:35.71ms
step:6/1840 train_time:270ms step_avg:44.97ms
step:7/1840 train_time:287ms step_avg:41.06ms
step:8/1840 train_time:437ms step_avg:54.65ms
step:9/1840 train_time:469ms step_avg:52.13ms
step:10/1840 train_time:503ms step_avg:50.32ms
step:11/1840 train_time:535ms step_avg:48.65ms
step:12/1840 train_time:569ms step_avg:47.45ms
step:13/1840 train_time:602ms step_avg:46.28ms
step:14/1840 train_time:636ms step_avg:45.41ms
step:15/1840 train_time:668ms step_avg:44.54ms
step:16/1840 train_time:702ms step_avg:43.89ms
step:17/1840 train_time:734ms step_avg:43.20ms
step:18/1840 train_time:769ms step_avg:42.70ms
step:19/1840 train_time:800ms step_avg:42.13ms
step:20/1840 train_time:835ms step_avg:41.74ms
step:21/1840 train_time:867ms step_avg:41.27ms
step:22/1840 train_time:901ms step_avg:40.95ms
step:23/1840 train_time:933ms step_avg:40.56ms
step:24/1840 train_time:967ms step_avg:40.30ms
step:25/1840 train_time:999ms step_avg:39.96ms
step:26/1840 train_time:1033ms step_avg:39.74ms
step:27/1840 train_time:1065ms step_avg:39.46ms
step:28/1840 train_time:1099ms step_avg:39.27ms
step:29/1840 train_time:1132ms step_avg:39.02ms
step:30/1840 train_time:1166ms step_avg:38.85ms
step:31/1840 train_time:1198ms step_avg:38.63ms
step:32/1840 train_time:1232ms step_avg:38.50ms
step:33/1840 train_time:1264ms step_avg:38.30ms
step:34/1840 train_time:1299ms step_avg:38.20ms
step:35/1840 train_time:1332ms step_avg:38.05ms
step:36/1840 train_time:1367ms step_avg:37.97ms
step:37/1840 train_time:1400ms step_avg:37.82ms
step:38/1840 train_time:1434ms step_avg:37.74ms
step:39/1840 train_time:1467ms step_avg:37.61ms
step:40/1840 train_time:1501ms step_avg:37.53ms
step:41/1840 train_time:1534ms step_avg:37.41ms
step:42/1840 train_time:1568ms step_avg:37.34ms
step:43/1840 train_time:1600ms step_avg:37.22ms
step:44/1840 train_time:1635ms step_avg:37.16ms
step:45/1840 train_time:1667ms step_avg:37.05ms
step:46/1840 train_time:1702ms step_avg:36.99ms
step:47/1840 train_time:1734ms step_avg:36.89ms
step:48/1840 train_time:1768ms step_avg:36.84ms
step:49/1840 train_time:1800ms step_avg:36.74ms
step:50/1840 train_time:1835ms step_avg:36.70ms
step:51/1840 train_time:1867ms step_avg:36.61ms
step:52/1840 train_time:1901ms step_avg:36.56ms
step:53/1840 train_time:1933ms step_avg:36.48ms
step:54/1840 train_time:1967ms step_avg:36.43ms
step:55/1840 train_time:2000ms step_avg:36.36ms
step:56/1840 train_time:2034ms step_avg:36.33ms
step:57/1840 train_time:2066ms step_avg:36.25ms
step:58/1840 train_time:2100ms step_avg:36.21ms
step:59/1840 train_time:2132ms step_avg:36.14ms
step:60/1840 train_time:2166ms step_avg:36.11ms
step:61/1840 train_time:2199ms step_avg:36.04ms
step:62/1840 train_time:2233ms step_avg:36.01ms
step:63/1840 train_time:2265ms step_avg:35.96ms
step:64/1840 train_time:2300ms step_avg:35.93ms
step:65/1840 train_time:2332ms step_avg:35.88ms
step:66/1840 train_time:2367ms step_avg:35.86ms
step:67/1840 train_time:2399ms step_avg:35.80ms
step:68/1840 train_time:2433ms step_avg:35.79ms
step:69/1840 train_time:2466ms step_avg:35.74ms
step:70/1840 train_time:2501ms step_avg:35.72ms
step:71/1840 train_time:2533ms step_avg:35.68ms
step:72/1840 train_time:2568ms step_avg:35.67ms
step:73/1840 train_time:2600ms step_avg:35.62ms
step:74/1840 train_time:2634ms step_avg:35.60ms
step:75/1840 train_time:2667ms step_avg:35.56ms
step:76/1840 train_time:2701ms step_avg:35.55ms
step:77/1840 train_time:2734ms step_avg:35.50ms
step:78/1840 train_time:2768ms step_avg:35.49ms
step:79/1840 train_time:2800ms step_avg:35.45ms
step:80/1840 train_time:2835ms step_avg:35.43ms
step:81/1840 train_time:2867ms step_avg:35.39ms
step:82/1840 train_time:2901ms step_avg:35.38ms
step:83/1840 train_time:2933ms step_avg:35.34ms
step:84/1840 train_time:2967ms step_avg:35.33ms
step:85/1840 train_time:2999ms step_avg:35.29ms
step:86/1840 train_time:3034ms step_avg:35.28ms
step:87/1840 train_time:3066ms step_avg:35.24ms
step:88/1840 train_time:3100ms step_avg:35.23ms
step:89/1840 train_time:3132ms step_avg:35.19ms
step:90/1840 train_time:3167ms step_avg:35.19ms
step:91/1840 train_time:3199ms step_avg:35.15ms
step:92/1840 train_time:3233ms step_avg:35.14ms
step:93/1840 train_time:3265ms step_avg:35.11ms
step:94/1840 train_time:3299ms step_avg:35.10ms
step:95/1840 train_time:3332ms step_avg:35.07ms
step:96/1840 train_time:3366ms step_avg:35.07ms
step:97/1840 train_time:3398ms step_avg:35.03ms
step:98/1840 train_time:3433ms step_avg:35.03ms
step:99/1840 train_time:3465ms step_avg:35.00ms
step:100/1840 train_time:3500ms step_avg:35.00ms
step:101/1840 train_time:3532ms step_avg:34.97ms
step:102/1840 train_time:3566ms step_avg:34.96ms
step:103/1840 train_time:3599ms step_avg:34.94ms
step:104/1840 train_time:3633ms step_avg:34.93ms
step:105/1840 train_time:3666ms step_avg:34.91ms
step:106/1840 train_time:3700ms step_avg:34.91ms
step:107/1840 train_time:3732ms step_avg:34.88ms
step:108/1840 train_time:3767ms step_avg:34.88ms
step:109/1840 train_time:3799ms step_avg:34.86ms
step:110/1840 train_time:3834ms step_avg:34.85ms
step:111/1840 train_time:3866ms step_avg:34.83ms
step:112/1840 train_time:3900ms step_avg:34.82ms
step:113/1840 train_time:3932ms step_avg:34.80ms
step:114/1840 train_time:3967ms step_avg:34.80ms
step:115/1840 train_time:3999ms step_avg:34.77ms
step:116/1840 train_time:4033ms step_avg:34.77ms
step:117/1840 train_time:4065ms step_avg:34.75ms
step:118/1840 train_time:4100ms step_avg:34.74ms
step:119/1840 train_time:4132ms step_avg:34.72ms
step:120/1840 train_time:4166ms step_avg:34.71ms
step:121/1840 train_time:4198ms step_avg:34.69ms
step:122/1840 train_time:4232ms step_avg:34.69ms
step:123/1840 train_time:4265ms step_avg:34.67ms
step:124/1840 train_time:4299ms step_avg:34.67ms
step:125/1840 train_time:4331ms step_avg:34.65ms
step:126/1840 train_time:4365ms step_avg:34.64ms
step:127/1840 train_time:4397ms step_avg:34.62ms
step:128/1840 train_time:4432ms step_avg:34.62ms
step:129/1840 train_time:4464ms step_avg:34.60ms
step:130/1840 train_time:4498ms step_avg:34.60ms
step:131/1840 train_time:4530ms step_avg:34.58ms
step:132/1840 train_time:4564ms step_avg:34.58ms
step:133/1840 train_time:4597ms step_avg:34.56ms
step:134/1840 train_time:4631ms step_avg:34.56ms
step:135/1840 train_time:4664ms step_avg:34.55ms
step:136/1840 train_time:4698ms step_avg:34.54ms
step:137/1840 train_time:4730ms step_avg:34.53ms
step:138/1840 train_time:4764ms step_avg:34.52ms
step:139/1840 train_time:4796ms step_avg:34.51ms
step:140/1840 train_time:4831ms step_avg:34.51ms
step:141/1840 train_time:4863ms step_avg:34.49ms
step:142/1840 train_time:4898ms step_avg:34.49ms
step:143/1840 train_time:4930ms step_avg:34.47ms
step:144/1840 train_time:4964ms step_avg:34.47ms
step:145/1840 train_time:4996ms step_avg:34.45ms
step:146/1840 train_time:5030ms step_avg:34.45ms
step:147/1840 train_time:5062ms step_avg:34.44ms
step:148/1840 train_time:5097ms step_avg:34.44ms
step:149/1840 train_time:5129ms step_avg:34.42ms
step:150/1840 train_time:5163ms step_avg:34.42ms
step:151/1840 train_time:5195ms step_avg:34.40ms
step:152/1840 train_time:5229ms step_avg:34.40ms
step:153/1840 train_time:5261ms step_avg:34.39ms
step:154/1840 train_time:5295ms step_avg:34.39ms
step:155/1840 train_time:5328ms step_avg:34.37ms
step:156/1840 train_time:5362ms step_avg:34.37ms
step:157/1840 train_time:5394ms step_avg:34.36ms
step:158/1840 train_time:5428ms step_avg:34.36ms
step:159/1840 train_time:5460ms step_avg:34.34ms
step:160/1840 train_time:5495ms step_avg:34.34ms
step:161/1840 train_time:5527ms step_avg:34.33ms
step:162/1840 train_time:5561ms step_avg:34.33ms
step:163/1840 train_time:5593ms step_avg:34.31ms
step:164/1840 train_time:5627ms step_avg:34.31ms
step:165/1840 train_time:5659ms step_avg:34.30ms
step:166/1840 train_time:5694ms step_avg:34.30ms
step:167/1840 train_time:5726ms step_avg:34.29ms
step:168/1840 train_time:5760ms step_avg:34.29ms
step:169/1840 train_time:5793ms step_avg:34.28ms
step:170/1840 train_time:5827ms step_avg:34.28ms
step:171/1840 train_time:5859ms step_avg:34.27ms
step:172/1840 train_time:5894ms step_avg:34.27ms
step:173/1840 train_time:5926ms step_avg:34.25ms
step:174/1840 train_time:5960ms step_avg:34.25ms
step:175/1840 train_time:5993ms step_avg:34.24ms
step:176/1840 train_time:6027ms step_avg:34.24ms
step:177/1840 train_time:6059ms step_avg:34.23ms
step:178/1840 train_time:6093ms step_avg:34.23ms
step:179/1840 train_time:6125ms step_avg:34.22ms
step:180/1840 train_time:6159ms step_avg:34.22ms
step:181/1840 train_time:6191ms step_avg:34.21ms
step:182/1840 train_time:6226ms step_avg:34.21ms
step:183/1840 train_time:6258ms step_avg:34.20ms
step:184/1840 train_time:6293ms step_avg:34.20ms
step:185/1840 train_time:6325ms step_avg:34.19ms
step:186/1840 train_time:6359ms step_avg:34.19ms
step:187/1840 train_time:6391ms step_avg:34.18ms
step:188/1840 train_time:6426ms step_avg:34.18ms
step:189/1840 train_time:6458ms step_avg:34.17ms
step:190/1840 train_time:6492ms step_avg:34.17ms
step:191/1840 train_time:6524ms step_avg:34.16ms
step:192/1840 train_time:6558ms step_avg:34.16ms
step:193/1840 train_time:6590ms step_avg:34.15ms
step:194/1840 train_time:6625ms step_avg:34.15ms
step:195/1840 train_time:6657ms step_avg:34.14ms
step:196/1840 train_time:6691ms step_avg:34.14ms
step:197/1840 train_time:6723ms step_avg:34.13ms
step:198/1840 train_time:6758ms step_avg:34.13ms
step:199/1840 train_time:6790ms step_avg:34.12ms
step:200/1840 train_time:6824ms step_avg:34.12ms
step:201/1840 train_time:6856ms step_avg:34.11ms
step:202/1840 train_time:6891ms step_avg:34.11ms
step:203/1840 train_time:6922ms step_avg:34.10ms
step:204/1840 train_time:6957ms step_avg:34.10ms
step:205/1840 train_time:6989ms step_avg:34.09ms
step:206/1840 train_time:7023ms step_avg:34.09ms
step:207/1840 train_time:7056ms step_avg:34.08ms
step:208/1840 train_time:7089ms step_avg:34.08ms
step:209/1840 train_time:7121ms step_avg:34.07ms
step:210/1840 train_time:7156ms step_avg:34.07ms
step:211/1840 train_time:7188ms step_avg:34.06ms
step:212/1840 train_time:7222ms step_avg:34.07ms
step:213/1840 train_time:7254ms step_avg:34.06ms
step:214/1840 train_time:7288ms step_avg:34.06ms
step:215/1840 train_time:7320ms step_avg:34.05ms
step:216/1840 train_time:7355ms step_avg:34.05ms
step:217/1840 train_time:7387ms step_avg:34.04ms
step:218/1840 train_time:7421ms step_avg:34.04ms
step:219/1840 train_time:7453ms step_avg:34.03ms
step:220/1840 train_time:7488ms step_avg:34.03ms
step:221/1840 train_time:7519ms step_avg:34.02ms
step:222/1840 train_time:7554ms step_avg:34.03ms
step:223/1840 train_time:7586ms step_avg:34.02ms
step:224/1840 train_time:7621ms step_avg:34.02ms
step:225/1840 train_time:7653ms step_avg:34.01ms
step:226/1840 train_time:7687ms step_avg:34.01ms
step:227/1840 train_time:7719ms step_avg:34.00ms
step:228/1840 train_time:7753ms step_avg:34.01ms
step:229/1840 train_time:7786ms step_avg:34.00ms
step:230/1840 train_time:7820ms step_avg:34.00ms
step:231/1840 train_time:7852ms step_avg:33.99ms
step:232/1840 train_time:7886ms step_avg:33.99ms
step:233/1840 train_time:7918ms step_avg:33.98ms
step:234/1840 train_time:7953ms step_avg:33.99ms
step:235/1840 train_time:7985ms step_avg:33.98ms
step:236/1840 train_time:8019ms step_avg:33.98ms
step:237/1840 train_time:8051ms step_avg:33.97ms
step:238/1840 train_time:8085ms step_avg:33.97ms
step:239/1840 train_time:8118ms step_avg:33.97ms
step:240/1840 train_time:8152ms step_avg:33.97ms
step:241/1840 train_time:8184ms step_avg:33.96ms
step:242/1840 train_time:8218ms step_avg:33.96ms
step:243/1840 train_time:8251ms step_avg:33.95ms
step:244/1840 train_time:8285ms step_avg:33.95ms
step:245/1840 train_time:8317ms step_avg:33.95ms
step:246/1840 train_time:8352ms step_avg:33.95ms
step:247/1840 train_time:8384ms step_avg:33.94ms
step:248/1840 train_time:8418ms step_avg:33.94ms
step:249/1840 train_time:8450ms step_avg:33.93ms
step:250/1840 train_time:8484ms step_avg:33.93ms
step:250/1840 val_loss:4.5966 train_time:8526ms step_avg:34.10ms
step:251/1840 train_time:8545ms step_avg:34.04ms
step:252/1840 train_time:8564ms step_avg:33.98ms
step:253/1840 train_time:8586ms step_avg:33.94ms
step:254/1840 train_time:8621ms step_avg:33.94ms
step:255/1840 train_time:8653ms step_avg:33.93ms
step:256/1840 train_time:8688ms step_avg:33.94ms
step:257/1840 train_time:8721ms step_avg:33.93ms
step:258/1840 train_time:8756ms step_avg:33.94ms
step:259/1840 train_time:8789ms step_avg:33.93ms
step:260/1840 train_time:8823ms step_avg:33.94ms
step:261/1840 train_time:8856ms step_avg:33.93ms
step:262/1840 train_time:8890ms step_avg:33.93ms
step:263/1840 train_time:8922ms step_avg:33.93ms
step:264/1840 train_time:8957ms step_avg:33.93ms
step:265/1840 train_time:8989ms step_avg:33.92ms
step:266/1840 train_time:9023ms step_avg:33.92ms
step:267/1840 train_time:9055ms step_avg:33.91ms
step:268/1840 train_time:9089ms step_avg:33.91ms
step:269/1840 train_time:9121ms step_avg:33.91ms
step:270/1840 train_time:9155ms step_avg:33.91ms
step:271/1840 train_time:9187ms step_avg:33.90ms
step:272/1840 train_time:9221ms step_avg:33.90ms
step:273/1840 train_time:9253ms step_avg:33.89ms
step:274/1840 train_time:9287ms step_avg:33.89ms
step:275/1840 train_time:9318ms step_avg:33.89ms
step:276/1840 train_time:9352ms step_avg:33.89ms
step:277/1840 train_time:9384ms step_avg:33.88ms
step:278/1840 train_time:9418ms step_avg:33.88ms
step:279/1840 train_time:9451ms step_avg:33.88ms
step:280/1840 train_time:9486ms step_avg:33.88ms
step:281/1840 train_time:9518ms step_avg:33.87ms
step:282/1840 train_time:9553ms step_avg:33.88ms
step:283/1840 train_time:9586ms step_avg:33.87ms
step:284/1840 train_time:9620ms step_avg:33.87ms
step:285/1840 train_time:9653ms step_avg:33.87ms
step:286/1840 train_time:9687ms step_avg:33.87ms
step:287/1840 train_time:9719ms step_avg:33.86ms
step:288/1840 train_time:9754ms step_avg:33.87ms
step:289/1840 train_time:9786ms step_avg:33.86ms
step:290/1840 train_time:9821ms step_avg:33.86ms
step:291/1840 train_time:9853ms step_avg:33.86ms
step:292/1840 train_time:9887ms step_avg:33.86ms
step:293/1840 train_time:9919ms step_avg:33.85ms
step:294/1840 train_time:9954ms step_avg:33.86ms
step:295/1840 train_time:9986ms step_avg:33.85ms
step:296/1840 train_time:10020ms step_avg:33.85ms
step:297/1840 train_time:10053ms step_avg:33.85ms
step:298/1840 train_time:10086ms step_avg:33.85ms
step:299/1840 train_time:10118ms step_avg:33.84ms
step:300/1840 train_time:10153ms step_avg:33.84ms
step:301/1840 train_time:10184ms step_avg:33.84ms
step:302/1840 train_time:10219ms step_avg:33.84ms
step:303/1840 train_time:10251ms step_avg:33.83ms
step:304/1840 train_time:10285ms step_avg:33.83ms
step:305/1840 train_time:10317ms step_avg:33.83ms
step:306/1840 train_time:10351ms step_avg:33.83ms
step:307/1840 train_time:10383ms step_avg:33.82ms
step:308/1840 train_time:10417ms step_avg:33.82ms
step:309/1840 train_time:10450ms step_avg:33.82ms
step:310/1840 train_time:10484ms step_avg:33.82ms
step:311/1840 train_time:10516ms step_avg:33.81ms
step:312/1840 train_time:10551ms step_avg:33.82ms
step:313/1840 train_time:10583ms step_avg:33.81ms
step:314/1840 train_time:10617ms step_avg:33.81ms
step:315/1840 train_time:10649ms step_avg:33.81ms
step:316/1840 train_time:10683ms step_avg:33.81ms
step:317/1840 train_time:10716ms step_avg:33.80ms
step:318/1840 train_time:10751ms step_avg:33.81ms
step:319/1840 train_time:10783ms step_avg:33.80ms
step:320/1840 train_time:10817ms step_avg:33.80ms
step:321/1840 train_time:10850ms step_avg:33.80ms
step:322/1840 train_time:10884ms step_avg:33.80ms
step:323/1840 train_time:10916ms step_avg:33.79ms
step:324/1840 train_time:10950ms step_avg:33.80ms
step:325/1840 train_time:10982ms step_avg:33.79ms
step:326/1840 train_time:11017ms step_avg:33.79ms
step:327/1840 train_time:11049ms step_avg:33.79ms
step:328/1840 train_time:11083ms step_avg:33.79ms
step:329/1840 train_time:11115ms step_avg:33.79ms
step:330/1840 train_time:11149ms step_avg:33.79ms
step:331/1840 train_time:11181ms step_avg:33.78ms
step:332/1840 train_time:11216ms step_avg:33.78ms
step:333/1840 train_time:11248ms step_avg:33.78ms
step:334/1840 train_time:11282ms step_avg:33.78ms
step:335/1840 train_time:11314ms step_avg:33.77ms
step:336/1840 train_time:11348ms step_avg:33.77ms
step:337/1840 train_time:11381ms step_avg:33.77ms
step:338/1840 train_time:11415ms step_avg:33.77ms
step:339/1840 train_time:11447ms step_avg:33.77ms
step:340/1840 train_time:11481ms step_avg:33.77ms
step:341/1840 train_time:11514ms step_avg:33.76ms
step:342/1840 train_time:11548ms step_avg:33.77ms
step:343/1840 train_time:11580ms step_avg:33.76ms
step:344/1840 train_time:11615ms step_avg:33.76ms
step:345/1840 train_time:11647ms step_avg:33.76ms
step:346/1840 train_time:11681ms step_avg:33.76ms
step:347/1840 train_time:11713ms step_avg:33.76ms
step:348/1840 train_time:11748ms step_avg:33.76ms
step:349/1840 train_time:11780ms step_avg:33.75ms
step:350/1840 train_time:11815ms step_avg:33.76ms
step:351/1840 train_time:11847ms step_avg:33.75ms
step:352/1840 train_time:11881ms step_avg:33.75ms
step:353/1840 train_time:11913ms step_avg:33.75ms
step:354/1840 train_time:11947ms step_avg:33.75ms
step:355/1840 train_time:11979ms step_avg:33.74ms
step:356/1840 train_time:12014ms step_avg:33.75ms
step:357/1840 train_time:12046ms step_avg:33.74ms
step:358/1840 train_time:12080ms step_avg:33.74ms
step:359/1840 train_time:12113ms step_avg:33.74ms
step:360/1840 train_time:12147ms step_avg:33.74ms
step:361/1840 train_time:12179ms step_avg:33.74ms
step:362/1840 train_time:12214ms step_avg:33.74ms
step:363/1840 train_time:12245ms step_avg:33.73ms
step:364/1840 train_time:12280ms step_avg:33.74ms
step:365/1840 train_time:12312ms step_avg:33.73ms
step:366/1840 train_time:12346ms step_avg:33.73ms
step:367/1840 train_time:12378ms step_avg:33.73ms
step:368/1840 train_time:12412ms step_avg:33.73ms
step:369/1840 train_time:12444ms step_avg:33.72ms
step:370/1840 train_time:12478ms step_avg:33.73ms
step:371/1840 train_time:12511ms step_avg:33.72ms
step:372/1840 train_time:12545ms step_avg:33.72ms
step:373/1840 train_time:12577ms step_avg:33.72ms
step:374/1840 train_time:12611ms step_avg:33.72ms
step:375/1840 train_time:12643ms step_avg:33.71ms
step:376/1840 train_time:12677ms step_avg:33.72ms
step:377/1840 train_time:12709ms step_avg:33.71ms
step:378/1840 train_time:12744ms step_avg:33.71ms
step:379/1840 train_time:12776ms step_avg:33.71ms
step:380/1840 train_time:12810ms step_avg:33.71ms
step:381/1840 train_time:12842ms step_avg:33.71ms
step:382/1840 train_time:12877ms step_avg:33.71ms
step:383/1840 train_time:12909ms step_avg:33.71ms
step:384/1840 train_time:12943ms step_avg:33.71ms
step:385/1840 train_time:12975ms step_avg:33.70ms
step:386/1840 train_time:13010ms step_avg:33.70ms
step:387/1840 train_time:13042ms step_avg:33.70ms
step:388/1840 train_time:13076ms step_avg:33.70ms
step:389/1840 train_time:13108ms step_avg:33.70ms
step:390/1840 train_time:13142ms step_avg:33.70ms
step:391/1840 train_time:13174ms step_avg:33.69ms
step:392/1840 train_time:13209ms step_avg:33.70ms
step:393/1840 train_time:13241ms step_avg:33.69ms
step:394/1840 train_time:13276ms step_avg:33.69ms
step:395/1840 train_time:13308ms step_avg:33.69ms
step:396/1840 train_time:13342ms step_avg:33.69ms
step:397/1840 train_time:13374ms step_avg:33.69ms
step:398/1840 train_time:13408ms step_avg:33.69ms
step:399/1840 train_time:13440ms step_avg:33.69ms
step:400/1840 train_time:13475ms step_avg:33.69ms
step:401/1840 train_time:13508ms step_avg:33.68ms
step:402/1840 train_time:13542ms step_avg:33.69ms
step:403/1840 train_time:13574ms step_avg:33.68ms
step:404/1840 train_time:13608ms step_avg:33.68ms
step:405/1840 train_time:13640ms step_avg:33.68ms
step:406/1840 train_time:13675ms step_avg:33.68ms
step:407/1840 train_time:13707ms step_avg:33.68ms
step:408/1840 train_time:13741ms step_avg:33.68ms
step:409/1840 train_time:13773ms step_avg:33.67ms
step:410/1840 train_time:13807ms step_avg:33.68ms
step:411/1840 train_time:13839ms step_avg:33.67ms
step:412/1840 train_time:13874ms step_avg:33.67ms
step:413/1840 train_time:13906ms step_avg:33.67ms
step:414/1840 train_time:13940ms step_avg:33.67ms
step:415/1840 train_time:13973ms step_avg:33.67ms
step:416/1840 train_time:14007ms step_avg:33.67ms
step:417/1840 train_time:14039ms step_avg:33.67ms
step:418/1840 train_time:14073ms step_avg:33.67ms
step:419/1840 train_time:14105ms step_avg:33.66ms
step:420/1840 train_time:14139ms step_avg:33.67ms
step:421/1840 train_time:14172ms step_avg:33.66ms
step:422/1840 train_time:14206ms step_avg:33.66ms
step:423/1840 train_time:14238ms step_avg:33.66ms
step:424/1840 train_time:14272ms step_avg:33.66ms
step:425/1840 train_time:14305ms step_avg:33.66ms
step:426/1840 train_time:14339ms step_avg:33.66ms
step:427/1840 train_time:14371ms step_avg:33.66ms
step:428/1840 train_time:14405ms step_avg:33.66ms
step:429/1840 train_time:14438ms step_avg:33.65ms
step:430/1840 train_time:14472ms step_avg:33.66ms
step:431/1840 train_time:14504ms step_avg:33.65ms
step:432/1840 train_time:14539ms step_avg:33.65ms
step:433/1840 train_time:14571ms step_avg:33.65ms
step:434/1840 train_time:14605ms step_avg:33.65ms
step:435/1840 train_time:14637ms step_avg:33.65ms
step:436/1840 train_time:14671ms step_avg:33.65ms
step:437/1840 train_time:14703ms step_avg:33.65ms
step:438/1840 train_time:14738ms step_avg:33.65ms
step:439/1840 train_time:14770ms step_avg:33.64ms
step:440/1840 train_time:14804ms step_avg:33.65ms
step:441/1840 train_time:14836ms step_avg:33.64ms
step:442/1840 train_time:14871ms step_avg:33.64ms
step:443/1840 train_time:14903ms step_avg:33.64ms
step:444/1840 train_time:14937ms step_avg:33.64ms
step:445/1840 train_time:14970ms step_avg:33.64ms
step:446/1840 train_time:15004ms step_avg:33.64ms
step:447/1840 train_time:15036ms step_avg:33.64ms
step:448/1840 train_time:15070ms step_avg:33.64ms
step:449/1840 train_time:15102ms step_avg:33.64ms
step:450/1840 train_time:15137ms step_avg:33.64ms
step:451/1840 train_time:15169ms step_avg:33.63ms
step:452/1840 train_time:15203ms step_avg:33.63ms
step:453/1840 train_time:15235ms step_avg:33.63ms
step:454/1840 train_time:15269ms step_avg:33.63ms
step:455/1840 train_time:15302ms step_avg:33.63ms
step:456/1840 train_time:15336ms step_avg:33.63ms
step:457/1840 train_time:15368ms step_avg:33.63ms
step:458/1840 train_time:15402ms step_avg:33.63ms
step:459/1840 train_time:15435ms step_avg:33.63ms
step:460/1840 train_time:15469ms step_avg:33.63ms
step:461/1840 train_time:15501ms step_avg:33.62ms
step:462/1840 train_time:15536ms step_avg:33.63ms
step:463/1840 train_time:15568ms step_avg:33.62ms
step:464/1840 train_time:15602ms step_avg:33.63ms
step:465/1840 train_time:15634ms step_avg:33.62ms
step:466/1840 train_time:15668ms step_avg:33.62ms
step:467/1840 train_time:15701ms step_avg:33.62ms
step:468/1840 train_time:15735ms step_avg:33.62ms
step:469/1840 train_time:15767ms step_avg:33.62ms
step:470/1840 train_time:15801ms step_avg:33.62ms
step:471/1840 train_time:15834ms step_avg:33.62ms
step:472/1840 train_time:15868ms step_avg:33.62ms
step:473/1840 train_time:15900ms step_avg:33.62ms
step:474/1840 train_time:15935ms step_avg:33.62ms
step:475/1840 train_time:15968ms step_avg:33.62ms
step:476/1840 train_time:16002ms step_avg:33.62ms
step:477/1840 train_time:16034ms step_avg:33.61ms
step:478/1840 train_time:16068ms step_avg:33.62ms
step:479/1840 train_time:16100ms step_avg:33.61ms
step:480/1840 train_time:16135ms step_avg:33.61ms
step:481/1840 train_time:16167ms step_avg:33.61ms
step:482/1840 train_time:16201ms step_avg:33.61ms
step:483/1840 train_time:16233ms step_avg:33.61ms
step:484/1840 train_time:16267ms step_avg:33.61ms
step:485/1840 train_time:16299ms step_avg:33.61ms
step:486/1840 train_time:16334ms step_avg:33.61ms
step:487/1840 train_time:16366ms step_avg:33.60ms
step:488/1840 train_time:16400ms step_avg:33.61ms
step:489/1840 train_time:16432ms step_avg:33.60ms
step:490/1840 train_time:16466ms step_avg:33.60ms
step:491/1840 train_time:16498ms step_avg:33.60ms
step:492/1840 train_time:16533ms step_avg:33.60ms
step:493/1840 train_time:16565ms step_avg:33.60ms
step:494/1840 train_time:16600ms step_avg:33.60ms
step:495/1840 train_time:16632ms step_avg:33.60ms
step:496/1840 train_time:16666ms step_avg:33.60ms
step:497/1840 train_time:16698ms step_avg:33.60ms
step:498/1840 train_time:16732ms step_avg:33.60ms
step:499/1840 train_time:16764ms step_avg:33.60ms
step:500/1840 train_time:16799ms step_avg:33.60ms
step:500/1840 val_loss:4.3452 train_time:16841ms step_avg:33.68ms
step:501/1840 train_time:16859ms step_avg:33.65ms
step:502/1840 train_time:16878ms step_avg:33.62ms
step:503/1840 train_time:16900ms step_avg:33.60ms
step:504/1840 train_time:16936ms step_avg:33.60ms
step:505/1840 train_time:16969ms step_avg:33.60ms
step:506/1840 train_time:17004ms step_avg:33.61ms
step:507/1840 train_time:17037ms step_avg:33.60ms
step:508/1840 train_time:17071ms step_avg:33.60ms
step:509/1840 train_time:17103ms step_avg:33.60ms
step:510/1840 train_time:17138ms step_avg:33.60ms
step:511/1840 train_time:17170ms step_avg:33.60ms
step:512/1840 train_time:17204ms step_avg:33.60ms
step:513/1840 train_time:17235ms step_avg:33.60ms
step:514/1840 train_time:17270ms step_avg:33.60ms
step:515/1840 train_time:17302ms step_avg:33.60ms
step:516/1840 train_time:17336ms step_avg:33.60ms
step:517/1840 train_time:17367ms step_avg:33.59ms
step:518/1840 train_time:17402ms step_avg:33.59ms
step:519/1840 train_time:17434ms step_avg:33.59ms
step:520/1840 train_time:17468ms step_avg:33.59ms
step:521/1840 train_time:17500ms step_avg:33.59ms
step:522/1840 train_time:17534ms step_avg:33.59ms
step:523/1840 train_time:17566ms step_avg:33.59ms
step:524/1840 train_time:17600ms step_avg:33.59ms
step:525/1840 train_time:17632ms step_avg:33.58ms
step:526/1840 train_time:17666ms step_avg:33.59ms
step:527/1840 train_time:17698ms step_avg:33.58ms
step:528/1840 train_time:17732ms step_avg:33.58ms
step:529/1840 train_time:17764ms step_avg:33.58ms
step:530/1840 train_time:17798ms step_avg:33.58ms
step:531/1840 train_time:17830ms step_avg:33.58ms
step:532/1840 train_time:17865ms step_avg:33.58ms
step:533/1840 train_time:17899ms step_avg:33.58ms
step:534/1840 train_time:17933ms step_avg:33.58ms
step:535/1840 train_time:17966ms step_avg:33.58ms
step:536/1840 train_time:18000ms step_avg:33.58ms
step:537/1840 train_time:18033ms step_avg:33.58ms
step:538/1840 train_time:18067ms step_avg:33.58ms
step:539/1840 train_time:18100ms step_avg:33.58ms
step:540/1840 train_time:18134ms step_avg:33.58ms
step:541/1840 train_time:18166ms step_avg:33.58ms
step:542/1840 train_time:18201ms step_avg:33.58ms
step:543/1840 train_time:18233ms step_avg:33.58ms
step:544/1840 train_time:18268ms step_avg:33.58ms
step:545/1840 train_time:18300ms step_avg:33.58ms
step:546/1840 train_time:18334ms step_avg:33.58ms
step:547/1840 train_time:18366ms step_avg:33.58ms
step:548/1840 train_time:18400ms step_avg:33.58ms
step:549/1840 train_time:18432ms step_avg:33.57ms
step:550/1840 train_time:18466ms step_avg:33.57ms
step:551/1840 train_time:18498ms step_avg:33.57ms
step:552/1840 train_time:18532ms step_avg:33.57ms
step:553/1840 train_time:18564ms step_avg:33.57ms
step:554/1840 train_time:18598ms step_avg:33.57ms
step:555/1840 train_time:18630ms step_avg:33.57ms
step:556/1840 train_time:18664ms step_avg:33.57ms
step:557/1840 train_time:18697ms step_avg:33.57ms
step:558/1840 train_time:18731ms step_avg:33.57ms
step:559/1840 train_time:18763ms step_avg:33.57ms
step:560/1840 train_time:18798ms step_avg:33.57ms
step:561/1840 train_time:18829ms step_avg:33.56ms
step:562/1840 train_time:18864ms step_avg:33.57ms
step:563/1840 train_time:18896ms step_avg:33.56ms
step:564/1840 train_time:18931ms step_avg:33.56ms
step:565/1840 train_time:18963ms step_avg:33.56ms
step:566/1840 train_time:18998ms step_avg:33.56ms
step:567/1840 train_time:19030ms step_avg:33.56ms
step:568/1840 train_time:19064ms step_avg:33.56ms
step:569/1840 train_time:19097ms step_avg:33.56ms
step:570/1840 train_time:19131ms step_avg:33.56ms
step:571/1840 train_time:19163ms step_avg:33.56ms
step:572/1840 train_time:19198ms step_avg:33.56ms
step:573/1840 train_time:19230ms step_avg:33.56ms
step:574/1840 train_time:19264ms step_avg:33.56ms
step:575/1840 train_time:19296ms step_avg:33.56ms
step:576/1840 train_time:19330ms step_avg:33.56ms
step:577/1840 train_time:19363ms step_avg:33.56ms
step:578/1840 train_time:19397ms step_avg:33.56ms
step:579/1840 train_time:19429ms step_avg:33.56ms
step:580/1840 train_time:19463ms step_avg:33.56ms
step:581/1840 train_time:19496ms step_avg:33.56ms
step:582/1840 train_time:19530ms step_avg:33.56ms
step:583/1840 train_time:19562ms step_avg:33.55ms
step:584/1840 train_time:19596ms step_avg:33.55ms
step:585/1840 train_time:19628ms step_avg:33.55ms
step:586/1840 train_time:19663ms step_avg:33.55ms
step:587/1840 train_time:19695ms step_avg:33.55ms
step:588/1840 train_time:19729ms step_avg:33.55ms
step:589/1840 train_time:19762ms step_avg:33.55ms
step:590/1840 train_time:19796ms step_avg:33.55ms
step:591/1840 train_time:19828ms step_avg:33.55ms
step:592/1840 train_time:19862ms step_avg:33.55ms
step:593/1840 train_time:19894ms step_avg:33.55ms
step:594/1840 train_time:19929ms step_avg:33.55ms
step:595/1840 train_time:19961ms step_avg:33.55ms
step:596/1840 train_time:19996ms step_avg:33.55ms
step:597/1840 train_time:20028ms step_avg:33.55ms
step:598/1840 train_time:20063ms step_avg:33.55ms
step:599/1840 train_time:20095ms step_avg:33.55ms
step:600/1840 train_time:20130ms step_avg:33.55ms
step:601/1840 train_time:20164ms step_avg:33.55ms
step:602/1840 train_time:20223ms step_avg:33.59ms
step:603/1840 train_time:20283ms step_avg:33.64ms
step:604/1840 train_time:20345ms step_avg:33.68ms
step:605/1840 train_time:20405ms step_avg:33.73ms
step:606/1840 train_time:20467ms step_avg:33.77ms
step:607/1840 train_time:20527ms step_avg:33.82ms
step:608/1840 train_time:20589ms step_avg:33.86ms
step:609/1840 train_time:20648ms step_avg:33.91ms
step:610/1840 train_time:20710ms step_avg:33.95ms
step:611/1840 train_time:20769ms step_avg:33.99ms
step:612/1840 train_time:20832ms step_avg:34.04ms
step:613/1840 train_time:20892ms step_avg:34.08ms
step:614/1840 train_time:20954ms step_avg:34.13ms
step:615/1840 train_time:21014ms step_avg:34.17ms
step:616/1840 train_time:21077ms step_avg:34.22ms
step:617/1840 train_time:21137ms step_avg:34.26ms
step:618/1840 train_time:21199ms step_avg:34.30ms
step:619/1840 train_time:21260ms step_avg:34.35ms
step:620/1840 train_time:21323ms step_avg:34.39ms
step:621/1840 train_time:21383ms step_avg:34.43ms
step:622/1840 train_time:21446ms step_avg:34.48ms
step:623/1840 train_time:21506ms step_avg:34.52ms
step:624/1840 train_time:21567ms step_avg:34.56ms
step:625/1840 train_time:21628ms step_avg:34.60ms
step:626/1840 train_time:21690ms step_avg:34.65ms
step:627/1840 train_time:21749ms step_avg:34.69ms
step:628/1840 train_time:21811ms step_avg:34.73ms
step:629/1840 train_time:21871ms step_avg:34.77ms
step:630/1840 train_time:21933ms step_avg:34.81ms
step:631/1840 train_time:21993ms step_avg:34.85ms
step:632/1840 train_time:22055ms step_avg:34.90ms
step:633/1840 train_time:22115ms step_avg:34.94ms
step:634/1840 train_time:22177ms step_avg:34.98ms
step:635/1840 train_time:22237ms step_avg:35.02ms
step:636/1840 train_time:22299ms step_avg:35.06ms
step:637/1840 train_time:22359ms step_avg:35.10ms
step:638/1840 train_time:22422ms step_avg:35.14ms
step:639/1840 train_time:22483ms step_avg:35.18ms
step:640/1840 train_time:22545ms step_avg:35.23ms
step:641/1840 train_time:22605ms step_avg:35.26ms
step:642/1840 train_time:22666ms step_avg:35.31ms
step:643/1840 train_time:22726ms step_avg:35.34ms
step:644/1840 train_time:22789ms step_avg:35.39ms
step:645/1840 train_time:22849ms step_avg:35.42ms
step:646/1840 train_time:22911ms step_avg:35.47ms
step:647/1840 train_time:22971ms step_avg:35.50ms
step:648/1840 train_time:23034ms step_avg:35.55ms
step:649/1840 train_time:23094ms step_avg:35.58ms
step:650/1840 train_time:23156ms step_avg:35.62ms
step:651/1840 train_time:23215ms step_avg:35.66ms
step:652/1840 train_time:23278ms step_avg:35.70ms
step:653/1840 train_time:23338ms step_avg:35.74ms
step:654/1840 train_time:23401ms step_avg:35.78ms
step:655/1840 train_time:23461ms step_avg:35.82ms
step:656/1840 train_time:23524ms step_avg:35.86ms
step:657/1840 train_time:23583ms step_avg:35.90ms
step:658/1840 train_time:23645ms step_avg:35.93ms
step:659/1840 train_time:23705ms step_avg:35.97ms
step:660/1840 train_time:23768ms step_avg:36.01ms
step:661/1840 train_time:23828ms step_avg:36.05ms
step:662/1840 train_time:23890ms step_avg:36.09ms
step:663/1840 train_time:23950ms step_avg:36.12ms
step:664/1840 train_time:24012ms step_avg:36.16ms
step:665/1840 train_time:24073ms step_avg:36.20ms
step:666/1840 train_time:24135ms step_avg:36.24ms
step:667/1840 train_time:24194ms step_avg:36.27ms
step:668/1840 train_time:24257ms step_avg:36.31ms
step:669/1840 train_time:24316ms step_avg:36.35ms
step:670/1840 train_time:24378ms step_avg:36.38ms
step:671/1840 train_time:24438ms step_avg:36.42ms
step:672/1840 train_time:24500ms step_avg:36.46ms
step:673/1840 train_time:24560ms step_avg:36.49ms
step:674/1840 train_time:24624ms step_avg:36.53ms
step:675/1840 train_time:24684ms step_avg:36.57ms
step:676/1840 train_time:24745ms step_avg:36.61ms
step:677/1840 train_time:24806ms step_avg:36.64ms
step:678/1840 train_time:24868ms step_avg:36.68ms
step:679/1840 train_time:24929ms step_avg:36.71ms
step:680/1840 train_time:24992ms step_avg:36.75ms
step:681/1840 train_time:25051ms step_avg:36.79ms
step:682/1840 train_time:25114ms step_avg:36.82ms
step:683/1840 train_time:25173ms step_avg:36.86ms
step:684/1840 train_time:25235ms step_avg:36.89ms
step:685/1840 train_time:25294ms step_avg:36.93ms
step:686/1840 train_time:25356ms step_avg:36.96ms
step:687/1840 train_time:25416ms step_avg:37.00ms
step:688/1840 train_time:25478ms step_avg:37.03ms
step:689/1840 train_time:25538ms step_avg:37.07ms
step:690/1840 train_time:25602ms step_avg:37.10ms
step:691/1840 train_time:25662ms step_avg:37.14ms
step:692/1840 train_time:25724ms step_avg:37.17ms
step:693/1840 train_time:25784ms step_avg:37.21ms
step:694/1840 train_time:25847ms step_avg:37.24ms
step:695/1840 train_time:25908ms step_avg:37.28ms
step:696/1840 train_time:25970ms step_avg:37.31ms
step:697/1840 train_time:26030ms step_avg:37.35ms
step:698/1840 train_time:26092ms step_avg:37.38ms
step:699/1840 train_time:26152ms step_avg:37.41ms
step:700/1840 train_time:26214ms step_avg:37.45ms
step:701/1840 train_time:26274ms step_avg:37.48ms
step:702/1840 train_time:26336ms step_avg:37.52ms
step:703/1840 train_time:26395ms step_avg:37.55ms
step:704/1840 train_time:26458ms step_avg:37.58ms
step:705/1840 train_time:26518ms step_avg:37.61ms
step:706/1840 train_time:26581ms step_avg:37.65ms
step:707/1840 train_time:26641ms step_avg:37.68ms
step:708/1840 train_time:26704ms step_avg:37.72ms
step:709/1840 train_time:26764ms step_avg:37.75ms
step:710/1840 train_time:26828ms step_avg:37.79ms
step:711/1840 train_time:26888ms step_avg:37.82ms
step:712/1840 train_time:26950ms step_avg:37.85ms
step:713/1840 train_time:27011ms step_avg:37.88ms
step:714/1840 train_time:27073ms step_avg:37.92ms
step:715/1840 train_time:27132ms step_avg:37.95ms
step:716/1840 train_time:27194ms step_avg:37.98ms
step:717/1840 train_time:27254ms step_avg:38.01ms
step:718/1840 train_time:27316ms step_avg:38.04ms
step:719/1840 train_time:27376ms step_avg:38.08ms
step:720/1840 train_time:27439ms step_avg:38.11ms
step:721/1840 train_time:27498ms step_avg:38.14ms
step:722/1840 train_time:27561ms step_avg:38.17ms
step:723/1840 train_time:27621ms step_avg:38.20ms
step:724/1840 train_time:27684ms step_avg:38.24ms
step:725/1840 train_time:27743ms step_avg:38.27ms
step:726/1840 train_time:27806ms step_avg:38.30ms
step:727/1840 train_time:27866ms step_avg:38.33ms
step:728/1840 train_time:27929ms step_avg:38.36ms
step:729/1840 train_time:27990ms step_avg:38.39ms
step:730/1840 train_time:28051ms step_avg:38.43ms
step:731/1840 train_time:28112ms step_avg:38.46ms
step:732/1840 train_time:28173ms step_avg:38.49ms
step:733/1840 train_time:28233ms step_avg:38.52ms
step:734/1840 train_time:28294ms step_avg:38.55ms
step:735/1840 train_time:28354ms step_avg:38.58ms
step:736/1840 train_time:28416ms step_avg:38.61ms
step:737/1840 train_time:28476ms step_avg:38.64ms
step:738/1840 train_time:28539ms step_avg:38.67ms
step:739/1840 train_time:28599ms step_avg:38.70ms
step:740/1840 train_time:28662ms step_avg:38.73ms
step:741/1840 train_time:28722ms step_avg:38.76ms
step:742/1840 train_time:28786ms step_avg:38.79ms
step:743/1840 train_time:28845ms step_avg:38.82ms
step:744/1840 train_time:28908ms step_avg:38.85ms
step:745/1840 train_time:28967ms step_avg:38.88ms
step:746/1840 train_time:29029ms step_avg:38.91ms
step:747/1840 train_time:29089ms step_avg:38.94ms
step:748/1840 train_time:29152ms step_avg:38.97ms
step:749/1840 train_time:29211ms step_avg:39.00ms
step:750/1840 train_time:29274ms step_avg:39.03ms
step:750/1840 val_loss:4.0177 train_time:29345ms step_avg:39.13ms
step:751/1840 train_time:29365ms step_avg:39.10ms
step:752/1840 train_time:29396ms step_avg:39.09ms
step:753/1840 train_time:29457ms step_avg:39.12ms
step:754/1840 train_time:29520ms step_avg:39.15ms
step:755/1840 train_time:29580ms step_avg:39.18ms
step:756/1840 train_time:29642ms step_avg:39.21ms
step:757/1840 train_time:29701ms step_avg:39.23ms
step:758/1840 train_time:29763ms step_avg:39.27ms
step:759/1840 train_time:29823ms step_avg:39.29ms
step:760/1840 train_time:29886ms step_avg:39.32ms
step:761/1840 train_time:29946ms step_avg:39.35ms
step:762/1840 train_time:30007ms step_avg:39.38ms
step:763/1840 train_time:30066ms step_avg:39.41ms
step:764/1840 train_time:30128ms step_avg:39.43ms
step:765/1840 train_time:30188ms step_avg:39.46ms
step:766/1840 train_time:30252ms step_avg:39.49ms
step:767/1840 train_time:30316ms step_avg:39.52ms
step:768/1840 train_time:30379ms step_avg:39.56ms
step:769/1840 train_time:30439ms step_avg:39.58ms
step:770/1840 train_time:30501ms step_avg:39.61ms
step:771/1840 train_time:30561ms step_avg:39.64ms
step:772/1840 train_time:30623ms step_avg:39.67ms
step:773/1840 train_time:30682ms step_avg:39.69ms
step:774/1840 train_time:30744ms step_avg:39.72ms
step:775/1840 train_time:30803ms step_avg:39.75ms
step:776/1840 train_time:30866ms step_avg:39.78ms
step:777/1840 train_time:30925ms step_avg:39.80ms
step:778/1840 train_time:30986ms step_avg:39.83ms
step:779/1840 train_time:31046ms step_avg:39.85ms
step:780/1840 train_time:31108ms step_avg:39.88ms
step:781/1840 train_time:31167ms step_avg:39.91ms
step:782/1840 train_time:31231ms step_avg:39.94ms
step:783/1840 train_time:31293ms step_avg:39.97ms
step:784/1840 train_time:31356ms step_avg:40.00ms
step:785/1840 train_time:31416ms step_avg:40.02ms
step:786/1840 train_time:31477ms step_avg:40.05ms
step:787/1840 train_time:31537ms step_avg:40.07ms
step:788/1840 train_time:31599ms step_avg:40.10ms
step:789/1840 train_time:31658ms step_avg:40.12ms
step:790/1840 train_time:31721ms step_avg:40.15ms
step:791/1840 train_time:31780ms step_avg:40.18ms
step:792/1840 train_time:31843ms step_avg:40.21ms
step:793/1840 train_time:31903ms step_avg:40.23ms
step:794/1840 train_time:31966ms step_avg:40.26ms
step:795/1840 train_time:32026ms step_avg:40.28ms
step:796/1840 train_time:32087ms step_avg:40.31ms
step:797/1840 train_time:32147ms step_avg:40.34ms
step:798/1840 train_time:32209ms step_avg:40.36ms
step:799/1840 train_time:32270ms step_avg:40.39ms
step:800/1840 train_time:32334ms step_avg:40.42ms
step:801/1840 train_time:32394ms step_avg:40.44ms
step:802/1840 train_time:32456ms step_avg:40.47ms
step:803/1840 train_time:32516ms step_avg:40.49ms
step:804/1840 train_time:32578ms step_avg:40.52ms
step:805/1840 train_time:32638ms step_avg:40.54ms
step:806/1840 train_time:32699ms step_avg:40.57ms
step:807/1840 train_time:32759ms step_avg:40.59ms
step:808/1840 train_time:32821ms step_avg:40.62ms
step:809/1840 train_time:32880ms step_avg:40.64ms
step:810/1840 train_time:32943ms step_avg:40.67ms
step:811/1840 train_time:33003ms step_avg:40.69ms
step:812/1840 train_time:33066ms step_avg:40.72ms
step:813/1840 train_time:33126ms step_avg:40.75ms
step:814/1840 train_time:33188ms step_avg:40.77ms
step:815/1840 train_time:33248ms step_avg:40.79ms
step:816/1840 train_time:33311ms step_avg:40.82ms
step:817/1840 train_time:33372ms step_avg:40.85ms
step:818/1840 train_time:33434ms step_avg:40.87ms
step:819/1840 train_time:33494ms step_avg:40.90ms
step:820/1840 train_time:33556ms step_avg:40.92ms
step:821/1840 train_time:33616ms step_avg:40.95ms
step:822/1840 train_time:33679ms step_avg:40.97ms
step:823/1840 train_time:33738ms step_avg:40.99ms
step:824/1840 train_time:33801ms step_avg:41.02ms
step:825/1840 train_time:33859ms step_avg:41.04ms
step:826/1840 train_time:33922ms step_avg:41.07ms
step:827/1840 train_time:33981ms step_avg:41.09ms
step:828/1840 train_time:34043ms step_avg:41.11ms
step:829/1840 train_time:34104ms step_avg:41.14ms
step:830/1840 train_time:34167ms step_avg:41.16ms
step:831/1840 train_time:34227ms step_avg:41.19ms
step:832/1840 train_time:34291ms step_avg:41.22ms
step:833/1840 train_time:34352ms step_avg:41.24ms
step:834/1840 train_time:34413ms step_avg:41.26ms
step:835/1840 train_time:34474ms step_avg:41.29ms
step:836/1840 train_time:34536ms step_avg:41.31ms
step:837/1840 train_time:34596ms step_avg:41.33ms
step:838/1840 train_time:34658ms step_avg:41.36ms
step:839/1840 train_time:34716ms step_avg:41.38ms
step:840/1840 train_time:34779ms step_avg:41.40ms
step:841/1840 train_time:34839ms step_avg:41.43ms
step:842/1840 train_time:34901ms step_avg:41.45ms
step:843/1840 train_time:34961ms step_avg:41.47ms
step:844/1840 train_time:35023ms step_avg:41.50ms
step:845/1840 train_time:35083ms step_avg:41.52ms
step:846/1840 train_time:35146ms step_avg:41.54ms
step:847/1840 train_time:35206ms step_avg:41.57ms
step:848/1840 train_time:35269ms step_avg:41.59ms
step:849/1840 train_time:35329ms step_avg:41.61ms
step:850/1840 train_time:35392ms step_avg:41.64ms
step:851/1840 train_time:35453ms step_avg:41.66ms
step:852/1840 train_time:35515ms step_avg:41.68ms
step:853/1840 train_time:35575ms step_avg:41.71ms
step:854/1840 train_time:35637ms step_avg:41.73ms
step:855/1840 train_time:35697ms step_avg:41.75ms
step:856/1840 train_time:35759ms step_avg:41.77ms
step:857/1840 train_time:35819ms step_avg:41.80ms
step:858/1840 train_time:35880ms step_avg:41.82ms
step:859/1840 train_time:35939ms step_avg:41.84ms
step:860/1840 train_time:36002ms step_avg:41.86ms
step:861/1840 train_time:36063ms step_avg:41.89ms
step:862/1840 train_time:36125ms step_avg:41.91ms
step:863/1840 train_time:36185ms step_avg:41.93ms
step:864/1840 train_time:36248ms step_avg:41.95ms
step:865/1840 train_time:36308ms step_avg:41.98ms
step:866/1840 train_time:36372ms step_avg:42.00ms
step:867/1840 train_time:36432ms step_avg:42.02ms
step:868/1840 train_time:36494ms step_avg:42.04ms
step:869/1840 train_time:36554ms step_avg:42.06ms
step:870/1840 train_time:36616ms step_avg:42.09ms
step:871/1840 train_time:36676ms step_avg:42.11ms
step:872/1840 train_time:36738ms step_avg:42.13ms
step:873/1840 train_time:36797ms step_avg:42.15ms
step:874/1840 train_time:36859ms step_avg:42.17ms
step:875/1840 train_time:36918ms step_avg:42.19ms
step:876/1840 train_time:36981ms step_avg:42.22ms
step:877/1840 train_time:37040ms step_avg:42.23ms
step:878/1840 train_time:37102ms step_avg:42.26ms
step:879/1840 train_time:37162ms step_avg:42.28ms
step:880/1840 train_time:37225ms step_avg:42.30ms
step:881/1840 train_time:37285ms step_avg:42.32ms
step:882/1840 train_time:37348ms step_avg:42.34ms
step:883/1840 train_time:37408ms step_avg:42.36ms
step:884/1840 train_time:37471ms step_avg:42.39ms
step:885/1840 train_time:37531ms step_avg:42.41ms
step:886/1840 train_time:37593ms step_avg:42.43ms
step:887/1840 train_time:37653ms step_avg:42.45ms
step:888/1840 train_time:37716ms step_avg:42.47ms
step:889/1840 train_time:37775ms step_avg:42.49ms
step:890/1840 train_time:37837ms step_avg:42.51ms
step:891/1840 train_time:37896ms step_avg:42.53ms
step:892/1840 train_time:37958ms step_avg:42.55ms
step:893/1840 train_time:38018ms step_avg:42.57ms
step:894/1840 train_time:38081ms step_avg:42.60ms
step:895/1840 train_time:38140ms step_avg:42.62ms
step:896/1840 train_time:38203ms step_avg:42.64ms
step:897/1840 train_time:38263ms step_avg:42.66ms
step:898/1840 train_time:38326ms step_avg:42.68ms
step:899/1840 train_time:38387ms step_avg:42.70ms
step:900/1840 train_time:38450ms step_avg:42.72ms
step:901/1840 train_time:38510ms step_avg:42.74ms
step:902/1840 train_time:38572ms step_avg:42.76ms
step:903/1840 train_time:38632ms step_avg:42.78ms
step:904/1840 train_time:38694ms step_avg:42.80ms
step:905/1840 train_time:38754ms step_avg:42.82ms
step:906/1840 train_time:38816ms step_avg:42.84ms
step:907/1840 train_time:38876ms step_avg:42.86ms
step:908/1840 train_time:38937ms step_avg:42.88ms
step:909/1840 train_time:38997ms step_avg:42.90ms
step:910/1840 train_time:39059ms step_avg:42.92ms
step:911/1840 train_time:39119ms step_avg:42.94ms
step:912/1840 train_time:39181ms step_avg:42.96ms
step:913/1840 train_time:39241ms step_avg:42.98ms
step:914/1840 train_time:39305ms step_avg:43.00ms
step:915/1840 train_time:39366ms step_avg:43.02ms
step:916/1840 train_time:39429ms step_avg:43.04ms
step:917/1840 train_time:39489ms step_avg:43.06ms
step:918/1840 train_time:39552ms step_avg:43.09ms
step:919/1840 train_time:39612ms step_avg:43.10ms
step:920/1840 train_time:39674ms step_avg:43.12ms
step:921/1840 train_time:39734ms step_avg:43.14ms
step:922/1840 train_time:39796ms step_avg:43.16ms
step:923/1840 train_time:39855ms step_avg:43.18ms
step:924/1840 train_time:39917ms step_avg:43.20ms
step:925/1840 train_time:39976ms step_avg:43.22ms
step:926/1840 train_time:40039ms step_avg:43.24ms
step:927/1840 train_time:40099ms step_avg:43.26ms
step:928/1840 train_time:40162ms step_avg:43.28ms
step:929/1840 train_time:40220ms step_avg:43.29ms
step:930/1840 train_time:40283ms step_avg:43.32ms
step:931/1840 train_time:40344ms step_avg:43.33ms
step:932/1840 train_time:40408ms step_avg:43.36ms
step:933/1840 train_time:40468ms step_avg:43.37ms
step:934/1840 train_time:40530ms step_avg:43.39ms
step:935/1840 train_time:40590ms step_avg:43.41ms
step:936/1840 train_time:40652ms step_avg:43.43ms
step:937/1840 train_time:40713ms step_avg:43.45ms
step:938/1840 train_time:40774ms step_avg:43.47ms
step:939/1840 train_time:40834ms step_avg:43.49ms
step:940/1840 train_time:40896ms step_avg:43.51ms
step:941/1840 train_time:40956ms step_avg:43.52ms
step:942/1840 train_time:41018ms step_avg:43.54ms
step:943/1840 train_time:41078ms step_avg:43.56ms
step:944/1840 train_time:41140ms step_avg:43.58ms
step:945/1840 train_time:41199ms step_avg:43.60ms
step:946/1840 train_time:41261ms step_avg:43.62ms
step:947/1840 train_time:41322ms step_avg:43.63ms
step:948/1840 train_time:41385ms step_avg:43.66ms
step:949/1840 train_time:41445ms step_avg:43.67ms
step:950/1840 train_time:41509ms step_avg:43.69ms
step:951/1840 train_time:41569ms step_avg:43.71ms
step:952/1840 train_time:41632ms step_avg:43.73ms
step:953/1840 train_time:41691ms step_avg:43.75ms
step:954/1840 train_time:41754ms step_avg:43.77ms
step:955/1840 train_time:41814ms step_avg:43.78ms
step:956/1840 train_time:41876ms step_avg:43.80ms
step:957/1840 train_time:41936ms step_avg:43.82ms
step:958/1840 train_time:41998ms step_avg:43.84ms
step:959/1840 train_time:42057ms step_avg:43.85ms
step:960/1840 train_time:42119ms step_avg:43.87ms
step:961/1840 train_time:42178ms step_avg:43.89ms
step:962/1840 train_time:42241ms step_avg:43.91ms
step:963/1840 train_time:42301ms step_avg:43.93ms
step:964/1840 train_time:42363ms step_avg:43.95ms
step:965/1840 train_time:42423ms step_avg:43.96ms
step:966/1840 train_time:42487ms step_avg:43.98ms
step:967/1840 train_time:42548ms step_avg:44.00ms
step:968/1840 train_time:42610ms step_avg:44.02ms
step:969/1840 train_time:42670ms step_avg:44.04ms
step:970/1840 train_time:42733ms step_avg:44.05ms
step:971/1840 train_time:42793ms step_avg:44.07ms
step:972/1840 train_time:42855ms step_avg:44.09ms
step:973/1840 train_time:42916ms step_avg:44.11ms
step:974/1840 train_time:42977ms step_avg:44.12ms
step:975/1840 train_time:43037ms step_avg:44.14ms
step:976/1840 train_time:43099ms step_avg:44.16ms
step:977/1840 train_time:43158ms step_avg:44.17ms
step:978/1840 train_time:43221ms step_avg:44.19ms
step:979/1840 train_time:43280ms step_avg:44.21ms
step:980/1840 train_time:43342ms step_avg:44.23ms
step:981/1840 train_time:43402ms step_avg:44.24ms
step:982/1840 train_time:43466ms step_avg:44.26ms
step:983/1840 train_time:43526ms step_avg:44.28ms
step:984/1840 train_time:43589ms step_avg:44.30ms
step:985/1840 train_time:43649ms step_avg:44.31ms
step:986/1840 train_time:43711ms step_avg:44.33ms
step:987/1840 train_time:43771ms step_avg:44.35ms
step:988/1840 train_time:43834ms step_avg:44.37ms
step:989/1840 train_time:43894ms step_avg:44.38ms
step:990/1840 train_time:43956ms step_avg:44.40ms
step:991/1840 train_time:44016ms step_avg:44.42ms
step:992/1840 train_time:44078ms step_avg:44.43ms
step:993/1840 train_time:44139ms step_avg:44.45ms
step:994/1840 train_time:44201ms step_avg:44.47ms
step:995/1840 train_time:44261ms step_avg:44.48ms
step:996/1840 train_time:44321ms step_avg:44.50ms
step:997/1840 train_time:44381ms step_avg:44.51ms
step:998/1840 train_time:44443ms step_avg:44.53ms
step:999/1840 train_time:44504ms step_avg:44.55ms
step:1000/1840 train_time:44567ms step_avg:44.57ms
step:1000/1840 val_loss:3.7718 train_time:44639ms step_avg:44.64ms
step:1001/1840 train_time:44659ms step_avg:44.61ms
step:1002/1840 train_time:44691ms step_avg:44.60ms
step:1003/1840 train_time:44753ms step_avg:44.62ms
step:1004/1840 train_time:44818ms step_avg:44.64ms
step:1005/1840 train_time:44879ms step_avg:44.66ms
step:1006/1840 train_time:44941ms step_avg:44.67ms
step:1007/1840 train_time:45000ms step_avg:44.69ms
step:1008/1840 train_time:45061ms step_avg:44.70ms
step:1009/1840 train_time:45122ms step_avg:44.72ms
step:1010/1840 train_time:45183ms step_avg:44.74ms
step:1011/1840 train_time:45242ms step_avg:44.75ms
step:1012/1840 train_time:45304ms step_avg:44.77ms
step:1013/1840 train_time:45363ms step_avg:44.78ms
step:1014/1840 train_time:45424ms step_avg:44.80ms
step:1015/1840 train_time:45483ms step_avg:44.81ms
step:1016/1840 train_time:45545ms step_avg:44.83ms
step:1017/1840 train_time:45607ms step_avg:44.84ms
step:1018/1840 train_time:45672ms step_avg:44.86ms
step:1019/1840 train_time:45733ms step_avg:44.88ms
step:1020/1840 train_time:45796ms step_avg:44.90ms
step:1021/1840 train_time:45856ms step_avg:44.91ms
step:1022/1840 train_time:45919ms step_avg:44.93ms
step:1023/1840 train_time:45978ms step_avg:44.94ms
step:1024/1840 train_time:46041ms step_avg:44.96ms
step:1025/1840 train_time:46100ms step_avg:44.98ms
step:1026/1840 train_time:46162ms step_avg:44.99ms
step:1027/1840 train_time:46222ms step_avg:45.01ms
step:1028/1840 train_time:46284ms step_avg:45.02ms
step:1029/1840 train_time:46342ms step_avg:45.04ms
step:1030/1840 train_time:46404ms step_avg:45.05ms
step:1031/1840 train_time:46463ms step_avg:45.07ms
step:1032/1840 train_time:46525ms step_avg:45.08ms
step:1033/1840 train_time:46586ms step_avg:45.10ms
step:1034/1840 train_time:46648ms step_avg:45.11ms
step:1035/1840 train_time:46710ms step_avg:45.13ms
step:1036/1840 train_time:46774ms step_avg:45.15ms
step:1037/1840 train_time:46834ms step_avg:45.16ms
step:1038/1840 train_time:46896ms step_avg:45.18ms
step:1039/1840 train_time:46956ms step_avg:45.19ms
step:1040/1840 train_time:47019ms step_avg:45.21ms
step:1041/1840 train_time:47078ms step_avg:45.22ms
step:1042/1840 train_time:47141ms step_avg:45.24ms
step:1043/1840 train_time:47201ms step_avg:45.25ms
step:1044/1840 train_time:47262ms step_avg:45.27ms
step:1045/1840 train_time:47322ms step_avg:45.28ms
step:1046/1840 train_time:47383ms step_avg:45.30ms
step:1047/1840 train_time:47443ms step_avg:45.31ms
step:1048/1840 train_time:47505ms step_avg:45.33ms
step:1049/1840 train_time:47565ms step_avg:45.34ms
step:1050/1840 train_time:47628ms step_avg:45.36ms
step:1051/1840 train_time:47688ms step_avg:45.37ms
step:1052/1840 train_time:47750ms step_avg:45.39ms
step:1053/1840 train_time:47810ms step_avg:45.40ms
step:1054/1840 train_time:47873ms step_avg:45.42ms
step:1055/1840 train_time:47933ms step_avg:45.43ms
step:1056/1840 train_time:47997ms step_avg:45.45ms
step:1057/1840 train_time:48057ms step_avg:45.47ms
step:1058/1840 train_time:48120ms step_avg:45.48ms
step:1059/1840 train_time:48179ms step_avg:45.50ms
step:1060/1840 train_time:48241ms step_avg:45.51ms
step:1061/1840 train_time:48301ms step_avg:45.52ms
step:1062/1840 train_time:48363ms step_avg:45.54ms
step:1063/1840 train_time:48423ms step_avg:45.55ms
step:1064/1840 train_time:48485ms step_avg:45.57ms
step:1065/1840 train_time:48545ms step_avg:45.58ms
step:1066/1840 train_time:48607ms step_avg:45.60ms
step:1067/1840 train_time:48667ms step_avg:45.61ms
step:1068/1840 train_time:48729ms step_avg:45.63ms
step:1069/1840 train_time:48789ms step_avg:45.64ms
step:1070/1840 train_time:48851ms step_avg:45.66ms
step:1071/1840 train_time:48911ms step_avg:45.67ms
step:1072/1840 train_time:48973ms step_avg:45.68ms
step:1073/1840 train_time:49034ms step_avg:45.70ms
step:1074/1840 train_time:49097ms step_avg:45.71ms
step:1075/1840 train_time:49157ms step_avg:45.73ms
step:1076/1840 train_time:49219ms step_avg:45.74ms
step:1077/1840 train_time:49279ms step_avg:45.76ms
step:1078/1840 train_time:49341ms step_avg:45.77ms
step:1079/1840 train_time:49401ms step_avg:45.78ms
step:1080/1840 train_time:49463ms step_avg:45.80ms
step:1081/1840 train_time:49523ms step_avg:45.81ms
step:1082/1840 train_time:49585ms step_avg:45.83ms
step:1083/1840 train_time:49645ms step_avg:45.84ms
step:1084/1840 train_time:49706ms step_avg:45.85ms
step:1085/1840 train_time:49767ms step_avg:45.87ms
step:1086/1840 train_time:49829ms step_avg:45.88ms
step:1087/1840 train_time:49889ms step_avg:45.90ms
step:1088/1840 train_time:49952ms step_avg:45.91ms
step:1089/1840 train_time:50012ms step_avg:45.92ms
step:1090/1840 train_time:50075ms step_avg:45.94ms
step:1091/1840 train_time:50135ms step_avg:45.95ms
step:1092/1840 train_time:50197ms step_avg:45.97ms
step:1093/1840 train_time:50256ms step_avg:45.98ms
step:1094/1840 train_time:50319ms step_avg:46.00ms
step:1095/1840 train_time:50380ms step_avg:46.01ms
step:1096/1840 train_time:50442ms step_avg:46.02ms
step:1097/1840 train_time:50503ms step_avg:46.04ms
step:1098/1840 train_time:50565ms step_avg:46.05ms
step:1099/1840 train_time:50624ms step_avg:46.06ms
step:1100/1840 train_time:50686ms step_avg:46.08ms
step:1101/1840 train_time:50746ms step_avg:46.09ms
step:1102/1840 train_time:50809ms step_avg:46.11ms
step:1103/1840 train_time:50868ms step_avg:46.12ms
step:1104/1840 train_time:50930ms step_avg:46.13ms
step:1105/1840 train_time:50990ms step_avg:46.14ms
step:1106/1840 train_time:51052ms step_avg:46.16ms
step:1107/1840 train_time:51112ms step_avg:46.17ms
step:1108/1840 train_time:51174ms step_avg:46.19ms
step:1109/1840 train_time:51235ms step_avg:46.20ms
step:1110/1840 train_time:51298ms step_avg:46.21ms
step:1111/1840 train_time:51358ms step_avg:46.23ms
step:1112/1840 train_time:51421ms step_avg:46.24ms
step:1113/1840 train_time:51481ms step_avg:46.25ms
step:1114/1840 train_time:51543ms step_avg:46.27ms
step:1115/1840 train_time:51603ms step_avg:46.28ms
step:1116/1840 train_time:51665ms step_avg:46.30ms
step:1117/1840 train_time:51725ms step_avg:46.31ms
step:1118/1840 train_time:51787ms step_avg:46.32ms
step:1119/1840 train_time:51847ms step_avg:46.33ms
step:1120/1840 train_time:51909ms step_avg:46.35ms
step:1121/1840 train_time:51968ms step_avg:46.36ms
step:1122/1840 train_time:52031ms step_avg:46.37ms
step:1123/1840 train_time:52090ms step_avg:46.39ms
step:1124/1840 train_time:52153ms step_avg:46.40ms
step:1125/1840 train_time:52213ms step_avg:46.41ms
step:1126/1840 train_time:52277ms step_avg:46.43ms
step:1127/1840 train_time:52337ms step_avg:46.44ms
step:1128/1840 train_time:52400ms step_avg:46.45ms
step:1129/1840 train_time:52460ms step_avg:46.47ms
step:1130/1840 train_time:52523ms step_avg:46.48ms
step:1131/1840 train_time:52583ms step_avg:46.49ms
step:1132/1840 train_time:52645ms step_avg:46.51ms
step:1133/1840 train_time:52705ms step_avg:46.52ms
step:1134/1840 train_time:52767ms step_avg:46.53ms
step:1135/1840 train_time:52827ms step_avg:46.54ms
step:1136/1840 train_time:52890ms step_avg:46.56ms
step:1137/1840 train_time:52948ms step_avg:46.57ms
step:1138/1840 train_time:53011ms step_avg:46.58ms
step:1139/1840 train_time:53070ms step_avg:46.59ms
step:1140/1840 train_time:53132ms step_avg:46.61ms
step:1141/1840 train_time:53192ms step_avg:46.62ms
step:1142/1840 train_time:53256ms step_avg:46.63ms
step:1143/1840 train_time:53316ms step_avg:46.65ms
step:1144/1840 train_time:53379ms step_avg:46.66ms
step:1145/1840 train_time:53439ms step_avg:46.67ms
step:1146/1840 train_time:53502ms step_avg:46.69ms
step:1147/1840 train_time:53562ms step_avg:46.70ms
step:1148/1840 train_time:53624ms step_avg:46.71ms
step:1149/1840 train_time:53684ms step_avg:46.72ms
step:1150/1840 train_time:53746ms step_avg:46.74ms
step:1151/1840 train_time:53805ms step_avg:46.75ms
step:1152/1840 train_time:53867ms step_avg:46.76ms
step:1153/1840 train_time:53927ms step_avg:46.77ms
step:1154/1840 train_time:53989ms step_avg:46.78ms
step:1155/1840 train_time:54049ms step_avg:46.80ms
step:1156/1840 train_time:54110ms step_avg:46.81ms
step:1157/1840 train_time:54170ms step_avg:46.82ms
step:1158/1840 train_time:54233ms step_avg:46.83ms
step:1159/1840 train_time:54293ms step_avg:46.85ms
step:1160/1840 train_time:54356ms step_avg:46.86ms
step:1161/1840 train_time:54417ms step_avg:46.87ms
step:1162/1840 train_time:54480ms step_avg:46.88ms
step:1163/1840 train_time:54540ms step_avg:46.90ms
step:1164/1840 train_time:54603ms step_avg:46.91ms
step:1165/1840 train_time:54663ms step_avg:46.92ms
step:1166/1840 train_time:54725ms step_avg:46.93ms
step:1167/1840 train_time:54784ms step_avg:46.94ms
step:1168/1840 train_time:54846ms step_avg:46.96ms
step:1169/1840 train_time:54905ms step_avg:46.97ms
step:1170/1840 train_time:54968ms step_avg:46.98ms
step:1171/1840 train_time:55028ms step_avg:46.99ms
step:1172/1840 train_time:55091ms step_avg:47.01ms
step:1173/1840 train_time:55150ms step_avg:47.02ms
step:1174/1840 train_time:55212ms step_avg:47.03ms
step:1175/1840 train_time:55273ms step_avg:47.04ms
step:1176/1840 train_time:55335ms step_avg:47.05ms
step:1177/1840 train_time:55395ms step_avg:47.06ms
step:1178/1840 train_time:55458ms step_avg:47.08ms
step:1179/1840 train_time:55518ms step_avg:47.09ms
step:1180/1840 train_time:55581ms step_avg:47.10ms
step:1181/1840 train_time:55641ms step_avg:47.11ms
step:1182/1840 train_time:55704ms step_avg:47.13ms
step:1183/1840 train_time:55764ms step_avg:47.14ms
step:1184/1840 train_time:55825ms step_avg:47.15ms
step:1185/1840 train_time:55885ms step_avg:47.16ms
step:1186/1840 train_time:55947ms step_avg:47.17ms
step:1187/1840 train_time:56007ms step_avg:47.18ms
step:1188/1840 train_time:56069ms step_avg:47.20ms
step:1189/1840 train_time:56129ms step_avg:47.21ms
step:1190/1840 train_time:56191ms step_avg:47.22ms
step:1191/1840 train_time:56251ms step_avg:47.23ms
step:1192/1840 train_time:56313ms step_avg:47.24ms
step:1193/1840 train_time:56373ms step_avg:47.25ms
step:1194/1840 train_time:56435ms step_avg:47.27ms
step:1195/1840 train_time:56496ms step_avg:47.28ms
step:1196/1840 train_time:56558ms step_avg:47.29ms
step:1197/1840 train_time:56619ms step_avg:47.30ms
step:1198/1840 train_time:56682ms step_avg:47.31ms
step:1199/1840 train_time:56742ms step_avg:47.32ms
step:1200/1840 train_time:56804ms step_avg:47.34ms
step:1201/1840 train_time:56864ms step_avg:47.35ms
step:1202/1840 train_time:56950ms step_avg:47.38ms
step:1203/1840 train_time:57036ms step_avg:47.41ms
step:1204/1840 train_time:57125ms step_avg:47.45ms
step:1205/1840 train_time:57212ms step_avg:47.48ms
step:1206/1840 train_time:57301ms step_avg:47.51ms
step:1207/1840 train_time:57386ms step_avg:47.54ms
step:1208/1840 train_time:57475ms step_avg:47.58ms
step:1209/1840 train_time:57563ms step_avg:47.61ms
step:1210/1840 train_time:57654ms step_avg:47.65ms
step:1211/1840 train_time:57742ms step_avg:47.68ms
step:1212/1840 train_time:57832ms step_avg:47.72ms
step:1213/1840 train_time:57918ms step_avg:47.75ms
step:1214/1840 train_time:58007ms step_avg:47.78ms
step:1215/1840 train_time:58093ms step_avg:47.81ms
step:1216/1840 train_time:58182ms step_avg:47.85ms
step:1217/1840 train_time:58268ms step_avg:47.88ms
step:1218/1840 train_time:58355ms step_avg:47.91ms
step:1219/1840 train_time:58442ms step_avg:47.94ms
step:1220/1840 train_time:58532ms step_avg:47.98ms
step:1221/1840 train_time:58619ms step_avg:48.01ms
step:1222/1840 train_time:58710ms step_avg:48.04ms
step:1223/1840 train_time:58795ms step_avg:48.07ms
step:1224/1840 train_time:58884ms step_avg:48.11ms
step:1225/1840 train_time:58972ms step_avg:48.14ms
step:1226/1840 train_time:59059ms step_avg:48.17ms
step:1227/1840 train_time:59145ms step_avg:48.20ms
step:1228/1840 train_time:59233ms step_avg:48.24ms
step:1229/1840 train_time:59319ms step_avg:48.27ms
step:1230/1840 train_time:59409ms step_avg:48.30ms
step:1231/1840 train_time:59494ms step_avg:48.33ms
step:1232/1840 train_time:59584ms step_avg:48.36ms
step:1233/1840 train_time:59671ms step_avg:48.40ms
step:1234/1840 train_time:59759ms step_avg:48.43ms
step:1235/1840 train_time:59846ms step_avg:48.46ms
step:1236/1840 train_time:59935ms step_avg:48.49ms
step:1237/1840 train_time:60021ms step_avg:48.52ms
step:1238/1840 train_time:60110ms step_avg:48.55ms
step:1239/1840 train_time:60195ms step_avg:48.58ms
step:1240/1840 train_time:60285ms step_avg:48.62ms
step:1241/1840 train_time:60371ms step_avg:48.65ms
step:1242/1840 train_time:60458ms step_avg:48.68ms
step:1243/1840 train_time:60546ms step_avg:48.71ms
step:1244/1840 train_time:60634ms step_avg:48.74ms
step:1245/1840 train_time:60721ms step_avg:48.77ms
step:1246/1840 train_time:60810ms step_avg:48.80ms
step:1247/1840 train_time:60895ms step_avg:48.83ms
step:1248/1840 train_time:60984ms step_avg:48.87ms
step:1249/1840 train_time:61070ms step_avg:48.90ms
step:1250/1840 train_time:61158ms step_avg:48.93ms
step:1250/1840 val_loss:3.5304 train_time:61259ms step_avg:49.01ms
step:1251/1840 train_time:61279ms step_avg:48.98ms
step:1252/1840 train_time:61336ms step_avg:48.99ms
step:1253/1840 train_time:61425ms step_avg:49.02ms
step:1254/1840 train_time:61515ms step_avg:49.05ms
step:1255/1840 train_time:61603ms step_avg:49.09ms
step:1256/1840 train_time:61692ms step_avg:49.12ms
step:1257/1840 train_time:61777ms step_avg:49.15ms
step:1258/1840 train_time:61866ms step_avg:49.18ms
step:1259/1840 train_time:61951ms step_avg:49.21ms
step:1260/1840 train_time:62039ms step_avg:49.24ms
step:1261/1840 train_time:62125ms step_avg:49.27ms
step:1262/1840 train_time:62214ms step_avg:49.30ms
step:1263/1840 train_time:62305ms step_avg:49.33ms
step:1264/1840 train_time:62395ms step_avg:49.36ms
step:1265/1840 train_time:62481ms step_avg:49.39ms
step:1266/1840 train_time:62573ms step_avg:49.43ms
step:1267/1840 train_time:62658ms step_avg:49.45ms
step:1268/1840 train_time:62749ms step_avg:49.49ms
step:1269/1840 train_time:62835ms step_avg:49.52ms
step:1270/1840 train_time:62923ms step_avg:49.55ms
step:1271/1840 train_time:63008ms step_avg:49.57ms
step:1272/1840 train_time:63097ms step_avg:49.60ms
step:1273/1840 train_time:63183ms step_avg:49.63ms
step:1274/1840 train_time:63273ms step_avg:49.66ms
step:1275/1840 train_time:63360ms step_avg:49.69ms
step:1276/1840 train_time:63450ms step_avg:49.73ms
step:1277/1840 train_time:63536ms step_avg:49.75ms
step:1278/1840 train_time:63626ms step_avg:49.79ms
step:1279/1840 train_time:63711ms step_avg:49.81ms
step:1280/1840 train_time:63800ms step_avg:49.84ms
step:1281/1840 train_time:63886ms step_avg:49.87ms
step:1282/1840 train_time:63972ms step_avg:49.90ms
step:1283/1840 train_time:64059ms step_avg:49.93ms
step:1284/1840 train_time:64148ms step_avg:49.96ms
step:1285/1840 train_time:64234ms step_avg:49.99ms
step:1286/1840 train_time:64326ms step_avg:50.02ms
step:1287/1840 train_time:64413ms step_avg:50.05ms
step:1288/1840 train_time:64503ms step_avg:50.08ms
step:1289/1840 train_time:64590ms step_avg:50.11ms
step:1290/1840 train_time:64677ms step_avg:50.14ms
step:1291/1840 train_time:64765ms step_avg:50.17ms
step:1292/1840 train_time:64852ms step_avg:50.20ms
step:1293/1840 train_time:64938ms step_avg:50.22ms
step:1294/1840 train_time:65029ms step_avg:50.25ms
step:1295/1840 train_time:65114ms step_avg:50.28ms
step:1296/1840 train_time:65203ms step_avg:50.31ms
step:1297/1840 train_time:65290ms step_avg:50.34ms
step:1298/1840 train_time:65380ms step_avg:50.37ms
step:1299/1840 train_time:65467ms step_avg:50.40ms
step:1300/1840 train_time:65555ms step_avg:50.43ms
step:1301/1840 train_time:65641ms step_avg:50.45ms
step:1302/1840 train_time:65731ms step_avg:50.48ms
step:1303/1840 train_time:65816ms step_avg:50.51ms
step:1304/1840 train_time:65906ms step_avg:50.54ms
step:1305/1840 train_time:65991ms step_avg:50.57ms
step:1306/1840 train_time:66079ms step_avg:50.60ms
step:1307/1840 train_time:66166ms step_avg:50.62ms
step:1308/1840 train_time:66254ms step_avg:50.65ms
step:1309/1840 train_time:66342ms step_avg:50.68ms
step:1310/1840 train_time:66432ms step_avg:50.71ms
step:1311/1840 train_time:66519ms step_avg:50.74ms
step:1312/1840 train_time:66607ms step_avg:50.77ms
step:1313/1840 train_time:66693ms step_avg:50.79ms
step:1314/1840 train_time:66782ms step_avg:50.82ms
step:1315/1840 train_time:66869ms step_avg:50.85ms
step:1316/1840 train_time:66957ms step_avg:50.88ms
step:1317/1840 train_time:67044ms step_avg:50.91ms
step:1318/1840 train_time:67132ms step_avg:50.93ms
step:1319/1840 train_time:67218ms step_avg:50.96ms
step:1320/1840 train_time:67307ms step_avg:50.99ms
step:1321/1840 train_time:67392ms step_avg:51.02ms
step:1322/1840 train_time:67482ms step_avg:51.05ms
step:1323/1840 train_time:67570ms step_avg:51.07ms
step:1324/1840 train_time:67658ms step_avg:51.10ms
step:1325/1840 train_time:67744ms step_avg:51.13ms
step:1326/1840 train_time:67833ms step_avg:51.16ms
step:1327/1840 train_time:67919ms step_avg:51.18ms
step:1328/1840 train_time:68009ms step_avg:51.21ms
step:1329/1840 train_time:68095ms step_avg:51.24ms
step:1330/1840 train_time:68183ms step_avg:51.27ms
step:1331/1840 train_time:68269ms step_avg:51.29ms
step:1332/1840 train_time:68359ms step_avg:51.32ms
step:1333/1840 train_time:68446ms step_avg:51.35ms
step:1334/1840 train_time:68533ms step_avg:51.37ms
step:1335/1840 train_time:68619ms step_avg:51.40ms
step:1336/1840 train_time:68709ms step_avg:51.43ms
step:1337/1840 train_time:68794ms step_avg:51.45ms
step:1338/1840 train_time:68883ms step_avg:51.48ms
step:1339/1840 train_time:68971ms step_avg:51.51ms
step:1340/1840 train_time:69060ms step_avg:51.54ms
step:1341/1840 train_time:69146ms step_avg:51.56ms
step:1342/1840 train_time:69234ms step_avg:51.59ms
step:1343/1840 train_time:69319ms step_avg:51.62ms
step:1344/1840 train_time:69410ms step_avg:51.64ms
step:1345/1840 train_time:69496ms step_avg:51.67ms
step:1346/1840 train_time:69586ms step_avg:51.70ms
step:1347/1840 train_time:69673ms step_avg:51.72ms
step:1348/1840 train_time:69760ms step_avg:51.75ms
step:1349/1840 train_time:69847ms step_avg:51.78ms
step:1350/1840 train_time:69935ms step_avg:51.80ms
step:1351/1840 train_time:70022ms step_avg:51.83ms
step:1352/1840 train_time:70111ms step_avg:51.86ms
step:1353/1840 train_time:70197ms step_avg:51.88ms
step:1354/1840 train_time:70286ms step_avg:51.91ms
step:1355/1840 train_time:70371ms step_avg:51.93ms
step:1356/1840 train_time:70460ms step_avg:51.96ms
step:1357/1840 train_time:70547ms step_avg:51.99ms
step:1358/1840 train_time:70635ms step_avg:52.01ms
step:1359/1840 train_time:70721ms step_avg:52.04ms
step:1360/1840 train_time:70811ms step_avg:52.07ms
step:1361/1840 train_time:70896ms step_avg:52.09ms
step:1362/1840 train_time:70986ms step_avg:52.12ms
step:1363/1840 train_time:71072ms step_avg:52.14ms
step:1364/1840 train_time:71159ms step_avg:52.17ms
step:1365/1840 train_time:71246ms step_avg:52.20ms
step:1366/1840 train_time:71335ms step_avg:52.22ms
step:1367/1840 train_time:71424ms step_avg:52.25ms
step:1368/1840 train_time:71512ms step_avg:52.27ms
step:1369/1840 train_time:71598ms step_avg:52.30ms
step:1370/1840 train_time:71686ms step_avg:52.33ms
step:1371/1840 train_time:71772ms step_avg:52.35ms
step:1372/1840 train_time:71862ms step_avg:52.38ms
step:1373/1840 train_time:71948ms step_avg:52.40ms
step:1374/1840 train_time:72036ms step_avg:52.43ms
step:1375/1840 train_time:72123ms step_avg:52.45ms
step:1376/1840 train_time:72211ms step_avg:52.48ms
step:1377/1840 train_time:72296ms step_avg:52.50ms
step:1378/1840 train_time:72387ms step_avg:52.53ms
step:1379/1840 train_time:72473ms step_avg:52.55ms
step:1380/1840 train_time:72562ms step_avg:52.58ms
step:1381/1840 train_time:72649ms step_avg:52.61ms
step:1382/1840 train_time:72736ms step_avg:52.63ms
step:1383/1840 train_time:72824ms step_avg:52.66ms
step:1384/1840 train_time:72911ms step_avg:52.68ms
step:1385/1840 train_time:72998ms step_avg:52.71ms
step:1386/1840 train_time:73088ms step_avg:52.73ms
step:1387/1840 train_time:73173ms step_avg:52.76ms
step:1388/1840 train_time:73261ms step_avg:52.78ms
step:1389/1840 train_time:73349ms step_avg:52.81ms
step:1390/1840 train_time:73437ms step_avg:52.83ms
step:1391/1840 train_time:73525ms step_avg:52.86ms
step:1392/1840 train_time:73613ms step_avg:52.88ms
step:1393/1840 train_time:73700ms step_avg:52.91ms
step:1394/1840 train_time:73789ms step_avg:52.93ms
step:1395/1840 train_time:73875ms step_avg:52.96ms
step:1396/1840 train_time:73965ms step_avg:52.98ms
step:1397/1840 train_time:74051ms step_avg:53.01ms
step:1398/1840 train_time:74139ms step_avg:53.03ms
step:1399/1840 train_time:74226ms step_avg:53.06ms
step:1400/1840 train_time:74314ms step_avg:53.08ms
step:1401/1840 train_time:74400ms step_avg:53.11ms
step:1402/1840 train_time:74490ms step_avg:53.13ms
step:1403/1840 train_time:74575ms step_avg:53.15ms
step:1404/1840 train_time:74665ms step_avg:53.18ms
step:1405/1840 train_time:74750ms step_avg:53.20ms
step:1406/1840 train_time:74839ms step_avg:53.23ms
step:1407/1840 train_time:74927ms step_avg:53.25ms
step:1408/1840 train_time:75016ms step_avg:53.28ms
step:1409/1840 train_time:75103ms step_avg:53.30ms
step:1410/1840 train_time:75191ms step_avg:53.33ms
step:1411/1840 train_time:75277ms step_avg:53.35ms
step:1412/1840 train_time:75368ms step_avg:53.38ms
step:1413/1840 train_time:75453ms step_avg:53.40ms
step:1414/1840 train_time:75543ms step_avg:53.42ms
step:1415/1840 train_time:75630ms step_avg:53.45ms
step:1416/1840 train_time:75717ms step_avg:53.47ms
step:1417/1840 train_time:75804ms step_avg:53.50ms
step:1418/1840 train_time:75892ms step_avg:53.52ms
step:1419/1840 train_time:75978ms step_avg:53.54ms
step:1420/1840 train_time:76070ms step_avg:53.57ms
step:1421/1840 train_time:76156ms step_avg:53.59ms
step:1422/1840 train_time:76244ms step_avg:53.62ms
step:1423/1840 train_time:76330ms step_avg:53.64ms
step:1424/1840 train_time:76418ms step_avg:53.66ms
step:1425/1840 train_time:76504ms step_avg:53.69ms
step:1426/1840 train_time:76593ms step_avg:53.71ms
step:1427/1840 train_time:76677ms step_avg:53.73ms
step:1428/1840 train_time:76768ms step_avg:53.76ms
step:1429/1840 train_time:76854ms step_avg:53.78ms
step:1430/1840 train_time:76943ms step_avg:53.81ms
step:1431/1840 train_time:77031ms step_avg:53.83ms
step:1432/1840 train_time:77120ms step_avg:53.85ms
step:1433/1840 train_time:77206ms step_avg:53.88ms
step:1434/1840 train_time:77294ms step_avg:53.90ms
step:1435/1840 train_time:77380ms step_avg:53.92ms
step:1436/1840 train_time:77470ms step_avg:53.95ms
step:1437/1840 train_time:77556ms step_avg:53.97ms
step:1438/1840 train_time:77645ms step_avg:53.99ms
step:1439/1840 train_time:77731ms step_avg:54.02ms
step:1440/1840 train_time:77820ms step_avg:54.04ms
step:1441/1840 train_time:77907ms step_avg:54.06ms
step:1442/1840 train_time:77994ms step_avg:54.09ms
step:1443/1840 train_time:78080ms step_avg:54.11ms
step:1444/1840 train_time:78170ms step_avg:54.13ms
step:1445/1840 train_time:78255ms step_avg:54.16ms
step:1446/1840 train_time:78344ms step_avg:54.18ms
step:1447/1840 train_time:78431ms step_avg:54.20ms
step:1448/1840 train_time:78518ms step_avg:54.23ms
step:1449/1840 train_time:78604ms step_avg:54.25ms
step:1450/1840 train_time:78693ms step_avg:54.27ms
step:1451/1840 train_time:78779ms step_avg:54.29ms
step:1452/1840 train_time:78870ms step_avg:54.32ms
step:1453/1840 train_time:78956ms step_avg:54.34ms
step:1454/1840 train_time:79044ms step_avg:54.36ms
step:1455/1840 train_time:79131ms step_avg:54.39ms
step:1456/1840 train_time:79219ms step_avg:54.41ms
step:1457/1840 train_time:79304ms step_avg:54.43ms
step:1458/1840 train_time:79393ms step_avg:54.45ms
step:1459/1840 train_time:79479ms step_avg:54.47ms
step:1460/1840 train_time:79569ms step_avg:54.50ms
step:1461/1840 train_time:79654ms step_avg:54.52ms
step:1462/1840 train_time:79743ms step_avg:54.54ms
step:1463/1840 train_time:79831ms step_avg:54.57ms
step:1464/1840 train_time:79919ms step_avg:54.59ms
step:1465/1840 train_time:80006ms step_avg:54.61ms
step:1466/1840 train_time:80094ms step_avg:54.63ms
step:1467/1840 train_time:80180ms step_avg:54.66ms
step:1468/1840 train_time:80270ms step_avg:54.68ms
step:1469/1840 train_time:80357ms step_avg:54.70ms
step:1470/1840 train_time:80446ms step_avg:54.73ms
step:1471/1840 train_time:80531ms step_avg:54.75ms
step:1472/1840 train_time:80620ms step_avg:54.77ms
step:1473/1840 train_time:80707ms step_avg:54.79ms
step:1474/1840 train_time:80796ms step_avg:54.81ms
step:1475/1840 train_time:80882ms step_avg:54.84ms
step:1476/1840 train_time:80971ms step_avg:54.86ms
step:1477/1840 train_time:81057ms step_avg:54.88ms
step:1478/1840 train_time:81145ms step_avg:54.90ms
step:1479/1840 train_time:81231ms step_avg:54.92ms
step:1480/1840 train_time:81321ms step_avg:54.95ms
step:1481/1840 train_time:81408ms step_avg:54.97ms
step:1482/1840 train_time:81496ms step_avg:54.99ms
step:1483/1840 train_time:81581ms step_avg:55.01ms
step:1484/1840 train_time:81672ms step_avg:55.03ms
step:1485/1840 train_time:81759ms step_avg:55.06ms
step:1486/1840 train_time:81848ms step_avg:55.08ms
step:1487/1840 train_time:81933ms step_avg:55.10ms
step:1488/1840 train_time:82023ms step_avg:55.12ms
step:1489/1840 train_time:82110ms step_avg:55.14ms
step:1490/1840 train_time:82198ms step_avg:55.17ms
step:1491/1840 train_time:82283ms step_avg:55.19ms
step:1492/1840 train_time:82372ms step_avg:55.21ms
step:1493/1840 train_time:82458ms step_avg:55.23ms
step:1494/1840 train_time:82547ms step_avg:55.25ms
step:1495/1840 train_time:82632ms step_avg:55.27ms
step:1496/1840 train_time:82720ms step_avg:55.29ms
step:1497/1840 train_time:82807ms step_avg:55.32ms
step:1498/1840 train_time:82895ms step_avg:55.34ms
step:1499/1840 train_time:82981ms step_avg:55.36ms
step:1500/1840 train_time:83071ms step_avg:55.38ms
step:1500/1840 val_loss:3.4004 train_time:83171ms step_avg:55.45ms
step:1501/1840 train_time:83191ms step_avg:55.42ms
step:1502/1840 train_time:83249ms step_avg:55.43ms
step:1503/1840 train_time:83338ms step_avg:55.45ms
step:1504/1840 train_time:83425ms step_avg:55.47ms
step:1505/1840 train_time:83510ms step_avg:55.49ms
step:1506/1840 train_time:83599ms step_avg:55.51ms
step:1507/1840 train_time:83684ms step_avg:55.53ms
step:1508/1840 train_time:83772ms step_avg:55.55ms
step:1509/1840 train_time:83857ms step_avg:55.57ms
step:1510/1840 train_time:83945ms step_avg:55.59ms
step:1511/1840 train_time:84031ms step_avg:55.61ms
step:1512/1840 train_time:84123ms step_avg:55.64ms
step:1513/1840 train_time:84213ms step_avg:55.66ms
step:1514/1840 train_time:84304ms step_avg:55.68ms
step:1515/1840 train_time:84392ms step_avg:55.70ms
step:1516/1840 train_time:84479ms step_avg:55.73ms
step:1517/1840 train_time:84565ms step_avg:55.74ms
step:1518/1840 train_time:84654ms step_avg:55.77ms
step:1519/1840 train_time:84741ms step_avg:55.79ms
step:1520/1840 train_time:84828ms step_avg:55.81ms
step:1521/1840 train_time:84914ms step_avg:55.83ms
step:1522/1840 train_time:85002ms step_avg:55.85ms
step:1523/1840 train_time:85088ms step_avg:55.87ms
step:1524/1840 train_time:85180ms step_avg:55.89ms
step:1525/1840 train_time:85268ms step_avg:55.91ms
step:1526/1840 train_time:85357ms step_avg:55.93ms
step:1527/1840 train_time:85443ms step_avg:55.95ms
step:1528/1840 train_time:85531ms step_avg:55.98ms
step:1529/1840 train_time:85616ms step_avg:56.00ms
step:1530/1840 train_time:85705ms step_avg:56.02ms
step:1531/1840 train_time:85790ms step_avg:56.03ms
step:1532/1840 train_time:85879ms step_avg:56.06ms
step:1533/1840 train_time:85964ms step_avg:56.08ms
step:1534/1840 train_time:86052ms step_avg:56.10ms
step:1535/1840 train_time:86141ms step_avg:56.12ms
step:1536/1840 train_time:86231ms step_avg:56.14ms
step:1537/1840 train_time:86317ms step_avg:56.16ms
step:1538/1840 train_time:86406ms step_avg:56.18ms
step:1539/1840 train_time:86491ms step_avg:56.20ms
step:1540/1840 train_time:86581ms step_avg:56.22ms
step:1541/1840 train_time:86668ms step_avg:56.24ms
step:1542/1840 train_time:86757ms step_avg:56.26ms
step:1543/1840 train_time:86842ms step_avg:56.28ms
step:1544/1840 train_time:86931ms step_avg:56.30ms
step:1545/1840 train_time:87017ms step_avg:56.32ms
step:1546/1840 train_time:87106ms step_avg:56.34ms
step:1547/1840 train_time:87191ms step_avg:56.36ms
step:1548/1840 train_time:87282ms step_avg:56.38ms
step:1549/1840 train_time:87368ms step_avg:56.40ms
step:1550/1840 train_time:87457ms step_avg:56.42ms
step:1551/1840 train_time:87542ms step_avg:56.44ms
step:1552/1840 train_time:87631ms step_avg:56.46ms
step:1553/1840 train_time:87717ms step_avg:56.48ms
step:1554/1840 train_time:87805ms step_avg:56.50ms
step:1555/1840 train_time:87891ms step_avg:56.52ms
step:1556/1840 train_time:87981ms step_avg:56.54ms
step:1557/1840 train_time:88067ms step_avg:56.56ms
step:1558/1840 train_time:88156ms step_avg:56.58ms
step:1559/1840 train_time:88244ms step_avg:56.60ms
step:1560/1840 train_time:88332ms step_avg:56.62ms
step:1561/1840 train_time:88418ms step_avg:56.64ms
step:1562/1840 train_time:88506ms step_avg:56.66ms
step:1563/1840 train_time:88592ms step_avg:56.68ms
step:1564/1840 train_time:88681ms step_avg:56.70ms
step:1565/1840 train_time:88767ms step_avg:56.72ms
step:1566/1840 train_time:88856ms step_avg:56.74ms
step:1567/1840 train_time:88943ms step_avg:56.76ms
step:1568/1840 train_time:89032ms step_avg:56.78ms
step:1569/1840 train_time:89118ms step_avg:56.80ms
step:1570/1840 train_time:89206ms step_avg:56.82ms
step:1571/1840 train_time:89292ms step_avg:56.84ms
step:1572/1840 train_time:89382ms step_avg:56.86ms
step:1573/1840 train_time:89468ms step_avg:56.88ms
step:1574/1840 train_time:89557ms step_avg:56.90ms
step:1575/1840 train_time:89643ms step_avg:56.92ms
step:1576/1840 train_time:89731ms step_avg:56.94ms
step:1577/1840 train_time:89817ms step_avg:56.95ms
step:1578/1840 train_time:89905ms step_avg:56.97ms
step:1579/1840 train_time:89991ms step_avg:56.99ms
step:1580/1840 train_time:90083ms step_avg:57.01ms
step:1581/1840 train_time:90168ms step_avg:57.03ms
step:1582/1840 train_time:90257ms step_avg:57.05ms
step:1583/1840 train_time:90345ms step_avg:57.07ms
step:1584/1840 train_time:90433ms step_avg:57.09ms
step:1585/1840 train_time:90519ms step_avg:57.11ms
step:1586/1840 train_time:90607ms step_avg:57.13ms
step:1587/1840 train_time:90693ms step_avg:57.15ms
step:1588/1840 train_time:90782ms step_avg:57.17ms
step:1589/1840 train_time:90867ms step_avg:57.18ms
step:1590/1840 train_time:90956ms step_avg:57.21ms
step:1591/1840 train_time:91043ms step_avg:57.22ms
step:1592/1840 train_time:91132ms step_avg:57.24ms
step:1593/1840 train_time:91219ms step_avg:57.26ms
step:1594/1840 train_time:91307ms step_avg:57.28ms
step:1595/1840 train_time:91392ms step_avg:57.30ms
step:1596/1840 train_time:91483ms step_avg:57.32ms
step:1597/1840 train_time:91568ms step_avg:57.34ms
step:1598/1840 train_time:91657ms step_avg:57.36ms
step:1599/1840 train_time:91743ms step_avg:57.38ms
step:1600/1840 train_time:91831ms step_avg:57.39ms
step:1601/1840 train_time:91917ms step_avg:57.41ms
step:1602/1840 train_time:92005ms step_avg:57.43ms
step:1603/1840 train_time:92092ms step_avg:57.45ms
step:1604/1840 train_time:92182ms step_avg:57.47ms
step:1605/1840 train_time:92267ms step_avg:57.49ms
step:1606/1840 train_time:92357ms step_avg:57.51ms
step:1607/1840 train_time:92443ms step_avg:57.53ms
step:1608/1840 train_time:92531ms step_avg:57.54ms
step:1609/1840 train_time:92616ms step_avg:57.56ms
step:1610/1840 train_time:92705ms step_avg:57.58ms
step:1611/1840 train_time:92791ms step_avg:57.60ms
step:1612/1840 train_time:92882ms step_avg:57.62ms
step:1613/1840 train_time:92967ms step_avg:57.64ms
step:1614/1840 train_time:93056ms step_avg:57.66ms
step:1615/1840 train_time:93142ms step_avg:57.67ms
step:1616/1840 train_time:93231ms step_avg:57.69ms
step:1617/1840 train_time:93318ms step_avg:57.71ms
step:1618/1840 train_time:93406ms step_avg:57.73ms
step:1619/1840 train_time:93492ms step_avg:57.75ms
step:1620/1840 train_time:93582ms step_avg:57.77ms
step:1621/1840 train_time:93667ms step_avg:57.78ms
step:1622/1840 train_time:93756ms step_avg:57.80ms
step:1623/1840 train_time:93843ms step_avg:57.82ms
step:1624/1840 train_time:93930ms step_avg:57.84ms
step:1625/1840 train_time:94017ms step_avg:57.86ms
step:1626/1840 train_time:94106ms step_avg:57.88ms
step:1627/1840 train_time:94191ms step_avg:57.89ms
step:1628/1840 train_time:94282ms step_avg:57.91ms
step:1629/1840 train_time:94367ms step_avg:57.93ms
step:1630/1840 train_time:94456ms step_avg:57.95ms
step:1631/1840 train_time:94544ms step_avg:57.97ms
step:1632/1840 train_time:94632ms step_avg:57.99ms
step:1633/1840 train_time:94718ms step_avg:58.00ms
step:1634/1840 train_time:94805ms step_avg:58.02ms
step:1635/1840 train_time:94891ms step_avg:58.04ms
step:1636/1840 train_time:94982ms step_avg:58.06ms
step:1637/1840 train_time:95067ms step_avg:58.07ms
step:1638/1840 train_time:95157ms step_avg:58.09ms
step:1639/1840 train_time:95244ms step_avg:58.11ms
step:1640/1840 train_time:95332ms step_avg:58.13ms
step:1641/1840 train_time:95417ms step_avg:58.15ms
step:1642/1840 train_time:95505ms step_avg:58.16ms
step:1643/1840 train_time:95592ms step_avg:58.18ms
step:1644/1840 train_time:95683ms step_avg:58.20ms
step:1645/1840 train_time:95769ms step_avg:58.22ms
step:1646/1840 train_time:95858ms step_avg:58.24ms
step:1647/1840 train_time:95944ms step_avg:58.25ms
step:1648/1840 train_time:96033ms step_avg:58.27ms
step:1649/1840 train_time:96119ms step_avg:58.29ms
step:1650/1840 train_time:96208ms step_avg:58.31ms
step:1651/1840 train_time:96294ms step_avg:58.32ms
step:1652/1840 train_time:96382ms step_avg:58.34ms
step:1653/1840 train_time:96468ms step_avg:58.36ms
step:1654/1840 train_time:96558ms step_avg:58.38ms
step:1655/1840 train_time:96644ms step_avg:58.40ms
step:1656/1840 train_time:96734ms step_avg:58.41ms
step:1657/1840 train_time:96820ms step_avg:58.43ms
step:1658/1840 train_time:96907ms step_avg:58.45ms
step:1659/1840 train_time:96993ms step_avg:58.47ms
step:1660/1840 train_time:97083ms step_avg:58.48ms
step:1661/1840 train_time:97169ms step_avg:58.50ms
step:1662/1840 train_time:97260ms step_avg:58.52ms
step:1663/1840 train_time:97345ms step_avg:58.54ms
step:1664/1840 train_time:97434ms step_avg:58.55ms
step:1665/1840 train_time:97520ms step_avg:58.57ms
step:1666/1840 train_time:97609ms step_avg:58.59ms
step:1667/1840 train_time:97695ms step_avg:58.61ms
step:1668/1840 train_time:97784ms step_avg:58.62ms
step:1669/1840 train_time:97869ms step_avg:58.64ms
step:1670/1840 train_time:97958ms step_avg:58.66ms
step:1671/1840 train_time:98044ms step_avg:58.67ms
step:1672/1840 train_time:98133ms step_avg:58.69ms
step:1673/1840 train_time:98219ms step_avg:58.71ms
step:1674/1840 train_time:98306ms step_avg:58.73ms
step:1675/1840 train_time:98391ms step_avg:58.74ms
step:1676/1840 train_time:98481ms step_avg:58.76ms
step:1677/1840 train_time:98567ms step_avg:58.78ms
step:1678/1840 train_time:98657ms step_avg:58.79ms
step:1679/1840 train_time:98743ms step_avg:58.81ms
step:1680/1840 train_time:98831ms step_avg:58.83ms
step:1681/1840 train_time:98917ms step_avg:58.84ms
step:1682/1840 train_time:99005ms step_avg:58.86ms
step:1683/1840 train_time:99091ms step_avg:58.88ms
step:1684/1840 train_time:99183ms step_avg:58.90ms
step:1685/1840 train_time:99268ms step_avg:58.91ms
step:1686/1840 train_time:99357ms step_avg:58.93ms
step:1687/1840 train_time:99443ms step_avg:58.95ms
step:1688/1840 train_time:99531ms step_avg:58.96ms
step:1689/1840 train_time:99618ms step_avg:58.98ms
step:1690/1840 train_time:99706ms step_avg:59.00ms
step:1691/1840 train_time:99793ms step_avg:59.01ms
step:1692/1840 train_time:99883ms step_avg:59.03ms
step:1693/1840 train_time:99968ms step_avg:59.05ms
step:1694/1840 train_time:100057ms step_avg:59.07ms
step:1695/1840 train_time:100143ms step_avg:59.08ms
step:1696/1840 train_time:100232ms step_avg:59.10ms
step:1697/1840 train_time:100318ms step_avg:59.11ms
step:1698/1840 train_time:100406ms step_avg:59.13ms
step:1699/1840 train_time:100491ms step_avg:59.15ms
step:1700/1840 train_time:100582ms step_avg:59.17ms
step:1701/1840 train_time:100667ms step_avg:59.18ms
step:1702/1840 train_time:100756ms step_avg:59.20ms
step:1703/1840 train_time:100843ms step_avg:59.21ms
step:1704/1840 train_time:100931ms step_avg:59.23ms
step:1705/1840 train_time:101017ms step_avg:59.25ms
step:1706/1840 train_time:101105ms step_avg:59.26ms
step:1707/1840 train_time:101191ms step_avg:59.28ms
step:1708/1840 train_time:101281ms step_avg:59.30ms
step:1709/1840 train_time:101367ms step_avg:59.31ms
step:1710/1840 train_time:101456ms step_avg:59.33ms
step:1711/1840 train_time:101543ms step_avg:59.35ms
step:1712/1840 train_time:101632ms step_avg:59.36ms
step:1713/1840 train_time:101718ms step_avg:59.38ms
step:1714/1840 train_time:101805ms step_avg:59.40ms
step:1715/1840 train_time:101891ms step_avg:59.41ms
step:1716/1840 train_time:101981ms step_avg:59.43ms
step:1717/1840 train_time:102066ms step_avg:59.44ms
step:1718/1840 train_time:102155ms step_avg:59.46ms
step:1719/1840 train_time:102242ms step_avg:59.48ms
step:1720/1840 train_time:102330ms step_avg:59.49ms
step:1721/1840 train_time:102416ms step_avg:59.51ms
step:1722/1840 train_time:102505ms step_avg:59.53ms
step:1723/1840 train_time:102591ms step_avg:59.54ms
step:1724/1840 train_time:102681ms step_avg:59.56ms
step:1725/1840 train_time:102768ms step_avg:59.58ms
step:1726/1840 train_time:102857ms step_avg:59.59ms
step:1727/1840 train_time:102943ms step_avg:59.61ms
step:1728/1840 train_time:103031ms step_avg:59.62ms
step:1729/1840 train_time:103117ms step_avg:59.64ms
step:1730/1840 train_time:103206ms step_avg:59.66ms
step:1731/1840 train_time:103292ms step_avg:59.67ms
step:1732/1840 train_time:103382ms step_avg:59.69ms
step:1733/1840 train_time:103467ms step_avg:59.70ms
step:1734/1840 train_time:103556ms step_avg:59.72ms
step:1735/1840 train_time:103643ms step_avg:59.74ms
step:1736/1840 train_time:103732ms step_avg:59.75ms
step:1737/1840 train_time:103818ms step_avg:59.77ms
step:1738/1840 train_time:103906ms step_avg:59.78ms
step:1739/1840 train_time:103991ms step_avg:59.80ms
step:1740/1840 train_time:104082ms step_avg:59.82ms
step:1741/1840 train_time:104167ms step_avg:59.83ms
step:1742/1840 train_time:104256ms step_avg:59.85ms
step:1743/1840 train_time:104343ms step_avg:59.86ms
step:1744/1840 train_time:104430ms step_avg:59.88ms
step:1745/1840 train_time:104517ms step_avg:59.90ms
step:1746/1840 train_time:104606ms step_avg:59.91ms
step:1747/1840 train_time:104692ms step_avg:59.93ms
step:1748/1840 train_time:104782ms step_avg:59.94ms
step:1749/1840 train_time:104867ms step_avg:59.96ms
step:1750/1840 train_time:104958ms step_avg:59.98ms
step:1750/1840 val_loss:3.3018 train_time:105058ms step_avg:60.03ms
step:1751/1840 train_time:105078ms step_avg:60.01ms
step:1752/1840 train_time:105133ms step_avg:60.01ms
step:1753/1840 train_time:105224ms step_avg:60.02ms
step:1754/1840 train_time:105317ms step_avg:60.04ms
step:1755/1840 train_time:105402ms step_avg:60.06ms
step:1756/1840 train_time:105492ms step_avg:60.08ms
step:1757/1840 train_time:105576ms step_avg:60.09ms
step:1758/1840 train_time:105664ms step_avg:60.10ms
step:1759/1840 train_time:105748ms step_avg:60.12ms
step:1760/1840 train_time:105836ms step_avg:60.13ms
step:1761/1840 train_time:105920ms step_avg:60.15ms
step:1762/1840 train_time:106010ms step_avg:60.16ms
step:1763/1840 train_time:106101ms step_avg:60.18ms
step:1764/1840 train_time:106193ms step_avg:60.20ms
step:1765/1840 train_time:106280ms step_avg:60.22ms
step:1766/1840 train_time:106370ms step_avg:60.23ms
step:1767/1840 train_time:106456ms step_avg:60.25ms
step:1768/1840 train_time:106544ms step_avg:60.26ms
step:1769/1840 train_time:106630ms step_avg:60.28ms
step:1770/1840 train_time:106717ms step_avg:60.29ms
step:1771/1840 train_time:106802ms step_avg:60.31ms
step:1772/1840 train_time:106890ms step_avg:60.32ms
step:1773/1840 train_time:106977ms step_avg:60.34ms
step:1774/1840 train_time:107066ms step_avg:60.35ms
step:1775/1840 train_time:107155ms step_avg:60.37ms
step:1776/1840 train_time:107244ms step_avg:60.38ms
step:1777/1840 train_time:107333ms step_avg:60.40ms
step:1778/1840 train_time:107421ms step_avg:60.42ms
step:1779/1840 train_time:107507ms step_avg:60.43ms
step:1780/1840 train_time:107596ms step_avg:60.45ms
step:1781/1840 train_time:107682ms step_avg:60.46ms
step:1782/1840 train_time:107769ms step_avg:60.48ms
step:1783/1840 train_time:107855ms step_avg:60.49ms
step:1784/1840 train_time:107943ms step_avg:60.51ms
step:1785/1840 train_time:108031ms step_avg:60.52ms
step:1786/1840 train_time:108121ms step_avg:60.54ms
step:1787/1840 train_time:108207ms step_avg:60.55ms
step:1788/1840 train_time:108296ms step_avg:60.57ms
step:1789/1840 train_time:108382ms step_avg:60.58ms
step:1790/1840 train_time:108470ms step_avg:60.60ms
step:1791/1840 train_time:108556ms step_avg:60.61ms
step:1792/1840 train_time:108644ms step_avg:60.63ms
step:1793/1840 train_time:108730ms step_avg:60.64ms
step:1794/1840 train_time:108818ms step_avg:60.66ms
step:1795/1840 train_time:108904ms step_avg:60.67ms
step:1796/1840 train_time:108992ms step_avg:60.69ms
step:1797/1840 train_time:109081ms step_avg:60.70ms
step:1798/1840 train_time:109170ms step_avg:60.72ms
step:1799/1840 train_time:109258ms step_avg:60.73ms
step:1800/1840 train_time:109345ms step_avg:60.75ms
step:1801/1840 train_time:109435ms step_avg:60.76ms
step:1802/1840 train_time:109523ms step_avg:60.78ms
step:1803/1840 train_time:109607ms step_avg:60.79ms
step:1804/1840 train_time:109696ms step_avg:60.81ms
step:1805/1840 train_time:109782ms step_avg:60.82ms
step:1806/1840 train_time:109871ms step_avg:60.84ms
step:1807/1840 train_time:109959ms step_avg:60.85ms
step:1808/1840 train_time:110046ms step_avg:60.87ms
step:1809/1840 train_time:110134ms step_avg:60.88ms
step:1810/1840 train_time:110223ms step_avg:60.90ms
step:1811/1840 train_time:110309ms step_avg:60.91ms
step:1812/1840 train_time:110400ms step_avg:60.93ms
step:1813/1840 train_time:110486ms step_avg:60.94ms
step:1814/1840 train_time:110574ms step_avg:60.96ms
step:1815/1840 train_time:110660ms step_avg:60.97ms
step:1816/1840 train_time:110748ms step_avg:60.98ms
step:1817/1840 train_time:110834ms step_avg:61.00ms
step:1818/1840 train_time:110923ms step_avg:61.01ms
step:1819/1840 train_time:111010ms step_avg:61.03ms
step:1820/1840 train_time:111099ms step_avg:61.04ms
step:1821/1840 train_time:111185ms step_avg:61.06ms
step:1822/1840 train_time:111275ms step_avg:61.07ms
step:1823/1840 train_time:111362ms step_avg:61.09ms
step:1824/1840 train_time:111451ms step_avg:61.10ms
step:1825/1840 train_time:111537ms step_avg:61.12ms
step:1826/1840 train_time:111626ms step_avg:61.13ms
step:1827/1840 train_time:111712ms step_avg:61.14ms
step:1828/1840 train_time:111801ms step_avg:61.16ms
step:1829/1840 train_time:111887ms step_avg:61.17ms
step:1830/1840 train_time:111977ms step_avg:61.19ms
step:1831/1840 train_time:112064ms step_avg:61.20ms
step:1832/1840 train_time:112154ms step_avg:61.22ms
step:1833/1840 train_time:112240ms step_avg:61.23ms
step:1834/1840 train_time:112328ms step_avg:61.25ms
step:1835/1840 train_time:112415ms step_avg:61.26ms
step:1836/1840 train_time:112503ms step_avg:61.28ms
step:1837/1840 train_time:112588ms step_avg:61.29ms
step:1838/1840 train_time:112678ms step_avg:61.30ms
step:1839/1840 train_time:112764ms step_avg:61.32ms
step:1840/1840 train_time:112852ms step_avg:61.33ms
step:1840/1840 val_loss:3.2767 train_time:112953ms step_avg:61.39ms
peak memory allocated: 28507 MiB reserved: 44038 MiB

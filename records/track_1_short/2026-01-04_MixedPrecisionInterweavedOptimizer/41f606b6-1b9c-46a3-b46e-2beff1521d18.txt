import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from collections import defaultdict
from itertools import accumulate
from pathlib import Path
import gc

os.environ["PYTORCH_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device=f"cuda:{os.environ['LOCAL_RANK']}", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng

@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor, split_baddbmm: bool = False):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    # Select batched vs unbatched
    if split_baddbmm:
        BX_matmul = torch.bmm if X.ndim > 2 else torch.mm
    else:
        aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A

        # Referencing X twice causes pytorch to make a defensive copy,
        # resulting in a cudaMemcpyAsync in baddbmm.
        # For large matrices (i.e., the mlp weights), it's faster to split
        # the operation into two kernels to avoid this.
        if split_baddbmm:
            BX_matmul(B, X, out=C)  # C = B @ X
            C.add_(X, alpha=a)      # C = C + a*X  (in-place, X only read)
        else:
            aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X

        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X


# -----------------------------------------------------------------------------
# Compiled helpers for NorMuon by @chrisjmccormick

@torch.compile(dynamic=False, fullgraph=True)
def cautious_wd_and_update_inplace(p, mantissa, grad, wd_tensor, lr_tensor):
    """
    Cautious weight decay + parameter update. wd_tensor and lr_tensor are 0-D CPU tensors.
    Mantissa is tracked to enable higher precision updates on bfloat16 parameters.
    bfloat16 format: 1 sign bit + 8 exponent bits + 7 mantissa bits = 16 bits total
    float32 format: 1 sign bit + 8 exponent bits + 23 mantissa bits = 32 bits total
    """
    assert p.dtype == mantissa.dtype == torch.uint16
    grad = grad.float()
    wd_factor = wd_tensor.to(torch.float32)
    lr_factor = lr_tensor.to(torch.float32)
    p_precise_raw = (p.to(torch.uint32) << 16) | mantissa.to(torch.uint32)
    p_precise = p_precise_raw.view(torch.float32)
    mask = (grad * p_precise) >= 0
    p_precise.copy_(p_precise - (p_precise * mask * wd_factor * lr_factor) - (grad * lr_factor))
    p.copy_((p_precise_raw >> 16).to(torch.uint16))
    mantissa.copy_(p_precise_raw.to(torch.uint16))

@torch.compile(dynamic=False, fullgraph=True)
def apply_normuon_variance_reduction(v_chunk, second_momentum_buffer, beta2, red_dim):
    """NorMuon variance reduction. Algebraically fuses the normalization steps to minimize memory ops."""
    v_mean = v_chunk.float().square().mean(dim=red_dim, keepdim=True)
    red_dim_size = v_chunk.size(red_dim)
    v_norm_sq = v_mean.sum(dim=(-2, -1), keepdim=True).mul_(red_dim_size)
    v_norm = v_norm_sq.sqrt_()
    second_momentum_buffer.lerp_(v_mean.to(dtype=second_momentum_buffer.dtype), 1 - beta2)
    step_size = second_momentum_buffer.clamp_min(1e-10).rsqrt_()
    scaled_sq_sum = (v_mean * red_dim_size) * step_size.float().square()
    v_norm_new = scaled_sq_sum.sum(dim=(-2, -1), keepdim=True).sqrt_()
    final_scale = step_size * (v_norm / v_norm_new.clamp_min_(1e-10))
    return v_chunk.mul_(final_scale.type_as(v_chunk))


# -----------------------------------------------------------------------------
# NorMuon optimizer

class NorMuon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).

    Differences from standard Muon:
    - Newton-Shulz is replaced with Polar Express for the orthogonalization step
    - NorMuon adds a low-rank variance estimator similar to Adafactor. https://arxiv.org/pdf/2510.05491
    - small 1D parameters handled here instead of in Adam
    - Cautious weight decay, a gated version of decoupled weight decay
    - Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as
    needed on the forward pass. This enables attn and mlp weights to be contained within the same
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn.
    The scheduling is:
        1. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        2. reduce scatter attn/mlp round 2 (16 mlp params)
        3. wait on step 1, then compute update of 1 and schedule all gather
        4. wait on step 2, then compute update of 2 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        5. wait for each all gather to complete and update params
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, beta2=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, beta2=beta2)
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def reset(self):
        # expose a reset for clearing buffers
        for group in self.param_groups:
            if "momentum_buffer" in group:
                group["momentum_buffer"].zero_()
                group["mantissa"].zero_()
                group["second_momentum_buffer"].zero_()

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per module.
        """
        groups = defaultdict(list)
        for param in params:
            groups[param.label].append(param)

        param_groups = []
        for module_name, group_params in groups.items():
            chunk_size = (len(group_params) + self.world_size - 1) // self.world_size
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))

        return param_groups

    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn
        and mlp params when a param group is split across GPUs.
        """
        params_list = list(params)
        module_group_order = ['attn', 'mlp']
        group_sizes = [16, 16]  # 10 attn + 6 mlp, then 16 mlp
        params_list.sort(key=lambda x: module_group_order.index(x.label))

        idx = 0
        assert len(params_list) == sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            chunk_size = (size + self.world_size - 1) // self.world_size
            group_params = params_list[idx: idx + size]
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))
            idx += size

        return param_groups

    def step(self):
        self.step_p1()
        self.step_p2()
        self.step_p3()
        
    @torch.no_grad()
    def step_p1(self):
        """
        Part 1: Launch distributed reduce_scatter operations for parameter groups.
        """
        rank = dist.get_rank()
        self.group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            stacked_grads = torch.empty(
                (padded_num_params, *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device
            )
            for i, p in enumerate(params):
                stacked_grads[i].copy_(p.grad, non_blocking=True)
            if len(params) < padded_num_params:
                stacked_grads[len(params):].zero_()

            grad_chunk = torch.empty_like(stacked_grads[:chunk_size])

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            self.group_infos.append(dict(grad_chunk=grad_chunk, reduce_future=reduce_future))
    
    @torch.no_grad()
    def step_p2(self):
        """
        Part 2: Compute gradient updates and launch all_gather operations
        Wait for all gather to complete for all parameter groups except final one
        """
        # Efficient distributed step by @YouJiacheng, @KonstantinWilleke, @alexrgilbert,
        # @adricarda, @tuttyfrutyee, @vdlad, @ryanyang0, @vagrawal, @varunneal, @chrisjmccormick
        rank = dist.get_rank()
        group_infos = self.group_infos
        
        self.all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = group["params"]
            grad_chunk = info["grad_chunk"].float()
            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            start_idx = rank * chunk_size
            module_idx = start_idx if start_idx < len(params) else 0

            num_params = min(chunk_size, max(0, len(params) - start_idx))  # num params for this rank

            if "momentum_buffer" not in group:
                group["momentum_buffer"]  = torch.zeros_like(grad_chunk[:num_params], dtype=torch.float32)
                
            momentum_buffer = group["momentum_buffer"]
            # Apply momentum update to the persistent momentum buffer in-place
            momentum_buffer.lerp_(grad_chunk[:num_params], 1 - group["momentum"])
            updated_grads = grad_chunk[:num_params].lerp_(momentum_buffer, group["momentum"])

            grad_shape = updated_grads.shape
            if params[module_idx].label == 'attn':
                for p in params[module_idx:module_idx + num_params]:
                    assert p.label == 'attn'
                updated_grads = updated_grads.view(4 * grad_shape[0], grad_shape[1] // 4, grad_shape[2])

            ref_param = params[module_idx]
            param_shape = ref_param.shape

            if "second_momentum_buffer" not in group:
                group["second_momentum_buffer"] = (torch.zeros_like(updated_grads[..., :, :1], dtype=torch.float32)
                    if param_shape[-2] >= param_shape[-1] else torch.zeros_like(updated_grads[..., :1, :])
                )
            second_momentum_buffer = group["second_momentum_buffer"]

            if "param_lr_cpu" not in group:
                # Define multipliers for ALL params in this group (global, not per-shard)
                lr_mults = []
                wd_mults = []
                for p in params:
                    # Increase learning rate for modules with larger inputs than outputs.
                    # This shape check also assumes rows=input, columns=output, so take care
                    # when changing memory layouts. @chrisjmccormick
                    shape = p.shape
                    if len(shape) >= 2:
                        shape_mult = max(1.0, shape[-2] / shape[-1]) ** 0.5
                    else:
                        shape_mult = 1.0
                    lr_mults.append(shape_mult * getattr(p, "lr_mul", 1.0))
                    wd_mults.append(getattr(p, "wd_mul", 1.0))
                # Define as cpu tensors to enable Inductor constant folding
                group["param_lr_cpu"] = torch.tensor(lr_mults, dtype=torch.float32, device="cpu")
                group["param_wd_cpu"] = torch.tensor(wd_mults, dtype=torch.float32, device="cpu")

            eff_lr_all = group["param_lr_cpu"] * group["lr"]
            eff_wd_all = group["param_wd_cpu"] * group["weight_decay"] * group["lr"]

            # Slice the portion corresponding to this rank's shard
            eff_lr_cpu = eff_lr_all[module_idx:module_idx + num_params]
            eff_wd_cpu = eff_wd_all[module_idx:module_idx + num_params]

            # Compute zeropower for the entire chunk in a single, batched call.
            if num_params == 0:
                v_chunk = updated_grads
            else:
                v_chunk = polar_express(updated_grads, split_baddbmm=(ref_param.label == 'mlp'))

            # Note that the head orientation in O is transposed relative to QKV, so red_dim
            # is 'incorrect' for O. However, correcting this showed no improvement. @chrisjmccormick
            red_dim = -1 if param_shape[-2] >= param_shape[-1] else -2

            v_chunk = apply_normuon_variance_reduction(
                v_chunk, second_momentum_buffer, group["beta2"], red_dim
            )

            v_chunk = v_chunk.view(grad_shape)

            # # "Cautious" weight decay (https://arxiv.org/abs/2510.12402)
            updated_params = torch.empty_like(grad_chunk, dtype=torch.bfloat16)
            if num_params > 0:
                # Work on a stacked copy to avoid touching original params
                param_chunk = torch.stack(params[module_idx:module_idx + num_params])

                if "mantissa" not in group:
                    group["mantissa"] = torch.zeros_like(param_chunk, dtype=torch.uint16)
                mantissa = group["mantissa"]

                for local_idx in range(num_params):
                    cautious_wd_and_update_inplace(
                        param_chunk[local_idx].view(torch.uint16),
                        mantissa[local_idx],
                        v_chunk[local_idx],
                        eff_wd_cpu[local_idx],
                        eff_lr_cpu[local_idx]
                    )
            else:
                param_chunk = torch.zeros_like(v_chunk)

            updated_params[:num_params].copy_(param_chunk)
            if num_params < chunk_size:
                updated_params[num_params:].zero_()

            stacked_params = torch.empty(
                (padded_num_params, *param_shape),
                dtype=updated_params.dtype,
                device=updated_params.device,
            )

            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_params, async_op=True
            ).get_future()

            self.all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete for all except final and copy results back into original parameter tensors.
        for info in self.all_gather_infos[:-1]:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)

    @torch.no_grad()
    def step_p3(self):
        """
        Part 3: Wait for final all gather to complete and copy results back into original parameter tensors
        """
        info = self.all_gather_infos[-1]
        info["gather_future"].wait()
        stacked_params = info["stacked_params"]
        orig_params = info["orig_params"]

        unstacked_params = torch.unbind(stacked_params)
        for i, p in enumerate(orig_params):
            p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, label_order: list[str], betas: list[list[float]], lr: float = 1e-3, eps: float = 1e-8, weight_decay: float = 0.01):
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        defaults = dict(lr=lr, eps=eps, weight_decay=weight_decay)
        params = list(params)
        # Group by label, with explicit ordering for execution control.
        params_by_label = defaultdict(list)
        for p in params:
            params_by_label[getattr(p, 'label', None)].append(p)
        param_groups = []
        for idx, label in enumerate(label_order):
            if label in params_by_label:
                param_groups.append(dict(params=params_by_label[label], betas=betas[idx]))
        # include any unlabeled params at the end (processed last)
        if None in params_by_label:
            param_groups.append(dict(params=params_by_label[None]))
        super().__init__(param_groups, defaults)
        # init state: small params (numel < 1024) use full-sized state, others use sharded
        for p in params:
            chunk = p if p.numel() < 1024 else p[:p.size(0) // self.world_size]
            exp_avg = torch.zeros_like(chunk, dtype=torch.float32, device=p.device)
            self.state[p] = dict(step=0, exp_avg=exp_avg, exp_avg_sq=torch.zeros_like(exp_avg))

        # tag the final param for optimizer pipelining, run all gather after muon copy
        param_groups[-1]['params'][-1].is_final_param = True
        
        # DistributedAdam implementation by @vagrawal, @akash5474
        self.should_sync = False
        self._reduce_scatter_hooks = []
        self._reduce_scatter_futures = {}
        # 0-D CPU tensors to avoid recompilation in _update_step
        self._step_size_t = torch.tensor(0.0, dtype=torch.float32, device="cpu")
        self._eff_wd_t = torch.tensor(0.0, dtype=torch.float32, device="cpu")
        self.register_backward_hooks()

    def register_backward_hooks(self):
        for group in self.param_groups:
            for param in group["params"]:
                self._reduce_scatter_hooks.append(param.register_post_accumulate_grad_hook(self._sync_gradient))

    def load_state_dict(self, state_dict):
        """Override to preserve optimizer state dtypes (avoid BFloat16->Float32 cast that causes recompilation)."""
        # Save original state dtypes before loading
        original_dtypes = {}
        for p, s in self.state.items():
            original_dtypes[p] = {k: v.dtype for k, v in s.items() if isinstance(v, torch.Tensor)}
        
        # Call parent load_state_dict (which may cast dtypes to match param dtype)
        super().load_state_dict(state_dict)
        
        # Restore original dtypes
        for p, s in self.state.items():
            if p in original_dtypes:
                for k, v in s.items():
                    if isinstance(v, torch.Tensor) and k in original_dtypes[p]:
                        if v.dtype != original_dtypes[p][k]:
                            s[k] = v.to(original_dtypes[p][k])

    @torch.no_grad()
    def _sync_gradient(self, param):
        if not self.should_sync:
            return

        grad = param.grad
        if param.numel() < 1024:
            # Small params: use all_reduce (no scatter/gather needed)
            self._reduce_scatter_futures[param] = (
                dist.all_reduce(grad, op=dist.ReduceOp.AVG, async_op=True).get_future(),
                grad
            )
        else:
            rank_size = grad.shape[0] // self.world_size
            if grad is not None:
                grad_slice = torch.empty_like(grad[:rank_size])
                self._reduce_scatter_futures[param] = (
                    dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future(),
                    grad_slice
                )

    def copy_lm_to_embed(self):
        # run at 2/3 of training
        lm_head = self.param_groups[0]['params'][0]
        embed = self.param_groups[-2]['params'][0]
        lm_head_state = self.state[lm_head]
        embed_state = self.state[embed]
        embed_state['step'] = lm_head_state['step']
        embed_state['exp_avg'] = lm_head_state['exp_avg'].clone()
        embed_state['exp_avg_sq'] = lm_head_state['exp_avg_sq'].clone()
        embed.data.copy_(lm_head.data)

    @staticmethod
    @torch.compile(dynamic=False, fullgraph=True)
    def _update_step(p_slice, g_slice, exp_avg, exp_avg_sq, beta1, beta2, eps, step_size_t, eff_wd_t):
        """Compiled Adam update step. step_size_t and eff_wd_t are 0-D CPU tensors to avoid recompilation."""
        exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)  # exp_avg = beta1 * exp_avg + (1 - beta1) * g_slice
        exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)  # exp_avg_sq = beta2 * exp_avg_sq + (1 - beta2) * g_slice^2
        # compute step
        update = exp_avg.div(exp_avg_sq.sqrt().add_(eps)).mul_(step_size_t)  # update = (exp_avg / (sqrt(exp_avg_sq) + eps)) * step_size
        # cautious weight decay
        mask = (update * p_slice) > 0
        update.addcmul_(p_slice, mask, value=eff_wd_t)  # update += eff_wd_t * p_slice * mask
        p_slice.add_(other=update, alpha=-1.0)  # p_slice -= update

    @torch.no_grad()
    def step(self, muon_opt):
        muon_opt.step_p1()
        rank = dist.get_rank()
        all_gather_futures: list[torch.Future] = []

        last_param = None
        last_p_slice = None
        for group in self.param_groups:      
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            for param in group['params']:
                if param not in self._reduce_scatter_futures:
                    continue

                fut, g_slice = self._reduce_scatter_futures[param]
                fut.wait()

                is_small = param.numel() < 1024
                if is_small:
                    # Small params: g_slice is actually full grad, p_slice is full param
                    p_slice = param
                else:
                    rank_size = param.shape[0] // self.world_size
                    p_slice = param[rank * rank_size:(rank + 1) * rank_size]

                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]
                state["step"] += 1
                t = state["step"]
                
                # Pre-compute changing values as 0-D CPU tensors to avoid recompilation.
                # `.fill_(value)` is the same as "= value", but doesn't change the tensor object.
                bias1, bias2 = 1 - beta1 ** t, 1 - beta2 ** t
                self._step_size_t.fill_(lr * (bias2 ** 0.5 / bias1))
                self._eff_wd_t.fill_(lr * lr * wd * getattr(param, "wd_mul", 1.0)) # `lr` included twice to serve as weight decay schedule.

                DistAdam._update_step(p_slice, g_slice, state["exp_avg"], state["exp_avg_sq"],
                                      beta1, beta2, eps, self._step_size_t, self._eff_wd_t)

                if not is_small:
                    if getattr(param, "is_final_param", False):
                        last_param = param
                        last_p_slice = p_slice
                    else:
                        all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())
        self._reduce_scatter_futures.clear()

        muon_opt.step_p2()
        torch.futures.collect_all(all_gather_futures).wait()

        if last_param is not None:
            last_all_gather_future = dist.all_gather_into_tensor(last_param, last_p_slice, async_op=True).get_future()
        muon_opt.step_p3()
        torch.futures.collect_all([last_all_gather_future]).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        with torch.no_grad():
            self.weight.zero_()  # @Grad62304977 and others

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))


# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float
    key_offset: bool
    attn_gate_w: torch.Tensor
    ve_gate_w: torch.Tensor

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = self.dim ** -0.5
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKVO weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # Simplified layout by @chrisjmccormick
        self.qkvo_w = nn.Parameter(torch.empty(self.dim * 4, self.hdim, dtype=torch.bfloat16))
        # label all modules for explicit optimizer grouping
        self.qkvo_w.label = 'attn'

        with torch.no_grad():
            self.qkvo_w[:self.dim * 3].uniform_(-bound, bound)  # init QKV weights
            self.qkvo_w[self.dim * 3:].zero_()  # init O weights to zero

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas, key_offset = attn_args.ve, attn_args.sa_lambdas, attn_args.key_offset
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size
        # sparse gated attention to enable context based no-op by @classiclarryd
        # only include gates on layers with value embeds used on forward pass
        attn_gate_w, ve_gate_w = attn_args.attn_gate_w, attn_args.ve_gate_w

        q, k, v = F.linear(x, sa_lambdas[0] * self.qkvo_w[:self.dim * 3].type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if key_offset:
            # shift keys forward for the stationary head dims. Enables 1-layer induction.
            k[:, 1:, :, self.head_dim // 4:self.head_dim // 2] = k[:, :-1, :, self.head_dim // 4:self.head_dim // 2]
            k[:, 1:, :, 3 * self.head_dim // 4:] = k[:, :-1, :, 3 * self.head_dim // 4:]
        if ve is not None:
            ve_gate_out = 2 * torch.sigmoid(F.linear(x[..., :12], ve_gate_w)).view(B, T, self.num_heads, 1)
            v = v + ve_gate_out * ve.view_as(v) # @ KoszarskyB & @Grad62304977

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens,
                                                        max_seqlen_q=max_len, max_seqlen_k=max_len,
                                                        causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(F.linear(x[..., :12], attn_gate_w)).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, sa_lambdas[1] * self.qkvo_w[self.dim * 3:].type_as(y))  # sa_lambdas[1] pre-multiplied to O @shenberg
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # Transposed layout to match attention weights
        self.c_fc = nn.Parameter(torch.empty(hdim, dim, dtype=torch.bfloat16))
        self.c_proj = nn.Parameter(torch.empty(hdim, dim, dtype=torch.bfloat16))
        # label all modules for explicit optimizer grouping
        self.c_fc.label = 'mlp'
        self.c_proj.label = 'mlp'
        self.c_proj.lr_mul = 2.

        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.T.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.6 (the 7th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads, layer_idx) if layer_idx != 6 else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim)

    def forward(self, x: Tensor, attn_args: AttnArgs):
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

@dataclass
class ForwardScheduleConfig:
    mtp_weights: torch.Tensor
    ws_short: int
    ws_long: int

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        self.num_layers = num_layers
        vocab_size = next_multiple_of_n(vocab_size, n=128)

        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.label = 'smear_gate'
        self.smear_gate.weight.lr_mul = 0.01
        self.smear_gate.weight.wd_mul = 0.0

        self.skip_gate = CastedLinear(12, 1)
        self.skip_gate.weight.label = 'skip_gate'
        self.skip_gate.weight.lr_mul = 0.05
        self.skip_gate.weight.wd_mul = 0.0

        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        for embed in self.value_embeds:
            nn.init.zeros_(embed.weight)
        for ve in self.value_embeds:
            ve.weight.label = 'value_embed'
        
        # parameter banks for attention and value embedding gate weights
        self.attn_gate_bank = nn.Parameter(torch.zeros(10, num_heads, 12)) # 10 layers
        self.attn_gate_bank.label = 'attn_gate_bank'
        self.ve_gate_bank = nn.Parameter(torch.zeros(5, num_heads, 12)) # 5 layers
        self.ve_gate_bank.label = 've_gate_bank'

        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)

        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=100/448, w_s=1.6/448, grad_s=0.75/448)
        nn.init.normal_(self.lm_head.weight, mean=0, std=0.005)
        self.lm_head.weight.label = 'lm_head'

        self.embed = nn.Embedding(vocab_size, model_dim)
        self.embed.weight.label = 'embed'

        # x0_lambdas separated out for different optimizer treatment (no beta smoothing)
        self.x0_lambdas = nn.Parameter(torch.zeros(num_layers))
        self.x0_lambdas.label = 'x0_lambdas'
        self.x0_lambdas.lr_mul = 5.0
        self.x0_lambdas.wd_mul = 0.0

        pad = (-num_layers * 3 - 3) % dist.get_world_size()  # updated: 3*num_layers instead of 4*
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    1.1 * torch.ones(num_layers),  # resid lambdas. 1.1 init such that layer i weight is i^(num_layers-i).
                    *[torch.tensor([0.5, 1.0]) for _ in range(num_layers)],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    -1.5 * torch.ones(1),  # skip_lambda -> σ(-1.5) ≈ 0.18
                    torch.ones(pad),
                ]
            )
        )

        self.scalars.label = 'scalars'
        # set learning rates
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
            param.wd_mul = 5.
        for param in self.embed.parameters():
            param.wd_mul = 150.
        for param in self.lm_head.parameters():
            param.wd_mul = 150.
        self.scalars.lr_mul = 5.0
        self.scalars.wd_mul = 0.0

        self.split_embed = False

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, schedule_cfg: ForwardScheduleConfig):
        assert input_seq.ndim == 1

        # unpack schedule_cfg
        mtp_weights, ws_short, ws_long = schedule_cfg.mtp_weights, schedule_cfg.ws_short, schedule_cfg.ws_long

        # set configs
        skip_connections = []
        skip_in = [3] # long attention window on layer 3
        skip_out = [6] # no attn op on layer 6
        x_backout = None
        backout_layer = 7

        # set lambdas
        resid_lambdas = self.scalars[: 1 * self.num_layers]
        x0_lambdas = self.x0_lambdas
        sa_lambdas = self.scalars[1 * self.num_layers: 3 * self.num_layers].view(-1, 2)
        smear_lambda = self.scalars[3 * self.num_layers]
        backout_lambda = self.scalars[3 * self.num_layers+1]
        skip_lambda = self.scalars[3 * self.num_layers+2]

        # set block masks and key shift
        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == self.num_layers
        key_offset = [b==long_bm for b in bm_sizes] # apply partial key offset to long windows

        # weight-tied: use lm_head.weight for embedding lookup (or separate embed after split)
        if self.split_embed:
            x = self.embed(input_seq)
        else:
            x = F.embedding(input_seq, self.lm_head.weight)
        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [ve[1], ve[2]] + [None] * (self.num_layers - 5) + [ve[0], ve[1], ve[2]]
        assert len(ve) == self.num_layers

        # smear token embed forward 1 position @classiclarryd
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # unbind gate banks to avoid select_backwards kernel
        ag = [w.bfloat16() for w in self.attn_gate_bank.unbind(0)] 
        veg = [w.bfloat16() for w in self.ve_gate_bank.unbind(0)]
        attn_gates = ag[:6] + [None] + ag[6:]
        ve_gates = [veg[0], veg[1]] + [None] * (self.num_layers - 5) + [veg[2], veg[3], veg[4]]
        assert len(attn_gates) == self.num_layers
        assert len(ve_gates) == self.num_layers

        for i in range(self.num_layers):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale,
                key_offset=key_offset[i],
                attn_gate_w=attn_gates[i],
                ve_gate_w=ve_gates[i]
            )
            if i in skip_out:
                skip_gate_out = torch.sigmoid(skip_lambda) * 2 * torch.sigmoid(self.skip_gate(x0[..., :self.skip_gate.weight.size(-1)]))
                x = x + skip_gate_out * skip_connections.pop()
            if i == 0:
                x = (resid_lambdas[0] + x0_lambdas[0]) * x
            else:
                x = resid_lambdas[i] * x + x0_lambdas[i] * x0
            x = self.blocks[i](x, attn_args)
            if i in skip_in:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 7 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15
        # @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1). @classiclarryd updated to 23*sigmoid((logits+5)/7.5)
        logits = 23 * torch.sigmoid((logits + 5) / 7.5)
        logits_for_loss = logits.float() if not self.training else logits

        n_predict = mtp_weights.size(0) if mtp_weights is not None else 1
        if self.training and n_predict > 1:
            # Multi-token prediction: take loss of the weighted average of next n_predict tokens
            logits_flat = logits_for_loss.view(-1, logits_for_loss.size(-1))
            idx = F.pad(target_seq, (0, n_predict - 1)).unfold(0, n_predict, 1)  # [T, n_predict] of shifted targets
            target_logits = logits_flat.gather(1, idx)
            cross_entropy = torch.logsumexp(logits_flat, dim=-1).unsqueeze(1) - target_logits
            for k in range(1, n_predict):  # zero out preds past end of sequence
                cross_entropy[-k:, k] = 0
            loss = (cross_entropy * mtp_weights).sum()
        elif self.training:
            loss = F.cross_entropy(logits_for_loss.view(-1, logits_for_loss.size(-1)), target_seq, reduction="sum")
        else:
            loss = F.cross_entropy(logits_for_loss.view(-1, logits_for_loss.size(-1)), target_seq, reduction="mean")
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()

    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        # Cast to int32 on CPU before transfer to avoid dtype conversion during .to()
        _inputs = _inputs.to(dtype=torch.int32)
        _targets = _targets.to(dtype=torch.int64)
        _cum_lengths = _cum_lengths.to(dtype=torch.int32)

        new_params = yield (
            _inputs.to(device="cuda", non_blocking=True),
            _targets.to(device="cuda", non_blocking=True),
            _cum_lengths.to(device="cuda", non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * new_grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens // new_grad_accum_steps
            max_seq_len = new_max_seq_len

# -----------------------------------------------------------------------------
# Training Management
 
def get_bs(step: int):
    if step >= args.num_scheduled_iterations:
        return args.train_bs_extension
    x = step / args.num_scheduled_iterations
    bs_idx = int(len(args.train_bs_schedule) * x)
    return args.train_bs_schedule[bs_idx]

def get_ws(step: int):
    # set short window size to half of long window size
    # Higher ws on "extension" steps
    if step >= args.num_scheduled_iterations:
        return args.ws_final // 2, args.ws_final
    x = step / args.num_scheduled_iterations
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

# learning rate schedule: tied to batch size schedule, with cooldown at the end.
def get_lr(step: int):
    if step > args.num_scheduled_iterations:
        return 0.1
    lr_max = 1.0
    x = step / args.num_scheduled_iterations
    if x > 1/3:
       lr_max = 1.52  # (16/8)**0.6
    if x > 2/3:
        lr_max = 1.73  # (24/8)**0.5
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = lr_max * w + (1 - w) * 0.1
        return lr
    return lr_max

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

class TrainingManager():
    """
    Manages three optimizers for Adam embed/lm_head, Adam scalars, and Muon weight matrices.
    Notable Features:
        1. Scalars are given higher momentum terms to smooth learning @ChrisJMcCormick
        2. Scalar weights are temporarily frozen during batch size or window size updates @ChrisJMcCormick
        3. Adam optimizers are only stepped on odd steps @classiclarryd
        4. Adam optimizers have hooks to start gradient communication during backwards pass @akash5474
        5. Muon has a linear momentum warmup and cooldown schedule
        6. Learning rates follow a linear decay schedule
        7. Embed/lm_head weights and optimizer state splits at 2/3 of training @classiclarryd

    Manages model architecture, data, and target that changes during training
    Notable Features:
        1. Multi Token Prediction schedule of [1, 0.5, 0.25->0] -> [1, 0.5->0] -> [1] @varunneal
        2. Sliding Attention window schedule of [1,3] -> [3,7] -> [5,11] -> [6,13]
        3. YaRN updates to RoPE on window changes
        4. Split embed and lm head at 2/3 of training
        5. Batch size schedule of 8 -> 16 -> 24
        6. Post training extension of long windows from 13 to 20
    """
    def __init__(self, model):
        self.mtp_weights_schedule = self._build_mtp_schedule()
        self.model = model
        adam_betas = {
            'lm_head': [0.5, 0.95],
            'smear_gate': [0.9, 0.99],
            'attn_gate_bank': [0.9, 0.99],
            've_gate_bank': [0.9, 0.99],
            'skip_gate': [0.9, 0.99],
            'x0_lambdas': [0.65, 0.95],
            'scalars': [0.9, 0.99],
            'embed': [0.5, 0.95],
            'value_embed': [0.75, 0.95]
        }
        adam_labels = list(adam_betas.keys())
        adam_beta_values = list(adam_betas.values())
        muon_labels = ['attn', 'mlp']
        adam_params = [p for p in model.parameters() if getattr(p, 'label', None) in adam_labels]
        muon_params = [p for p in model.parameters() if getattr(p, 'label', None) in muon_labels]
        assert set(getattr(p, 'label', None) for p in model.parameters()) == set(adam_labels + muon_labels), "All params must have label"

        self.adam_opt = DistAdam(adam_params, adam_labels, adam_beta_values, lr=0.008, eps=1e-10, weight_decay=0.005)
        self.muon_opt = NorMuon(muon_params, lr=0.023, momentum=0.95, beta2=0.95, weight_decay=1.2)
        self.optimizers = [self.adam_opt, self.muon_opt]

        # split after odd number step
        self.split_step = math.ceil(args.split_embed_frac * args.num_scheduled_iterations) | 1

        # set defaults
        for opt in self.optimizers:
            opt.odd_step_only = False 
            opt.should_sync = True
            for group in opt.param_groups:
                group["initial_lr"] = group["lr"]

        # on even steps, only step Muon params
        self.adam_opt.odd_step_only = True

        self.reset()

    def _build_mtp_schedule(self):
        # Precompute MTP weights for all steps to avoid tensor allocation during training
        # Schedule: [1, 0.5, 0.25->0] -> [1, 0.5->0] -> [1]
        mtp_weights_schedule = []
        for s in range(args.num_iterations + 1):
            x = s / args.num_scheduled_iterations
            if x < 1/3:
                w = [1.0, 0.5, 0.25 * (1 - 3*x)]
            elif x < 2/3:
                w = [1.0, 0.5 * (1 - (3*x - 1))]
            else:
                w = [1.0]
            mtp_weights_schedule.append(torch.tensor(w, device=device))
        return mtp_weights_schedule

    def apply_final_ws_ext(self):
        self.ws_long = args.ws_validate_post_yarn_ext

    def get_forward_args(self):
        return ForwardScheduleConfig(
            mtp_weights = self.mtp_weights,
            ws_short = self.ws_short,
            ws_long = self.ws_long
        )
    
    def _is_active_step(self, opt, step: int):
        return (opt.odd_step_only and step%2==1) or not opt.odd_step_only

    def get_transition_steps(self):
        transition_steps = []
        ws_short, ws_long = get_ws(0)
        for step in range(1, args.num_iterations):
            ws_short, new_ws_long = get_ws(step)
            if new_ws_long != ws_long:
                transition_steps.append(step)
                ws_long = new_ws_long
        return transition_steps

    def advance_schedule(self, step: int):
        self.ws_short, new_ws_long = get_ws(step)
        if new_ws_long != self.ws_long:
            self.model.yarn.apply(self.ws_long, new_ws_long)

        new_batch_size = get_bs(step)
        if new_batch_size != self.batch_size:
            self.train_loader_send_args = (new_batch_size, args.train_max_seq_len, grad_accum_steps)
        else:
            self.train_loader_send_args = None

        self.ws_long = new_ws_long
        self.mtp_weights = self.mtp_weights_schedule[step]
    
    def step_optimizers(self, step: int):                
        step_lr = get_lr(step)
        muon_momentum = get_muon_momentum(step)
        for group in self.muon_opt.param_groups:
            group["momentum"] = muon_momentum

        for opt in self.optimizers:
            for group in opt.param_groups:
                group["lr"] = group["initial_lr"] * step_lr
                
        if self._is_active_step(self.adam_opt, step):
            # adam will interleave calls to muon step
            self.adam_opt.step(self.muon_opt)
            self.model.zero_grad(set_to_none=True)
            self.adam_opt.should_sync = False
        else:
            self.muon_opt.step()
            self.muon_opt.zero_grad(set_to_none=True)
            
        if step == self.split_step:
            self.adam_opt.copy_lm_to_embed()
            self.model.split_embed = True
    
    def activate_hooks(self, step: int):
        for opt in self.optimizers:
            if self._is_active_step(opt, step):
                opt.should_sync = True

    def reset(self, state=None):
        if state is not None:
            for opt, opt_state in zip(self.optimizers, state):
                opt.should_sync = False
                opt.load_state_dict(opt_state)

        # muon momentum buffers not in state dict
        self.muon_opt.reset()
        self.model.split_embed = False

        self.ws_short, self.ws_long = get_ws(0)
        self.batch_size = get_bs(0)
        self.model.yarn.reset()

    def get_state(self):
        return [copy.deepcopy(opt.state_dict()) for opt in self.optimizers]

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # batch sizes
    train_bs_schedule: tuple = (8 * 2048 * 8, 16 * 2048 * 8, 24 * 2048 * 8)
    train_bs_extension: int = 24 * 2048 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 1800  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: float = 0.50  # fraction of num_scheduled_iterations spent cooling down the learning rate
    split_embed_frac: float = 2/3  # fraction of training when embeddings split from lm_head
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_final: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=11,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=args.val_batch_size // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.weight.data = m.weight.data.bfloat16()
model.attn_gate_bank.data = model.attn_gate_bank.data.bfloat16()
model.ve_gate_bank.data = model.ve_gate_bank.data.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)
training_manager = TrainingManager(model)

########################################
#            Warmup kernels            #
########################################
print0("Compiling model and warming up kernels (~7 minutes on first execution)", console=True)
# Warmup the training kernels, then re-initialize the state so we aren't cheating
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=training_manager.get_state()) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_bs_schedule[0], args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)

transition_steps = training_manager.get_transition_steps()
# first few steps plus transitions
warmup_steps = sorted({0, 1, 2} | set(s + offset for s in transition_steps for offset in [-1, 0, 1] if s + offset >= 0)) 
print0(f"Sampling steps {warmup_steps} for warmup", console=True)
for step in warmup_steps:
    training_manager.advance_schedule(step)
    model.eval()
    with torch.no_grad():
        inputs, targets, cum_seqlens = next(val_loader)
        model(inputs, targets, cum_seqlens, training_manager.get_forward_args())
    model.train()
    for idx in range(grad_accum_steps):
        # enable gradient sync for the DistAdam optimizers on the last iteration before we step them
        if idx == grad_accum_steps - 1:
            training_manager.activate_hooks(step)
        send_args = training_manager.train_loader_send_args
        inputs, targets, cum_seqlens = train_loader.send(send_args)
        (model(inputs, targets, cum_seqlens, training_manager.get_forward_args()) / grad_accum_steps).backward()
    training_manager.step_optimizers(step)
print0("Resetting Model", console=True)
model.zero_grad(set_to_none=True)
model.load_state_dict(initial_state["model"])
training_manager.reset(initial_state["optimizers"])
del val_loader, train_loader, initial_state
model.train()

########################################
#        Training and validation       #
########################################
train_loader = distributed_data_generator(args.train_files, args.train_bs_schedule[0], args.train_max_seq_len, grad_accum_steps=grad_accum_steps)

gc.collect()

training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    training_manager.advance_schedule(step)
    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            training_manager.apply_final_ws_ext()
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, training_manager.get_forward_args())
        val_loss /= val_steps
        del val_loader
        dist.reduce(val_loss, 0, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for idx in range(grad_accum_steps):
        # enable gradient sync for the DistAdam optimizers on the last iteration before we step them
        if idx == grad_accum_steps - 1:
            training_manager.activate_hooks(step)
        send_args = training_manager.train_loader_send_args
        inputs, targets, cum_seqlens = train_loader.send(send_args)
        (model(inputs, targets, cum_seqlens, training_manager.get_forward_args()) / grad_accum_steps).backward()
    training_manager.step_optimizers(step)

    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.10.12 (main, Nov  4 2025, 08:48:33) [GCC 11.4.0]
Running PyTorch 2.10.0.dev20251210+cu126 compiled for CUDA 12.6
Running Triton version 3.6.0
Mon Jan  5 08:13:53 2026       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:19:00.0 Off |                    0 |
| N/A   42C    P0            127W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:3B:00.0 Off |                    0 |
| N/A   34C    P0            130W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:4C:00.0 Off |                    0 |
| N/A   33C    P0            127W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:5D:00.0 Off |                    0 |
| N/A   41C    P0            130W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:9B:00.0 Off |                    0 |
| N/A   43C    P0            132W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:BB:00.0 Off |                    0 |
| N/A   35C    P0            125W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:CB:00.0 Off |                    0 |
| N/A   41C    P0            127W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:DB:00.0 Off |                    0 |
| N/A   31C    P0            125W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A     60352      C   /usr/bin/python                                 0MiB |
|    1   N/A  N/A     60353      C   /usr/bin/python                                 0MiB |
|    2   N/A  N/A     60354      C   /usr/bin/python                                 0MiB |
|    3   N/A  N/A     60355      C   /usr/bin/python                                 0MiB |
|    4   N/A  N/A     60356      C   /usr/bin/python                                 0MiB |
|    5   N/A  N/A     60357      C   /usr/bin/python                                 0MiB |
|    6   N/A  N/A     60358      C   /usr/bin/python                                 0MiB |
|    7   N/A  N/A     60359      C   /usr/bin/python                                 0MiB |
+-----------------------------------------------------------------------------------------+

====================================================================================================
Compiling model and warming up kernels (~7 minutes on first execution)
Sampling steps [0, 1, 2, 599, 600, 601, 1199, 1200, 1201, 1799, 1800, 1801] for warmup
Resetting Model
step:0/1840 val_loss:10.8289 train_time:0ms step_avg:0.04ms
step:1/1840 train_time:62ms step_avg:61.72ms
step:2/1840 train_time:84ms step_avg:41.99ms
step:3/1840 train_time:104ms step_avg:34.56ms
step:4/1840 train_time:138ms step_avg:34.40ms
step:5/1840 train_time:169ms step_avg:33.89ms
step:6/1840 train_time:246ms step_avg:41.08ms
step:7/1840 train_time:264ms step_avg:37.64ms
step:8/1840 train_time:298ms step_avg:37.23ms
step:9/1840 train_time:330ms step_avg:36.64ms
step:10/1840 train_time:364ms step_avg:36.36ms
step:11/1840 train_time:396ms step_avg:35.98ms
step:12/1840 train_time:430ms step_avg:35.83ms
step:13/1840 train_time:462ms step_avg:35.53ms
step:14/1840 train_time:496ms step_avg:35.42ms
step:15/1840 train_time:528ms step_avg:35.20ms
step:16/1840 train_time:562ms step_avg:35.13ms
step:17/1840 train_time:594ms step_avg:34.95ms
step:18/1840 train_time:628ms step_avg:34.91ms
step:19/1840 train_time:660ms step_avg:34.75ms
step:20/1840 train_time:694ms step_avg:34.72ms
step:21/1840 train_time:727ms step_avg:34.60ms
step:22/1840 train_time:761ms step_avg:34.57ms
step:23/1840 train_time:793ms step_avg:34.46ms
step:24/1840 train_time:827ms step_avg:34.44ms
step:25/1840 train_time:859ms step_avg:34.34ms
step:26/1840 train_time:893ms step_avg:34.34ms
step:27/1840 train_time:925ms step_avg:34.26ms
step:28/1840 train_time:959ms step_avg:34.25ms
step:29/1840 train_time:991ms step_avg:34.17ms
step:30/1840 train_time:1025ms step_avg:34.17ms
step:31/1840 train_time:1057ms step_avg:34.10ms
step:32/1840 train_time:1091ms step_avg:34.11ms
step:33/1840 train_time:1123ms step_avg:34.04ms
step:34/1840 train_time:1158ms step_avg:34.06ms
step:35/1840 train_time:1191ms step_avg:34.03ms
step:36/1840 train_time:1226ms step_avg:34.06ms
step:37/1840 train_time:1259ms step_avg:34.02ms
step:38/1840 train_time:1293ms step_avg:34.03ms
step:39/1840 train_time:1325ms step_avg:33.98ms
step:40/1840 train_time:1360ms step_avg:34.00ms
step:41/1840 train_time:1392ms step_avg:33.96ms
step:42/1840 train_time:1427ms step_avg:33.97ms
step:43/1840 train_time:1459ms step_avg:33.93ms
step:44/1840 train_time:1493ms step_avg:33.94ms
step:45/1840 train_time:1525ms step_avg:33.89ms
step:46/1840 train_time:1559ms step_avg:33.89ms
step:47/1840 train_time:1591ms step_avg:33.86ms
step:48/1840 train_time:1626ms step_avg:33.87ms
step:49/1840 train_time:1658ms step_avg:33.83ms
step:50/1840 train_time:1692ms step_avg:33.84ms
step:51/1840 train_time:1724ms step_avg:33.81ms
step:52/1840 train_time:1758ms step_avg:33.82ms
step:53/1840 train_time:1791ms step_avg:33.78ms
step:54/1840 train_time:1825ms step_avg:33.79ms
step:55/1840 train_time:1857ms step_avg:33.76ms
step:56/1840 train_time:1891ms step_avg:33.76ms
step:57/1840 train_time:1923ms step_avg:33.73ms
step:58/1840 train_time:1957ms step_avg:33.74ms
step:59/1840 train_time:1989ms step_avg:33.71ms
step:60/1840 train_time:2023ms step_avg:33.72ms
step:61/1840 train_time:2055ms step_avg:33.69ms
step:62/1840 train_time:2089ms step_avg:33.70ms
step:63/1840 train_time:2121ms step_avg:33.67ms
step:64/1840 train_time:2156ms step_avg:33.68ms
step:65/1840 train_time:2188ms step_avg:33.67ms
step:66/1840 train_time:2223ms step_avg:33.68ms
step:67/1840 train_time:2255ms step_avg:33.66ms
step:68/1840 train_time:2290ms step_avg:33.67ms
step:69/1840 train_time:2322ms step_avg:33.65ms
step:70/1840 train_time:2357ms step_avg:33.67ms
step:71/1840 train_time:2389ms step_avg:33.65ms
step:72/1840 train_time:2423ms step_avg:33.66ms
step:73/1840 train_time:2456ms step_avg:33.64ms
step:74/1840 train_time:2490ms step_avg:33.65ms
step:75/1840 train_time:2522ms step_avg:33.63ms
step:76/1840 train_time:2557ms step_avg:33.64ms
step:77/1840 train_time:2589ms step_avg:33.62ms
step:78/1840 train_time:2623ms step_avg:33.63ms
step:79/1840 train_time:2655ms step_avg:33.61ms
step:80/1840 train_time:2690ms step_avg:33.62ms
step:81/1840 train_time:2722ms step_avg:33.60ms
step:82/1840 train_time:2756ms step_avg:33.61ms
step:83/1840 train_time:2788ms step_avg:33.59ms
step:84/1840 train_time:2823ms step_avg:33.60ms
step:85/1840 train_time:2854ms step_avg:33.58ms
step:86/1840 train_time:2888ms step_avg:33.59ms
step:87/1840 train_time:2920ms step_avg:33.57ms
step:88/1840 train_time:2955ms step_avg:33.58ms
step:89/1840 train_time:2987ms step_avg:33.56ms
step:90/1840 train_time:3021ms step_avg:33.56ms
step:91/1840 train_time:3053ms step_avg:33.55ms
step:92/1840 train_time:3087ms step_avg:33.55ms
step:93/1840 train_time:3119ms step_avg:33.54ms
step:94/1840 train_time:3153ms step_avg:33.55ms
step:95/1840 train_time:3185ms step_avg:33.53ms
step:96/1840 train_time:3220ms step_avg:33.54ms
step:97/1840 train_time:3251ms step_avg:33.52ms
step:98/1840 train_time:3286ms step_avg:33.53ms
step:99/1840 train_time:3318ms step_avg:33.52ms
step:100/1840 train_time:3354ms step_avg:33.54ms
step:101/1840 train_time:3385ms step_avg:33.51ms
step:102/1840 train_time:3419ms step_avg:33.52ms
step:103/1840 train_time:3451ms step_avg:33.50ms
step:104/1840 train_time:3485ms step_avg:33.51ms
step:105/1840 train_time:3517ms step_avg:33.50ms
step:106/1840 train_time:3551ms step_avg:33.50ms
step:107/1840 train_time:3583ms step_avg:33.49ms
step:108/1840 train_time:3618ms step_avg:33.50ms
step:109/1840 train_time:3650ms step_avg:33.49ms
step:110/1840 train_time:3684ms step_avg:33.49ms
step:111/1840 train_time:3717ms step_avg:33.48ms
step:112/1840 train_time:3751ms step_avg:33.49ms
step:113/1840 train_time:3783ms step_avg:33.47ms
step:114/1840 train_time:3817ms step_avg:33.48ms
step:115/1840 train_time:3849ms step_avg:33.47ms
step:116/1840 train_time:3883ms step_avg:33.48ms
step:117/1840 train_time:3916ms step_avg:33.47ms
step:118/1840 train_time:3950ms step_avg:33.47ms
step:119/1840 train_time:3982ms step_avg:33.46ms
step:120/1840 train_time:4016ms step_avg:33.47ms
step:121/1840 train_time:4048ms step_avg:33.46ms
step:122/1840 train_time:4083ms step_avg:33.46ms
step:123/1840 train_time:4115ms step_avg:33.45ms
step:124/1840 train_time:4149ms step_avg:33.46ms
step:125/1840 train_time:4181ms step_avg:33.45ms
step:126/1840 train_time:4215ms step_avg:33.46ms
step:127/1840 train_time:4248ms step_avg:33.45ms
step:128/1840 train_time:4282ms step_avg:33.45ms
step:129/1840 train_time:4314ms step_avg:33.44ms
step:130/1840 train_time:4348ms step_avg:33.45ms
step:131/1840 train_time:4381ms step_avg:33.44ms
step:132/1840 train_time:4415ms step_avg:33.45ms
step:133/1840 train_time:4447ms step_avg:33.44ms
step:134/1840 train_time:4482ms step_avg:33.45ms
step:135/1840 train_time:4514ms step_avg:33.44ms
step:136/1840 train_time:4548ms step_avg:33.44ms
step:137/1840 train_time:4580ms step_avg:33.43ms
step:138/1840 train_time:4614ms step_avg:33.44ms
step:139/1840 train_time:4646ms step_avg:33.43ms
step:140/1840 train_time:4680ms step_avg:33.43ms
step:141/1840 train_time:4712ms step_avg:33.42ms
step:142/1840 train_time:4747ms step_avg:33.43ms
step:143/1840 train_time:4779ms step_avg:33.42ms
step:144/1840 train_time:4813ms step_avg:33.42ms
step:145/1840 train_time:4845ms step_avg:33.41ms
step:146/1840 train_time:4879ms step_avg:33.42ms
step:147/1840 train_time:4911ms step_avg:33.41ms
step:148/1840 train_time:4945ms step_avg:33.41ms
step:149/1840 train_time:4977ms step_avg:33.40ms
step:150/1840 train_time:5012ms step_avg:33.41ms
step:151/1840 train_time:5044ms step_avg:33.40ms
step:152/1840 train_time:5078ms step_avg:33.40ms
step:153/1840 train_time:5110ms step_avg:33.40ms
step:154/1840 train_time:5144ms step_avg:33.40ms
step:155/1840 train_time:5176ms step_avg:33.39ms
step:156/1840 train_time:5210ms step_avg:33.40ms
step:157/1840 train_time:5242ms step_avg:33.39ms
step:158/1840 train_time:5276ms step_avg:33.39ms
step:159/1840 train_time:5309ms step_avg:33.39ms
step:160/1840 train_time:5343ms step_avg:33.40ms
step:161/1840 train_time:5376ms step_avg:33.39ms
step:162/1840 train_time:5410ms step_avg:33.40ms
step:163/1840 train_time:5443ms step_avg:33.39ms
step:164/1840 train_time:5477ms step_avg:33.40ms
step:165/1840 train_time:5509ms step_avg:33.39ms
step:166/1840 train_time:5544ms step_avg:33.39ms
step:167/1840 train_time:5576ms step_avg:33.39ms
step:168/1840 train_time:5610ms step_avg:33.39ms
step:169/1840 train_time:5642ms step_avg:33.39ms
step:170/1840 train_time:5676ms step_avg:33.39ms
step:171/1840 train_time:5708ms step_avg:33.38ms
step:172/1840 train_time:5743ms step_avg:33.39ms
step:173/1840 train_time:5775ms step_avg:33.38ms
step:174/1840 train_time:5809ms step_avg:33.38ms
step:175/1840 train_time:5841ms step_avg:33.38ms
step:176/1840 train_time:5876ms step_avg:33.38ms
step:177/1840 train_time:5908ms step_avg:33.38ms
step:178/1840 train_time:5942ms step_avg:33.38ms
step:179/1840 train_time:5974ms step_avg:33.38ms
step:180/1840 train_time:6008ms step_avg:33.38ms
step:181/1840 train_time:6040ms step_avg:33.37ms
step:182/1840 train_time:6075ms step_avg:33.38ms
step:183/1840 train_time:6107ms step_avg:33.37ms
step:184/1840 train_time:6141ms step_avg:33.38ms
step:185/1840 train_time:6173ms step_avg:33.37ms
step:186/1840 train_time:6207ms step_avg:33.37ms
step:187/1840 train_time:6239ms step_avg:33.37ms
step:188/1840 train_time:6273ms step_avg:33.37ms
step:189/1840 train_time:6305ms step_avg:33.36ms
step:190/1840 train_time:6340ms step_avg:33.37ms
step:191/1840 train_time:6372ms step_avg:33.36ms
step:192/1840 train_time:6406ms step_avg:33.36ms
step:193/1840 train_time:6438ms step_avg:33.36ms
step:194/1840 train_time:6473ms step_avg:33.36ms
step:195/1840 train_time:6505ms step_avg:33.36ms
step:196/1840 train_time:6539ms step_avg:33.36ms
step:197/1840 train_time:6571ms step_avg:33.36ms
step:198/1840 train_time:6605ms step_avg:33.36ms
step:199/1840 train_time:6637ms step_avg:33.35ms
step:200/1840 train_time:6672ms step_avg:33.36ms
step:201/1840 train_time:6704ms step_avg:33.35ms
step:202/1840 train_time:6738ms step_avg:33.36ms
step:203/1840 train_time:6771ms step_avg:33.35ms
step:204/1840 train_time:6805ms step_avg:33.36ms
step:205/1840 train_time:6837ms step_avg:33.35ms
step:206/1840 train_time:6872ms step_avg:33.36ms
step:207/1840 train_time:6904ms step_avg:33.35ms
step:208/1840 train_time:6938ms step_avg:33.35ms
step:209/1840 train_time:6970ms step_avg:33.35ms
step:210/1840 train_time:7004ms step_avg:33.35ms
step:211/1840 train_time:7037ms step_avg:33.35ms
step:212/1840 train_time:7071ms step_avg:33.35ms
step:213/1840 train_time:7103ms step_avg:33.35ms
step:214/1840 train_time:7137ms step_avg:33.35ms
step:215/1840 train_time:7169ms step_avg:33.35ms
step:216/1840 train_time:7203ms step_avg:33.35ms
step:217/1840 train_time:7235ms step_avg:33.34ms
step:218/1840 train_time:7270ms step_avg:33.35ms
step:219/1840 train_time:7302ms step_avg:33.34ms
step:220/1840 train_time:7336ms step_avg:33.35ms
step:221/1840 train_time:7368ms step_avg:33.34ms
step:222/1840 train_time:7403ms step_avg:33.35ms
step:223/1840 train_time:7435ms step_avg:33.34ms
step:224/1840 train_time:7469ms step_avg:33.34ms
step:225/1840 train_time:7501ms step_avg:33.34ms
step:226/1840 train_time:7535ms step_avg:33.34ms
step:227/1840 train_time:7567ms step_avg:33.34ms
step:228/1840 train_time:7602ms step_avg:33.34ms
step:229/1840 train_time:7634ms step_avg:33.33ms
step:230/1840 train_time:7667ms step_avg:33.34ms
step:231/1840 train_time:7699ms step_avg:33.33ms
step:232/1840 train_time:7733ms step_avg:33.33ms
step:233/1840 train_time:7766ms step_avg:33.33ms
step:234/1840 train_time:7800ms step_avg:33.33ms
step:235/1840 train_time:7832ms step_avg:33.33ms
step:236/1840 train_time:7866ms step_avg:33.33ms
step:237/1840 train_time:7898ms step_avg:33.33ms
step:238/1840 train_time:7933ms step_avg:33.33ms
step:239/1840 train_time:7964ms step_avg:33.32ms
step:240/1840 train_time:7998ms step_avg:33.33ms
step:241/1840 train_time:8031ms step_avg:33.32ms
step:242/1840 train_time:8065ms step_avg:33.33ms
step:243/1840 train_time:8097ms step_avg:33.32ms
step:244/1840 train_time:8132ms step_avg:33.33ms
step:245/1840 train_time:8163ms step_avg:33.32ms
step:246/1840 train_time:8197ms step_avg:33.32ms
step:247/1840 train_time:8230ms step_avg:33.32ms
step:248/1840 train_time:8264ms step_avg:33.32ms
step:249/1840 train_time:8296ms step_avg:33.32ms
step:250/1840 train_time:8330ms step_avg:33.32ms
step:250/1840 val_loss:4.6059 train_time:8372ms step_avg:33.49ms
step:251/1840 train_time:8390ms step_avg:33.43ms
step:252/1840 train_time:8408ms step_avg:33.37ms
step:253/1840 train_time:8430ms step_avg:33.32ms
step:254/1840 train_time:8466ms step_avg:33.33ms
step:255/1840 train_time:8500ms step_avg:33.33ms
step:256/1840 train_time:8536ms step_avg:33.34ms
step:257/1840 train_time:8568ms step_avg:33.34ms
step:258/1840 train_time:8602ms step_avg:33.34ms
step:259/1840 train_time:8634ms step_avg:33.34ms
step:260/1840 train_time:8668ms step_avg:33.34ms
step:261/1840 train_time:8700ms step_avg:33.33ms
step:262/1840 train_time:8734ms step_avg:33.34ms
step:263/1840 train_time:8766ms step_avg:33.33ms
step:264/1840 train_time:8800ms step_avg:33.33ms
step:265/1840 train_time:8832ms step_avg:33.33ms
step:266/1840 train_time:8866ms step_avg:33.33ms
step:267/1840 train_time:8898ms step_avg:33.33ms
step:268/1840 train_time:8932ms step_avg:33.33ms
step:269/1840 train_time:8964ms step_avg:33.32ms
step:270/1840 train_time:8998ms step_avg:33.33ms
step:271/1840 train_time:9030ms step_avg:33.32ms
step:272/1840 train_time:9064ms step_avg:33.32ms
step:273/1840 train_time:9096ms step_avg:33.32ms
step:274/1840 train_time:9129ms step_avg:33.32ms
step:275/1840 train_time:9161ms step_avg:33.31ms
step:276/1840 train_time:9196ms step_avg:33.32ms
step:277/1840 train_time:9228ms step_avg:33.31ms
step:278/1840 train_time:9261ms step_avg:33.31ms
step:279/1840 train_time:9293ms step_avg:33.31ms
step:280/1840 train_time:9328ms step_avg:33.31ms
step:281/1840 train_time:9361ms step_avg:33.31ms
step:282/1840 train_time:9395ms step_avg:33.31ms
step:283/1840 train_time:9427ms step_avg:33.31ms
step:284/1840 train_time:9463ms step_avg:33.32ms
step:285/1840 train_time:9495ms step_avg:33.31ms
step:286/1840 train_time:9529ms step_avg:33.32ms
step:287/1840 train_time:9562ms step_avg:33.32ms
step:288/1840 train_time:9596ms step_avg:33.32ms
step:289/1840 train_time:9628ms step_avg:33.32ms
step:290/1840 train_time:9663ms step_avg:33.32ms
step:291/1840 train_time:9695ms step_avg:33.32ms
step:292/1840 train_time:9730ms step_avg:33.32ms
step:293/1840 train_time:9762ms step_avg:33.32ms
step:294/1840 train_time:9796ms step_avg:33.32ms
step:295/1840 train_time:9828ms step_avg:33.31ms
step:296/1840 train_time:9862ms step_avg:33.32ms
step:297/1840 train_time:9894ms step_avg:33.31ms
step:298/1840 train_time:9927ms step_avg:33.31ms
step:299/1840 train_time:9959ms step_avg:33.31ms
step:300/1840 train_time:9993ms step_avg:33.31ms
step:301/1840 train_time:10025ms step_avg:33.31ms
step:302/1840 train_time:10059ms step_avg:33.31ms
step:303/1840 train_time:10091ms step_avg:33.30ms
step:304/1840 train_time:10126ms step_avg:33.31ms
step:305/1840 train_time:10158ms step_avg:33.30ms
step:306/1840 train_time:10192ms step_avg:33.31ms
step:307/1840 train_time:10224ms step_avg:33.30ms
step:308/1840 train_time:10258ms step_avg:33.30ms
step:309/1840 train_time:10289ms step_avg:33.30ms
step:310/1840 train_time:10325ms step_avg:33.31ms
step:311/1840 train_time:10357ms step_avg:33.30ms
step:312/1840 train_time:10391ms step_avg:33.30ms
step:313/1840 train_time:10423ms step_avg:33.30ms
step:314/1840 train_time:10458ms step_avg:33.31ms
step:315/1840 train_time:10491ms step_avg:33.30ms
step:316/1840 train_time:10525ms step_avg:33.31ms
step:317/1840 train_time:10557ms step_avg:33.30ms
step:318/1840 train_time:10592ms step_avg:33.31ms
step:319/1840 train_time:10624ms step_avg:33.30ms
step:320/1840 train_time:10658ms step_avg:33.31ms
step:321/1840 train_time:10691ms step_avg:33.30ms
step:322/1840 train_time:10725ms step_avg:33.31ms
step:323/1840 train_time:10758ms step_avg:33.30ms
step:324/1840 train_time:10792ms step_avg:33.31ms
step:325/1840 train_time:10824ms step_avg:33.30ms
step:326/1840 train_time:10858ms step_avg:33.31ms
step:327/1840 train_time:10890ms step_avg:33.30ms
step:328/1840 train_time:10924ms step_avg:33.31ms
step:329/1840 train_time:10956ms step_avg:33.30ms
step:330/1840 train_time:10990ms step_avg:33.30ms
step:331/1840 train_time:11022ms step_avg:33.30ms
step:332/1840 train_time:11056ms step_avg:33.30ms
step:333/1840 train_time:11088ms step_avg:33.30ms
step:334/1840 train_time:11123ms step_avg:33.30ms
step:335/1840 train_time:11155ms step_avg:33.30ms
step:336/1840 train_time:11189ms step_avg:33.30ms
step:337/1840 train_time:11221ms step_avg:33.30ms
step:338/1840 train_time:11255ms step_avg:33.30ms
step:339/1840 train_time:11287ms step_avg:33.30ms
step:340/1840 train_time:11321ms step_avg:33.30ms
step:341/1840 train_time:11353ms step_avg:33.29ms
step:342/1840 train_time:11388ms step_avg:33.30ms
step:343/1840 train_time:11420ms step_avg:33.30ms
step:344/1840 train_time:11454ms step_avg:33.30ms
step:345/1840 train_time:11486ms step_avg:33.29ms
step:346/1840 train_time:11521ms step_avg:33.30ms
step:347/1840 train_time:11553ms step_avg:33.29ms
step:348/1840 train_time:11588ms step_avg:33.30ms
step:349/1840 train_time:11620ms step_avg:33.29ms
step:350/1840 train_time:11654ms step_avg:33.30ms
step:351/1840 train_time:11686ms step_avg:33.29ms
step:352/1840 train_time:11721ms step_avg:33.30ms
step:353/1840 train_time:11753ms step_avg:33.30ms
step:354/1840 train_time:11787ms step_avg:33.30ms
step:355/1840 train_time:11820ms step_avg:33.30ms
step:356/1840 train_time:11854ms step_avg:33.30ms
step:357/1840 train_time:11886ms step_avg:33.29ms
step:358/1840 train_time:11920ms step_avg:33.30ms
step:359/1840 train_time:11952ms step_avg:33.29ms
step:360/1840 train_time:11986ms step_avg:33.30ms
step:361/1840 train_time:12018ms step_avg:33.29ms
step:362/1840 train_time:12053ms step_avg:33.29ms
step:363/1840 train_time:12084ms step_avg:33.29ms
step:364/1840 train_time:12119ms step_avg:33.29ms
step:365/1840 train_time:12150ms step_avg:33.29ms
step:366/1840 train_time:12185ms step_avg:33.29ms
step:367/1840 train_time:12217ms step_avg:33.29ms
step:368/1840 train_time:12251ms step_avg:33.29ms
step:369/1840 train_time:12284ms step_avg:33.29ms
step:370/1840 train_time:12318ms step_avg:33.29ms
step:371/1840 train_time:12350ms step_avg:33.29ms
step:372/1840 train_time:12384ms step_avg:33.29ms
step:373/1840 train_time:12416ms step_avg:33.29ms
step:374/1840 train_time:12450ms step_avg:33.29ms
step:375/1840 train_time:12482ms step_avg:33.29ms
step:376/1840 train_time:12516ms step_avg:33.29ms
step:377/1840 train_time:12548ms step_avg:33.28ms
step:378/1840 train_time:12583ms step_avg:33.29ms
step:379/1840 train_time:12615ms step_avg:33.28ms
step:380/1840 train_time:12649ms step_avg:33.29ms
step:381/1840 train_time:12681ms step_avg:33.28ms
step:382/1840 train_time:12715ms step_avg:33.29ms
step:383/1840 train_time:12747ms step_avg:33.28ms
step:384/1840 train_time:12782ms step_avg:33.29ms
step:385/1840 train_time:12813ms step_avg:33.28ms
step:386/1840 train_time:12848ms step_avg:33.28ms
step:387/1840 train_time:12880ms step_avg:33.28ms
step:388/1840 train_time:12914ms step_avg:33.28ms
step:389/1840 train_time:12946ms step_avg:33.28ms
step:390/1840 train_time:12980ms step_avg:33.28ms
step:391/1840 train_time:13012ms step_avg:33.28ms
step:392/1840 train_time:13046ms step_avg:33.28ms
step:393/1840 train_time:13078ms step_avg:33.28ms
step:394/1840 train_time:13112ms step_avg:33.28ms
step:395/1840 train_time:13144ms step_avg:33.28ms
step:396/1840 train_time:13178ms step_avg:33.28ms
step:397/1840 train_time:13210ms step_avg:33.28ms
step:398/1840 train_time:13245ms step_avg:33.28ms
step:399/1840 train_time:13277ms step_avg:33.28ms
step:400/1840 train_time:13311ms step_avg:33.28ms
step:401/1840 train_time:13343ms step_avg:33.27ms
step:402/1840 train_time:13377ms step_avg:33.28ms
step:403/1840 train_time:13409ms step_avg:33.27ms
step:404/1840 train_time:13443ms step_avg:33.28ms
step:405/1840 train_time:13475ms step_avg:33.27ms
step:406/1840 train_time:13509ms step_avg:33.27ms
step:407/1840 train_time:13541ms step_avg:33.27ms
step:408/1840 train_time:13576ms step_avg:33.27ms
step:409/1840 train_time:13608ms step_avg:33.27ms
step:410/1840 train_time:13642ms step_avg:33.27ms
step:411/1840 train_time:13674ms step_avg:33.27ms
step:412/1840 train_time:13708ms step_avg:33.27ms
step:413/1840 train_time:13740ms step_avg:33.27ms
step:414/1840 train_time:13775ms step_avg:33.27ms
step:415/1840 train_time:13807ms step_avg:33.27ms
step:416/1840 train_time:13841ms step_avg:33.27ms
step:417/1840 train_time:13873ms step_avg:33.27ms
step:418/1840 train_time:13908ms step_avg:33.27ms
step:419/1840 train_time:13940ms step_avg:33.27ms
step:420/1840 train_time:13974ms step_avg:33.27ms
step:421/1840 train_time:14006ms step_avg:33.27ms
step:422/1840 train_time:14040ms step_avg:33.27ms
step:423/1840 train_time:14071ms step_avg:33.27ms
step:424/1840 train_time:14106ms step_avg:33.27ms
step:425/1840 train_time:14138ms step_avg:33.27ms
step:426/1840 train_time:14172ms step_avg:33.27ms
step:427/1840 train_time:14204ms step_avg:33.26ms
step:428/1840 train_time:14238ms step_avg:33.27ms
step:429/1840 train_time:14271ms step_avg:33.26ms
step:430/1840 train_time:14305ms step_avg:33.27ms
step:431/1840 train_time:14337ms step_avg:33.26ms
step:432/1840 train_time:14371ms step_avg:33.27ms
step:433/1840 train_time:14403ms step_avg:33.26ms
step:434/1840 train_time:14437ms step_avg:33.27ms
step:435/1840 train_time:14469ms step_avg:33.26ms
step:436/1840 train_time:14504ms step_avg:33.27ms
step:437/1840 train_time:14536ms step_avg:33.26ms
step:438/1840 train_time:14571ms step_avg:33.27ms
step:439/1840 train_time:14603ms step_avg:33.26ms
step:440/1840 train_time:14637ms step_avg:33.27ms
step:441/1840 train_time:14669ms step_avg:33.26ms
step:442/1840 train_time:14703ms step_avg:33.27ms
step:443/1840 train_time:14736ms step_avg:33.26ms
step:444/1840 train_time:14770ms step_avg:33.27ms
step:445/1840 train_time:14802ms step_avg:33.26ms
step:446/1840 train_time:14836ms step_avg:33.26ms
step:447/1840 train_time:14868ms step_avg:33.26ms
step:448/1840 train_time:14902ms step_avg:33.26ms
step:449/1840 train_time:14934ms step_avg:33.26ms
step:450/1840 train_time:14968ms step_avg:33.26ms
step:451/1840 train_time:15000ms step_avg:33.26ms
step:452/1840 train_time:15034ms step_avg:33.26ms
step:453/1840 train_time:15066ms step_avg:33.26ms
step:454/1840 train_time:15100ms step_avg:33.26ms
step:455/1840 train_time:15132ms step_avg:33.26ms
step:456/1840 train_time:15166ms step_avg:33.26ms
step:457/1840 train_time:15198ms step_avg:33.26ms
step:458/1840 train_time:15233ms step_avg:33.26ms
step:459/1840 train_time:15265ms step_avg:33.26ms
step:460/1840 train_time:15299ms step_avg:33.26ms
step:461/1840 train_time:15331ms step_avg:33.26ms
step:462/1840 train_time:15365ms step_avg:33.26ms
step:463/1840 train_time:15397ms step_avg:33.26ms
step:464/1840 train_time:15432ms step_avg:33.26ms
step:465/1840 train_time:15464ms step_avg:33.26ms
step:466/1840 train_time:15498ms step_avg:33.26ms
step:467/1840 train_time:15530ms step_avg:33.26ms
step:468/1840 train_time:15565ms step_avg:33.26ms
step:469/1840 train_time:15597ms step_avg:33.26ms
step:470/1840 train_time:15631ms step_avg:33.26ms
step:471/1840 train_time:15663ms step_avg:33.26ms
step:472/1840 train_time:15697ms step_avg:33.26ms
step:473/1840 train_time:15729ms step_avg:33.25ms
step:474/1840 train_time:15764ms step_avg:33.26ms
step:475/1840 train_time:15796ms step_avg:33.26ms
step:476/1840 train_time:15830ms step_avg:33.26ms
step:477/1840 train_time:15863ms step_avg:33.26ms
step:478/1840 train_time:15897ms step_avg:33.26ms
step:479/1840 train_time:15929ms step_avg:33.25ms
step:480/1840 train_time:15963ms step_avg:33.26ms
step:481/1840 train_time:15995ms step_avg:33.25ms
step:482/1840 train_time:16029ms step_avg:33.26ms
step:483/1840 train_time:16061ms step_avg:33.25ms
step:484/1840 train_time:16095ms step_avg:33.25ms
step:485/1840 train_time:16127ms step_avg:33.25ms
step:486/1840 train_time:16161ms step_avg:33.25ms
step:487/1840 train_time:16193ms step_avg:33.25ms
step:488/1840 train_time:16227ms step_avg:33.25ms
step:489/1840 train_time:16259ms step_avg:33.25ms
step:490/1840 train_time:16293ms step_avg:33.25ms
step:491/1840 train_time:16326ms step_avg:33.25ms
step:492/1840 train_time:16360ms step_avg:33.25ms
step:493/1840 train_time:16392ms step_avg:33.25ms
step:494/1840 train_time:16427ms step_avg:33.25ms
step:495/1840 train_time:16459ms step_avg:33.25ms
step:496/1840 train_time:16493ms step_avg:33.25ms
step:497/1840 train_time:16525ms step_avg:33.25ms
step:498/1840 train_time:16559ms step_avg:33.25ms
step:499/1840 train_time:16591ms step_avg:33.25ms
step:500/1840 train_time:16626ms step_avg:33.25ms
step:500/1840 val_loss:4.2909 train_time:16668ms step_avg:33.34ms
step:501/1840 train_time:16688ms step_avg:33.31ms
step:502/1840 train_time:16706ms step_avg:33.28ms
step:503/1840 train_time:16727ms step_avg:33.26ms
step:504/1840 train_time:16763ms step_avg:33.26ms
step:505/1840 train_time:16797ms step_avg:33.26ms
step:506/1840 train_time:16833ms step_avg:33.27ms
step:507/1840 train_time:16866ms step_avg:33.27ms
step:508/1840 train_time:16900ms step_avg:33.27ms
step:509/1840 train_time:16932ms step_avg:33.27ms
step:510/1840 train_time:16966ms step_avg:33.27ms
step:511/1840 train_time:16999ms step_avg:33.27ms
step:512/1840 train_time:17033ms step_avg:33.27ms
step:513/1840 train_time:17065ms step_avg:33.27ms
step:514/1840 train_time:17099ms step_avg:33.27ms
step:515/1840 train_time:17131ms step_avg:33.26ms
step:516/1840 train_time:17165ms step_avg:33.27ms
step:517/1840 train_time:17197ms step_avg:33.26ms
step:518/1840 train_time:17230ms step_avg:33.26ms
step:519/1840 train_time:17262ms step_avg:33.26ms
step:520/1840 train_time:17296ms step_avg:33.26ms
step:521/1840 train_time:17328ms step_avg:33.26ms
step:522/1840 train_time:17362ms step_avg:33.26ms
step:523/1840 train_time:17394ms step_avg:33.26ms
step:524/1840 train_time:17428ms step_avg:33.26ms
step:525/1840 train_time:17460ms step_avg:33.26ms
step:526/1840 train_time:17494ms step_avg:33.26ms
step:527/1840 train_time:17526ms step_avg:33.26ms
step:528/1840 train_time:17560ms step_avg:33.26ms
step:529/1840 train_time:17592ms step_avg:33.26ms
step:530/1840 train_time:17626ms step_avg:33.26ms
step:531/1840 train_time:17658ms step_avg:33.26ms
step:532/1840 train_time:17693ms step_avg:33.26ms
step:533/1840 train_time:17726ms step_avg:33.26ms
step:534/1840 train_time:17760ms step_avg:33.26ms
step:535/1840 train_time:17793ms step_avg:33.26ms
step:536/1840 train_time:17828ms step_avg:33.26ms
step:537/1840 train_time:17861ms step_avg:33.26ms
step:538/1840 train_time:17895ms step_avg:33.26ms
step:539/1840 train_time:17928ms step_avg:33.26ms
step:540/1840 train_time:17962ms step_avg:33.26ms
step:541/1840 train_time:17995ms step_avg:33.26ms
step:542/1840 train_time:18029ms step_avg:33.26ms
step:543/1840 train_time:18061ms step_avg:33.26ms
step:544/1840 train_time:18095ms step_avg:33.26ms
step:545/1840 train_time:18127ms step_avg:33.26ms
step:546/1840 train_time:18162ms step_avg:33.26ms
step:547/1840 train_time:18194ms step_avg:33.26ms
step:548/1840 train_time:18228ms step_avg:33.26ms
step:549/1840 train_time:18260ms step_avg:33.26ms
step:550/1840 train_time:18294ms step_avg:33.26ms
step:551/1840 train_time:18326ms step_avg:33.26ms
step:552/1840 train_time:18360ms step_avg:33.26ms
step:553/1840 train_time:18392ms step_avg:33.26ms
step:554/1840 train_time:18426ms step_avg:33.26ms
step:555/1840 train_time:18458ms step_avg:33.26ms
step:556/1840 train_time:18492ms step_avg:33.26ms
step:557/1840 train_time:18524ms step_avg:33.26ms
step:558/1840 train_time:18558ms step_avg:33.26ms
step:559/1840 train_time:18590ms step_avg:33.26ms
step:560/1840 train_time:18624ms step_avg:33.26ms
step:561/1840 train_time:18656ms step_avg:33.25ms
step:562/1840 train_time:18690ms step_avg:33.26ms
step:563/1840 train_time:18723ms step_avg:33.26ms
step:564/1840 train_time:18757ms step_avg:33.26ms
step:565/1840 train_time:18790ms step_avg:33.26ms
step:566/1840 train_time:18824ms step_avg:33.26ms
step:567/1840 train_time:18856ms step_avg:33.26ms
step:568/1840 train_time:18891ms step_avg:33.26ms
step:569/1840 train_time:18923ms step_avg:33.26ms
step:570/1840 train_time:18957ms step_avg:33.26ms
step:571/1840 train_time:18989ms step_avg:33.26ms
step:572/1840 train_time:19024ms step_avg:33.26ms
step:573/1840 train_time:19056ms step_avg:33.26ms
step:574/1840 train_time:19090ms step_avg:33.26ms
step:575/1840 train_time:19123ms step_avg:33.26ms
step:576/1840 train_time:19157ms step_avg:33.26ms
step:577/1840 train_time:19189ms step_avg:33.26ms
step:578/1840 train_time:19223ms step_avg:33.26ms
step:579/1840 train_time:19255ms step_avg:33.26ms
step:580/1840 train_time:19289ms step_avg:33.26ms
step:581/1840 train_time:19322ms step_avg:33.26ms
step:582/1840 train_time:19356ms step_avg:33.26ms
step:583/1840 train_time:19388ms step_avg:33.26ms
step:584/1840 train_time:19422ms step_avg:33.26ms
step:585/1840 train_time:19454ms step_avg:33.25ms
step:586/1840 train_time:19488ms step_avg:33.26ms
step:587/1840 train_time:19520ms step_avg:33.25ms
step:588/1840 train_time:19554ms step_avg:33.26ms
step:589/1840 train_time:19586ms step_avg:33.25ms
step:590/1840 train_time:19620ms step_avg:33.25ms
step:591/1840 train_time:19652ms step_avg:33.25ms
step:592/1840 train_time:19687ms step_avg:33.25ms
step:593/1840 train_time:19719ms step_avg:33.25ms
step:594/1840 train_time:19753ms step_avg:33.26ms
step:595/1840 train_time:19786ms step_avg:33.25ms
step:596/1840 train_time:19820ms step_avg:33.25ms
step:597/1840 train_time:19852ms step_avg:33.25ms
step:598/1840 train_time:19886ms step_avg:33.25ms
step:599/1840 train_time:19918ms step_avg:33.25ms
step:600/1840 train_time:19953ms step_avg:33.25ms
step:601/1840 train_time:19987ms step_avg:33.26ms
step:602/1840 train_time:20046ms step_avg:33.30ms
step:603/1840 train_time:20106ms step_avg:33.34ms
step:604/1840 train_time:20168ms step_avg:33.39ms
step:605/1840 train_time:20228ms step_avg:33.43ms
step:606/1840 train_time:20290ms step_avg:33.48ms
step:607/1840 train_time:20349ms step_avg:33.52ms
step:608/1840 train_time:20411ms step_avg:33.57ms
step:609/1840 train_time:20472ms step_avg:33.61ms
step:610/1840 train_time:20534ms step_avg:33.66ms
step:611/1840 train_time:20593ms step_avg:33.70ms
step:612/1840 train_time:20655ms step_avg:33.75ms
step:613/1840 train_time:20715ms step_avg:33.79ms
step:614/1840 train_time:20777ms step_avg:33.84ms
step:615/1840 train_time:20837ms step_avg:33.88ms
step:616/1840 train_time:20899ms step_avg:33.93ms
step:617/1840 train_time:20958ms step_avg:33.97ms
step:618/1840 train_time:21021ms step_avg:34.01ms
step:619/1840 train_time:21081ms step_avg:34.06ms
step:620/1840 train_time:21143ms step_avg:34.10ms
step:621/1840 train_time:21203ms step_avg:34.14ms
step:622/1840 train_time:21266ms step_avg:34.19ms
step:623/1840 train_time:21325ms step_avg:34.23ms
step:624/1840 train_time:21388ms step_avg:34.28ms
step:625/1840 train_time:21448ms step_avg:34.32ms
step:626/1840 train_time:21510ms step_avg:34.36ms
step:627/1840 train_time:21569ms step_avg:34.40ms
step:628/1840 train_time:21632ms step_avg:34.45ms
step:629/1840 train_time:21692ms step_avg:34.49ms
step:630/1840 train_time:21754ms step_avg:34.53ms
step:631/1840 train_time:21814ms step_avg:34.57ms
step:632/1840 train_time:21877ms step_avg:34.62ms
step:633/1840 train_time:21936ms step_avg:34.65ms
step:634/1840 train_time:21999ms step_avg:34.70ms
step:635/1840 train_time:22058ms step_avg:34.74ms
step:636/1840 train_time:22120ms step_avg:34.78ms
step:637/1840 train_time:22181ms step_avg:34.82ms
step:638/1840 train_time:22244ms step_avg:34.86ms
step:639/1840 train_time:22303ms step_avg:34.90ms
step:640/1840 train_time:22365ms step_avg:34.95ms
step:641/1840 train_time:22425ms step_avg:34.98ms
step:642/1840 train_time:22487ms step_avg:35.03ms
step:643/1840 train_time:22547ms step_avg:35.06ms
step:644/1840 train_time:22609ms step_avg:35.11ms
step:645/1840 train_time:22668ms step_avg:35.14ms
step:646/1840 train_time:22731ms step_avg:35.19ms
step:647/1840 train_time:22791ms step_avg:35.23ms
step:648/1840 train_time:22854ms step_avg:35.27ms
step:649/1840 train_time:22915ms step_avg:35.31ms
step:650/1840 train_time:22977ms step_avg:35.35ms
step:651/1840 train_time:23037ms step_avg:35.39ms
step:652/1840 train_time:23099ms step_avg:35.43ms
step:653/1840 train_time:23158ms step_avg:35.46ms
step:654/1840 train_time:23220ms step_avg:35.50ms
step:655/1840 train_time:23280ms step_avg:35.54ms
step:656/1840 train_time:23342ms step_avg:35.58ms
step:657/1840 train_time:23403ms step_avg:35.62ms
step:658/1840 train_time:23464ms step_avg:35.66ms
step:659/1840 train_time:23524ms step_avg:35.70ms
step:660/1840 train_time:23586ms step_avg:35.74ms
step:661/1840 train_time:23647ms step_avg:35.77ms
step:662/1840 train_time:23709ms step_avg:35.81ms
step:663/1840 train_time:23769ms step_avg:35.85ms
step:664/1840 train_time:23832ms step_avg:35.89ms
step:665/1840 train_time:23893ms step_avg:35.93ms
step:666/1840 train_time:23955ms step_avg:35.97ms
step:667/1840 train_time:24015ms step_avg:36.00ms
step:668/1840 train_time:24077ms step_avg:36.04ms
step:669/1840 train_time:24137ms step_avg:36.08ms
step:670/1840 train_time:24200ms step_avg:36.12ms
step:671/1840 train_time:24259ms step_avg:36.15ms
step:672/1840 train_time:24322ms step_avg:36.19ms
step:673/1840 train_time:24381ms step_avg:36.23ms
step:674/1840 train_time:24444ms step_avg:36.27ms
step:675/1840 train_time:24504ms step_avg:36.30ms
step:676/1840 train_time:24565ms step_avg:36.34ms
step:677/1840 train_time:24626ms step_avg:36.38ms
step:678/1840 train_time:24689ms step_avg:36.41ms
step:679/1840 train_time:24749ms step_avg:36.45ms
step:680/1840 train_time:24812ms step_avg:36.49ms
step:681/1840 train_time:24873ms step_avg:36.52ms
step:682/1840 train_time:24935ms step_avg:36.56ms
step:683/1840 train_time:24996ms step_avg:36.60ms
step:684/1840 train_time:25058ms step_avg:36.63ms
step:685/1840 train_time:25118ms step_avg:36.67ms
step:686/1840 train_time:25180ms step_avg:36.71ms
step:687/1840 train_time:25240ms step_avg:36.74ms
step:688/1840 train_time:25301ms step_avg:36.78ms
step:689/1840 train_time:25361ms step_avg:36.81ms
step:690/1840 train_time:25424ms step_avg:36.85ms
step:691/1840 train_time:25483ms step_avg:36.88ms
step:692/1840 train_time:25546ms step_avg:36.92ms
step:693/1840 train_time:25606ms step_avg:36.95ms
step:694/1840 train_time:25668ms step_avg:36.99ms
step:695/1840 train_time:25728ms step_avg:37.02ms
step:696/1840 train_time:25791ms step_avg:37.06ms
step:697/1840 train_time:25851ms step_avg:37.09ms
step:698/1840 train_time:25913ms step_avg:37.12ms
step:699/1840 train_time:25974ms step_avg:37.16ms
step:700/1840 train_time:26036ms step_avg:37.19ms
step:701/1840 train_time:26097ms step_avg:37.23ms
step:702/1840 train_time:26159ms step_avg:37.26ms
step:703/1840 train_time:26219ms step_avg:37.30ms
step:704/1840 train_time:26282ms step_avg:37.33ms
step:705/1840 train_time:26342ms step_avg:37.36ms
step:706/1840 train_time:26403ms step_avg:37.40ms
step:707/1840 train_time:26462ms step_avg:37.43ms
step:708/1840 train_time:26525ms step_avg:37.47ms
step:709/1840 train_time:26585ms step_avg:37.50ms
step:710/1840 train_time:26647ms step_avg:37.53ms
step:711/1840 train_time:26707ms step_avg:37.56ms
step:712/1840 train_time:26770ms step_avg:37.60ms
step:713/1840 train_time:26830ms step_avg:37.63ms
step:714/1840 train_time:26892ms step_avg:37.66ms
step:715/1840 train_time:26953ms step_avg:37.70ms
step:716/1840 train_time:27016ms step_avg:37.73ms
step:717/1840 train_time:27076ms step_avg:37.76ms
step:718/1840 train_time:27138ms step_avg:37.80ms
step:719/1840 train_time:27197ms step_avg:37.83ms
step:720/1840 train_time:27259ms step_avg:37.86ms
step:721/1840 train_time:27319ms step_avg:37.89ms
step:722/1840 train_time:27382ms step_avg:37.93ms
step:723/1840 train_time:27442ms step_avg:37.96ms
step:724/1840 train_time:27505ms step_avg:37.99ms
step:725/1840 train_time:27564ms step_avg:38.02ms
step:726/1840 train_time:27626ms step_avg:38.05ms
step:727/1840 train_time:27686ms step_avg:38.08ms
step:728/1840 train_time:27748ms step_avg:38.12ms
step:729/1840 train_time:27808ms step_avg:38.15ms
step:730/1840 train_time:27871ms step_avg:38.18ms
step:731/1840 train_time:27933ms step_avg:38.21ms
step:732/1840 train_time:27995ms step_avg:38.25ms
step:733/1840 train_time:28056ms step_avg:38.28ms
step:734/1840 train_time:28118ms step_avg:38.31ms
step:735/1840 train_time:28178ms step_avg:38.34ms
step:736/1840 train_time:28240ms step_avg:38.37ms
step:737/1840 train_time:28301ms step_avg:38.40ms
step:738/1840 train_time:28363ms step_avg:38.43ms
step:739/1840 train_time:28423ms step_avg:38.46ms
step:740/1840 train_time:28485ms step_avg:38.49ms
step:741/1840 train_time:28544ms step_avg:38.52ms
step:742/1840 train_time:28607ms step_avg:38.55ms
step:743/1840 train_time:28666ms step_avg:38.58ms
step:744/1840 train_time:28728ms step_avg:38.61ms
step:745/1840 train_time:28788ms step_avg:38.64ms
step:746/1840 train_time:28850ms step_avg:38.67ms
step:747/1840 train_time:28910ms step_avg:38.70ms
step:748/1840 train_time:28973ms step_avg:38.73ms
step:749/1840 train_time:29034ms step_avg:38.76ms
step:750/1840 train_time:29096ms step_avg:38.79ms
step:750/1840 val_loss:4.0140 train_time:29168ms step_avg:38.89ms
step:751/1840 train_time:29189ms step_avg:38.87ms
step:752/1840 train_time:29222ms step_avg:38.86ms
step:753/1840 train_time:29283ms step_avg:38.89ms
step:754/1840 train_time:29348ms step_avg:38.92ms
step:755/1840 train_time:29411ms step_avg:38.95ms
step:756/1840 train_time:29473ms step_avg:38.98ms
step:757/1840 train_time:29532ms step_avg:39.01ms
step:758/1840 train_time:29593ms step_avg:39.04ms
step:759/1840 train_time:29652ms step_avg:39.07ms
step:760/1840 train_time:29713ms step_avg:39.10ms
step:761/1840 train_time:29772ms step_avg:39.12ms
step:762/1840 train_time:29834ms step_avg:39.15ms
step:763/1840 train_time:29892ms step_avg:39.18ms
step:764/1840 train_time:29953ms step_avg:39.21ms
step:765/1840 train_time:30012ms step_avg:39.23ms
step:766/1840 train_time:30074ms step_avg:39.26ms
step:767/1840 train_time:30135ms step_avg:39.29ms
step:768/1840 train_time:30200ms step_avg:39.32ms
step:769/1840 train_time:30262ms step_avg:39.35ms
step:770/1840 train_time:30326ms step_avg:39.38ms
step:771/1840 train_time:30386ms step_avg:39.41ms
step:772/1840 train_time:30448ms step_avg:39.44ms
step:773/1840 train_time:30508ms step_avg:39.47ms
step:774/1840 train_time:30570ms step_avg:39.50ms
step:775/1840 train_time:30629ms step_avg:39.52ms
step:776/1840 train_time:30691ms step_avg:39.55ms
step:777/1840 train_time:30750ms step_avg:39.58ms
step:778/1840 train_time:30812ms step_avg:39.60ms
step:779/1840 train_time:30871ms step_avg:39.63ms
step:780/1840 train_time:30933ms step_avg:39.66ms
step:781/1840 train_time:30991ms step_avg:39.68ms
step:782/1840 train_time:31054ms step_avg:39.71ms
step:783/1840 train_time:31113ms step_avg:39.74ms
step:784/1840 train_time:31177ms step_avg:39.77ms
step:785/1840 train_time:31239ms step_avg:39.80ms
step:786/1840 train_time:31302ms step_avg:39.82ms
step:787/1840 train_time:31363ms step_avg:39.85ms
step:788/1840 train_time:31426ms step_avg:39.88ms
step:789/1840 train_time:31486ms step_avg:39.91ms
step:790/1840 train_time:31548ms step_avg:39.93ms
step:791/1840 train_time:31608ms step_avg:39.96ms
step:792/1840 train_time:31670ms step_avg:39.99ms
step:793/1840 train_time:31729ms step_avg:40.01ms
step:794/1840 train_time:31791ms step_avg:40.04ms
step:795/1840 train_time:31850ms step_avg:40.06ms
step:796/1840 train_time:31913ms step_avg:40.09ms
step:797/1840 train_time:31972ms step_avg:40.12ms
step:798/1840 train_time:32034ms step_avg:40.14ms
step:799/1840 train_time:32093ms step_avg:40.17ms
step:800/1840 train_time:32156ms step_avg:40.19ms
step:801/1840 train_time:32216ms step_avg:40.22ms
step:802/1840 train_time:32280ms step_avg:40.25ms
step:803/1840 train_time:32341ms step_avg:40.28ms
step:804/1840 train_time:32403ms step_avg:40.30ms
step:805/1840 train_time:32463ms step_avg:40.33ms
step:806/1840 train_time:32526ms step_avg:40.35ms
step:807/1840 train_time:32586ms step_avg:40.38ms
step:808/1840 train_time:32647ms step_avg:40.40ms
step:809/1840 train_time:32707ms step_avg:40.43ms
step:810/1840 train_time:32769ms step_avg:40.46ms
step:811/1840 train_time:32828ms step_avg:40.48ms
step:812/1840 train_time:32890ms step_avg:40.50ms
step:813/1840 train_time:32950ms step_avg:40.53ms
step:814/1840 train_time:33012ms step_avg:40.56ms
step:815/1840 train_time:33072ms step_avg:40.58ms
step:816/1840 train_time:33135ms step_avg:40.61ms
step:817/1840 train_time:33194ms step_avg:40.63ms
step:818/1840 train_time:33257ms step_avg:40.66ms
step:819/1840 train_time:33317ms step_avg:40.68ms
step:820/1840 train_time:33380ms step_avg:40.71ms
step:821/1840 train_time:33440ms step_avg:40.73ms
step:822/1840 train_time:33503ms step_avg:40.76ms
step:823/1840 train_time:33563ms step_avg:40.78ms
step:824/1840 train_time:33627ms step_avg:40.81ms
step:825/1840 train_time:33686ms step_avg:40.83ms
step:826/1840 train_time:33748ms step_avg:40.86ms
step:827/1840 train_time:33807ms step_avg:40.88ms
step:828/1840 train_time:33869ms step_avg:40.91ms
step:829/1840 train_time:33930ms step_avg:40.93ms
step:830/1840 train_time:33991ms step_avg:40.95ms
step:831/1840 train_time:34052ms step_avg:40.98ms
step:832/1840 train_time:34115ms step_avg:41.00ms
step:833/1840 train_time:34174ms step_avg:41.03ms
step:834/1840 train_time:34237ms step_avg:41.05ms
step:835/1840 train_time:34297ms step_avg:41.07ms
step:836/1840 train_time:34360ms step_avg:41.10ms
step:837/1840 train_time:34419ms step_avg:41.12ms
step:838/1840 train_time:34482ms step_avg:41.15ms
step:839/1840 train_time:34542ms step_avg:41.17ms
step:840/1840 train_time:34605ms step_avg:41.20ms
step:841/1840 train_time:34665ms step_avg:41.22ms
step:842/1840 train_time:34727ms step_avg:41.24ms
step:843/1840 train_time:34787ms step_avg:41.27ms
step:844/1840 train_time:34849ms step_avg:41.29ms
step:845/1840 train_time:34910ms step_avg:41.31ms
step:846/1840 train_time:34971ms step_avg:41.34ms
step:847/1840 train_time:35032ms step_avg:41.36ms
step:848/1840 train_time:35093ms step_avg:41.38ms
step:849/1840 train_time:35153ms step_avg:41.41ms
step:850/1840 train_time:35215ms step_avg:41.43ms
step:851/1840 train_time:35275ms step_avg:41.45ms
step:852/1840 train_time:35337ms step_avg:41.48ms
step:853/1840 train_time:35397ms step_avg:41.50ms
step:854/1840 train_time:35460ms step_avg:41.52ms
step:855/1840 train_time:35520ms step_avg:41.54ms
step:856/1840 train_time:35582ms step_avg:41.57ms
step:857/1840 train_time:35643ms step_avg:41.59ms
step:858/1840 train_time:35705ms step_avg:41.61ms
step:859/1840 train_time:35765ms step_avg:41.64ms
step:860/1840 train_time:35828ms step_avg:41.66ms
step:861/1840 train_time:35888ms step_avg:41.68ms
step:862/1840 train_time:35950ms step_avg:41.71ms
step:863/1840 train_time:36010ms step_avg:41.73ms
step:864/1840 train_time:36072ms step_avg:41.75ms
step:865/1840 train_time:36132ms step_avg:41.77ms
step:866/1840 train_time:36194ms step_avg:41.79ms
step:867/1840 train_time:36254ms step_avg:41.82ms
step:868/1840 train_time:36317ms step_avg:41.84ms
step:869/1840 train_time:36376ms step_avg:41.86ms
step:870/1840 train_time:36438ms step_avg:41.88ms
step:871/1840 train_time:36498ms step_avg:41.90ms
step:872/1840 train_time:36560ms step_avg:41.93ms
step:873/1840 train_time:36620ms step_avg:41.95ms
step:874/1840 train_time:36684ms step_avg:41.97ms
step:875/1840 train_time:36745ms step_avg:41.99ms
step:876/1840 train_time:36808ms step_avg:42.02ms
step:877/1840 train_time:36867ms step_avg:42.04ms
step:878/1840 train_time:36929ms step_avg:42.06ms
step:879/1840 train_time:36989ms step_avg:42.08ms
step:880/1840 train_time:37051ms step_avg:42.10ms
step:881/1840 train_time:37111ms step_avg:42.12ms
step:882/1840 train_time:37173ms step_avg:42.15ms
step:883/1840 train_time:37233ms step_avg:42.17ms
step:884/1840 train_time:37295ms step_avg:42.19ms
step:885/1840 train_time:37355ms step_avg:42.21ms
step:886/1840 train_time:37417ms step_avg:42.23ms
step:887/1840 train_time:37477ms step_avg:42.25ms
step:888/1840 train_time:37540ms step_avg:42.27ms
step:889/1840 train_time:37600ms step_avg:42.30ms
step:890/1840 train_time:37663ms step_avg:42.32ms
step:891/1840 train_time:37724ms step_avg:42.34ms
step:892/1840 train_time:37786ms step_avg:42.36ms
step:893/1840 train_time:37846ms step_avg:42.38ms
step:894/1840 train_time:37908ms step_avg:42.40ms
step:895/1840 train_time:37968ms step_avg:42.42ms
step:896/1840 train_time:38030ms step_avg:42.44ms
step:897/1840 train_time:38091ms step_avg:42.46ms
step:898/1840 train_time:38152ms step_avg:42.49ms
step:899/1840 train_time:38211ms step_avg:42.50ms
step:900/1840 train_time:38274ms step_avg:42.53ms
step:901/1840 train_time:38334ms step_avg:42.55ms
step:902/1840 train_time:38397ms step_avg:42.57ms
step:903/1840 train_time:38457ms step_avg:42.59ms
step:904/1840 train_time:38519ms step_avg:42.61ms
step:905/1840 train_time:38578ms step_avg:42.63ms
step:906/1840 train_time:38641ms step_avg:42.65ms
step:907/1840 train_time:38702ms step_avg:42.67ms
step:908/1840 train_time:38764ms step_avg:42.69ms
step:909/1840 train_time:38824ms step_avg:42.71ms
step:910/1840 train_time:38888ms step_avg:42.73ms
step:911/1840 train_time:38948ms step_avg:42.75ms
step:912/1840 train_time:39010ms step_avg:42.77ms
step:913/1840 train_time:39069ms step_avg:42.79ms
step:914/1840 train_time:39132ms step_avg:42.81ms
step:915/1840 train_time:39192ms step_avg:42.83ms
step:916/1840 train_time:39254ms step_avg:42.85ms
step:917/1840 train_time:39313ms step_avg:42.87ms
step:918/1840 train_time:39375ms step_avg:42.89ms
step:919/1840 train_time:39435ms step_avg:42.91ms
step:920/1840 train_time:39497ms step_avg:42.93ms
step:921/1840 train_time:39558ms step_avg:42.95ms
step:922/1840 train_time:39620ms step_avg:42.97ms
step:923/1840 train_time:39680ms step_avg:42.99ms
step:924/1840 train_time:39743ms step_avg:43.01ms
step:925/1840 train_time:39803ms step_avg:43.03ms
step:926/1840 train_time:39866ms step_avg:43.05ms
step:927/1840 train_time:39925ms step_avg:43.07ms
step:928/1840 train_time:39988ms step_avg:43.09ms
step:929/1840 train_time:40047ms step_avg:43.11ms
step:930/1840 train_time:40110ms step_avg:43.13ms
step:931/1840 train_time:40169ms step_avg:43.15ms
step:932/1840 train_time:40231ms step_avg:43.17ms
step:933/1840 train_time:40290ms step_avg:43.18ms
step:934/1840 train_time:40353ms step_avg:43.20ms
step:935/1840 train_time:40413ms step_avg:43.22ms
step:936/1840 train_time:40476ms step_avg:43.24ms
step:937/1840 train_time:40536ms step_avg:43.26ms
step:938/1840 train_time:40598ms step_avg:43.28ms
step:939/1840 train_time:40658ms step_avg:43.30ms
step:940/1840 train_time:40722ms step_avg:43.32ms
step:941/1840 train_time:40781ms step_avg:43.34ms
step:942/1840 train_time:40845ms step_avg:43.36ms
step:943/1840 train_time:40905ms step_avg:43.38ms
step:944/1840 train_time:40968ms step_avg:43.40ms
step:945/1840 train_time:41028ms step_avg:43.42ms
step:946/1840 train_time:41089ms step_avg:43.43ms
step:947/1840 train_time:41149ms step_avg:43.45ms
step:948/1840 train_time:41211ms step_avg:43.47ms
step:949/1840 train_time:41271ms step_avg:43.49ms
step:950/1840 train_time:41332ms step_avg:43.51ms
step:951/1840 train_time:41392ms step_avg:43.52ms
step:952/1840 train_time:41454ms step_avg:43.54ms
step:953/1840 train_time:41514ms step_avg:43.56ms
step:954/1840 train_time:41576ms step_avg:43.58ms
step:955/1840 train_time:41636ms step_avg:43.60ms
step:956/1840 train_time:41699ms step_avg:43.62ms
step:957/1840 train_time:41758ms step_avg:43.63ms
step:958/1840 train_time:41821ms step_avg:43.65ms
step:959/1840 train_time:41882ms step_avg:43.67ms
step:960/1840 train_time:41945ms step_avg:43.69ms
step:961/1840 train_time:42006ms step_avg:43.71ms
step:962/1840 train_time:42068ms step_avg:43.73ms
step:963/1840 train_time:42127ms step_avg:43.75ms
step:964/1840 train_time:42189ms step_avg:43.76ms
step:965/1840 train_time:42249ms step_avg:43.78ms
step:966/1840 train_time:42312ms step_avg:43.80ms
step:967/1840 train_time:42371ms step_avg:43.82ms
step:968/1840 train_time:42433ms step_avg:43.84ms
step:969/1840 train_time:42493ms step_avg:43.85ms
step:970/1840 train_time:42555ms step_avg:43.87ms
step:971/1840 train_time:42615ms step_avg:43.89ms
step:972/1840 train_time:42677ms step_avg:43.91ms
step:973/1840 train_time:42737ms step_avg:43.92ms
step:974/1840 train_time:42799ms step_avg:43.94ms
step:975/1840 train_time:42861ms step_avg:43.96ms
step:976/1840 train_time:42924ms step_avg:43.98ms
step:977/1840 train_time:42985ms step_avg:44.00ms
step:978/1840 train_time:43046ms step_avg:44.01ms
step:979/1840 train_time:43107ms step_avg:44.03ms
step:980/1840 train_time:43168ms step_avg:44.05ms
step:981/1840 train_time:43228ms step_avg:44.07ms
step:982/1840 train_time:43290ms step_avg:44.08ms
step:983/1840 train_time:43350ms step_avg:44.10ms
step:984/1840 train_time:43412ms step_avg:44.12ms
step:985/1840 train_time:43471ms step_avg:44.13ms
step:986/1840 train_time:43534ms step_avg:44.15ms
step:987/1840 train_time:43593ms step_avg:44.17ms
step:988/1840 train_time:43656ms step_avg:44.19ms
step:989/1840 train_time:43715ms step_avg:44.20ms
step:990/1840 train_time:43778ms step_avg:44.22ms
step:991/1840 train_time:43839ms step_avg:44.24ms
step:992/1840 train_time:43902ms step_avg:44.26ms
step:993/1840 train_time:43963ms step_avg:44.27ms
step:994/1840 train_time:44026ms step_avg:44.29ms
step:995/1840 train_time:44087ms step_avg:44.31ms
step:996/1840 train_time:44148ms step_avg:44.33ms
step:997/1840 train_time:44208ms step_avg:44.34ms
step:998/1840 train_time:44270ms step_avg:44.36ms
step:999/1840 train_time:44330ms step_avg:44.37ms
step:1000/1840 train_time:44392ms step_avg:44.39ms
step:1000/1840 val_loss:3.7744 train_time:44463ms step_avg:44.46ms
step:1001/1840 train_time:44482ms step_avg:44.44ms
step:1002/1840 train_time:44516ms step_avg:44.43ms
step:1003/1840 train_time:44578ms step_avg:44.45ms
step:1004/1840 train_time:44641ms step_avg:44.46ms
step:1005/1840 train_time:44703ms step_avg:44.48ms
step:1006/1840 train_time:44765ms step_avg:44.50ms
step:1007/1840 train_time:44823ms step_avg:44.51ms
step:1008/1840 train_time:44885ms step_avg:44.53ms
step:1009/1840 train_time:44944ms step_avg:44.54ms
step:1010/1840 train_time:45005ms step_avg:44.56ms
step:1011/1840 train_time:45065ms step_avg:44.57ms
step:1012/1840 train_time:45127ms step_avg:44.59ms
step:1013/1840 train_time:45187ms step_avg:44.61ms
step:1014/1840 train_time:45249ms step_avg:44.62ms
step:1015/1840 train_time:45309ms step_avg:44.64ms
step:1016/1840 train_time:45372ms step_avg:44.66ms
step:1017/1840 train_time:45432ms step_avg:44.67ms
step:1018/1840 train_time:45495ms step_avg:44.69ms
step:1019/1840 train_time:45555ms step_avg:44.71ms
step:1020/1840 train_time:45618ms step_avg:44.72ms
step:1021/1840 train_time:45678ms step_avg:44.74ms
step:1022/1840 train_time:45741ms step_avg:44.76ms
step:1023/1840 train_time:45801ms step_avg:44.77ms
step:1024/1840 train_time:45863ms step_avg:44.79ms
step:1025/1840 train_time:45922ms step_avg:44.80ms
step:1026/1840 train_time:45984ms step_avg:44.82ms
step:1027/1840 train_time:46042ms step_avg:44.83ms
step:1028/1840 train_time:46104ms step_avg:44.85ms
step:1029/1840 train_time:46163ms step_avg:44.86ms
step:1030/1840 train_time:46225ms step_avg:44.88ms
step:1031/1840 train_time:46285ms step_avg:44.89ms
step:1032/1840 train_time:46348ms step_avg:44.91ms
step:1033/1840 train_time:46408ms step_avg:44.93ms
step:1034/1840 train_time:46473ms step_avg:44.94ms
step:1035/1840 train_time:46534ms step_avg:44.96ms
step:1036/1840 train_time:46597ms step_avg:44.98ms
step:1037/1840 train_time:46657ms step_avg:44.99ms
step:1038/1840 train_time:46719ms step_avg:45.01ms
step:1039/1840 train_time:46778ms step_avg:45.02ms
step:1040/1840 train_time:46840ms step_avg:45.04ms
step:1041/1840 train_time:46900ms step_avg:45.05ms
step:1042/1840 train_time:46962ms step_avg:45.07ms
step:1043/1840 train_time:47020ms step_avg:45.08ms
step:1044/1840 train_time:47082ms step_avg:45.10ms
step:1045/1840 train_time:47141ms step_avg:45.11ms
step:1046/1840 train_time:47204ms step_avg:45.13ms
step:1047/1840 train_time:47263ms step_avg:45.14ms
step:1048/1840 train_time:47326ms step_avg:45.16ms
step:1049/1840 train_time:47386ms step_avg:45.17ms
step:1050/1840 train_time:47450ms step_avg:45.19ms
step:1051/1840 train_time:47510ms step_avg:45.20ms
step:1052/1840 train_time:47574ms step_avg:45.22ms
step:1053/1840 train_time:47634ms step_avg:45.24ms
step:1054/1840 train_time:47696ms step_avg:45.25ms
step:1055/1840 train_time:47756ms step_avg:45.27ms
step:1056/1840 train_time:47818ms step_avg:45.28ms
step:1057/1840 train_time:47878ms step_avg:45.30ms
step:1058/1840 train_time:47940ms step_avg:45.31ms
step:1059/1840 train_time:47999ms step_avg:45.33ms
step:1060/1840 train_time:48061ms step_avg:45.34ms
step:1061/1840 train_time:48120ms step_avg:45.35ms
step:1062/1840 train_time:48182ms step_avg:45.37ms
step:1063/1840 train_time:48242ms step_avg:45.38ms
step:1064/1840 train_time:48304ms step_avg:45.40ms
step:1065/1840 train_time:48364ms step_avg:45.41ms
step:1066/1840 train_time:48427ms step_avg:45.43ms
step:1067/1840 train_time:48488ms step_avg:45.44ms
step:1068/1840 train_time:48552ms step_avg:45.46ms
step:1069/1840 train_time:48613ms step_avg:45.48ms
step:1070/1840 train_time:48675ms step_avg:45.49ms
step:1071/1840 train_time:48735ms step_avg:45.50ms
step:1072/1840 train_time:48797ms step_avg:45.52ms
step:1073/1840 train_time:48857ms step_avg:45.53ms
step:1074/1840 train_time:48918ms step_avg:45.55ms
step:1075/1840 train_time:48978ms step_avg:45.56ms
step:1076/1840 train_time:49040ms step_avg:45.58ms
step:1077/1840 train_time:49100ms step_avg:45.59ms
step:1078/1840 train_time:49162ms step_avg:45.60ms
step:1079/1840 train_time:49221ms step_avg:45.62ms
step:1080/1840 train_time:49283ms step_avg:45.63ms
step:1081/1840 train_time:49343ms step_avg:45.65ms
step:1082/1840 train_time:49405ms step_avg:45.66ms
step:1083/1840 train_time:49466ms step_avg:45.68ms
step:1084/1840 train_time:49529ms step_avg:45.69ms
step:1085/1840 train_time:49589ms step_avg:45.70ms
step:1086/1840 train_time:49651ms step_avg:45.72ms
step:1087/1840 train_time:49712ms step_avg:45.73ms
step:1088/1840 train_time:49775ms step_avg:45.75ms
step:1089/1840 train_time:49835ms step_avg:45.76ms
step:1090/1840 train_time:49896ms step_avg:45.78ms
step:1091/1840 train_time:49956ms step_avg:45.79ms
step:1092/1840 train_time:50018ms step_avg:45.80ms
step:1093/1840 train_time:50079ms step_avg:45.82ms
step:1094/1840 train_time:50140ms step_avg:45.83ms
step:1095/1840 train_time:50200ms step_avg:45.84ms
step:1096/1840 train_time:50262ms step_avg:45.86ms
step:1097/1840 train_time:50321ms step_avg:45.87ms
step:1098/1840 train_time:50385ms step_avg:45.89ms
step:1099/1840 train_time:50444ms step_avg:45.90ms
step:1100/1840 train_time:50507ms step_avg:45.92ms
step:1101/1840 train_time:50567ms step_avg:45.93ms
step:1102/1840 train_time:50629ms step_avg:45.94ms
step:1103/1840 train_time:50690ms step_avg:45.96ms
step:1104/1840 train_time:50753ms step_avg:45.97ms
step:1105/1840 train_time:50813ms step_avg:45.98ms
step:1106/1840 train_time:50876ms step_avg:46.00ms
step:1107/1840 train_time:50936ms step_avg:46.01ms
step:1108/1840 train_time:50998ms step_avg:46.03ms
step:1109/1840 train_time:51057ms step_avg:46.04ms
step:1110/1840 train_time:51120ms step_avg:46.05ms
step:1111/1840 train_time:51179ms step_avg:46.07ms
step:1112/1840 train_time:51242ms step_avg:46.08ms
step:1113/1840 train_time:51301ms step_avg:46.09ms
step:1114/1840 train_time:51364ms step_avg:46.11ms
step:1115/1840 train_time:51424ms step_avg:46.12ms
step:1116/1840 train_time:51485ms step_avg:46.13ms
step:1117/1840 train_time:51545ms step_avg:46.15ms
step:1118/1840 train_time:51609ms step_avg:46.16ms
step:1119/1840 train_time:51668ms step_avg:46.17ms
step:1120/1840 train_time:51730ms step_avg:46.19ms
step:1121/1840 train_time:51791ms step_avg:46.20ms
step:1122/1840 train_time:51854ms step_avg:46.22ms
step:1123/1840 train_time:51915ms step_avg:46.23ms
step:1124/1840 train_time:51976ms step_avg:46.24ms
step:1125/1840 train_time:52037ms step_avg:46.26ms
step:1126/1840 train_time:52099ms step_avg:46.27ms
step:1127/1840 train_time:52158ms step_avg:46.28ms
step:1128/1840 train_time:52219ms step_avg:46.29ms
step:1129/1840 train_time:52279ms step_avg:46.31ms
step:1130/1840 train_time:52341ms step_avg:46.32ms
step:1131/1840 train_time:52401ms step_avg:46.33ms
step:1132/1840 train_time:52464ms step_avg:46.35ms
step:1133/1840 train_time:52523ms step_avg:46.36ms
step:1134/1840 train_time:52585ms step_avg:46.37ms
step:1135/1840 train_time:52645ms step_avg:46.38ms
step:1136/1840 train_time:52707ms step_avg:46.40ms
step:1137/1840 train_time:52767ms step_avg:46.41ms
step:1138/1840 train_time:52830ms step_avg:46.42ms
step:1139/1840 train_time:52891ms step_avg:46.44ms
step:1140/1840 train_time:52954ms step_avg:46.45ms
step:1141/1840 train_time:53014ms step_avg:46.46ms
step:1142/1840 train_time:53077ms step_avg:46.48ms
step:1143/1840 train_time:53137ms step_avg:46.49ms
step:1144/1840 train_time:53198ms step_avg:46.50ms
step:1145/1840 train_time:53258ms step_avg:46.51ms
step:1146/1840 train_time:53320ms step_avg:46.53ms
step:1147/1840 train_time:53379ms step_avg:46.54ms
step:1148/1840 train_time:53442ms step_avg:46.55ms
step:1149/1840 train_time:53501ms step_avg:46.56ms
step:1150/1840 train_time:53564ms step_avg:46.58ms
step:1151/1840 train_time:53623ms step_avg:46.59ms
step:1152/1840 train_time:53685ms step_avg:46.60ms
step:1153/1840 train_time:53746ms step_avg:46.61ms
step:1154/1840 train_time:53808ms step_avg:46.63ms
step:1155/1840 train_time:53869ms step_avg:46.64ms
step:1156/1840 train_time:53932ms step_avg:46.65ms
step:1157/1840 train_time:53993ms step_avg:46.67ms
step:1158/1840 train_time:54056ms step_avg:46.68ms
step:1159/1840 train_time:54116ms step_avg:46.69ms
step:1160/1840 train_time:54178ms step_avg:46.70ms
step:1161/1840 train_time:54237ms step_avg:46.72ms
step:1162/1840 train_time:54299ms step_avg:46.73ms
step:1163/1840 train_time:54359ms step_avg:46.74ms
step:1164/1840 train_time:54420ms step_avg:46.75ms
step:1165/1840 train_time:54480ms step_avg:46.76ms
step:1166/1840 train_time:54542ms step_avg:46.78ms
step:1167/1840 train_time:54601ms step_avg:46.79ms
step:1168/1840 train_time:54665ms step_avg:46.80ms
step:1169/1840 train_time:54724ms step_avg:46.81ms
step:1170/1840 train_time:54787ms step_avg:46.83ms
step:1171/1840 train_time:54847ms step_avg:46.84ms
step:1172/1840 train_time:54910ms step_avg:46.85ms
step:1173/1840 train_time:54972ms step_avg:46.86ms
step:1174/1840 train_time:55034ms step_avg:46.88ms
step:1175/1840 train_time:55095ms step_avg:46.89ms
step:1176/1840 train_time:55157ms step_avg:46.90ms
step:1177/1840 train_time:55217ms step_avg:46.91ms
step:1178/1840 train_time:55278ms step_avg:46.93ms
step:1179/1840 train_time:55338ms step_avg:46.94ms
step:1180/1840 train_time:55401ms step_avg:46.95ms
step:1181/1840 train_time:55460ms step_avg:46.96ms
step:1182/1840 train_time:55521ms step_avg:46.97ms
step:1183/1840 train_time:55582ms step_avg:46.98ms
step:1184/1840 train_time:55643ms step_avg:47.00ms
step:1185/1840 train_time:55702ms step_avg:47.01ms
step:1186/1840 train_time:55766ms step_avg:47.02ms
step:1187/1840 train_time:55826ms step_avg:47.03ms
step:1188/1840 train_time:55889ms step_avg:47.04ms
step:1189/1840 train_time:55950ms step_avg:47.06ms
step:1190/1840 train_time:56013ms step_avg:47.07ms
step:1191/1840 train_time:56073ms step_avg:47.08ms
step:1192/1840 train_time:56135ms step_avg:47.09ms
step:1193/1840 train_time:56195ms step_avg:47.10ms
step:1194/1840 train_time:56257ms step_avg:47.12ms
step:1195/1840 train_time:56317ms step_avg:47.13ms
step:1196/1840 train_time:56379ms step_avg:47.14ms
step:1197/1840 train_time:56438ms step_avg:47.15ms
step:1198/1840 train_time:56500ms step_avg:47.16ms
step:1199/1840 train_time:56560ms step_avg:47.17ms
step:1200/1840 train_time:56622ms step_avg:47.18ms
step:1201/1840 train_time:56683ms step_avg:47.20ms
step:1202/1840 train_time:56767ms step_avg:47.23ms
step:1203/1840 train_time:56856ms step_avg:47.26ms
step:1204/1840 train_time:56947ms step_avg:47.30ms
step:1205/1840 train_time:57034ms step_avg:47.33ms
step:1206/1840 train_time:57125ms step_avg:47.37ms
step:1207/1840 train_time:57212ms step_avg:47.40ms
step:1208/1840 train_time:57301ms step_avg:47.43ms
step:1209/1840 train_time:57389ms step_avg:47.47ms
step:1210/1840 train_time:57478ms step_avg:47.50ms
step:1211/1840 train_time:57565ms step_avg:47.53ms
step:1212/1840 train_time:57654ms step_avg:47.57ms
step:1213/1840 train_time:57740ms step_avg:47.60ms
step:1214/1840 train_time:57828ms step_avg:47.63ms
step:1215/1840 train_time:57916ms step_avg:47.67ms
step:1216/1840 train_time:58005ms step_avg:47.70ms
step:1217/1840 train_time:58092ms step_avg:47.73ms
step:1218/1840 train_time:58181ms step_avg:47.77ms
step:1219/1840 train_time:58267ms step_avg:47.80ms
step:1220/1840 train_time:58356ms step_avg:47.83ms
step:1221/1840 train_time:58443ms step_avg:47.87ms
step:1222/1840 train_time:58531ms step_avg:47.90ms
step:1223/1840 train_time:58617ms step_avg:47.93ms
step:1224/1840 train_time:58705ms step_avg:47.96ms
step:1225/1840 train_time:58792ms step_avg:47.99ms
step:1226/1840 train_time:58881ms step_avg:48.03ms
step:1227/1840 train_time:58967ms step_avg:48.06ms
step:1228/1840 train_time:59056ms step_avg:48.09ms
step:1229/1840 train_time:59144ms step_avg:48.12ms
step:1230/1840 train_time:59231ms step_avg:48.16ms
step:1231/1840 train_time:59318ms step_avg:48.19ms
step:1232/1840 train_time:59407ms step_avg:48.22ms
step:1233/1840 train_time:59493ms step_avg:48.25ms
step:1234/1840 train_time:59583ms step_avg:48.28ms
step:1235/1840 train_time:59668ms step_avg:48.31ms
step:1236/1840 train_time:59756ms step_avg:48.35ms
step:1237/1840 train_time:59843ms step_avg:48.38ms
step:1238/1840 train_time:59931ms step_avg:48.41ms
step:1239/1840 train_time:60017ms step_avg:48.44ms
step:1240/1840 train_time:60107ms step_avg:48.47ms
step:1241/1840 train_time:60192ms step_avg:48.50ms
step:1242/1840 train_time:60281ms step_avg:48.54ms
step:1243/1840 train_time:60367ms step_avg:48.57ms
step:1244/1840 train_time:60457ms step_avg:48.60ms
step:1245/1840 train_time:60543ms step_avg:48.63ms
step:1246/1840 train_time:60632ms step_avg:48.66ms
step:1247/1840 train_time:60720ms step_avg:48.69ms
step:1248/1840 train_time:60807ms step_avg:48.72ms
step:1249/1840 train_time:60892ms step_avg:48.75ms
step:1250/1840 train_time:60983ms step_avg:48.79ms
step:1250/1840 val_loss:3.5321 train_time:61083ms step_avg:48.87ms
step:1251/1840 train_time:61102ms step_avg:48.84ms
step:1252/1840 train_time:61160ms step_avg:48.85ms
step:1253/1840 train_time:61252ms step_avg:48.88ms
step:1254/1840 train_time:61343ms step_avg:48.92ms
step:1255/1840 train_time:61428ms step_avg:48.95ms
step:1256/1840 train_time:61517ms step_avg:48.98ms
step:1257/1840 train_time:61603ms step_avg:49.01ms
step:1258/1840 train_time:61690ms step_avg:49.04ms
step:1259/1840 train_time:61777ms step_avg:49.07ms
step:1260/1840 train_time:61864ms step_avg:49.10ms
step:1261/1840 train_time:61950ms step_avg:49.13ms
step:1262/1840 train_time:62038ms step_avg:49.16ms
step:1263/1840 train_time:62129ms step_avg:49.19ms
step:1264/1840 train_time:62220ms step_avg:49.22ms
step:1265/1840 train_time:62307ms step_avg:49.25ms
step:1266/1840 train_time:62396ms step_avg:49.29ms
step:1267/1840 train_time:62481ms step_avg:49.31ms
step:1268/1840 train_time:62570ms step_avg:49.35ms
step:1269/1840 train_time:62656ms step_avg:49.37ms
step:1270/1840 train_time:62745ms step_avg:49.41ms
step:1271/1840 train_time:62830ms step_avg:49.43ms
step:1272/1840 train_time:62917ms step_avg:49.46ms
step:1273/1840 train_time:63004ms step_avg:49.49ms
step:1274/1840 train_time:63092ms step_avg:49.52ms
step:1275/1840 train_time:63181ms step_avg:49.55ms
step:1276/1840 train_time:63270ms step_avg:49.58ms
step:1277/1840 train_time:63356ms step_avg:49.61ms
step:1278/1840 train_time:63446ms step_avg:49.64ms
step:1279/1840 train_time:63531ms step_avg:49.67ms
step:1280/1840 train_time:63619ms step_avg:49.70ms
step:1281/1840 train_time:63706ms step_avg:49.73ms
step:1282/1840 train_time:63793ms step_avg:49.76ms
step:1283/1840 train_time:63879ms step_avg:49.79ms
step:1284/1840 train_time:63967ms step_avg:49.82ms
step:1285/1840 train_time:64053ms step_avg:49.85ms
step:1286/1840 train_time:64146ms step_avg:49.88ms
step:1287/1840 train_time:64232ms step_avg:49.91ms
step:1288/1840 train_time:64323ms step_avg:49.94ms
step:1289/1840 train_time:64408ms step_avg:49.97ms
step:1290/1840 train_time:64497ms step_avg:50.00ms
step:1291/1840 train_time:64583ms step_avg:50.03ms
step:1292/1840 train_time:64670ms step_avg:50.05ms
step:1293/1840 train_time:64757ms step_avg:50.08ms
step:1294/1840 train_time:64845ms step_avg:50.11ms
step:1295/1840 train_time:64930ms step_avg:50.14ms
step:1296/1840 train_time:65020ms step_avg:50.17ms
step:1297/1840 train_time:65107ms step_avg:50.20ms
step:1298/1840 train_time:65197ms step_avg:50.23ms
step:1299/1840 train_time:65284ms step_avg:50.26ms
step:1300/1840 train_time:65372ms step_avg:50.29ms
step:1301/1840 train_time:65459ms step_avg:50.31ms
step:1302/1840 train_time:65548ms step_avg:50.34ms
step:1303/1840 train_time:65633ms step_avg:50.37ms
step:1304/1840 train_time:65722ms step_avg:50.40ms
step:1305/1840 train_time:65808ms step_avg:50.43ms
step:1306/1840 train_time:65896ms step_avg:50.46ms
step:1307/1840 train_time:65982ms step_avg:50.48ms
step:1308/1840 train_time:66071ms step_avg:50.51ms
step:1309/1840 train_time:66158ms step_avg:50.54ms
step:1310/1840 train_time:66248ms step_avg:50.57ms
step:1311/1840 train_time:66334ms step_avg:50.60ms
step:1312/1840 train_time:66423ms step_avg:50.63ms
step:1313/1840 train_time:66510ms step_avg:50.65ms
step:1314/1840 train_time:66599ms step_avg:50.68ms
step:1315/1840 train_time:66685ms step_avg:50.71ms
step:1316/1840 train_time:66773ms step_avg:50.74ms
step:1317/1840 train_time:66859ms step_avg:50.77ms
step:1318/1840 train_time:66947ms step_avg:50.79ms
step:1319/1840 train_time:67032ms step_avg:50.82ms
step:1320/1840 train_time:67124ms step_avg:50.85ms
step:1321/1840 train_time:67210ms step_avg:50.88ms
step:1322/1840 train_time:67299ms step_avg:50.91ms
step:1323/1840 train_time:67386ms step_avg:50.93ms
step:1324/1840 train_time:67474ms step_avg:50.96ms
step:1325/1840 train_time:67559ms step_avg:50.99ms
step:1326/1840 train_time:67648ms step_avg:51.02ms
step:1327/1840 train_time:67734ms step_avg:51.04ms
step:1328/1840 train_time:67823ms step_avg:51.07ms
step:1329/1840 train_time:67910ms step_avg:51.10ms
step:1330/1840 train_time:67999ms step_avg:51.13ms
step:1331/1840 train_time:68086ms step_avg:51.15ms
step:1332/1840 train_time:68175ms step_avg:51.18ms
step:1333/1840 train_time:68262ms step_avg:51.21ms
step:1334/1840 train_time:68350ms step_avg:51.24ms
step:1335/1840 train_time:68436ms step_avg:51.26ms
step:1336/1840 train_time:68524ms step_avg:51.29ms
step:1337/1840 train_time:68612ms step_avg:51.32ms
step:1338/1840 train_time:68703ms step_avg:51.35ms
step:1339/1840 train_time:68789ms step_avg:51.37ms
step:1340/1840 train_time:68876ms step_avg:51.40ms
step:1341/1840 train_time:68962ms step_avg:51.43ms
step:1342/1840 train_time:69050ms step_avg:51.45ms
step:1343/1840 train_time:69138ms step_avg:51.48ms
step:1344/1840 train_time:69227ms step_avg:51.51ms
step:1345/1840 train_time:69312ms step_avg:51.53ms
step:1346/1840 train_time:69402ms step_avg:51.56ms
step:1347/1840 train_time:69488ms step_avg:51.59ms
step:1348/1840 train_time:69576ms step_avg:51.61ms
step:1349/1840 train_time:69663ms step_avg:51.64ms
step:1350/1840 train_time:69750ms step_avg:51.67ms
step:1351/1840 train_time:69837ms step_avg:51.69ms
step:1352/1840 train_time:69926ms step_avg:51.72ms
step:1353/1840 train_time:70011ms step_avg:51.75ms
step:1354/1840 train_time:70100ms step_avg:51.77ms
step:1355/1840 train_time:70186ms step_avg:51.80ms
step:1356/1840 train_time:70275ms step_avg:51.83ms
step:1357/1840 train_time:70361ms step_avg:51.85ms
step:1358/1840 train_time:70449ms step_avg:51.88ms
step:1359/1840 train_time:70535ms step_avg:51.90ms
step:1360/1840 train_time:70624ms step_avg:51.93ms
step:1361/1840 train_time:70710ms step_avg:51.95ms
step:1362/1840 train_time:70799ms step_avg:51.98ms
step:1363/1840 train_time:70886ms step_avg:52.01ms
step:1364/1840 train_time:70974ms step_avg:52.03ms
step:1365/1840 train_time:71060ms step_avg:52.06ms
step:1366/1840 train_time:71149ms step_avg:52.09ms
step:1367/1840 train_time:71235ms step_avg:52.11ms
step:1368/1840 train_time:71325ms step_avg:52.14ms
step:1369/1840 train_time:71410ms step_avg:52.16ms
step:1370/1840 train_time:71498ms step_avg:52.19ms
step:1371/1840 train_time:71585ms step_avg:52.21ms
step:1372/1840 train_time:71674ms step_avg:52.24ms
step:1373/1840 train_time:71760ms step_avg:52.26ms
step:1374/1840 train_time:71849ms step_avg:52.29ms
step:1375/1840 train_time:71935ms step_avg:52.32ms
step:1376/1840 train_time:72024ms step_avg:52.34ms
step:1377/1840 train_time:72109ms step_avg:52.37ms
step:1378/1840 train_time:72199ms step_avg:52.39ms
step:1379/1840 train_time:72285ms step_avg:52.42ms
step:1380/1840 train_time:72374ms step_avg:52.44ms
step:1381/1840 train_time:72460ms step_avg:52.47ms
step:1382/1840 train_time:72549ms step_avg:52.50ms
step:1383/1840 train_time:72635ms step_avg:52.52ms
step:1384/1840 train_time:72724ms step_avg:52.55ms
step:1385/1840 train_time:72812ms step_avg:52.57ms
step:1386/1840 train_time:72902ms step_avg:52.60ms
step:1387/1840 train_time:72987ms step_avg:52.62ms
step:1388/1840 train_time:73076ms step_avg:52.65ms
step:1389/1840 train_time:73163ms step_avg:52.67ms
step:1390/1840 train_time:73250ms step_avg:52.70ms
step:1391/1840 train_time:73337ms step_avg:52.72ms
step:1392/1840 train_time:73426ms step_avg:52.75ms
step:1393/1840 train_time:73512ms step_avg:52.77ms
step:1394/1840 train_time:73600ms step_avg:52.80ms
step:1395/1840 train_time:73686ms step_avg:52.82ms
step:1396/1840 train_time:73775ms step_avg:52.85ms
step:1397/1840 train_time:73861ms step_avg:52.87ms
step:1398/1840 train_time:73949ms step_avg:52.90ms
step:1399/1840 train_time:74035ms step_avg:52.92ms
step:1400/1840 train_time:74124ms step_avg:52.95ms
step:1401/1840 train_time:74210ms step_avg:52.97ms
step:1402/1840 train_time:74299ms step_avg:53.00ms
step:1403/1840 train_time:74386ms step_avg:53.02ms
step:1404/1840 train_time:74473ms step_avg:53.04ms
step:1405/1840 train_time:74560ms step_avg:53.07ms
step:1406/1840 train_time:74649ms step_avg:53.09ms
step:1407/1840 train_time:74734ms step_avg:53.12ms
step:1408/1840 train_time:74824ms step_avg:53.14ms
step:1409/1840 train_time:74911ms step_avg:53.17ms
step:1410/1840 train_time:75000ms step_avg:53.19ms
step:1411/1840 train_time:75086ms step_avg:53.21ms
step:1412/1840 train_time:75175ms step_avg:53.24ms
step:1413/1840 train_time:75261ms step_avg:53.26ms
step:1414/1840 train_time:75350ms step_avg:53.29ms
step:1415/1840 train_time:75437ms step_avg:53.31ms
step:1416/1840 train_time:75526ms step_avg:53.34ms
step:1417/1840 train_time:75614ms step_avg:53.36ms
step:1418/1840 train_time:75702ms step_avg:53.39ms
step:1419/1840 train_time:75787ms step_avg:53.41ms
step:1420/1840 train_time:75877ms step_avg:53.43ms
step:1421/1840 train_time:75963ms step_avg:53.46ms
step:1422/1840 train_time:76050ms step_avg:53.48ms
step:1423/1840 train_time:76136ms step_avg:53.50ms
step:1424/1840 train_time:76226ms step_avg:53.53ms
step:1425/1840 train_time:76312ms step_avg:53.55ms
step:1426/1840 train_time:76401ms step_avg:53.58ms
step:1427/1840 train_time:76486ms step_avg:53.60ms
step:1428/1840 train_time:76577ms step_avg:53.63ms
step:1429/1840 train_time:76664ms step_avg:53.65ms
step:1430/1840 train_time:76751ms step_avg:53.67ms
step:1431/1840 train_time:76838ms step_avg:53.70ms
step:1432/1840 train_time:76927ms step_avg:53.72ms
step:1433/1840 train_time:77013ms step_avg:53.74ms
step:1434/1840 train_time:77103ms step_avg:53.77ms
step:1435/1840 train_time:77189ms step_avg:53.79ms
step:1436/1840 train_time:77277ms step_avg:53.81ms
step:1437/1840 train_time:77363ms step_avg:53.84ms
step:1438/1840 train_time:77450ms step_avg:53.86ms
step:1439/1840 train_time:77537ms step_avg:53.88ms
step:1440/1840 train_time:77626ms step_avg:53.91ms
step:1441/1840 train_time:77712ms step_avg:53.93ms
step:1442/1840 train_time:77800ms step_avg:53.95ms
step:1443/1840 train_time:77887ms step_avg:53.98ms
step:1444/1840 train_time:77976ms step_avg:54.00ms
step:1445/1840 train_time:78063ms step_avg:54.02ms
step:1446/1840 train_time:78150ms step_avg:54.05ms
step:1447/1840 train_time:78237ms step_avg:54.07ms
step:1448/1840 train_time:78326ms step_avg:54.09ms
step:1449/1840 train_time:78412ms step_avg:54.11ms
step:1450/1840 train_time:78502ms step_avg:54.14ms
step:1451/1840 train_time:78588ms step_avg:54.16ms
step:1452/1840 train_time:78676ms step_avg:54.18ms
step:1453/1840 train_time:78763ms step_avg:54.21ms
step:1454/1840 train_time:78852ms step_avg:54.23ms
step:1455/1840 train_time:78939ms step_avg:54.25ms
step:1456/1840 train_time:79028ms step_avg:54.28ms
step:1457/1840 train_time:79113ms step_avg:54.30ms
step:1458/1840 train_time:79204ms step_avg:54.32ms
step:1459/1840 train_time:79289ms step_avg:54.34ms
step:1460/1840 train_time:79377ms step_avg:54.37ms
step:1461/1840 train_time:79464ms step_avg:54.39ms
step:1462/1840 train_time:79551ms step_avg:54.41ms
step:1463/1840 train_time:79637ms step_avg:54.43ms
step:1464/1840 train_time:79726ms step_avg:54.46ms
step:1465/1840 train_time:79812ms step_avg:54.48ms
step:1466/1840 train_time:79901ms step_avg:54.50ms
step:1467/1840 train_time:79987ms step_avg:54.52ms
step:1468/1840 train_time:80077ms step_avg:54.55ms
step:1469/1840 train_time:80164ms step_avg:54.57ms
step:1470/1840 train_time:80251ms step_avg:54.59ms
step:1471/1840 train_time:80336ms step_avg:54.61ms
step:1472/1840 train_time:80427ms step_avg:54.64ms
step:1473/1840 train_time:80513ms step_avg:54.66ms
step:1474/1840 train_time:80602ms step_avg:54.68ms
step:1475/1840 train_time:80688ms step_avg:54.70ms
step:1476/1840 train_time:80777ms step_avg:54.73ms
step:1477/1840 train_time:80863ms step_avg:54.75ms
step:1478/1840 train_time:80950ms step_avg:54.77ms
step:1479/1840 train_time:81037ms step_avg:54.79ms
step:1480/1840 train_time:81127ms step_avg:54.82ms
step:1481/1840 train_time:81213ms step_avg:54.84ms
step:1482/1840 train_time:81303ms step_avg:54.86ms
step:1483/1840 train_time:81388ms step_avg:54.88ms
step:1484/1840 train_time:81476ms step_avg:54.90ms
step:1485/1840 train_time:81562ms step_avg:54.92ms
step:1486/1840 train_time:81650ms step_avg:54.95ms
step:1487/1840 train_time:81736ms step_avg:54.97ms
step:1488/1840 train_time:81826ms step_avg:54.99ms
step:1489/1840 train_time:81913ms step_avg:55.01ms
step:1490/1840 train_time:82002ms step_avg:55.04ms
step:1491/1840 train_time:82087ms step_avg:55.06ms
step:1492/1840 train_time:82177ms step_avg:55.08ms
step:1493/1840 train_time:82264ms step_avg:55.10ms
step:1494/1840 train_time:82351ms step_avg:55.12ms
step:1495/1840 train_time:82437ms step_avg:55.14ms
step:1496/1840 train_time:82526ms step_avg:55.16ms
step:1497/1840 train_time:82612ms step_avg:55.19ms
step:1498/1840 train_time:82702ms step_avg:55.21ms
step:1499/1840 train_time:82787ms step_avg:55.23ms
step:1500/1840 train_time:82877ms step_avg:55.25ms
step:1500/1840 val_loss:3.3997 train_time:82976ms step_avg:55.32ms
step:1501/1840 train_time:82999ms step_avg:55.30ms
step:1502/1840 train_time:83055ms step_avg:55.30ms
step:1503/1840 train_time:83146ms step_avg:55.32ms
step:1504/1840 train_time:83234ms step_avg:55.34ms
step:1505/1840 train_time:83320ms step_avg:55.36ms
step:1506/1840 train_time:83408ms step_avg:55.38ms
step:1507/1840 train_time:83493ms step_avg:55.40ms
step:1508/1840 train_time:83580ms step_avg:55.42ms
step:1509/1840 train_time:83666ms step_avg:55.44ms
step:1510/1840 train_time:83754ms step_avg:55.47ms
step:1511/1840 train_time:83838ms step_avg:55.49ms
step:1512/1840 train_time:83929ms step_avg:55.51ms
step:1513/1840 train_time:84018ms step_avg:55.53ms
step:1514/1840 train_time:84109ms step_avg:55.55ms
step:1515/1840 train_time:84198ms step_avg:55.58ms
step:1516/1840 train_time:84286ms step_avg:55.60ms
step:1517/1840 train_time:84373ms step_avg:55.62ms
step:1518/1840 train_time:84462ms step_avg:55.64ms
step:1519/1840 train_time:84548ms step_avg:55.66ms
step:1520/1840 train_time:84636ms step_avg:55.68ms
step:1521/1840 train_time:84721ms step_avg:55.70ms
step:1522/1840 train_time:84810ms step_avg:55.72ms
step:1523/1840 train_time:84897ms step_avg:55.74ms
step:1524/1840 train_time:84987ms step_avg:55.77ms
step:1525/1840 train_time:85073ms step_avg:55.79ms
step:1526/1840 train_time:85165ms step_avg:55.81ms
step:1527/1840 train_time:85252ms step_avg:55.83ms
step:1528/1840 train_time:85342ms step_avg:55.85ms
step:1529/1840 train_time:85428ms step_avg:55.87ms
step:1530/1840 train_time:85516ms step_avg:55.89ms
step:1531/1840 train_time:85602ms step_avg:55.91ms
step:1532/1840 train_time:85689ms step_avg:55.93ms
step:1533/1840 train_time:85774ms step_avg:55.95ms
step:1534/1840 train_time:85863ms step_avg:55.97ms
step:1535/1840 train_time:85949ms step_avg:55.99ms
step:1536/1840 train_time:86039ms step_avg:56.02ms
step:1537/1840 train_time:86126ms step_avg:56.04ms
step:1538/1840 train_time:86216ms step_avg:56.06ms
step:1539/1840 train_time:86303ms step_avg:56.08ms
step:1540/1840 train_time:86391ms step_avg:56.10ms
step:1541/1840 train_time:86477ms step_avg:56.12ms
step:1542/1840 train_time:86565ms step_avg:56.14ms
step:1543/1840 train_time:86650ms step_avg:56.16ms
step:1544/1840 train_time:86740ms step_avg:56.18ms
step:1545/1840 train_time:86826ms step_avg:56.20ms
step:1546/1840 train_time:86915ms step_avg:56.22ms
step:1547/1840 train_time:87002ms step_avg:56.24ms
step:1548/1840 train_time:87090ms step_avg:56.26ms
step:1549/1840 train_time:87177ms step_avg:56.28ms
step:1550/1840 train_time:87267ms step_avg:56.30ms
step:1551/1840 train_time:87353ms step_avg:56.32ms
step:1552/1840 train_time:87441ms step_avg:56.34ms
step:1553/1840 train_time:87528ms step_avg:56.36ms
step:1554/1840 train_time:87616ms step_avg:56.38ms
step:1555/1840 train_time:87701ms step_avg:56.40ms
step:1556/1840 train_time:87789ms step_avg:56.42ms
step:1557/1840 train_time:87876ms step_avg:56.44ms
step:1558/1840 train_time:87965ms step_avg:56.46ms
step:1559/1840 train_time:88051ms step_avg:56.48ms
step:1560/1840 train_time:88140ms step_avg:56.50ms
step:1561/1840 train_time:88228ms step_avg:56.52ms
step:1562/1840 train_time:88318ms step_avg:56.54ms
step:1563/1840 train_time:88403ms step_avg:56.56ms
step:1564/1840 train_time:88491ms step_avg:56.58ms
step:1565/1840 train_time:88577ms step_avg:56.60ms
step:1566/1840 train_time:88666ms step_avg:56.62ms
step:1567/1840 train_time:88751ms step_avg:56.64ms
step:1568/1840 train_time:88840ms step_avg:56.66ms
step:1569/1840 train_time:88927ms step_avg:56.68ms
step:1570/1840 train_time:89016ms step_avg:56.70ms
step:1571/1840 train_time:89103ms step_avg:56.72ms
step:1572/1840 train_time:89192ms step_avg:56.74ms
step:1573/1840 train_time:89277ms step_avg:56.76ms
step:1574/1840 train_time:89367ms step_avg:56.78ms
step:1575/1840 train_time:89453ms step_avg:56.80ms
step:1576/1840 train_time:89544ms step_avg:56.82ms
step:1577/1840 train_time:89630ms step_avg:56.84ms
step:1578/1840 train_time:89718ms step_avg:56.86ms
step:1579/1840 train_time:89803ms step_avg:56.87ms
step:1580/1840 train_time:89891ms step_avg:56.89ms
step:1581/1840 train_time:89978ms step_avg:56.91ms
step:1582/1840 train_time:90067ms step_avg:56.93ms
step:1583/1840 train_time:90155ms step_avg:56.95ms
step:1584/1840 train_time:90244ms step_avg:56.97ms
step:1585/1840 train_time:90330ms step_avg:56.99ms
step:1586/1840 train_time:90420ms step_avg:57.01ms
step:1587/1840 train_time:90505ms step_avg:57.03ms
step:1588/1840 train_time:90594ms step_avg:57.05ms
step:1589/1840 train_time:90680ms step_avg:57.07ms
step:1590/1840 train_time:90769ms step_avg:57.09ms
step:1591/1840 train_time:90854ms step_avg:57.10ms
step:1592/1840 train_time:90943ms step_avg:57.12ms
step:1593/1840 train_time:91029ms step_avg:57.14ms
step:1594/1840 train_time:91119ms step_avg:57.16ms
step:1595/1840 train_time:91205ms step_avg:57.18ms
step:1596/1840 train_time:91294ms step_avg:57.20ms
step:1597/1840 train_time:91380ms step_avg:57.22ms
step:1598/1840 train_time:91468ms step_avg:57.24ms
step:1599/1840 train_time:91555ms step_avg:57.26ms
step:1600/1840 train_time:91644ms step_avg:57.28ms
step:1601/1840 train_time:91729ms step_avg:57.29ms
step:1602/1840 train_time:91818ms step_avg:57.31ms
step:1603/1840 train_time:91905ms step_avg:57.33ms
step:1604/1840 train_time:91993ms step_avg:57.35ms
step:1605/1840 train_time:92080ms step_avg:57.37ms
step:1606/1840 train_time:92169ms step_avg:57.39ms
step:1607/1840 train_time:92255ms step_avg:57.41ms
step:1608/1840 train_time:92344ms step_avg:57.43ms
step:1609/1840 train_time:92429ms step_avg:57.44ms
step:1610/1840 train_time:92518ms step_avg:57.46ms
step:1611/1840 train_time:92603ms step_avg:57.48ms
step:1612/1840 train_time:92691ms step_avg:57.50ms
step:1613/1840 train_time:92777ms step_avg:57.52ms
step:1614/1840 train_time:92867ms step_avg:57.54ms
step:1615/1840 train_time:92953ms step_avg:57.56ms
step:1616/1840 train_time:93042ms step_avg:57.58ms
step:1617/1840 train_time:93130ms step_avg:57.59ms
step:1618/1840 train_time:93218ms step_avg:57.61ms
step:1619/1840 train_time:93305ms step_avg:57.63ms
step:1620/1840 train_time:93394ms step_avg:57.65ms
step:1621/1840 train_time:93479ms step_avg:57.67ms
step:1622/1840 train_time:93568ms step_avg:57.69ms
step:1623/1840 train_time:93654ms step_avg:57.70ms
step:1624/1840 train_time:93742ms step_avg:57.72ms
step:1625/1840 train_time:93829ms step_avg:57.74ms
step:1626/1840 train_time:93916ms step_avg:57.76ms
step:1627/1840 train_time:94002ms step_avg:57.78ms
step:1628/1840 train_time:94090ms step_avg:57.79ms
step:1629/1840 train_time:94177ms step_avg:57.81ms
step:1630/1840 train_time:94267ms step_avg:57.83ms
step:1631/1840 train_time:94352ms step_avg:57.85ms
step:1632/1840 train_time:94441ms step_avg:57.87ms
step:1633/1840 train_time:94527ms step_avg:57.89ms
step:1634/1840 train_time:94616ms step_avg:57.90ms
step:1635/1840 train_time:94702ms step_avg:57.92ms
step:1636/1840 train_time:94790ms step_avg:57.94ms
step:1637/1840 train_time:94876ms step_avg:57.96ms
step:1638/1840 train_time:94966ms step_avg:57.98ms
step:1639/1840 train_time:95052ms step_avg:57.99ms
step:1640/1840 train_time:95140ms step_avg:58.01ms
step:1641/1840 train_time:95228ms step_avg:58.03ms
step:1642/1840 train_time:95316ms step_avg:58.05ms
step:1643/1840 train_time:95401ms step_avg:58.07ms
step:1644/1840 train_time:95489ms step_avg:58.08ms
step:1645/1840 train_time:95575ms step_avg:58.10ms
step:1646/1840 train_time:95664ms step_avg:58.12ms
step:1647/1840 train_time:95749ms step_avg:58.14ms
step:1648/1840 train_time:95839ms step_avg:58.15ms
step:1649/1840 train_time:95926ms step_avg:58.17ms
step:1650/1840 train_time:96016ms step_avg:58.19ms
step:1651/1840 train_time:96102ms step_avg:58.21ms
step:1652/1840 train_time:96191ms step_avg:58.23ms
step:1653/1840 train_time:96277ms step_avg:58.24ms
step:1654/1840 train_time:96367ms step_avg:58.26ms
step:1655/1840 train_time:96452ms step_avg:58.28ms
step:1656/1840 train_time:96542ms step_avg:58.30ms
step:1657/1840 train_time:96629ms step_avg:58.32ms
step:1658/1840 train_time:96718ms step_avg:58.33ms
step:1659/1840 train_time:96804ms step_avg:58.35ms
step:1660/1840 train_time:96892ms step_avg:58.37ms
step:1661/1840 train_time:96978ms step_avg:58.39ms
step:1662/1840 train_time:97067ms step_avg:58.40ms
step:1663/1840 train_time:97153ms step_avg:58.42ms
step:1664/1840 train_time:97242ms step_avg:58.44ms
step:1665/1840 train_time:97328ms step_avg:58.46ms
step:1666/1840 train_time:97418ms step_avg:58.47ms
step:1667/1840 train_time:97504ms step_avg:58.49ms
step:1668/1840 train_time:97592ms step_avg:58.51ms
step:1669/1840 train_time:97679ms step_avg:58.53ms
step:1670/1840 train_time:97768ms step_avg:58.54ms
step:1671/1840 train_time:97853ms step_avg:58.56ms
step:1672/1840 train_time:97942ms step_avg:58.58ms
step:1673/1840 train_time:98027ms step_avg:58.59ms
step:1674/1840 train_time:98117ms step_avg:58.61ms
step:1675/1840 train_time:98204ms step_avg:58.63ms
step:1676/1840 train_time:98293ms step_avg:58.65ms
step:1677/1840 train_time:98379ms step_avg:58.66ms
step:1678/1840 train_time:98468ms step_avg:58.68ms
step:1679/1840 train_time:98553ms step_avg:58.70ms
step:1680/1840 train_time:98643ms step_avg:58.72ms
step:1681/1840 train_time:98730ms step_avg:58.73ms
step:1682/1840 train_time:98818ms step_avg:58.75ms
step:1683/1840 train_time:98903ms step_avg:58.77ms
step:1684/1840 train_time:98991ms step_avg:58.78ms
step:1685/1840 train_time:99078ms step_avg:58.80ms
step:1686/1840 train_time:99168ms step_avg:58.82ms
step:1687/1840 train_time:99254ms step_avg:58.83ms
step:1688/1840 train_time:99343ms step_avg:58.85ms
step:1689/1840 train_time:99430ms step_avg:58.87ms
step:1690/1840 train_time:99518ms step_avg:58.89ms
step:1691/1840 train_time:99605ms step_avg:58.90ms
step:1692/1840 train_time:99695ms step_avg:58.92ms
step:1693/1840 train_time:99781ms step_avg:58.94ms
step:1694/1840 train_time:99869ms step_avg:58.95ms
step:1695/1840 train_time:99954ms step_avg:58.97ms
step:1696/1840 train_time:100044ms step_avg:58.99ms
step:1697/1840 train_time:100130ms step_avg:59.00ms
step:1698/1840 train_time:100220ms step_avg:59.02ms
step:1699/1840 train_time:100306ms step_avg:59.04ms
step:1700/1840 train_time:100394ms step_avg:59.06ms
step:1701/1840 train_time:100480ms step_avg:59.07ms
step:1702/1840 train_time:100570ms step_avg:59.09ms
step:1703/1840 train_time:100655ms step_avg:59.10ms
step:1704/1840 train_time:100745ms step_avg:59.12ms
step:1705/1840 train_time:100831ms step_avg:59.14ms
step:1706/1840 train_time:100918ms step_avg:59.16ms
step:1707/1840 train_time:101005ms step_avg:59.17ms
step:1708/1840 train_time:101093ms step_avg:59.19ms
step:1709/1840 train_time:101179ms step_avg:59.20ms
step:1710/1840 train_time:101269ms step_avg:59.22ms
step:1711/1840 train_time:101354ms step_avg:59.24ms
step:1712/1840 train_time:101444ms step_avg:59.25ms
step:1713/1840 train_time:101530ms step_avg:59.27ms
step:1714/1840 train_time:101618ms step_avg:59.29ms
step:1715/1840 train_time:101706ms step_avg:59.30ms
step:1716/1840 train_time:101794ms step_avg:59.32ms
step:1717/1840 train_time:101880ms step_avg:59.34ms
step:1718/1840 train_time:101969ms step_avg:59.35ms
step:1719/1840 train_time:102056ms step_avg:59.37ms
step:1720/1840 train_time:102146ms step_avg:59.39ms
step:1721/1840 train_time:102231ms step_avg:59.40ms
step:1722/1840 train_time:102320ms step_avg:59.42ms
step:1723/1840 train_time:102407ms step_avg:59.44ms
step:1724/1840 train_time:102496ms step_avg:59.45ms
step:1725/1840 train_time:102582ms step_avg:59.47ms
step:1726/1840 train_time:102670ms step_avg:59.48ms
step:1727/1840 train_time:102756ms step_avg:59.50ms
step:1728/1840 train_time:102845ms step_avg:59.52ms
step:1729/1840 train_time:102931ms step_avg:59.53ms
step:1730/1840 train_time:103020ms step_avg:59.55ms
step:1731/1840 train_time:103107ms step_avg:59.56ms
step:1732/1840 train_time:103196ms step_avg:59.58ms
step:1733/1840 train_time:103282ms step_avg:59.60ms
step:1734/1840 train_time:103370ms step_avg:59.61ms
step:1735/1840 train_time:103457ms step_avg:59.63ms
step:1736/1840 train_time:103547ms step_avg:59.65ms
step:1737/1840 train_time:103632ms step_avg:59.66ms
step:1738/1840 train_time:103721ms step_avg:59.68ms
step:1739/1840 train_time:103808ms step_avg:59.69ms
step:1740/1840 train_time:103897ms step_avg:59.71ms
step:1741/1840 train_time:103982ms step_avg:59.73ms
step:1742/1840 train_time:104071ms step_avg:59.74ms
step:1743/1840 train_time:104157ms step_avg:59.76ms
step:1744/1840 train_time:104246ms step_avg:59.77ms
step:1745/1840 train_time:104332ms step_avg:59.79ms
step:1746/1840 train_time:104420ms step_avg:59.81ms
step:1747/1840 train_time:104506ms step_avg:59.82ms
step:1748/1840 train_time:104595ms step_avg:59.84ms
step:1749/1840 train_time:104681ms step_avg:59.85ms
step:1750/1840 train_time:104769ms step_avg:59.87ms
step:1750/1840 val_loss:3.3015 train_time:104870ms step_avg:59.93ms
step:1751/1840 train_time:104892ms step_avg:59.90ms
step:1752/1840 train_time:104950ms step_avg:59.90ms
step:1753/1840 train_time:105039ms step_avg:59.92ms
step:1754/1840 train_time:105127ms step_avg:59.94ms
step:1755/1840 train_time:105213ms step_avg:59.95ms
step:1756/1840 train_time:105302ms step_avg:59.97ms
step:1757/1840 train_time:105387ms step_avg:59.98ms
step:1758/1840 train_time:105474ms step_avg:60.00ms
step:1759/1840 train_time:105560ms step_avg:60.01ms
step:1760/1840 train_time:105646ms step_avg:60.03ms
step:1761/1840 train_time:105732ms step_avg:60.04ms
step:1762/1840 train_time:105825ms step_avg:60.06ms
step:1763/1840 train_time:105913ms step_avg:60.08ms
step:1764/1840 train_time:106006ms step_avg:60.09ms
step:1765/1840 train_time:106092ms step_avg:60.11ms
step:1766/1840 train_time:106182ms step_avg:60.13ms
step:1767/1840 train_time:106268ms step_avg:60.14ms
step:1768/1840 train_time:106356ms step_avg:60.16ms
step:1769/1840 train_time:106441ms step_avg:60.17ms
step:1770/1840 train_time:106529ms step_avg:60.19ms
step:1771/1840 train_time:106613ms step_avg:60.20ms
step:1772/1840 train_time:106701ms step_avg:60.22ms
step:1773/1840 train_time:106789ms step_avg:60.23ms
step:1774/1840 train_time:106881ms step_avg:60.25ms
step:1775/1840 train_time:106968ms step_avg:60.26ms
step:1776/1840 train_time:107057ms step_avg:60.28ms
step:1777/1840 train_time:107144ms step_avg:60.29ms
step:1778/1840 train_time:107231ms step_avg:60.31ms
step:1779/1840 train_time:107318ms step_avg:60.32ms
step:1780/1840 train_time:107405ms step_avg:60.34ms
step:1781/1840 train_time:107490ms step_avg:60.35ms
step:1782/1840 train_time:107578ms step_avg:60.37ms
step:1783/1840 train_time:107665ms step_avg:60.38ms
step:1784/1840 train_time:107755ms step_avg:60.40ms
step:1785/1840 train_time:107842ms step_avg:60.42ms
step:1786/1840 train_time:107930ms step_avg:60.43ms
step:1787/1840 train_time:108018ms step_avg:60.45ms
step:1788/1840 train_time:108107ms step_avg:60.46ms
step:1789/1840 train_time:108193ms step_avg:60.48ms
step:1790/1840 train_time:108281ms step_avg:60.49ms
step:1791/1840 train_time:108367ms step_avg:60.51ms
step:1792/1840 train_time:108455ms step_avg:60.52ms
step:1793/1840 train_time:108542ms step_avg:60.54ms
step:1794/1840 train_time:108629ms step_avg:60.55ms
step:1795/1840 train_time:108715ms step_avg:60.57ms
step:1796/1840 train_time:108804ms step_avg:60.58ms
step:1797/1840 train_time:108892ms step_avg:60.60ms
step:1798/1840 train_time:108981ms step_avg:60.61ms
step:1799/1840 train_time:109068ms step_avg:60.63ms
step:1800/1840 train_time:109158ms step_avg:60.64ms
step:1801/1840 train_time:109247ms step_avg:60.66ms
step:1802/1840 train_time:109334ms step_avg:60.67ms
step:1803/1840 train_time:109421ms step_avg:60.69ms
step:1804/1840 train_time:109509ms step_avg:60.70ms
step:1805/1840 train_time:109594ms step_avg:60.72ms
step:1806/1840 train_time:109684ms step_avg:60.73ms
step:1807/1840 train_time:109770ms step_avg:60.75ms
step:1808/1840 train_time:109859ms step_avg:60.76ms
step:1809/1840 train_time:109947ms step_avg:60.78ms
step:1810/1840 train_time:110036ms step_avg:60.79ms
step:1811/1840 train_time:110124ms step_avg:60.81ms
step:1812/1840 train_time:110213ms step_avg:60.82ms
step:1813/1840 train_time:110300ms step_avg:60.84ms
step:1814/1840 train_time:110388ms step_avg:60.85ms
step:1815/1840 train_time:110473ms step_avg:60.87ms
step:1816/1840 train_time:110563ms step_avg:60.88ms
step:1817/1840 train_time:110648ms step_avg:60.90ms
step:1818/1840 train_time:110738ms step_avg:60.91ms
step:1819/1840 train_time:110824ms step_avg:60.93ms
step:1820/1840 train_time:110914ms step_avg:60.94ms
step:1821/1840 train_time:111000ms step_avg:60.96ms
step:1822/1840 train_time:111089ms step_avg:60.97ms
step:1823/1840 train_time:111175ms step_avg:60.98ms
step:1824/1840 train_time:111265ms step_avg:61.00ms
step:1825/1840 train_time:111351ms step_avg:61.01ms
step:1826/1840 train_time:111440ms step_avg:61.03ms
step:1827/1840 train_time:111527ms step_avg:61.04ms
step:1828/1840 train_time:111615ms step_avg:61.06ms
step:1829/1840 train_time:111701ms step_avg:61.07ms
step:1830/1840 train_time:111790ms step_avg:61.09ms
step:1831/1840 train_time:111876ms step_avg:61.10ms
step:1832/1840 train_time:111967ms step_avg:61.12ms
step:1833/1840 train_time:112053ms step_avg:61.13ms
step:1834/1840 train_time:112143ms step_avg:61.15ms
step:1835/1840 train_time:112229ms step_avg:61.16ms
step:1836/1840 train_time:112318ms step_avg:61.18ms
step:1837/1840 train_time:112404ms step_avg:61.19ms
step:1838/1840 train_time:112492ms step_avg:61.20ms
step:1839/1840 train_time:112579ms step_avg:61.22ms
step:1840/1840 train_time:112667ms step_avg:61.23ms
step:1840/1840 val_loss:3.2763 train_time:112767ms step_avg:61.29ms
peak memory allocated: 28507 MiB reserved: 43898 MiB

import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from collections import defaultdict
from itertools import accumulate
from pathlib import Path
import gc

os.environ["PYTORCH_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device=f"cuda:{os.environ['LOCAL_RANK']}", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng

@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor, split_baddbmm: bool = False):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    # Select batched vs unbatched
    if split_baddbmm:
        BX_matmul = torch.bmm if X.ndim > 2 else torch.mm
    else:
        aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A

        # Referencing X twice causes pytorch to make a defensive copy,
        # resulting in a cudaMemcpyAsync in baddbmm.
        # For large matrices (i.e., the mlp weights), it's faster to split
        # the operation into two kernels to avoid this.
        if split_baddbmm:
            BX_matmul(B, X, out=C)  # C = B @ X
            C.add_(X, alpha=a)      # C = C + a*X  (in-place, X only read)
        else:
            aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X

        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X


# -----------------------------------------------------------------------------
# Compiled helpers for NorMuon by @chrisjmccormick

@torch.compile(dynamic=False, fullgraph=True)
def cautious_wd_and_update_inplace(p, mantissa, grad, wd_tensor, lr_tensor):
    """
    Cautious weight decay + parameter update. wd_tensor and lr_tensor are 0-D CPU tensors.
    Mantissa is tracked to enable higher precision updates on bfloat16 parameters.
    bfloat16 format: 1 sign bit + 8 exponent bits + 7 mantissa bits = 16 bits total
    float32 format: 1 sign bit + 8 exponent bits + 23 mantissa bits = 32 bits total
    """
    assert p.dtype == mantissa.dtype == torch.uint16
    grad = grad.float()
    wd_factor = wd_tensor.to(torch.float32)
    lr_factor = lr_tensor.to(torch.float32)
    p_precise_raw = (p.to(torch.uint32) << 16) | mantissa.to(torch.uint32)
    p_precise = p_precise_raw.view(torch.float32)
    mask = (grad * p_precise) >= 0
    p_precise.copy_(p_precise - (p_precise * mask * wd_factor * lr_factor) - (grad * lr_factor))
    p.copy_((p_precise_raw >> 16).to(torch.uint16))
    mantissa.copy_(p_precise_raw.to(torch.uint16))

@torch.compile(dynamic=False, fullgraph=True)
def apply_normuon_variance_reduction(v_chunk, second_momentum_buffer, beta2, red_dim):
    """NorMuon variance reduction. Algebraically fuses the normalization steps to minimize memory ops."""
    v_mean = v_chunk.float().square().mean(dim=red_dim, keepdim=True)
    red_dim_size = v_chunk.size(red_dim)
    v_norm_sq = v_mean.sum(dim=(-2, -1), keepdim=True).mul_(red_dim_size)
    v_norm = v_norm_sq.sqrt_()
    second_momentum_buffer.lerp_(v_mean.to(dtype=second_momentum_buffer.dtype), 1 - beta2)
    step_size = second_momentum_buffer.clamp_min(1e-10).rsqrt_()
    scaled_sq_sum = (v_mean * red_dim_size) * step_size.float().square()
    v_norm_new = scaled_sq_sum.sum(dim=(-2, -1), keepdim=True).sqrt_()
    final_scale = step_size * (v_norm / v_norm_new.clamp_min_(1e-10))
    return v_chunk.mul_(final_scale.type_as(v_chunk))


# -----------------------------------------------------------------------------
# NorMuon optimizer

class NorMuon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).

    Differences from standard Muon:
    - Newton-Shulz is replaced with Polar Express for the orthogonalization step
    - NorMuon adds a low-rank variance estimator similar to Adafactor. https://arxiv.org/pdf/2510.05491
    - small 1D parameters handled here instead of in Adam
    - Cautious weight decay, a gated version of decoupled weight decay
    - Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as
    needed on the forward pass. This enables attn and mlp weights to be contained within the same
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn.
    The scheduling is:
        1. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        2. reduce scatter attn/mlp round 2 (16 mlp params)
        3. wait on step 1, then compute update of 1 and schedule all gather
        4. wait on step 2, then compute update of 2 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        5. wait for each all gather to complete and update params
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, beta2=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, beta2=beta2)
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def reset(self):
        # expose a reset for clearing buffers
        for group in self.param_groups:
            if "momentum_buffer" in group:
                group["momentum_buffer"].zero_()
                group["mantissa"].zero_()
                group["second_momentum_buffer"].zero_()

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per module.
        """
        groups = defaultdict(list)
        for param in params:
            groups[param.label].append(param)

        param_groups = []
        for module_name, group_params in groups.items():
            chunk_size = (len(group_params) + self.world_size - 1) // self.world_size
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))

        return param_groups

    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn
        and mlp params when a param group is split across GPUs.
        """
        params_list = list(params)
        module_group_order = ['attn', 'mlp']
        group_sizes = [16, 16]  # 10 attn + 6 mlp, then 16 mlp
        params_list.sort(key=lambda x: module_group_order.index(x.label))

        idx = 0
        assert len(params_list) == sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            chunk_size = (size + self.world_size - 1) // self.world_size
            group_params = params_list[idx: idx + size]
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))
            idx += size

        return param_groups

    def step(self):
        self.step_p1()
        self.step_p2()
        self.step_p3()
        
    @torch.no_grad()
    def step_p1(self):
        """
        Part 1: Launch distributed reduce_scatter operations for parameter groups.
        """
        rank = dist.get_rank()
        self.group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            stacked_grads = torch.empty(
                (padded_num_params, *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device
            )
            for i, p in enumerate(params):
                stacked_grads[i].copy_(p.grad, non_blocking=True)
            if len(params) < padded_num_params:
                stacked_grads[len(params):].zero_()

            grad_chunk = torch.empty_like(stacked_grads[:chunk_size])

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            self.group_infos.append(dict(grad_chunk=grad_chunk, reduce_future=reduce_future))
    
    @torch.no_grad()
    def step_p2(self):
        """
        Part 2: Compute gradient updates and launch all_gather operations
        Wait for all gather to complete for all parameter groups except final one
        """
        # Efficient distributed step by @YouJiacheng, @KonstantinWilleke, @alexrgilbert,
        # @adricarda, @tuttyfrutyee, @vdlad, @ryanyang0, @vagrawal, @varunneal, @chrisjmccormick
        rank = dist.get_rank()
        group_infos = self.group_infos
        
        self.all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = group["params"]
            grad_chunk = info["grad_chunk"].float()
            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            start_idx = rank * chunk_size
            module_idx = start_idx if start_idx < len(params) else 0

            num_params = min(chunk_size, max(0, len(params) - start_idx))  # num params for this rank

            if "momentum_buffer" not in group:
                group["momentum_buffer"]  = torch.zeros_like(grad_chunk[:num_params], dtype=torch.float32)
                
            momentum_buffer = group["momentum_buffer"]
            # Apply momentum update to the persistent momentum buffer in-place
            momentum_buffer.lerp_(grad_chunk[:num_params], 1 - group["momentum"])
            updated_grads = grad_chunk[:num_params].lerp_(momentum_buffer, group["momentum"])

            grad_shape = updated_grads.shape
            if params[module_idx].label == 'attn':
                for p in params[module_idx:module_idx + num_params]:
                    assert p.label == 'attn'
                updated_grads = updated_grads.view(4 * grad_shape[0], grad_shape[1] // 4, grad_shape[2])

            ref_param = params[module_idx]
            param_shape = ref_param.shape

            if "second_momentum_buffer" not in group:
                group["second_momentum_buffer"] = (torch.zeros_like(updated_grads[..., :, :1], dtype=torch.float32)
                    if param_shape[-2] >= param_shape[-1] else torch.zeros_like(updated_grads[..., :1, :])
                )
            second_momentum_buffer = group["second_momentum_buffer"]

            if "param_lr_cpu" not in group:
                # Define multipliers for ALL params in this group (global, not per-shard)
                lr_mults = []
                wd_mults = []
                for p in params:
                    # Increase learning rate for modules with larger inputs than outputs.
                    # This shape check also assumes rows=input, columns=output, so take care
                    # when changing memory layouts. @chrisjmccormick
                    shape = p.shape
                    if len(shape) >= 2:
                        shape_mult = max(1.0, shape[-2] / shape[-1]) ** 0.5
                    else:
                        shape_mult = 1.0
                    lr_mults.append(shape_mult * getattr(p, "lr_mul", 1.0))
                    wd_mults.append(getattr(p, "wd_mul", 1.0))
                # Define as cpu tensors to enable Inductor constant folding
                group["param_lr_cpu"] = torch.tensor(lr_mults, dtype=torch.float32, device="cpu")
                group["param_wd_cpu"] = torch.tensor(wd_mults, dtype=torch.float32, device="cpu")

            eff_lr_all = group["param_lr_cpu"] * group["lr"]
            eff_wd_all = group["param_wd_cpu"] * group["weight_decay"] * group["lr"]

            # Slice the portion corresponding to this rank's shard
            eff_lr_cpu = eff_lr_all[module_idx:module_idx + num_params]
            eff_wd_cpu = eff_wd_all[module_idx:module_idx + num_params]

            # Compute zeropower for the entire chunk in a single, batched call.
            if num_params == 0:
                v_chunk = updated_grads
            else:
                v_chunk = polar_express(updated_grads, split_baddbmm=(ref_param.label == 'mlp'))

            # Note that the head orientation in O is transposed relative to QKV, so red_dim
            # is 'incorrect' for O. However, correcting this showed no improvement. @chrisjmccormick
            red_dim = -1 if param_shape[-2] >= param_shape[-1] else -2

            v_chunk = apply_normuon_variance_reduction(
                v_chunk, second_momentum_buffer, group["beta2"], red_dim
            )

            v_chunk = v_chunk.view(grad_shape)

            # # "Cautious" weight decay (https://arxiv.org/abs/2510.12402)
            updated_params = torch.empty_like(grad_chunk, dtype=torch.bfloat16)
            if num_params > 0:
                # Work on a stacked copy to avoid touching original params
                param_chunk = torch.stack(params[module_idx:module_idx + num_params])

                if "mantissa" not in group:
                    group["mantissa"] = torch.zeros_like(param_chunk, dtype=torch.uint16)
                mantissa = group["mantissa"]

                for local_idx in range(num_params):
                    cautious_wd_and_update_inplace(
                        param_chunk[local_idx].view(torch.uint16),
                        mantissa[local_idx],
                        v_chunk[local_idx],
                        eff_wd_cpu[local_idx],
                        eff_lr_cpu[local_idx]
                    )
            else:
                param_chunk = torch.zeros_like(v_chunk)

            updated_params[:num_params].copy_(param_chunk)
            if num_params < chunk_size:
                updated_params[num_params:].zero_()

            stacked_params = torch.empty(
                (padded_num_params, *param_shape),
                dtype=updated_params.dtype,
                device=updated_params.device,
            )

            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_params, async_op=True
            ).get_future()

            self.all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete for all except final and copy results back into original parameter tensors.
        for info in self.all_gather_infos[:-1]:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)

    @torch.no_grad()
    def step_p3(self):
        """
        Part 3: Wait for final all gather to complete and copy results back into original parameter tensors
        """
        info = self.all_gather_infos[-1]
        info["gather_future"].wait()
        stacked_params = info["stacked_params"]
        orig_params = info["orig_params"]

        unstacked_params = torch.unbind(stacked_params)
        for i, p in enumerate(orig_params):
            p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, label_order: list[str], betas: list[list[float]], lr: float = 1e-3, eps: float = 1e-8, weight_decay: float = 0.01):
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        defaults = dict(lr=lr, eps=eps, weight_decay=weight_decay)
        params = list(params)
        # Group by label, with explicit ordering for execution control.
        params_by_label = defaultdict(list)
        for p in params:
            params_by_label[getattr(p, 'label', None)].append(p)
        param_groups = []
        for idx, label in enumerate(label_order):
            if label in params_by_label:
                param_groups.append(dict(params=params_by_label[label], betas=betas[idx]))
        # include any unlabeled params at the end (processed last)
        if None in params_by_label:
            param_groups.append(dict(params=params_by_label[None]))
        super().__init__(param_groups, defaults)
        # init state: small params (numel < 1024) use full-sized state, others use sharded
        for p in params:
            chunk = p if p.numel() < 1024 else p[:p.size(0) // self.world_size]
            exp_avg = torch.zeros_like(chunk, dtype=torch.float32, device=p.device)
            self.state[p] = dict(step=0, exp_avg=exp_avg, exp_avg_sq=torch.zeros_like(exp_avg))

        # tag the final param for optimizer pipelining, run all gather after muon copy
        param_groups[-1]['params'][-1].is_final_param = True
        
        # DistributedAdam implementation by @vagrawal, @akash5474
        self.should_sync = False
        self._reduce_scatter_hooks = []
        self._reduce_scatter_futures = {}
        # 0-D CPU tensors to avoid recompilation in _update_step
        self._step_size_t = torch.tensor(0.0, dtype=torch.float32, device="cpu")
        self._eff_wd_t = torch.tensor(0.0, dtype=torch.float32, device="cpu")
        self.register_backward_hooks()

    def register_backward_hooks(self):
        for group in self.param_groups:
            for param in group["params"]:
                self._reduce_scatter_hooks.append(param.register_post_accumulate_grad_hook(self._sync_gradient))

    def load_state_dict(self, state_dict):
        """Override to preserve optimizer state dtypes (avoid BFloat16->Float32 cast that causes recompilation)."""
        # Save original state dtypes before loading
        original_dtypes = {}
        for p, s in self.state.items():
            original_dtypes[p] = {k: v.dtype for k, v in s.items() if isinstance(v, torch.Tensor)}
        
        # Call parent load_state_dict (which may cast dtypes to match param dtype)
        super().load_state_dict(state_dict)
        
        # Restore original dtypes
        for p, s in self.state.items():
            if p in original_dtypes:
                for k, v in s.items():
                    if isinstance(v, torch.Tensor) and k in original_dtypes[p]:
                        if v.dtype != original_dtypes[p][k]:
                            s[k] = v.to(original_dtypes[p][k])

    @torch.no_grad()
    def _sync_gradient(self, param):
        if not self.should_sync:
            return

        grad = param.grad
        if param.numel() < 1024:
            # Small params: use all_reduce (no scatter/gather needed)
            self._reduce_scatter_futures[param] = (
                dist.all_reduce(grad, op=dist.ReduceOp.AVG, async_op=True).get_future(),
                grad
            )
        else:
            rank_size = grad.shape[0] // self.world_size
            if grad is not None:
                grad_slice = torch.empty_like(grad[:rank_size])
                self._reduce_scatter_futures[param] = (
                    dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future(),
                    grad_slice
                )

    def copy_lm_to_embed(self):
        # run at 2/3 of training
        lm_head = self.param_groups[0]['params'][0]
        embed = self.param_groups[-2]['params'][0]
        lm_head_state = self.state[lm_head]
        embed_state = self.state[embed]
        embed_state['step'] = lm_head_state['step']
        embed_state['exp_avg'] = lm_head_state['exp_avg'].clone()
        embed_state['exp_avg_sq'] = lm_head_state['exp_avg_sq'].clone()
        embed.data.copy_(lm_head.data)

    @staticmethod
    @torch.compile(dynamic=False, fullgraph=True)
    def _update_step(p_slice, g_slice, exp_avg, exp_avg_sq, beta1, beta2, eps, step_size_t, eff_wd_t):
        """Compiled Adam update step. step_size_t and eff_wd_t are 0-D CPU tensors to avoid recompilation."""
        exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)  # exp_avg = beta1 * exp_avg + (1 - beta1) * g_slice
        exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)  # exp_avg_sq = beta2 * exp_avg_sq + (1 - beta2) * g_slice^2
        # compute step
        update = exp_avg.div(exp_avg_sq.sqrt().add_(eps)).mul_(step_size_t)  # update = (exp_avg / (sqrt(exp_avg_sq) + eps)) * step_size
        # cautious weight decay
        mask = (update * p_slice) > 0
        update.addcmul_(p_slice, mask, value=eff_wd_t)  # update += eff_wd_t * p_slice * mask
        p_slice.add_(other=update, alpha=-1.0)  # p_slice -= update

    @torch.no_grad()
    def step(self, muon_opt):
        muon_opt.step_p1()
        rank = dist.get_rank()
        all_gather_futures: list[torch.Future] = []

        last_param = None
        last_p_slice = None
        for group in self.param_groups:      
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            for param in group['params']:
                if param not in self._reduce_scatter_futures:
                    continue

                fut, g_slice = self._reduce_scatter_futures[param]
                fut.wait()

                is_small = param.numel() < 1024
                if is_small:
                    # Small params: g_slice is actually full grad, p_slice is full param
                    p_slice = param
                else:
                    rank_size = param.shape[0] // self.world_size
                    p_slice = param[rank * rank_size:(rank + 1) * rank_size]

                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]
                state["step"] += 1
                t = state["step"]
                
                # Pre-compute changing values as 0-D CPU tensors to avoid recompilation.
                # `.fill_(value)` is the same as "= value", but doesn't change the tensor object.
                bias1, bias2 = 1 - beta1 ** t, 1 - beta2 ** t
                self._step_size_t.fill_(lr * (bias2 ** 0.5 / bias1))
                self._eff_wd_t.fill_(lr * lr * wd * getattr(param, "wd_mul", 1.0)) # `lr` included twice to serve as weight decay schedule.

                DistAdam._update_step(p_slice, g_slice, state["exp_avg"], state["exp_avg_sq"],
                                      beta1, beta2, eps, self._step_size_t, self._eff_wd_t)

                if not is_small:
                    if getattr(param, "is_final_param", False):
                        last_param = param
                        last_p_slice = p_slice
                    else:
                        all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())
        self._reduce_scatter_futures.clear()

        muon_opt.step_p2()
        torch.futures.collect_all(all_gather_futures).wait()

        if last_param is not None:
            last_all_gather_future = dist.all_gather_into_tensor(last_param, last_p_slice, async_op=True).get_future()
        muon_opt.step_p3()
        torch.futures.collect_all([last_all_gather_future]).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        with torch.no_grad():
            self.weight.zero_()  # @Grad62304977 and others

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))


# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float
    key_offset: bool
    attn_gate_w: torch.Tensor
    ve_gate_w: torch.Tensor

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = self.dim ** -0.5
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKVO weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # Simplified layout by @chrisjmccormick
        self.qkvo_w = nn.Parameter(torch.empty(self.dim * 4, self.hdim, dtype=torch.bfloat16))
        # label all modules for explicit optimizer grouping
        self.qkvo_w.label = 'attn'

        with torch.no_grad():
            self.qkvo_w[:self.dim * 3].uniform_(-bound, bound)  # init QKV weights
            self.qkvo_w[self.dim * 3:].zero_()  # init O weights to zero

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas, key_offset = attn_args.ve, attn_args.sa_lambdas, attn_args.key_offset
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size
        # sparse gated attention to enable context based no-op by @classiclarryd
        # only include gates on layers with value embeds used on forward pass
        attn_gate_w, ve_gate_w = attn_args.attn_gate_w, attn_args.ve_gate_w

        q, k, v = F.linear(x, sa_lambdas[0] * self.qkvo_w[:self.dim * 3].type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if key_offset:
            # shift keys forward for the stationary head dims. Enables 1-layer induction.
            k[:, 1:, :, self.head_dim // 4:self.head_dim // 2] = k[:, :-1, :, self.head_dim // 4:self.head_dim // 2]
            k[:, 1:, :, 3 * self.head_dim // 4:] = k[:, :-1, :, 3 * self.head_dim // 4:]
        if ve is not None:
            ve_gate_out = 2 * torch.sigmoid(F.linear(x[..., :12], ve_gate_w)).view(B, T, self.num_heads, 1)
            v = v + ve_gate_out * ve.view_as(v) # @ KoszarskyB & @Grad62304977

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens,
                                                        max_seqlen_q=max_len, max_seqlen_k=max_len,
                                                        causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(F.linear(x[..., :12], attn_gate_w)).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, sa_lambdas[1] * self.qkvo_w[self.dim * 3:].type_as(y))  # sa_lambdas[1] pre-multiplied to O @shenberg
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # Transposed layout to match attention weights
        self.c_fc = nn.Parameter(torch.empty(hdim, dim, dtype=torch.bfloat16))
        self.c_proj = nn.Parameter(torch.empty(hdim, dim, dtype=torch.bfloat16))
        # label all modules for explicit optimizer grouping
        self.c_fc.label = 'mlp'
        self.c_proj.label = 'mlp'
        self.c_proj.lr_mul = 2.

        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.T.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.6 (the 7th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads, layer_idx) if layer_idx != 6 else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim)

    def forward(self, x: Tensor, attn_args: AttnArgs):
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

@dataclass
class ForwardScheduleConfig:
    mtp_weights: torch.Tensor
    ws_short: int
    ws_long: int

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        self.num_layers = num_layers
        vocab_size = next_multiple_of_n(vocab_size, n=128)

        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.label = 'smear_gate'
        self.smear_gate.weight.lr_mul = 0.01
        self.smear_gate.weight.wd_mul = 0.0

        self.skip_gate = CastedLinear(12, 1)
        self.skip_gate.weight.label = 'skip_gate'
        self.skip_gate.weight.lr_mul = 0.05
        self.skip_gate.weight.wd_mul = 0.0

        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        for embed in self.value_embeds:
            nn.init.zeros_(embed.weight)
        for ve in self.value_embeds:
            ve.weight.label = 'value_embed'
        
        # parameter banks for attention and value embedding gate weights
        self.attn_gate_bank = nn.Parameter(torch.zeros(10, num_heads, 12)) # 10 layers
        self.attn_gate_bank.label = 'attn_gate_bank'
        self.ve_gate_bank = nn.Parameter(torch.zeros(5, num_heads, 12)) # 5 layers
        self.ve_gate_bank.label = 've_gate_bank'

        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)

        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=100/448, w_s=1.6/448, grad_s=0.75/448)
        nn.init.normal_(self.lm_head.weight, mean=0, std=0.005)
        self.lm_head.weight.label = 'lm_head'

        self.embed = nn.Embedding(vocab_size, model_dim)
        self.embed.weight.label = 'embed'

        # x0_lambdas separated out for different optimizer treatment (no beta smoothing)
        self.x0_lambdas = nn.Parameter(torch.zeros(num_layers))
        self.x0_lambdas.label = 'x0_lambdas'
        self.x0_lambdas.lr_mul = 5.0
        self.x0_lambdas.wd_mul = 0.0

        pad = (-num_layers * 3 - 3) % dist.get_world_size()  # updated: 3*num_layers instead of 4*
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    1.1 * torch.ones(num_layers),  # resid lambdas. 1.1 init such that layer i weight is i^(num_layers-i).
                    *[torch.tensor([0.5, 1.0]) for _ in range(num_layers)],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    -1.5 * torch.ones(1),  # skip_lambda -> σ(-1.5) ≈ 0.18
                    torch.ones(pad),
                ]
            )
        )

        self.scalars.label = 'scalars'
        # set learning rates
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
            param.wd_mul = 5.
        for param in self.embed.parameters():
            param.wd_mul = 150.
        for param in self.lm_head.parameters():
            param.wd_mul = 150.
        self.scalars.lr_mul = 5.0
        self.scalars.wd_mul = 0.0

        self.split_embed = False

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, schedule_cfg: ForwardScheduleConfig):
        assert input_seq.ndim == 1

        # unpack schedule_cfg
        mtp_weights, ws_short, ws_long = schedule_cfg.mtp_weights, schedule_cfg.ws_short, schedule_cfg.ws_long

        # set configs
        skip_connections = []
        skip_in = [3] # long attention window on layer 3
        skip_out = [6] # no attn op on layer 6
        x_backout = None
        backout_layer = 7

        # set lambdas
        resid_lambdas = self.scalars[: 1 * self.num_layers]
        x0_lambdas = self.x0_lambdas
        sa_lambdas = self.scalars[1 * self.num_layers: 3 * self.num_layers].view(-1, 2)
        smear_lambda = self.scalars[3 * self.num_layers]
        backout_lambda = self.scalars[3 * self.num_layers+1]
        skip_lambda = self.scalars[3 * self.num_layers+2]

        # set block masks and key shift
        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == self.num_layers
        key_offset = [b==long_bm for b in bm_sizes] # apply partial key offset to long windows

        # weight-tied: use lm_head.weight for embedding lookup (or separate embed after split)
        if self.split_embed:
            x = self.embed(input_seq)
        else:
            x = F.embedding(input_seq, self.lm_head.weight)
        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [ve[1], ve[2]] + [None] * (self.num_layers - 5) + [ve[0], ve[1], ve[2]]
        assert len(ve) == self.num_layers

        # smear token embed forward 1 position @classiclarryd
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # unbind gate banks to avoid select_backwards kernel
        ag = [w.bfloat16() for w in self.attn_gate_bank.unbind(0)] 
        veg = [w.bfloat16() for w in self.ve_gate_bank.unbind(0)]
        attn_gates = ag[:6] + [None] + ag[6:]
        ve_gates = [veg[0], veg[1]] + [None] * (self.num_layers - 5) + [veg[2], veg[3], veg[4]]
        assert len(attn_gates) == self.num_layers
        assert len(ve_gates) == self.num_layers

        for i in range(self.num_layers):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale,
                key_offset=key_offset[i],
                attn_gate_w=attn_gates[i],
                ve_gate_w=ve_gates[i]
            )
            if i in skip_out:
                skip_gate_out = torch.sigmoid(skip_lambda) * 2 * torch.sigmoid(self.skip_gate(x0[..., :self.skip_gate.weight.size(-1)]))
                x = x + skip_gate_out * skip_connections.pop()
            if i == 0:
                x = (resid_lambdas[0] + x0_lambdas[0]) * x
            else:
                x = resid_lambdas[i] * x + x0_lambdas[i] * x0
            x = self.blocks[i](x, attn_args)
            if i in skip_in:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 7 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15
        # @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1). @classiclarryd updated to 23*sigmoid((logits+5)/7.5)
        logits = 23 * torch.sigmoid((logits + 5) / 7.5)
        logits_for_loss = logits.float() if not self.training else logits

        n_predict = mtp_weights.size(0) if mtp_weights is not None else 1
        if self.training and n_predict > 1:
            # Multi-token prediction: take loss of the weighted average of next n_predict tokens
            logits_flat = logits_for_loss.view(-1, logits_for_loss.size(-1))
            idx = F.pad(target_seq, (0, n_predict - 1)).unfold(0, n_predict, 1)  # [T, n_predict] of shifted targets
            target_logits = logits_flat.gather(1, idx)
            cross_entropy = torch.logsumexp(logits_flat, dim=-1).unsqueeze(1) - target_logits
            for k in range(1, n_predict):  # zero out preds past end of sequence
                cross_entropy[-k:, k] = 0
            loss = (cross_entropy * mtp_weights).sum()
        elif self.training:
            loss = F.cross_entropy(logits_for_loss.view(-1, logits_for_loss.size(-1)), target_seq, reduction="sum")
        else:
            loss = F.cross_entropy(logits_for_loss.view(-1, logits_for_loss.size(-1)), target_seq, reduction="mean")
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()

    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        # Cast to int32 on CPU before transfer to avoid dtype conversion during .to()
        _inputs = _inputs.to(dtype=torch.int32)
        _targets = _targets.to(dtype=torch.int64)
        _cum_lengths = _cum_lengths.to(dtype=torch.int32)

        new_params = yield (
            _inputs.to(device="cuda", non_blocking=True),
            _targets.to(device="cuda", non_blocking=True),
            _cum_lengths.to(device="cuda", non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * new_grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens // new_grad_accum_steps
            max_seq_len = new_max_seq_len

# -----------------------------------------------------------------------------
# Training Management
 
def get_bs(step: int):
    if step >= args.num_scheduled_iterations:
        return args.train_bs_extension
    x = step / args.num_scheduled_iterations
    bs_idx = int(len(args.train_bs_schedule) * x)
    return args.train_bs_schedule[bs_idx]

def get_ws(step: int):
    # set short window size to half of long window size
    # Higher ws on "extension" steps
    if step >= args.num_scheduled_iterations:
        return args.ws_final // 2, args.ws_final
    x = step / args.num_scheduled_iterations
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

# learning rate schedule: tied to batch size schedule, with cooldown at the end.
def get_lr(step: int):
    if step > args.num_scheduled_iterations:
        return 0.1
    lr_max = 1.0
    x = step / args.num_scheduled_iterations
    if x > 1/3:
       lr_max = 1.52  # (16/8)**0.6
    if x > 2/3:
        lr_max = 1.73  # (24/8)**0.5
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = lr_max * w + (1 - w) * 0.1
        return lr
    return lr_max

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

class TrainingManager():
    """
    Manages three optimizers for Adam embed/lm_head, Adam scalars, and Muon weight matrices.
    Notable Features:
        1. Scalars are given higher momentum terms to smooth learning @ChrisJMcCormick
        2. Scalar weights are temporarily frozen during batch size or window size updates @ChrisJMcCormick
        3. Adam optimizers are only stepped on odd steps @classiclarryd
        4. Adam optimizers have hooks to start gradient communication during backwards pass @akash5474
        5. Muon has a linear momentum warmup and cooldown schedule
        6. Learning rates follow a linear decay schedule
        7. Embed/lm_head weights and optimizer state splits at 2/3 of training @classiclarryd

    Manages model architecture, data, and target that changes during training
    Notable Features:
        1. Multi Token Prediction schedule of [1, 0.5, 0.25->0] -> [1, 0.5->0] -> [1] @varunneal
        2. Sliding Attention window schedule of [1,3] -> [3,7] -> [5,11] -> [6,13]
        3. YaRN updates to RoPE on window changes
        4. Split embed and lm head at 2/3 of training
        5. Batch size schedule of 8 -> 16 -> 24
        6. Post training extension of long windows from 13 to 20
    """
    def __init__(self, model):
        self.mtp_weights_schedule = self._build_mtp_schedule()
        self.model = model
        adam_betas = {
            'lm_head': [0.5, 0.95],
            'smear_gate': [0.9, 0.99],
            'attn_gate_bank': [0.9, 0.99],
            've_gate_bank': [0.9, 0.99],
            'skip_gate': [0.9, 0.99],
            'x0_lambdas': [0.65, 0.95],
            'scalars': [0.9, 0.99],
            'embed': [0.5, 0.95],
            'value_embed': [0.75, 0.95]
        }
        adam_labels = list(adam_betas.keys())
        adam_beta_values = list(adam_betas.values())
        muon_labels = ['attn', 'mlp']
        adam_params = [p for p in model.parameters() if getattr(p, 'label', None) in adam_labels]
        muon_params = [p for p in model.parameters() if getattr(p, 'label', None) in muon_labels]
        assert set(getattr(p, 'label', None) for p in model.parameters()) == set(adam_labels + muon_labels), "All params must have label"

        self.adam_opt = DistAdam(adam_params, adam_labels, adam_beta_values, lr=0.008, eps=1e-10, weight_decay=0.005)
        self.muon_opt = NorMuon(muon_params, lr=0.023, momentum=0.95, beta2=0.95, weight_decay=1.2)
        self.optimizers = [self.adam_opt, self.muon_opt]

        # split after odd number step
        self.split_step = math.ceil(args.split_embed_frac * args.num_scheduled_iterations) | 1

        # set defaults
        for opt in self.optimizers:
            opt.odd_step_only = False 
            opt.should_sync = True
            for group in opt.param_groups:
                group["initial_lr"] = group["lr"]

        # on even steps, only step Muon params
        self.adam_opt.odd_step_only = True

        self.reset()

    def _build_mtp_schedule(self):
        # Precompute MTP weights for all steps to avoid tensor allocation during training
        # Schedule: [1, 0.5, 0.25->0] -> [1, 0.5->0] -> [1]
        mtp_weights_schedule = []
        for s in range(args.num_iterations + 1):
            x = s / args.num_scheduled_iterations
            if x < 1/3:
                w = [1.0, 0.5, 0.25 * (1 - 3*x)]
            elif x < 2/3:
                w = [1.0, 0.5 * (1 - (3*x - 1))]
            else:
                w = [1.0]
            mtp_weights_schedule.append(torch.tensor(w, device=device))
        return mtp_weights_schedule

    def apply_final_ws_ext(self):
        self.ws_long = args.ws_validate_post_yarn_ext

    def get_forward_args(self):
        return ForwardScheduleConfig(
            mtp_weights = self.mtp_weights,
            ws_short = self.ws_short,
            ws_long = self.ws_long
        )
    
    def _is_active_step(self, opt, step: int):
        return (opt.odd_step_only and step%2==1) or not opt.odd_step_only

    def get_transition_steps(self):
        transition_steps = []
        ws_short, ws_long = get_ws(0)
        for step in range(1, args.num_iterations):
            ws_short, new_ws_long = get_ws(step)
            if new_ws_long != ws_long:
                transition_steps.append(step)
                ws_long = new_ws_long
        return transition_steps

    def advance_schedule(self, step: int):
        self.ws_short, new_ws_long = get_ws(step)
        if new_ws_long != self.ws_long:
            self.model.yarn.apply(self.ws_long, new_ws_long)

        new_batch_size = get_bs(step)
        if new_batch_size != self.batch_size:
            self.train_loader_send_args = (new_batch_size, args.train_max_seq_len, grad_accum_steps)
        else:
            self.train_loader_send_args = None

        self.ws_long = new_ws_long
        self.mtp_weights = self.mtp_weights_schedule[step]
    
    def step_optimizers(self, step: int):                
        step_lr = get_lr(step)
        muon_momentum = get_muon_momentum(step)
        for group in self.muon_opt.param_groups:
            group["momentum"] = muon_momentum

        for opt in self.optimizers:
            for group in opt.param_groups:
                group["lr"] = group["initial_lr"] * step_lr
                
        if self._is_active_step(self.adam_opt, step):
            # adam will interleave calls to muon step
            self.adam_opt.step(self.muon_opt)
            self.model.zero_grad(set_to_none=True)
            self.adam_opt.should_sync = False
        else:
            self.muon_opt.step()
            self.muon_opt.zero_grad(set_to_none=True)
            
        if step == self.split_step:
            self.adam_opt.copy_lm_to_embed()
            self.model.split_embed = True
    
    def activate_hooks(self, step: int):
        for opt in self.optimizers:
            if self._is_active_step(opt, step):
                opt.should_sync = True

    def reset(self, state=None):
        if state is not None:
            for opt, opt_state in zip(self.optimizers, state):
                opt.should_sync = False
                opt.load_state_dict(opt_state)

        # muon momentum buffers not in state dict
        self.muon_opt.reset()
        self.model.split_embed = False

        self.ws_short, self.ws_long = get_ws(0)
        self.batch_size = get_bs(0)
        self.model.yarn.reset()

    def get_state(self):
        return [copy.deepcopy(opt.state_dict()) for opt in self.optimizers]

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # batch sizes
    train_bs_schedule: tuple = (8 * 2048 * 8, 16 * 2048 * 8, 24 * 2048 * 8)
    train_bs_extension: int = 24 * 2048 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 1800  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: float = 0.50  # fraction of num_scheduled_iterations spent cooling down the learning rate
    split_embed_frac: float = 2/3  # fraction of training when embeddings split from lm_head
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_final: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=11,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=args.val_batch_size // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.weight.data = m.weight.data.bfloat16()
model.attn_gate_bank.data = model.attn_gate_bank.data.bfloat16()
model.ve_gate_bank.data = model.ve_gate_bank.data.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)
training_manager = TrainingManager(model)

########################################
#            Warmup kernels            #
########################################
print0("Compiling model and warming up kernels (~7 minutes on first execution)", console=True)
# Warmup the training kernels, then re-initialize the state so we aren't cheating
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=training_manager.get_state()) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_bs_schedule[0], args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)

transition_steps = training_manager.get_transition_steps()
# first few steps plus transitions
warmup_steps = sorted({0, 1, 2} | set(s + offset for s in transition_steps for offset in [-1, 0, 1] if s + offset >= 0)) 
print0(f"Sampling steps {warmup_steps} for warmup", console=True)
for step in warmup_steps:
    training_manager.advance_schedule(step)
    model.eval()
    with torch.no_grad():
        inputs, targets, cum_seqlens = next(val_loader)
        model(inputs, targets, cum_seqlens, training_manager.get_forward_args())
    model.train()
    for idx in range(grad_accum_steps):
        # enable gradient sync for the DistAdam optimizers on the last iteration before we step them
        if idx == grad_accum_steps - 1:
            training_manager.activate_hooks(step)
        send_args = training_manager.train_loader_send_args
        inputs, targets, cum_seqlens = train_loader.send(send_args)
        (model(inputs, targets, cum_seqlens, training_manager.get_forward_args()) / grad_accum_steps).backward()
    training_manager.step_optimizers(step)
print0("Resetting Model", console=True)
model.zero_grad(set_to_none=True)
model.load_state_dict(initial_state["model"])
training_manager.reset(initial_state["optimizers"])
del val_loader, train_loader, initial_state
model.train()

########################################
#        Training and validation       #
########################################
train_loader = distributed_data_generator(args.train_files, args.train_bs_schedule[0], args.train_max_seq_len, grad_accum_steps=grad_accum_steps)

gc.collect()

training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    training_manager.advance_schedule(step)
    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            training_manager.apply_final_ws_ext()
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, training_manager.get_forward_args())
        val_loss /= val_steps
        del val_loader
        dist.reduce(val_loss, 0, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for idx in range(grad_accum_steps):
        # enable gradient sync for the DistAdam optimizers on the last iteration before we step them
        if idx == grad_accum_steps - 1:
            training_manager.activate_hooks(step)
        send_args = training_manager.train_loader_send_args
        inputs, targets, cum_seqlens = train_loader.send(send_args)
        (model(inputs, targets, cum_seqlens, training_manager.get_forward_args()) / grad_accum_steps).backward()
    training_manager.step_optimizers(step)

    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.10.12 (main, Nov  4 2025, 08:48:33) [GCC 11.4.0]
Running PyTorch 2.10.0.dev20251210+cu126 compiled for CUDA 12.6
Running Triton version 3.6.0
Mon Jan  5 07:44:22 2026       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:19:00.0 Off |                    0 |
| N/A   35C    P0            124W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:3B:00.0 Off |                    0 |
| N/A   32C    P0            127W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:4C:00.0 Off |                    0 |
| N/A   30C    P0            124W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:5D:00.0 Off |                    0 |
| N/A   35C    P0            123W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:9B:00.0 Off |                    0 |
| N/A   37C    P0            127W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:BB:00.0 Off |                    0 |
| N/A   32C    P0            122W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:CB:00.0 Off |                    0 |
| N/A   35C    P0            124W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:DB:00.0 Off |                    0 |
| N/A   28C    P0            122W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A     44681      C   /usr/bin/python                                 0MiB |
|    1   N/A  N/A     44682      C   /usr/bin/python                                 0MiB |
|    2   N/A  N/A     44683      C   /usr/bin/python                                 0MiB |
|    3   N/A  N/A     44684      C   /usr/bin/python                                 0MiB |
|    4   N/A  N/A     44685      C   /usr/bin/python                                 0MiB |
|    5   N/A  N/A     44686      C   /usr/bin/python                                 0MiB |
|    6   N/A  N/A     44687      C   /usr/bin/python                                 0MiB |
|    7   N/A  N/A     44688      C   /usr/bin/python                                 0MiB |
+-----------------------------------------------------------------------------------------+

====================================================================================================
Compiling model and warming up kernels (~7 minutes on first execution)
Sampling steps [0, 1, 2, 599, 600, 601, 1199, 1200, 1201, 1799, 1800, 1801] for warmup
Resetting Model
step:0/1840 val_loss:10.8297 train_time:0ms step_avg:0.03ms
step:1/1840 train_time:63ms step_avg:63.26ms
step:2/1840 train_time:84ms step_avg:42.01ms
step:3/1840 train_time:108ms step_avg:35.91ms
step:4/1840 train_time:142ms step_avg:35.42ms
step:5/1840 train_time:173ms step_avg:34.69ms
step:6/1840 train_time:264ms step_avg:43.95ms
step:7/1840 train_time:280ms step_avg:40.07ms
step:8/1840 train_time:409ms step_avg:51.15ms
step:9/1840 train_time:441ms step_avg:48.99ms
step:10/1840 train_time:475ms step_avg:47.48ms
step:11/1840 train_time:507ms step_avg:46.05ms
step:12/1840 train_time:541ms step_avg:45.06ms
step:13/1840 train_time:573ms step_avg:44.06ms
step:14/1840 train_time:607ms step_avg:43.35ms
step:15/1840 train_time:639ms step_avg:42.60ms
step:16/1840 train_time:673ms step_avg:42.07ms
step:17/1840 train_time:705ms step_avg:41.47ms
step:18/1840 train_time:739ms step_avg:41.08ms
step:19/1840 train_time:771ms step_avg:40.59ms
step:20/1840 train_time:805ms step_avg:40.27ms
step:21/1840 train_time:838ms step_avg:39.89ms
step:22/1840 train_time:872ms step_avg:39.62ms
step:23/1840 train_time:903ms step_avg:39.28ms
step:24/1840 train_time:938ms step_avg:39.07ms
step:25/1840 train_time:970ms step_avg:38.78ms
step:26/1840 train_time:1003ms step_avg:38.59ms
step:27/1840 train_time:1036ms step_avg:38.36ms
step:28/1840 train_time:1070ms step_avg:38.21ms
step:29/1840 train_time:1102ms step_avg:37.99ms
step:30/1840 train_time:1136ms step_avg:37.87ms
step:31/1840 train_time:1168ms step_avg:37.67ms
step:32/1840 train_time:1202ms step_avg:37.57ms
step:33/1840 train_time:1234ms step_avg:37.40ms
step:34/1840 train_time:1268ms step_avg:37.30ms
step:35/1840 train_time:1302ms step_avg:37.19ms
step:36/1840 train_time:1337ms step_avg:37.14ms
step:37/1840 train_time:1369ms step_avg:37.01ms
step:38/1840 train_time:1404ms step_avg:36.95ms
step:39/1840 train_time:1437ms step_avg:36.83ms
step:40/1840 train_time:1471ms step_avg:36.77ms
step:41/1840 train_time:1503ms step_avg:36.67ms
step:42/1840 train_time:1538ms step_avg:36.62ms
step:43/1840 train_time:1570ms step_avg:36.51ms
step:44/1840 train_time:1604ms step_avg:36.46ms
step:45/1840 train_time:1637ms step_avg:36.37ms
step:46/1840 train_time:1671ms step_avg:36.33ms
step:47/1840 train_time:1703ms step_avg:36.24ms
step:48/1840 train_time:1738ms step_avg:36.20ms
step:49/1840 train_time:1770ms step_avg:36.12ms
step:50/1840 train_time:1804ms step_avg:36.08ms
step:51/1840 train_time:1837ms step_avg:36.01ms
step:52/1840 train_time:1871ms step_avg:35.98ms
step:53/1840 train_time:1903ms step_avg:35.90ms
step:54/1840 train_time:1937ms step_avg:35.87ms
step:55/1840 train_time:1969ms step_avg:35.79ms
step:56/1840 train_time:2003ms step_avg:35.77ms
step:57/1840 train_time:2035ms step_avg:35.70ms
step:58/1840 train_time:2069ms step_avg:35.67ms
step:59/1840 train_time:2101ms step_avg:35.61ms
step:60/1840 train_time:2135ms step_avg:35.59ms
step:61/1840 train_time:2167ms step_avg:35.53ms
step:62/1840 train_time:2201ms step_avg:35.51ms
step:63/1840 train_time:2234ms step_avg:35.46ms
step:64/1840 train_time:2268ms step_avg:35.44ms
step:65/1840 train_time:2300ms step_avg:35.39ms
step:66/1840 train_time:2335ms step_avg:35.38ms
step:67/1840 train_time:2367ms step_avg:35.33ms
step:68/1840 train_time:2401ms step_avg:35.31ms
step:69/1840 train_time:2434ms step_avg:35.27ms
step:70/1840 train_time:2468ms step_avg:35.26ms
step:71/1840 train_time:2500ms step_avg:35.22ms
step:72/1840 train_time:2535ms step_avg:35.21ms
step:73/1840 train_time:2567ms step_avg:35.16ms
step:74/1840 train_time:2601ms step_avg:35.15ms
step:75/1840 train_time:2633ms step_avg:35.11ms
step:76/1840 train_time:2668ms step_avg:35.10ms
step:77/1840 train_time:2700ms step_avg:35.06ms
step:78/1840 train_time:2734ms step_avg:35.05ms
step:79/1840 train_time:2766ms step_avg:35.02ms
step:80/1840 train_time:2800ms step_avg:35.01ms
step:81/1840 train_time:2832ms step_avg:34.97ms
step:82/1840 train_time:2867ms step_avg:34.96ms
step:83/1840 train_time:2899ms step_avg:34.93ms
step:84/1840 train_time:2933ms step_avg:34.92ms
step:85/1840 train_time:2965ms step_avg:34.89ms
step:86/1840 train_time:3000ms step_avg:34.88ms
step:87/1840 train_time:3032ms step_avg:34.85ms
step:88/1840 train_time:3066ms step_avg:34.85ms
step:89/1840 train_time:3098ms step_avg:34.81ms
step:90/1840 train_time:3133ms step_avg:34.81ms
step:91/1840 train_time:3165ms step_avg:34.78ms
step:92/1840 train_time:3199ms step_avg:34.77ms
step:93/1840 train_time:3231ms step_avg:34.74ms
step:94/1840 train_time:3266ms step_avg:34.74ms
step:95/1840 train_time:3298ms step_avg:34.71ms
step:96/1840 train_time:3332ms step_avg:34.71ms
step:97/1840 train_time:3364ms step_avg:34.68ms
step:98/1840 train_time:3398ms step_avg:34.68ms
step:99/1840 train_time:3431ms step_avg:34.65ms
step:100/1840 train_time:3465ms step_avg:34.65ms
step:101/1840 train_time:3497ms step_avg:34.62ms
step:102/1840 train_time:3531ms step_avg:34.62ms
step:103/1840 train_time:3563ms step_avg:34.60ms
step:104/1840 train_time:3598ms step_avg:34.59ms
step:105/1840 train_time:3630ms step_avg:34.57ms
step:106/1840 train_time:3664ms step_avg:34.57ms
step:107/1840 train_time:3697ms step_avg:34.55ms
step:108/1840 train_time:3731ms step_avg:34.54ms
step:109/1840 train_time:3763ms step_avg:34.52ms
step:110/1840 train_time:3798ms step_avg:34.52ms
step:111/1840 train_time:3830ms step_avg:34.50ms
step:112/1840 train_time:3864ms step_avg:34.50ms
step:113/1840 train_time:3896ms step_avg:34.48ms
step:114/1840 train_time:3930ms step_avg:34.47ms
step:115/1840 train_time:3962ms step_avg:34.46ms
step:116/1840 train_time:3997ms step_avg:34.46ms
step:117/1840 train_time:4029ms step_avg:34.44ms
step:118/1840 train_time:4063ms step_avg:34.44ms
step:119/1840 train_time:4096ms step_avg:34.42ms
step:120/1840 train_time:4129ms step_avg:34.41ms
step:121/1840 train_time:4161ms step_avg:34.39ms
step:122/1840 train_time:4196ms step_avg:34.39ms
step:123/1840 train_time:4228ms step_avg:34.37ms
step:124/1840 train_time:4263ms step_avg:34.38ms
step:125/1840 train_time:4295ms step_avg:34.36ms
step:126/1840 train_time:4329ms step_avg:34.36ms
step:127/1840 train_time:4361ms step_avg:34.34ms
step:128/1840 train_time:4395ms step_avg:34.34ms
step:129/1840 train_time:4427ms step_avg:34.32ms
step:130/1840 train_time:4462ms step_avg:34.32ms
step:131/1840 train_time:4494ms step_avg:34.30ms
step:132/1840 train_time:4528ms step_avg:34.30ms
step:133/1840 train_time:4560ms step_avg:34.29ms
step:134/1840 train_time:4594ms step_avg:34.29ms
step:135/1840 train_time:4627ms step_avg:34.27ms
step:136/1840 train_time:4661ms step_avg:34.27ms
step:137/1840 train_time:4693ms step_avg:34.25ms
step:138/1840 train_time:4727ms step_avg:34.25ms
step:139/1840 train_time:4759ms step_avg:34.24ms
step:140/1840 train_time:4794ms step_avg:34.24ms
step:141/1840 train_time:4826ms step_avg:34.22ms
step:142/1840 train_time:4860ms step_avg:34.22ms
step:143/1840 train_time:4892ms step_avg:34.21ms
step:144/1840 train_time:4926ms step_avg:34.21ms
step:145/1840 train_time:4959ms step_avg:34.20ms
step:146/1840 train_time:4993ms step_avg:34.20ms
step:147/1840 train_time:5025ms step_avg:34.18ms
step:148/1840 train_time:5059ms step_avg:34.18ms
step:149/1840 train_time:5091ms step_avg:34.17ms
step:150/1840 train_time:5126ms step_avg:34.17ms
step:151/1840 train_time:5158ms step_avg:34.16ms
step:152/1840 train_time:5191ms step_avg:34.15ms
step:153/1840 train_time:5223ms step_avg:34.14ms
step:154/1840 train_time:5258ms step_avg:34.14ms
step:155/1840 train_time:5289ms step_avg:34.13ms
step:156/1840 train_time:5324ms step_avg:34.13ms
step:157/1840 train_time:5356ms step_avg:34.12ms
step:158/1840 train_time:5390ms step_avg:34.12ms
step:159/1840 train_time:5422ms step_avg:34.10ms
step:160/1840 train_time:5457ms step_avg:34.11ms
step:161/1840 train_time:5489ms step_avg:34.09ms
step:162/1840 train_time:5523ms step_avg:34.09ms
step:163/1840 train_time:5555ms step_avg:34.08ms
step:164/1840 train_time:5589ms step_avg:34.08ms
step:165/1840 train_time:5621ms step_avg:34.07ms
step:166/1840 train_time:5655ms step_avg:34.07ms
step:167/1840 train_time:5687ms step_avg:34.06ms
step:168/1840 train_time:5722ms step_avg:34.06ms
step:169/1840 train_time:5754ms step_avg:34.05ms
step:170/1840 train_time:5788ms step_avg:34.05ms
step:171/1840 train_time:5820ms step_avg:34.04ms
step:172/1840 train_time:5855ms step_avg:34.04ms
step:173/1840 train_time:5887ms step_avg:34.03ms
step:174/1840 train_time:5922ms step_avg:34.03ms
step:175/1840 train_time:5954ms step_avg:34.02ms
step:176/1840 train_time:5988ms step_avg:34.02ms
step:177/1840 train_time:6020ms step_avg:34.01ms
step:178/1840 train_time:6054ms step_avg:34.01ms
step:179/1840 train_time:6086ms step_avg:34.00ms
step:180/1840 train_time:6121ms step_avg:34.00ms
step:181/1840 train_time:6153ms step_avg:33.99ms
step:182/1840 train_time:6187ms step_avg:33.99ms
step:183/1840 train_time:6219ms step_avg:33.98ms
step:184/1840 train_time:6253ms step_avg:33.98ms
step:185/1840 train_time:6285ms step_avg:33.97ms
step:186/1840 train_time:6320ms step_avg:33.98ms
step:187/1840 train_time:6352ms step_avg:33.97ms
step:188/1840 train_time:6385ms step_avg:33.97ms
step:189/1840 train_time:6417ms step_avg:33.95ms
step:190/1840 train_time:6451ms step_avg:33.96ms
step:191/1840 train_time:6484ms step_avg:33.95ms
step:192/1840 train_time:6518ms step_avg:33.95ms
step:193/1840 train_time:6550ms step_avg:33.94ms
step:194/1840 train_time:6584ms step_avg:33.94ms
step:195/1840 train_time:6616ms step_avg:33.93ms
step:196/1840 train_time:6650ms step_avg:33.93ms
step:197/1840 train_time:6682ms step_avg:33.92ms
step:198/1840 train_time:6716ms step_avg:33.92ms
step:199/1840 train_time:6748ms step_avg:33.91ms
step:200/1840 train_time:6782ms step_avg:33.91ms
step:201/1840 train_time:6815ms step_avg:33.90ms
step:202/1840 train_time:6849ms step_avg:33.91ms
step:203/1840 train_time:6881ms step_avg:33.90ms
step:204/1840 train_time:6916ms step_avg:33.90ms
step:205/1840 train_time:6948ms step_avg:33.89ms
step:206/1840 train_time:6982ms step_avg:33.89ms
step:207/1840 train_time:7014ms step_avg:33.88ms
step:208/1840 train_time:7048ms step_avg:33.89ms
step:209/1840 train_time:7080ms step_avg:33.88ms
step:210/1840 train_time:7115ms step_avg:33.88ms
step:211/1840 train_time:7147ms step_avg:33.87ms
step:212/1840 train_time:7181ms step_avg:33.87ms
step:213/1840 train_time:7214ms step_avg:33.87ms
step:214/1840 train_time:7248ms step_avg:33.87ms
step:215/1840 train_time:7280ms step_avg:33.86ms
step:216/1840 train_time:7314ms step_avg:33.86ms
step:217/1840 train_time:7347ms step_avg:33.85ms
step:218/1840 train_time:7381ms step_avg:33.86ms
step:219/1840 train_time:7413ms step_avg:33.85ms
step:220/1840 train_time:7448ms step_avg:33.85ms
step:221/1840 train_time:7480ms step_avg:33.84ms
step:222/1840 train_time:7514ms step_avg:33.85ms
step:223/1840 train_time:7546ms step_avg:33.84ms
step:224/1840 train_time:7580ms step_avg:33.84ms
step:225/1840 train_time:7612ms step_avg:33.83ms
step:226/1840 train_time:7646ms step_avg:33.83ms
step:227/1840 train_time:7678ms step_avg:33.82ms
step:228/1840 train_time:7712ms step_avg:33.82ms
step:229/1840 train_time:7744ms step_avg:33.81ms
step:230/1840 train_time:7778ms step_avg:33.82ms
step:231/1840 train_time:7810ms step_avg:33.81ms
step:232/1840 train_time:7844ms step_avg:33.81ms
step:233/1840 train_time:7876ms step_avg:33.80ms
step:234/1840 train_time:7910ms step_avg:33.80ms
step:235/1840 train_time:7942ms step_avg:33.80ms
step:236/1840 train_time:7977ms step_avg:33.80ms
step:237/1840 train_time:8009ms step_avg:33.79ms
step:238/1840 train_time:8043ms step_avg:33.79ms
step:239/1840 train_time:8075ms step_avg:33.79ms
step:240/1840 train_time:8109ms step_avg:33.79ms
step:241/1840 train_time:8141ms step_avg:33.78ms
step:242/1840 train_time:8176ms step_avg:33.78ms
step:243/1840 train_time:8208ms step_avg:33.78ms
step:244/1840 train_time:8242ms step_avg:33.78ms
step:245/1840 train_time:8274ms step_avg:33.77ms
step:246/1840 train_time:8308ms step_avg:33.77ms
step:247/1840 train_time:8340ms step_avg:33.77ms
step:248/1840 train_time:8374ms step_avg:33.77ms
step:249/1840 train_time:8406ms step_avg:33.76ms
step:250/1840 train_time:8440ms step_avg:33.76ms
step:250/1840 val_loss:4.6069 train_time:8482ms step_avg:33.93ms
step:251/1840 train_time:8500ms step_avg:33.87ms
step:252/1840 train_time:8522ms step_avg:33.82ms
step:253/1840 train_time:8543ms step_avg:33.77ms
step:254/1840 train_time:8577ms step_avg:33.77ms
step:255/1840 train_time:8611ms step_avg:33.77ms
step:256/1840 train_time:8645ms step_avg:33.77ms
step:257/1840 train_time:8678ms step_avg:33.77ms
step:258/1840 train_time:8712ms step_avg:33.77ms
step:259/1840 train_time:8744ms step_avg:33.76ms
step:260/1840 train_time:8778ms step_avg:33.76ms
step:261/1840 train_time:8810ms step_avg:33.75ms
step:262/1840 train_time:8844ms step_avg:33.76ms
step:263/1840 train_time:8876ms step_avg:33.75ms
step:264/1840 train_time:8910ms step_avg:33.75ms
step:265/1840 train_time:8941ms step_avg:33.74ms
step:266/1840 train_time:8976ms step_avg:33.74ms
step:267/1840 train_time:9007ms step_avg:33.74ms
step:268/1840 train_time:9041ms step_avg:33.74ms
step:269/1840 train_time:9073ms step_avg:33.73ms
step:270/1840 train_time:9107ms step_avg:33.73ms
step:271/1840 train_time:9139ms step_avg:33.72ms
step:272/1840 train_time:9173ms step_avg:33.72ms
step:273/1840 train_time:9205ms step_avg:33.72ms
step:274/1840 train_time:9239ms step_avg:33.72ms
step:275/1840 train_time:9271ms step_avg:33.71ms
step:276/1840 train_time:9305ms step_avg:33.71ms
step:277/1840 train_time:9337ms step_avg:33.71ms
step:278/1840 train_time:9371ms step_avg:33.71ms
step:279/1840 train_time:9403ms step_avg:33.70ms
step:280/1840 train_time:9438ms step_avg:33.71ms
step:281/1840 train_time:9470ms step_avg:33.70ms
step:282/1840 train_time:9505ms step_avg:33.71ms
step:283/1840 train_time:9537ms step_avg:33.70ms
step:284/1840 train_time:9572ms step_avg:33.70ms
step:285/1840 train_time:9604ms step_avg:33.70ms
step:286/1840 train_time:9639ms step_avg:33.70ms
step:287/1840 train_time:9671ms step_avg:33.70ms
step:288/1840 train_time:9706ms step_avg:33.70ms
step:289/1840 train_time:9738ms step_avg:33.69ms
step:290/1840 train_time:9772ms step_avg:33.70ms
step:291/1840 train_time:9804ms step_avg:33.69ms
step:292/1840 train_time:9838ms step_avg:33.69ms
step:293/1840 train_time:9870ms step_avg:33.69ms
step:294/1840 train_time:9904ms step_avg:33.69ms
step:295/1840 train_time:9936ms step_avg:33.68ms
step:296/1840 train_time:9970ms step_avg:33.68ms
step:297/1840 train_time:10002ms step_avg:33.68ms
step:298/1840 train_time:10036ms step_avg:33.68ms
step:299/1840 train_time:10068ms step_avg:33.67ms
step:300/1840 train_time:10102ms step_avg:33.67ms
step:301/1840 train_time:10134ms step_avg:33.67ms
step:302/1840 train_time:10169ms step_avg:33.67ms
step:303/1840 train_time:10200ms step_avg:33.66ms
step:304/1840 train_time:10234ms step_avg:33.67ms
step:305/1840 train_time:10266ms step_avg:33.66ms
step:306/1840 train_time:10300ms step_avg:33.66ms
step:307/1840 train_time:10332ms step_avg:33.66ms
step:308/1840 train_time:10366ms step_avg:33.66ms
step:309/1840 train_time:10398ms step_avg:33.65ms
step:310/1840 train_time:10433ms step_avg:33.66ms
step:311/1840 train_time:10465ms step_avg:33.65ms
step:312/1840 train_time:10499ms step_avg:33.65ms
step:313/1840 train_time:10531ms step_avg:33.65ms
step:314/1840 train_time:10566ms step_avg:33.65ms
step:315/1840 train_time:10598ms step_avg:33.64ms
step:316/1840 train_time:10633ms step_avg:33.65ms
step:317/1840 train_time:10665ms step_avg:33.64ms
step:318/1840 train_time:10699ms step_avg:33.64ms
step:319/1840 train_time:10731ms step_avg:33.64ms
step:320/1840 train_time:10766ms step_avg:33.64ms
step:321/1840 train_time:10798ms step_avg:33.64ms
step:322/1840 train_time:10832ms step_avg:33.64ms
step:323/1840 train_time:10865ms step_avg:33.64ms
step:324/1840 train_time:10899ms step_avg:33.64ms
step:325/1840 train_time:10931ms step_avg:33.63ms
step:326/1840 train_time:10965ms step_avg:33.63ms
step:327/1840 train_time:10997ms step_avg:33.63ms
step:328/1840 train_time:11031ms step_avg:33.63ms
step:329/1840 train_time:11063ms step_avg:33.63ms
step:330/1840 train_time:11097ms step_avg:33.63ms
step:331/1840 train_time:11128ms step_avg:33.62ms
step:332/1840 train_time:11162ms step_avg:33.62ms
step:333/1840 train_time:11195ms step_avg:33.62ms
step:334/1840 train_time:11229ms step_avg:33.62ms
step:335/1840 train_time:11260ms step_avg:33.61ms
step:336/1840 train_time:11295ms step_avg:33.61ms
step:337/1840 train_time:11326ms step_avg:33.61ms
step:338/1840 train_time:11360ms step_avg:33.61ms
step:339/1840 train_time:11393ms step_avg:33.61ms
step:340/1840 train_time:11427ms step_avg:33.61ms
step:341/1840 train_time:11459ms step_avg:33.60ms
step:342/1840 train_time:11493ms step_avg:33.61ms
step:343/1840 train_time:11526ms step_avg:33.60ms
step:344/1840 train_time:11560ms step_avg:33.60ms
step:345/1840 train_time:11592ms step_avg:33.60ms
step:346/1840 train_time:11627ms step_avg:33.60ms
step:347/1840 train_time:11659ms step_avg:33.60ms
step:348/1840 train_time:11693ms step_avg:33.60ms
step:349/1840 train_time:11725ms step_avg:33.60ms
step:350/1840 train_time:11760ms step_avg:33.60ms
step:351/1840 train_time:11792ms step_avg:33.59ms
step:352/1840 train_time:11826ms step_avg:33.60ms
step:353/1840 train_time:11858ms step_avg:33.59ms
step:354/1840 train_time:11893ms step_avg:33.60ms
step:355/1840 train_time:11925ms step_avg:33.59ms
step:356/1840 train_time:11959ms step_avg:33.59ms
step:357/1840 train_time:11991ms step_avg:33.59ms
step:358/1840 train_time:12025ms step_avg:33.59ms
step:359/1840 train_time:12057ms step_avg:33.58ms
step:360/1840 train_time:12091ms step_avg:33.59ms
step:361/1840 train_time:12123ms step_avg:33.58ms
step:362/1840 train_time:12157ms step_avg:33.58ms
step:363/1840 train_time:12189ms step_avg:33.58ms
step:364/1840 train_time:12223ms step_avg:33.58ms
step:365/1840 train_time:12255ms step_avg:33.57ms
step:366/1840 train_time:12289ms step_avg:33.58ms
step:367/1840 train_time:12321ms step_avg:33.57ms
step:368/1840 train_time:12355ms step_avg:33.57ms
step:369/1840 train_time:12387ms step_avg:33.57ms
step:370/1840 train_time:12421ms step_avg:33.57ms
step:371/1840 train_time:12453ms step_avg:33.56ms
step:372/1840 train_time:12487ms step_avg:33.57ms
step:373/1840 train_time:12519ms step_avg:33.56ms
step:374/1840 train_time:12553ms step_avg:33.56ms
step:375/1840 train_time:12585ms step_avg:33.56ms
step:376/1840 train_time:12620ms step_avg:33.56ms
step:377/1840 train_time:12652ms step_avg:33.56ms
step:378/1840 train_time:12686ms step_avg:33.56ms
step:379/1840 train_time:12718ms step_avg:33.56ms
step:380/1840 train_time:12752ms step_avg:33.56ms
step:381/1840 train_time:12785ms step_avg:33.56ms
step:382/1840 train_time:12819ms step_avg:33.56ms
step:383/1840 train_time:12851ms step_avg:33.55ms
step:384/1840 train_time:12885ms step_avg:33.56ms
step:385/1840 train_time:12918ms step_avg:33.55ms
step:386/1840 train_time:12951ms step_avg:33.55ms
step:387/1840 train_time:12983ms step_avg:33.55ms
step:388/1840 train_time:13018ms step_avg:33.55ms
step:389/1840 train_time:13050ms step_avg:33.55ms
step:390/1840 train_time:13084ms step_avg:33.55ms
step:391/1840 train_time:13116ms step_avg:33.54ms
step:392/1840 train_time:13150ms step_avg:33.55ms
step:393/1840 train_time:13182ms step_avg:33.54ms
step:394/1840 train_time:13216ms step_avg:33.54ms
step:395/1840 train_time:13248ms step_avg:33.54ms
step:396/1840 train_time:13282ms step_avg:33.54ms
step:397/1840 train_time:13314ms step_avg:33.54ms
step:398/1840 train_time:13348ms step_avg:33.54ms
step:399/1840 train_time:13380ms step_avg:33.53ms
step:400/1840 train_time:13414ms step_avg:33.53ms
step:401/1840 train_time:13446ms step_avg:33.53ms
step:402/1840 train_time:13480ms step_avg:33.53ms
step:403/1840 train_time:13512ms step_avg:33.53ms
step:404/1840 train_time:13546ms step_avg:33.53ms
step:405/1840 train_time:13578ms step_avg:33.53ms
step:406/1840 train_time:13613ms step_avg:33.53ms
step:407/1840 train_time:13645ms step_avg:33.53ms
step:408/1840 train_time:13679ms step_avg:33.53ms
step:409/1840 train_time:13711ms step_avg:33.52ms
step:410/1840 train_time:13745ms step_avg:33.52ms
step:411/1840 train_time:13777ms step_avg:33.52ms
step:412/1840 train_time:13811ms step_avg:33.52ms
step:413/1840 train_time:13843ms step_avg:33.52ms
step:414/1840 train_time:13878ms step_avg:33.52ms
step:415/1840 train_time:13910ms step_avg:33.52ms
step:416/1840 train_time:13944ms step_avg:33.52ms
step:417/1840 train_time:13976ms step_avg:33.52ms
step:418/1840 train_time:14011ms step_avg:33.52ms
step:419/1840 train_time:14043ms step_avg:33.51ms
step:420/1840 train_time:14077ms step_avg:33.52ms
step:421/1840 train_time:14109ms step_avg:33.51ms
step:422/1840 train_time:14143ms step_avg:33.51ms
step:423/1840 train_time:14175ms step_avg:33.51ms
step:424/1840 train_time:14210ms step_avg:33.51ms
step:425/1840 train_time:14242ms step_avg:33.51ms
step:426/1840 train_time:14276ms step_avg:33.51ms
step:427/1840 train_time:14309ms step_avg:33.51ms
step:428/1840 train_time:14342ms step_avg:33.51ms
step:429/1840 train_time:14374ms step_avg:33.51ms
step:430/1840 train_time:14409ms step_avg:33.51ms
step:431/1840 train_time:14441ms step_avg:33.51ms
step:432/1840 train_time:14475ms step_avg:33.51ms
step:433/1840 train_time:14507ms step_avg:33.50ms
step:434/1840 train_time:14541ms step_avg:33.51ms
step:435/1840 train_time:14574ms step_avg:33.50ms
step:436/1840 train_time:14608ms step_avg:33.50ms
step:437/1840 train_time:14640ms step_avg:33.50ms
step:438/1840 train_time:14674ms step_avg:33.50ms
step:439/1840 train_time:14706ms step_avg:33.50ms
step:440/1840 train_time:14740ms step_avg:33.50ms
step:441/1840 train_time:14772ms step_avg:33.50ms
step:442/1840 train_time:14807ms step_avg:33.50ms
step:443/1840 train_time:14839ms step_avg:33.50ms
step:444/1840 train_time:14873ms step_avg:33.50ms
step:445/1840 train_time:14905ms step_avg:33.50ms
step:446/1840 train_time:14940ms step_avg:33.50ms
step:447/1840 train_time:14971ms step_avg:33.49ms
step:448/1840 train_time:15006ms step_avg:33.50ms
step:449/1840 train_time:15038ms step_avg:33.49ms
step:450/1840 train_time:15071ms step_avg:33.49ms
step:451/1840 train_time:15104ms step_avg:33.49ms
step:452/1840 train_time:15138ms step_avg:33.49ms
step:453/1840 train_time:15169ms step_avg:33.49ms
step:454/1840 train_time:15204ms step_avg:33.49ms
step:455/1840 train_time:15236ms step_avg:33.48ms
step:456/1840 train_time:15270ms step_avg:33.49ms
step:457/1840 train_time:15302ms step_avg:33.48ms
step:458/1840 train_time:15336ms step_avg:33.48ms
step:459/1840 train_time:15368ms step_avg:33.48ms
step:460/1840 train_time:15402ms step_avg:33.48ms
step:461/1840 train_time:15434ms step_avg:33.48ms
step:462/1840 train_time:15468ms step_avg:33.48ms
step:463/1840 train_time:15500ms step_avg:33.48ms
step:464/1840 train_time:15535ms step_avg:33.48ms
step:465/1840 train_time:15567ms step_avg:33.48ms
step:466/1840 train_time:15601ms step_avg:33.48ms
step:467/1840 train_time:15633ms step_avg:33.48ms
step:468/1840 train_time:15667ms step_avg:33.48ms
step:469/1840 train_time:15699ms step_avg:33.47ms
step:470/1840 train_time:15733ms step_avg:33.47ms
step:471/1840 train_time:15765ms step_avg:33.47ms
step:472/1840 train_time:15800ms step_avg:33.47ms
step:473/1840 train_time:15832ms step_avg:33.47ms
step:474/1840 train_time:15866ms step_avg:33.47ms
step:475/1840 train_time:15898ms step_avg:33.47ms
step:476/1840 train_time:15932ms step_avg:33.47ms
step:477/1840 train_time:15964ms step_avg:33.47ms
step:478/1840 train_time:15999ms step_avg:33.47ms
step:479/1840 train_time:16031ms step_avg:33.47ms
step:480/1840 train_time:16065ms step_avg:33.47ms
step:481/1840 train_time:16097ms step_avg:33.47ms
step:482/1840 train_time:16131ms step_avg:33.47ms
step:483/1840 train_time:16163ms step_avg:33.46ms
step:484/1840 train_time:16198ms step_avg:33.47ms
step:485/1840 train_time:16230ms step_avg:33.46ms
step:486/1840 train_time:16264ms step_avg:33.46ms
step:487/1840 train_time:16296ms step_avg:33.46ms
step:488/1840 train_time:16330ms step_avg:33.46ms
step:489/1840 train_time:16363ms step_avg:33.46ms
step:490/1840 train_time:16396ms step_avg:33.46ms
step:491/1840 train_time:16429ms step_avg:33.46ms
step:492/1840 train_time:16463ms step_avg:33.46ms
step:493/1840 train_time:16495ms step_avg:33.46ms
step:494/1840 train_time:16529ms step_avg:33.46ms
step:495/1840 train_time:16561ms step_avg:33.46ms
step:496/1840 train_time:16595ms step_avg:33.46ms
step:497/1840 train_time:16627ms step_avg:33.46ms
step:498/1840 train_time:16661ms step_avg:33.46ms
step:499/1840 train_time:16694ms step_avg:33.45ms
step:500/1840 train_time:16728ms step_avg:33.46ms
step:500/1840 val_loss:4.2844 train_time:16770ms step_avg:33.54ms
step:501/1840 train_time:16790ms step_avg:33.51ms
step:502/1840 train_time:16808ms step_avg:33.48ms
step:503/1840 train_time:16829ms step_avg:33.46ms
step:504/1840 train_time:16864ms step_avg:33.46ms
step:505/1840 train_time:16897ms step_avg:33.46ms
step:506/1840 train_time:16932ms step_avg:33.46ms
step:507/1840 train_time:16964ms step_avg:33.46ms
step:508/1840 train_time:16998ms step_avg:33.46ms
step:509/1840 train_time:17030ms step_avg:33.46ms
step:510/1840 train_time:17065ms step_avg:33.46ms
step:511/1840 train_time:17097ms step_avg:33.46ms
step:512/1840 train_time:17131ms step_avg:33.46ms
step:513/1840 train_time:17163ms step_avg:33.46ms
step:514/1840 train_time:17197ms step_avg:33.46ms
step:515/1840 train_time:17228ms step_avg:33.45ms
step:516/1840 train_time:17262ms step_avg:33.45ms
step:517/1840 train_time:17294ms step_avg:33.45ms
step:518/1840 train_time:17328ms step_avg:33.45ms
step:519/1840 train_time:17360ms step_avg:33.45ms
step:520/1840 train_time:17394ms step_avg:33.45ms
step:521/1840 train_time:17426ms step_avg:33.45ms
step:522/1840 train_time:17460ms step_avg:33.45ms
step:523/1840 train_time:17492ms step_avg:33.45ms
step:524/1840 train_time:17526ms step_avg:33.45ms
step:525/1840 train_time:17559ms step_avg:33.44ms
step:526/1840 train_time:17592ms step_avg:33.45ms
step:527/1840 train_time:17624ms step_avg:33.44ms
step:528/1840 train_time:17659ms step_avg:33.45ms
step:529/1840 train_time:17691ms step_avg:33.44ms
step:530/1840 train_time:17726ms step_avg:33.44ms
step:531/1840 train_time:17758ms step_avg:33.44ms
step:532/1840 train_time:17792ms step_avg:33.44ms
step:533/1840 train_time:17825ms step_avg:33.44ms
step:534/1840 train_time:17859ms step_avg:33.44ms
step:535/1840 train_time:17891ms step_avg:33.44ms
step:536/1840 train_time:17926ms step_avg:33.44ms
step:537/1840 train_time:17958ms step_avg:33.44ms
step:538/1840 train_time:17992ms step_avg:33.44ms
step:539/1840 train_time:18025ms step_avg:33.44ms
step:540/1840 train_time:18059ms step_avg:33.44ms
step:541/1840 train_time:18091ms step_avg:33.44ms
step:542/1840 train_time:18125ms step_avg:33.44ms
step:543/1840 train_time:18158ms step_avg:33.44ms
step:544/1840 train_time:18192ms step_avg:33.44ms
step:545/1840 train_time:18224ms step_avg:33.44ms
step:546/1840 train_time:18259ms step_avg:33.44ms
step:547/1840 train_time:18291ms step_avg:33.44ms
step:548/1840 train_time:18325ms step_avg:33.44ms
step:549/1840 train_time:18357ms step_avg:33.44ms
step:550/1840 train_time:18391ms step_avg:33.44ms
step:551/1840 train_time:18423ms step_avg:33.44ms
step:552/1840 train_time:18457ms step_avg:33.44ms
step:553/1840 train_time:18489ms step_avg:33.43ms
step:554/1840 train_time:18523ms step_avg:33.43ms
step:555/1840 train_time:18555ms step_avg:33.43ms
step:556/1840 train_time:18589ms step_avg:33.43ms
step:557/1840 train_time:18621ms step_avg:33.43ms
step:558/1840 train_time:18656ms step_avg:33.43ms
step:559/1840 train_time:18688ms step_avg:33.43ms
step:560/1840 train_time:18722ms step_avg:33.43ms
step:561/1840 train_time:18754ms step_avg:33.43ms
step:562/1840 train_time:18789ms step_avg:33.43ms
step:563/1840 train_time:18821ms step_avg:33.43ms
step:564/1840 train_time:18855ms step_avg:33.43ms
step:565/1840 train_time:18887ms step_avg:33.43ms
step:566/1840 train_time:18922ms step_avg:33.43ms
step:567/1840 train_time:18954ms step_avg:33.43ms
step:568/1840 train_time:18989ms step_avg:33.43ms
step:569/1840 train_time:19021ms step_avg:33.43ms
step:570/1840 train_time:19055ms step_avg:33.43ms
step:571/1840 train_time:19087ms step_avg:33.43ms
step:572/1840 train_time:19121ms step_avg:33.43ms
step:573/1840 train_time:19153ms step_avg:33.43ms
step:574/1840 train_time:19187ms step_avg:33.43ms
step:575/1840 train_time:19219ms step_avg:33.42ms
step:576/1840 train_time:19253ms step_avg:33.43ms
step:577/1840 train_time:19286ms step_avg:33.42ms
step:578/1840 train_time:19319ms step_avg:33.42ms
step:579/1840 train_time:19351ms step_avg:33.42ms
step:580/1840 train_time:19386ms step_avg:33.42ms
step:581/1840 train_time:19418ms step_avg:33.42ms
step:582/1840 train_time:19452ms step_avg:33.42ms
step:583/1840 train_time:19484ms step_avg:33.42ms
step:584/1840 train_time:19518ms step_avg:33.42ms
step:585/1840 train_time:19550ms step_avg:33.42ms
step:586/1840 train_time:19584ms step_avg:33.42ms
step:587/1840 train_time:19616ms step_avg:33.42ms
step:588/1840 train_time:19650ms step_avg:33.42ms
step:589/1840 train_time:19682ms step_avg:33.42ms
step:590/1840 train_time:19716ms step_avg:33.42ms
step:591/1840 train_time:19748ms step_avg:33.41ms
step:592/1840 train_time:19783ms step_avg:33.42ms
step:593/1840 train_time:19815ms step_avg:33.41ms
step:594/1840 train_time:19849ms step_avg:33.42ms
step:595/1840 train_time:19881ms step_avg:33.41ms
step:596/1840 train_time:19916ms step_avg:33.42ms
step:597/1840 train_time:19949ms step_avg:33.41ms
step:598/1840 train_time:19983ms step_avg:33.42ms
step:599/1840 train_time:20015ms step_avg:33.41ms
step:600/1840 train_time:20049ms step_avg:33.41ms
step:601/1840 train_time:20084ms step_avg:33.42ms
step:602/1840 train_time:20142ms step_avg:33.46ms
step:603/1840 train_time:20202ms step_avg:33.50ms
step:604/1840 train_time:20264ms step_avg:33.55ms
step:605/1840 train_time:20324ms step_avg:33.59ms
step:606/1840 train_time:20386ms step_avg:33.64ms
step:607/1840 train_time:20446ms step_avg:33.68ms
step:608/1840 train_time:20507ms step_avg:33.73ms
step:609/1840 train_time:20567ms step_avg:33.77ms
step:610/1840 train_time:20629ms step_avg:33.82ms
step:611/1840 train_time:20689ms step_avg:33.86ms
step:612/1840 train_time:20751ms step_avg:33.91ms
step:613/1840 train_time:20810ms step_avg:33.95ms
step:614/1840 train_time:20873ms step_avg:34.00ms
step:615/1840 train_time:20933ms step_avg:34.04ms
step:616/1840 train_time:20996ms step_avg:34.08ms
step:617/1840 train_time:21054ms step_avg:34.12ms
step:618/1840 train_time:21117ms step_avg:34.17ms
step:619/1840 train_time:21176ms step_avg:34.21ms
step:620/1840 train_time:21238ms step_avg:34.26ms
step:621/1840 train_time:21299ms step_avg:34.30ms
step:622/1840 train_time:21362ms step_avg:34.34ms
step:623/1840 train_time:21421ms step_avg:34.38ms
step:624/1840 train_time:21483ms step_avg:34.43ms
step:625/1840 train_time:21543ms step_avg:34.47ms
step:626/1840 train_time:21606ms step_avg:34.51ms
step:627/1840 train_time:21667ms step_avg:34.56ms
step:628/1840 train_time:21729ms step_avg:34.60ms
step:629/1840 train_time:21788ms step_avg:34.64ms
step:630/1840 train_time:21850ms step_avg:34.68ms
step:631/1840 train_time:21911ms step_avg:34.72ms
step:632/1840 train_time:21972ms step_avg:34.77ms
step:633/1840 train_time:22031ms step_avg:34.80ms
step:634/1840 train_time:22093ms step_avg:34.85ms
step:635/1840 train_time:22153ms step_avg:34.89ms
step:636/1840 train_time:22215ms step_avg:34.93ms
step:637/1840 train_time:22275ms step_avg:34.97ms
step:638/1840 train_time:22338ms step_avg:35.01ms
step:639/1840 train_time:22398ms step_avg:35.05ms
step:640/1840 train_time:22460ms step_avg:35.09ms
step:641/1840 train_time:22520ms step_avg:35.13ms
step:642/1840 train_time:22583ms step_avg:35.18ms
step:643/1840 train_time:22643ms step_avg:35.21ms
step:644/1840 train_time:22705ms step_avg:35.26ms
step:645/1840 train_time:22765ms step_avg:35.29ms
step:646/1840 train_time:22828ms step_avg:35.34ms
step:647/1840 train_time:22888ms step_avg:35.38ms
step:648/1840 train_time:22950ms step_avg:35.42ms
step:649/1840 train_time:23010ms step_avg:35.45ms
step:650/1840 train_time:23073ms step_avg:35.50ms
step:651/1840 train_time:23133ms step_avg:35.53ms
step:652/1840 train_time:23194ms step_avg:35.57ms
step:653/1840 train_time:23254ms step_avg:35.61ms
step:654/1840 train_time:23317ms step_avg:35.65ms
step:655/1840 train_time:23377ms step_avg:35.69ms
step:656/1840 train_time:23440ms step_avg:35.73ms
step:657/1840 train_time:23499ms step_avg:35.77ms
step:658/1840 train_time:23562ms step_avg:35.81ms
step:659/1840 train_time:23622ms step_avg:35.85ms
step:660/1840 train_time:23684ms step_avg:35.88ms
step:661/1840 train_time:23744ms step_avg:35.92ms
step:662/1840 train_time:23806ms step_avg:35.96ms
step:663/1840 train_time:23866ms step_avg:36.00ms
step:664/1840 train_time:23929ms step_avg:36.04ms
step:665/1840 train_time:23989ms step_avg:36.07ms
step:666/1840 train_time:24051ms step_avg:36.11ms
step:667/1840 train_time:24110ms step_avg:36.15ms
step:668/1840 train_time:24173ms step_avg:36.19ms
step:669/1840 train_time:24233ms step_avg:36.22ms
step:670/1840 train_time:24295ms step_avg:36.26ms
step:671/1840 train_time:24354ms step_avg:36.30ms
step:672/1840 train_time:24417ms step_avg:36.33ms
step:673/1840 train_time:24477ms step_avg:36.37ms
step:674/1840 train_time:24540ms step_avg:36.41ms
step:675/1840 train_time:24599ms step_avg:36.44ms
step:676/1840 train_time:24661ms step_avg:36.48ms
step:677/1840 train_time:24721ms step_avg:36.52ms
step:678/1840 train_time:24784ms step_avg:36.55ms
step:679/1840 train_time:24844ms step_avg:36.59ms
step:680/1840 train_time:24906ms step_avg:36.63ms
step:681/1840 train_time:24966ms step_avg:36.66ms
step:682/1840 train_time:25028ms step_avg:36.70ms
step:683/1840 train_time:25088ms step_avg:36.73ms
step:684/1840 train_time:25150ms step_avg:36.77ms
step:685/1840 train_time:25211ms step_avg:36.80ms
step:686/1840 train_time:25272ms step_avg:36.84ms
step:687/1840 train_time:25332ms step_avg:36.87ms
step:688/1840 train_time:25395ms step_avg:36.91ms
step:689/1840 train_time:25454ms step_avg:36.94ms
step:690/1840 train_time:25516ms step_avg:36.98ms
step:691/1840 train_time:25576ms step_avg:37.01ms
step:692/1840 train_time:25638ms step_avg:37.05ms
step:693/1840 train_time:25698ms step_avg:37.08ms
step:694/1840 train_time:25761ms step_avg:37.12ms
step:695/1840 train_time:25821ms step_avg:37.15ms
step:696/1840 train_time:25884ms step_avg:37.19ms
step:697/1840 train_time:25944ms step_avg:37.22ms
step:698/1840 train_time:26007ms step_avg:37.26ms
step:699/1840 train_time:26067ms step_avg:37.29ms
step:700/1840 train_time:26130ms step_avg:37.33ms
step:701/1840 train_time:26190ms step_avg:37.36ms
step:702/1840 train_time:26252ms step_avg:37.40ms
step:703/1840 train_time:26311ms step_avg:37.43ms
step:704/1840 train_time:26373ms step_avg:37.46ms
step:705/1840 train_time:26432ms step_avg:37.49ms
step:706/1840 train_time:26495ms step_avg:37.53ms
step:707/1840 train_time:26555ms step_avg:37.56ms
step:708/1840 train_time:26617ms step_avg:37.59ms
step:709/1840 train_time:26676ms step_avg:37.63ms
step:710/1840 train_time:26739ms step_avg:37.66ms
step:711/1840 train_time:26799ms step_avg:37.69ms
step:712/1840 train_time:26861ms step_avg:37.73ms
step:713/1840 train_time:26921ms step_avg:37.76ms
step:714/1840 train_time:26983ms step_avg:37.79ms
step:715/1840 train_time:27043ms step_avg:37.82ms
step:716/1840 train_time:27107ms step_avg:37.86ms
step:717/1840 train_time:27167ms step_avg:37.89ms
step:718/1840 train_time:27230ms step_avg:37.92ms
step:719/1840 train_time:27289ms step_avg:37.95ms
step:720/1840 train_time:27351ms step_avg:37.99ms
step:721/1840 train_time:27411ms step_avg:38.02ms
step:722/1840 train_time:27474ms step_avg:38.05ms
step:723/1840 train_time:27533ms step_avg:38.08ms
step:724/1840 train_time:27595ms step_avg:38.12ms
step:725/1840 train_time:27655ms step_avg:38.14ms
step:726/1840 train_time:27718ms step_avg:38.18ms
step:727/1840 train_time:27777ms step_avg:38.21ms
step:728/1840 train_time:27840ms step_avg:38.24ms
step:729/1840 train_time:27900ms step_avg:38.27ms
step:730/1840 train_time:27963ms step_avg:38.31ms
step:731/1840 train_time:28023ms step_avg:38.34ms
step:732/1840 train_time:28086ms step_avg:38.37ms
step:733/1840 train_time:28146ms step_avg:38.40ms
step:734/1840 train_time:28208ms step_avg:38.43ms
step:735/1840 train_time:28268ms step_avg:38.46ms
step:736/1840 train_time:28330ms step_avg:38.49ms
step:737/1840 train_time:28390ms step_avg:38.52ms
step:738/1840 train_time:28452ms step_avg:38.55ms
step:739/1840 train_time:28511ms step_avg:38.58ms
step:740/1840 train_time:28574ms step_avg:38.61ms
step:741/1840 train_time:28633ms step_avg:38.64ms
step:742/1840 train_time:28697ms step_avg:38.67ms
step:743/1840 train_time:28756ms step_avg:38.70ms
step:744/1840 train_time:28819ms step_avg:38.74ms
step:745/1840 train_time:28878ms step_avg:38.76ms
step:746/1840 train_time:28941ms step_avg:38.79ms
step:747/1840 train_time:29001ms step_avg:38.82ms
step:748/1840 train_time:29064ms step_avg:38.86ms
step:749/1840 train_time:29123ms step_avg:38.88ms
step:750/1840 train_time:29186ms step_avg:38.91ms
step:750/1840 val_loss:4.0142 train_time:29258ms step_avg:39.01ms
step:751/1840 train_time:29277ms step_avg:38.98ms
step:752/1840 train_time:29310ms step_avg:38.98ms
step:753/1840 train_time:29373ms step_avg:39.01ms
step:754/1840 train_time:29435ms step_avg:39.04ms
step:755/1840 train_time:29495ms step_avg:39.07ms
step:756/1840 train_time:29557ms step_avg:39.10ms
step:757/1840 train_time:29616ms step_avg:39.12ms
step:758/1840 train_time:29678ms step_avg:39.15ms
step:759/1840 train_time:29736ms step_avg:39.18ms
step:760/1840 train_time:29798ms step_avg:39.21ms
step:761/1840 train_time:29857ms step_avg:39.23ms
step:762/1840 train_time:29919ms step_avg:39.26ms
step:763/1840 train_time:29978ms step_avg:39.29ms
step:764/1840 train_time:30040ms step_avg:39.32ms
step:765/1840 train_time:30100ms step_avg:39.35ms
step:766/1840 train_time:30162ms step_avg:39.38ms
step:767/1840 train_time:30226ms step_avg:39.41ms
step:768/1840 train_time:30290ms step_avg:39.44ms
step:769/1840 train_time:30351ms step_avg:39.47ms
step:770/1840 train_time:30414ms step_avg:39.50ms
step:771/1840 train_time:30474ms step_avg:39.53ms
step:772/1840 train_time:30537ms step_avg:39.56ms
step:773/1840 train_time:30596ms step_avg:39.58ms
step:774/1840 train_time:30658ms step_avg:39.61ms
step:775/1840 train_time:30718ms step_avg:39.64ms
step:776/1840 train_time:30780ms step_avg:39.66ms
step:777/1840 train_time:30838ms step_avg:39.69ms
step:778/1840 train_time:30900ms step_avg:39.72ms
step:779/1840 train_time:30959ms step_avg:39.74ms
step:780/1840 train_time:31021ms step_avg:39.77ms
step:781/1840 train_time:31081ms step_avg:39.80ms
step:782/1840 train_time:31143ms step_avg:39.82ms
step:783/1840 train_time:31204ms step_avg:39.85ms
step:784/1840 train_time:31268ms step_avg:39.88ms
step:785/1840 train_time:31328ms step_avg:39.91ms
step:786/1840 train_time:31391ms step_avg:39.94ms
step:787/1840 train_time:31451ms step_avg:39.96ms
step:788/1840 train_time:31513ms step_avg:39.99ms
step:789/1840 train_time:31573ms step_avg:40.02ms
step:790/1840 train_time:31635ms step_avg:40.04ms
step:791/1840 train_time:31695ms step_avg:40.07ms
step:792/1840 train_time:31757ms step_avg:40.10ms
step:793/1840 train_time:31817ms step_avg:40.12ms
step:794/1840 train_time:31878ms step_avg:40.15ms
step:795/1840 train_time:31938ms step_avg:40.17ms
step:796/1840 train_time:31999ms step_avg:40.20ms
step:797/1840 train_time:32059ms step_avg:40.22ms
step:798/1840 train_time:32121ms step_avg:40.25ms
step:799/1840 train_time:32181ms step_avg:40.28ms
step:800/1840 train_time:32244ms step_avg:40.31ms
step:801/1840 train_time:32304ms step_avg:40.33ms
step:802/1840 train_time:32367ms step_avg:40.36ms
step:803/1840 train_time:32428ms step_avg:40.38ms
step:804/1840 train_time:32490ms step_avg:40.41ms
step:805/1840 train_time:32549ms step_avg:40.43ms
step:806/1840 train_time:32612ms step_avg:40.46ms
step:807/1840 train_time:32671ms step_avg:40.48ms
step:808/1840 train_time:32734ms step_avg:40.51ms
step:809/1840 train_time:32793ms step_avg:40.54ms
step:810/1840 train_time:32855ms step_avg:40.56ms
step:811/1840 train_time:32915ms step_avg:40.59ms
step:812/1840 train_time:32978ms step_avg:40.61ms
step:813/1840 train_time:33037ms step_avg:40.64ms
step:814/1840 train_time:33100ms step_avg:40.66ms
step:815/1840 train_time:33159ms step_avg:40.69ms
step:816/1840 train_time:33222ms step_avg:40.71ms
step:817/1840 train_time:33282ms step_avg:40.74ms
step:818/1840 train_time:33345ms step_avg:40.76ms
step:819/1840 train_time:33405ms step_avg:40.79ms
step:820/1840 train_time:33467ms step_avg:40.81ms
step:821/1840 train_time:33528ms step_avg:40.84ms
step:822/1840 train_time:33590ms step_avg:40.86ms
step:823/1840 train_time:33650ms step_avg:40.89ms
step:824/1840 train_time:33712ms step_avg:40.91ms
step:825/1840 train_time:33772ms step_avg:40.94ms
step:826/1840 train_time:33835ms step_avg:40.96ms
step:827/1840 train_time:33894ms step_avg:40.98ms
step:828/1840 train_time:33956ms step_avg:41.01ms
step:829/1840 train_time:34016ms step_avg:41.03ms
step:830/1840 train_time:34078ms step_avg:41.06ms
step:831/1840 train_time:34139ms step_avg:41.08ms
step:832/1840 train_time:34202ms step_avg:41.11ms
step:833/1840 train_time:34261ms step_avg:41.13ms
step:834/1840 train_time:34324ms step_avg:41.16ms
step:835/1840 train_time:34384ms step_avg:41.18ms
step:836/1840 train_time:34446ms step_avg:41.20ms
step:837/1840 train_time:34505ms step_avg:41.22ms
step:838/1840 train_time:34567ms step_avg:41.25ms
step:839/1840 train_time:34628ms step_avg:41.27ms
step:840/1840 train_time:34690ms step_avg:41.30ms
step:841/1840 train_time:34750ms step_avg:41.32ms
step:842/1840 train_time:34812ms step_avg:41.34ms
step:843/1840 train_time:34872ms step_avg:41.37ms
step:844/1840 train_time:34934ms step_avg:41.39ms
step:845/1840 train_time:34993ms step_avg:41.41ms
step:846/1840 train_time:35055ms step_avg:41.44ms
step:847/1840 train_time:35115ms step_avg:41.46ms
step:848/1840 train_time:35178ms step_avg:41.48ms
step:849/1840 train_time:35239ms step_avg:41.51ms
step:850/1840 train_time:35302ms step_avg:41.53ms
step:851/1840 train_time:35362ms step_avg:41.55ms
step:852/1840 train_time:35423ms step_avg:41.58ms
step:853/1840 train_time:35484ms step_avg:41.60ms
step:854/1840 train_time:35545ms step_avg:41.62ms
step:855/1840 train_time:35605ms step_avg:41.64ms
step:856/1840 train_time:35668ms step_avg:41.67ms
step:857/1840 train_time:35728ms step_avg:41.69ms
step:858/1840 train_time:35791ms step_avg:41.71ms
step:859/1840 train_time:35850ms step_avg:41.74ms
step:860/1840 train_time:35913ms step_avg:41.76ms
step:861/1840 train_time:35973ms step_avg:41.78ms
step:862/1840 train_time:36036ms step_avg:41.80ms
step:863/1840 train_time:36095ms step_avg:41.83ms
step:864/1840 train_time:36157ms step_avg:41.85ms
step:865/1840 train_time:36217ms step_avg:41.87ms
step:866/1840 train_time:36280ms step_avg:41.89ms
step:867/1840 train_time:36340ms step_avg:41.91ms
step:868/1840 train_time:36402ms step_avg:41.94ms
step:869/1840 train_time:36461ms step_avg:41.96ms
step:870/1840 train_time:36524ms step_avg:41.98ms
step:871/1840 train_time:36584ms step_avg:42.00ms
step:872/1840 train_time:36646ms step_avg:42.03ms
step:873/1840 train_time:36705ms step_avg:42.04ms
step:874/1840 train_time:36768ms step_avg:42.07ms
step:875/1840 train_time:36828ms step_avg:42.09ms
step:876/1840 train_time:36891ms step_avg:42.11ms
step:877/1840 train_time:36950ms step_avg:42.13ms
step:878/1840 train_time:37013ms step_avg:42.16ms
step:879/1840 train_time:37073ms step_avg:42.18ms
step:880/1840 train_time:37134ms step_avg:42.20ms
step:881/1840 train_time:37194ms step_avg:42.22ms
step:882/1840 train_time:37257ms step_avg:42.24ms
step:883/1840 train_time:37317ms step_avg:42.26ms
step:884/1840 train_time:37379ms step_avg:42.28ms
step:885/1840 train_time:37440ms step_avg:42.30ms
step:886/1840 train_time:37503ms step_avg:42.33ms
step:887/1840 train_time:37562ms step_avg:42.35ms
step:888/1840 train_time:37624ms step_avg:42.37ms
step:889/1840 train_time:37684ms step_avg:42.39ms
step:890/1840 train_time:37746ms step_avg:42.41ms
step:891/1840 train_time:37805ms step_avg:42.43ms
step:892/1840 train_time:37869ms step_avg:42.45ms
step:893/1840 train_time:37929ms step_avg:42.47ms
step:894/1840 train_time:37992ms step_avg:42.50ms
step:895/1840 train_time:38051ms step_avg:42.52ms
step:896/1840 train_time:38113ms step_avg:42.54ms
step:897/1840 train_time:38174ms step_avg:42.56ms
step:898/1840 train_time:38236ms step_avg:42.58ms
step:899/1840 train_time:38296ms step_avg:42.60ms
step:900/1840 train_time:38359ms step_avg:42.62ms
step:901/1840 train_time:38418ms step_avg:42.64ms
step:902/1840 train_time:38481ms step_avg:42.66ms
step:903/1840 train_time:38540ms step_avg:42.68ms
step:904/1840 train_time:38602ms step_avg:42.70ms
step:905/1840 train_time:38662ms step_avg:42.72ms
step:906/1840 train_time:38726ms step_avg:42.74ms
step:907/1840 train_time:38785ms step_avg:42.76ms
step:908/1840 train_time:38848ms step_avg:42.78ms
step:909/1840 train_time:38908ms step_avg:42.80ms
step:910/1840 train_time:38970ms step_avg:42.82ms
step:911/1840 train_time:39030ms step_avg:42.84ms
step:912/1840 train_time:39092ms step_avg:42.86ms
step:913/1840 train_time:39152ms step_avg:42.88ms
step:914/1840 train_time:39215ms step_avg:42.90ms
step:915/1840 train_time:39275ms step_avg:42.92ms
step:916/1840 train_time:39337ms step_avg:42.94ms
step:917/1840 train_time:39398ms step_avg:42.96ms
step:918/1840 train_time:39460ms step_avg:42.98ms
step:919/1840 train_time:39521ms step_avg:43.00ms
step:920/1840 train_time:39583ms step_avg:43.02ms
step:921/1840 train_time:39642ms step_avg:43.04ms
step:922/1840 train_time:39705ms step_avg:43.06ms
step:923/1840 train_time:39764ms step_avg:43.08ms
step:924/1840 train_time:39827ms step_avg:43.10ms
step:925/1840 train_time:39886ms step_avg:43.12ms
step:926/1840 train_time:39948ms step_avg:43.14ms
step:927/1840 train_time:40008ms step_avg:43.16ms
step:928/1840 train_time:40070ms step_avg:43.18ms
step:929/1840 train_time:40131ms step_avg:43.20ms
step:930/1840 train_time:40193ms step_avg:43.22ms
step:931/1840 train_time:40253ms step_avg:43.24ms
step:932/1840 train_time:40316ms step_avg:43.26ms
step:933/1840 train_time:40375ms step_avg:43.27ms
step:934/1840 train_time:40439ms step_avg:43.30ms
step:935/1840 train_time:40499ms step_avg:43.31ms
step:936/1840 train_time:40561ms step_avg:43.33ms
step:937/1840 train_time:40621ms step_avg:43.35ms
step:938/1840 train_time:40682ms step_avg:43.37ms
step:939/1840 train_time:40743ms step_avg:43.39ms
step:940/1840 train_time:40804ms step_avg:43.41ms
step:941/1840 train_time:40865ms step_avg:43.43ms
step:942/1840 train_time:40926ms step_avg:43.45ms
step:943/1840 train_time:40986ms step_avg:43.46ms
step:944/1840 train_time:41050ms step_avg:43.48ms
step:945/1840 train_time:41109ms step_avg:43.50ms
step:946/1840 train_time:41172ms step_avg:43.52ms
step:947/1840 train_time:41231ms step_avg:43.54ms
step:948/1840 train_time:41293ms step_avg:43.56ms
step:949/1840 train_time:41353ms step_avg:43.58ms
step:950/1840 train_time:41416ms step_avg:43.60ms
step:951/1840 train_time:41476ms step_avg:43.61ms
step:952/1840 train_time:41538ms step_avg:43.63ms
step:953/1840 train_time:41599ms step_avg:43.65ms
step:954/1840 train_time:41662ms step_avg:43.67ms
step:955/1840 train_time:41722ms step_avg:43.69ms
step:956/1840 train_time:41784ms step_avg:43.71ms
step:957/1840 train_time:41843ms step_avg:43.72ms
step:958/1840 train_time:41906ms step_avg:43.74ms
step:959/1840 train_time:41965ms step_avg:43.76ms
step:960/1840 train_time:42028ms step_avg:43.78ms
step:961/1840 train_time:42088ms step_avg:43.80ms
step:962/1840 train_time:42151ms step_avg:43.82ms
step:963/1840 train_time:42210ms step_avg:43.83ms
step:964/1840 train_time:42272ms step_avg:43.85ms
step:965/1840 train_time:42333ms step_avg:43.87ms
step:966/1840 train_time:42395ms step_avg:43.89ms
step:967/1840 train_time:42454ms step_avg:43.90ms
step:968/1840 train_time:42516ms step_avg:43.92ms
step:969/1840 train_time:42576ms step_avg:43.94ms
step:970/1840 train_time:42638ms step_avg:43.96ms
step:971/1840 train_time:42698ms step_avg:43.97ms
step:972/1840 train_time:42760ms step_avg:43.99ms
step:973/1840 train_time:42821ms step_avg:44.01ms
step:974/1840 train_time:42883ms step_avg:44.03ms
step:975/1840 train_time:42944ms step_avg:44.04ms
step:976/1840 train_time:43006ms step_avg:44.06ms
step:977/1840 train_time:43065ms step_avg:44.08ms
step:978/1840 train_time:43128ms step_avg:44.10ms
step:979/1840 train_time:43188ms step_avg:44.11ms
step:980/1840 train_time:43252ms step_avg:44.13ms
step:981/1840 train_time:43312ms step_avg:44.15ms
step:982/1840 train_time:43375ms step_avg:44.17ms
step:983/1840 train_time:43435ms step_avg:44.19ms
step:984/1840 train_time:43497ms step_avg:44.20ms
step:985/1840 train_time:43557ms step_avg:44.22ms
step:986/1840 train_time:43619ms step_avg:44.24ms
step:987/1840 train_time:43679ms step_avg:44.25ms
step:988/1840 train_time:43741ms step_avg:44.27ms
step:989/1840 train_time:43801ms step_avg:44.29ms
step:990/1840 train_time:43863ms step_avg:44.31ms
step:991/1840 train_time:43924ms step_avg:44.32ms
step:992/1840 train_time:43985ms step_avg:44.34ms
step:993/1840 train_time:44046ms step_avg:44.36ms
step:994/1840 train_time:44108ms step_avg:44.37ms
step:995/1840 train_time:44168ms step_avg:44.39ms
step:996/1840 train_time:44231ms step_avg:44.41ms
step:997/1840 train_time:44291ms step_avg:44.42ms
step:998/1840 train_time:44352ms step_avg:44.44ms
step:999/1840 train_time:44414ms step_avg:44.46ms
step:1000/1840 train_time:44476ms step_avg:44.48ms
step:1000/1840 val_loss:3.7707 train_time:44547ms step_avg:44.55ms
step:1001/1840 train_time:44566ms step_avg:44.52ms
step:1002/1840 train_time:44599ms step_avg:44.51ms
step:1003/1840 train_time:44659ms step_avg:44.53ms
step:1004/1840 train_time:44722ms step_avg:44.54ms
step:1005/1840 train_time:44782ms step_avg:44.56ms
step:1006/1840 train_time:44845ms step_avg:44.58ms
step:1007/1840 train_time:44904ms step_avg:44.59ms
step:1008/1840 train_time:44966ms step_avg:44.61ms
step:1009/1840 train_time:45025ms step_avg:44.62ms
step:1010/1840 train_time:45086ms step_avg:44.64ms
step:1011/1840 train_time:45145ms step_avg:44.65ms
step:1012/1840 train_time:45207ms step_avg:44.67ms
step:1013/1840 train_time:45266ms step_avg:44.68ms
step:1014/1840 train_time:45327ms step_avg:44.70ms
step:1015/1840 train_time:45386ms step_avg:44.72ms
step:1016/1840 train_time:45448ms step_avg:44.73ms
step:1017/1840 train_time:45509ms step_avg:44.75ms
step:1018/1840 train_time:45572ms step_avg:44.77ms
step:1019/1840 train_time:45634ms step_avg:44.78ms
step:1020/1840 train_time:45698ms step_avg:44.80ms
step:1021/1840 train_time:45757ms step_avg:44.82ms
step:1022/1840 train_time:45819ms step_avg:44.83ms
step:1023/1840 train_time:45878ms step_avg:44.85ms
step:1024/1840 train_time:45940ms step_avg:44.86ms
step:1025/1840 train_time:45999ms step_avg:44.88ms
step:1026/1840 train_time:46061ms step_avg:44.89ms
step:1027/1840 train_time:46120ms step_avg:44.91ms
step:1028/1840 train_time:46182ms step_avg:44.92ms
step:1029/1840 train_time:46241ms step_avg:44.94ms
step:1030/1840 train_time:46303ms step_avg:44.95ms
step:1031/1840 train_time:46363ms step_avg:44.97ms
step:1032/1840 train_time:46425ms step_avg:44.99ms
step:1033/1840 train_time:46484ms step_avg:45.00ms
step:1034/1840 train_time:46548ms step_avg:45.02ms
step:1035/1840 train_time:46608ms step_avg:45.03ms
step:1036/1840 train_time:46671ms step_avg:45.05ms
step:1037/1840 train_time:46732ms step_avg:45.06ms
step:1038/1840 train_time:46794ms step_avg:45.08ms
step:1039/1840 train_time:46854ms step_avg:45.09ms
step:1040/1840 train_time:46916ms step_avg:45.11ms
step:1041/1840 train_time:46976ms step_avg:45.13ms
step:1042/1840 train_time:47037ms step_avg:45.14ms
step:1043/1840 train_time:47096ms step_avg:45.15ms
step:1044/1840 train_time:47159ms step_avg:45.17ms
step:1045/1840 train_time:47218ms step_avg:45.18ms
step:1046/1840 train_time:47280ms step_avg:45.20ms
step:1047/1840 train_time:47340ms step_avg:45.21ms
step:1048/1840 train_time:47402ms step_avg:45.23ms
step:1049/1840 train_time:47462ms step_avg:45.24ms
step:1050/1840 train_time:47525ms step_avg:45.26ms
step:1051/1840 train_time:47586ms step_avg:45.28ms
step:1052/1840 train_time:47648ms step_avg:45.29ms
step:1053/1840 train_time:47708ms step_avg:45.31ms
step:1054/1840 train_time:47771ms step_avg:45.32ms
step:1055/1840 train_time:47831ms step_avg:45.34ms
step:1056/1840 train_time:47893ms step_avg:45.35ms
step:1057/1840 train_time:47953ms step_avg:45.37ms
step:1058/1840 train_time:48015ms step_avg:45.38ms
step:1059/1840 train_time:48075ms step_avg:45.40ms
step:1060/1840 train_time:48137ms step_avg:45.41ms
step:1061/1840 train_time:48197ms step_avg:45.43ms
step:1062/1840 train_time:48259ms step_avg:45.44ms
step:1063/1840 train_time:48318ms step_avg:45.45ms
step:1064/1840 train_time:48380ms step_avg:45.47ms
step:1065/1840 train_time:48440ms step_avg:45.48ms
step:1066/1840 train_time:48503ms step_avg:45.50ms
step:1067/1840 train_time:48563ms step_avg:45.51ms
step:1068/1840 train_time:48627ms step_avg:45.53ms
step:1069/1840 train_time:48686ms step_avg:45.54ms
step:1070/1840 train_time:48749ms step_avg:45.56ms
step:1071/1840 train_time:48809ms step_avg:45.57ms
step:1072/1840 train_time:48872ms step_avg:45.59ms
step:1073/1840 train_time:48932ms step_avg:45.60ms
step:1074/1840 train_time:48994ms step_avg:45.62ms
step:1075/1840 train_time:49053ms step_avg:45.63ms
step:1076/1840 train_time:49116ms step_avg:45.65ms
step:1077/1840 train_time:49176ms step_avg:45.66ms
step:1078/1840 train_time:49238ms step_avg:45.68ms
step:1079/1840 train_time:49297ms step_avg:45.69ms
step:1080/1840 train_time:49359ms step_avg:45.70ms
step:1081/1840 train_time:49419ms step_avg:45.72ms
step:1082/1840 train_time:49482ms step_avg:45.73ms
step:1083/1840 train_time:49541ms step_avg:45.74ms
step:1084/1840 train_time:49604ms step_avg:45.76ms
step:1085/1840 train_time:49665ms step_avg:45.77ms
step:1086/1840 train_time:49727ms step_avg:45.79ms
step:1087/1840 train_time:49786ms step_avg:45.80ms
step:1088/1840 train_time:49849ms step_avg:45.82ms
step:1089/1840 train_time:49909ms step_avg:45.83ms
step:1090/1840 train_time:49972ms step_avg:45.85ms
step:1091/1840 train_time:50032ms step_avg:45.86ms
step:1092/1840 train_time:50094ms step_avg:45.87ms
step:1093/1840 train_time:50155ms step_avg:45.89ms
step:1094/1840 train_time:50217ms step_avg:45.90ms
step:1095/1840 train_time:50277ms step_avg:45.91ms
step:1096/1840 train_time:50338ms step_avg:45.93ms
step:1097/1840 train_time:50398ms step_avg:45.94ms
step:1098/1840 train_time:50461ms step_avg:45.96ms
step:1099/1840 train_time:50521ms step_avg:45.97ms
step:1100/1840 train_time:50583ms step_avg:45.98ms
step:1101/1840 train_time:50643ms step_avg:46.00ms
step:1102/1840 train_time:50705ms step_avg:46.01ms
step:1103/1840 train_time:50765ms step_avg:46.02ms
step:1104/1840 train_time:50827ms step_avg:46.04ms
step:1105/1840 train_time:50887ms step_avg:46.05ms
step:1106/1840 train_time:50951ms step_avg:46.07ms
step:1107/1840 train_time:51011ms step_avg:46.08ms
step:1108/1840 train_time:51073ms step_avg:46.10ms
step:1109/1840 train_time:51133ms step_avg:46.11ms
step:1110/1840 train_time:51195ms step_avg:46.12ms
step:1111/1840 train_time:51255ms step_avg:46.13ms
step:1112/1840 train_time:51317ms step_avg:46.15ms
step:1113/1840 train_time:51377ms step_avg:46.16ms
step:1114/1840 train_time:51439ms step_avg:46.17ms
step:1115/1840 train_time:51498ms step_avg:46.19ms
step:1116/1840 train_time:51561ms step_avg:46.20ms
step:1117/1840 train_time:51620ms step_avg:46.21ms
step:1118/1840 train_time:51682ms step_avg:46.23ms
step:1119/1840 train_time:51742ms step_avg:46.24ms
step:1120/1840 train_time:51805ms step_avg:46.25ms
step:1121/1840 train_time:51866ms step_avg:46.27ms
step:1122/1840 train_time:51929ms step_avg:46.28ms
step:1123/1840 train_time:51989ms step_avg:46.29ms
step:1124/1840 train_time:52051ms step_avg:46.31ms
step:1125/1840 train_time:52111ms step_avg:46.32ms
step:1126/1840 train_time:52173ms step_avg:46.34ms
step:1127/1840 train_time:52234ms step_avg:46.35ms
step:1128/1840 train_time:52296ms step_avg:46.36ms
step:1129/1840 train_time:52357ms step_avg:46.37ms
step:1130/1840 train_time:52419ms step_avg:46.39ms
step:1131/1840 train_time:52478ms step_avg:46.40ms
step:1132/1840 train_time:52540ms step_avg:46.41ms
step:1133/1840 train_time:52600ms step_avg:46.43ms
step:1134/1840 train_time:52662ms step_avg:46.44ms
step:1135/1840 train_time:52722ms step_avg:46.45ms
step:1136/1840 train_time:52784ms step_avg:46.46ms
step:1137/1840 train_time:52844ms step_avg:46.48ms
step:1138/1840 train_time:52906ms step_avg:46.49ms
step:1139/1840 train_time:52966ms step_avg:46.50ms
step:1140/1840 train_time:53029ms step_avg:46.52ms
step:1141/1840 train_time:53089ms step_avg:46.53ms
step:1142/1840 train_time:53151ms step_avg:46.54ms
step:1143/1840 train_time:53211ms step_avg:46.55ms
step:1144/1840 train_time:53273ms step_avg:46.57ms
step:1145/1840 train_time:53333ms step_avg:46.58ms
step:1146/1840 train_time:53396ms step_avg:46.59ms
step:1147/1840 train_time:53457ms step_avg:46.61ms
step:1148/1840 train_time:53520ms step_avg:46.62ms
step:1149/1840 train_time:53579ms step_avg:46.63ms
step:1150/1840 train_time:53641ms step_avg:46.64ms
step:1151/1840 train_time:53701ms step_avg:46.66ms
step:1152/1840 train_time:53764ms step_avg:46.67ms
step:1153/1840 train_time:53823ms step_avg:46.68ms
step:1154/1840 train_time:53886ms step_avg:46.70ms
step:1155/1840 train_time:53946ms step_avg:46.71ms
step:1156/1840 train_time:54008ms step_avg:46.72ms
step:1157/1840 train_time:54068ms step_avg:46.73ms
step:1158/1840 train_time:54131ms step_avg:46.74ms
step:1159/1840 train_time:54190ms step_avg:46.76ms
step:1160/1840 train_time:54252ms step_avg:46.77ms
step:1161/1840 train_time:54312ms step_avg:46.78ms
step:1162/1840 train_time:54375ms step_avg:46.79ms
step:1163/1840 train_time:54436ms step_avg:46.81ms
step:1164/1840 train_time:54498ms step_avg:46.82ms
step:1165/1840 train_time:54558ms step_avg:46.83ms
step:1166/1840 train_time:54620ms step_avg:46.84ms
step:1167/1840 train_time:54680ms step_avg:46.85ms
step:1168/1840 train_time:54742ms step_avg:46.87ms
step:1169/1840 train_time:54802ms step_avg:46.88ms
step:1170/1840 train_time:54865ms step_avg:46.89ms
step:1171/1840 train_time:54924ms step_avg:46.90ms
step:1172/1840 train_time:54987ms step_avg:46.92ms
step:1173/1840 train_time:55046ms step_avg:46.93ms
step:1174/1840 train_time:55109ms step_avg:46.94ms
step:1175/1840 train_time:55170ms step_avg:46.95ms
step:1176/1840 train_time:55233ms step_avg:46.97ms
step:1177/1840 train_time:55292ms step_avg:46.98ms
step:1178/1840 train_time:55354ms step_avg:46.99ms
step:1179/1840 train_time:55415ms step_avg:47.00ms
step:1180/1840 train_time:55477ms step_avg:47.01ms
step:1181/1840 train_time:55537ms step_avg:47.03ms
step:1182/1840 train_time:55598ms step_avg:47.04ms
step:1183/1840 train_time:55658ms step_avg:47.05ms
step:1184/1840 train_time:55721ms step_avg:47.06ms
step:1185/1840 train_time:55780ms step_avg:47.07ms
step:1186/1840 train_time:55842ms step_avg:47.08ms
step:1187/1840 train_time:55902ms step_avg:47.10ms
step:1188/1840 train_time:55965ms step_avg:47.11ms
step:1189/1840 train_time:56025ms step_avg:47.12ms
step:1190/1840 train_time:56087ms step_avg:47.13ms
step:1191/1840 train_time:56147ms step_avg:47.14ms
step:1192/1840 train_time:56209ms step_avg:47.16ms
step:1193/1840 train_time:56269ms step_avg:47.17ms
step:1194/1840 train_time:56331ms step_avg:47.18ms
step:1195/1840 train_time:56391ms step_avg:47.19ms
step:1196/1840 train_time:56454ms step_avg:47.20ms
step:1197/1840 train_time:56515ms step_avg:47.21ms
step:1198/1840 train_time:56577ms step_avg:47.23ms
step:1199/1840 train_time:56637ms step_avg:47.24ms
step:1200/1840 train_time:56699ms step_avg:47.25ms
step:1201/1840 train_time:56761ms step_avg:47.26ms
step:1202/1840 train_time:56847ms step_avg:47.29ms
step:1203/1840 train_time:56933ms step_avg:47.33ms
step:1204/1840 train_time:57023ms step_avg:47.36ms
step:1205/1840 train_time:57109ms step_avg:47.39ms
step:1206/1840 train_time:57201ms step_avg:47.43ms
step:1207/1840 train_time:57288ms step_avg:47.46ms
step:1208/1840 train_time:57379ms step_avg:47.50ms
step:1209/1840 train_time:57466ms step_avg:47.53ms
step:1210/1840 train_time:57554ms step_avg:47.57ms
step:1211/1840 train_time:57641ms step_avg:47.60ms
step:1212/1840 train_time:57728ms step_avg:47.63ms
step:1213/1840 train_time:57816ms step_avg:47.66ms
step:1214/1840 train_time:57906ms step_avg:47.70ms
step:1215/1840 train_time:57991ms step_avg:47.73ms
step:1216/1840 train_time:58080ms step_avg:47.76ms
step:1217/1840 train_time:58167ms step_avg:47.80ms
step:1218/1840 train_time:58258ms step_avg:47.83ms
step:1219/1840 train_time:58344ms step_avg:47.86ms
step:1220/1840 train_time:58432ms step_avg:47.90ms
step:1221/1840 train_time:58520ms step_avg:47.93ms
step:1222/1840 train_time:58608ms step_avg:47.96ms
step:1223/1840 train_time:58694ms step_avg:47.99ms
step:1224/1840 train_time:58783ms step_avg:48.03ms
step:1225/1840 train_time:58869ms step_avg:48.06ms
step:1226/1840 train_time:58958ms step_avg:48.09ms
step:1227/1840 train_time:59045ms step_avg:48.12ms
step:1228/1840 train_time:59131ms step_avg:48.15ms
step:1229/1840 train_time:59218ms step_avg:48.18ms
step:1230/1840 train_time:59308ms step_avg:48.22ms
step:1231/1840 train_time:59396ms step_avg:48.25ms
step:1232/1840 train_time:59486ms step_avg:48.28ms
step:1233/1840 train_time:59572ms step_avg:48.31ms
step:1234/1840 train_time:59661ms step_avg:48.35ms
step:1235/1840 train_time:59747ms step_avg:48.38ms
step:1236/1840 train_time:59835ms step_avg:48.41ms
step:1237/1840 train_time:59922ms step_avg:48.44ms
step:1238/1840 train_time:60010ms step_avg:48.47ms
step:1239/1840 train_time:60096ms step_avg:48.50ms
step:1240/1840 train_time:60185ms step_avg:48.54ms
step:1241/1840 train_time:60271ms step_avg:48.57ms
step:1242/1840 train_time:60360ms step_avg:48.60ms
step:1243/1840 train_time:60447ms step_avg:48.63ms
step:1244/1840 train_time:60535ms step_avg:48.66ms
step:1245/1840 train_time:60622ms step_avg:48.69ms
step:1246/1840 train_time:60710ms step_avg:48.72ms
step:1247/1840 train_time:60796ms step_avg:48.75ms
step:1248/1840 train_time:60886ms step_avg:48.79ms
step:1249/1840 train_time:60971ms step_avg:48.82ms
step:1250/1840 train_time:61060ms step_avg:48.85ms
step:1250/1840 val_loss:3.5329 train_time:61160ms step_avg:48.93ms
step:1251/1840 train_time:61179ms step_avg:48.90ms
step:1252/1840 train_time:61237ms step_avg:48.91ms
step:1253/1840 train_time:61327ms step_avg:48.94ms
step:1254/1840 train_time:61417ms step_avg:48.98ms
step:1255/1840 train_time:61503ms step_avg:49.01ms
step:1256/1840 train_time:61591ms step_avg:49.04ms
step:1257/1840 train_time:61675ms step_avg:49.07ms
step:1258/1840 train_time:61763ms step_avg:49.10ms
step:1259/1840 train_time:61849ms step_avg:49.13ms
step:1260/1840 train_time:61937ms step_avg:49.16ms
step:1261/1840 train_time:62022ms step_avg:49.19ms
step:1262/1840 train_time:62112ms step_avg:49.22ms
step:1263/1840 train_time:62199ms step_avg:49.25ms
step:1264/1840 train_time:62293ms step_avg:49.28ms
step:1265/1840 train_time:62380ms step_avg:49.31ms
step:1266/1840 train_time:62470ms step_avg:49.34ms
step:1267/1840 train_time:62556ms step_avg:49.37ms
step:1268/1840 train_time:62644ms step_avg:49.40ms
step:1269/1840 train_time:62730ms step_avg:49.43ms
step:1270/1840 train_time:62817ms step_avg:49.46ms
step:1271/1840 train_time:62902ms step_avg:49.49ms
step:1272/1840 train_time:62991ms step_avg:49.52ms
step:1273/1840 train_time:63078ms step_avg:49.55ms
step:1274/1840 train_time:63167ms step_avg:49.58ms
step:1275/1840 train_time:63256ms step_avg:49.61ms
step:1276/1840 train_time:63345ms step_avg:49.64ms
step:1277/1840 train_time:63431ms step_avg:49.67ms
step:1278/1840 train_time:63519ms step_avg:49.70ms
step:1279/1840 train_time:63605ms step_avg:49.73ms
step:1280/1840 train_time:63694ms step_avg:49.76ms
step:1281/1840 train_time:63779ms step_avg:49.79ms
step:1282/1840 train_time:63868ms step_avg:49.82ms
step:1283/1840 train_time:63955ms step_avg:49.85ms
step:1284/1840 train_time:64043ms step_avg:49.88ms
step:1285/1840 train_time:64130ms step_avg:49.91ms
step:1286/1840 train_time:64220ms step_avg:49.94ms
step:1287/1840 train_time:64308ms step_avg:49.97ms
step:1288/1840 train_time:64398ms step_avg:50.00ms
step:1289/1840 train_time:64484ms step_avg:50.03ms
step:1290/1840 train_time:64572ms step_avg:50.06ms
step:1291/1840 train_time:64658ms step_avg:50.08ms
step:1292/1840 train_time:64747ms step_avg:50.11ms
step:1293/1840 train_time:64833ms step_avg:50.14ms
step:1294/1840 train_time:64920ms step_avg:50.17ms
step:1295/1840 train_time:65008ms step_avg:50.20ms
step:1296/1840 train_time:65097ms step_avg:50.23ms
step:1297/1840 train_time:65183ms step_avg:50.26ms
step:1298/1840 train_time:65273ms step_avg:50.29ms
step:1299/1840 train_time:65358ms step_avg:50.31ms
step:1300/1840 train_time:65447ms step_avg:50.34ms
step:1301/1840 train_time:65534ms step_avg:50.37ms
step:1302/1840 train_time:65622ms step_avg:50.40ms
step:1303/1840 train_time:65708ms step_avg:50.43ms
step:1304/1840 train_time:65795ms step_avg:50.46ms
step:1305/1840 train_time:65882ms step_avg:50.48ms
step:1306/1840 train_time:65972ms step_avg:50.51ms
step:1307/1840 train_time:66058ms step_avg:50.54ms
step:1308/1840 train_time:66147ms step_avg:50.57ms
step:1309/1840 train_time:66234ms step_avg:50.60ms
step:1310/1840 train_time:66323ms step_avg:50.63ms
step:1311/1840 train_time:66409ms step_avg:50.66ms
step:1312/1840 train_time:66496ms step_avg:50.68ms
step:1313/1840 train_time:66583ms step_avg:50.71ms
step:1314/1840 train_time:66672ms step_avg:50.74ms
step:1315/1840 train_time:66758ms step_avg:50.77ms
step:1316/1840 train_time:66847ms step_avg:50.80ms
step:1317/1840 train_time:66932ms step_avg:50.82ms
step:1318/1840 train_time:67021ms step_avg:50.85ms
step:1319/1840 train_time:67107ms step_avg:50.88ms
step:1320/1840 train_time:67196ms step_avg:50.91ms
step:1321/1840 train_time:67283ms step_avg:50.93ms
step:1322/1840 train_time:67372ms step_avg:50.96ms
step:1323/1840 train_time:67458ms step_avg:50.99ms
step:1324/1840 train_time:67547ms step_avg:51.02ms
step:1325/1840 train_time:67634ms step_avg:51.04ms
step:1326/1840 train_time:67723ms step_avg:51.07ms
step:1327/1840 train_time:67808ms step_avg:51.10ms
step:1328/1840 train_time:67896ms step_avg:51.13ms
step:1329/1840 train_time:67982ms step_avg:51.15ms
step:1330/1840 train_time:68072ms step_avg:51.18ms
step:1331/1840 train_time:68158ms step_avg:51.21ms
step:1332/1840 train_time:68249ms step_avg:51.24ms
step:1333/1840 train_time:68335ms step_avg:51.26ms
step:1334/1840 train_time:68423ms step_avg:51.29ms
step:1335/1840 train_time:68509ms step_avg:51.32ms
step:1336/1840 train_time:68597ms step_avg:51.35ms
step:1337/1840 train_time:68683ms step_avg:51.37ms
step:1338/1840 train_time:68774ms step_avg:51.40ms
step:1339/1840 train_time:68859ms step_avg:51.43ms
step:1340/1840 train_time:68948ms step_avg:51.45ms
step:1341/1840 train_time:69035ms step_avg:51.48ms
step:1342/1840 train_time:69124ms step_avg:51.51ms
step:1343/1840 train_time:69210ms step_avg:51.53ms
step:1344/1840 train_time:69298ms step_avg:51.56ms
step:1345/1840 train_time:69384ms step_avg:51.59ms
step:1346/1840 train_time:69474ms step_avg:51.62ms
step:1347/1840 train_time:69559ms step_avg:51.64ms
step:1348/1840 train_time:69649ms step_avg:51.67ms
step:1349/1840 train_time:69735ms step_avg:51.69ms
step:1350/1840 train_time:69824ms step_avg:51.72ms
step:1351/1840 train_time:69910ms step_avg:51.75ms
step:1352/1840 train_time:69998ms step_avg:51.77ms
step:1353/1840 train_time:70084ms step_avg:51.80ms
step:1354/1840 train_time:70173ms step_avg:51.83ms
step:1355/1840 train_time:70259ms step_avg:51.85ms
step:1356/1840 train_time:70348ms step_avg:51.88ms
step:1357/1840 train_time:70435ms step_avg:51.90ms
step:1358/1840 train_time:70524ms step_avg:51.93ms
step:1359/1840 train_time:70610ms step_avg:51.96ms
step:1360/1840 train_time:70698ms step_avg:51.98ms
step:1361/1840 train_time:70784ms step_avg:52.01ms
step:1362/1840 train_time:70873ms step_avg:52.04ms
step:1363/1840 train_time:70958ms step_avg:52.06ms
step:1364/1840 train_time:71048ms step_avg:52.09ms
step:1365/1840 train_time:71134ms step_avg:52.11ms
step:1366/1840 train_time:71222ms step_avg:52.14ms
step:1367/1840 train_time:71309ms step_avg:52.16ms
step:1368/1840 train_time:71397ms step_avg:52.19ms
step:1369/1840 train_time:71484ms step_avg:52.22ms
step:1370/1840 train_time:71573ms step_avg:52.24ms
step:1371/1840 train_time:71658ms step_avg:52.27ms
step:1372/1840 train_time:71747ms step_avg:52.29ms
step:1373/1840 train_time:71833ms step_avg:52.32ms
step:1374/1840 train_time:71920ms step_avg:52.34ms
step:1375/1840 train_time:72008ms step_avg:52.37ms
step:1376/1840 train_time:72096ms step_avg:52.40ms
step:1377/1840 train_time:72182ms step_avg:52.42ms
step:1378/1840 train_time:72272ms step_avg:52.45ms
step:1379/1840 train_time:72357ms step_avg:52.47ms
step:1380/1840 train_time:72447ms step_avg:52.50ms
step:1381/1840 train_time:72534ms step_avg:52.52ms
step:1382/1840 train_time:72622ms step_avg:52.55ms
step:1383/1840 train_time:72709ms step_avg:52.57ms
step:1384/1840 train_time:72797ms step_avg:52.60ms
step:1385/1840 train_time:72884ms step_avg:52.62ms
step:1386/1840 train_time:72973ms step_avg:52.65ms
step:1387/1840 train_time:73060ms step_avg:52.67ms
step:1388/1840 train_time:73151ms step_avg:52.70ms
step:1389/1840 train_time:73237ms step_avg:52.73ms
step:1390/1840 train_time:73325ms step_avg:52.75ms
step:1391/1840 train_time:73412ms step_avg:52.78ms
step:1392/1840 train_time:73500ms step_avg:52.80ms
step:1393/1840 train_time:73588ms step_avg:52.83ms
step:1394/1840 train_time:73675ms step_avg:52.85ms
step:1395/1840 train_time:73761ms step_avg:52.88ms
step:1396/1840 train_time:73850ms step_avg:52.90ms
step:1397/1840 train_time:73936ms step_avg:52.92ms
step:1398/1840 train_time:74024ms step_avg:52.95ms
step:1399/1840 train_time:74111ms step_avg:52.97ms
step:1400/1840 train_time:74199ms step_avg:53.00ms
step:1401/1840 train_time:74286ms step_avg:53.02ms
step:1402/1840 train_time:74374ms step_avg:53.05ms
step:1403/1840 train_time:74459ms step_avg:53.07ms
step:1404/1840 train_time:74549ms step_avg:53.10ms
step:1405/1840 train_time:74635ms step_avg:53.12ms
step:1406/1840 train_time:74725ms step_avg:53.15ms
step:1407/1840 train_time:74810ms step_avg:53.17ms
step:1408/1840 train_time:74899ms step_avg:53.19ms
step:1409/1840 train_time:74985ms step_avg:53.22ms
step:1410/1840 train_time:75076ms step_avg:53.25ms
step:1411/1840 train_time:75161ms step_avg:53.27ms
step:1412/1840 train_time:75252ms step_avg:53.29ms
step:1413/1840 train_time:75337ms step_avg:53.32ms
step:1414/1840 train_time:75426ms step_avg:53.34ms
step:1415/1840 train_time:75511ms step_avg:53.36ms
step:1416/1840 train_time:75598ms step_avg:53.39ms
step:1417/1840 train_time:75685ms step_avg:53.41ms
step:1418/1840 train_time:75774ms step_avg:53.44ms
step:1419/1840 train_time:75859ms step_avg:53.46ms
step:1420/1840 train_time:75949ms step_avg:53.48ms
step:1421/1840 train_time:76036ms step_avg:53.51ms
step:1422/1840 train_time:76124ms step_avg:53.53ms
step:1423/1840 train_time:76212ms step_avg:53.56ms
step:1424/1840 train_time:76299ms step_avg:53.58ms
step:1425/1840 train_time:76385ms step_avg:53.60ms
step:1426/1840 train_time:76474ms step_avg:53.63ms
step:1427/1840 train_time:76559ms step_avg:53.65ms
step:1428/1840 train_time:76649ms step_avg:53.68ms
step:1429/1840 train_time:76736ms step_avg:53.70ms
step:1430/1840 train_time:76824ms step_avg:53.72ms
step:1431/1840 train_time:76910ms step_avg:53.75ms
step:1432/1840 train_time:76998ms step_avg:53.77ms
step:1433/1840 train_time:77084ms step_avg:53.79ms
step:1434/1840 train_time:77174ms step_avg:53.82ms
step:1435/1840 train_time:77259ms step_avg:53.84ms
step:1436/1840 train_time:77347ms step_avg:53.86ms
step:1437/1840 train_time:77433ms step_avg:53.89ms
step:1438/1840 train_time:77522ms step_avg:53.91ms
step:1439/1840 train_time:77608ms step_avg:53.93ms
step:1440/1840 train_time:77696ms step_avg:53.96ms
step:1441/1840 train_time:77782ms step_avg:53.98ms
step:1442/1840 train_time:77872ms step_avg:54.00ms
step:1443/1840 train_time:77958ms step_avg:54.02ms
step:1444/1840 train_time:78046ms step_avg:54.05ms
step:1445/1840 train_time:78133ms step_avg:54.07ms
step:1446/1840 train_time:78222ms step_avg:54.10ms
step:1447/1840 train_time:78307ms step_avg:54.12ms
step:1448/1840 train_time:78396ms step_avg:54.14ms
step:1449/1840 train_time:78481ms step_avg:54.16ms
step:1450/1840 train_time:78571ms step_avg:54.19ms
step:1451/1840 train_time:78657ms step_avg:54.21ms
step:1452/1840 train_time:78746ms step_avg:54.23ms
step:1453/1840 train_time:78832ms step_avg:54.25ms
step:1454/1840 train_time:78920ms step_avg:54.28ms
step:1455/1840 train_time:79008ms step_avg:54.30ms
step:1456/1840 train_time:79097ms step_avg:54.32ms
step:1457/1840 train_time:79183ms step_avg:54.35ms
step:1458/1840 train_time:79272ms step_avg:54.37ms
step:1459/1840 train_time:79358ms step_avg:54.39ms
step:1460/1840 train_time:79446ms step_avg:54.42ms
step:1461/1840 train_time:79532ms step_avg:54.44ms
step:1462/1840 train_time:79620ms step_avg:54.46ms
step:1463/1840 train_time:79707ms step_avg:54.48ms
step:1464/1840 train_time:79796ms step_avg:54.51ms
step:1465/1840 train_time:79882ms step_avg:54.53ms
step:1466/1840 train_time:79971ms step_avg:54.55ms
step:1467/1840 train_time:80057ms step_avg:54.57ms
step:1468/1840 train_time:80147ms step_avg:54.60ms
step:1469/1840 train_time:80233ms step_avg:54.62ms
step:1470/1840 train_time:80320ms step_avg:54.64ms
step:1471/1840 train_time:80408ms step_avg:54.66ms
step:1472/1840 train_time:80496ms step_avg:54.68ms
step:1473/1840 train_time:80583ms step_avg:54.71ms
step:1474/1840 train_time:80671ms step_avg:54.73ms
step:1475/1840 train_time:80757ms step_avg:54.75ms
step:1476/1840 train_time:80846ms step_avg:54.77ms
step:1477/1840 train_time:80934ms step_avg:54.80ms
step:1478/1840 train_time:81022ms step_avg:54.82ms
step:1479/1840 train_time:81108ms step_avg:54.84ms
step:1480/1840 train_time:81196ms step_avg:54.86ms
step:1481/1840 train_time:81282ms step_avg:54.88ms
step:1482/1840 train_time:81371ms step_avg:54.91ms
step:1483/1840 train_time:81457ms step_avg:54.93ms
step:1484/1840 train_time:81546ms step_avg:54.95ms
step:1485/1840 train_time:81632ms step_avg:54.97ms
step:1486/1840 train_time:81720ms step_avg:54.99ms
step:1487/1840 train_time:81807ms step_avg:55.02ms
step:1488/1840 train_time:81897ms step_avg:55.04ms
step:1489/1840 train_time:81983ms step_avg:55.06ms
step:1490/1840 train_time:82072ms step_avg:55.08ms
step:1491/1840 train_time:82158ms step_avg:55.10ms
step:1492/1840 train_time:82248ms step_avg:55.13ms
step:1493/1840 train_time:82334ms step_avg:55.15ms
step:1494/1840 train_time:82422ms step_avg:55.17ms
step:1495/1840 train_time:82508ms step_avg:55.19ms
step:1496/1840 train_time:82596ms step_avg:55.21ms
step:1497/1840 train_time:82681ms step_avg:55.23ms
step:1498/1840 train_time:82771ms step_avg:55.25ms
step:1499/1840 train_time:82857ms step_avg:55.27ms
step:1500/1840 train_time:82946ms step_avg:55.30ms
step:1500/1840 val_loss:3.4001 train_time:83046ms step_avg:55.36ms
step:1501/1840 train_time:83065ms step_avg:55.34ms
step:1502/1840 train_time:83124ms step_avg:55.34ms
step:1503/1840 train_time:83212ms step_avg:55.36ms
step:1504/1840 train_time:83302ms step_avg:55.39ms
step:1505/1840 train_time:83389ms step_avg:55.41ms
step:1506/1840 train_time:83478ms step_avg:55.43ms
step:1507/1840 train_time:83562ms step_avg:55.45ms
step:1508/1840 train_time:83649ms step_avg:55.47ms
step:1509/1840 train_time:83734ms step_avg:55.49ms
step:1510/1840 train_time:83823ms step_avg:55.51ms
step:1511/1840 train_time:83908ms step_avg:55.53ms
step:1512/1840 train_time:83997ms step_avg:55.55ms
step:1513/1840 train_time:84084ms step_avg:55.57ms
step:1514/1840 train_time:84178ms step_avg:55.60ms
step:1515/1840 train_time:84264ms step_avg:55.62ms
step:1516/1840 train_time:84355ms step_avg:55.64ms
step:1517/1840 train_time:84442ms step_avg:55.66ms
step:1518/1840 train_time:84530ms step_avg:55.68ms
step:1519/1840 train_time:84614ms step_avg:55.70ms
step:1520/1840 train_time:84702ms step_avg:55.72ms
step:1521/1840 train_time:84787ms step_avg:55.74ms
step:1522/1840 train_time:84876ms step_avg:55.77ms
step:1523/1840 train_time:84962ms step_avg:55.79ms
step:1524/1840 train_time:85053ms step_avg:55.81ms
step:1525/1840 train_time:85142ms step_avg:55.83ms
step:1526/1840 train_time:85229ms step_avg:55.85ms
step:1527/1840 train_time:85317ms step_avg:55.87ms
step:1528/1840 train_time:85405ms step_avg:55.89ms
step:1529/1840 train_time:85490ms step_avg:55.91ms
step:1530/1840 train_time:85579ms step_avg:55.93ms
step:1531/1840 train_time:85664ms step_avg:55.95ms
step:1532/1840 train_time:85753ms step_avg:55.97ms
step:1533/1840 train_time:85840ms step_avg:55.99ms
step:1534/1840 train_time:85928ms step_avg:56.02ms
step:1535/1840 train_time:86015ms step_avg:56.04ms
step:1536/1840 train_time:86104ms step_avg:56.06ms
step:1537/1840 train_time:86192ms step_avg:56.08ms
step:1538/1840 train_time:86282ms step_avg:56.10ms
step:1539/1840 train_time:86368ms step_avg:56.12ms
step:1540/1840 train_time:86457ms step_avg:56.14ms
step:1541/1840 train_time:86543ms step_avg:56.16ms
step:1542/1840 train_time:86631ms step_avg:56.18ms
step:1543/1840 train_time:86716ms step_avg:56.20ms
step:1544/1840 train_time:86804ms step_avg:56.22ms
step:1545/1840 train_time:86890ms step_avg:56.24ms
step:1546/1840 train_time:86980ms step_avg:56.26ms
step:1547/1840 train_time:87065ms step_avg:56.28ms
step:1548/1840 train_time:87156ms step_avg:56.30ms
step:1549/1840 train_time:87243ms step_avg:56.32ms
step:1550/1840 train_time:87333ms step_avg:56.34ms
step:1551/1840 train_time:87418ms step_avg:56.36ms
step:1552/1840 train_time:87505ms step_avg:56.38ms
step:1553/1840 train_time:87592ms step_avg:56.40ms
step:1554/1840 train_time:87681ms step_avg:56.42ms
step:1555/1840 train_time:87765ms step_avg:56.44ms
step:1556/1840 train_time:87854ms step_avg:56.46ms
step:1557/1840 train_time:87940ms step_avg:56.48ms
step:1558/1840 train_time:88028ms step_avg:56.50ms
step:1559/1840 train_time:88115ms step_avg:56.52ms
step:1560/1840 train_time:88205ms step_avg:56.54ms
step:1561/1840 train_time:88291ms step_avg:56.56ms
step:1562/1840 train_time:88382ms step_avg:56.58ms
step:1563/1840 train_time:88467ms step_avg:56.60ms
step:1564/1840 train_time:88555ms step_avg:56.62ms
step:1565/1840 train_time:88642ms step_avg:56.64ms
step:1566/1840 train_time:88730ms step_avg:56.66ms
step:1567/1840 train_time:88814ms step_avg:56.68ms
step:1568/1840 train_time:88902ms step_avg:56.70ms
step:1569/1840 train_time:88988ms step_avg:56.72ms
step:1570/1840 train_time:89079ms step_avg:56.74ms
step:1571/1840 train_time:89166ms step_avg:56.76ms
step:1572/1840 train_time:89256ms step_avg:56.78ms
step:1573/1840 train_time:89342ms step_avg:56.80ms
step:1574/1840 train_time:89431ms step_avg:56.82ms
step:1575/1840 train_time:89517ms step_avg:56.84ms
step:1576/1840 train_time:89605ms step_avg:56.86ms
step:1577/1840 train_time:89690ms step_avg:56.87ms
step:1578/1840 train_time:89779ms step_avg:56.89ms
step:1579/1840 train_time:89864ms step_avg:56.91ms
step:1580/1840 train_time:89954ms step_avg:56.93ms
step:1581/1840 train_time:90040ms step_avg:56.95ms
step:1582/1840 train_time:90128ms step_avg:56.97ms
step:1583/1840 train_time:90215ms step_avg:56.99ms
step:1584/1840 train_time:90303ms step_avg:57.01ms
step:1585/1840 train_time:90388ms step_avg:57.03ms
step:1586/1840 train_time:90479ms step_avg:57.05ms
step:1587/1840 train_time:90564ms step_avg:57.07ms
step:1588/1840 train_time:90652ms step_avg:57.09ms
step:1589/1840 train_time:90738ms step_avg:57.10ms
step:1590/1840 train_time:90826ms step_avg:57.12ms
step:1591/1840 train_time:90913ms step_avg:57.14ms
step:1592/1840 train_time:91002ms step_avg:57.16ms
step:1593/1840 train_time:91087ms step_avg:57.18ms
step:1594/1840 train_time:91179ms step_avg:57.20ms
step:1595/1840 train_time:91265ms step_avg:57.22ms
step:1596/1840 train_time:91353ms step_avg:57.24ms
step:1597/1840 train_time:91440ms step_avg:57.26ms
step:1598/1840 train_time:91528ms step_avg:57.28ms
step:1599/1840 train_time:91614ms step_avg:57.29ms
step:1600/1840 train_time:91703ms step_avg:57.31ms
step:1601/1840 train_time:91788ms step_avg:57.33ms
step:1602/1840 train_time:91878ms step_avg:57.35ms
step:1603/1840 train_time:91964ms step_avg:57.37ms
step:1604/1840 train_time:92052ms step_avg:57.39ms
step:1605/1840 train_time:92139ms step_avg:57.41ms
step:1606/1840 train_time:92227ms step_avg:57.43ms
step:1607/1840 train_time:92314ms step_avg:57.45ms
step:1608/1840 train_time:92403ms step_avg:57.46ms
step:1609/1840 train_time:92488ms step_avg:57.48ms
step:1610/1840 train_time:92578ms step_avg:57.50ms
step:1611/1840 train_time:92663ms step_avg:57.52ms
step:1612/1840 train_time:92753ms step_avg:57.54ms
step:1613/1840 train_time:92840ms step_avg:57.56ms
step:1614/1840 train_time:92928ms step_avg:57.58ms
step:1615/1840 train_time:93014ms step_avg:57.59ms
step:1616/1840 train_time:93102ms step_avg:57.61ms
step:1617/1840 train_time:93188ms step_avg:57.63ms
step:1618/1840 train_time:93277ms step_avg:57.65ms
step:1619/1840 train_time:93363ms step_avg:57.67ms
step:1620/1840 train_time:93451ms step_avg:57.69ms
step:1621/1840 train_time:93537ms step_avg:57.70ms
step:1622/1840 train_time:93624ms step_avg:57.72ms
step:1623/1840 train_time:93712ms step_avg:57.74ms
step:1624/1840 train_time:93800ms step_avg:57.76ms
step:1625/1840 train_time:93886ms step_avg:57.78ms
step:1626/1840 train_time:93976ms step_avg:57.80ms
step:1627/1840 train_time:94063ms step_avg:57.81ms
step:1628/1840 train_time:94153ms step_avg:57.83ms
step:1629/1840 train_time:94240ms step_avg:57.85ms
step:1630/1840 train_time:94328ms step_avg:57.87ms
step:1631/1840 train_time:94414ms step_avg:57.89ms
step:1632/1840 train_time:94502ms step_avg:57.91ms
step:1633/1840 train_time:94588ms step_avg:57.92ms
step:1634/1840 train_time:94678ms step_avg:57.94ms
step:1635/1840 train_time:94763ms step_avg:57.96ms
step:1636/1840 train_time:94852ms step_avg:57.98ms
step:1637/1840 train_time:94938ms step_avg:58.00ms
step:1638/1840 train_time:95026ms step_avg:58.01ms
step:1639/1840 train_time:95114ms step_avg:58.03ms
step:1640/1840 train_time:95203ms step_avg:58.05ms
step:1641/1840 train_time:95288ms step_avg:58.07ms
step:1642/1840 train_time:95378ms step_avg:58.09ms
step:1643/1840 train_time:95464ms step_avg:58.10ms
step:1644/1840 train_time:95554ms step_avg:58.12ms
step:1645/1840 train_time:95641ms step_avg:58.14ms
step:1646/1840 train_time:95729ms step_avg:58.16ms
step:1647/1840 train_time:95813ms step_avg:58.17ms
step:1648/1840 train_time:95902ms step_avg:58.19ms
step:1649/1840 train_time:95988ms step_avg:58.21ms
step:1650/1840 train_time:96080ms step_avg:58.23ms
step:1651/1840 train_time:96166ms step_avg:58.25ms
step:1652/1840 train_time:96255ms step_avg:58.27ms
step:1653/1840 train_time:96343ms step_avg:58.28ms
step:1654/1840 train_time:96432ms step_avg:58.30ms
step:1655/1840 train_time:96518ms step_avg:58.32ms
step:1656/1840 train_time:96606ms step_avg:58.34ms
step:1657/1840 train_time:96691ms step_avg:58.35ms
step:1658/1840 train_time:96780ms step_avg:58.37ms
step:1659/1840 train_time:96866ms step_avg:58.39ms
step:1660/1840 train_time:96955ms step_avg:58.41ms
step:1661/1840 train_time:97043ms step_avg:58.42ms
step:1662/1840 train_time:97132ms step_avg:58.44ms
step:1663/1840 train_time:97218ms step_avg:58.46ms
step:1664/1840 train_time:97306ms step_avg:58.48ms
step:1665/1840 train_time:97392ms step_avg:58.49ms
step:1666/1840 train_time:97483ms step_avg:58.51ms
step:1667/1840 train_time:97568ms step_avg:58.53ms
step:1668/1840 train_time:97657ms step_avg:58.55ms
step:1669/1840 train_time:97743ms step_avg:58.56ms
step:1670/1840 train_time:97832ms step_avg:58.58ms
step:1671/1840 train_time:97919ms step_avg:58.60ms
step:1672/1840 train_time:98007ms step_avg:58.62ms
step:1673/1840 train_time:98092ms step_avg:58.63ms
step:1674/1840 train_time:98182ms step_avg:58.65ms
step:1675/1840 train_time:98268ms step_avg:58.67ms
step:1676/1840 train_time:98357ms step_avg:58.69ms
step:1677/1840 train_time:98443ms step_avg:58.70ms
step:1678/1840 train_time:98532ms step_avg:58.72ms
step:1679/1840 train_time:98617ms step_avg:58.74ms
step:1680/1840 train_time:98705ms step_avg:58.75ms
step:1681/1840 train_time:98791ms step_avg:58.77ms
step:1682/1840 train_time:98881ms step_avg:58.79ms
step:1683/1840 train_time:98966ms step_avg:58.80ms
step:1684/1840 train_time:99056ms step_avg:58.82ms
step:1685/1840 train_time:99143ms step_avg:58.84ms
step:1686/1840 train_time:99231ms step_avg:58.86ms
step:1687/1840 train_time:99317ms step_avg:58.87ms
step:1688/1840 train_time:99405ms step_avg:58.89ms
step:1689/1840 train_time:99492ms step_avg:58.91ms
step:1690/1840 train_time:99580ms step_avg:58.92ms
step:1691/1840 train_time:99665ms step_avg:58.94ms
step:1692/1840 train_time:99755ms step_avg:58.96ms
step:1693/1840 train_time:99841ms step_avg:58.97ms
step:1694/1840 train_time:99930ms step_avg:58.99ms
step:1695/1840 train_time:100015ms step_avg:59.01ms
step:1696/1840 train_time:100103ms step_avg:59.02ms
step:1697/1840 train_time:100190ms step_avg:59.04ms
step:1698/1840 train_time:100280ms step_avg:59.06ms
step:1699/1840 train_time:100366ms step_avg:59.07ms
step:1700/1840 train_time:100454ms step_avg:59.09ms
step:1701/1840 train_time:100540ms step_avg:59.11ms
step:1702/1840 train_time:100628ms step_avg:59.12ms
step:1703/1840 train_time:100715ms step_avg:59.14ms
step:1704/1840 train_time:100803ms step_avg:59.16ms
step:1705/1840 train_time:100888ms step_avg:59.17ms
step:1706/1840 train_time:100978ms step_avg:59.19ms
step:1707/1840 train_time:101063ms step_avg:59.21ms
step:1708/1840 train_time:101154ms step_avg:59.22ms
step:1709/1840 train_time:101240ms step_avg:59.24ms
step:1710/1840 train_time:101329ms step_avg:59.26ms
step:1711/1840 train_time:101415ms step_avg:59.27ms
step:1712/1840 train_time:101504ms step_avg:59.29ms
step:1713/1840 train_time:101589ms step_avg:59.30ms
step:1714/1840 train_time:101679ms step_avg:59.32ms
step:1715/1840 train_time:101766ms step_avg:59.34ms
step:1716/1840 train_time:101854ms step_avg:59.36ms
step:1717/1840 train_time:101940ms step_avg:59.37ms
step:1718/1840 train_time:102029ms step_avg:59.39ms
step:1719/1840 train_time:102115ms step_avg:59.40ms
step:1720/1840 train_time:102203ms step_avg:59.42ms
step:1721/1840 train_time:102290ms step_avg:59.44ms
step:1722/1840 train_time:102381ms step_avg:59.45ms
step:1723/1840 train_time:102467ms step_avg:59.47ms
step:1724/1840 train_time:102555ms step_avg:59.49ms
step:1725/1840 train_time:102642ms step_avg:59.50ms
step:1726/1840 train_time:102731ms step_avg:59.52ms
step:1727/1840 train_time:102815ms step_avg:59.53ms
step:1728/1840 train_time:102903ms step_avg:59.55ms
step:1729/1840 train_time:102989ms step_avg:59.57ms
step:1730/1840 train_time:103079ms step_avg:59.58ms
step:1731/1840 train_time:103167ms step_avg:59.60ms
step:1732/1840 train_time:103258ms step_avg:59.62ms
step:1733/1840 train_time:103344ms step_avg:59.63ms
step:1734/1840 train_time:103432ms step_avg:59.65ms
step:1735/1840 train_time:103518ms step_avg:59.66ms
step:1736/1840 train_time:103606ms step_avg:59.68ms
step:1737/1840 train_time:103691ms step_avg:59.70ms
step:1738/1840 train_time:103782ms step_avg:59.71ms
step:1739/1840 train_time:103868ms step_avg:59.73ms
step:1740/1840 train_time:103957ms step_avg:59.75ms
step:1741/1840 train_time:104042ms step_avg:59.76ms
step:1742/1840 train_time:104131ms step_avg:59.78ms
step:1743/1840 train_time:104218ms step_avg:59.79ms
step:1744/1840 train_time:104305ms step_avg:59.81ms
step:1745/1840 train_time:104392ms step_avg:59.82ms
step:1746/1840 train_time:104482ms step_avg:59.84ms
step:1747/1840 train_time:104567ms step_avg:59.86ms
step:1748/1840 train_time:104657ms step_avg:59.87ms
step:1749/1840 train_time:104743ms step_avg:59.89ms
step:1750/1840 train_time:104832ms step_avg:59.90ms
step:1750/1840 val_loss:3.3025 train_time:104932ms step_avg:59.96ms
step:1751/1840 train_time:104951ms step_avg:59.94ms
step:1752/1840 train_time:105008ms step_avg:59.94ms
step:1753/1840 train_time:105095ms step_avg:59.95ms
step:1754/1840 train_time:105190ms step_avg:59.97ms
step:1755/1840 train_time:105279ms step_avg:59.99ms
step:1756/1840 train_time:105368ms step_avg:60.00ms
step:1757/1840 train_time:105454ms step_avg:60.02ms
step:1758/1840 train_time:105542ms step_avg:60.04ms
step:1759/1840 train_time:105628ms step_avg:60.05ms
step:1760/1840 train_time:105716ms step_avg:60.07ms
step:1761/1840 train_time:105801ms step_avg:60.08ms
step:1762/1840 train_time:105891ms step_avg:60.10ms
step:1763/1840 train_time:105979ms step_avg:60.11ms
step:1764/1840 train_time:106070ms step_avg:60.13ms
step:1765/1840 train_time:106159ms step_avg:60.15ms
step:1766/1840 train_time:106248ms step_avg:60.16ms
step:1767/1840 train_time:106334ms step_avg:60.18ms
step:1768/1840 train_time:106423ms step_avg:60.19ms
step:1769/1840 train_time:106509ms step_avg:60.21ms
step:1770/1840 train_time:106596ms step_avg:60.22ms
step:1771/1840 train_time:106682ms step_avg:60.24ms
step:1772/1840 train_time:106770ms step_avg:60.25ms
step:1773/1840 train_time:106856ms step_avg:60.27ms
step:1774/1840 train_time:106946ms step_avg:60.29ms
step:1775/1840 train_time:107034ms step_avg:60.30ms
step:1776/1840 train_time:107124ms step_avg:60.32ms
step:1777/1840 train_time:107210ms step_avg:60.33ms
step:1778/1840 train_time:107299ms step_avg:60.35ms
step:1779/1840 train_time:107384ms step_avg:60.36ms
step:1780/1840 train_time:107472ms step_avg:60.38ms
step:1781/1840 train_time:107557ms step_avg:60.39ms
step:1782/1840 train_time:107646ms step_avg:60.41ms
step:1783/1840 train_time:107732ms step_avg:60.42ms
step:1784/1840 train_time:107820ms step_avg:60.44ms
step:1785/1840 train_time:107908ms step_avg:60.45ms
step:1786/1840 train_time:107998ms step_avg:60.47ms
step:1787/1840 train_time:108087ms step_avg:60.48ms
step:1788/1840 train_time:108176ms step_avg:60.50ms
step:1789/1840 train_time:108262ms step_avg:60.52ms
step:1790/1840 train_time:108350ms step_avg:60.53ms
step:1791/1840 train_time:108436ms step_avg:60.55ms
step:1792/1840 train_time:108524ms step_avg:60.56ms
step:1793/1840 train_time:108609ms step_avg:60.57ms
step:1794/1840 train_time:108699ms step_avg:60.59ms
step:1795/1840 train_time:108786ms step_avg:60.60ms
step:1796/1840 train_time:108874ms step_avg:60.62ms
step:1797/1840 train_time:108962ms step_avg:60.64ms
step:1798/1840 train_time:109050ms step_avg:60.65ms
step:1799/1840 train_time:109138ms step_avg:60.67ms
step:1800/1840 train_time:109228ms step_avg:60.68ms
step:1801/1840 train_time:109316ms step_avg:60.70ms
step:1802/1840 train_time:109404ms step_avg:60.71ms
step:1803/1840 train_time:109489ms step_avg:60.73ms
step:1804/1840 train_time:109578ms step_avg:60.74ms
step:1805/1840 train_time:109665ms step_avg:60.76ms
step:1806/1840 train_time:109753ms step_avg:60.77ms
step:1807/1840 train_time:109839ms step_avg:60.79ms
step:1808/1840 train_time:109929ms step_avg:60.80ms
step:1809/1840 train_time:110015ms step_avg:60.82ms
step:1810/1840 train_time:110106ms step_avg:60.83ms
step:1811/1840 train_time:110192ms step_avg:60.85ms
step:1812/1840 train_time:110282ms step_avg:60.86ms
step:1813/1840 train_time:110369ms step_avg:60.88ms
step:1814/1840 train_time:110457ms step_avg:60.89ms
step:1815/1840 train_time:110541ms step_avg:60.90ms
step:1816/1840 train_time:110630ms step_avg:60.92ms
step:1817/1840 train_time:110716ms step_avg:60.93ms
step:1818/1840 train_time:110806ms step_avg:60.95ms
step:1819/1840 train_time:110892ms step_avg:60.96ms
step:1820/1840 train_time:110981ms step_avg:60.98ms
step:1821/1840 train_time:111068ms step_avg:60.99ms
step:1822/1840 train_time:111157ms step_avg:61.01ms
step:1823/1840 train_time:111243ms step_avg:61.02ms
step:1824/1840 train_time:111332ms step_avg:61.04ms
step:1825/1840 train_time:111419ms step_avg:61.05ms
step:1826/1840 train_time:111507ms step_avg:61.07ms
step:1827/1840 train_time:111593ms step_avg:61.08ms
step:1828/1840 train_time:111681ms step_avg:61.09ms
step:1829/1840 train_time:111769ms step_avg:61.11ms
step:1830/1840 train_time:111858ms step_avg:61.12ms
step:1831/1840 train_time:111945ms step_avg:61.14ms
step:1832/1840 train_time:112033ms step_avg:61.15ms
step:1833/1840 train_time:112119ms step_avg:61.17ms
step:1834/1840 train_time:112209ms step_avg:61.18ms
step:1835/1840 train_time:112296ms step_avg:61.20ms
step:1836/1840 train_time:112386ms step_avg:61.21ms
step:1837/1840 train_time:112473ms step_avg:61.23ms
step:1838/1840 train_time:112562ms step_avg:61.24ms
step:1839/1840 train_time:112647ms step_avg:61.25ms
step:1840/1840 train_time:112736ms step_avg:61.27ms
step:1840/1840 val_loss:3.2776 train_time:112837ms step_avg:61.32ms
peak memory allocated: 28507 MiB reserved: 43618 MiB

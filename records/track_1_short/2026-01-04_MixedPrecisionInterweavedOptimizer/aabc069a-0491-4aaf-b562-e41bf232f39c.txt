import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from collections import defaultdict
from itertools import accumulate
from pathlib import Path
import gc

os.environ["PYTORCH_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device=f"cuda:{os.environ['LOCAL_RANK']}", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng

@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor, split_baddbmm: bool = False):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    # Select batched vs unbatched
    if split_baddbmm:
        BX_matmul = torch.bmm if X.ndim > 2 else torch.mm
    else:
        aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A

        # Referencing X twice causes pytorch to make a defensive copy,
        # resulting in a cudaMemcpyAsync in baddbmm.
        # For large matrices (i.e., the mlp weights), it's faster to split
        # the operation into two kernels to avoid this.
        if split_baddbmm:
            BX_matmul(B, X, out=C)  # C = B @ X
            C.add_(X, alpha=a)      # C = C + a*X  (in-place, X only read)
        else:
            aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X

        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X


# -----------------------------------------------------------------------------
# Compiled helpers for NorMuon by @chrisjmccormick

@torch.compile(dynamic=False, fullgraph=True)
def cautious_wd_and_update_inplace(p, mantissa, grad, wd_tensor, lr_tensor):
    """
    Cautious weight decay + parameter update. wd_tensor and lr_tensor are 0-D CPU tensors.
    Mantissa is tracked to enable higher precision updates on bfloat16 parameters.
    bfloat16 format: 1 sign bit + 8 exponent bits + 7 mantissa bits = 16 bits total
    float32 format: 1 sign bit + 8 exponent bits + 23 mantissa bits = 32 bits total
    """
    assert p.dtype == mantissa.dtype == torch.uint16
    grad = grad.float()
    wd_factor = wd_tensor.to(torch.float32)
    lr_factor = lr_tensor.to(torch.float32)
    p_precise_raw = (p.to(torch.uint32) << 16) | mantissa.to(torch.uint32)
    p_precise = p_precise_raw.view(torch.float32)
    mask = (grad * p_precise) >= 0
    p_precise.copy_(p_precise - (p_precise * mask * wd_factor * lr_factor) - (grad * lr_factor))
    p.copy_((p_precise_raw >> 16).to(torch.uint16))
    mantissa.copy_(p_precise_raw.to(torch.uint16))

@torch.compile(dynamic=False, fullgraph=True)
def apply_normuon_variance_reduction(v_chunk, second_momentum_buffer, beta2, red_dim):
    """NorMuon variance reduction. Algebraically fuses the normalization steps to minimize memory ops."""
    v_mean = v_chunk.float().square().mean(dim=red_dim, keepdim=True)
    red_dim_size = v_chunk.size(red_dim)
    v_norm_sq = v_mean.sum(dim=(-2, -1), keepdim=True).mul_(red_dim_size)
    v_norm = v_norm_sq.sqrt_()
    second_momentum_buffer.lerp_(v_mean.to(dtype=second_momentum_buffer.dtype), 1 - beta2)
    step_size = second_momentum_buffer.clamp_min(1e-10).rsqrt_()
    scaled_sq_sum = (v_mean * red_dim_size) * step_size.float().square()
    v_norm_new = scaled_sq_sum.sum(dim=(-2, -1), keepdim=True).sqrt_()
    final_scale = step_size * (v_norm / v_norm_new.clamp_min_(1e-10))
    return v_chunk.mul_(final_scale.type_as(v_chunk))


# -----------------------------------------------------------------------------
# NorMuon optimizer

class NorMuon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).

    Differences from standard Muon:
    - Newton-Shulz is replaced with Polar Express for the orthogonalization step
    - NorMuon adds a low-rank variance estimator similar to Adafactor. https://arxiv.org/pdf/2510.05491
    - small 1D parameters handled here instead of in Adam
    - Cautious weight decay, a gated version of decoupled weight decay
    - Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as
    needed on the forward pass. This enables attn and mlp weights to be contained within the same
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn.
    The scheduling is:
        1. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        2. reduce scatter attn/mlp round 2 (16 mlp params)
        3. wait on step 1, then compute update of 1 and schedule all gather
        4. wait on step 2, then compute update of 2 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        5. wait for each all gather to complete and update params
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, beta2=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, beta2=beta2)
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def reset(self):
        # expose a reset for clearing buffers
        for group in self.param_groups:
            if "momentum_buffer" in group:
                group["momentum_buffer"].zero_()
                group["mantissa"].zero_()
                group["second_momentum_buffer"].zero_()

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per module.
        """
        groups = defaultdict(list)
        for param in params:
            groups[param.label].append(param)

        param_groups = []
        for module_name, group_params in groups.items():
            chunk_size = (len(group_params) + self.world_size - 1) // self.world_size
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))

        return param_groups

    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn
        and mlp params when a param group is split across GPUs.
        """
        params_list = list(params)
        module_group_order = ['attn', 'mlp']
        group_sizes = [16, 16]  # 10 attn + 6 mlp, then 16 mlp
        params_list.sort(key=lambda x: module_group_order.index(x.label))

        idx = 0
        assert len(params_list) == sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            chunk_size = (size + self.world_size - 1) // self.world_size
            group_params = params_list[idx: idx + size]
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))
            idx += size

        return param_groups

    def step(self):
        self.step_p1()
        self.step_p2()
        self.step_p3()
        
    @torch.no_grad()
    def step_p1(self):
        """
        Part 1: Launch distributed reduce_scatter operations for parameter groups.
        """
        rank = dist.get_rank()
        self.group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            stacked_grads = torch.empty(
                (padded_num_params, *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device
            )
            for i, p in enumerate(params):
                stacked_grads[i].copy_(p.grad, non_blocking=True)
            if len(params) < padded_num_params:
                stacked_grads[len(params):].zero_()

            grad_chunk = torch.empty_like(stacked_grads[:chunk_size])

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            self.group_infos.append(dict(grad_chunk=grad_chunk, reduce_future=reduce_future))
    
    @torch.no_grad()
    def step_p2(self):
        """
        Part 2: Compute gradient updates and launch all_gather operations
        Wait for all gather to complete for all parameter groups except final one
        """
        # Efficient distributed step by @YouJiacheng, @KonstantinWilleke, @alexrgilbert,
        # @adricarda, @tuttyfrutyee, @vdlad, @ryanyang0, @vagrawal, @varunneal, @chrisjmccormick
        rank = dist.get_rank()
        group_infos = self.group_infos
        
        self.all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = group["params"]
            grad_chunk = info["grad_chunk"].float()
            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            start_idx = rank * chunk_size
            module_idx = start_idx if start_idx < len(params) else 0

            num_params = min(chunk_size, max(0, len(params) - start_idx))  # num params for this rank

            if "momentum_buffer" not in group:
                group["momentum_buffer"]  = torch.zeros_like(grad_chunk[:num_params], dtype=torch.float32)
                
            momentum_buffer = group["momentum_buffer"]
            # Apply momentum update to the persistent momentum buffer in-place
            momentum_buffer.lerp_(grad_chunk[:num_params], 1 - group["momentum"])
            updated_grads = grad_chunk[:num_params].lerp_(momentum_buffer, group["momentum"])

            grad_shape = updated_grads.shape
            if params[module_idx].label == 'attn':
                for p in params[module_idx:module_idx + num_params]:
                    assert p.label == 'attn'
                updated_grads = updated_grads.view(4 * grad_shape[0], grad_shape[1] // 4, grad_shape[2])

            ref_param = params[module_idx]
            param_shape = ref_param.shape

            if "second_momentum_buffer" not in group:
                group["second_momentum_buffer"] = (torch.zeros_like(updated_grads[..., :, :1], dtype=torch.float32)
                    if param_shape[-2] >= param_shape[-1] else torch.zeros_like(updated_grads[..., :1, :])
                )
            second_momentum_buffer = group["second_momentum_buffer"]

            if "param_lr_cpu" not in group:
                # Define multipliers for ALL params in this group (global, not per-shard)
                lr_mults = []
                wd_mults = []
                for p in params:
                    # Increase learning rate for modules with larger inputs than outputs.
                    # This shape check also assumes rows=input, columns=output, so take care
                    # when changing memory layouts. @chrisjmccormick
                    shape = p.shape
                    if len(shape) >= 2:
                        shape_mult = max(1.0, shape[-2] / shape[-1]) ** 0.5
                    else:
                        shape_mult = 1.0
                    lr_mults.append(shape_mult * getattr(p, "lr_mul", 1.0))
                    wd_mults.append(getattr(p, "wd_mul", 1.0))
                # Define as cpu tensors to enable Inductor constant folding
                group["param_lr_cpu"] = torch.tensor(lr_mults, dtype=torch.float32, device="cpu")
                group["param_wd_cpu"] = torch.tensor(wd_mults, dtype=torch.float32, device="cpu")

            eff_lr_all = group["param_lr_cpu"] * group["lr"]
            eff_wd_all = group["param_wd_cpu"] * group["weight_decay"] * group["lr"]

            # Slice the portion corresponding to this rank's shard
            eff_lr_cpu = eff_lr_all[module_idx:module_idx + num_params]
            eff_wd_cpu = eff_wd_all[module_idx:module_idx + num_params]

            # Compute zeropower for the entire chunk in a single, batched call.
            if num_params == 0:
                v_chunk = updated_grads
            else:
                v_chunk = polar_express(updated_grads, split_baddbmm=(ref_param.label == 'mlp'))

            # Note that the head orientation in O is transposed relative to QKV, so red_dim
            # is 'incorrect' for O. However, correcting this showed no improvement. @chrisjmccormick
            red_dim = -1 if param_shape[-2] >= param_shape[-1] else -2

            v_chunk = apply_normuon_variance_reduction(
                v_chunk, second_momentum_buffer, group["beta2"], red_dim
            )

            v_chunk = v_chunk.view(grad_shape)

            # # "Cautious" weight decay (https://arxiv.org/abs/2510.12402)
            updated_params = torch.empty_like(grad_chunk, dtype=torch.bfloat16)
            if num_params > 0:
                # Work on a stacked copy to avoid touching original params
                param_chunk = torch.stack(params[module_idx:module_idx + num_params])

                if "mantissa" not in group:
                    group["mantissa"] = torch.zeros_like(param_chunk, dtype=torch.uint16)
                mantissa = group["mantissa"]

                for local_idx in range(num_params):
                    cautious_wd_and_update_inplace(
                        param_chunk[local_idx].view(torch.uint16),
                        mantissa[local_idx],
                        v_chunk[local_idx],
                        eff_wd_cpu[local_idx],
                        eff_lr_cpu[local_idx]
                    )
            else:
                param_chunk = torch.zeros_like(v_chunk)

            updated_params[:num_params].copy_(param_chunk)
            if num_params < chunk_size:
                updated_params[num_params:].zero_()

            stacked_params = torch.empty(
                (padded_num_params, *param_shape),
                dtype=updated_params.dtype,
                device=updated_params.device,
            )

            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_params, async_op=True
            ).get_future()

            self.all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete for all except final and copy results back into original parameter tensors.
        for info in self.all_gather_infos[:-1]:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)

    @torch.no_grad()
    def step_p3(self):
        """
        Part 3: Wait for final all gather to complete and copy results back into original parameter tensors
        """
        info = self.all_gather_infos[-1]
        info["gather_future"].wait()
        stacked_params = info["stacked_params"]
        orig_params = info["orig_params"]

        unstacked_params = torch.unbind(stacked_params)
        for i, p in enumerate(orig_params):
            p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, label_order: list[str], betas: list[list[float]], lr: float = 1e-3, eps: float = 1e-8, weight_decay: float = 0.01):
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        defaults = dict(lr=lr, eps=eps, weight_decay=weight_decay)
        params = list(params)
        # Group by label, with explicit ordering for execution control.
        params_by_label = defaultdict(list)
        for p in params:
            params_by_label[getattr(p, 'label', None)].append(p)
        param_groups = []
        for idx, label in enumerate(label_order):
            if label in params_by_label:
                param_groups.append(dict(params=params_by_label[label], betas=betas[idx]))
        # include any unlabeled params at the end (processed last)
        if None in params_by_label:
            param_groups.append(dict(params=params_by_label[None]))
        super().__init__(param_groups, defaults)
        # init state: small params (numel < 1024) use full-sized state, others use sharded
        for p in params:
            chunk = p if p.numel() < 1024 else p[:p.size(0) // self.world_size]
            exp_avg = torch.zeros_like(chunk, dtype=torch.float32, device=p.device)
            self.state[p] = dict(step=0, exp_avg=exp_avg, exp_avg_sq=torch.zeros_like(exp_avg))

        # tag the final param for optimizer pipelining, run all gather after muon copy
        param_groups[-1]['params'][-1].is_final_param = True
        
        # DistributedAdam implementation by @vagrawal, @akash5474
        self.should_sync = False
        self._reduce_scatter_hooks = []
        self._reduce_scatter_futures = {}
        # 0-D CPU tensors to avoid recompilation in _update_step
        self._step_size_t = torch.tensor(0.0, dtype=torch.float32, device="cpu")
        self._eff_wd_t = torch.tensor(0.0, dtype=torch.float32, device="cpu")
        self.register_backward_hooks()

    def register_backward_hooks(self):
        for group in self.param_groups:
            for param in group["params"]:
                self._reduce_scatter_hooks.append(param.register_post_accumulate_grad_hook(self._sync_gradient))

    def load_state_dict(self, state_dict):
        """Override to preserve optimizer state dtypes (avoid BFloat16->Float32 cast that causes recompilation)."""
        # Save original state dtypes before loading
        original_dtypes = {}
        for p, s in self.state.items():
            original_dtypes[p] = {k: v.dtype for k, v in s.items() if isinstance(v, torch.Tensor)}
        
        # Call parent load_state_dict (which may cast dtypes to match param dtype)
        super().load_state_dict(state_dict)
        
        # Restore original dtypes
        for p, s in self.state.items():
            if p in original_dtypes:
                for k, v in s.items():
                    if isinstance(v, torch.Tensor) and k in original_dtypes[p]:
                        if v.dtype != original_dtypes[p][k]:
                            s[k] = v.to(original_dtypes[p][k])

    @torch.no_grad()
    def _sync_gradient(self, param):
        if not self.should_sync:
            return

        grad = param.grad
        if param.numel() < 1024:
            # Small params: use all_reduce (no scatter/gather needed)
            self._reduce_scatter_futures[param] = (
                dist.all_reduce(grad, op=dist.ReduceOp.AVG, async_op=True).get_future(),
                grad
            )
        else:
            rank_size = grad.shape[0] // self.world_size
            if grad is not None:
                grad_slice = torch.empty_like(grad[:rank_size])
                self._reduce_scatter_futures[param] = (
                    dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future(),
                    grad_slice
                )

    def copy_lm_to_embed(self):
        # run at 2/3 of training
        lm_head = self.param_groups[0]['params'][0]
        embed = self.param_groups[-2]['params'][0]
        lm_head_state = self.state[lm_head]
        embed_state = self.state[embed]
        embed_state['step'] = lm_head_state['step']
        embed_state['exp_avg'] = lm_head_state['exp_avg'].clone()
        embed_state['exp_avg_sq'] = lm_head_state['exp_avg_sq'].clone()
        embed.data.copy_(lm_head.data)

    @staticmethod
    @torch.compile(dynamic=False, fullgraph=True)
    def _update_step(p_slice, g_slice, exp_avg, exp_avg_sq, beta1, beta2, eps, step_size_t, eff_wd_t):
        """Compiled Adam update step. step_size_t and eff_wd_t are 0-D CPU tensors to avoid recompilation."""
        exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)  # exp_avg = beta1 * exp_avg + (1 - beta1) * g_slice
        exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)  # exp_avg_sq = beta2 * exp_avg_sq + (1 - beta2) * g_slice^2
        # compute step
        update = exp_avg.div(exp_avg_sq.sqrt().add_(eps)).mul_(step_size_t)  # update = (exp_avg / (sqrt(exp_avg_sq) + eps)) * step_size
        # cautious weight decay
        mask = (update * p_slice) > 0
        update.addcmul_(p_slice, mask, value=eff_wd_t)  # update += eff_wd_t * p_slice * mask
        p_slice.add_(other=update, alpha=-1.0)  # p_slice -= update

    @torch.no_grad()
    def step(self, muon_opt):
        muon_opt.step_p1()
        rank = dist.get_rank()
        all_gather_futures: list[torch.Future] = []

        last_param = None
        last_p_slice = None
        for group in self.param_groups:      
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            for param in group['params']:
                if param not in self._reduce_scatter_futures:
                    continue

                fut, g_slice = self._reduce_scatter_futures[param]
                fut.wait()

                is_small = param.numel() < 1024
                if is_small:
                    # Small params: g_slice is actually full grad, p_slice is full param
                    p_slice = param
                else:
                    rank_size = param.shape[0] // self.world_size
                    p_slice = param[rank * rank_size:(rank + 1) * rank_size]

                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]
                state["step"] += 1
                t = state["step"]
                
                # Pre-compute changing values as 0-D CPU tensors to avoid recompilation.
                # `.fill_(value)` is the same as "= value", but doesn't change the tensor object.
                bias1, bias2 = 1 - beta1 ** t, 1 - beta2 ** t
                self._step_size_t.fill_(lr * (bias2 ** 0.5 / bias1))
                self._eff_wd_t.fill_(lr * lr * wd * getattr(param, "wd_mul", 1.0)) # `lr` included twice to serve as weight decay schedule.

                DistAdam._update_step(p_slice, g_slice, state["exp_avg"], state["exp_avg_sq"],
                                      beta1, beta2, eps, self._step_size_t, self._eff_wd_t)

                if not is_small:
                    if getattr(param, "is_final_param", False):
                        last_param = param
                        last_p_slice = p_slice
                    else:
                        all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())
        self._reduce_scatter_futures.clear()

        muon_opt.step_p2()
        torch.futures.collect_all(all_gather_futures).wait()

        if last_param is not None:
            last_all_gather_future = dist.all_gather_into_tensor(last_param, last_p_slice, async_op=True).get_future()
        muon_opt.step_p3()
        torch.futures.collect_all([last_all_gather_future]).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        with torch.no_grad():
            self.weight.zero_()  # @Grad62304977 and others

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))


# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float
    key_offset: bool
    attn_gate_w: torch.Tensor
    ve_gate_w: torch.Tensor

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = self.dim ** -0.5
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKVO weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # Simplified layout by @chrisjmccormick
        self.qkvo_w = nn.Parameter(torch.empty(self.dim * 4, self.hdim, dtype=torch.bfloat16))
        # label all modules for explicit optimizer grouping
        self.qkvo_w.label = 'attn'

        with torch.no_grad():
            self.qkvo_w[:self.dim * 3].uniform_(-bound, bound)  # init QKV weights
            self.qkvo_w[self.dim * 3:].zero_()  # init O weights to zero

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas, key_offset = attn_args.ve, attn_args.sa_lambdas, attn_args.key_offset
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size
        # sparse gated attention to enable context based no-op by @classiclarryd
        # only include gates on layers with value embeds used on forward pass
        attn_gate_w, ve_gate_w = attn_args.attn_gate_w, attn_args.ve_gate_w

        q, k, v = F.linear(x, sa_lambdas[0] * self.qkvo_w[:self.dim * 3].type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if key_offset:
            # shift keys forward for the stationary head dims. Enables 1-layer induction.
            k[:, 1:, :, self.head_dim // 4:self.head_dim // 2] = k[:, :-1, :, self.head_dim // 4:self.head_dim // 2]
            k[:, 1:, :, 3 * self.head_dim // 4:] = k[:, :-1, :, 3 * self.head_dim // 4:]
        if ve is not None:
            ve_gate_out = 2 * torch.sigmoid(F.linear(x[..., :12], ve_gate_w)).view(B, T, self.num_heads, 1)
            v = v + ve_gate_out * ve.view_as(v) # @ KoszarskyB & @Grad62304977

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens,
                                                        max_seqlen_q=max_len, max_seqlen_k=max_len,
                                                        causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(F.linear(x[..., :12], attn_gate_w)).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, sa_lambdas[1] * self.qkvo_w[self.dim * 3:].type_as(y))  # sa_lambdas[1] pre-multiplied to O @shenberg
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # Transposed layout to match attention weights
        self.c_fc = nn.Parameter(torch.empty(hdim, dim, dtype=torch.bfloat16))
        self.c_proj = nn.Parameter(torch.empty(hdim, dim, dtype=torch.bfloat16))
        # label all modules for explicit optimizer grouping
        self.c_fc.label = 'mlp'
        self.c_proj.label = 'mlp'
        self.c_proj.lr_mul = 2.

        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.T.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.6 (the 7th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads, layer_idx) if layer_idx != 6 else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim)

    def forward(self, x: Tensor, attn_args: AttnArgs):
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

@dataclass
class ForwardScheduleConfig:
    mtp_weights: torch.Tensor
    ws_short: int
    ws_long: int

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        self.num_layers = num_layers
        vocab_size = next_multiple_of_n(vocab_size, n=128)

        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.label = 'smear_gate'
        self.smear_gate.weight.lr_mul = 0.01
        self.smear_gate.weight.wd_mul = 0.0

        self.skip_gate = CastedLinear(12, 1)
        self.skip_gate.weight.label = 'skip_gate'
        self.skip_gate.weight.lr_mul = 0.05
        self.skip_gate.weight.wd_mul = 0.0

        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        for embed in self.value_embeds:
            nn.init.zeros_(embed.weight)
        for ve in self.value_embeds:
            ve.weight.label = 'value_embed'
        
        # parameter banks for attention and value embedding gate weights
        self.attn_gate_bank = nn.Parameter(torch.zeros(10, num_heads, 12)) # 10 layers
        self.attn_gate_bank.label = 'attn_gate_bank'
        self.ve_gate_bank = nn.Parameter(torch.zeros(5, num_heads, 12)) # 5 layers
        self.ve_gate_bank.label = 've_gate_bank'

        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)

        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=100/448, w_s=1.6/448, grad_s=0.75/448)
        nn.init.normal_(self.lm_head.weight, mean=0, std=0.005)
        self.lm_head.weight.label = 'lm_head'

        self.embed = nn.Embedding(vocab_size, model_dim)
        self.embed.weight.label = 'embed'

        # x0_lambdas separated out for different optimizer treatment (no beta smoothing)
        self.x0_lambdas = nn.Parameter(torch.zeros(num_layers))
        self.x0_lambdas.label = 'x0_lambdas'
        self.x0_lambdas.lr_mul = 5.0
        self.x0_lambdas.wd_mul = 0.0

        pad = (-num_layers * 3 - 3) % dist.get_world_size()  # updated: 3*num_layers instead of 4*
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    1.1 * torch.ones(num_layers),  # resid lambdas. 1.1 init such that layer i weight is i^(num_layers-i).
                    *[torch.tensor([0.5, 1.0]) for _ in range(num_layers)],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    -1.5 * torch.ones(1),  # skip_lambda -> σ(-1.5) ≈ 0.18
                    torch.ones(pad),
                ]
            )
        )

        self.scalars.label = 'scalars'
        # set learning rates
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
            param.wd_mul = 5.
        for param in self.embed.parameters():
            param.wd_mul = 150.
        for param in self.lm_head.parameters():
            param.wd_mul = 150.
        self.scalars.lr_mul = 5.0
        self.scalars.wd_mul = 0.0

        self.split_embed = False

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, schedule_cfg: ForwardScheduleConfig):
        assert input_seq.ndim == 1

        # unpack schedule_cfg
        mtp_weights, ws_short, ws_long = schedule_cfg.mtp_weights, schedule_cfg.ws_short, schedule_cfg.ws_long

        # set configs
        skip_connections = []
        skip_in = [3] # long attention window on layer 3
        skip_out = [6] # no attn op on layer 6
        x_backout = None
        backout_layer = 7

        # set lambdas
        resid_lambdas = self.scalars[: 1 * self.num_layers]
        x0_lambdas = self.x0_lambdas
        sa_lambdas = self.scalars[1 * self.num_layers: 3 * self.num_layers].view(-1, 2)
        smear_lambda = self.scalars[3 * self.num_layers]
        backout_lambda = self.scalars[3 * self.num_layers+1]
        skip_lambda = self.scalars[3 * self.num_layers+2]

        # set block masks and key shift
        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == self.num_layers
        key_offset = [b==long_bm for b in bm_sizes] # apply partial key offset to long windows

        # weight-tied: use lm_head.weight for embedding lookup (or separate embed after split)
        if self.split_embed:
            x = self.embed(input_seq)
        else:
            x = F.embedding(input_seq, self.lm_head.weight)
        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [ve[1], ve[2]] + [None] * (self.num_layers - 5) + [ve[0], ve[1], ve[2]]
        assert len(ve) == self.num_layers

        # smear token embed forward 1 position @classiclarryd
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # unbind gate banks to avoid select_backwards kernel
        ag = [w.bfloat16() for w in self.attn_gate_bank.unbind(0)] 
        veg = [w.bfloat16() for w in self.ve_gate_bank.unbind(0)]
        attn_gates = ag[:6] + [None] + ag[6:]
        ve_gates = [veg[0], veg[1]] + [None] * (self.num_layers - 5) + [veg[2], veg[3], veg[4]]
        assert len(attn_gates) == self.num_layers
        assert len(ve_gates) == self.num_layers

        for i in range(self.num_layers):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale,
                key_offset=key_offset[i],
                attn_gate_w=attn_gates[i],
                ve_gate_w=ve_gates[i]
            )
            if i in skip_out:
                skip_gate_out = torch.sigmoid(skip_lambda) * 2 * torch.sigmoid(self.skip_gate(x0[..., :self.skip_gate.weight.size(-1)]))
                x = x + skip_gate_out * skip_connections.pop()
            if i == 0:
                x = (resid_lambdas[0] + x0_lambdas[0]) * x
            else:
                x = resid_lambdas[i] * x + x0_lambdas[i] * x0
            x = self.blocks[i](x, attn_args)
            if i in skip_in:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 7 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15
        # @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1). @classiclarryd updated to 23*sigmoid((logits+5)/7.5)
        logits = 23 * torch.sigmoid((logits + 5) / 7.5)
        logits_for_loss = logits.float() if not self.training else logits

        n_predict = mtp_weights.size(0) if mtp_weights is not None else 1
        if self.training and n_predict > 1:
            # Multi-token prediction: take loss of the weighted average of next n_predict tokens
            logits_flat = logits_for_loss.view(-1, logits_for_loss.size(-1))
            idx = F.pad(target_seq, (0, n_predict - 1)).unfold(0, n_predict, 1)  # [T, n_predict] of shifted targets
            target_logits = logits_flat.gather(1, idx)
            cross_entropy = torch.logsumexp(logits_flat, dim=-1).unsqueeze(1) - target_logits
            for k in range(1, n_predict):  # zero out preds past end of sequence
                cross_entropy[-k:, k] = 0
            loss = (cross_entropy * mtp_weights).sum()
        elif self.training:
            loss = F.cross_entropy(logits_for_loss.view(-1, logits_for_loss.size(-1)), target_seq, reduction="sum")
        else:
            loss = F.cross_entropy(logits_for_loss.view(-1, logits_for_loss.size(-1)), target_seq, reduction="mean")
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()

    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        # Cast to int32 on CPU before transfer to avoid dtype conversion during .to()
        _inputs = _inputs.to(dtype=torch.int32)
        _targets = _targets.to(dtype=torch.int64)
        _cum_lengths = _cum_lengths.to(dtype=torch.int32)

        new_params = yield (
            _inputs.to(device="cuda", non_blocking=True),
            _targets.to(device="cuda", non_blocking=True),
            _cum_lengths.to(device="cuda", non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * new_grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens // new_grad_accum_steps
            max_seq_len = new_max_seq_len

# -----------------------------------------------------------------------------
# Training Management
 
def get_bs(step: int):
    if step >= args.num_scheduled_iterations:
        return args.train_bs_extension
    x = step / args.num_scheduled_iterations
    bs_idx = int(len(args.train_bs_schedule) * x)
    return args.train_bs_schedule[bs_idx]

def get_ws(step: int):
    # set short window size to half of long window size
    # Higher ws on "extension" steps
    if step >= args.num_scheduled_iterations:
        return args.ws_final // 2, args.ws_final
    x = step / args.num_scheduled_iterations
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

# learning rate schedule: tied to batch size schedule, with cooldown at the end.
def get_lr(step: int):
    if step > args.num_scheduled_iterations:
        return 0.1
    lr_max = 1.0
    x = step / args.num_scheduled_iterations
    if x > 1/3:
       lr_max = 1.52  # (16/8)**0.6
    if x > 2/3:
        lr_max = 1.73  # (24/8)**0.5
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = lr_max * w + (1 - w) * 0.1
        return lr
    return lr_max

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

class TrainingManager():
    """
    Manages three optimizers for Adam embed/lm_head, Adam scalars, and Muon weight matrices.
    Notable Features:
        1. Scalars are given higher momentum terms to smooth learning @ChrisJMcCormick
        2. Scalar weights are temporarily frozen during batch size or window size updates @ChrisJMcCormick
        3. Adam optimizers are only stepped on odd steps @classiclarryd
        4. Adam optimizers have hooks to start gradient communication during backwards pass @akash5474
        5. Muon has a linear momentum warmup and cooldown schedule
        6. Learning rates follow a linear decay schedule
        7. Embed/lm_head weights and optimizer state splits at 2/3 of training @classiclarryd

    Manages model architecture, data, and target that changes during training
    Notable Features:
        1. Multi Token Prediction schedule of [1, 0.5, 0.25->0] -> [1, 0.5->0] -> [1] @varunneal
        2. Sliding Attention window schedule of [1,3] -> [3,7] -> [5,11] -> [6,13]
        3. YaRN updates to RoPE on window changes
        4. Split embed and lm head at 2/3 of training
        5. Batch size schedule of 8 -> 16 -> 24
        6. Post training extension of long windows from 13 to 20
    """
    def __init__(self, model):
        self.mtp_weights_schedule = self._build_mtp_schedule()
        self.model = model
        adam_betas = {
            'lm_head': [0.5, 0.95],
            'smear_gate': [0.9, 0.99],
            'attn_gate_bank': [0.9, 0.99],
            've_gate_bank': [0.9, 0.99],
            'skip_gate': [0.9, 0.99],
            'x0_lambdas': [0.65, 0.95],
            'scalars': [0.9, 0.99],
            'embed': [0.5, 0.95],
            'value_embed': [0.75, 0.95]
        }
        adam_labels = list(adam_betas.keys())
        adam_beta_values = list(adam_betas.values())
        muon_labels = ['attn', 'mlp']
        adam_params = [p for p in model.parameters() if getattr(p, 'label', None) in adam_labels]
        muon_params = [p for p in model.parameters() if getattr(p, 'label', None) in muon_labels]
        assert set(getattr(p, 'label', None) for p in model.parameters()) == set(adam_labels + muon_labels), "All params must have label"

        self.adam_opt = DistAdam(adam_params, adam_labels, adam_beta_values, lr=0.008, eps=1e-10, weight_decay=0.005)
        self.muon_opt = NorMuon(muon_params, lr=0.023, momentum=0.95, beta2=0.95, weight_decay=1.2)
        self.optimizers = [self.adam_opt, self.muon_opt]

        # split after odd number step
        self.split_step = math.ceil(args.split_embed_frac * args.num_scheduled_iterations) | 1

        # set defaults
        for opt in self.optimizers:
            opt.odd_step_only = False 
            opt.should_sync = True
            for group in opt.param_groups:
                group["initial_lr"] = group["lr"]

        # on even steps, only step Muon params
        self.adam_opt.odd_step_only = True

        self.reset()

    def _build_mtp_schedule(self):
        # Precompute MTP weights for all steps to avoid tensor allocation during training
        # Schedule: [1, 0.5, 0.25->0] -> [1, 0.5->0] -> [1]
        mtp_weights_schedule = []
        for s in range(args.num_iterations + 1):
            x = s / args.num_scheduled_iterations
            if x < 1/3:
                w = [1.0, 0.5, 0.25 * (1 - 3*x)]
            elif x < 2/3:
                w = [1.0, 0.5 * (1 - (3*x - 1))]
            else:
                w = [1.0]
            mtp_weights_schedule.append(torch.tensor(w, device=device))
        return mtp_weights_schedule

    def apply_final_ws_ext(self):
        self.ws_long = args.ws_validate_post_yarn_ext

    def get_forward_args(self):
        return ForwardScheduleConfig(
            mtp_weights = self.mtp_weights,
            ws_short = self.ws_short,
            ws_long = self.ws_long
        )
    
    def _is_active_step(self, opt, step: int):
        return (opt.odd_step_only and step%2==1) or not opt.odd_step_only

    def get_transition_steps(self):
        transition_steps = []
        ws_short, ws_long = get_ws(0)
        for step in range(1, args.num_iterations):
            ws_short, new_ws_long = get_ws(step)
            if new_ws_long != ws_long:
                transition_steps.append(step)
                ws_long = new_ws_long
        return transition_steps

    def advance_schedule(self, step: int):
        self.ws_short, new_ws_long = get_ws(step)
        if new_ws_long != self.ws_long:
            self.model.yarn.apply(self.ws_long, new_ws_long)

        new_batch_size = get_bs(step)
        if new_batch_size != self.batch_size:
            self.train_loader_send_args = (new_batch_size, args.train_max_seq_len, grad_accum_steps)
        else:
            self.train_loader_send_args = None

        self.ws_long = new_ws_long
        self.mtp_weights = self.mtp_weights_schedule[step]
    
    def step_optimizers(self, step: int):                
        step_lr = get_lr(step)
        muon_momentum = get_muon_momentum(step)
        for group in self.muon_opt.param_groups:
            group["momentum"] = muon_momentum

        for opt in self.optimizers:
            for group in opt.param_groups:
                group["lr"] = group["initial_lr"] * step_lr
                
        if self._is_active_step(self.adam_opt, step):
            # adam will interleave calls to muon step
            self.adam_opt.step(self.muon_opt)
            self.model.zero_grad(set_to_none=True)
            self.adam_opt.should_sync = False
        else:
            self.muon_opt.step()
            self.muon_opt.zero_grad(set_to_none=True)
            
        if step == self.split_step:
            self.adam_opt.copy_lm_to_embed()
            self.model.split_embed = True
    
    def activate_hooks(self, step: int):
        for opt in self.optimizers:
            if self._is_active_step(opt, step):
                opt.should_sync = True

    def reset(self, state=None):
        if state is not None:
            for opt, opt_state in zip(self.optimizers, state):
                opt.should_sync = False
                opt.load_state_dict(opt_state)

        # muon momentum buffers not in state dict
        self.muon_opt.reset()
        self.model.split_embed = False

        self.ws_short, self.ws_long = get_ws(0)
        self.batch_size = get_bs(0)
        self.model.yarn.reset()

    def get_state(self):
        return [copy.deepcopy(opt.state_dict()) for opt in self.optimizers]

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # batch sizes
    train_bs_schedule: tuple = (8 * 2048 * 8, 16 * 2048 * 8, 24 * 2048 * 8)
    train_bs_extension: int = 24 * 2048 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 1800  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: float = 0.50  # fraction of num_scheduled_iterations spent cooling down the learning rate
    split_embed_frac: float = 2/3  # fraction of training when embeddings split from lm_head
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_final: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=11,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=args.val_batch_size // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.weight.data = m.weight.data.bfloat16()
model.attn_gate_bank.data = model.attn_gate_bank.data.bfloat16()
model.ve_gate_bank.data = model.ve_gate_bank.data.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)
training_manager = TrainingManager(model)

########################################
#            Warmup kernels            #
########################################
print0("Compiling model and warming up kernels (~7 minutes on first execution)", console=True)
# Warmup the training kernels, then re-initialize the state so we aren't cheating
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=training_manager.get_state()) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_bs_schedule[0], args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)

transition_steps = training_manager.get_transition_steps()
# first few steps plus transitions
warmup_steps = sorted({0, 1, 2} | set(s + offset for s in transition_steps for offset in [-1, 0, 1] if s + offset >= 0)) 
print0(f"Sampling steps {warmup_steps} for warmup", console=True)
for step in warmup_steps:
    training_manager.advance_schedule(step)
    model.eval()
    with torch.no_grad():
        inputs, targets, cum_seqlens = next(val_loader)
        model(inputs, targets, cum_seqlens, training_manager.get_forward_args())
    model.train()
    for idx in range(grad_accum_steps):
        # enable gradient sync for the DistAdam optimizers on the last iteration before we step them
        if idx == grad_accum_steps - 1:
            training_manager.activate_hooks(step)
        send_args = training_manager.train_loader_send_args
        inputs, targets, cum_seqlens = train_loader.send(send_args)
        (model(inputs, targets, cum_seqlens, training_manager.get_forward_args()) / grad_accum_steps).backward()
    training_manager.step_optimizers(step)
print0("Resetting Model", console=True)
model.zero_grad(set_to_none=True)
model.load_state_dict(initial_state["model"])
training_manager.reset(initial_state["optimizers"])
del val_loader, train_loader, initial_state
model.train()

########################################
#        Training and validation       #
########################################
train_loader = distributed_data_generator(args.train_files, args.train_bs_schedule[0], args.train_max_seq_len, grad_accum_steps=grad_accum_steps)

gc.collect()

training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    training_manager.advance_schedule(step)
    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            training_manager.apply_final_ws_ext()
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, training_manager.get_forward_args())
        val_loss /= val_steps
        del val_loader
        dist.reduce(val_loss, 0, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for idx in range(grad_accum_steps):
        # enable gradient sync for the DistAdam optimizers on the last iteration before we step them
        if idx == grad_accum_steps - 1:
            training_manager.activate_hooks(step)
        send_args = training_manager.train_loader_send_args
        inputs, targets, cum_seqlens = train_loader.send(send_args)
        (model(inputs, targets, cum_seqlens, training_manager.get_forward_args()) / grad_accum_steps).backward()
    training_manager.step_optimizers(step)

    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.10.12 (main, Nov  4 2025, 08:48:33) [GCC 11.4.0]
Running PyTorch 2.10.0.dev20251210+cu126 compiled for CUDA 12.6
Running Triton version 3.6.0
Mon Jan  5 08:06:31 2026       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:19:00.0 Off |                    0 |
| N/A   42C    P0            127W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:3B:00.0 Off |                    0 |
| N/A   35C    P0            131W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:4C:00.0 Off |                    0 |
| N/A   33C    P0            127W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:5D:00.0 Off |                    0 |
| N/A   41C    P0            129W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:9B:00.0 Off |                    0 |
| N/A   44C    P0            133W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:BB:00.0 Off |                    0 |
| N/A   35C    P0            125W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:CB:00.0 Off |                    0 |
| N/A   41C    P0            130W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:DB:00.0 Off |                    0 |
| N/A   32C    P0            125W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A     56437      C   /usr/bin/python                                 0MiB |
|    1   N/A  N/A     56438      C   /usr/bin/python                                 0MiB |
|    2   N/A  N/A     56439      C   /usr/bin/python                                 0MiB |
|    3   N/A  N/A     56440      C   /usr/bin/python                                 0MiB |
|    4   N/A  N/A     56441      C   /usr/bin/python                                 0MiB |
|    5   N/A  N/A     56442      C   /usr/bin/python                                 0MiB |
|    6   N/A  N/A     56443      C   /usr/bin/python                                 0MiB |
|    7   N/A  N/A     56444      C   /usr/bin/python                                 0MiB |
+-----------------------------------------------------------------------------------------+

====================================================================================================
Compiling model and warming up kernels (~7 minutes on first execution)
Sampling steps [0, 1, 2, 599, 600, 601, 1199, 1200, 1201, 1799, 1800, 1801] for warmup
Resetting Model
step:0/1840 val_loss:10.8279 train_time:0ms step_avg:0.03ms
step:1/1840 train_time:71ms step_avg:70.50ms
step:2/1840 train_time:92ms step_avg:45.84ms
step:3/1840 train_time:110ms step_avg:36.66ms
step:4/1840 train_time:141ms step_avg:35.37ms
step:5/1840 train_time:173ms step_avg:34.66ms
step:6/1840 train_time:263ms step_avg:43.81ms
step:7/1840 train_time:280ms step_avg:39.94ms
step:8/1840 train_time:303ms step_avg:37.83ms
step:9/1840 train_time:335ms step_avg:37.17ms
step:10/1840 train_time:369ms step_avg:36.85ms
step:11/1840 train_time:400ms step_avg:36.40ms
step:12/1840 train_time:435ms step_avg:36.23ms
step:13/1840 train_time:467ms step_avg:35.90ms
step:14/1840 train_time:501ms step_avg:35.78ms
step:15/1840 train_time:533ms step_avg:35.55ms
step:16/1840 train_time:567ms step_avg:35.46ms
step:17/1840 train_time:599ms step_avg:35.25ms
step:18/1840 train_time:633ms step_avg:35.19ms
step:19/1840 train_time:666ms step_avg:35.05ms
step:20/1840 train_time:700ms step_avg:35.00ms
step:21/1840 train_time:732ms step_avg:34.87ms
step:22/1840 train_time:766ms step_avg:34.84ms
step:23/1840 train_time:798ms step_avg:34.71ms
step:24/1840 train_time:833ms step_avg:34.70ms
step:25/1840 train_time:865ms step_avg:34.59ms
step:26/1840 train_time:899ms step_avg:34.57ms
step:27/1840 train_time:931ms step_avg:34.48ms
step:28/1840 train_time:965ms step_avg:34.47ms
step:29/1840 train_time:997ms step_avg:34.39ms
step:30/1840 train_time:1031ms step_avg:34.38ms
step:31/1840 train_time:1064ms step_avg:34.31ms
step:32/1840 train_time:1098ms step_avg:34.30ms
step:33/1840 train_time:1130ms step_avg:34.26ms
step:34/1840 train_time:1166ms step_avg:34.30ms
step:35/1840 train_time:1199ms step_avg:34.27ms
step:36/1840 train_time:1234ms step_avg:34.29ms
step:37/1840 train_time:1267ms step_avg:34.24ms
step:38/1840 train_time:1301ms step_avg:34.24ms
step:39/1840 train_time:1333ms step_avg:34.19ms
step:40/1840 train_time:1368ms step_avg:34.20ms
step:41/1840 train_time:1400ms step_avg:34.15ms
step:42/1840 train_time:1435ms step_avg:34.16ms
step:43/1840 train_time:1467ms step_avg:34.12ms
step:44/1840 train_time:1501ms step_avg:34.12ms
step:45/1840 train_time:1533ms step_avg:34.07ms
step:46/1840 train_time:1568ms step_avg:34.08ms
step:47/1840 train_time:1600ms step_avg:34.04ms
step:48/1840 train_time:1634ms step_avg:34.05ms
step:49/1840 train_time:1667ms step_avg:34.02ms
step:50/1840 train_time:1701ms step_avg:34.02ms
step:51/1840 train_time:1733ms step_avg:33.98ms
step:52/1840 train_time:1768ms step_avg:34.00ms
step:53/1840 train_time:1800ms step_avg:33.96ms
step:54/1840 train_time:1834ms step_avg:33.97ms
step:55/1840 train_time:1867ms step_avg:33.94ms
step:56/1840 train_time:1901ms step_avg:33.94ms
step:57/1840 train_time:1933ms step_avg:33.91ms
step:58/1840 train_time:1967ms step_avg:33.92ms
step:59/1840 train_time:1999ms step_avg:33.89ms
step:60/1840 train_time:2033ms step_avg:33.89ms
step:61/1840 train_time:2066ms step_avg:33.87ms
step:62/1840 train_time:2100ms step_avg:33.87ms
step:63/1840 train_time:2132ms step_avg:33.84ms
step:64/1840 train_time:2167ms step_avg:33.85ms
step:65/1840 train_time:2199ms step_avg:33.83ms
step:66/1840 train_time:2233ms step_avg:33.84ms
step:67/1840 train_time:2266ms step_avg:33.81ms
step:68/1840 train_time:2300ms step_avg:33.82ms
step:69/1840 train_time:2332ms step_avg:33.80ms
step:70/1840 train_time:2367ms step_avg:33.81ms
step:71/1840 train_time:2399ms step_avg:33.79ms
step:72/1840 train_time:2433ms step_avg:33.79ms
step:73/1840 train_time:2466ms step_avg:33.77ms
step:74/1840 train_time:2500ms step_avg:33.78ms
step:75/1840 train_time:2532ms step_avg:33.76ms
step:76/1840 train_time:2566ms step_avg:33.76ms
step:77/1840 train_time:2598ms step_avg:33.74ms
step:78/1840 train_time:2632ms step_avg:33.75ms
step:79/1840 train_time:2665ms step_avg:33.73ms
step:80/1840 train_time:2699ms step_avg:33.74ms
step:81/1840 train_time:2731ms step_avg:33.72ms
step:82/1840 train_time:2766ms step_avg:33.73ms
step:83/1840 train_time:2798ms step_avg:33.71ms
step:84/1840 train_time:2832ms step_avg:33.71ms
step:85/1840 train_time:2864ms step_avg:33.69ms
step:86/1840 train_time:2898ms step_avg:33.70ms
step:87/1840 train_time:2930ms step_avg:33.68ms
step:88/1840 train_time:2965ms step_avg:33.69ms
step:89/1840 train_time:2997ms step_avg:33.67ms
step:90/1840 train_time:3031ms step_avg:33.67ms
step:91/1840 train_time:3063ms step_avg:33.66ms
step:92/1840 train_time:3098ms step_avg:33.67ms
step:93/1840 train_time:3130ms step_avg:33.65ms
step:94/1840 train_time:3164ms step_avg:33.66ms
step:95/1840 train_time:3196ms step_avg:33.64ms
step:96/1840 train_time:3230ms step_avg:33.65ms
step:97/1840 train_time:3263ms step_avg:33.64ms
step:98/1840 train_time:3297ms step_avg:33.64ms
step:99/1840 train_time:3329ms step_avg:33.63ms
step:100/1840 train_time:3364ms step_avg:33.64ms
step:101/1840 train_time:3396ms step_avg:33.63ms
step:102/1840 train_time:3431ms step_avg:33.63ms
step:103/1840 train_time:3463ms step_avg:33.62ms
step:104/1840 train_time:3497ms step_avg:33.62ms
step:105/1840 train_time:3529ms step_avg:33.61ms
step:106/1840 train_time:3564ms step_avg:33.62ms
step:107/1840 train_time:3596ms step_avg:33.61ms
step:108/1840 train_time:3630ms step_avg:33.61ms
step:109/1840 train_time:3662ms step_avg:33.60ms
step:110/1840 train_time:3696ms step_avg:33.60ms
step:111/1840 train_time:3728ms step_avg:33.59ms
step:112/1840 train_time:3763ms step_avg:33.59ms
step:113/1840 train_time:3795ms step_avg:33.59ms
step:114/1840 train_time:3830ms step_avg:33.59ms
step:115/1840 train_time:3862ms step_avg:33.58ms
step:116/1840 train_time:3896ms step_avg:33.59ms
step:117/1840 train_time:3928ms step_avg:33.58ms
step:118/1840 train_time:3963ms step_avg:33.58ms
step:119/1840 train_time:3995ms step_avg:33.57ms
step:120/1840 train_time:4029ms step_avg:33.58ms
step:121/1840 train_time:4061ms step_avg:33.57ms
step:122/1840 train_time:4096ms step_avg:33.57ms
step:123/1840 train_time:4128ms step_avg:33.56ms
step:124/1840 train_time:4162ms step_avg:33.57ms
step:125/1840 train_time:4195ms step_avg:33.56ms
step:126/1840 train_time:4229ms step_avg:33.56ms
step:127/1840 train_time:4262ms step_avg:33.56ms
step:128/1840 train_time:4296ms step_avg:33.56ms
step:129/1840 train_time:4328ms step_avg:33.55ms
step:130/1840 train_time:4363ms step_avg:33.56ms
step:131/1840 train_time:4395ms step_avg:33.55ms
step:132/1840 train_time:4429ms step_avg:33.56ms
step:133/1840 train_time:4462ms step_avg:33.55ms
step:134/1840 train_time:4496ms step_avg:33.55ms
step:135/1840 train_time:4528ms step_avg:33.54ms
step:136/1840 train_time:4563ms step_avg:33.55ms
step:137/1840 train_time:4595ms step_avg:33.54ms
step:138/1840 train_time:4629ms step_avg:33.54ms
step:139/1840 train_time:4661ms step_avg:33.53ms
step:140/1840 train_time:4695ms step_avg:33.54ms
step:141/1840 train_time:4727ms step_avg:33.53ms
step:142/1840 train_time:4762ms step_avg:33.53ms
step:143/1840 train_time:4794ms step_avg:33.52ms
step:144/1840 train_time:4828ms step_avg:33.53ms
step:145/1840 train_time:4860ms step_avg:33.52ms
step:146/1840 train_time:4895ms step_avg:33.52ms
step:147/1840 train_time:4927ms step_avg:33.52ms
step:148/1840 train_time:4961ms step_avg:33.52ms
step:149/1840 train_time:4994ms step_avg:33.52ms
step:150/1840 train_time:5028ms step_avg:33.52ms
step:151/1840 train_time:5061ms step_avg:33.51ms
step:152/1840 train_time:5095ms step_avg:33.52ms
step:153/1840 train_time:5127ms step_avg:33.51ms
step:154/1840 train_time:5162ms step_avg:33.52ms
step:155/1840 train_time:5194ms step_avg:33.51ms
step:156/1840 train_time:5228ms step_avg:33.51ms
step:157/1840 train_time:5260ms step_avg:33.51ms
step:158/1840 train_time:5295ms step_avg:33.51ms
step:159/1840 train_time:5327ms step_avg:33.50ms
step:160/1840 train_time:5361ms step_avg:33.51ms
step:161/1840 train_time:5394ms step_avg:33.50ms
step:162/1840 train_time:5428ms step_avg:33.51ms
step:163/1840 train_time:5460ms step_avg:33.50ms
step:164/1840 train_time:5495ms step_avg:33.50ms
step:165/1840 train_time:5527ms step_avg:33.50ms
step:166/1840 train_time:5561ms step_avg:33.50ms
step:167/1840 train_time:5593ms step_avg:33.49ms
step:168/1840 train_time:5627ms step_avg:33.50ms
step:169/1840 train_time:5660ms step_avg:33.49ms
step:170/1840 train_time:5694ms step_avg:33.50ms
step:171/1840 train_time:5726ms step_avg:33.49ms
step:172/1840 train_time:5761ms step_avg:33.49ms
step:173/1840 train_time:5793ms step_avg:33.48ms
step:174/1840 train_time:5827ms step_avg:33.49ms
step:175/1840 train_time:5860ms step_avg:33.48ms
step:176/1840 train_time:5894ms step_avg:33.49ms
step:177/1840 train_time:5926ms step_avg:33.48ms
step:178/1840 train_time:5960ms step_avg:33.48ms
step:179/1840 train_time:5992ms step_avg:33.48ms
step:180/1840 train_time:6027ms step_avg:33.48ms
step:181/1840 train_time:6059ms step_avg:33.47ms
step:182/1840 train_time:6093ms step_avg:33.48ms
step:183/1840 train_time:6126ms step_avg:33.47ms
step:184/1840 train_time:6160ms step_avg:33.48ms
step:185/1840 train_time:6191ms step_avg:33.47ms
step:186/1840 train_time:6226ms step_avg:33.47ms
step:187/1840 train_time:6258ms step_avg:33.46ms
step:188/1840 train_time:6292ms step_avg:33.47ms
step:189/1840 train_time:6324ms step_avg:33.46ms
step:190/1840 train_time:6358ms step_avg:33.46ms
step:191/1840 train_time:6390ms step_avg:33.46ms
step:192/1840 train_time:6425ms step_avg:33.46ms
step:193/1840 train_time:6457ms step_avg:33.45ms
step:194/1840 train_time:6491ms step_avg:33.46ms
step:195/1840 train_time:6524ms step_avg:33.45ms
step:196/1840 train_time:6558ms step_avg:33.46ms
step:197/1840 train_time:6590ms step_avg:33.45ms
step:198/1840 train_time:6624ms step_avg:33.46ms
step:199/1840 train_time:6656ms step_avg:33.45ms
step:200/1840 train_time:6691ms step_avg:33.45ms
step:201/1840 train_time:6723ms step_avg:33.45ms
step:202/1840 train_time:6757ms step_avg:33.45ms
step:203/1840 train_time:6789ms step_avg:33.45ms
step:204/1840 train_time:6824ms step_avg:33.45ms
step:205/1840 train_time:6856ms step_avg:33.44ms
step:206/1840 train_time:6890ms step_avg:33.44ms
step:207/1840 train_time:6921ms step_avg:33.44ms
step:208/1840 train_time:6956ms step_avg:33.44ms
step:209/1840 train_time:6988ms step_avg:33.43ms
step:210/1840 train_time:7022ms step_avg:33.44ms
step:211/1840 train_time:7055ms step_avg:33.43ms
step:212/1840 train_time:7089ms step_avg:33.44ms
step:213/1840 train_time:7121ms step_avg:33.43ms
step:214/1840 train_time:7155ms step_avg:33.44ms
step:215/1840 train_time:7187ms step_avg:33.43ms
step:216/1840 train_time:7221ms step_avg:33.43ms
step:217/1840 train_time:7254ms step_avg:33.43ms
step:218/1840 train_time:7288ms step_avg:33.43ms
step:219/1840 train_time:7320ms step_avg:33.43ms
step:220/1840 train_time:7355ms step_avg:33.43ms
step:221/1840 train_time:7387ms step_avg:33.42ms
step:222/1840 train_time:7421ms step_avg:33.43ms
step:223/1840 train_time:7453ms step_avg:33.42ms
step:224/1840 train_time:7487ms step_avg:33.43ms
step:225/1840 train_time:7519ms step_avg:33.42ms
step:226/1840 train_time:7553ms step_avg:33.42ms
step:227/1840 train_time:7586ms step_avg:33.42ms
step:228/1840 train_time:7620ms step_avg:33.42ms
step:229/1840 train_time:7652ms step_avg:33.42ms
step:230/1840 train_time:7686ms step_avg:33.42ms
step:231/1840 train_time:7718ms step_avg:33.41ms
step:232/1840 train_time:7753ms step_avg:33.42ms
step:233/1840 train_time:7785ms step_avg:33.41ms
step:234/1840 train_time:7819ms step_avg:33.42ms
step:235/1840 train_time:7851ms step_avg:33.41ms
step:236/1840 train_time:7886ms step_avg:33.41ms
step:237/1840 train_time:7917ms step_avg:33.41ms
step:238/1840 train_time:7952ms step_avg:33.41ms
step:239/1840 train_time:7983ms step_avg:33.40ms
step:240/1840 train_time:8018ms step_avg:33.41ms
step:241/1840 train_time:8050ms step_avg:33.40ms
step:242/1840 train_time:8084ms step_avg:33.40ms
step:243/1840 train_time:8116ms step_avg:33.40ms
step:244/1840 train_time:8151ms step_avg:33.40ms
step:245/1840 train_time:8183ms step_avg:33.40ms
step:246/1840 train_time:8217ms step_avg:33.40ms
step:247/1840 train_time:8249ms step_avg:33.40ms
step:248/1840 train_time:8283ms step_avg:33.40ms
step:249/1840 train_time:8315ms step_avg:33.39ms
step:250/1840 train_time:8350ms step_avg:33.40ms
step:250/1840 val_loss:4.6176 train_time:8392ms step_avg:33.57ms
step:251/1840 train_time:8410ms step_avg:33.51ms
step:252/1840 train_time:8428ms step_avg:33.45ms
step:253/1840 train_time:8449ms step_avg:33.40ms
step:254/1840 train_time:8484ms step_avg:33.40ms
step:255/1840 train_time:8516ms step_avg:33.40ms
step:256/1840 train_time:8551ms step_avg:33.40ms
step:257/1840 train_time:8583ms step_avg:33.40ms
step:258/1840 train_time:8617ms step_avg:33.40ms
step:259/1840 train_time:8649ms step_avg:33.39ms
step:260/1840 train_time:8683ms step_avg:33.40ms
step:261/1840 train_time:8715ms step_avg:33.39ms
step:262/1840 train_time:8749ms step_avg:33.39ms
step:263/1840 train_time:8781ms step_avg:33.39ms
step:264/1840 train_time:8815ms step_avg:33.39ms
step:265/1840 train_time:8847ms step_avg:33.39ms
step:266/1840 train_time:8881ms step_avg:33.39ms
step:267/1840 train_time:8913ms step_avg:33.38ms
step:268/1840 train_time:8947ms step_avg:33.39ms
step:269/1840 train_time:8979ms step_avg:33.38ms
step:270/1840 train_time:9013ms step_avg:33.38ms
step:271/1840 train_time:9045ms step_avg:33.38ms
step:272/1840 train_time:9079ms step_avg:33.38ms
step:273/1840 train_time:9111ms step_avg:33.37ms
step:274/1840 train_time:9145ms step_avg:33.38ms
step:275/1840 train_time:9177ms step_avg:33.37ms
step:276/1840 train_time:9211ms step_avg:33.37ms
step:277/1840 train_time:9243ms step_avg:33.37ms
step:278/1840 train_time:9277ms step_avg:33.37ms
step:279/1840 train_time:9309ms step_avg:33.37ms
step:280/1840 train_time:9344ms step_avg:33.37ms
step:281/1840 train_time:9377ms step_avg:33.37ms
step:282/1840 train_time:9412ms step_avg:33.37ms
step:283/1840 train_time:9444ms step_avg:33.37ms
step:284/1840 train_time:9478ms step_avg:33.37ms
step:285/1840 train_time:9510ms step_avg:33.37ms
step:286/1840 train_time:9545ms step_avg:33.37ms
step:287/1840 train_time:9577ms step_avg:33.37ms
step:288/1840 train_time:9611ms step_avg:33.37ms
step:289/1840 train_time:9643ms step_avg:33.37ms
step:290/1840 train_time:9677ms step_avg:33.37ms
step:291/1840 train_time:9709ms step_avg:33.37ms
step:292/1840 train_time:9743ms step_avg:33.37ms
step:293/1840 train_time:9776ms step_avg:33.36ms
step:294/1840 train_time:9810ms step_avg:33.37ms
step:295/1840 train_time:9842ms step_avg:33.36ms
step:296/1840 train_time:9876ms step_avg:33.36ms
step:297/1840 train_time:9908ms step_avg:33.36ms
step:298/1840 train_time:9942ms step_avg:33.36ms
step:299/1840 train_time:9974ms step_avg:33.36ms
step:300/1840 train_time:10008ms step_avg:33.36ms
step:301/1840 train_time:10040ms step_avg:33.36ms
step:302/1840 train_time:10074ms step_avg:33.36ms
step:303/1840 train_time:10106ms step_avg:33.35ms
step:304/1840 train_time:10141ms step_avg:33.36ms
step:305/1840 train_time:10172ms step_avg:33.35ms
step:306/1840 train_time:10207ms step_avg:33.35ms
step:307/1840 train_time:10239ms step_avg:33.35ms
step:308/1840 train_time:10273ms step_avg:33.35ms
step:309/1840 train_time:10305ms step_avg:33.35ms
step:310/1840 train_time:10339ms step_avg:33.35ms
step:311/1840 train_time:10371ms step_avg:33.35ms
step:312/1840 train_time:10406ms step_avg:33.35ms
step:313/1840 train_time:10438ms step_avg:33.35ms
step:314/1840 train_time:10473ms step_avg:33.35ms
step:315/1840 train_time:10505ms step_avg:33.35ms
step:316/1840 train_time:10539ms step_avg:33.35ms
step:317/1840 train_time:10572ms step_avg:33.35ms
step:318/1840 train_time:10606ms step_avg:33.35ms
step:319/1840 train_time:10639ms step_avg:33.35ms
step:320/1840 train_time:10673ms step_avg:33.35ms
step:321/1840 train_time:10705ms step_avg:33.35ms
step:322/1840 train_time:10739ms step_avg:33.35ms
step:323/1840 train_time:10771ms step_avg:33.35ms
step:324/1840 train_time:10805ms step_avg:33.35ms
step:325/1840 train_time:10838ms step_avg:33.35ms
step:326/1840 train_time:10872ms step_avg:33.35ms
step:327/1840 train_time:10904ms step_avg:33.35ms
step:328/1840 train_time:10938ms step_avg:33.35ms
step:329/1840 train_time:10970ms step_avg:33.34ms
step:330/1840 train_time:11004ms step_avg:33.35ms
step:331/1840 train_time:11036ms step_avg:33.34ms
step:332/1840 train_time:11071ms step_avg:33.35ms
step:333/1840 train_time:11103ms step_avg:33.34ms
step:334/1840 train_time:11137ms step_avg:33.34ms
step:335/1840 train_time:11169ms step_avg:33.34ms
step:336/1840 train_time:11203ms step_avg:33.34ms
step:337/1840 train_time:11235ms step_avg:33.34ms
step:338/1840 train_time:11269ms step_avg:33.34ms
step:339/1840 train_time:11301ms step_avg:33.34ms
step:340/1840 train_time:11335ms step_avg:33.34ms
step:341/1840 train_time:11368ms step_avg:33.34ms
step:342/1840 train_time:11402ms step_avg:33.34ms
step:343/1840 train_time:11434ms step_avg:33.34ms
step:344/1840 train_time:11468ms step_avg:33.34ms
step:345/1840 train_time:11500ms step_avg:33.33ms
step:346/1840 train_time:11535ms step_avg:33.34ms
step:347/1840 train_time:11567ms step_avg:33.33ms
step:348/1840 train_time:11602ms step_avg:33.34ms
step:349/1840 train_time:11634ms step_avg:33.33ms
step:350/1840 train_time:11668ms step_avg:33.34ms
step:351/1840 train_time:11700ms step_avg:33.33ms
step:352/1840 train_time:11734ms step_avg:33.34ms
step:353/1840 train_time:11767ms step_avg:33.33ms
step:354/1840 train_time:11801ms step_avg:33.34ms
step:355/1840 train_time:11833ms step_avg:33.33ms
step:356/1840 train_time:11867ms step_avg:33.34ms
step:357/1840 train_time:11900ms step_avg:33.33ms
step:358/1840 train_time:11934ms step_avg:33.33ms
step:359/1840 train_time:11966ms step_avg:33.33ms
step:360/1840 train_time:12000ms step_avg:33.33ms
step:361/1840 train_time:12032ms step_avg:33.33ms
step:362/1840 train_time:12067ms step_avg:33.33ms
step:363/1840 train_time:12099ms step_avg:33.33ms
step:364/1840 train_time:12133ms step_avg:33.33ms
step:365/1840 train_time:12165ms step_avg:33.33ms
step:366/1840 train_time:12200ms step_avg:33.33ms
step:367/1840 train_time:12232ms step_avg:33.33ms
step:368/1840 train_time:12266ms step_avg:33.33ms
step:369/1840 train_time:12298ms step_avg:33.33ms
step:370/1840 train_time:12332ms step_avg:33.33ms
step:371/1840 train_time:12364ms step_avg:33.33ms
step:372/1840 train_time:12398ms step_avg:33.33ms
step:373/1840 train_time:12430ms step_avg:33.33ms
step:374/1840 train_time:12465ms step_avg:33.33ms
step:375/1840 train_time:12497ms step_avg:33.33ms
step:376/1840 train_time:12531ms step_avg:33.33ms
step:377/1840 train_time:12563ms step_avg:33.32ms
step:378/1840 train_time:12598ms step_avg:33.33ms
step:379/1840 train_time:12629ms step_avg:33.32ms
step:380/1840 train_time:12664ms step_avg:33.33ms
step:381/1840 train_time:12696ms step_avg:33.32ms
step:382/1840 train_time:12730ms step_avg:33.32ms
step:383/1840 train_time:12762ms step_avg:33.32ms
step:384/1840 train_time:12796ms step_avg:33.32ms
step:385/1840 train_time:12828ms step_avg:33.32ms
step:386/1840 train_time:12862ms step_avg:33.32ms
step:387/1840 train_time:12894ms step_avg:33.32ms
step:388/1840 train_time:12928ms step_avg:33.32ms
step:389/1840 train_time:12961ms step_avg:33.32ms
step:390/1840 train_time:12995ms step_avg:33.32ms
step:391/1840 train_time:13027ms step_avg:33.32ms
step:392/1840 train_time:13061ms step_avg:33.32ms
step:393/1840 train_time:13093ms step_avg:33.32ms
step:394/1840 train_time:13128ms step_avg:33.32ms
step:395/1840 train_time:13160ms step_avg:33.32ms
step:396/1840 train_time:13194ms step_avg:33.32ms
step:397/1840 train_time:13226ms step_avg:33.31ms
step:398/1840 train_time:13260ms step_avg:33.32ms
step:399/1840 train_time:13292ms step_avg:33.31ms
step:400/1840 train_time:13327ms step_avg:33.32ms
step:401/1840 train_time:13359ms step_avg:33.31ms
step:402/1840 train_time:13393ms step_avg:33.32ms
step:403/1840 train_time:13425ms step_avg:33.31ms
step:404/1840 train_time:13460ms step_avg:33.32ms
step:405/1840 train_time:13492ms step_avg:33.31ms
step:406/1840 train_time:13526ms step_avg:33.32ms
step:407/1840 train_time:13558ms step_avg:33.31ms
step:408/1840 train_time:13593ms step_avg:33.32ms
step:409/1840 train_time:13625ms step_avg:33.31ms
step:410/1840 train_time:13659ms step_avg:33.32ms
step:411/1840 train_time:13691ms step_avg:33.31ms
step:412/1840 train_time:13726ms step_avg:33.31ms
step:413/1840 train_time:13758ms step_avg:33.31ms
step:414/1840 train_time:13792ms step_avg:33.31ms
step:415/1840 train_time:13825ms step_avg:33.31ms
step:416/1840 train_time:13859ms step_avg:33.31ms
step:417/1840 train_time:13891ms step_avg:33.31ms
step:418/1840 train_time:13925ms step_avg:33.31ms
step:419/1840 train_time:13958ms step_avg:33.31ms
step:420/1840 train_time:13992ms step_avg:33.31ms
step:421/1840 train_time:14023ms step_avg:33.31ms
step:422/1840 train_time:14058ms step_avg:33.31ms
step:423/1840 train_time:14090ms step_avg:33.31ms
step:424/1840 train_time:14124ms step_avg:33.31ms
step:425/1840 train_time:14156ms step_avg:33.31ms
step:426/1840 train_time:14190ms step_avg:33.31ms
step:427/1840 train_time:14222ms step_avg:33.31ms
step:428/1840 train_time:14257ms step_avg:33.31ms
step:429/1840 train_time:14289ms step_avg:33.31ms
step:430/1840 train_time:14323ms step_avg:33.31ms
step:431/1840 train_time:14355ms step_avg:33.31ms
step:432/1840 train_time:14389ms step_avg:33.31ms
step:433/1840 train_time:14421ms step_avg:33.31ms
step:434/1840 train_time:14455ms step_avg:33.31ms
step:435/1840 train_time:14487ms step_avg:33.30ms
step:436/1840 train_time:14522ms step_avg:33.31ms
step:437/1840 train_time:14554ms step_avg:33.30ms
step:438/1840 train_time:14589ms step_avg:33.31ms
step:439/1840 train_time:14621ms step_avg:33.30ms
step:440/1840 train_time:14655ms step_avg:33.31ms
step:441/1840 train_time:14687ms step_avg:33.30ms
step:442/1840 train_time:14722ms step_avg:33.31ms
step:443/1840 train_time:14754ms step_avg:33.30ms
step:444/1840 train_time:14788ms step_avg:33.31ms
step:445/1840 train_time:14820ms step_avg:33.30ms
step:446/1840 train_time:14854ms step_avg:33.31ms
step:447/1840 train_time:14886ms step_avg:33.30ms
step:448/1840 train_time:14920ms step_avg:33.30ms
step:449/1840 train_time:14953ms step_avg:33.30ms
step:450/1840 train_time:14987ms step_avg:33.30ms
step:451/1840 train_time:15019ms step_avg:33.30ms
step:452/1840 train_time:15054ms step_avg:33.30ms
step:453/1840 train_time:15086ms step_avg:33.30ms
step:454/1840 train_time:15120ms step_avg:33.30ms
step:455/1840 train_time:15152ms step_avg:33.30ms
step:456/1840 train_time:15187ms step_avg:33.30ms
step:457/1840 train_time:15219ms step_avg:33.30ms
step:458/1840 train_time:15253ms step_avg:33.30ms
step:459/1840 train_time:15285ms step_avg:33.30ms
step:460/1840 train_time:15320ms step_avg:33.30ms
step:461/1840 train_time:15352ms step_avg:33.30ms
step:462/1840 train_time:15386ms step_avg:33.30ms
step:463/1840 train_time:15418ms step_avg:33.30ms
step:464/1840 train_time:15452ms step_avg:33.30ms
step:465/1840 train_time:15484ms step_avg:33.30ms
step:466/1840 train_time:15518ms step_avg:33.30ms
step:467/1840 train_time:15550ms step_avg:33.30ms
step:468/1840 train_time:15584ms step_avg:33.30ms
step:469/1840 train_time:15616ms step_avg:33.30ms
step:470/1840 train_time:15651ms step_avg:33.30ms
step:471/1840 train_time:15683ms step_avg:33.30ms
step:472/1840 train_time:15718ms step_avg:33.30ms
step:473/1840 train_time:15750ms step_avg:33.30ms
step:474/1840 train_time:15784ms step_avg:33.30ms
step:475/1840 train_time:15816ms step_avg:33.30ms
step:476/1840 train_time:15850ms step_avg:33.30ms
step:477/1840 train_time:15882ms step_avg:33.30ms
step:478/1840 train_time:15917ms step_avg:33.30ms
step:479/1840 train_time:15949ms step_avg:33.30ms
step:480/1840 train_time:15983ms step_avg:33.30ms
step:481/1840 train_time:16015ms step_avg:33.30ms
step:482/1840 train_time:16050ms step_avg:33.30ms
step:483/1840 train_time:16081ms step_avg:33.29ms
step:484/1840 train_time:16116ms step_avg:33.30ms
step:485/1840 train_time:16148ms step_avg:33.30ms
step:486/1840 train_time:16182ms step_avg:33.30ms
step:487/1840 train_time:16214ms step_avg:33.29ms
step:488/1840 train_time:16249ms step_avg:33.30ms
step:489/1840 train_time:16281ms step_avg:33.29ms
step:490/1840 train_time:16315ms step_avg:33.30ms
step:491/1840 train_time:16347ms step_avg:33.29ms
step:492/1840 train_time:16381ms step_avg:33.30ms
step:493/1840 train_time:16413ms step_avg:33.29ms
step:494/1840 train_time:16448ms step_avg:33.30ms
step:495/1840 train_time:16480ms step_avg:33.29ms
step:496/1840 train_time:16514ms step_avg:33.30ms
step:497/1840 train_time:16547ms step_avg:33.29ms
step:498/1840 train_time:16581ms step_avg:33.30ms
step:499/1840 train_time:16613ms step_avg:33.29ms
step:500/1840 train_time:16648ms step_avg:33.30ms
step:500/1840 val_loss:4.2953 train_time:16690ms step_avg:33.38ms
step:501/1840 train_time:16709ms step_avg:33.35ms
step:502/1840 train_time:16727ms step_avg:33.32ms
step:503/1840 train_time:16750ms step_avg:33.30ms
step:504/1840 train_time:16784ms step_avg:33.30ms
step:505/1840 train_time:16816ms step_avg:33.30ms
step:506/1840 train_time:16851ms step_avg:33.30ms
step:507/1840 train_time:16882ms step_avg:33.30ms
step:508/1840 train_time:16916ms step_avg:33.30ms
step:509/1840 train_time:16948ms step_avg:33.30ms
step:510/1840 train_time:16982ms step_avg:33.30ms
step:511/1840 train_time:17015ms step_avg:33.30ms
step:512/1840 train_time:17049ms step_avg:33.30ms
step:513/1840 train_time:17081ms step_avg:33.30ms
step:514/1840 train_time:17115ms step_avg:33.30ms
step:515/1840 train_time:17146ms step_avg:33.29ms
step:516/1840 train_time:17180ms step_avg:33.30ms
step:517/1840 train_time:17212ms step_avg:33.29ms
step:518/1840 train_time:17247ms step_avg:33.29ms
step:519/1840 train_time:17278ms step_avg:33.29ms
step:520/1840 train_time:17312ms step_avg:33.29ms
step:521/1840 train_time:17344ms step_avg:33.29ms
step:522/1840 train_time:17378ms step_avg:33.29ms
step:523/1840 train_time:17410ms step_avg:33.29ms
step:524/1840 train_time:17444ms step_avg:33.29ms
step:525/1840 train_time:17476ms step_avg:33.29ms
step:526/1840 train_time:17510ms step_avg:33.29ms
step:527/1840 train_time:17543ms step_avg:33.29ms
step:528/1840 train_time:17577ms step_avg:33.29ms
step:529/1840 train_time:17609ms step_avg:33.29ms
step:530/1840 train_time:17644ms step_avg:33.29ms
step:531/1840 train_time:17677ms step_avg:33.29ms
step:532/1840 train_time:17712ms step_avg:33.29ms
step:533/1840 train_time:17744ms step_avg:33.29ms
step:534/1840 train_time:17778ms step_avg:33.29ms
step:535/1840 train_time:17811ms step_avg:33.29ms
step:536/1840 train_time:17845ms step_avg:33.29ms
step:537/1840 train_time:17878ms step_avg:33.29ms
step:538/1840 train_time:17912ms step_avg:33.29ms
step:539/1840 train_time:17944ms step_avg:33.29ms
step:540/1840 train_time:17978ms step_avg:33.29ms
step:541/1840 train_time:18010ms step_avg:33.29ms
step:542/1840 train_time:18044ms step_avg:33.29ms
step:543/1840 train_time:18076ms step_avg:33.29ms
step:544/1840 train_time:18110ms step_avg:33.29ms
step:545/1840 train_time:18142ms step_avg:33.29ms
step:546/1840 train_time:18176ms step_avg:33.29ms
step:547/1840 train_time:18208ms step_avg:33.29ms
step:548/1840 train_time:18243ms step_avg:33.29ms
step:549/1840 train_time:18275ms step_avg:33.29ms
step:550/1840 train_time:18309ms step_avg:33.29ms
step:551/1840 train_time:18341ms step_avg:33.29ms
step:552/1840 train_time:18375ms step_avg:33.29ms
step:553/1840 train_time:18407ms step_avg:33.29ms
step:554/1840 train_time:18442ms step_avg:33.29ms
step:555/1840 train_time:18474ms step_avg:33.29ms
step:556/1840 train_time:18508ms step_avg:33.29ms
step:557/1840 train_time:18540ms step_avg:33.29ms
step:558/1840 train_time:18574ms step_avg:33.29ms
step:559/1840 train_time:18607ms step_avg:33.29ms
step:560/1840 train_time:18641ms step_avg:33.29ms
step:561/1840 train_time:18673ms step_avg:33.29ms
step:562/1840 train_time:18708ms step_avg:33.29ms
step:563/1840 train_time:18740ms step_avg:33.29ms
step:564/1840 train_time:18774ms step_avg:33.29ms
step:565/1840 train_time:18806ms step_avg:33.29ms
step:566/1840 train_time:18841ms step_avg:33.29ms
step:567/1840 train_time:18873ms step_avg:33.29ms
step:568/1840 train_time:18907ms step_avg:33.29ms
step:569/1840 train_time:18940ms step_avg:33.29ms
step:570/1840 train_time:18974ms step_avg:33.29ms
step:571/1840 train_time:19006ms step_avg:33.29ms
step:572/1840 train_time:19040ms step_avg:33.29ms
step:573/1840 train_time:19072ms step_avg:33.28ms
step:574/1840 train_time:19106ms step_avg:33.29ms
step:575/1840 train_time:19138ms step_avg:33.28ms
step:576/1840 train_time:19172ms step_avg:33.29ms
step:577/1840 train_time:19204ms step_avg:33.28ms
step:578/1840 train_time:19238ms step_avg:33.28ms
step:579/1840 train_time:19270ms step_avg:33.28ms
step:580/1840 train_time:19305ms step_avg:33.28ms
step:581/1840 train_time:19337ms step_avg:33.28ms
step:582/1840 train_time:19371ms step_avg:33.28ms
step:583/1840 train_time:19403ms step_avg:33.28ms
step:584/1840 train_time:19437ms step_avg:33.28ms
step:585/1840 train_time:19469ms step_avg:33.28ms
step:586/1840 train_time:19503ms step_avg:33.28ms
step:587/1840 train_time:19535ms step_avg:33.28ms
step:588/1840 train_time:19570ms step_avg:33.28ms
step:589/1840 train_time:19602ms step_avg:33.28ms
step:590/1840 train_time:19637ms step_avg:33.28ms
step:591/1840 train_time:19669ms step_avg:33.28ms
step:592/1840 train_time:19704ms step_avg:33.28ms
step:593/1840 train_time:19736ms step_avg:33.28ms
step:594/1840 train_time:19770ms step_avg:33.28ms
step:595/1840 train_time:19803ms step_avg:33.28ms
step:596/1840 train_time:19837ms step_avg:33.28ms
step:597/1840 train_time:19869ms step_avg:33.28ms
step:598/1840 train_time:19904ms step_avg:33.28ms
step:599/1840 train_time:19936ms step_avg:33.28ms
step:600/1840 train_time:19971ms step_avg:33.28ms
step:601/1840 train_time:20005ms step_avg:33.29ms
step:602/1840 train_time:20063ms step_avg:33.33ms
step:603/1840 train_time:20123ms step_avg:33.37ms
step:604/1840 train_time:20185ms step_avg:33.42ms
step:605/1840 train_time:20244ms step_avg:33.46ms
step:606/1840 train_time:20307ms step_avg:33.51ms
step:607/1840 train_time:20366ms step_avg:33.55ms
step:608/1840 train_time:20429ms step_avg:33.60ms
step:609/1840 train_time:20489ms step_avg:33.64ms
step:610/1840 train_time:20551ms step_avg:33.69ms
step:611/1840 train_time:20610ms step_avg:33.73ms
step:612/1840 train_time:20672ms step_avg:33.78ms
step:613/1840 train_time:20733ms step_avg:33.82ms
step:614/1840 train_time:20794ms step_avg:33.87ms
step:615/1840 train_time:20854ms step_avg:33.91ms
step:616/1840 train_time:20916ms step_avg:33.95ms
step:617/1840 train_time:20975ms step_avg:34.00ms
step:618/1840 train_time:21038ms step_avg:34.04ms
step:619/1840 train_time:21097ms step_avg:34.08ms
step:620/1840 train_time:21160ms step_avg:34.13ms
step:621/1840 train_time:21219ms step_avg:34.17ms
step:622/1840 train_time:21280ms step_avg:34.21ms
step:623/1840 train_time:21340ms step_avg:34.25ms
step:624/1840 train_time:21402ms step_avg:34.30ms
step:625/1840 train_time:21462ms step_avg:34.34ms
step:626/1840 train_time:21525ms step_avg:34.38ms
step:627/1840 train_time:21585ms step_avg:34.43ms
step:628/1840 train_time:21648ms step_avg:34.47ms
step:629/1840 train_time:21709ms step_avg:34.51ms
step:630/1840 train_time:21772ms step_avg:34.56ms
step:631/1840 train_time:21832ms step_avg:34.60ms
step:632/1840 train_time:21894ms step_avg:34.64ms
step:633/1840 train_time:21954ms step_avg:34.68ms
step:634/1840 train_time:22016ms step_avg:34.73ms
step:635/1840 train_time:22076ms step_avg:34.77ms
step:636/1840 train_time:22138ms step_avg:34.81ms
step:637/1840 train_time:22198ms step_avg:34.85ms
step:638/1840 train_time:22259ms step_avg:34.89ms
step:639/1840 train_time:22318ms step_avg:34.93ms
step:640/1840 train_time:22380ms step_avg:34.97ms
step:641/1840 train_time:22440ms step_avg:35.01ms
step:642/1840 train_time:22502ms step_avg:35.05ms
step:643/1840 train_time:22562ms step_avg:35.09ms
step:644/1840 train_time:22625ms step_avg:35.13ms
step:645/1840 train_time:22685ms step_avg:35.17ms
step:646/1840 train_time:22748ms step_avg:35.21ms
step:647/1840 train_time:22809ms step_avg:35.25ms
step:648/1840 train_time:22872ms step_avg:35.30ms
step:649/1840 train_time:22932ms step_avg:35.33ms
step:650/1840 train_time:22995ms step_avg:35.38ms
step:651/1840 train_time:23055ms step_avg:35.41ms
step:652/1840 train_time:23117ms step_avg:35.46ms
step:653/1840 train_time:23176ms step_avg:35.49ms
step:654/1840 train_time:23238ms step_avg:35.53ms
step:655/1840 train_time:23297ms step_avg:35.57ms
step:656/1840 train_time:23360ms step_avg:35.61ms
step:657/1840 train_time:23419ms step_avg:35.65ms
step:658/1840 train_time:23482ms step_avg:35.69ms
step:659/1840 train_time:23542ms step_avg:35.72ms
step:660/1840 train_time:23605ms step_avg:35.77ms
step:661/1840 train_time:23665ms step_avg:35.80ms
step:662/1840 train_time:23728ms step_avg:35.84ms
step:663/1840 train_time:23789ms step_avg:35.88ms
step:664/1840 train_time:23852ms step_avg:35.92ms
step:665/1840 train_time:23913ms step_avg:35.96ms
step:666/1840 train_time:23975ms step_avg:36.00ms
step:667/1840 train_time:24034ms step_avg:36.03ms
step:668/1840 train_time:24096ms step_avg:36.07ms
step:669/1840 train_time:24156ms step_avg:36.11ms
step:670/1840 train_time:24219ms step_avg:36.15ms
step:671/1840 train_time:24278ms step_avg:36.18ms
step:672/1840 train_time:24340ms step_avg:36.22ms
step:673/1840 train_time:24399ms step_avg:36.25ms
step:674/1840 train_time:24461ms step_avg:36.29ms
step:675/1840 train_time:24520ms step_avg:36.33ms
step:676/1840 train_time:24583ms step_avg:36.37ms
step:677/1840 train_time:24643ms step_avg:36.40ms
step:678/1840 train_time:24706ms step_avg:36.44ms
step:679/1840 train_time:24766ms step_avg:36.47ms
step:680/1840 train_time:24829ms step_avg:36.51ms
step:681/1840 train_time:24889ms step_avg:36.55ms
step:682/1840 train_time:24952ms step_avg:36.59ms
step:683/1840 train_time:25012ms step_avg:36.62ms
step:684/1840 train_time:25074ms step_avg:36.66ms
step:685/1840 train_time:25134ms step_avg:36.69ms
step:686/1840 train_time:25196ms step_avg:36.73ms
step:687/1840 train_time:25255ms step_avg:36.76ms
step:688/1840 train_time:25318ms step_avg:36.80ms
step:689/1840 train_time:25377ms step_avg:36.83ms
step:690/1840 train_time:25439ms step_avg:36.87ms
step:691/1840 train_time:25499ms step_avg:36.90ms
step:692/1840 train_time:25561ms step_avg:36.94ms
step:693/1840 train_time:25620ms step_avg:36.97ms
step:694/1840 train_time:25683ms step_avg:37.01ms
step:695/1840 train_time:25744ms step_avg:37.04ms
step:696/1840 train_time:25806ms step_avg:37.08ms
step:697/1840 train_time:25867ms step_avg:37.11ms
step:698/1840 train_time:25931ms step_avg:37.15ms
step:699/1840 train_time:25991ms step_avg:37.18ms
step:700/1840 train_time:26053ms step_avg:37.22ms
step:701/1840 train_time:26113ms step_avg:37.25ms
step:702/1840 train_time:26176ms step_avg:37.29ms
step:703/1840 train_time:26236ms step_avg:37.32ms
step:704/1840 train_time:26297ms step_avg:37.35ms
step:705/1840 train_time:26356ms step_avg:37.38ms
step:706/1840 train_time:26418ms step_avg:37.42ms
step:707/1840 train_time:26478ms step_avg:37.45ms
step:708/1840 train_time:26540ms step_avg:37.49ms
step:709/1840 train_time:26599ms step_avg:37.52ms
step:710/1840 train_time:26662ms step_avg:37.55ms
step:711/1840 train_time:26722ms step_avg:37.58ms
step:712/1840 train_time:26785ms step_avg:37.62ms
step:713/1840 train_time:26845ms step_avg:37.65ms
step:714/1840 train_time:26907ms step_avg:37.69ms
step:715/1840 train_time:26967ms step_avg:37.72ms
step:716/1840 train_time:27031ms step_avg:37.75ms
step:717/1840 train_time:27092ms step_avg:37.79ms
step:718/1840 train_time:27154ms step_avg:37.82ms
step:719/1840 train_time:27214ms step_avg:37.85ms
step:720/1840 train_time:27276ms step_avg:37.88ms
step:721/1840 train_time:27336ms step_avg:37.91ms
step:722/1840 train_time:27398ms step_avg:37.95ms
step:723/1840 train_time:27458ms step_avg:37.98ms
step:724/1840 train_time:27519ms step_avg:38.01ms
step:725/1840 train_time:27578ms step_avg:38.04ms
step:726/1840 train_time:27641ms step_avg:38.07ms
step:727/1840 train_time:27700ms step_avg:38.10ms
step:728/1840 train_time:27764ms step_avg:38.14ms
step:729/1840 train_time:27824ms step_avg:38.17ms
step:730/1840 train_time:27887ms step_avg:38.20ms
step:731/1840 train_time:27947ms step_avg:38.23ms
step:732/1840 train_time:28011ms step_avg:38.27ms
step:733/1840 train_time:28071ms step_avg:38.30ms
step:734/1840 train_time:28133ms step_avg:38.33ms
step:735/1840 train_time:28193ms step_avg:38.36ms
step:736/1840 train_time:28254ms step_avg:38.39ms
step:737/1840 train_time:28315ms step_avg:38.42ms
step:738/1840 train_time:28377ms step_avg:38.45ms
step:739/1840 train_time:28436ms step_avg:38.48ms
step:740/1840 train_time:28498ms step_avg:38.51ms
step:741/1840 train_time:28558ms step_avg:38.54ms
step:742/1840 train_time:28621ms step_avg:38.57ms
step:743/1840 train_time:28680ms step_avg:38.60ms
step:744/1840 train_time:28743ms step_avg:38.63ms
step:745/1840 train_time:28802ms step_avg:38.66ms
step:746/1840 train_time:28865ms step_avg:38.69ms
step:747/1840 train_time:28925ms step_avg:38.72ms
step:748/1840 train_time:28988ms step_avg:38.75ms
step:749/1840 train_time:29048ms step_avg:38.78ms
step:750/1840 train_time:29111ms step_avg:38.81ms
step:750/1840 val_loss:4.0304 train_time:29182ms step_avg:38.91ms
step:751/1840 train_time:29200ms step_avg:38.88ms
step:752/1840 train_time:29232ms step_avg:38.87ms
step:753/1840 train_time:29295ms step_avg:38.90ms
step:754/1840 train_time:29358ms step_avg:38.94ms
step:755/1840 train_time:29417ms step_avg:38.96ms
step:756/1840 train_time:29479ms step_avg:38.99ms
step:757/1840 train_time:29539ms step_avg:39.02ms
step:758/1840 train_time:29601ms step_avg:39.05ms
step:759/1840 train_time:29661ms step_avg:39.08ms
step:760/1840 train_time:29723ms step_avg:39.11ms
step:761/1840 train_time:29783ms step_avg:39.14ms
step:762/1840 train_time:29845ms step_avg:39.17ms
step:763/1840 train_time:29905ms step_avg:39.19ms
step:764/1840 train_time:29967ms step_avg:39.22ms
step:765/1840 train_time:30026ms step_avg:39.25ms
step:766/1840 train_time:30088ms step_avg:39.28ms
step:767/1840 train_time:30150ms step_avg:39.31ms
step:768/1840 train_time:30213ms step_avg:39.34ms
step:769/1840 train_time:30274ms step_avg:39.37ms
step:770/1840 train_time:30336ms step_avg:39.40ms
step:771/1840 train_time:30396ms step_avg:39.42ms
step:772/1840 train_time:30457ms step_avg:39.45ms
step:773/1840 train_time:30517ms step_avg:39.48ms
step:774/1840 train_time:30579ms step_avg:39.51ms
step:775/1840 train_time:30638ms step_avg:39.53ms
step:776/1840 train_time:30701ms step_avg:39.56ms
step:777/1840 train_time:30761ms step_avg:39.59ms
step:778/1840 train_time:30824ms step_avg:39.62ms
step:779/1840 train_time:30883ms step_avg:39.64ms
step:780/1840 train_time:30946ms step_avg:39.67ms
step:781/1840 train_time:31005ms step_avg:39.70ms
step:782/1840 train_time:31068ms step_avg:39.73ms
step:783/1840 train_time:31129ms step_avg:39.76ms
step:784/1840 train_time:31192ms step_avg:39.79ms
step:785/1840 train_time:31251ms step_avg:39.81ms
step:786/1840 train_time:31313ms step_avg:39.84ms
step:787/1840 train_time:31373ms step_avg:39.86ms
step:788/1840 train_time:31435ms step_avg:39.89ms
step:789/1840 train_time:31495ms step_avg:39.92ms
step:790/1840 train_time:31557ms step_avg:39.95ms
step:791/1840 train_time:31616ms step_avg:39.97ms
step:792/1840 train_time:31679ms step_avg:40.00ms
step:793/1840 train_time:31738ms step_avg:40.02ms
step:794/1840 train_time:31800ms step_avg:40.05ms
step:795/1840 train_time:31860ms step_avg:40.08ms
step:796/1840 train_time:31921ms step_avg:40.10ms
step:797/1840 train_time:31982ms step_avg:40.13ms
step:798/1840 train_time:32044ms step_avg:40.16ms
step:799/1840 train_time:32104ms step_avg:40.18ms
step:800/1840 train_time:32169ms step_avg:40.21ms
step:801/1840 train_time:32229ms step_avg:40.24ms
step:802/1840 train_time:32291ms step_avg:40.26ms
step:803/1840 train_time:32352ms step_avg:40.29ms
step:804/1840 train_time:32414ms step_avg:40.32ms
step:805/1840 train_time:32473ms step_avg:40.34ms
step:806/1840 train_time:32535ms step_avg:40.37ms
step:807/1840 train_time:32596ms step_avg:40.39ms
step:808/1840 train_time:32658ms step_avg:40.42ms
step:809/1840 train_time:32717ms step_avg:40.44ms
step:810/1840 train_time:32779ms step_avg:40.47ms
step:811/1840 train_time:32838ms step_avg:40.49ms
step:812/1840 train_time:32900ms step_avg:40.52ms
step:813/1840 train_time:32960ms step_avg:40.54ms
step:814/1840 train_time:33022ms step_avg:40.57ms
step:815/1840 train_time:33084ms step_avg:40.59ms
step:816/1840 train_time:33147ms step_avg:40.62ms
step:817/1840 train_time:33207ms step_avg:40.64ms
step:818/1840 train_time:33270ms step_avg:40.67ms
step:819/1840 train_time:33331ms step_avg:40.70ms
step:820/1840 train_time:33392ms step_avg:40.72ms
step:821/1840 train_time:33453ms step_avg:40.75ms
step:822/1840 train_time:33514ms step_avg:40.77ms
step:823/1840 train_time:33575ms step_avg:40.80ms
step:824/1840 train_time:33637ms step_avg:40.82ms
step:825/1840 train_time:33697ms step_avg:40.84ms
step:826/1840 train_time:33758ms step_avg:40.87ms
step:827/1840 train_time:33818ms step_avg:40.89ms
step:828/1840 train_time:33880ms step_avg:40.92ms
step:829/1840 train_time:33939ms step_avg:40.94ms
step:830/1840 train_time:34002ms step_avg:40.97ms
step:831/1840 train_time:34062ms step_avg:40.99ms
step:832/1840 train_time:34124ms step_avg:41.01ms
step:833/1840 train_time:34185ms step_avg:41.04ms
step:834/1840 train_time:34250ms step_avg:41.07ms
step:835/1840 train_time:34310ms step_avg:41.09ms
step:836/1840 train_time:34372ms step_avg:41.12ms
step:837/1840 train_time:34432ms step_avg:41.14ms
step:838/1840 train_time:34495ms step_avg:41.16ms
step:839/1840 train_time:34555ms step_avg:41.19ms
step:840/1840 train_time:34617ms step_avg:41.21ms
step:841/1840 train_time:34676ms step_avg:41.23ms
step:842/1840 train_time:34737ms step_avg:41.26ms
step:843/1840 train_time:34796ms step_avg:41.28ms
step:844/1840 train_time:34859ms step_avg:41.30ms
step:845/1840 train_time:34919ms step_avg:41.32ms
step:846/1840 train_time:34982ms step_avg:41.35ms
step:847/1840 train_time:35041ms step_avg:41.37ms
step:848/1840 train_time:35103ms step_avg:41.40ms
step:849/1840 train_time:35163ms step_avg:41.42ms
step:850/1840 train_time:35227ms step_avg:41.44ms
step:851/1840 train_time:35287ms step_avg:41.47ms
step:852/1840 train_time:35351ms step_avg:41.49ms
step:853/1840 train_time:35411ms step_avg:41.51ms
step:854/1840 train_time:35473ms step_avg:41.54ms
step:855/1840 train_time:35532ms step_avg:41.56ms
step:856/1840 train_time:35594ms step_avg:41.58ms
step:857/1840 train_time:35654ms step_avg:41.60ms
step:858/1840 train_time:35717ms step_avg:41.63ms
step:859/1840 train_time:35776ms step_avg:41.65ms
step:860/1840 train_time:35838ms step_avg:41.67ms
step:861/1840 train_time:35898ms step_avg:41.69ms
step:862/1840 train_time:35960ms step_avg:41.72ms
step:863/1840 train_time:36020ms step_avg:41.74ms
step:864/1840 train_time:36082ms step_avg:41.76ms
step:865/1840 train_time:36142ms step_avg:41.78ms
step:866/1840 train_time:36204ms step_avg:41.81ms
step:867/1840 train_time:36266ms step_avg:41.83ms
step:868/1840 train_time:36329ms step_avg:41.85ms
step:869/1840 train_time:36390ms step_avg:41.88ms
step:870/1840 train_time:36453ms step_avg:41.90ms
step:871/1840 train_time:36512ms step_avg:41.92ms
step:872/1840 train_time:36574ms step_avg:41.94ms
step:873/1840 train_time:36634ms step_avg:41.96ms
step:874/1840 train_time:36696ms step_avg:41.99ms
step:875/1840 train_time:36755ms step_avg:42.01ms
step:876/1840 train_time:36817ms step_avg:42.03ms
step:877/1840 train_time:36876ms step_avg:42.05ms
step:878/1840 train_time:36938ms step_avg:42.07ms
step:879/1840 train_time:36998ms step_avg:42.09ms
step:880/1840 train_time:37060ms step_avg:42.11ms
step:881/1840 train_time:37119ms step_avg:42.13ms
step:882/1840 train_time:37183ms step_avg:42.16ms
step:883/1840 train_time:37243ms step_avg:42.18ms
step:884/1840 train_time:37307ms step_avg:42.20ms
step:885/1840 train_time:37368ms step_avg:42.22ms
step:886/1840 train_time:37431ms step_avg:42.25ms
step:887/1840 train_time:37490ms step_avg:42.27ms
step:888/1840 train_time:37553ms step_avg:42.29ms
step:889/1840 train_time:37612ms step_avg:42.31ms
step:890/1840 train_time:37675ms step_avg:42.33ms
step:891/1840 train_time:37734ms step_avg:42.35ms
step:892/1840 train_time:37796ms step_avg:42.37ms
step:893/1840 train_time:37856ms step_avg:42.39ms
step:894/1840 train_time:37918ms step_avg:42.41ms
step:895/1840 train_time:37978ms step_avg:42.43ms
step:896/1840 train_time:38040ms step_avg:42.46ms
step:897/1840 train_time:38100ms step_avg:42.47ms
step:898/1840 train_time:38162ms step_avg:42.50ms
step:899/1840 train_time:38223ms step_avg:42.52ms
step:900/1840 train_time:38286ms step_avg:42.54ms
step:901/1840 train_time:38347ms step_avg:42.56ms
step:902/1840 train_time:38409ms step_avg:42.58ms
step:903/1840 train_time:38470ms step_avg:42.60ms
step:904/1840 train_time:38531ms step_avg:42.62ms
step:905/1840 train_time:38591ms step_avg:42.64ms
step:906/1840 train_time:38654ms step_avg:42.66ms
step:907/1840 train_time:38714ms step_avg:42.68ms
step:908/1840 train_time:38776ms step_avg:42.71ms
step:909/1840 train_time:38835ms step_avg:42.72ms
step:910/1840 train_time:38898ms step_avg:42.74ms
step:911/1840 train_time:38956ms step_avg:42.76ms
step:912/1840 train_time:39018ms step_avg:42.78ms
step:913/1840 train_time:39078ms step_avg:42.80ms
step:914/1840 train_time:39140ms step_avg:42.82ms
step:915/1840 train_time:39200ms step_avg:42.84ms
step:916/1840 train_time:39263ms step_avg:42.86ms
step:917/1840 train_time:39323ms step_avg:42.88ms
step:918/1840 train_time:39386ms step_avg:42.90ms
step:919/1840 train_time:39448ms step_avg:42.92ms
step:920/1840 train_time:39511ms step_avg:42.95ms
step:921/1840 train_time:39570ms step_avg:42.96ms
step:922/1840 train_time:39633ms step_avg:42.99ms
step:923/1840 train_time:39693ms step_avg:43.00ms
step:924/1840 train_time:39755ms step_avg:43.02ms
step:925/1840 train_time:39814ms step_avg:43.04ms
step:926/1840 train_time:39877ms step_avg:43.06ms
step:927/1840 train_time:39936ms step_avg:43.08ms
step:928/1840 train_time:39998ms step_avg:43.10ms
step:929/1840 train_time:40057ms step_avg:43.12ms
step:930/1840 train_time:40119ms step_avg:43.14ms
step:931/1840 train_time:40179ms step_avg:43.16ms
step:932/1840 train_time:40242ms step_avg:43.18ms
step:933/1840 train_time:40302ms step_avg:43.20ms
step:934/1840 train_time:40365ms step_avg:43.22ms
step:935/1840 train_time:40426ms step_avg:43.24ms
step:936/1840 train_time:40490ms step_avg:43.26ms
step:937/1840 train_time:40549ms step_avg:43.28ms
step:938/1840 train_time:40612ms step_avg:43.30ms
step:939/1840 train_time:40672ms step_avg:43.31ms
step:940/1840 train_time:40734ms step_avg:43.33ms
step:941/1840 train_time:40794ms step_avg:43.35ms
step:942/1840 train_time:40857ms step_avg:43.37ms
step:943/1840 train_time:40916ms step_avg:43.39ms
step:944/1840 train_time:40977ms step_avg:43.41ms
step:945/1840 train_time:41037ms step_avg:43.42ms
step:946/1840 train_time:41099ms step_avg:43.45ms
step:947/1840 train_time:41159ms step_avg:43.46ms
step:948/1840 train_time:41221ms step_avg:43.48ms
step:949/1840 train_time:41282ms step_avg:43.50ms
step:950/1840 train_time:41344ms step_avg:43.52ms
step:951/1840 train_time:41405ms step_avg:43.54ms
step:952/1840 train_time:41467ms step_avg:43.56ms
step:953/1840 train_time:41527ms step_avg:43.57ms
step:954/1840 train_time:41590ms step_avg:43.60ms
step:955/1840 train_time:41650ms step_avg:43.61ms
step:956/1840 train_time:41711ms step_avg:43.63ms
step:957/1840 train_time:41772ms step_avg:43.65ms
step:958/1840 train_time:41834ms step_avg:43.67ms
step:959/1840 train_time:41894ms step_avg:43.69ms
step:960/1840 train_time:41956ms step_avg:43.70ms
step:961/1840 train_time:42015ms step_avg:43.72ms
step:962/1840 train_time:42078ms step_avg:43.74ms
step:963/1840 train_time:42136ms step_avg:43.76ms
step:964/1840 train_time:42199ms step_avg:43.77ms
step:965/1840 train_time:42258ms step_avg:43.79ms
step:966/1840 train_time:42321ms step_avg:43.81ms
step:967/1840 train_time:42382ms step_avg:43.83ms
step:968/1840 train_time:42445ms step_avg:43.85ms
step:969/1840 train_time:42505ms step_avg:43.87ms
step:970/1840 train_time:42569ms step_avg:43.89ms
step:971/1840 train_time:42629ms step_avg:43.90ms
step:972/1840 train_time:42691ms step_avg:43.92ms
step:973/1840 train_time:42751ms step_avg:43.94ms
step:974/1840 train_time:42812ms step_avg:43.96ms
step:975/1840 train_time:42872ms step_avg:43.97ms
step:976/1840 train_time:42935ms step_avg:43.99ms
step:977/1840 train_time:42994ms step_avg:44.01ms
step:978/1840 train_time:43057ms step_avg:44.03ms
step:979/1840 train_time:43117ms step_avg:44.04ms
step:980/1840 train_time:43178ms step_avg:44.06ms
step:981/1840 train_time:43237ms step_avg:44.07ms
step:982/1840 train_time:43300ms step_avg:44.09ms
step:983/1840 train_time:43359ms step_avg:44.11ms
step:984/1840 train_time:43422ms step_avg:44.13ms
step:985/1840 train_time:43483ms step_avg:44.14ms
step:986/1840 train_time:43545ms step_avg:44.16ms
step:987/1840 train_time:43606ms step_avg:44.18ms
step:988/1840 train_time:43670ms step_avg:44.20ms
step:989/1840 train_time:43730ms step_avg:44.22ms
step:990/1840 train_time:43791ms step_avg:44.23ms
step:991/1840 train_time:43851ms step_avg:44.25ms
step:992/1840 train_time:43913ms step_avg:44.27ms
step:993/1840 train_time:43973ms step_avg:44.28ms
step:994/1840 train_time:44035ms step_avg:44.30ms
step:995/1840 train_time:44095ms step_avg:44.32ms
step:996/1840 train_time:44157ms step_avg:44.33ms
step:997/1840 train_time:44216ms step_avg:44.35ms
step:998/1840 train_time:44278ms step_avg:44.37ms
step:999/1840 train_time:44337ms step_avg:44.38ms
step:1000/1840 train_time:44400ms step_avg:44.40ms
step:1000/1840 val_loss:3.7764 train_time:44473ms step_avg:44.47ms
step:1001/1840 train_time:44492ms step_avg:44.45ms
step:1002/1840 train_time:44525ms step_avg:44.44ms
step:1003/1840 train_time:44588ms step_avg:44.45ms
step:1004/1840 train_time:44649ms step_avg:44.47ms
step:1005/1840 train_time:44709ms step_avg:44.49ms
step:1006/1840 train_time:44770ms step_avg:44.50ms
step:1007/1840 train_time:44830ms step_avg:44.52ms
step:1008/1840 train_time:44892ms step_avg:44.54ms
step:1009/1840 train_time:44951ms step_avg:44.55ms
step:1010/1840 train_time:45013ms step_avg:44.57ms
step:1011/1840 train_time:45073ms step_avg:44.58ms
step:1012/1840 train_time:45134ms step_avg:44.60ms
step:1013/1840 train_time:45194ms step_avg:44.61ms
step:1014/1840 train_time:45256ms step_avg:44.63ms
step:1015/1840 train_time:45317ms step_avg:44.65ms
step:1016/1840 train_time:45379ms step_avg:44.66ms
step:1017/1840 train_time:45439ms step_avg:44.68ms
step:1018/1840 train_time:45503ms step_avg:44.70ms
step:1019/1840 train_time:45563ms step_avg:44.71ms
step:1020/1840 train_time:45626ms step_avg:44.73ms
step:1021/1840 train_time:45686ms step_avg:44.75ms
step:1022/1840 train_time:45748ms step_avg:44.76ms
step:1023/1840 train_time:45808ms step_avg:44.78ms
step:1024/1840 train_time:45869ms step_avg:44.79ms
step:1025/1840 train_time:45929ms step_avg:44.81ms
step:1026/1840 train_time:45991ms step_avg:44.83ms
step:1027/1840 train_time:46050ms step_avg:44.84ms
step:1028/1840 train_time:46112ms step_avg:44.86ms
step:1029/1840 train_time:46171ms step_avg:44.87ms
step:1030/1840 train_time:46234ms step_avg:44.89ms
step:1031/1840 train_time:46293ms step_avg:44.90ms
step:1032/1840 train_time:46355ms step_avg:44.92ms
step:1033/1840 train_time:46415ms step_avg:44.93ms
step:1034/1840 train_time:46478ms step_avg:44.95ms
step:1035/1840 train_time:46540ms step_avg:44.97ms
step:1036/1840 train_time:46604ms step_avg:44.98ms
step:1037/1840 train_time:46664ms step_avg:45.00ms
step:1038/1840 train_time:46726ms step_avg:45.02ms
step:1039/1840 train_time:46786ms step_avg:45.03ms
step:1040/1840 train_time:46847ms step_avg:45.05ms
step:1041/1840 train_time:46908ms step_avg:45.06ms
step:1042/1840 train_time:46970ms step_avg:45.08ms
step:1043/1840 train_time:47029ms step_avg:45.09ms
step:1044/1840 train_time:47091ms step_avg:45.11ms
step:1045/1840 train_time:47150ms step_avg:45.12ms
step:1046/1840 train_time:47213ms step_avg:45.14ms
step:1047/1840 train_time:47272ms step_avg:45.15ms
step:1048/1840 train_time:47333ms step_avg:45.17ms
step:1049/1840 train_time:47393ms step_avg:45.18ms
step:1050/1840 train_time:47456ms step_avg:45.20ms
step:1051/1840 train_time:47517ms step_avg:45.21ms
step:1052/1840 train_time:47580ms step_avg:45.23ms
step:1053/1840 train_time:47640ms step_avg:45.24ms
step:1054/1840 train_time:47704ms step_avg:45.26ms
step:1055/1840 train_time:47764ms step_avg:45.27ms
step:1056/1840 train_time:47826ms step_avg:45.29ms
step:1057/1840 train_time:47886ms step_avg:45.30ms
step:1058/1840 train_time:47947ms step_avg:45.32ms
step:1059/1840 train_time:48007ms step_avg:45.33ms
step:1060/1840 train_time:48068ms step_avg:45.35ms
step:1061/1840 train_time:48128ms step_avg:45.36ms
step:1062/1840 train_time:48190ms step_avg:45.38ms
step:1063/1840 train_time:48250ms step_avg:45.39ms
step:1064/1840 train_time:48312ms step_avg:45.41ms
step:1065/1840 train_time:48371ms step_avg:45.42ms
step:1066/1840 train_time:48433ms step_avg:45.43ms
step:1067/1840 train_time:48494ms step_avg:45.45ms
step:1068/1840 train_time:48557ms step_avg:45.47ms
step:1069/1840 train_time:48617ms step_avg:45.48ms
step:1070/1840 train_time:48680ms step_avg:45.50ms
step:1071/1840 train_time:48740ms step_avg:45.51ms
step:1072/1840 train_time:48803ms step_avg:45.53ms
step:1073/1840 train_time:48863ms step_avg:45.54ms
step:1074/1840 train_time:48925ms step_avg:45.55ms
step:1075/1840 train_time:48984ms step_avg:45.57ms
step:1076/1840 train_time:49046ms step_avg:45.58ms
step:1077/1840 train_time:49106ms step_avg:45.60ms
step:1078/1840 train_time:49169ms step_avg:45.61ms
step:1079/1840 train_time:49228ms step_avg:45.62ms
step:1080/1840 train_time:49290ms step_avg:45.64ms
step:1081/1840 train_time:49350ms step_avg:45.65ms
step:1082/1840 train_time:49413ms step_avg:45.67ms
step:1083/1840 train_time:49472ms step_avg:45.68ms
step:1084/1840 train_time:49534ms step_avg:45.70ms
step:1085/1840 train_time:49594ms step_avg:45.71ms
step:1086/1840 train_time:49657ms step_avg:45.73ms
step:1087/1840 train_time:49717ms step_avg:45.74ms
step:1088/1840 train_time:49780ms step_avg:45.75ms
step:1089/1840 train_time:49841ms step_avg:45.77ms
step:1090/1840 train_time:49905ms step_avg:45.78ms
step:1091/1840 train_time:49965ms step_avg:45.80ms
step:1092/1840 train_time:50027ms step_avg:45.81ms
step:1093/1840 train_time:50086ms step_avg:45.82ms
step:1094/1840 train_time:50149ms step_avg:45.84ms
step:1095/1840 train_time:50209ms step_avg:45.85ms
step:1096/1840 train_time:50271ms step_avg:45.87ms
step:1097/1840 train_time:50331ms step_avg:45.88ms
step:1098/1840 train_time:50393ms step_avg:45.90ms
step:1099/1840 train_time:50453ms step_avg:45.91ms
step:1100/1840 train_time:50515ms step_avg:45.92ms
step:1101/1840 train_time:50575ms step_avg:45.94ms
step:1102/1840 train_time:50637ms step_avg:45.95ms
step:1103/1840 train_time:50697ms step_avg:45.96ms
step:1104/1840 train_time:50759ms step_avg:45.98ms
step:1105/1840 train_time:50820ms step_avg:45.99ms
step:1106/1840 train_time:50884ms step_avg:46.01ms
step:1107/1840 train_time:50944ms step_avg:46.02ms
step:1108/1840 train_time:51006ms step_avg:46.03ms
step:1109/1840 train_time:51067ms step_avg:46.05ms
step:1110/1840 train_time:51128ms step_avg:46.06ms
step:1111/1840 train_time:51188ms step_avg:46.07ms
step:1112/1840 train_time:51249ms step_avg:46.09ms
step:1113/1840 train_time:51309ms step_avg:46.10ms
step:1114/1840 train_time:51371ms step_avg:46.11ms
step:1115/1840 train_time:51430ms step_avg:46.13ms
step:1116/1840 train_time:51493ms step_avg:46.14ms
step:1117/1840 train_time:51552ms step_avg:46.15ms
step:1118/1840 train_time:51614ms step_avg:46.17ms
step:1119/1840 train_time:51674ms step_avg:46.18ms
step:1120/1840 train_time:51737ms step_avg:46.19ms
step:1121/1840 train_time:51797ms step_avg:46.21ms
step:1122/1840 train_time:51861ms step_avg:46.22ms
step:1123/1840 train_time:51922ms step_avg:46.23ms
step:1124/1840 train_time:51985ms step_avg:46.25ms
step:1125/1840 train_time:52045ms step_avg:46.26ms
step:1126/1840 train_time:52107ms step_avg:46.28ms
step:1127/1840 train_time:52167ms step_avg:46.29ms
step:1128/1840 train_time:52228ms step_avg:46.30ms
step:1129/1840 train_time:52288ms step_avg:46.31ms
step:1130/1840 train_time:52351ms step_avg:46.33ms
step:1131/1840 train_time:52410ms step_avg:46.34ms
step:1132/1840 train_time:52472ms step_avg:46.35ms
step:1133/1840 train_time:52531ms step_avg:46.36ms
step:1134/1840 train_time:52594ms step_avg:46.38ms
step:1135/1840 train_time:52653ms step_avg:46.39ms
step:1136/1840 train_time:52716ms step_avg:46.40ms
step:1137/1840 train_time:52776ms step_avg:46.42ms
step:1138/1840 train_time:52839ms step_avg:46.43ms
step:1139/1840 train_time:52899ms step_avg:46.44ms
step:1140/1840 train_time:52963ms step_avg:46.46ms
step:1141/1840 train_time:53023ms step_avg:46.47ms
step:1142/1840 train_time:53086ms step_avg:46.49ms
step:1143/1840 train_time:53146ms step_avg:46.50ms
step:1144/1840 train_time:53208ms step_avg:46.51ms
step:1145/1840 train_time:53267ms step_avg:46.52ms
step:1146/1840 train_time:53329ms step_avg:46.54ms
step:1147/1840 train_time:53389ms step_avg:46.55ms
step:1148/1840 train_time:53452ms step_avg:46.56ms
step:1149/1840 train_time:53512ms step_avg:46.57ms
step:1150/1840 train_time:53574ms step_avg:46.59ms
step:1151/1840 train_time:53633ms step_avg:46.60ms
step:1152/1840 train_time:53695ms step_avg:46.61ms
step:1153/1840 train_time:53755ms step_avg:46.62ms
step:1154/1840 train_time:53817ms step_avg:46.64ms
step:1155/1840 train_time:53877ms step_avg:46.65ms
step:1156/1840 train_time:53941ms step_avg:46.66ms
step:1157/1840 train_time:54000ms step_avg:46.67ms
step:1158/1840 train_time:54064ms step_avg:46.69ms
step:1159/1840 train_time:54125ms step_avg:46.70ms
step:1160/1840 train_time:54187ms step_avg:46.71ms
step:1161/1840 train_time:54247ms step_avg:46.72ms
step:1162/1840 train_time:54308ms step_avg:46.74ms
step:1163/1840 train_time:54367ms step_avg:46.75ms
step:1164/1840 train_time:54430ms step_avg:46.76ms
step:1165/1840 train_time:54489ms step_avg:46.77ms
step:1166/1840 train_time:54552ms step_avg:46.79ms
step:1167/1840 train_time:54612ms step_avg:46.80ms
step:1168/1840 train_time:54674ms step_avg:46.81ms
step:1169/1840 train_time:54734ms step_avg:46.82ms
step:1170/1840 train_time:54796ms step_avg:46.83ms
step:1171/1840 train_time:54855ms step_avg:46.84ms
step:1172/1840 train_time:54918ms step_avg:46.86ms
step:1173/1840 train_time:54978ms step_avg:46.87ms
step:1174/1840 train_time:55041ms step_avg:46.88ms
step:1175/1840 train_time:55104ms step_avg:46.90ms
step:1176/1840 train_time:55166ms step_avg:46.91ms
step:1177/1840 train_time:55226ms step_avg:46.92ms
step:1178/1840 train_time:55288ms step_avg:46.93ms
step:1179/1840 train_time:55347ms step_avg:46.94ms
step:1180/1840 train_time:55409ms step_avg:46.96ms
step:1181/1840 train_time:55469ms step_avg:46.97ms
step:1182/1840 train_time:55531ms step_avg:46.98ms
step:1183/1840 train_time:55590ms step_avg:46.99ms
step:1184/1840 train_time:55652ms step_avg:47.00ms
step:1185/1840 train_time:55712ms step_avg:47.01ms
step:1186/1840 train_time:55774ms step_avg:47.03ms
step:1187/1840 train_time:55834ms step_avg:47.04ms
step:1188/1840 train_time:55897ms step_avg:47.05ms
step:1189/1840 train_time:55957ms step_avg:47.06ms
step:1190/1840 train_time:56020ms step_avg:47.08ms
step:1191/1840 train_time:56080ms step_avg:47.09ms
step:1192/1840 train_time:56143ms step_avg:47.10ms
step:1193/1840 train_time:56203ms step_avg:47.11ms
step:1194/1840 train_time:56265ms step_avg:47.12ms
step:1195/1840 train_time:56325ms step_avg:47.13ms
step:1196/1840 train_time:56387ms step_avg:47.15ms
step:1197/1840 train_time:56447ms step_avg:47.16ms
step:1198/1840 train_time:56508ms step_avg:47.17ms
step:1199/1840 train_time:56568ms step_avg:47.18ms
step:1200/1840 train_time:56631ms step_avg:47.19ms
step:1201/1840 train_time:56692ms step_avg:47.20ms
step:1202/1840 train_time:56776ms step_avg:47.23ms
step:1203/1840 train_time:56863ms step_avg:47.27ms
step:1204/1840 train_time:56951ms step_avg:47.30ms
step:1205/1840 train_time:57038ms step_avg:47.33ms
step:1206/1840 train_time:57129ms step_avg:47.37ms
step:1207/1840 train_time:57215ms step_avg:47.40ms
step:1208/1840 train_time:57304ms step_avg:47.44ms
step:1209/1840 train_time:57392ms step_avg:47.47ms
step:1210/1840 train_time:57483ms step_avg:47.51ms
step:1211/1840 train_time:57572ms step_avg:47.54ms
step:1212/1840 train_time:57661ms step_avg:47.57ms
step:1213/1840 train_time:57747ms step_avg:47.61ms
step:1214/1840 train_time:57835ms step_avg:47.64ms
step:1215/1840 train_time:57919ms step_avg:47.67ms
step:1216/1840 train_time:58008ms step_avg:47.70ms
step:1217/1840 train_time:58094ms step_avg:47.74ms
step:1218/1840 train_time:58183ms step_avg:47.77ms
step:1219/1840 train_time:58271ms step_avg:47.80ms
step:1220/1840 train_time:58360ms step_avg:47.84ms
step:1221/1840 train_time:58448ms step_avg:47.87ms
step:1222/1840 train_time:58537ms step_avg:47.90ms
step:1223/1840 train_time:58623ms step_avg:47.93ms
step:1224/1840 train_time:58712ms step_avg:47.97ms
step:1225/1840 train_time:58797ms step_avg:48.00ms
step:1226/1840 train_time:58887ms step_avg:48.03ms
step:1227/1840 train_time:58974ms step_avg:48.06ms
step:1228/1840 train_time:59061ms step_avg:48.10ms
step:1229/1840 train_time:59148ms step_avg:48.13ms
step:1230/1840 train_time:59238ms step_avg:48.16ms
step:1231/1840 train_time:59324ms step_avg:48.19ms
step:1232/1840 train_time:59414ms step_avg:48.23ms
step:1233/1840 train_time:59499ms step_avg:48.26ms
step:1234/1840 train_time:59588ms step_avg:48.29ms
step:1235/1840 train_time:59675ms step_avg:48.32ms
step:1236/1840 train_time:59764ms step_avg:48.35ms
step:1237/1840 train_time:59850ms step_avg:48.38ms
step:1238/1840 train_time:59939ms step_avg:48.42ms
step:1239/1840 train_time:60025ms step_avg:48.45ms
step:1240/1840 train_time:60114ms step_avg:48.48ms
step:1241/1840 train_time:60200ms step_avg:48.51ms
step:1242/1840 train_time:60289ms step_avg:48.54ms
step:1243/1840 train_time:60377ms step_avg:48.57ms
step:1244/1840 train_time:60466ms step_avg:48.61ms
step:1245/1840 train_time:60552ms step_avg:48.64ms
step:1246/1840 train_time:60641ms step_avg:48.67ms
step:1247/1840 train_time:60727ms step_avg:48.70ms
step:1248/1840 train_time:60816ms step_avg:48.73ms
step:1249/1840 train_time:60901ms step_avg:48.76ms
step:1250/1840 train_time:60989ms step_avg:48.79ms
step:1250/1840 val_loss:3.5357 train_time:61090ms step_avg:48.87ms
step:1251/1840 train_time:61109ms step_avg:48.85ms
step:1252/1840 train_time:61165ms step_avg:48.85ms
step:1253/1840 train_time:61252ms step_avg:48.88ms
step:1254/1840 train_time:61344ms step_avg:48.92ms
step:1255/1840 train_time:61434ms step_avg:48.95ms
step:1256/1840 train_time:61525ms step_avg:48.99ms
step:1257/1840 train_time:61611ms step_avg:49.01ms
step:1258/1840 train_time:61700ms step_avg:49.05ms
step:1259/1840 train_time:61786ms step_avg:49.08ms
step:1260/1840 train_time:61873ms step_avg:49.11ms
step:1261/1840 train_time:61958ms step_avg:49.13ms
step:1262/1840 train_time:62051ms step_avg:49.17ms
step:1263/1840 train_time:62138ms step_avg:49.20ms
step:1264/1840 train_time:62226ms step_avg:49.23ms
step:1265/1840 train_time:62313ms step_avg:49.26ms
step:1266/1840 train_time:62404ms step_avg:49.29ms
step:1267/1840 train_time:62492ms step_avg:49.32ms
step:1268/1840 train_time:62582ms step_avg:49.35ms
step:1269/1840 train_time:62667ms step_avg:49.38ms
step:1270/1840 train_time:62755ms step_avg:49.41ms
step:1271/1840 train_time:62841ms step_avg:49.44ms
step:1272/1840 train_time:62930ms step_avg:49.47ms
step:1273/1840 train_time:63018ms step_avg:49.50ms
step:1274/1840 train_time:63107ms step_avg:49.53ms
step:1275/1840 train_time:63192ms step_avg:49.56ms
step:1276/1840 train_time:63281ms step_avg:49.59ms
step:1277/1840 train_time:63368ms step_avg:49.62ms
step:1278/1840 train_time:63458ms step_avg:49.65ms
step:1279/1840 train_time:63544ms step_avg:49.68ms
step:1280/1840 train_time:63633ms step_avg:49.71ms
step:1281/1840 train_time:63719ms step_avg:49.74ms
step:1282/1840 train_time:63807ms step_avg:49.77ms
step:1283/1840 train_time:63892ms step_avg:49.80ms
step:1284/1840 train_time:63981ms step_avg:49.83ms
step:1285/1840 train_time:64069ms step_avg:49.86ms
step:1286/1840 train_time:64157ms step_avg:49.89ms
step:1287/1840 train_time:64244ms step_avg:49.92ms
step:1288/1840 train_time:64332ms step_avg:49.95ms
step:1289/1840 train_time:64419ms step_avg:49.98ms
step:1290/1840 train_time:64508ms step_avg:50.01ms
step:1291/1840 train_time:64593ms step_avg:50.03ms
step:1292/1840 train_time:64682ms step_avg:50.06ms
step:1293/1840 train_time:64768ms step_avg:50.09ms
step:1294/1840 train_time:64857ms step_avg:50.12ms
step:1295/1840 train_time:64943ms step_avg:50.15ms
step:1296/1840 train_time:65031ms step_avg:50.18ms
step:1297/1840 train_time:65117ms step_avg:50.21ms
step:1298/1840 train_time:65206ms step_avg:50.24ms
step:1299/1840 train_time:65292ms step_avg:50.26ms
step:1300/1840 train_time:65380ms step_avg:50.29ms
step:1301/1840 train_time:65467ms step_avg:50.32ms
step:1302/1840 train_time:65556ms step_avg:50.35ms
step:1303/1840 train_time:65642ms step_avg:50.38ms
step:1304/1840 train_time:65730ms step_avg:50.41ms
step:1305/1840 train_time:65817ms step_avg:50.43ms
step:1306/1840 train_time:65905ms step_avg:50.46ms
step:1307/1840 train_time:65992ms step_avg:50.49ms
step:1308/1840 train_time:66080ms step_avg:50.52ms
step:1309/1840 train_time:66166ms step_avg:50.55ms
step:1310/1840 train_time:66255ms step_avg:50.58ms
step:1311/1840 train_time:66341ms step_avg:50.60ms
step:1312/1840 train_time:66431ms step_avg:50.63ms
step:1313/1840 train_time:66517ms step_avg:50.66ms
step:1314/1840 train_time:66606ms step_avg:50.69ms
step:1315/1840 train_time:66692ms step_avg:50.72ms
step:1316/1840 train_time:66780ms step_avg:50.74ms
step:1317/1840 train_time:66866ms step_avg:50.77ms
step:1318/1840 train_time:66954ms step_avg:50.80ms
step:1319/1840 train_time:67039ms step_avg:50.83ms
step:1320/1840 train_time:67129ms step_avg:50.86ms
step:1321/1840 train_time:67215ms step_avg:50.88ms
step:1322/1840 train_time:67303ms step_avg:50.91ms
step:1323/1840 train_time:67391ms step_avg:50.94ms
step:1324/1840 train_time:67479ms step_avg:50.97ms
step:1325/1840 train_time:67564ms step_avg:50.99ms
step:1326/1840 train_time:67652ms step_avg:51.02ms
step:1327/1840 train_time:67739ms step_avg:51.05ms
step:1328/1840 train_time:67828ms step_avg:51.08ms
step:1329/1840 train_time:67915ms step_avg:51.10ms
step:1330/1840 train_time:68003ms step_avg:51.13ms
step:1331/1840 train_time:68091ms step_avg:51.16ms
step:1332/1840 train_time:68179ms step_avg:51.19ms
step:1333/1840 train_time:68265ms step_avg:51.21ms
step:1334/1840 train_time:68354ms step_avg:51.24ms
step:1335/1840 train_time:68441ms step_avg:51.27ms
step:1336/1840 train_time:68529ms step_avg:51.29ms
step:1337/1840 train_time:68614ms step_avg:51.32ms
step:1338/1840 train_time:68703ms step_avg:51.35ms
step:1339/1840 train_time:68790ms step_avg:51.37ms
step:1340/1840 train_time:68878ms step_avg:51.40ms
step:1341/1840 train_time:68963ms step_avg:51.43ms
step:1342/1840 train_time:69053ms step_avg:51.46ms
step:1343/1840 train_time:69139ms step_avg:51.48ms
step:1344/1840 train_time:69228ms step_avg:51.51ms
step:1345/1840 train_time:69313ms step_avg:51.53ms
step:1346/1840 train_time:69403ms step_avg:51.56ms
step:1347/1840 train_time:69490ms step_avg:51.59ms
step:1348/1840 train_time:69578ms step_avg:51.62ms
step:1349/1840 train_time:69663ms step_avg:51.64ms
step:1350/1840 train_time:69752ms step_avg:51.67ms
step:1351/1840 train_time:69838ms step_avg:51.69ms
step:1352/1840 train_time:69928ms step_avg:51.72ms
step:1353/1840 train_time:70013ms step_avg:51.75ms
step:1354/1840 train_time:70102ms step_avg:51.77ms
step:1355/1840 train_time:70190ms step_avg:51.80ms
step:1356/1840 train_time:70278ms step_avg:51.83ms
step:1357/1840 train_time:70364ms step_avg:51.85ms
step:1358/1840 train_time:70453ms step_avg:51.88ms
step:1359/1840 train_time:70540ms step_avg:51.91ms
step:1360/1840 train_time:70628ms step_avg:51.93ms
step:1361/1840 train_time:70714ms step_avg:51.96ms
step:1362/1840 train_time:70802ms step_avg:51.98ms
step:1363/1840 train_time:70889ms step_avg:52.01ms
step:1364/1840 train_time:70978ms step_avg:52.04ms
step:1365/1840 train_time:71065ms step_avg:52.06ms
step:1366/1840 train_time:71153ms step_avg:52.09ms
step:1367/1840 train_time:71239ms step_avg:52.11ms
step:1368/1840 train_time:71329ms step_avg:52.14ms
step:1369/1840 train_time:71414ms step_avg:52.17ms
step:1370/1840 train_time:71503ms step_avg:52.19ms
step:1371/1840 train_time:71591ms step_avg:52.22ms
step:1372/1840 train_time:71679ms step_avg:52.24ms
step:1373/1840 train_time:71764ms step_avg:52.27ms
step:1374/1840 train_time:71853ms step_avg:52.29ms
step:1375/1840 train_time:71940ms step_avg:52.32ms
step:1376/1840 train_time:72029ms step_avg:52.35ms
step:1377/1840 train_time:72115ms step_avg:52.37ms
step:1378/1840 train_time:72203ms step_avg:52.40ms
step:1379/1840 train_time:72291ms step_avg:52.42ms
step:1380/1840 train_time:72379ms step_avg:52.45ms
step:1381/1840 train_time:72464ms step_avg:52.47ms
step:1382/1840 train_time:72553ms step_avg:52.50ms
step:1383/1840 train_time:72640ms step_avg:52.52ms
step:1384/1840 train_time:72728ms step_avg:52.55ms
step:1385/1840 train_time:72814ms step_avg:52.57ms
step:1386/1840 train_time:72902ms step_avg:52.60ms
step:1387/1840 train_time:72990ms step_avg:52.62ms
step:1388/1840 train_time:73079ms step_avg:52.65ms
step:1389/1840 train_time:73165ms step_avg:52.67ms
step:1390/1840 train_time:73254ms step_avg:52.70ms
step:1391/1840 train_time:73342ms step_avg:52.73ms
step:1392/1840 train_time:73430ms step_avg:52.75ms
step:1393/1840 train_time:73516ms step_avg:52.78ms
step:1394/1840 train_time:73604ms step_avg:52.80ms
step:1395/1840 train_time:73691ms step_avg:52.83ms
step:1396/1840 train_time:73779ms step_avg:52.85ms
step:1397/1840 train_time:73865ms step_avg:52.87ms
step:1398/1840 train_time:73954ms step_avg:52.90ms
step:1399/1840 train_time:74041ms step_avg:52.92ms
step:1400/1840 train_time:74129ms step_avg:52.95ms
step:1401/1840 train_time:74215ms step_avg:52.97ms
step:1402/1840 train_time:74303ms step_avg:53.00ms
step:1403/1840 train_time:74390ms step_avg:53.02ms
step:1404/1840 train_time:74478ms step_avg:53.05ms
step:1405/1840 train_time:74564ms step_avg:53.07ms
step:1406/1840 train_time:74654ms step_avg:53.10ms
step:1407/1840 train_time:74740ms step_avg:53.12ms
step:1408/1840 train_time:74828ms step_avg:53.14ms
step:1409/1840 train_time:74915ms step_avg:53.17ms
step:1410/1840 train_time:75004ms step_avg:53.19ms
step:1411/1840 train_time:75090ms step_avg:53.22ms
step:1412/1840 train_time:75178ms step_avg:53.24ms
step:1413/1840 train_time:75265ms step_avg:53.27ms
step:1414/1840 train_time:75354ms step_avg:53.29ms
step:1415/1840 train_time:75441ms step_avg:53.32ms
step:1416/1840 train_time:75530ms step_avg:53.34ms
step:1417/1840 train_time:75615ms step_avg:53.36ms
step:1418/1840 train_time:75703ms step_avg:53.39ms
step:1419/1840 train_time:75791ms step_avg:53.41ms
step:1420/1840 train_time:75879ms step_avg:53.44ms
step:1421/1840 train_time:75965ms step_avg:53.46ms
step:1422/1840 train_time:76054ms step_avg:53.48ms
step:1423/1840 train_time:76140ms step_avg:53.51ms
step:1424/1840 train_time:76230ms step_avg:53.53ms
step:1425/1840 train_time:76316ms step_avg:53.56ms
step:1426/1840 train_time:76404ms step_avg:53.58ms
step:1427/1840 train_time:76490ms step_avg:53.60ms
step:1428/1840 train_time:76579ms step_avg:53.63ms
step:1429/1840 train_time:76665ms step_avg:53.65ms
step:1430/1840 train_time:76754ms step_avg:53.67ms
step:1431/1840 train_time:76840ms step_avg:53.70ms
step:1432/1840 train_time:76929ms step_avg:53.72ms
step:1433/1840 train_time:77015ms step_avg:53.74ms
step:1434/1840 train_time:77103ms step_avg:53.77ms
step:1435/1840 train_time:77190ms step_avg:53.79ms
step:1436/1840 train_time:77279ms step_avg:53.82ms
step:1437/1840 train_time:77364ms step_avg:53.84ms
step:1438/1840 train_time:77453ms step_avg:53.86ms
step:1439/1840 train_time:77540ms step_avg:53.88ms
step:1440/1840 train_time:77629ms step_avg:53.91ms
step:1441/1840 train_time:77715ms step_avg:53.93ms
step:1442/1840 train_time:77803ms step_avg:53.95ms
step:1443/1840 train_time:77891ms step_avg:53.98ms
step:1444/1840 train_time:77979ms step_avg:54.00ms
step:1445/1840 train_time:78065ms step_avg:54.02ms
step:1446/1840 train_time:78154ms step_avg:54.05ms
step:1447/1840 train_time:78241ms step_avg:54.07ms
step:1448/1840 train_time:78329ms step_avg:54.09ms
step:1449/1840 train_time:78414ms step_avg:54.12ms
step:1450/1840 train_time:78503ms step_avg:54.14ms
step:1451/1840 train_time:78590ms step_avg:54.16ms
step:1452/1840 train_time:78678ms step_avg:54.19ms
step:1453/1840 train_time:78765ms step_avg:54.21ms
step:1454/1840 train_time:78854ms step_avg:54.23ms
step:1455/1840 train_time:78940ms step_avg:54.25ms
step:1456/1840 train_time:79029ms step_avg:54.28ms
step:1457/1840 train_time:79115ms step_avg:54.30ms
step:1458/1840 train_time:79203ms step_avg:54.32ms
step:1459/1840 train_time:79291ms step_avg:54.35ms
step:1460/1840 train_time:79379ms step_avg:54.37ms
step:1461/1840 train_time:79465ms step_avg:54.39ms
step:1462/1840 train_time:79555ms step_avg:54.41ms
step:1463/1840 train_time:79642ms step_avg:54.44ms
step:1464/1840 train_time:79731ms step_avg:54.46ms
step:1465/1840 train_time:79817ms step_avg:54.48ms
step:1466/1840 train_time:79905ms step_avg:54.51ms
step:1467/1840 train_time:79992ms step_avg:54.53ms
step:1468/1840 train_time:80080ms step_avg:54.55ms
step:1469/1840 train_time:80167ms step_avg:54.57ms
step:1470/1840 train_time:80256ms step_avg:54.60ms
step:1471/1840 train_time:80341ms step_avg:54.62ms
step:1472/1840 train_time:80430ms step_avg:54.64ms
step:1473/1840 train_time:80517ms step_avg:54.66ms
step:1474/1840 train_time:80605ms step_avg:54.68ms
step:1475/1840 train_time:80691ms step_avg:54.71ms
step:1476/1840 train_time:80781ms step_avg:54.73ms
step:1477/1840 train_time:80867ms step_avg:54.75ms
step:1478/1840 train_time:80956ms step_avg:54.77ms
step:1479/1840 train_time:81043ms step_avg:54.80ms
step:1480/1840 train_time:81131ms step_avg:54.82ms
step:1481/1840 train_time:81217ms step_avg:54.84ms
step:1482/1840 train_time:81305ms step_avg:54.86ms
step:1483/1840 train_time:81392ms step_avg:54.88ms
step:1484/1840 train_time:81481ms step_avg:54.91ms
step:1485/1840 train_time:81567ms step_avg:54.93ms
step:1486/1840 train_time:81655ms step_avg:54.95ms
step:1487/1840 train_time:81741ms step_avg:54.97ms
step:1488/1840 train_time:81831ms step_avg:54.99ms
step:1489/1840 train_time:81915ms step_avg:55.01ms
step:1490/1840 train_time:82004ms step_avg:55.04ms
step:1491/1840 train_time:82090ms step_avg:55.06ms
step:1492/1840 train_time:82179ms step_avg:55.08ms
step:1493/1840 train_time:82266ms step_avg:55.10ms
step:1494/1840 train_time:82354ms step_avg:55.12ms
step:1495/1840 train_time:82439ms step_avg:55.14ms
step:1496/1840 train_time:82529ms step_avg:55.17ms
step:1497/1840 train_time:82615ms step_avg:55.19ms
step:1498/1840 train_time:82705ms step_avg:55.21ms
step:1499/1840 train_time:82791ms step_avg:55.23ms
step:1500/1840 train_time:82879ms step_avg:55.25ms
step:1500/1840 val_loss:3.4024 train_time:82978ms step_avg:55.32ms
step:1501/1840 train_time:82997ms step_avg:55.29ms
step:1502/1840 train_time:83053ms step_avg:55.29ms
step:1503/1840 train_time:83141ms step_avg:55.32ms
step:1504/1840 train_time:83231ms step_avg:55.34ms
step:1505/1840 train_time:83316ms step_avg:55.36ms
step:1506/1840 train_time:83404ms step_avg:55.38ms
step:1507/1840 train_time:83489ms step_avg:55.40ms
step:1508/1840 train_time:83576ms step_avg:55.42ms
step:1509/1840 train_time:83662ms step_avg:55.44ms
step:1510/1840 train_time:83750ms step_avg:55.46ms
step:1511/1840 train_time:83835ms step_avg:55.48ms
step:1512/1840 train_time:83927ms step_avg:55.51ms
step:1513/1840 train_time:84015ms step_avg:55.53ms
step:1514/1840 train_time:84107ms step_avg:55.55ms
step:1515/1840 train_time:84195ms step_avg:55.57ms
step:1516/1840 train_time:84284ms step_avg:55.60ms
step:1517/1840 train_time:84371ms step_avg:55.62ms
step:1518/1840 train_time:84457ms step_avg:55.64ms
step:1519/1840 train_time:84543ms step_avg:55.66ms
step:1520/1840 train_time:84631ms step_avg:55.68ms
step:1521/1840 train_time:84717ms step_avg:55.70ms
step:1522/1840 train_time:84805ms step_avg:55.72ms
step:1523/1840 train_time:84892ms step_avg:55.74ms
step:1524/1840 train_time:84982ms step_avg:55.76ms
step:1525/1840 train_time:85071ms step_avg:55.78ms
step:1526/1840 train_time:85160ms step_avg:55.81ms
step:1527/1840 train_time:85247ms step_avg:55.83ms
step:1528/1840 train_time:85334ms step_avg:55.85ms
step:1529/1840 train_time:85420ms step_avg:55.87ms
step:1530/1840 train_time:85508ms step_avg:55.89ms
step:1531/1840 train_time:85594ms step_avg:55.91ms
step:1532/1840 train_time:85683ms step_avg:55.93ms
step:1533/1840 train_time:85769ms step_avg:55.95ms
step:1534/1840 train_time:85857ms step_avg:55.97ms
step:1535/1840 train_time:85945ms step_avg:55.99ms
step:1536/1840 train_time:86035ms step_avg:56.01ms
step:1537/1840 train_time:86122ms step_avg:56.03ms
step:1538/1840 train_time:86212ms step_avg:56.05ms
step:1539/1840 train_time:86296ms step_avg:56.07ms
step:1540/1840 train_time:86386ms step_avg:56.09ms
step:1541/1840 train_time:86472ms step_avg:56.11ms
step:1542/1840 train_time:86561ms step_avg:56.14ms
step:1543/1840 train_time:86646ms step_avg:56.15ms
step:1544/1840 train_time:86734ms step_avg:56.17ms
step:1545/1840 train_time:86821ms step_avg:56.19ms
step:1546/1840 train_time:86911ms step_avg:56.22ms
step:1547/1840 train_time:86998ms step_avg:56.24ms
step:1548/1840 train_time:87087ms step_avg:56.26ms
step:1549/1840 train_time:87174ms step_avg:56.28ms
step:1550/1840 train_time:87262ms step_avg:56.30ms
step:1551/1840 train_time:87347ms step_avg:56.32ms
step:1552/1840 train_time:87435ms step_avg:56.34ms
step:1553/1840 train_time:87521ms step_avg:56.36ms
step:1554/1840 train_time:87611ms step_avg:56.38ms
step:1555/1840 train_time:87696ms step_avg:56.40ms
step:1556/1840 train_time:87785ms step_avg:56.42ms
step:1557/1840 train_time:87872ms step_avg:56.44ms
step:1558/1840 train_time:87961ms step_avg:56.46ms
step:1559/1840 train_time:88047ms step_avg:56.48ms
step:1560/1840 train_time:88136ms step_avg:56.50ms
step:1561/1840 train_time:88222ms step_avg:56.52ms
step:1562/1840 train_time:88311ms step_avg:56.54ms
step:1563/1840 train_time:88396ms step_avg:56.56ms
step:1564/1840 train_time:88484ms step_avg:56.58ms
step:1565/1840 train_time:88572ms step_avg:56.60ms
step:1566/1840 train_time:88660ms step_avg:56.62ms
step:1567/1840 train_time:88746ms step_avg:56.63ms
step:1568/1840 train_time:88834ms step_avg:56.65ms
step:1569/1840 train_time:88921ms step_avg:56.67ms
step:1570/1840 train_time:89010ms step_avg:56.69ms
step:1571/1840 train_time:89096ms step_avg:56.71ms
step:1572/1840 train_time:89185ms step_avg:56.73ms
step:1573/1840 train_time:89272ms step_avg:56.75ms
step:1574/1840 train_time:89360ms step_avg:56.77ms
step:1575/1840 train_time:89446ms step_avg:56.79ms
step:1576/1840 train_time:89535ms step_avg:56.81ms
step:1577/1840 train_time:89620ms step_avg:56.83ms
step:1578/1840 train_time:89709ms step_avg:56.85ms
step:1579/1840 train_time:89795ms step_avg:56.87ms
step:1580/1840 train_time:89884ms step_avg:56.89ms
step:1581/1840 train_time:89972ms step_avg:56.91ms
step:1582/1840 train_time:90061ms step_avg:56.93ms
step:1583/1840 train_time:90148ms step_avg:56.95ms
step:1584/1840 train_time:90236ms step_avg:56.97ms
step:1585/1840 train_time:90320ms step_avg:56.98ms
step:1586/1840 train_time:90410ms step_avg:57.01ms
step:1587/1840 train_time:90496ms step_avg:57.02ms
step:1588/1840 train_time:90585ms step_avg:57.04ms
step:1589/1840 train_time:90671ms step_avg:57.06ms
step:1590/1840 train_time:90759ms step_avg:57.08ms
step:1591/1840 train_time:90845ms step_avg:57.10ms
step:1592/1840 train_time:90935ms step_avg:57.12ms
step:1593/1840 train_time:91022ms step_avg:57.14ms
step:1594/1840 train_time:91113ms step_avg:57.16ms
step:1595/1840 train_time:91197ms step_avg:57.18ms
step:1596/1840 train_time:91286ms step_avg:57.20ms
step:1597/1840 train_time:91373ms step_avg:57.22ms
step:1598/1840 train_time:91462ms step_avg:57.24ms
step:1599/1840 train_time:91548ms step_avg:57.25ms
step:1600/1840 train_time:91635ms step_avg:57.27ms
step:1601/1840 train_time:91721ms step_avg:57.29ms
step:1602/1840 train_time:91810ms step_avg:57.31ms
step:1603/1840 train_time:91896ms step_avg:57.33ms
step:1604/1840 train_time:91986ms step_avg:57.35ms
step:1605/1840 train_time:92073ms step_avg:57.37ms
step:1606/1840 train_time:92162ms step_avg:57.39ms
step:1607/1840 train_time:92248ms step_avg:57.40ms
step:1608/1840 train_time:92337ms step_avg:57.42ms
step:1609/1840 train_time:92424ms step_avg:57.44ms
step:1610/1840 train_time:92512ms step_avg:57.46ms
step:1611/1840 train_time:92597ms step_avg:57.48ms
step:1612/1840 train_time:92686ms step_avg:57.50ms
step:1613/1840 train_time:92773ms step_avg:57.52ms
step:1614/1840 train_time:92861ms step_avg:57.53ms
step:1615/1840 train_time:92947ms step_avg:57.55ms
step:1616/1840 train_time:93036ms step_avg:57.57ms
step:1617/1840 train_time:93123ms step_avg:57.59ms
step:1618/1840 train_time:93212ms step_avg:57.61ms
step:1619/1840 train_time:93297ms step_avg:57.63ms
step:1620/1840 train_time:93386ms step_avg:57.65ms
step:1621/1840 train_time:93473ms step_avg:57.66ms
step:1622/1840 train_time:93561ms step_avg:57.68ms
step:1623/1840 train_time:93647ms step_avg:57.70ms
step:1624/1840 train_time:93736ms step_avg:57.72ms
step:1625/1840 train_time:93822ms step_avg:57.74ms
step:1626/1840 train_time:93912ms step_avg:57.76ms
step:1627/1840 train_time:93997ms step_avg:57.77ms
step:1628/1840 train_time:94086ms step_avg:57.79ms
step:1629/1840 train_time:94173ms step_avg:57.81ms
step:1630/1840 train_time:94262ms step_avg:57.83ms
step:1631/1840 train_time:94350ms step_avg:57.85ms
step:1632/1840 train_time:94438ms step_avg:57.87ms
step:1633/1840 train_time:94524ms step_avg:57.88ms
step:1634/1840 train_time:94612ms step_avg:57.90ms
step:1635/1840 train_time:94699ms step_avg:57.92ms
step:1636/1840 train_time:94788ms step_avg:57.94ms
step:1637/1840 train_time:94873ms step_avg:57.96ms
step:1638/1840 train_time:94962ms step_avg:57.97ms
step:1639/1840 train_time:95048ms step_avg:57.99ms
step:1640/1840 train_time:95137ms step_avg:58.01ms
step:1641/1840 train_time:95223ms step_avg:58.03ms
step:1642/1840 train_time:95312ms step_avg:58.05ms
step:1643/1840 train_time:95397ms step_avg:58.06ms
step:1644/1840 train_time:95487ms step_avg:58.08ms
step:1645/1840 train_time:95573ms step_avg:58.10ms
step:1646/1840 train_time:95662ms step_avg:58.12ms
step:1647/1840 train_time:95748ms step_avg:58.13ms
step:1648/1840 train_time:95837ms step_avg:58.15ms
step:1649/1840 train_time:95923ms step_avg:58.17ms
step:1650/1840 train_time:96012ms step_avg:58.19ms
step:1651/1840 train_time:96099ms step_avg:58.21ms
step:1652/1840 train_time:96188ms step_avg:58.23ms
step:1653/1840 train_time:96275ms step_avg:58.24ms
step:1654/1840 train_time:96363ms step_avg:58.26ms
step:1655/1840 train_time:96450ms step_avg:58.28ms
step:1656/1840 train_time:96539ms step_avg:58.30ms
step:1657/1840 train_time:96625ms step_avg:58.31ms
step:1658/1840 train_time:96713ms step_avg:58.33ms
step:1659/1840 train_time:96798ms step_avg:58.35ms
step:1660/1840 train_time:96887ms step_avg:58.37ms
step:1661/1840 train_time:96973ms step_avg:58.38ms
step:1662/1840 train_time:97062ms step_avg:58.40ms
step:1663/1840 train_time:97149ms step_avg:58.42ms
step:1664/1840 train_time:97237ms step_avg:58.44ms
step:1665/1840 train_time:97322ms step_avg:58.45ms
step:1666/1840 train_time:97412ms step_avg:58.47ms
step:1667/1840 train_time:97498ms step_avg:58.49ms
step:1668/1840 train_time:97588ms step_avg:58.51ms
step:1669/1840 train_time:97674ms step_avg:58.52ms
step:1670/1840 train_time:97764ms step_avg:58.54ms
step:1671/1840 train_time:97850ms step_avg:58.56ms
step:1672/1840 train_time:97938ms step_avg:58.58ms
step:1673/1840 train_time:98024ms step_avg:58.59ms
step:1674/1840 train_time:98113ms step_avg:58.61ms
step:1675/1840 train_time:98198ms step_avg:58.63ms
step:1676/1840 train_time:98286ms step_avg:58.64ms
step:1677/1840 train_time:98373ms step_avg:58.66ms
step:1678/1840 train_time:98462ms step_avg:58.68ms
step:1679/1840 train_time:98549ms step_avg:58.70ms
step:1680/1840 train_time:98639ms step_avg:58.71ms
step:1681/1840 train_time:98725ms step_avg:58.73ms
step:1682/1840 train_time:98813ms step_avg:58.75ms
step:1683/1840 train_time:98899ms step_avg:58.76ms
step:1684/1840 train_time:98989ms step_avg:58.78ms
step:1685/1840 train_time:99075ms step_avg:58.80ms
step:1686/1840 train_time:99163ms step_avg:58.82ms
step:1687/1840 train_time:99250ms step_avg:58.83ms
step:1688/1840 train_time:99339ms step_avg:58.85ms
step:1689/1840 train_time:99425ms step_avg:58.87ms
step:1690/1840 train_time:99513ms step_avg:58.88ms
step:1691/1840 train_time:99600ms step_avg:58.90ms
step:1692/1840 train_time:99690ms step_avg:58.92ms
step:1693/1840 train_time:99775ms step_avg:58.93ms
step:1694/1840 train_time:99864ms step_avg:58.95ms
step:1695/1840 train_time:99950ms step_avg:58.97ms
step:1696/1840 train_time:100039ms step_avg:58.99ms
step:1697/1840 train_time:100125ms step_avg:59.00ms
step:1698/1840 train_time:100213ms step_avg:59.02ms
step:1699/1840 train_time:100299ms step_avg:59.03ms
step:1700/1840 train_time:100388ms step_avg:59.05ms
step:1701/1840 train_time:100474ms step_avg:59.07ms
step:1702/1840 train_time:100562ms step_avg:59.08ms
step:1703/1840 train_time:100648ms step_avg:59.10ms
step:1704/1840 train_time:100737ms step_avg:59.12ms
step:1705/1840 train_time:100823ms step_avg:59.13ms
step:1706/1840 train_time:100912ms step_avg:59.15ms
step:1707/1840 train_time:100998ms step_avg:59.17ms
step:1708/1840 train_time:101086ms step_avg:59.18ms
step:1709/1840 train_time:101172ms step_avg:59.20ms
step:1710/1840 train_time:101261ms step_avg:59.22ms
step:1711/1840 train_time:101348ms step_avg:59.23ms
step:1712/1840 train_time:101436ms step_avg:59.25ms
step:1713/1840 train_time:101522ms step_avg:59.27ms
step:1714/1840 train_time:101611ms step_avg:59.28ms
step:1715/1840 train_time:101696ms step_avg:59.30ms
step:1716/1840 train_time:101785ms step_avg:59.32ms
step:1717/1840 train_time:101872ms step_avg:59.33ms
step:1718/1840 train_time:101961ms step_avg:59.35ms
step:1719/1840 train_time:102047ms step_avg:59.36ms
step:1720/1840 train_time:102136ms step_avg:59.38ms
step:1721/1840 train_time:102222ms step_avg:59.40ms
step:1722/1840 train_time:102311ms step_avg:59.41ms
step:1723/1840 train_time:102398ms step_avg:59.43ms
step:1724/1840 train_time:102487ms step_avg:59.45ms
step:1725/1840 train_time:102574ms step_avg:59.46ms
step:1726/1840 train_time:102662ms step_avg:59.48ms
step:1727/1840 train_time:102749ms step_avg:59.50ms
step:1728/1840 train_time:102838ms step_avg:59.51ms
step:1729/1840 train_time:102923ms step_avg:59.53ms
step:1730/1840 train_time:103012ms step_avg:59.54ms
step:1731/1840 train_time:103097ms step_avg:59.56ms
step:1732/1840 train_time:103186ms step_avg:59.58ms
step:1733/1840 train_time:103273ms step_avg:59.59ms
step:1734/1840 train_time:103361ms step_avg:59.61ms
step:1735/1840 train_time:103448ms step_avg:59.62ms
step:1736/1840 train_time:103537ms step_avg:59.64ms
step:1737/1840 train_time:103623ms step_avg:59.66ms
step:1738/1840 train_time:103712ms step_avg:59.67ms
step:1739/1840 train_time:103797ms step_avg:59.69ms
step:1740/1840 train_time:103886ms step_avg:59.70ms
step:1741/1840 train_time:103972ms step_avg:59.72ms
step:1742/1840 train_time:104060ms step_avg:59.74ms
step:1743/1840 train_time:104147ms step_avg:59.75ms
step:1744/1840 train_time:104235ms step_avg:59.77ms
step:1745/1840 train_time:104322ms step_avg:59.78ms
step:1746/1840 train_time:104412ms step_avg:59.80ms
step:1747/1840 train_time:104498ms step_avg:59.82ms
step:1748/1840 train_time:104586ms step_avg:59.83ms
step:1749/1840 train_time:104673ms step_avg:59.85ms
step:1750/1840 train_time:104762ms step_avg:59.86ms
step:1750/1840 val_loss:3.3030 train_time:104862ms step_avg:59.92ms
step:1751/1840 train_time:104881ms step_avg:59.90ms
step:1752/1840 train_time:104938ms step_avg:59.90ms
step:1753/1840 train_time:105027ms step_avg:59.91ms
step:1754/1840 train_time:105118ms step_avg:59.93ms
step:1755/1840 train_time:105203ms step_avg:59.94ms
step:1756/1840 train_time:105290ms step_avg:59.96ms
step:1757/1840 train_time:105374ms step_avg:59.97ms
step:1758/1840 train_time:105463ms step_avg:59.99ms
step:1759/1840 train_time:105548ms step_avg:60.00ms
step:1760/1840 train_time:105636ms step_avg:60.02ms
step:1761/1840 train_time:105723ms step_avg:60.04ms
step:1762/1840 train_time:105813ms step_avg:60.05ms
step:1763/1840 train_time:105904ms step_avg:60.07ms
step:1764/1840 train_time:105995ms step_avg:60.09ms
step:1765/1840 train_time:106082ms step_avg:60.10ms
step:1766/1840 train_time:106170ms step_avg:60.12ms
step:1767/1840 train_time:106255ms step_avg:60.13ms
step:1768/1840 train_time:106343ms step_avg:60.15ms
step:1769/1840 train_time:106428ms step_avg:60.16ms
step:1770/1840 train_time:106516ms step_avg:60.18ms
step:1771/1840 train_time:106602ms step_avg:60.19ms
step:1772/1840 train_time:106690ms step_avg:60.21ms
step:1773/1840 train_time:106777ms step_avg:60.22ms
step:1774/1840 train_time:106866ms step_avg:60.24ms
step:1775/1840 train_time:106954ms step_avg:60.26ms
step:1776/1840 train_time:107044ms step_avg:60.27ms
step:1777/1840 train_time:107130ms step_avg:60.29ms
step:1778/1840 train_time:107218ms step_avg:60.30ms
step:1779/1840 train_time:107304ms step_avg:60.32ms
step:1780/1840 train_time:107393ms step_avg:60.33ms
step:1781/1840 train_time:107477ms step_avg:60.35ms
step:1782/1840 train_time:107565ms step_avg:60.36ms
step:1783/1840 train_time:107651ms step_avg:60.38ms
step:1784/1840 train_time:107740ms step_avg:60.39ms
step:1785/1840 train_time:107826ms step_avg:60.41ms
step:1786/1840 train_time:107918ms step_avg:60.42ms
step:1787/1840 train_time:108006ms step_avg:60.44ms
step:1788/1840 train_time:108094ms step_avg:60.46ms
step:1789/1840 train_time:108180ms step_avg:60.47ms
step:1790/1840 train_time:108267ms step_avg:60.48ms
step:1791/1840 train_time:108353ms step_avg:60.50ms
step:1792/1840 train_time:108442ms step_avg:60.51ms
step:1793/1840 train_time:108527ms step_avg:60.53ms
step:1794/1840 train_time:108616ms step_avg:60.54ms
step:1795/1840 train_time:108703ms step_avg:60.56ms
step:1796/1840 train_time:108792ms step_avg:60.57ms
step:1797/1840 train_time:108880ms step_avg:60.59ms
step:1798/1840 train_time:108969ms step_avg:60.61ms
step:1799/1840 train_time:109056ms step_avg:60.62ms
step:1800/1840 train_time:109145ms step_avg:60.64ms
step:1801/1840 train_time:109233ms step_avg:60.65ms
step:1802/1840 train_time:109321ms step_avg:60.67ms
step:1803/1840 train_time:109407ms step_avg:60.68ms
step:1804/1840 train_time:109496ms step_avg:60.70ms
step:1805/1840 train_time:109584ms step_avg:60.71ms
step:1806/1840 train_time:109672ms step_avg:60.73ms
step:1807/1840 train_time:109757ms step_avg:60.74ms
step:1808/1840 train_time:109847ms step_avg:60.76ms
step:1809/1840 train_time:109934ms step_avg:60.77ms
step:1810/1840 train_time:110024ms step_avg:60.79ms
step:1811/1840 train_time:110111ms step_avg:60.80ms
step:1812/1840 train_time:110198ms step_avg:60.82ms
step:1813/1840 train_time:110285ms step_avg:60.83ms
step:1814/1840 train_time:110374ms step_avg:60.85ms
step:1815/1840 train_time:110460ms step_avg:60.86ms
step:1816/1840 train_time:110549ms step_avg:60.87ms
step:1817/1840 train_time:110635ms step_avg:60.89ms
step:1818/1840 train_time:110724ms step_avg:60.90ms
step:1819/1840 train_time:110811ms step_avg:60.92ms
step:1820/1840 train_time:110900ms step_avg:60.93ms
step:1821/1840 train_time:110986ms step_avg:60.95ms
step:1822/1840 train_time:111076ms step_avg:60.96ms
step:1823/1840 train_time:111163ms step_avg:60.98ms
step:1824/1840 train_time:111251ms step_avg:60.99ms
step:1825/1840 train_time:111337ms step_avg:61.01ms
step:1826/1840 train_time:111426ms step_avg:61.02ms
step:1827/1840 train_time:111512ms step_avg:61.04ms
step:1828/1840 train_time:111602ms step_avg:61.05ms
step:1829/1840 train_time:111687ms step_avg:61.06ms
step:1830/1840 train_time:111777ms step_avg:61.08ms
step:1831/1840 train_time:111864ms step_avg:61.09ms
step:1832/1840 train_time:111952ms step_avg:61.11ms
step:1833/1840 train_time:112039ms step_avg:61.12ms
step:1834/1840 train_time:112128ms step_avg:61.14ms
step:1835/1840 train_time:112216ms step_avg:61.15ms
step:1836/1840 train_time:112305ms step_avg:61.17ms
step:1837/1840 train_time:112390ms step_avg:61.18ms
step:1838/1840 train_time:112478ms step_avg:61.20ms
step:1839/1840 train_time:112565ms step_avg:61.21ms
step:1840/1840 train_time:112654ms step_avg:61.22ms
step:1840/1840 val_loss:3.2777 train_time:112755ms step_avg:61.28ms
peak memory allocated: 28379 MiB reserved: 39958 MiB

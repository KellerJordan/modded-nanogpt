import os
import sys
with open(sys.argv[0]) as f:
    code = f.read() # read the code of this file ASAP, for logging
import uuid
import time
from dataclasses import dataclass
from functools import lru_cache
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch
torch.empty(1, device="cuda", requires_grad=True).backward() # prevents a bug on some systems
from torch import Tensor, nn
import torch.nn.functional as F
import torch.distributed as dist
# use of FlexAttention contributed by @KoszarskyB
from torch.nn.attention.flex_attention import BlockMask, flex_attention
torch._inductor.config.coordinate_descent_tuning = True # turn this off for a faster compile time (but slightly slower run)

# -----------------------------------------------------------------------------
# Custom operators : FP8 matmul for lm_head by @YouJiacheng

@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.mul(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.mul(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.t(),
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(1 / x_s, dtype=torch.float32),
            scale_b=x.new_tensor(1 / w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.t(), x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(1 / x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(1 / w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(1 / grad_s, dtype=torch.float32)
        grad_f8 = grad.mul(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.t().contiguous().t(),
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.t().contiguous(),
            grad_f8.t().contiguous().t(),
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).t()
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Muon optimizer

@torch.compile
def zeropower_via_newtonschulz5(G: Tensor, steps: int) -> Tensor:
    """
    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a
    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose
    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at
    zero even beyond the point where the iteration no longer converges all the way to one everywhere
    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T
    where S' is diagonal with S_{ii}' ~ Uniform(0.5, 1.5), which turns out not to hurt model
    performance at all relative to UV^T, where USV^T = G is the SVD.
    """
    assert G.ndim >= 2 # batched Muon implementation by @scottjmaddox, and put into practice in the record by @YouJiacheng
    a, b, c = (3.4445, -4.7750,  2.0315)
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) + 1e-7)
    # Perform the NS iterations
    for _ in range(steps):
        A = X @ X.mT
        B = b * A + c * A @ A # quintic computation strategy adapted from suggestion by @jxbz, @leloykun, and @YouJiacheng
        X = a * X + B @ X
    
    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Some warnings:
    - This optimizer assumes that all parameters passed in are 2D.
    - It should not be used for the embedding layer, the final fully connected layer, or any {0,1}-D
    parameters; those should all be optimized by a standard method (e.g., AdamW).
    - To use it with 4D convolutional filters, it works well to just flatten their last 3 dimensions.
    - We believe it is unlikely to work well for training with small batch size.
    - We believe it may not work well for finetuning pretrained models, but we haven"t tested this.
    - We have not yet tried this optimizer for training scenarios larger than NanoGPT (124M).

    Arguments:
        lr: The learning rate used by the internal SGD.
        momentum: The momentum used by the internal SGD.
        nesterov: Whether to use Nesterov-style momentum in the internal SGD. (recommended)
        ns_steps: The number of Newton-Schulz iteration steps to use.
    """
    def __init__(self, params, lr=0.02, momentum=0.95, nesterov=True, ns_steps=5, rank=0, world_size=1):
        self.rank = rank
        self.world_size = world_size
        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, ns_steps=ns_steps)
        params: list[Tensor] = [*params]
        assert all(isinstance(p, Tensor) for p in params)
        sizes = {p.numel() for p in params}
        def create_update_buffer(size: int):
            b = torch.empty(self.world_size, size, dtype=torch.bfloat16, device="cuda")
            return dict(update_buffer=b, update_buffer_views=[b[i] for i in range(self.world_size)])
        param_groups = [
            dict(params=[p for p in params if p.numel() == size], **create_update_buffer(size)) for size in sizes]
        super().__init__(param_groups, defaults)

    @torch.no_grad()
    def step(self):
        for group in self.param_groups:
            lr = group["lr"]
            momentum = group["momentum"]
            nesterov = group["nesterov"]
            ns_steps = group["ns_steps"]
            update_buffer = group["update_buffer"]
            update_buffer_views: list[Tensor] = group["update_buffer_views"]
            # generate weight updates in distributed fashion
            params: list[Tensor] = group["params"]
            handle = None
            params_world = None
            def update_prev(): # optimized Muon implementation contributed by @YouJiacheng
                if params_world is None:
                    return
                assert handle is not None
                handle.wait()
                for p_world, g_world in zip(params_world, update_buffer_views):
                    p_world.add_(
                        g_world.view_as(p_world),
                        alpha=-lr * max(1, p_world.size(-2) / p_world.size(-1)) ** 0.5,
                    )
            for base_i in range(len(params))[::self.world_size]:
                if base_i + self.rank < len(params):
                    p = params[base_i + self.rank]
                    g = p.grad
                    assert g is not None
                    state = self.state[p]
                    if "momentum_buffer" not in state:
                        state["momentum_buffer"] = torch.zeros_like(g)
                    buf: Tensor = state["momentum_buffer"]
                    buf.lerp_(g, 1 - momentum)
                    g = g.lerp_(buf, momentum) if nesterov else buf
                    g = zeropower_via_newtonschulz5(g, steps=ns_steps).flatten()
                else:
                    g = update_buffer_views[self.rank]
                update_prev() # async all_gather instead of sync all_reduce by @YouJiacheng
                handle = dist.all_gather_into_tensor(update_buffer, g, async_op=True)
                params_world = params[base_i : base_i + self.world_size]
            update_prev()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8: bool = False, x_scale: float = 1.0, w_scale: float = 1.0, grad_scale: float = 1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_scale = x_scale
        self.w_scale = w_scale
        self.grad_scale = grad_scale

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_scale, w_s=self.w_scale, grad_s=self.grad_scale)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

class Rotary(nn.Module):
    def __init__(self, dim: int, max_seq_len: int):
        super().__init__()
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=dim//4, dtype=torch.float32)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(dim//4)])
        t = torch.arange(max_seq_len, dtype=torch.float32)
        theta = torch.einsum("i,j -> ij", t, angular_freq)
        self.cos = nn.Buffer(theta.cos(), persistent=False)
        self.sin = nn.Buffer(theta.sin(), persistent=False)

    def forward(self, x_BTHD: Tensor):
        assert self.cos.size(0) >= x_BTHD.size(-3)
        cos, sin = self.cos[None, :x_BTHD.size(-3), None, :], self.sin[None, :x_BTHD.size(-3), None, :]
        x1, x2 = x_BTHD.to(dtype=torch.float32).chunk(2, dim=-1)
        y1 = x1 * cos + x2 * sin
        y2 = x1 * (-sin) + x2 * cos
        return torch.cat((y1, y2), 3).type_as(x_BTHD)

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, num_heads: int, max_seq_len: int, head_dim=128):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        hdim = num_heads * head_dim
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        self.qkv_w = nn.Parameter(torch.empty(3, hdim, dim).uniform_(-bound, bound))
        self.lambdas = nn.Parameter(torch.tensor([0.5, 0.5]))
        self.rotary = Rotary(head_dim, max_seq_len)
        self.c_proj = CastedLinear(hdim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977
        # scale the attention logits by given constant, instead of the default head_dim**-0.5, by @leloykun
        # inspired by learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.12

    def forward(self, x: Tensor, ve: Tensor | None, block_mask: BlockMask):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "Must use batch size = 1 for FlexAttention"
        q, k, v = F.linear(x, self.qkv_w.flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = self.rotary(q), self.rotary(k)
        if ve is not None:
            v = self.lambdas[0] * v + self.lambdas[1] * ve.view_as(v) # @KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = self.lambdas[0] * v
        y = flex_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), block_mask=block_mask, scale=self.attn_scale).transpose(1, 2)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = self.c_proj(y)
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        self.c_fc = CastedLinear(dim, hdim)
        self.c_proj = CastedLinear(hdim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = self.c_fc(x)
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = self.c_proj(x)
        return x

class Block(nn.Module):
    def __init__(self, dim: int, num_heads: int, layer_idx: int, max_seq_len: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, num_heads, max_seq_len) if layer_idx != 7 else None
        self.mlp = MLP(dim)
        self.lambdas = nn.Parameter(torch.tensor([1., 0.]))

    def forward(self, x: Tensor, ve: Tensor | None, x0: Tensor, block_mask: BlockMask):
        x = self.lambdas[0] * x + self.lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), ve, block_mask)
        x = x + self.mlp(norm(x))
        return x

class ValueEmbedding(nn.Module):
    def __init__(self, vocab_size: int, embedding_dim: int, num_layers: int, num_embeddings: int = 3):
        super().__init__()
        self.num_layers = num_layers
        self.num_embeddings = num_embeddings
        self.embed = nn.ModuleList([nn.Embedding(vocab_size, embedding_dim) for _ in range(num_embeddings)])

    def forward(self, input_seq: Tensor) -> list[Tensor | None]:
        ve = [emb(input_seq) for emb in self.embed]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        ve = [ve[0], ve[1], ve[2]] + [None] * (self.num_layers - 2 * self.num_embeddings) + [ve[0], ve[1], ve[2]]
        return ve

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, model_dim: int, max_seq_len: int):
        super().__init__()
        self.embed = nn.Embedding(vocab_size, model_dim)
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        self.value_embeds = ValueEmbedding(vocab_size, model_dim, num_layers)
        self.blocks = nn.ModuleList([Block(model_dim, num_heads, layer_idx, max_seq_len) for layer_idx in range(num_layers)])
        # U-net design by @brendanh0gan
        self.num_encoder_layers = num_layers // 2 # Half of the layers for encoder
        self.num_decoder_layers = num_layers - self.num_encoder_layers # Remaining for decoder
        # Add learnable skip connection weights for decoder layers
        self.skip_weights = nn.Parameter(torch.ones(self.num_decoder_layers))
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        self.lm_head = CastedLinear(model_dim, next_multiple_of_n(vocab_size, n=128), use_fp8=True, x_scale=2.0, w_scale=2.0**9, grad_scale=2.0**19)
        self.lm_head.weight.detach().zero_() # @Grad62304977

    def create_block_masks(self, input_seq: Tensor, sliding_window_num_blocks: Tensor):
        BLOCK_SIZE = 128
        docs = (input_seq == 50256).cumsum(0)

        def document_causal(b, h, q_idx, kv_idx):
            causal_mask = q_idx >= kv_idx
            document_mask = docs[q_idx] == docs[kv_idx]
            return causal_mask & document_mask

        def dense_to_ordered(dense_mask: Tensor):
            num_blocks = dense_mask.sum(dim=-1, dtype=torch.int32)
            indices = dense_mask.argsort(dim=-1, descending=False, stable=True).flip(-1).to(torch.int32)
            return num_blocks[None, None].contiguous(), indices[None, None].contiguous()

        # manual block mask creation by @YouJiacheng
        assert len(input_seq) % BLOCK_SIZE == 0
        NUM_BLOCKS = len(input_seq) // BLOCK_SIZE
        block_idx = torch.arange(NUM_BLOCKS, dtype=torch.int32, device="cuda")
        any_causal_bm = block_idx[:, None] >= block_idx
        all_causal_bm = block_idx[:, None] > block_idx
        docs_low = docs.view(-1, BLOCK_SIZE)[:, 0].contiguous()
        docs_high = docs.view(-1, BLOCK_SIZE)[:, -1].contiguous()
        any_document_bm = (docs_low[:, None] <= docs_high) & (docs_high[:, None] >= docs_low)
        all_document_bm = (docs_low[:, None] == docs_high) & (docs_high[:, None] == docs_low)
        any_bm = any_causal_bm & any_document_bm
        all_bm = all_causal_bm & all_document_bm
        partial_kv_num_blocks, partial_kv_indices = dense_to_ordered(any_bm & ~all_bm)
        full_kv_num_blocks, full_kv_indices = dense_to_ordered(all_bm)
        def build_bm(sw_num_blocks: Tensor) -> BlockMask:
            return BlockMask.from_kv_blocks(
                torch.clamp_max(partial_kv_num_blocks, torch.clamp_min(sw_num_blocks - full_kv_num_blocks, 1)),
                partial_kv_indices,
                torch.clamp_max(full_kv_num_blocks, sw_num_blocks - 1),
                full_kv_indices,
                BLOCK_SIZE=BLOCK_SIZE,
                mask_mod=document_causal,
            )
        # Long-short SWA block masks by @leloykun & @YouJiacheng, adapated from suggestion by @Grad62304977, following Gemma 2 paper
        return build_bm(sliding_window_num_blocks), build_bm(sliding_window_num_blocks // 2)

    def forward(self, input_seq: Tensor, target_seq: Tensor, sliding_window_num_blocks: Tensor):
        assert input_seq.ndim == 1

        long_bm, short_bm = self.create_block_masks(input_seq, sliding_window_num_blocks)

        x = x0 = norm(self.embed(input_seq)[None]) # use of norm here by @Grad62304977
        ve = self.value_embeds(input_seq)
        assert len(ve) == len(self.blocks)
        ve_enc, ve_dec = ve[:self.num_encoder_layers], ve[self.num_encoder_layers:]
        assert len(ve_enc) == self.num_encoder_layers and len(ve_dec) == self.num_decoder_layers

        # Store outputs for U-Net skip connections
        skip_connections = []
        # Encoder pass - process only the first half of the blocks
        block_masks = [long_bm, short_bm, short_bm, short_bm, long_bm, short_bm]
        assert len(block_masks) == self.num_encoder_layers
        for i in range(self.num_encoder_layers):
            x = self.blocks[i](x, ve_enc[i], x0, block_masks[i])
            skip_connections.append(x)
        # Decoder pass - process the remaining blocks with weighted skip connections
        block_masks.reverse()
        assert len(block_masks) == self.num_decoder_layers
        for i in range(self.num_decoder_layers):
            x = x + self.skip_weights[i] * skip_connections.pop()
            x = self.blocks[self.num_encoder_layers + i](x, ve_dec[i], x0, block_masks[i])
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits.float() / 7.5)
        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), target_seq)
        return loss

# -----------------------------------------------------------------------------
# Our own simple Distributed Data Loader

def _load_data_shard(file: Path):
    header = torch.from_file(f"{file}", False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

def distributed_data_generator(filename_pattern: str, batch_size: int, rank : int, world_size : int):
    files = sorted(Path.cwd().glob(filename_pattern))
    assert batch_size % world_size == 0
    local_batch_size = batch_size // world_size
    file_iter = iter(files) # use itertools.cycle(files) instead if you want to do multi-epoch training
    tokens, pos = _load_data_shard(next(file_iter)), 0
    while True:
        if pos + batch_size + 1 >= len(tokens):
            tokens, pos = _load_data_shard(next(file_iter)), 0
        buf = tokens[pos + rank * local_batch_size:][:local_batch_size + 1]
        inputs = buf[:-1].to(device="cuda", dtype=torch.int32, non_blocking=True) # no sync on host side;
        targets = buf[1:].to(device="cuda", dtype=torch.int64, non_blocking=True) # H2D in another stream isn"t helpful.
        pos += batch_size
        yield inputs, targets

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # optimization
    num_iterations = 1770 # number of iterations to run
    cooldown_frac = 0.4 # fraction of training spent cooling down the learning rate
    # evaluation and logging
    val_loss_every = 125 # every how many steps to evaluate val loss? 0 for only at the end
    # implementation
    seq_len = 48*1024 # FlexAttention sequence length
    val_seq_len = 4*64*1024 # FlexAttention sequence length for validation
    save_checkpoint = False
args = Hyperparameters()

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = uuid.uuid4()
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

# load data
train_batch_size = world_size * args.seq_len
train_loader = distributed_data_generator(args.train_files, train_batch_size, rank, world_size)

model: nn.Module = GPT(vocab_size=50257, num_layers=12, num_heads=6, model_dim=768, max_seq_len=max(args.seq_len, args.val_seq_len)).cuda()
for m in model.modules():
    if isinstance(m, nn.Embedding):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]

# init the optimizer(s)
adam_params = [dict(params=head_params, lr=0.008), dict(params=embed_params, lr=0.6), dict(params=scalar_params, lr=0.04)]
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = torch.optim.Adam(adam_params, betas=(0.8, 0.95), eps=1e-10, fused=True)
optimizer2 = Muon(hidden_matrix_params, lr=0.05, momentum=0.95, rank=rank, world_size=world_size)
optimizers = [optimizer1, optimizer2]

# learning rate schedule: stable then decay
def get_lr(step: int):
    t = 1 - step / args.num_iterations # time remaining in training
    assert 1 >= t >= 0
    w = min(t / args.cooldown_frac, 1.0) # 1 -> 0
    return w * 1.0 + (1 - w) * 0.1
schedulers = [torch.optim.lr_scheduler.LambdaLR(opt, get_lr) for opt in optimizers]
@lru_cache(1)
def sw_num_blks(window_size: int):
    return torch.tensor(window_size // 128, dtype=torch.int32, pin_memory=True).cuda(non_blocking=True)

model: nn.Module = torch.compile(model, dynamic=False)

training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    # This effectively ignores timing first 10 steps, which are slower for weird reasons.
    # Alternately, and slightly more correctly in terms of benchmarking, we could do 10
    # steps with dummy data first, and then re-initialize the model and reset the loader.
    if step == 10:
        training_time_ms = 0
        t0 = time.perf_counter()
    timed_steps = float("nan") if step <= 11 else (step - 10) + 1 # <= 11 to avoid bug in val

    # Linearly increase the block-wise sliding window size over training 128 -> 1792:
    # increase by @fernbear.bsky.social; block-wise by @YouJiacheng
    window_size = next_multiple_of_n(1728 * step / train_steps, n=128)

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        val_batch_size = world_size * args.val_seq_len
        assert args.val_tokens % val_batch_size == 0
        val_steps = args.val_tokens // val_batch_size
        val_loader = distributed_data_generator(args.val_files, val_batch_size , rank, world_size)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                x, y = next(val_loader)
                val_loss += model(x, y, sw_num_blks(window_size))
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/(timed_steps-1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    inputs, targets = next(train_loader)
    for input_seq, target_seq in zip(inputs.split(args.seq_len), targets.split(args.seq_len)):
        model(input_seq, target_seq, sw_num_blks(window_size)).backward()
    for param in model.parameters():
        dist.all_reduce(param.grad, op=dist.ReduceOp.AVG)
    # momentum warmup for Muon
    frac = min(step / 300, 1)
    for group in optimizer2.param_groups:
        group["momentum"] = (1 - frac) * 0.85 + frac * 0.95
    # step the optimizers and schedulers
    for opt, sched in zip(optimizers, schedulers):
        opt.step()
        sched.step()
    # null the gradients
    model.zero_grad(set_to_none=True)
    # logging
    approx_time = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms", console=True)

print0(
    f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
    f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB",
    console=True,
)
dist.destroy_process_group()

====================================================================================================
Running Python 3.10.12 (main, Sep 11 2024, 15:47:36) [GCC 11.4.0]
Running PyTorch 2.7.0.dev20250125+cu126 compiled for CUDA 12.6
Mon Jan 27 16:55:02 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.183.06             Driver Version: 535.183.06   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:19:00.0 Off |                    0 |
| N/A   38C    P0             121W / 700W |   7713MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:3B:00.0 Off |                    0 |
| N/A   29C    P0             113W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:4C:00.0 Off |                    0 |
| N/A   28C    P0             114W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   38C    P0             118W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:9B:00.0 Off |                    0 |
| N/A   38C    P0             120W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:BB:00.0 Off |                    0 |
| N/A   30C    P0             114W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:CB:00.0 Off |                    0 |
| N/A   37C    P0             116W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:DB:00.0 Off |                    0 |
| N/A   29C    P0             113W / 700W |   3211MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
step:0/1770 val_loss:10.8258 train_time:0ms step_avg:nanms
step:1/1770 train_time:24286ms step_avg:nanms
step:2/1770 train_time:24883ms step_avg:nanms
step:3/1770 train_time:24977ms step_avg:nanms
step:4/1770 train_time:25070ms step_avg:nanms
step:5/1770 train_time:25164ms step_avg:nanms
step:6/1770 train_time:25257ms step_avg:nanms
step:7/1770 train_time:25351ms step_avg:nanms
step:8/1770 train_time:25445ms step_avg:nanms
step:9/1770 train_time:25539ms step_avg:nanms
step:10/1770 train_time:25632ms step_avg:nanms
step:11/1770 train_time:94ms step_avg:nanms
step:12/1770 train_time:188ms step_avg:nanms
step:13/1770 train_time:282ms step_avg:94.11ms
step:14/1770 train_time:377ms step_avg:94.19ms
step:15/1770 train_time:471ms step_avg:94.30ms
step:16/1770 train_time:566ms step_avg:94.30ms
step:17/1770 train_time:661ms step_avg:94.40ms
step:18/1770 train_time:754ms step_avg:94.24ms
step:19/1770 train_time:848ms step_avg:94.25ms
step:20/1770 train_time:942ms step_avg:94.23ms
step:21/1770 train_time:1036ms step_avg:94.20ms
step:22/1770 train_time:1130ms step_avg:94.19ms
step:23/1770 train_time:1224ms step_avg:94.19ms
step:24/1770 train_time:1319ms step_avg:94.21ms
step:25/1770 train_time:1413ms step_avg:94.20ms
step:26/1770 train_time:1507ms step_avg:94.19ms
step:27/1770 train_time:1601ms step_avg:94.20ms
step:28/1770 train_time:1695ms step_avg:94.17ms
step:29/1770 train_time:1789ms step_avg:94.18ms
step:30/1770 train_time:1883ms step_avg:94.15ms
step:31/1770 train_time:1977ms step_avg:94.15ms
step:32/1770 train_time:2071ms step_avg:94.14ms
step:33/1770 train_time:2165ms step_avg:94.14ms
step:34/1770 train_time:2259ms step_avg:94.12ms
step:35/1770 train_time:2353ms step_avg:94.14ms
step:36/1770 train_time:2448ms step_avg:94.16ms
step:37/1770 train_time:2542ms step_avg:94.16ms
step:38/1770 train_time:2637ms step_avg:94.16ms
step:39/1770 train_time:2731ms step_avg:94.16ms
step:40/1770 train_time:2825ms step_avg:94.16ms
step:41/1770 train_time:2919ms step_avg:94.15ms
step:42/1770 train_time:3013ms step_avg:94.14ms
step:43/1770 train_time:3107ms step_avg:94.14ms
step:44/1770 train_time:3201ms step_avg:94.13ms
step:45/1770 train_time:3294ms step_avg:94.13ms
step:46/1770 train_time:3389ms step_avg:94.13ms
step:47/1770 train_time:3483ms step_avg:94.14ms
step:48/1770 train_time:3577ms step_avg:94.13ms
step:49/1770 train_time:3671ms step_avg:94.12ms
step:50/1770 train_time:3765ms step_avg:94.12ms
step:51/1770 train_time:3859ms step_avg:94.12ms
step:52/1770 train_time:3953ms step_avg:94.11ms
step:53/1770 train_time:4047ms step_avg:94.11ms
step:54/1770 train_time:4141ms step_avg:94.12ms
step:55/1770 train_time:4235ms step_avg:94.12ms
step:56/1770 train_time:4330ms step_avg:94.12ms
step:57/1770 train_time:4423ms step_avg:94.11ms
step:58/1770 train_time:4517ms step_avg:94.11ms
step:59/1770 train_time:4612ms step_avg:94.12ms
step:60/1770 train_time:4706ms step_avg:94.11ms
step:61/1770 train_time:4799ms step_avg:94.11ms
step:62/1770 train_time:4893ms step_avg:94.10ms
step:63/1770 train_time:4988ms step_avg:94.11ms
step:64/1770 train_time:5081ms step_avg:94.10ms
step:65/1770 train_time:5175ms step_avg:94.10ms
step:66/1770 train_time:5270ms step_avg:94.10ms
step:67/1770 train_time:5364ms step_avg:94.10ms
step:68/1770 train_time:5458ms step_avg:94.10ms
step:69/1770 train_time:5552ms step_avg:94.10ms
step:70/1770 train_time:5647ms step_avg:94.11ms
step:71/1770 train_time:5740ms step_avg:94.11ms
step:72/1770 train_time:5835ms step_avg:94.11ms
step:73/1770 train_time:5928ms step_avg:94.10ms
step:74/1770 train_time:6022ms step_avg:94.09ms
step:75/1770 train_time:6116ms step_avg:94.09ms
step:76/1770 train_time:6210ms step_avg:94.09ms
step:77/1770 train_time:6303ms step_avg:94.08ms
step:78/1770 train_time:6397ms step_avg:94.08ms
step:79/1770 train_time:6491ms step_avg:94.08ms
step:80/1770 train_time:6585ms step_avg:94.08ms
step:81/1770 train_time:6679ms step_avg:94.07ms
step:82/1770 train_time:6773ms step_avg:94.07ms
step:83/1770 train_time:6867ms step_avg:94.07ms
step:84/1770 train_time:6961ms step_avg:94.07ms
step:85/1770 train_time:7055ms step_avg:94.07ms
step:86/1770 train_time:7150ms step_avg:94.07ms
step:87/1770 train_time:7244ms step_avg:94.08ms
step:88/1770 train_time:7338ms step_avg:94.07ms
step:89/1770 train_time:7432ms step_avg:94.07ms
step:90/1770 train_time:7525ms step_avg:94.07ms
step:91/1770 train_time:7619ms step_avg:94.06ms
step:92/1770 train_time:7713ms step_avg:94.06ms
step:93/1770 train_time:7807ms step_avg:94.06ms
step:94/1770 train_time:7901ms step_avg:94.06ms
step:95/1770 train_time:7995ms step_avg:94.06ms
step:96/1770 train_time:8089ms step_avg:94.06ms
step:97/1770 train_time:8183ms step_avg:94.06ms
step:98/1770 train_time:8277ms step_avg:94.05ms
step:99/1770 train_time:8372ms step_avg:94.07ms
step:100/1770 train_time:8465ms step_avg:94.06ms
step:101/1770 train_time:8559ms step_avg:94.06ms
step:102/1770 train_time:8653ms step_avg:94.05ms
step:103/1770 train_time:8747ms step_avg:94.06ms
step:104/1770 train_time:8840ms step_avg:94.05ms
step:105/1770 train_time:8935ms step_avg:94.05ms
step:106/1770 train_time:9028ms step_avg:94.05ms
step:107/1770 train_time:9122ms step_avg:94.04ms
step:108/1770 train_time:9216ms step_avg:94.04ms
step:109/1770 train_time:9311ms step_avg:94.05ms
step:110/1770 train_time:9405ms step_avg:94.05ms
step:111/1770 train_time:9499ms step_avg:94.05ms
step:112/1770 train_time:9593ms step_avg:94.05ms
step:113/1770 train_time:9687ms step_avg:94.05ms
step:114/1770 train_time:9781ms step_avg:94.04ms
step:115/1770 train_time:9874ms step_avg:94.04ms
step:116/1770 train_time:9968ms step_avg:94.04ms
step:117/1770 train_time:10062ms step_avg:94.04ms
step:118/1770 train_time:10156ms step_avg:94.04ms
step:119/1770 train_time:10250ms step_avg:94.03ms
step:120/1770 train_time:10344ms step_avg:94.03ms
step:121/1770 train_time:10438ms step_avg:94.04ms
step:122/1770 train_time:10532ms step_avg:94.04ms
step:123/1770 train_time:10627ms step_avg:94.04ms
step:124/1770 train_time:10721ms step_avg:94.05ms
step:125/1770 train_time:10815ms step_avg:94.04ms
step:125/1770 val_loss:4.6560 train_time:10907ms step_avg:94.84ms
step:126/1770 train_time:10930ms step_avg:94.23ms
step:127/1770 train_time:11010ms step_avg:94.10ms
step:128/1770 train_time:11106ms step_avg:94.12ms
step:129/1770 train_time:11203ms step_avg:94.15ms
step:130/1770 train_time:11298ms step_avg:94.15ms
step:131/1770 train_time:11392ms step_avg:94.15ms
step:132/1770 train_time:11485ms step_avg:94.14ms
step:133/1770 train_time:11579ms step_avg:94.14ms
step:134/1770 train_time:11673ms step_avg:94.14ms
step:135/1770 train_time:11768ms step_avg:94.14ms
step:136/1770 train_time:11862ms step_avg:94.14ms
step:137/1770 train_time:11956ms step_avg:94.14ms
step:138/1770 train_time:12051ms step_avg:94.15ms
step:139/1770 train_time:12148ms step_avg:94.17ms
step:140/1770 train_time:12243ms step_avg:94.18ms
step:141/1770 train_time:12337ms step_avg:94.18ms
step:142/1770 train_time:12432ms step_avg:94.18ms
step:143/1770 train_time:12527ms step_avg:94.19ms
step:144/1770 train_time:12621ms step_avg:94.19ms
step:145/1770 train_time:12715ms step_avg:94.19ms
step:146/1770 train_time:12810ms step_avg:94.19ms
step:147/1770 train_time:12904ms step_avg:94.19ms
step:148/1770 train_time:12998ms step_avg:94.19ms
step:149/1770 train_time:13093ms step_avg:94.19ms
step:150/1770 train_time:13188ms step_avg:94.20ms
step:151/1770 train_time:13283ms step_avg:94.20ms
step:152/1770 train_time:13377ms step_avg:94.20ms
step:153/1770 train_time:13472ms step_avg:94.21ms
step:154/1770 train_time:13566ms step_avg:94.21ms
step:155/1770 train_time:13662ms step_avg:94.22ms
step:156/1770 train_time:13756ms step_avg:94.22ms
step:157/1770 train_time:13850ms step_avg:94.22ms
step:158/1770 train_time:13945ms step_avg:94.22ms
step:159/1770 train_time:14039ms step_avg:94.22ms
step:160/1770 train_time:14134ms step_avg:94.23ms
step:161/1770 train_time:14229ms step_avg:94.23ms
step:162/1770 train_time:14323ms step_avg:94.23ms
step:163/1770 train_time:14418ms step_avg:94.24ms
step:164/1770 train_time:14514ms step_avg:94.24ms
step:165/1770 train_time:14609ms step_avg:94.25ms
step:166/1770 train_time:14704ms step_avg:94.25ms
step:167/1770 train_time:14798ms step_avg:94.26ms
step:168/1770 train_time:14893ms step_avg:94.26ms
step:169/1770 train_time:14987ms step_avg:94.26ms
step:170/1770 train_time:15082ms step_avg:94.26ms
step:171/1770 train_time:15176ms step_avg:94.26ms
step:172/1770 train_time:15271ms step_avg:94.27ms
step:173/1770 train_time:15366ms step_avg:94.27ms
step:174/1770 train_time:15461ms step_avg:94.28ms
step:175/1770 train_time:15556ms step_avg:94.28ms
step:176/1770 train_time:15651ms step_avg:94.28ms
step:177/1770 train_time:15746ms step_avg:94.29ms
step:178/1770 train_time:15840ms step_avg:94.28ms
step:179/1770 train_time:15934ms step_avg:94.29ms
step:180/1770 train_time:16029ms step_avg:94.29ms
step:181/1770 train_time:16124ms step_avg:94.29ms
step:182/1770 train_time:16218ms step_avg:94.29ms
step:183/1770 train_time:16313ms step_avg:94.29ms
step:184/1770 train_time:16408ms step_avg:94.30ms
step:185/1770 train_time:16502ms step_avg:94.30ms
step:186/1770 train_time:16597ms step_avg:94.30ms
step:187/1770 train_time:16691ms step_avg:94.30ms
step:188/1770 train_time:16786ms step_avg:94.31ms
step:189/1770 train_time:16881ms step_avg:94.31ms
step:190/1770 train_time:16976ms step_avg:94.31ms
step:191/1770 train_time:17071ms step_avg:94.31ms
step:192/1770 train_time:17165ms step_avg:94.32ms
step:193/1770 train_time:17260ms step_avg:94.32ms
step:194/1770 train_time:17355ms step_avg:94.32ms
step:195/1770 train_time:17449ms step_avg:94.32ms
step:196/1770 train_time:17544ms step_avg:94.32ms
step:197/1770 train_time:17638ms step_avg:94.32ms
step:198/1770 train_time:17733ms step_avg:94.33ms
step:199/1770 train_time:17829ms step_avg:94.33ms
step:200/1770 train_time:17923ms step_avg:94.33ms
step:201/1770 train_time:18017ms step_avg:94.33ms
step:202/1770 train_time:18113ms step_avg:94.34ms
step:203/1770 train_time:18207ms step_avg:94.34ms
step:204/1770 train_time:18301ms step_avg:94.34ms
step:205/1770 train_time:18396ms step_avg:94.34ms
step:206/1770 train_time:18491ms step_avg:94.34ms
step:207/1770 train_time:18586ms step_avg:94.34ms
step:208/1770 train_time:18681ms step_avg:94.35ms
step:209/1770 train_time:18775ms step_avg:94.35ms
step:210/1770 train_time:18871ms step_avg:94.35ms
step:211/1770 train_time:18965ms step_avg:94.35ms
step:212/1770 train_time:19059ms step_avg:94.35ms
step:213/1770 train_time:19154ms step_avg:94.36ms
step:214/1770 train_time:19249ms step_avg:94.36ms
step:215/1770 train_time:19344ms step_avg:94.36ms
step:216/1770 train_time:19438ms step_avg:94.36ms
step:217/1770 train_time:19533ms step_avg:94.36ms
step:218/1770 train_time:19628ms step_avg:94.37ms
step:219/1770 train_time:19722ms step_avg:94.36ms
step:220/1770 train_time:19817ms step_avg:94.37ms
step:221/1770 train_time:19911ms step_avg:94.37ms
step:222/1770 train_time:20006ms step_avg:94.37ms
step:223/1770 train_time:20100ms step_avg:94.37ms
step:224/1770 train_time:20195ms step_avg:94.37ms
step:225/1770 train_time:20289ms step_avg:94.37ms
step:226/1770 train_time:20384ms step_avg:94.37ms
step:227/1770 train_time:20478ms step_avg:94.37ms
step:228/1770 train_time:20573ms step_avg:94.37ms
step:229/1770 train_time:20668ms step_avg:94.37ms
step:230/1770 train_time:20762ms step_avg:94.37ms
step:231/1770 train_time:20857ms step_avg:94.37ms
step:232/1770 train_time:20952ms step_avg:94.38ms
step:233/1770 train_time:21047ms step_avg:94.38ms
step:234/1770 train_time:21141ms step_avg:94.38ms
step:235/1770 train_time:21236ms step_avg:94.38ms
step:236/1770 train_time:21330ms step_avg:94.38ms
step:237/1770 train_time:21425ms step_avg:94.38ms
step:238/1770 train_time:21519ms step_avg:94.38ms
step:239/1770 train_time:21614ms step_avg:94.38ms
step:240/1770 train_time:21709ms step_avg:94.38ms
step:241/1770 train_time:21803ms step_avg:94.38ms
step:242/1770 train_time:21898ms step_avg:94.39ms
step:243/1770 train_time:21992ms step_avg:94.39ms
step:244/1770 train_time:22087ms step_avg:94.39ms
step:245/1770 train_time:22182ms step_avg:94.39ms
step:246/1770 train_time:22276ms step_avg:94.39ms
step:247/1770 train_time:22371ms step_avg:94.39ms
step:248/1770 train_time:22466ms step_avg:94.40ms
step:249/1770 train_time:22560ms step_avg:94.40ms
step:250/1770 train_time:22655ms step_avg:94.40ms
step:250/1770 val_loss:4.1008 train_time:22748ms step_avg:94.78ms
step:251/1770 train_time:22770ms step_avg:94.48ms
step:252/1770 train_time:22850ms step_avg:94.42ms
step:253/1770 train_time:22948ms step_avg:94.44ms
step:254/1770 train_time:23043ms step_avg:94.44ms
step:255/1770 train_time:23138ms step_avg:94.44ms
step:256/1770 train_time:23232ms step_avg:94.44ms
step:257/1770 train_time:23326ms step_avg:94.44ms
step:258/1770 train_time:23420ms step_avg:94.44ms
step:259/1770 train_time:23515ms step_avg:94.44ms
step:260/1770 train_time:23609ms step_avg:94.44ms
step:261/1770 train_time:23703ms step_avg:94.44ms
step:262/1770 train_time:23799ms step_avg:94.44ms
step:263/1770 train_time:23895ms step_avg:94.45ms
step:264/1770 train_time:23990ms step_avg:94.45ms
step:265/1770 train_time:24085ms step_avg:94.45ms
step:266/1770 train_time:24181ms step_avg:94.46ms
step:267/1770 train_time:24277ms step_avg:94.46ms
step:268/1770 train_time:24371ms step_avg:94.46ms
step:269/1770 train_time:24466ms step_avg:94.46ms
step:270/1770 train_time:24561ms step_avg:94.46ms
step:271/1770 train_time:24656ms step_avg:94.47ms
step:272/1770 train_time:24752ms step_avg:94.47ms
step:273/1770 train_time:24847ms step_avg:94.47ms
step:274/1770 train_time:24942ms step_avg:94.48ms
step:275/1770 train_time:25038ms step_avg:94.48ms
step:276/1770 train_time:25133ms step_avg:94.49ms
step:277/1770 train_time:25228ms step_avg:94.49ms
step:278/1770 train_time:25323ms step_avg:94.49ms
step:279/1770 train_time:25419ms step_avg:94.49ms
step:280/1770 train_time:25516ms step_avg:94.50ms
step:281/1770 train_time:25611ms step_avg:94.51ms
step:282/1770 train_time:25706ms step_avg:94.51ms
step:283/1770 train_time:25802ms step_avg:94.51ms
step:284/1770 train_time:25896ms step_avg:94.51ms
step:285/1770 train_time:25991ms step_avg:94.51ms
step:286/1770 train_time:26086ms step_avg:94.52ms
step:287/1770 train_time:26181ms step_avg:94.52ms
step:288/1770 train_time:26276ms step_avg:94.52ms
step:289/1770 train_time:26371ms step_avg:94.52ms
step:290/1770 train_time:26466ms step_avg:94.52ms
step:291/1770 train_time:26561ms step_avg:94.52ms
step:292/1770 train_time:26657ms step_avg:94.53ms
step:293/1770 train_time:26752ms step_avg:94.53ms
step:294/1770 train_time:26846ms step_avg:94.53ms
step:295/1770 train_time:26941ms step_avg:94.53ms
step:296/1770 train_time:27036ms step_avg:94.53ms
step:297/1770 train_time:27131ms step_avg:94.53ms
step:298/1770 train_time:27226ms step_avg:94.54ms
step:299/1770 train_time:27321ms step_avg:94.54ms
step:300/1770 train_time:27417ms step_avg:94.54ms
step:301/1770 train_time:27512ms step_avg:94.54ms
step:302/1770 train_time:27607ms step_avg:94.54ms
step:303/1770 train_time:27702ms step_avg:94.55ms
step:304/1770 train_time:27798ms step_avg:94.55ms
step:305/1770 train_time:27893ms step_avg:94.55ms
step:306/1770 train_time:27988ms step_avg:94.55ms
step:307/1770 train_time:28083ms step_avg:94.55ms
step:308/1770 train_time:28177ms step_avg:94.56ms
step:309/1770 train_time:28272ms step_avg:94.56ms
step:310/1770 train_time:28367ms step_avg:94.56ms
step:311/1770 train_time:28463ms step_avg:94.56ms
step:312/1770 train_time:28558ms step_avg:94.56ms
step:313/1770 train_time:28654ms step_avg:94.57ms
step:314/1770 train_time:28748ms step_avg:94.57ms
step:315/1770 train_time:28843ms step_avg:94.57ms
step:316/1770 train_time:28939ms step_avg:94.57ms
step:317/1770 train_time:29034ms step_avg:94.57ms
step:318/1770 train_time:29128ms step_avg:94.57ms
step:319/1770 train_time:29223ms step_avg:94.57ms
step:320/1770 train_time:29318ms step_avg:94.57ms
step:321/1770 train_time:29413ms step_avg:94.58ms
step:322/1770 train_time:29508ms step_avg:94.58ms
step:323/1770 train_time:29603ms step_avg:94.58ms
step:324/1770 train_time:29698ms step_avg:94.58ms
step:325/1770 train_time:29792ms step_avg:94.58ms
step:326/1770 train_time:29887ms step_avg:94.58ms
step:327/1770 train_time:29982ms step_avg:94.58ms
step:328/1770 train_time:30077ms step_avg:94.58ms
step:329/1770 train_time:30173ms step_avg:94.58ms
step:330/1770 train_time:30267ms step_avg:94.58ms
step:331/1770 train_time:30362ms step_avg:94.59ms
step:332/1770 train_time:30457ms step_avg:94.59ms
step:333/1770 train_time:30552ms step_avg:94.59ms
step:334/1770 train_time:30647ms step_avg:94.59ms
step:335/1770 train_time:30742ms step_avg:94.59ms
step:336/1770 train_time:30837ms step_avg:94.59ms
step:337/1770 train_time:30932ms step_avg:94.59ms
step:338/1770 train_time:31027ms step_avg:94.59ms
step:339/1770 train_time:31122ms step_avg:94.60ms
step:340/1770 train_time:31217ms step_avg:94.60ms
step:341/1770 train_time:31312ms step_avg:94.60ms
step:342/1770 train_time:31407ms step_avg:94.60ms
step:343/1770 train_time:31502ms step_avg:94.60ms
step:344/1770 train_time:31598ms step_avg:94.60ms
step:345/1770 train_time:31694ms step_avg:94.61ms
step:346/1770 train_time:31788ms step_avg:94.61ms
step:347/1770 train_time:31883ms step_avg:94.61ms
step:348/1770 train_time:31979ms step_avg:94.61ms
step:349/1770 train_time:32074ms step_avg:94.61ms
step:350/1770 train_time:32169ms step_avg:94.62ms
step:351/1770 train_time:32264ms step_avg:94.62ms
step:352/1770 train_time:32359ms step_avg:94.62ms
step:353/1770 train_time:32454ms step_avg:94.62ms
step:354/1770 train_time:32549ms step_avg:94.62ms
step:355/1770 train_time:32644ms step_avg:94.62ms
step:356/1770 train_time:32740ms step_avg:94.62ms
step:357/1770 train_time:32835ms step_avg:94.63ms
step:358/1770 train_time:32929ms step_avg:94.62ms
step:359/1770 train_time:33025ms step_avg:94.63ms
step:360/1770 train_time:33121ms step_avg:94.63ms
step:361/1770 train_time:33216ms step_avg:94.63ms
step:362/1770 train_time:33311ms step_avg:94.63ms
step:363/1770 train_time:33406ms step_avg:94.63ms
step:364/1770 train_time:33501ms step_avg:94.64ms
step:365/1770 train_time:33596ms step_avg:94.64ms
step:366/1770 train_time:33691ms step_avg:94.64ms
step:367/1770 train_time:33786ms step_avg:94.64ms
step:368/1770 train_time:33881ms step_avg:94.64ms
step:369/1770 train_time:33976ms step_avg:94.64ms
step:370/1770 train_time:34072ms step_avg:94.64ms
step:371/1770 train_time:34166ms step_avg:94.64ms
step:372/1770 train_time:34261ms step_avg:94.64ms
step:373/1770 train_time:34357ms step_avg:94.65ms
step:374/1770 train_time:34452ms step_avg:94.65ms
step:375/1770 train_time:34547ms step_avg:94.65ms
step:375/1770 val_loss:3.8984 train_time:34640ms step_avg:94.90ms
step:376/1770 train_time:34662ms step_avg:94.70ms
step:377/1770 train_time:34742ms step_avg:94.66ms
step:378/1770 train_time:34838ms step_avg:94.67ms
step:379/1770 train_time:34933ms step_avg:94.67ms
step:380/1770 train_time:35027ms step_avg:94.67ms
step:381/1770 train_time:35122ms step_avg:94.67ms
step:382/1770 train_time:35217ms step_avg:94.67ms
step:383/1770 train_time:35312ms step_avg:94.67ms
step:384/1770 train_time:35406ms step_avg:94.67ms
step:385/1770 train_time:35501ms step_avg:94.67ms
step:386/1770 train_time:35595ms step_avg:94.67ms
step:387/1770 train_time:35690ms step_avg:94.67ms
step:388/1770 train_time:35787ms step_avg:94.67ms
step:389/1770 train_time:35882ms step_avg:94.68ms
step:390/1770 train_time:35978ms step_avg:94.68ms
step:391/1770 train_time:36072ms step_avg:94.68ms
step:392/1770 train_time:36167ms step_avg:94.68ms
step:393/1770 train_time:36261ms step_avg:94.68ms
step:394/1770 train_time:36356ms step_avg:94.68ms
step:395/1770 train_time:36451ms step_avg:94.68ms
step:396/1770 train_time:36547ms step_avg:94.68ms
step:397/1770 train_time:36644ms step_avg:94.69ms
step:398/1770 train_time:36741ms step_avg:94.69ms
step:399/1770 train_time:36838ms step_avg:94.70ms
step:400/1770 train_time:36935ms step_avg:94.71ms
step:401/1770 train_time:37032ms step_avg:94.71ms
step:402/1770 train_time:37128ms step_avg:94.72ms
step:403/1770 train_time:37225ms step_avg:94.72ms
step:404/1770 train_time:37322ms step_avg:94.73ms
step:405/1770 train_time:37419ms step_avg:94.73ms
step:406/1770 train_time:37517ms step_avg:94.74ms
step:407/1770 train_time:37613ms step_avg:94.74ms
step:408/1770 train_time:37711ms step_avg:94.75ms
step:409/1770 train_time:37807ms step_avg:94.75ms
step:410/1770 train_time:37905ms step_avg:94.76ms
step:411/1770 train_time:38002ms step_avg:94.77ms
step:412/1770 train_time:38099ms step_avg:94.77ms
step:413/1770 train_time:38196ms step_avg:94.78ms
step:414/1770 train_time:38293ms step_avg:94.78ms
step:415/1770 train_time:38389ms step_avg:94.79ms
step:416/1770 train_time:38486ms step_avg:94.79ms
step:417/1770 train_time:38584ms step_avg:94.80ms
step:418/1770 train_time:38681ms step_avg:94.81ms
step:419/1770 train_time:38778ms step_avg:94.81ms
step:420/1770 train_time:38874ms step_avg:94.82ms
step:421/1770 train_time:38971ms step_avg:94.82ms
step:422/1770 train_time:39068ms step_avg:94.83ms
step:423/1770 train_time:39165ms step_avg:94.83ms
step:424/1770 train_time:39263ms step_avg:94.84ms
step:425/1770 train_time:39361ms step_avg:94.85ms
step:426/1770 train_time:39458ms step_avg:94.85ms
step:427/1770 train_time:39555ms step_avg:94.86ms
step:428/1770 train_time:39651ms step_avg:94.86ms
step:429/1770 train_time:39748ms step_avg:94.86ms
step:430/1770 train_time:39844ms step_avg:94.87ms
step:431/1770 train_time:39942ms step_avg:94.87ms
step:432/1770 train_time:40039ms step_avg:94.88ms
step:433/1770 train_time:40136ms step_avg:94.88ms
step:434/1770 train_time:40233ms step_avg:94.89ms
step:435/1770 train_time:40330ms step_avg:94.89ms
step:436/1770 train_time:40427ms step_avg:94.90ms
step:437/1770 train_time:40524ms step_avg:94.90ms
step:438/1770 train_time:40621ms step_avg:94.91ms
step:439/1770 train_time:40719ms step_avg:94.92ms
step:440/1770 train_time:40815ms step_avg:94.92ms
step:441/1770 train_time:40912ms step_avg:94.92ms
step:442/1770 train_time:41009ms step_avg:94.93ms
step:443/1770 train_time:41106ms step_avg:94.93ms
step:444/1770 train_time:41204ms step_avg:94.94ms
step:445/1770 train_time:41301ms step_avg:94.95ms
step:446/1770 train_time:41399ms step_avg:94.95ms
step:447/1770 train_time:41496ms step_avg:94.96ms
step:448/1770 train_time:41594ms step_avg:94.96ms
step:449/1770 train_time:41690ms step_avg:94.97ms
step:450/1770 train_time:41787ms step_avg:94.97ms
step:451/1770 train_time:41884ms step_avg:94.97ms
step:452/1770 train_time:41981ms step_avg:94.98ms
step:453/1770 train_time:42078ms step_avg:94.98ms
step:454/1770 train_time:42175ms step_avg:94.99ms
step:455/1770 train_time:42271ms step_avg:94.99ms
step:456/1770 train_time:42368ms step_avg:95.00ms
step:457/1770 train_time:42465ms step_avg:95.00ms
step:458/1770 train_time:42563ms step_avg:95.01ms
step:459/1770 train_time:42661ms step_avg:95.01ms
step:460/1770 train_time:42758ms step_avg:95.02ms
step:461/1770 train_time:42855ms step_avg:95.02ms
step:462/1770 train_time:42951ms step_avg:95.03ms
step:463/1770 train_time:43048ms step_avg:95.03ms
step:464/1770 train_time:43145ms step_avg:95.03ms
step:465/1770 train_time:43244ms step_avg:95.04ms
step:466/1770 train_time:43341ms step_avg:95.05ms
step:467/1770 train_time:43439ms step_avg:95.05ms
step:468/1770 train_time:43535ms step_avg:95.06ms
step:469/1770 train_time:43632ms step_avg:95.06ms
step:470/1770 train_time:43729ms step_avg:95.06ms
step:471/1770 train_time:43826ms step_avg:95.07ms
step:472/1770 train_time:43924ms step_avg:95.07ms
step:473/1770 train_time:44022ms step_avg:95.08ms
step:474/1770 train_time:44118ms step_avg:95.08ms
step:475/1770 train_time:44215ms step_avg:95.09ms
step:476/1770 train_time:44312ms step_avg:95.09ms
step:477/1770 train_time:44408ms step_avg:95.09ms
step:478/1770 train_time:44505ms step_avg:95.10ms
step:479/1770 train_time:44602ms step_avg:95.10ms
step:480/1770 train_time:44699ms step_avg:95.10ms
step:481/1770 train_time:44796ms step_avg:95.11ms
step:482/1770 train_time:44893ms step_avg:95.11ms
step:483/1770 train_time:44990ms step_avg:95.12ms
step:484/1770 train_time:45087ms step_avg:95.12ms
step:485/1770 train_time:45184ms step_avg:95.12ms
step:486/1770 train_time:45282ms step_avg:95.13ms
step:487/1770 train_time:45379ms step_avg:95.13ms
step:488/1770 train_time:45477ms step_avg:95.14ms
step:489/1770 train_time:45573ms step_avg:95.14ms
step:490/1770 train_time:45670ms step_avg:95.15ms
step:491/1770 train_time:45767ms step_avg:95.15ms
step:492/1770 train_time:45864ms step_avg:95.15ms
step:493/1770 train_time:45961ms step_avg:95.16ms
step:494/1770 train_time:46058ms step_avg:95.16ms
step:495/1770 train_time:46155ms step_avg:95.17ms
step:496/1770 train_time:46252ms step_avg:95.17ms
step:497/1770 train_time:46349ms step_avg:95.17ms
step:498/1770 train_time:46446ms step_avg:95.18ms
step:499/1770 train_time:46543ms step_avg:95.18ms
step:500/1770 train_time:46641ms step_avg:95.19ms
step:500/1770 val_loss:3.7493 train_time:46737ms step_avg:95.38ms
step:501/1770 train_time:46762ms step_avg:95.24ms
step:502/1770 train_time:46841ms step_avg:95.21ms
step:503/1770 train_time:46939ms step_avg:95.21ms
step:504/1770 train_time:47035ms step_avg:95.21ms
step:505/1770 train_time:47131ms step_avg:95.21ms
step:506/1770 train_time:47228ms step_avg:95.22ms
step:507/1770 train_time:47325ms step_avg:95.22ms
step:508/1770 train_time:47422ms step_avg:95.22ms
step:509/1770 train_time:47518ms step_avg:95.23ms
step:510/1770 train_time:47614ms step_avg:95.23ms
step:511/1770 train_time:47712ms step_avg:95.23ms
step:512/1770 train_time:47810ms step_avg:95.24ms
step:513/1770 train_time:47908ms step_avg:95.24ms
step:514/1770 train_time:48006ms step_avg:95.25ms
step:515/1770 train_time:48103ms step_avg:95.25ms
step:516/1770 train_time:48201ms step_avg:95.26ms
step:517/1770 train_time:48297ms step_avg:95.26ms
step:518/1770 train_time:48394ms step_avg:95.26ms
step:519/1770 train_time:48490ms step_avg:95.27ms
step:520/1770 train_time:48587ms step_avg:95.27ms
step:521/1770 train_time:48685ms step_avg:95.27ms
step:522/1770 train_time:48782ms step_avg:95.28ms
step:523/1770 train_time:48879ms step_avg:95.28ms
step:524/1770 train_time:48976ms step_avg:95.29ms
step:525/1770 train_time:49074ms step_avg:95.29ms
step:526/1770 train_time:49170ms step_avg:95.29ms
step:527/1770 train_time:49268ms step_avg:95.30ms
step:528/1770 train_time:49366ms step_avg:95.30ms
step:529/1770 train_time:49463ms step_avg:95.30ms
step:530/1770 train_time:49560ms step_avg:95.31ms
step:531/1770 train_time:49657ms step_avg:95.31ms
step:532/1770 train_time:49754ms step_avg:95.32ms
step:533/1770 train_time:49852ms step_avg:95.32ms
step:534/1770 train_time:49949ms step_avg:95.32ms
step:535/1770 train_time:50047ms step_avg:95.33ms
step:536/1770 train_time:50144ms step_avg:95.33ms
step:537/1770 train_time:50242ms step_avg:95.34ms
step:538/1770 train_time:50339ms step_avg:95.34ms
step:539/1770 train_time:50437ms step_avg:95.34ms
step:540/1770 train_time:50533ms step_avg:95.35ms
step:541/1770 train_time:50631ms step_avg:95.35ms
step:542/1770 train_time:50728ms step_avg:95.35ms
step:543/1770 train_time:50826ms step_avg:95.36ms
step:544/1770 train_time:50923ms step_avg:95.36ms
step:545/1770 train_time:51021ms step_avg:95.37ms
step:546/1770 train_time:51118ms step_avg:95.37ms
step:547/1770 train_time:51215ms step_avg:95.37ms
step:548/1770 train_time:51312ms step_avg:95.38ms
step:549/1770 train_time:51409ms step_avg:95.38ms
step:550/1770 train_time:51506ms step_avg:95.38ms
step:551/1770 train_time:51604ms step_avg:95.39ms
step:552/1770 train_time:51702ms step_avg:95.39ms
step:553/1770 train_time:51799ms step_avg:95.39ms
step:554/1770 train_time:51897ms step_avg:95.40ms
step:555/1770 train_time:51993ms step_avg:95.40ms
step:556/1770 train_time:52091ms step_avg:95.40ms
step:557/1770 train_time:52188ms step_avg:95.41ms
step:558/1770 train_time:52286ms step_avg:95.41ms
step:559/1770 train_time:52384ms step_avg:95.42ms
step:560/1770 train_time:52481ms step_avg:95.42ms
step:561/1770 train_time:52578ms step_avg:95.42ms
step:562/1770 train_time:52675ms step_avg:95.43ms
step:563/1770 train_time:52773ms step_avg:95.43ms
step:564/1770 train_time:52871ms step_avg:95.43ms
step:565/1770 train_time:52967ms step_avg:95.44ms
step:566/1770 train_time:53065ms step_avg:95.44ms
step:567/1770 train_time:53163ms step_avg:95.45ms
step:568/1770 train_time:53261ms step_avg:95.45ms
step:569/1770 train_time:53358ms step_avg:95.45ms
step:570/1770 train_time:53455ms step_avg:95.46ms
step:571/1770 train_time:53552ms step_avg:95.46ms
step:572/1770 train_time:53649ms step_avg:95.46ms
step:573/1770 train_time:53747ms step_avg:95.47ms
step:574/1770 train_time:53845ms step_avg:95.47ms
step:575/1770 train_time:53943ms step_avg:95.47ms
step:576/1770 train_time:54040ms step_avg:95.48ms
step:577/1770 train_time:54137ms step_avg:95.48ms
step:578/1770 train_time:54234ms step_avg:95.48ms
step:579/1770 train_time:54331ms step_avg:95.49ms
step:580/1770 train_time:54429ms step_avg:95.49ms
step:581/1770 train_time:54526ms step_avg:95.49ms
step:582/1770 train_time:54624ms step_avg:95.50ms
step:583/1770 train_time:54722ms step_avg:95.50ms
step:584/1770 train_time:54819ms step_avg:95.50ms
step:585/1770 train_time:54916ms step_avg:95.51ms
step:586/1770 train_time:55013ms step_avg:95.51ms
step:587/1770 train_time:55110ms step_avg:95.51ms
step:588/1770 train_time:55207ms step_avg:95.51ms
step:589/1770 train_time:55305ms step_avg:95.52ms
step:590/1770 train_time:55402ms step_avg:95.52ms
step:591/1770 train_time:55500ms step_avg:95.53ms
step:592/1770 train_time:55597ms step_avg:95.53ms
step:593/1770 train_time:55694ms step_avg:95.53ms
step:594/1770 train_time:55791ms step_avg:95.53ms
step:595/1770 train_time:55890ms step_avg:95.54ms
step:596/1770 train_time:55988ms step_avg:95.54ms
step:597/1770 train_time:56085ms step_avg:95.55ms
step:598/1770 train_time:56183ms step_avg:95.55ms
step:599/1770 train_time:56280ms step_avg:95.55ms
step:600/1770 train_time:56377ms step_avg:95.55ms
step:601/1770 train_time:56474ms step_avg:95.56ms
step:602/1770 train_time:56571ms step_avg:95.56ms
step:603/1770 train_time:56669ms step_avg:95.56ms
step:604/1770 train_time:56767ms step_avg:95.57ms
step:605/1770 train_time:56865ms step_avg:95.57ms
step:606/1770 train_time:56962ms step_avg:95.57ms
step:607/1770 train_time:57059ms step_avg:95.58ms
step:608/1770 train_time:57157ms step_avg:95.58ms
step:609/1770 train_time:57254ms step_avg:95.58ms
step:610/1770 train_time:57351ms step_avg:95.58ms
step:611/1770 train_time:57448ms step_avg:95.59ms
step:612/1770 train_time:57546ms step_avg:95.59ms
step:613/1770 train_time:57643ms step_avg:95.59ms
step:614/1770 train_time:57741ms step_avg:95.60ms
step:615/1770 train_time:57839ms step_avg:95.60ms
step:616/1770 train_time:57936ms step_avg:95.60ms
step:617/1770 train_time:58033ms step_avg:95.61ms
step:618/1770 train_time:58131ms step_avg:95.61ms
step:619/1770 train_time:58228ms step_avg:95.61ms
step:620/1770 train_time:58326ms step_avg:95.62ms
step:621/1770 train_time:58423ms step_avg:95.62ms
step:622/1770 train_time:58520ms step_avg:95.62ms
step:623/1770 train_time:58617ms step_avg:95.62ms
step:624/1770 train_time:58714ms step_avg:95.63ms
step:625/1770 train_time:58811ms step_avg:95.63ms
step:625/1770 val_loss:3.6642 train_time:58907ms step_avg:95.78ms
step:626/1770 train_time:58929ms step_avg:95.66ms
step:627/1770 train_time:59011ms step_avg:95.64ms
step:628/1770 train_time:59109ms step_avg:95.65ms
step:629/1770 train_time:59206ms step_avg:95.65ms
step:630/1770 train_time:59303ms step_avg:95.65ms
step:631/1770 train_time:59400ms step_avg:95.65ms
step:632/1770 train_time:59498ms step_avg:95.66ms
step:633/1770 train_time:59594ms step_avg:95.66ms
step:634/1770 train_time:59691ms step_avg:95.66ms
step:635/1770 train_time:59788ms step_avg:95.66ms
step:636/1770 train_time:59885ms step_avg:95.66ms
step:637/1770 train_time:59983ms step_avg:95.67ms
step:638/1770 train_time:60082ms step_avg:95.67ms
step:639/1770 train_time:60180ms step_avg:95.68ms
step:640/1770 train_time:60278ms step_avg:95.68ms
step:641/1770 train_time:60374ms step_avg:95.68ms
step:642/1770 train_time:60471ms step_avg:95.68ms
step:643/1770 train_time:60567ms step_avg:95.68ms
step:644/1770 train_time:60665ms step_avg:95.69ms
step:645/1770 train_time:60762ms step_avg:95.69ms
step:646/1770 train_time:60860ms step_avg:95.69ms
step:647/1770 train_time:60958ms step_avg:95.70ms
step:648/1770 train_time:61055ms step_avg:95.70ms
step:649/1770 train_time:61153ms step_avg:95.70ms
step:650/1770 train_time:61250ms step_avg:95.70ms
step:651/1770 train_time:61347ms step_avg:95.71ms
step:652/1770 train_time:61444ms step_avg:95.71ms
step:653/1770 train_time:61543ms step_avg:95.71ms
step:654/1770 train_time:61639ms step_avg:95.71ms
step:655/1770 train_time:61736ms step_avg:95.72ms
step:656/1770 train_time:61834ms step_avg:95.72ms
step:657/1770 train_time:61931ms step_avg:95.72ms
step:658/1770 train_time:62031ms step_avg:95.73ms
step:659/1770 train_time:62129ms step_avg:95.73ms
step:660/1770 train_time:62228ms step_avg:95.74ms
step:661/1770 train_time:62327ms step_avg:95.74ms
step:662/1770 train_time:62427ms step_avg:95.75ms
step:663/1770 train_time:62526ms step_avg:95.75ms
step:664/1770 train_time:62626ms step_avg:95.76ms
step:665/1770 train_time:62725ms step_avg:95.76ms
step:666/1770 train_time:62824ms step_avg:95.77ms
step:667/1770 train_time:62925ms step_avg:95.78ms
step:668/1770 train_time:63024ms step_avg:95.78ms
step:669/1770 train_time:63124ms step_avg:95.79ms
step:670/1770 train_time:63225ms step_avg:95.80ms
step:671/1770 train_time:63325ms step_avg:95.80ms
step:672/1770 train_time:63424ms step_avg:95.81ms
step:673/1770 train_time:63525ms step_avg:95.81ms
step:674/1770 train_time:63623ms step_avg:95.82ms
step:675/1770 train_time:63722ms step_avg:95.82ms
step:676/1770 train_time:63822ms step_avg:95.83ms
step:677/1770 train_time:63921ms step_avg:95.83ms
step:678/1770 train_time:64021ms step_avg:95.84ms
step:679/1770 train_time:64120ms step_avg:95.84ms
step:680/1770 train_time:64219ms step_avg:95.85ms
step:681/1770 train_time:64318ms step_avg:95.85ms
step:682/1770 train_time:64418ms step_avg:95.86ms
step:683/1770 train_time:64518ms step_avg:95.87ms
step:684/1770 train_time:64616ms step_avg:95.87ms
step:685/1770 train_time:64715ms step_avg:95.87ms
step:686/1770 train_time:64814ms step_avg:95.88ms
step:687/1770 train_time:64913ms step_avg:95.88ms
step:688/1770 train_time:65012ms step_avg:95.89ms
step:689/1770 train_time:65111ms step_avg:95.89ms
step:690/1770 train_time:65210ms step_avg:95.90ms
step:691/1770 train_time:65309ms step_avg:95.90ms
step:692/1770 train_time:65409ms step_avg:95.91ms
step:693/1770 train_time:65509ms step_avg:95.91ms
step:694/1770 train_time:65608ms step_avg:95.92ms
step:695/1770 train_time:65707ms step_avg:95.92ms
step:696/1770 train_time:65806ms step_avg:95.93ms
step:697/1770 train_time:65906ms step_avg:95.93ms
step:698/1770 train_time:66007ms step_avg:95.94ms
step:699/1770 train_time:66106ms step_avg:95.94ms
step:700/1770 train_time:66206ms step_avg:95.95ms
step:701/1770 train_time:66306ms step_avg:95.96ms
step:702/1770 train_time:66406ms step_avg:95.96ms
step:703/1770 train_time:66507ms step_avg:95.97ms
step:704/1770 train_time:66608ms step_avg:95.98ms
step:705/1770 train_time:66708ms step_avg:95.98ms
step:706/1770 train_time:66807ms step_avg:95.99ms
step:707/1770 train_time:66906ms step_avg:95.99ms
step:708/1770 train_time:67005ms step_avg:96.00ms
step:709/1770 train_time:67104ms step_avg:96.00ms
step:710/1770 train_time:67203ms step_avg:96.00ms
step:711/1770 train_time:67302ms step_avg:96.01ms
step:712/1770 train_time:67402ms step_avg:96.01ms
step:713/1770 train_time:67502ms step_avg:96.02ms
step:714/1770 train_time:67602ms step_avg:96.03ms
step:715/1770 train_time:67702ms step_avg:96.03ms
step:716/1770 train_time:67802ms step_avg:96.04ms
step:717/1770 train_time:67903ms step_avg:96.04ms
step:718/1770 train_time:68003ms step_avg:96.05ms
step:719/1770 train_time:68102ms step_avg:96.05ms
step:720/1770 train_time:68201ms step_avg:96.06ms
step:721/1770 train_time:68300ms step_avg:96.06ms
step:722/1770 train_time:68398ms step_avg:96.07ms
step:723/1770 train_time:68497ms step_avg:96.07ms
step:724/1770 train_time:68596ms step_avg:96.07ms
step:725/1770 train_time:68694ms step_avg:96.08ms
step:726/1770 train_time:68793ms step_avg:96.08ms
step:727/1770 train_time:68891ms step_avg:96.08ms
step:728/1770 train_time:68990ms step_avg:96.09ms
step:729/1770 train_time:69089ms step_avg:96.09ms
step:730/1770 train_time:69189ms step_avg:96.10ms
step:731/1770 train_time:69287ms step_avg:96.10ms
step:732/1770 train_time:69386ms step_avg:96.10ms
step:733/1770 train_time:69486ms step_avg:96.11ms
step:734/1770 train_time:69585ms step_avg:96.11ms
step:735/1770 train_time:69685ms step_avg:96.12ms
step:736/1770 train_time:69786ms step_avg:96.12ms
step:737/1770 train_time:69886ms step_avg:96.13ms
step:738/1770 train_time:69985ms step_avg:96.13ms
step:739/1770 train_time:70085ms step_avg:96.14ms
step:740/1770 train_time:70185ms step_avg:96.14ms
step:741/1770 train_time:70283ms step_avg:96.15ms
step:742/1770 train_time:70382ms step_avg:96.15ms
step:743/1770 train_time:70481ms step_avg:96.15ms
step:744/1770 train_time:70579ms step_avg:96.16ms
step:745/1770 train_time:70678ms step_avg:96.16ms
step:746/1770 train_time:70777ms step_avg:96.16ms
step:747/1770 train_time:70876ms step_avg:96.17ms
step:748/1770 train_time:70975ms step_avg:96.17ms
step:749/1770 train_time:71073ms step_avg:96.17ms
step:750/1770 train_time:71172ms step_avg:96.18ms
step:750/1770 val_loss:3.6009 train_time:71269ms step_avg:96.31ms
step:751/1770 train_time:71297ms step_avg:96.22ms
step:752/1770 train_time:71377ms step_avg:96.19ms
step:753/1770 train_time:71476ms step_avg:96.20ms
step:754/1770 train_time:71575ms step_avg:96.20ms
step:755/1770 train_time:71674ms step_avg:96.21ms
step:756/1770 train_time:71773ms step_avg:96.21ms
step:757/1770 train_time:71873ms step_avg:96.22ms
step:758/1770 train_time:71972ms step_avg:96.22ms
step:759/1770 train_time:72070ms step_avg:96.22ms
step:760/1770 train_time:72170ms step_avg:96.23ms
step:761/1770 train_time:72270ms step_avg:96.23ms
step:762/1770 train_time:72371ms step_avg:96.24ms
step:763/1770 train_time:72471ms step_avg:96.24ms
step:764/1770 train_time:72571ms step_avg:96.25ms
step:765/1770 train_time:72671ms step_avg:96.25ms
step:766/1770 train_time:72770ms step_avg:96.26ms
step:767/1770 train_time:72871ms step_avg:96.26ms
step:768/1770 train_time:72970ms step_avg:96.27ms
step:769/1770 train_time:73069ms step_avg:96.27ms
step:770/1770 train_time:73168ms step_avg:96.27ms
step:771/1770 train_time:73268ms step_avg:96.28ms
step:772/1770 train_time:73367ms step_avg:96.28ms
step:773/1770 train_time:73466ms step_avg:96.29ms
step:774/1770 train_time:73566ms step_avg:96.29ms
step:775/1770 train_time:73665ms step_avg:96.29ms
step:776/1770 train_time:73763ms step_avg:96.30ms
step:777/1770 train_time:73862ms step_avg:96.30ms
step:778/1770 train_time:73961ms step_avg:96.30ms
step:779/1770 train_time:74059ms step_avg:96.31ms
step:780/1770 train_time:74158ms step_avg:96.31ms
step:781/1770 train_time:74257ms step_avg:96.31ms
step:782/1770 train_time:74356ms step_avg:96.32ms
step:783/1770 train_time:74457ms step_avg:96.32ms
step:784/1770 train_time:74556ms step_avg:96.33ms
step:785/1770 train_time:74656ms step_avg:96.33ms
step:786/1770 train_time:74756ms step_avg:96.33ms
step:787/1770 train_time:74855ms step_avg:96.34ms
step:788/1770 train_time:74955ms step_avg:96.34ms
step:789/1770 train_time:75055ms step_avg:96.35ms
step:790/1770 train_time:75154ms step_avg:96.35ms
step:791/1770 train_time:75255ms step_avg:96.36ms
step:792/1770 train_time:75354ms step_avg:96.36ms
step:793/1770 train_time:75454ms step_avg:96.37ms
step:794/1770 train_time:75555ms step_avg:96.37ms
step:795/1770 train_time:75654ms step_avg:96.37ms
step:796/1770 train_time:75754ms step_avg:96.38ms
step:797/1770 train_time:75854ms step_avg:96.38ms
step:798/1770 train_time:75954ms step_avg:96.39ms
step:799/1770 train_time:76054ms step_avg:96.39ms
step:800/1770 train_time:76153ms step_avg:96.40ms
step:801/1770 train_time:76254ms step_avg:96.40ms
step:802/1770 train_time:76353ms step_avg:96.41ms
step:803/1770 train_time:76453ms step_avg:96.41ms
step:804/1770 train_time:76553ms step_avg:96.41ms
step:805/1770 train_time:76654ms step_avg:96.42ms
step:806/1770 train_time:76753ms step_avg:96.42ms
step:807/1770 train_time:76853ms step_avg:96.43ms
step:808/1770 train_time:76952ms step_avg:96.43ms
step:809/1770 train_time:77053ms step_avg:96.44ms
step:810/1770 train_time:77152ms step_avg:96.44ms
step:811/1770 train_time:77251ms step_avg:96.44ms
step:812/1770 train_time:77351ms step_avg:96.45ms
step:813/1770 train_time:77451ms step_avg:96.45ms
step:814/1770 train_time:77551ms step_avg:96.46ms
step:815/1770 train_time:77651ms step_avg:96.46ms
step:816/1770 train_time:77750ms step_avg:96.46ms
step:817/1770 train_time:77850ms step_avg:96.47ms
step:818/1770 train_time:77950ms step_avg:96.47ms
step:819/1770 train_time:78051ms step_avg:96.48ms
step:820/1770 train_time:78150ms step_avg:96.48ms
step:821/1770 train_time:78250ms step_avg:96.49ms
step:822/1770 train_time:78349ms step_avg:96.49ms
step:823/1770 train_time:78449ms step_avg:96.49ms
step:824/1770 train_time:78548ms step_avg:96.50ms
step:825/1770 train_time:78647ms step_avg:96.50ms
step:826/1770 train_time:78746ms step_avg:96.50ms
step:827/1770 train_time:78845ms step_avg:96.51ms
step:828/1770 train_time:78945ms step_avg:96.51ms
step:829/1770 train_time:79045ms step_avg:96.51ms
step:830/1770 train_time:79144ms step_avg:96.52ms
step:831/1770 train_time:79243ms step_avg:96.52ms
step:832/1770 train_time:79342ms step_avg:96.52ms
step:833/1770 train_time:79442ms step_avg:96.53ms
step:834/1770 train_time:79541ms step_avg:96.53ms
step:835/1770 train_time:79642ms step_avg:96.54ms
step:836/1770 train_time:79739ms step_avg:96.54ms
step:837/1770 train_time:79838ms step_avg:96.54ms
step:838/1770 train_time:79937ms step_avg:96.54ms
step:839/1770 train_time:80037ms step_avg:96.55ms
step:840/1770 train_time:80136ms step_avg:96.55ms
step:841/1770 train_time:80236ms step_avg:96.55ms
step:842/1770 train_time:80335ms step_avg:96.56ms
step:843/1770 train_time:80435ms step_avg:96.56ms
step:844/1770 train_time:80535ms step_avg:96.56ms
step:845/1770 train_time:80635ms step_avg:96.57ms
step:846/1770 train_time:80735ms step_avg:96.57ms
step:847/1770 train_time:80835ms step_avg:96.58ms
step:848/1770 train_time:80934ms step_avg:96.58ms
step:849/1770 train_time:81034ms step_avg:96.58ms
step:850/1770 train_time:81135ms step_avg:96.59ms
step:851/1770 train_time:81235ms step_avg:96.59ms
step:852/1770 train_time:81334ms step_avg:96.60ms
step:853/1770 train_time:81434ms step_avg:96.60ms
step:854/1770 train_time:81534ms step_avg:96.60ms
step:855/1770 train_time:81633ms step_avg:96.61ms
step:856/1770 train_time:81733ms step_avg:96.61ms
step:857/1770 train_time:81833ms step_avg:96.61ms
step:858/1770 train_time:81932ms step_avg:96.62ms
step:859/1770 train_time:82032ms step_avg:96.62ms
step:860/1770 train_time:82131ms step_avg:96.62ms
step:861/1770 train_time:82230ms step_avg:96.63ms
step:862/1770 train_time:82330ms step_avg:96.63ms
step:863/1770 train_time:82430ms step_avg:96.64ms
step:864/1770 train_time:82529ms step_avg:96.64ms
step:865/1770 train_time:82628ms step_avg:96.64ms
step:866/1770 train_time:82728ms step_avg:96.64ms
step:867/1770 train_time:82827ms step_avg:96.65ms
step:868/1770 train_time:82928ms step_avg:96.65ms
step:869/1770 train_time:83027ms step_avg:96.66ms
step:870/1770 train_time:83126ms step_avg:96.66ms
step:871/1770 train_time:83226ms step_avg:96.66ms
step:872/1770 train_time:83325ms step_avg:96.66ms
step:873/1770 train_time:83424ms step_avg:96.67ms
step:874/1770 train_time:83523ms step_avg:96.67ms
step:875/1770 train_time:83623ms step_avg:96.67ms
step:875/1770 val_loss:3.5497 train_time:83720ms step_avg:96.79ms
step:876/1770 train_time:83748ms step_avg:96.71ms
step:877/1770 train_time:83826ms step_avg:96.68ms
step:878/1770 train_time:83927ms step_avg:96.69ms
step:879/1770 train_time:84026ms step_avg:96.69ms
step:880/1770 train_time:84125ms step_avg:96.70ms
step:881/1770 train_time:84224ms step_avg:96.70ms
step:882/1770 train_time:84324ms step_avg:96.70ms
step:883/1770 train_time:84423ms step_avg:96.70ms
step:884/1770 train_time:84522ms step_avg:96.71ms
step:885/1770 train_time:84621ms step_avg:96.71ms
step:886/1770 train_time:84721ms step_avg:96.71ms
step:887/1770 train_time:84822ms step_avg:96.72ms
step:888/1770 train_time:84922ms step_avg:96.72ms
step:889/1770 train_time:85023ms step_avg:96.73ms
step:890/1770 train_time:85123ms step_avg:96.73ms
step:891/1770 train_time:85222ms step_avg:96.73ms
step:892/1770 train_time:85322ms step_avg:96.74ms
step:893/1770 train_time:85421ms step_avg:96.74ms
step:894/1770 train_time:85520ms step_avg:96.74ms
step:895/1770 train_time:85621ms step_avg:96.75ms
step:896/1770 train_time:85719ms step_avg:96.75ms
step:897/1770 train_time:85819ms step_avg:96.75ms
step:898/1770 train_time:85920ms step_avg:96.76ms
step:899/1770 train_time:86019ms step_avg:96.76ms
step:900/1770 train_time:86120ms step_avg:96.76ms
step:901/1770 train_time:86220ms step_avg:96.77ms
step:902/1770 train_time:86321ms step_avg:96.77ms
step:903/1770 train_time:86421ms step_avg:96.78ms
step:904/1770 train_time:86521ms step_avg:96.78ms
step:905/1770 train_time:86620ms step_avg:96.78ms
step:906/1770 train_time:86721ms step_avg:96.79ms
step:907/1770 train_time:86822ms step_avg:96.79ms
step:908/1770 train_time:86921ms step_avg:96.79ms
step:909/1770 train_time:87021ms step_avg:96.80ms
step:910/1770 train_time:87122ms step_avg:96.80ms
step:911/1770 train_time:87221ms step_avg:96.81ms
step:912/1770 train_time:87322ms step_avg:96.81ms
step:913/1770 train_time:87422ms step_avg:96.81ms
step:914/1770 train_time:87522ms step_avg:96.82ms
step:915/1770 train_time:87622ms step_avg:96.82ms
step:916/1770 train_time:87723ms step_avg:96.82ms
step:917/1770 train_time:87822ms step_avg:96.83ms
step:918/1770 train_time:87921ms step_avg:96.83ms
step:919/1770 train_time:88021ms step_avg:96.83ms
step:920/1770 train_time:88124ms step_avg:96.84ms
step:921/1770 train_time:88225ms step_avg:96.84ms
step:922/1770 train_time:88326ms step_avg:96.85ms
step:923/1770 train_time:88426ms step_avg:96.85ms
step:924/1770 train_time:88526ms step_avg:96.86ms
step:925/1770 train_time:88627ms step_avg:96.86ms
step:926/1770 train_time:88727ms step_avg:96.86ms
step:927/1770 train_time:88828ms step_avg:96.87ms
step:928/1770 train_time:88929ms step_avg:96.87ms
step:929/1770 train_time:89030ms step_avg:96.88ms
step:930/1770 train_time:89131ms step_avg:96.88ms
step:931/1770 train_time:89232ms step_avg:96.89ms
step:932/1770 train_time:89333ms step_avg:96.89ms
step:933/1770 train_time:89434ms step_avg:96.89ms
step:934/1770 train_time:89535ms step_avg:96.90ms
step:935/1770 train_time:89636ms step_avg:96.90ms
step:936/1770 train_time:89736ms step_avg:96.91ms
step:937/1770 train_time:89836ms step_avg:96.91ms
step:938/1770 train_time:89937ms step_avg:96.91ms
step:939/1770 train_time:90037ms step_avg:96.92ms
step:940/1770 train_time:90140ms step_avg:96.92ms
step:941/1770 train_time:90242ms step_avg:96.93ms
step:942/1770 train_time:90344ms step_avg:96.94ms
step:943/1770 train_time:90446ms step_avg:96.94ms
step:944/1770 train_time:90546ms step_avg:96.94ms
step:945/1770 train_time:90647ms step_avg:96.95ms
step:946/1770 train_time:90748ms step_avg:96.95ms
step:947/1770 train_time:90849ms step_avg:96.96ms
step:948/1770 train_time:90949ms step_avg:96.96ms
step:949/1770 train_time:91051ms step_avg:96.97ms
step:950/1770 train_time:91153ms step_avg:96.97ms
step:951/1770 train_time:91253ms step_avg:96.98ms
step:952/1770 train_time:91354ms step_avg:96.98ms
step:953/1770 train_time:91455ms step_avg:96.98ms
step:954/1770 train_time:91556ms step_avg:96.99ms
step:955/1770 train_time:91658ms step_avg:96.99ms
step:956/1770 train_time:91758ms step_avg:97.00ms
step:957/1770 train_time:91860ms step_avg:97.00ms
step:958/1770 train_time:91961ms step_avg:97.00ms
step:959/1770 train_time:92063ms step_avg:97.01ms
step:960/1770 train_time:92163ms step_avg:97.01ms
step:961/1770 train_time:92265ms step_avg:97.02ms
step:962/1770 train_time:92366ms step_avg:97.02ms
step:963/1770 train_time:92468ms step_avg:97.03ms
step:964/1770 train_time:92568ms step_avg:97.03ms
step:965/1770 train_time:92669ms step_avg:97.04ms
step:966/1770 train_time:92770ms step_avg:97.04ms
step:967/1770 train_time:92871ms step_avg:97.04ms
step:968/1770 train_time:92972ms step_avg:97.05ms
step:969/1770 train_time:93075ms step_avg:97.05ms
step:970/1770 train_time:93175ms step_avg:97.06ms
step:971/1770 train_time:93275ms step_avg:97.06ms
step:972/1770 train_time:93376ms step_avg:97.06ms
step:973/1770 train_time:93477ms step_avg:97.07ms
step:974/1770 train_time:93578ms step_avg:97.07ms
step:975/1770 train_time:93680ms step_avg:97.08ms
step:976/1770 train_time:93782ms step_avg:97.08ms
step:977/1770 train_time:93884ms step_avg:97.09ms
step:978/1770 train_time:93986ms step_avg:97.09ms
step:979/1770 train_time:94088ms step_avg:97.10ms
step:980/1770 train_time:94189ms step_avg:97.10ms
step:981/1770 train_time:94290ms step_avg:97.11ms
step:982/1770 train_time:94390ms step_avg:97.11ms
step:983/1770 train_time:94491ms step_avg:97.11ms
step:984/1770 train_time:94593ms step_avg:97.12ms
step:985/1770 train_time:94694ms step_avg:97.12ms
step:986/1770 train_time:94796ms step_avg:97.13ms
step:987/1770 train_time:94897ms step_avg:97.13ms
step:988/1770 train_time:94998ms step_avg:97.13ms
step:989/1770 train_time:95100ms step_avg:97.14ms
step:990/1770 train_time:95202ms step_avg:97.14ms
step:991/1770 train_time:95303ms step_avg:97.15ms
step:992/1770 train_time:95406ms step_avg:97.15ms
step:993/1770 train_time:95507ms step_avg:97.16ms
step:994/1770 train_time:95608ms step_avg:97.16ms
step:995/1770 train_time:95708ms step_avg:97.17ms
step:996/1770 train_time:95809ms step_avg:97.17ms
step:997/1770 train_time:95910ms step_avg:97.17ms
step:998/1770 train_time:96010ms step_avg:97.18ms
step:999/1770 train_time:96111ms step_avg:97.18ms
step:1000/1770 train_time:96212ms step_avg:97.18ms
step:1000/1770 val_loss:3.5126 train_time:96310ms step_avg:97.28ms
step:1001/1770 train_time:96338ms step_avg:97.21ms
step:1002/1770 train_time:96421ms step_avg:97.20ms
step:1003/1770 train_time:96523ms step_avg:97.20ms
step:1004/1770 train_time:96624ms step_avg:97.21ms
step:1005/1770 train_time:96724ms step_avg:97.21ms
step:1006/1770 train_time:96824ms step_avg:97.21ms
step:1007/1770 train_time:96924ms step_avg:97.22ms
step:1008/1770 train_time:97025ms step_avg:97.22ms
step:1009/1770 train_time:97125ms step_avg:97.22ms
step:1010/1770 train_time:97226ms step_avg:97.23ms
step:1011/1770 train_time:97328ms step_avg:97.23ms
step:1012/1770 train_time:97430ms step_avg:97.24ms
step:1013/1770 train_time:97532ms step_avg:97.24ms
step:1014/1770 train_time:97633ms step_avg:97.24ms
step:1015/1770 train_time:97734ms step_avg:97.25ms
step:1016/1770 train_time:97833ms step_avg:97.25ms
step:1017/1770 train_time:97934ms step_avg:97.25ms
step:1018/1770 train_time:98034ms step_avg:97.26ms
step:1019/1770 train_time:98135ms step_avg:97.26ms
step:1020/1770 train_time:98236ms step_avg:97.26ms
step:1021/1770 train_time:98337ms step_avg:97.27ms
step:1022/1770 train_time:98438ms step_avg:97.27ms
step:1023/1770 train_time:98539ms step_avg:97.27ms
step:1024/1770 train_time:98641ms step_avg:97.28ms
step:1025/1770 train_time:98742ms step_avg:97.28ms
step:1026/1770 train_time:98844ms step_avg:97.29ms
step:1027/1770 train_time:98946ms step_avg:97.29ms
step:1028/1770 train_time:99047ms step_avg:97.30ms
step:1029/1770 train_time:99149ms step_avg:97.30ms
step:1030/1770 train_time:99250ms step_avg:97.30ms
step:1031/1770 train_time:99351ms step_avg:97.31ms
step:1032/1770 train_time:99451ms step_avg:97.31ms
step:1033/1770 train_time:99553ms step_avg:97.31ms
step:1034/1770 train_time:99653ms step_avg:97.32ms
step:1035/1770 train_time:99755ms step_avg:97.32ms
step:1036/1770 train_time:99856ms step_avg:97.33ms
step:1037/1770 train_time:99957ms step_avg:97.33ms
step:1038/1770 train_time:100058ms step_avg:97.33ms
step:1039/1770 train_time:100159ms step_avg:97.34ms
step:1040/1770 train_time:100259ms step_avg:97.34ms
step:1041/1770 train_time:100359ms step_avg:97.34ms
step:1042/1770 train_time:100461ms step_avg:97.35ms
step:1043/1770 train_time:100562ms step_avg:97.35ms
step:1044/1770 train_time:100663ms step_avg:97.35ms
step:1045/1770 train_time:100764ms step_avg:97.36ms
step:1046/1770 train_time:100866ms step_avg:97.36ms
step:1047/1770 train_time:100967ms step_avg:97.36ms
step:1048/1770 train_time:101068ms step_avg:97.37ms
step:1049/1770 train_time:101169ms step_avg:97.37ms
step:1050/1770 train_time:101271ms step_avg:97.38ms
step:1051/1770 train_time:101371ms step_avg:97.38ms
step:1052/1770 train_time:101472ms step_avg:97.38ms
step:1053/1770 train_time:101573ms step_avg:97.39ms
step:1054/1770 train_time:101673ms step_avg:97.39ms
step:1055/1770 train_time:101774ms step_avg:97.39ms
step:1056/1770 train_time:101875ms step_avg:97.39ms
step:1057/1770 train_time:101975ms step_avg:97.40ms
step:1058/1770 train_time:102075ms step_avg:97.40ms
step:1059/1770 train_time:102176ms step_avg:97.40ms
step:1060/1770 train_time:102277ms step_avg:97.41ms
step:1061/1770 train_time:102379ms step_avg:97.41ms
step:1062/1770 train_time:102480ms step_avg:97.41ms
step:1063/1770 train_time:102582ms step_avg:97.42ms
step:1064/1770 train_time:102684ms step_avg:97.42ms
step:1065/1770 train_time:102786ms step_avg:97.43ms
step:1066/1770 train_time:102888ms step_avg:97.43ms
step:1067/1770 train_time:102990ms step_avg:97.44ms
step:1068/1770 train_time:103093ms step_avg:97.44ms
step:1069/1770 train_time:103193ms step_avg:97.44ms
step:1070/1770 train_time:103293ms step_avg:97.45ms
step:1071/1770 train_time:103395ms step_avg:97.45ms
step:1072/1770 train_time:103496ms step_avg:97.45ms
step:1073/1770 train_time:103597ms step_avg:97.46ms
step:1074/1770 train_time:103698ms step_avg:97.46ms
step:1075/1770 train_time:103799ms step_avg:97.46ms
step:1076/1770 train_time:103900ms step_avg:97.47ms
step:1077/1770 train_time:104001ms step_avg:97.47ms
step:1078/1770 train_time:104103ms step_avg:97.47ms
step:1079/1770 train_time:104204ms step_avg:97.48ms
step:1080/1770 train_time:104306ms step_avg:97.48ms
step:1081/1770 train_time:104408ms step_avg:97.49ms
step:1082/1770 train_time:104510ms step_avg:97.49ms
step:1083/1770 train_time:104612ms step_avg:97.49ms
step:1084/1770 train_time:104713ms step_avg:97.50ms
step:1085/1770 train_time:104813ms step_avg:97.50ms
step:1086/1770 train_time:104913ms step_avg:97.50ms
step:1087/1770 train_time:105014ms step_avg:97.51ms
step:1088/1770 train_time:105115ms step_avg:97.51ms
step:1089/1770 train_time:105217ms step_avg:97.51ms
step:1090/1770 train_time:105319ms step_avg:97.52ms
step:1091/1770 train_time:105420ms step_avg:97.52ms
step:1092/1770 train_time:105520ms step_avg:97.52ms
step:1093/1770 train_time:105621ms step_avg:97.53ms
step:1094/1770 train_time:105723ms step_avg:97.53ms
step:1095/1770 train_time:105824ms step_avg:97.53ms
step:1096/1770 train_time:105926ms step_avg:97.54ms
step:1097/1770 train_time:106027ms step_avg:97.54ms
step:1098/1770 train_time:106129ms step_avg:97.54ms
step:1099/1770 train_time:106230ms step_avg:97.55ms
step:1100/1770 train_time:106331ms step_avg:97.55ms
step:1101/1770 train_time:106431ms step_avg:97.55ms
step:1102/1770 train_time:106532ms step_avg:97.56ms
step:1103/1770 train_time:106633ms step_avg:97.56ms
step:1104/1770 train_time:106735ms step_avg:97.56ms
step:1105/1770 train_time:106837ms step_avg:97.57ms
step:1106/1770 train_time:106938ms step_avg:97.57ms
step:1107/1770 train_time:107039ms step_avg:97.57ms
step:1108/1770 train_time:107140ms step_avg:97.58ms
step:1109/1770 train_time:107241ms step_avg:97.58ms
step:1110/1770 train_time:107341ms step_avg:97.58ms
step:1111/1770 train_time:107443ms step_avg:97.59ms
step:1112/1770 train_time:107544ms step_avg:97.59ms
step:1113/1770 train_time:107646ms step_avg:97.59ms
step:1114/1770 train_time:107748ms step_avg:97.60ms
step:1115/1770 train_time:107850ms step_avg:97.60ms
step:1116/1770 train_time:107953ms step_avg:97.61ms
step:1117/1770 train_time:108054ms step_avg:97.61ms
step:1118/1770 train_time:108154ms step_avg:97.61ms
step:1119/1770 train_time:108254ms step_avg:97.61ms
step:1120/1770 train_time:108356ms step_avg:97.62ms
step:1121/1770 train_time:108457ms step_avg:97.62ms
step:1122/1770 train_time:108557ms step_avg:97.62ms
step:1123/1770 train_time:108659ms step_avg:97.63ms
step:1124/1770 train_time:108760ms step_avg:97.63ms
step:1125/1770 train_time:108861ms step_avg:97.63ms
step:1125/1770 val_loss:3.4709 train_time:108959ms step_avg:97.72ms
step:1126/1770 train_time:108986ms step_avg:97.66ms
step:1127/1770 train_time:109069ms step_avg:97.64ms
step:1128/1770 train_time:109170ms step_avg:97.65ms
step:1129/1770 train_time:109270ms step_avg:97.65ms
step:1130/1770 train_time:109371ms step_avg:97.65ms
step:1131/1770 train_time:109471ms step_avg:97.65ms
step:1132/1770 train_time:109572ms step_avg:97.66ms
step:1133/1770 train_time:109672ms step_avg:97.66ms
step:1134/1770 train_time:109773ms step_avg:97.66ms
step:1135/1770 train_time:109874ms step_avg:97.67ms
step:1136/1770 train_time:109978ms step_avg:97.67ms
step:1137/1770 train_time:110079ms step_avg:97.67ms
step:1138/1770 train_time:110180ms step_avg:97.68ms
step:1139/1770 train_time:110280ms step_avg:97.68ms
step:1140/1770 train_time:110382ms step_avg:97.68ms
step:1141/1770 train_time:110483ms step_avg:97.69ms
step:1142/1770 train_time:110584ms step_avg:97.69ms
step:1143/1770 train_time:110687ms step_avg:97.69ms
step:1144/1770 train_time:110788ms step_avg:97.70ms
step:1145/1770 train_time:110888ms step_avg:97.70ms
step:1146/1770 train_time:110990ms step_avg:97.70ms
step:1147/1770 train_time:111090ms step_avg:97.70ms
step:1148/1770 train_time:111192ms step_avg:97.71ms
step:1149/1770 train_time:111293ms step_avg:97.71ms
step:1150/1770 train_time:111398ms step_avg:97.72ms
step:1151/1770 train_time:111496ms step_avg:97.72ms
step:1152/1770 train_time:111597ms step_avg:97.72ms
step:1153/1770 train_time:111698ms step_avg:97.72ms
step:1154/1770 train_time:111799ms step_avg:97.73ms
step:1155/1770 train_time:111902ms step_avg:97.73ms
step:1156/1770 train_time:112004ms step_avg:97.73ms
step:1157/1770 train_time:112106ms step_avg:97.74ms
step:1158/1770 train_time:112208ms step_avg:97.74ms
step:1159/1770 train_time:112309ms step_avg:97.74ms
step:1160/1770 train_time:112410ms step_avg:97.75ms
step:1161/1770 train_time:112510ms step_avg:97.75ms
step:1162/1770 train_time:112612ms step_avg:97.75ms
step:1163/1770 train_time:112713ms step_avg:97.76ms
step:1164/1770 train_time:112814ms step_avg:97.76ms
step:1165/1770 train_time:112917ms step_avg:97.76ms
step:1166/1770 train_time:113018ms step_avg:97.77ms
step:1167/1770 train_time:113118ms step_avg:97.77ms
step:1168/1770 train_time:113219ms step_avg:97.77ms
step:1169/1770 train_time:113321ms step_avg:97.77ms
step:1170/1770 train_time:113422ms step_avg:97.78ms
step:1171/1770 train_time:113524ms step_avg:97.78ms
step:1172/1770 train_time:113625ms step_avg:97.78ms
step:1173/1770 train_time:113727ms step_avg:97.79ms
step:1174/1770 train_time:113828ms step_avg:97.79ms
step:1175/1770 train_time:113929ms step_avg:97.79ms
step:1176/1770 train_time:114030ms step_avg:97.80ms
step:1177/1770 train_time:114131ms step_avg:97.80ms
step:1178/1770 train_time:114232ms step_avg:97.80ms
step:1179/1770 train_time:114333ms step_avg:97.80ms
step:1180/1770 train_time:114433ms step_avg:97.81ms
step:1181/1770 train_time:114533ms step_avg:97.81ms
step:1182/1770 train_time:114635ms step_avg:97.81ms
step:1183/1770 train_time:114738ms step_avg:97.82ms
step:1184/1770 train_time:114841ms step_avg:97.82ms
step:1185/1770 train_time:114944ms step_avg:97.82ms
step:1186/1770 train_time:115047ms step_avg:97.83ms
step:1187/1770 train_time:115151ms step_avg:97.83ms
step:1188/1770 train_time:115252ms step_avg:97.84ms
step:1189/1770 train_time:115353ms step_avg:97.84ms
step:1190/1770 train_time:115454ms step_avg:97.84ms
step:1191/1770 train_time:115557ms step_avg:97.85ms
step:1192/1770 train_time:115659ms step_avg:97.85ms
step:1193/1770 train_time:115761ms step_avg:97.85ms
step:1194/1770 train_time:115863ms step_avg:97.86ms
step:1195/1770 train_time:115967ms step_avg:97.86ms
step:1196/1770 train_time:116070ms step_avg:97.87ms
step:1197/1770 train_time:116172ms step_avg:97.87ms
step:1198/1770 train_time:116274ms step_avg:97.87ms
step:1199/1770 train_time:116376ms step_avg:97.88ms
step:1200/1770 train_time:116479ms step_avg:97.88ms
step:1201/1770 train_time:116581ms step_avg:97.89ms
step:1202/1770 train_time:116682ms step_avg:97.89ms
step:1203/1770 train_time:116785ms step_avg:97.89ms
step:1204/1770 train_time:116888ms step_avg:97.90ms
step:1205/1770 train_time:116990ms step_avg:97.90ms
step:1206/1770 train_time:117093ms step_avg:97.90ms
step:1207/1770 train_time:117196ms step_avg:97.91ms
step:1208/1770 train_time:117297ms step_avg:97.91ms
step:1209/1770 train_time:117399ms step_avg:97.91ms
step:1210/1770 train_time:117501ms step_avg:97.92ms
step:1211/1770 train_time:117604ms step_avg:97.92ms
step:1212/1770 train_time:117708ms step_avg:97.93ms
step:1213/1770 train_time:117810ms step_avg:97.93ms
step:1214/1770 train_time:117911ms step_avg:97.93ms
step:1215/1770 train_time:118014ms step_avg:97.94ms
step:1216/1770 train_time:118118ms step_avg:97.94ms
step:1217/1770 train_time:118221ms step_avg:97.95ms
step:1218/1770 train_time:118322ms step_avg:97.95ms
step:1219/1770 train_time:118424ms step_avg:97.95ms
step:1220/1770 train_time:118527ms step_avg:97.96ms
step:1221/1770 train_time:118628ms step_avg:97.96ms
step:1222/1770 train_time:118732ms step_avg:97.96ms
step:1223/1770 train_time:118833ms step_avg:97.97ms
step:1224/1770 train_time:118936ms step_avg:97.97ms
step:1225/1770 train_time:119037ms step_avg:97.97ms
step:1226/1770 train_time:119139ms step_avg:97.98ms
step:1227/1770 train_time:119244ms step_avg:97.98ms
step:1228/1770 train_time:119349ms step_avg:97.99ms
step:1229/1770 train_time:119451ms step_avg:97.99ms
step:1230/1770 train_time:119554ms step_avg:97.99ms
step:1231/1770 train_time:119655ms step_avg:98.00ms
step:1232/1770 train_time:119758ms step_avg:98.00ms
step:1233/1770 train_time:119859ms step_avg:98.00ms
step:1234/1770 train_time:119961ms step_avg:98.01ms
step:1235/1770 train_time:120063ms step_avg:98.01ms
step:1236/1770 train_time:120166ms step_avg:98.01ms
step:1237/1770 train_time:120268ms step_avg:98.02ms
step:1238/1770 train_time:120371ms step_avg:98.02ms
step:1239/1770 train_time:120473ms step_avg:98.03ms
step:1240/1770 train_time:120576ms step_avg:98.03ms
step:1241/1770 train_time:120677ms step_avg:98.03ms
step:1242/1770 train_time:120780ms step_avg:98.04ms
step:1243/1770 train_time:120883ms step_avg:98.04ms
step:1244/1770 train_time:120985ms step_avg:98.04ms
step:1245/1770 train_time:121086ms step_avg:98.05ms
step:1246/1770 train_time:121188ms step_avg:98.05ms
step:1247/1770 train_time:121292ms step_avg:98.05ms
step:1248/1770 train_time:121395ms step_avg:98.06ms
step:1249/1770 train_time:121497ms step_avg:98.06ms
step:1250/1770 train_time:121598ms step_avg:98.06ms
step:1250/1770 val_loss:3.4229 train_time:121700ms step_avg:98.15ms
step:1251/1770 train_time:121725ms step_avg:98.09ms
step:1252/1770 train_time:121807ms step_avg:98.07ms
step:1253/1770 train_time:121910ms step_avg:98.08ms
step:1254/1770 train_time:122012ms step_avg:98.08ms
step:1255/1770 train_time:122117ms step_avg:98.09ms
step:1256/1770 train_time:122217ms step_avg:98.09ms
step:1257/1770 train_time:122318ms step_avg:98.09ms
step:1258/1770 train_time:122420ms step_avg:98.09ms
step:1259/1770 train_time:122523ms step_avg:98.10ms
step:1260/1770 train_time:122625ms step_avg:98.10ms
step:1261/1770 train_time:122728ms step_avg:98.10ms
step:1262/1770 train_time:122831ms step_avg:98.11ms
step:1263/1770 train_time:122933ms step_avg:98.11ms
step:1264/1770 train_time:123037ms step_avg:98.12ms
step:1265/1770 train_time:123139ms step_avg:98.12ms
step:1266/1770 train_time:123241ms step_avg:98.12ms
step:1267/1770 train_time:123344ms step_avg:98.13ms
step:1268/1770 train_time:123446ms step_avg:98.13ms
step:1269/1770 train_time:123548ms step_avg:98.13ms
step:1270/1770 train_time:123651ms step_avg:98.14ms
step:1271/1770 train_time:123754ms step_avg:98.14ms
step:1272/1770 train_time:123855ms step_avg:98.14ms
step:1273/1770 train_time:123958ms step_avg:98.15ms
step:1274/1770 train_time:124061ms step_avg:98.15ms
step:1275/1770 train_time:124162ms step_avg:98.15ms
step:1276/1770 train_time:124265ms step_avg:98.16ms
step:1277/1770 train_time:124366ms step_avg:98.16ms
step:1278/1770 train_time:124470ms step_avg:98.16ms
step:1279/1770 train_time:124573ms step_avg:98.17ms
step:1280/1770 train_time:124675ms step_avg:98.17ms
step:1281/1770 train_time:124777ms step_avg:98.17ms
step:1282/1770 train_time:124880ms step_avg:98.18ms
step:1283/1770 train_time:124983ms step_avg:98.18ms
step:1284/1770 train_time:125085ms step_avg:98.18ms
step:1285/1770 train_time:125187ms step_avg:98.19ms
step:1286/1770 train_time:125291ms step_avg:98.19ms
step:1287/1770 train_time:125395ms step_avg:98.19ms
step:1288/1770 train_time:125497ms step_avg:98.20ms
step:1289/1770 train_time:125599ms step_avg:98.20ms
step:1290/1770 train_time:125701ms step_avg:98.20ms
step:1291/1770 train_time:125803ms step_avg:98.21ms
step:1292/1770 train_time:125906ms step_avg:98.21ms
step:1293/1770 train_time:126008ms step_avg:98.21ms
step:1294/1770 train_time:126109ms step_avg:98.22ms
step:1295/1770 train_time:126212ms step_avg:98.22ms
step:1296/1770 train_time:126314ms step_avg:98.22ms
step:1297/1770 train_time:126415ms step_avg:98.22ms
step:1298/1770 train_time:126517ms step_avg:98.23ms
step:1299/1770 train_time:126620ms step_avg:98.23ms
step:1300/1770 train_time:126721ms step_avg:98.23ms
step:1301/1770 train_time:126823ms step_avg:98.24ms
step:1302/1770 train_time:126926ms step_avg:98.24ms
step:1303/1770 train_time:127029ms step_avg:98.24ms
step:1304/1770 train_time:127132ms step_avg:98.25ms
step:1305/1770 train_time:127234ms step_avg:98.25ms
step:1306/1770 train_time:127336ms step_avg:98.25ms
step:1307/1770 train_time:127438ms step_avg:98.26ms
step:1308/1770 train_time:127540ms step_avg:98.26ms
step:1309/1770 train_time:127642ms step_avg:98.26ms
step:1310/1770 train_time:127743ms step_avg:98.26ms
step:1311/1770 train_time:127845ms step_avg:98.27ms
step:1312/1770 train_time:127947ms step_avg:98.27ms
step:1313/1770 train_time:128049ms step_avg:98.27ms
step:1314/1770 train_time:128151ms step_avg:98.28ms
step:1315/1770 train_time:128254ms step_avg:98.28ms
step:1316/1770 train_time:128356ms step_avg:98.28ms
step:1317/1770 train_time:128459ms step_avg:98.29ms
step:1318/1770 train_time:128564ms step_avg:98.29ms
step:1319/1770 train_time:128666ms step_avg:98.29ms
step:1320/1770 train_time:128768ms step_avg:98.30ms
step:1321/1770 train_time:128871ms step_avg:98.30ms
step:1322/1770 train_time:128973ms step_avg:98.30ms
step:1323/1770 train_time:129077ms step_avg:98.31ms
step:1324/1770 train_time:129179ms step_avg:98.31ms
step:1325/1770 train_time:129282ms step_avg:98.31ms
step:1326/1770 train_time:129383ms step_avg:98.32ms
step:1327/1770 train_time:129488ms step_avg:98.32ms
step:1328/1770 train_time:129590ms step_avg:98.32ms
step:1329/1770 train_time:129693ms step_avg:98.33ms
step:1330/1770 train_time:129794ms step_avg:98.33ms
step:1331/1770 train_time:129896ms step_avg:98.33ms
step:1332/1770 train_time:129998ms step_avg:98.33ms
step:1333/1770 train_time:130099ms step_avg:98.34ms
step:1334/1770 train_time:130201ms step_avg:98.34ms
step:1335/1770 train_time:130303ms step_avg:98.34ms
step:1336/1770 train_time:130404ms step_avg:98.34ms
step:1337/1770 train_time:130507ms step_avg:98.35ms
step:1338/1770 train_time:130609ms step_avg:98.35ms
step:1339/1770 train_time:130713ms step_avg:98.35ms
step:1340/1770 train_time:130817ms step_avg:98.36ms
step:1341/1770 train_time:130919ms step_avg:98.36ms
step:1342/1770 train_time:131022ms step_avg:98.36ms
step:1343/1770 train_time:131125ms step_avg:98.37ms
step:1344/1770 train_time:131228ms step_avg:98.37ms
step:1345/1770 train_time:131330ms step_avg:98.37ms
step:1346/1770 train_time:131432ms step_avg:98.38ms
step:1347/1770 train_time:131535ms step_avg:98.38ms
step:1348/1770 train_time:131641ms step_avg:98.39ms
step:1349/1770 train_time:131743ms step_avg:98.39ms
step:1350/1770 train_time:131847ms step_avg:98.39ms
step:1351/1770 train_time:131949ms step_avg:98.40ms
step:1352/1770 train_time:132052ms step_avg:98.40ms
step:1353/1770 train_time:132154ms step_avg:98.40ms
step:1354/1770 train_time:132256ms step_avg:98.40ms
step:1355/1770 train_time:132358ms step_avg:98.41ms
step:1356/1770 train_time:132461ms step_avg:98.41ms
step:1357/1770 train_time:132563ms step_avg:98.41ms
step:1358/1770 train_time:132665ms step_avg:98.42ms
step:1359/1770 train_time:132768ms step_avg:98.42ms
step:1360/1770 train_time:132871ms step_avg:98.42ms
step:1361/1770 train_time:132973ms step_avg:98.43ms
step:1362/1770 train_time:133075ms step_avg:98.43ms
step:1363/1770 train_time:133178ms step_avg:98.43ms
step:1364/1770 train_time:133281ms step_avg:98.43ms
step:1365/1770 train_time:133382ms step_avg:98.44ms
step:1366/1770 train_time:133484ms step_avg:98.44ms
step:1367/1770 train_time:133587ms step_avg:98.44ms
step:1368/1770 train_time:133688ms step_avg:98.45ms
step:1369/1770 train_time:133791ms step_avg:98.45ms
step:1370/1770 train_time:133894ms step_avg:98.45ms
step:1371/1770 train_time:133997ms step_avg:98.45ms
step:1372/1770 train_time:134098ms step_avg:98.46ms
step:1373/1770 train_time:134200ms step_avg:98.46ms
step:1374/1770 train_time:134303ms step_avg:98.46ms
step:1375/1770 train_time:134405ms step_avg:98.47ms
step:1375/1770 val_loss:3.3801 train_time:134506ms step_avg:98.54ms
step:1376/1770 train_time:134529ms step_avg:98.48ms
step:1377/1770 train_time:134616ms step_avg:98.48ms
step:1378/1770 train_time:134718ms step_avg:98.48ms
step:1379/1770 train_time:134819ms step_avg:98.48ms
step:1380/1770 train_time:134920ms step_avg:98.48ms
step:1381/1770 train_time:135023ms step_avg:98.48ms
step:1382/1770 train_time:135124ms step_avg:98.49ms
step:1383/1770 train_time:135226ms step_avg:98.49ms
step:1384/1770 train_time:135328ms step_avg:98.49ms
step:1385/1770 train_time:135431ms step_avg:98.49ms
step:1386/1770 train_time:135534ms step_avg:98.50ms
step:1387/1770 train_time:135638ms step_avg:98.50ms
step:1388/1770 train_time:135740ms step_avg:98.51ms
step:1389/1770 train_time:135843ms step_avg:98.51ms
step:1390/1770 train_time:135944ms step_avg:98.51ms
step:1391/1770 train_time:136046ms step_avg:98.51ms
step:1392/1770 train_time:136149ms step_avg:98.52ms
step:1393/1770 train_time:136251ms step_avg:98.52ms
step:1394/1770 train_time:136353ms step_avg:98.52ms
step:1395/1770 train_time:136456ms step_avg:98.52ms
step:1396/1770 train_time:136560ms step_avg:98.53ms
step:1397/1770 train_time:136662ms step_avg:98.53ms
step:1398/1770 train_time:136765ms step_avg:98.53ms
step:1399/1770 train_time:136868ms step_avg:98.54ms
step:1400/1770 train_time:136971ms step_avg:98.54ms
step:1401/1770 train_time:137073ms step_avg:98.54ms
step:1402/1770 train_time:137175ms step_avg:98.55ms
step:1403/1770 train_time:137278ms step_avg:98.55ms
step:1404/1770 train_time:137381ms step_avg:98.55ms
step:1405/1770 train_time:137482ms step_avg:98.55ms
step:1406/1770 train_time:137585ms step_avg:98.56ms
step:1407/1770 train_time:137686ms step_avg:98.56ms
step:1408/1770 train_time:137789ms step_avg:98.56ms
step:1409/1770 train_time:137892ms step_avg:98.56ms
step:1410/1770 train_time:137994ms step_avg:98.57ms
step:1411/1770 train_time:138096ms step_avg:98.57ms
step:1412/1770 train_time:138198ms step_avg:98.57ms
step:1413/1770 train_time:138300ms step_avg:98.57ms
step:1414/1770 train_time:138404ms step_avg:98.58ms
step:1415/1770 train_time:138507ms step_avg:98.58ms
step:1416/1770 train_time:138610ms step_avg:98.58ms
step:1417/1770 train_time:138713ms step_avg:98.59ms
step:1418/1770 train_time:138814ms step_avg:98.59ms
step:1419/1770 train_time:138917ms step_avg:98.59ms
step:1420/1770 train_time:139019ms step_avg:98.59ms
step:1421/1770 train_time:139121ms step_avg:98.60ms
step:1422/1770 train_time:139223ms step_avg:98.60ms
step:1423/1770 train_time:139326ms step_avg:98.60ms
step:1424/1770 train_time:139428ms step_avg:98.61ms
step:1425/1770 train_time:139530ms step_avg:98.61ms
step:1426/1770 train_time:139632ms step_avg:98.61ms
step:1427/1770 train_time:139734ms step_avg:98.61ms
step:1428/1770 train_time:139838ms step_avg:98.62ms
step:1429/1770 train_time:139940ms step_avg:98.62ms
step:1430/1770 train_time:140041ms step_avg:98.62ms
step:1431/1770 train_time:140144ms step_avg:98.62ms
step:1432/1770 train_time:140246ms step_avg:98.63ms
step:1433/1770 train_time:140348ms step_avg:98.63ms
step:1434/1770 train_time:140450ms step_avg:98.63ms
step:1435/1770 train_time:140553ms step_avg:98.63ms
step:1436/1770 train_time:140657ms step_avg:98.64ms
step:1437/1770 train_time:140759ms step_avg:98.64ms
step:1438/1770 train_time:140860ms step_avg:98.64ms
step:1439/1770 train_time:140962ms step_avg:98.64ms
step:1440/1770 train_time:141064ms step_avg:98.65ms
step:1441/1770 train_time:141169ms step_avg:98.65ms
step:1442/1770 train_time:141272ms step_avg:98.65ms
step:1443/1770 train_time:141374ms step_avg:98.66ms
step:1444/1770 train_time:141476ms step_avg:98.66ms
step:1445/1770 train_time:141579ms step_avg:98.66ms
step:1446/1770 train_time:141683ms step_avg:98.66ms
step:1447/1770 train_time:141786ms step_avg:98.67ms
step:1448/1770 train_time:141890ms step_avg:98.67ms
step:1449/1770 train_time:141994ms step_avg:98.68ms
step:1450/1770 train_time:142096ms step_avg:98.68ms
step:1451/1770 train_time:142201ms step_avg:98.68ms
step:1452/1770 train_time:142305ms step_avg:98.69ms
step:1453/1770 train_time:142408ms step_avg:98.69ms
step:1454/1770 train_time:142512ms step_avg:98.69ms
step:1455/1770 train_time:142616ms step_avg:98.70ms
step:1456/1770 train_time:142720ms step_avg:98.70ms
step:1457/1770 train_time:142823ms step_avg:98.70ms
step:1458/1770 train_time:142927ms step_avg:98.71ms
step:1459/1770 train_time:143031ms step_avg:98.71ms
step:1460/1770 train_time:143134ms step_avg:98.71ms
step:1461/1770 train_time:143237ms step_avg:98.72ms
step:1462/1770 train_time:143340ms step_avg:98.72ms
step:1463/1770 train_time:143443ms step_avg:98.72ms
step:1464/1770 train_time:143548ms step_avg:98.73ms
step:1465/1770 train_time:143651ms step_avg:98.73ms
step:1466/1770 train_time:143755ms step_avg:98.73ms
step:1467/1770 train_time:143859ms step_avg:98.74ms
step:1468/1770 train_time:143962ms step_avg:98.74ms
step:1469/1770 train_time:144065ms step_avg:98.74ms
step:1470/1770 train_time:144169ms step_avg:98.75ms
step:1471/1770 train_time:144272ms step_avg:98.75ms
step:1472/1770 train_time:144375ms step_avg:98.75ms
step:1473/1770 train_time:144479ms step_avg:98.76ms
step:1474/1770 train_time:144584ms step_avg:98.76ms
step:1475/1770 train_time:144687ms step_avg:98.76ms
step:1476/1770 train_time:144791ms step_avg:98.77ms
step:1477/1770 train_time:144898ms step_avg:98.77ms
step:1478/1770 train_time:145002ms step_avg:98.78ms
step:1479/1770 train_time:145105ms step_avg:98.78ms
step:1480/1770 train_time:145208ms step_avg:98.78ms
step:1481/1770 train_time:145316ms step_avg:98.79ms
step:1482/1770 train_time:145419ms step_avg:98.79ms
step:1483/1770 train_time:145522ms step_avg:98.79ms
step:1484/1770 train_time:145625ms step_avg:98.80ms
step:1485/1770 train_time:145727ms step_avg:98.80ms
step:1486/1770 train_time:145831ms step_avg:98.80ms
step:1487/1770 train_time:145935ms step_avg:98.80ms
step:1488/1770 train_time:146039ms step_avg:98.81ms
step:1489/1770 train_time:146143ms step_avg:98.81ms
step:1490/1770 train_time:146247ms step_avg:98.82ms
step:1491/1770 train_time:146350ms step_avg:98.82ms
step:1492/1770 train_time:146454ms step_avg:98.82ms
step:1493/1770 train_time:146560ms step_avg:98.83ms
step:1494/1770 train_time:146666ms step_avg:98.83ms
step:1495/1770 train_time:146769ms step_avg:98.83ms
step:1496/1770 train_time:146873ms step_avg:98.84ms
step:1497/1770 train_time:146976ms step_avg:98.84ms
step:1498/1770 train_time:147079ms step_avg:98.84ms
step:1499/1770 train_time:147182ms step_avg:98.85ms
step:1500/1770 train_time:147285ms step_avg:98.85ms
step:1500/1770 val_loss:3.3418 train_time:147386ms step_avg:98.92ms
step:1501/1770 train_time:147410ms step_avg:98.87ms
step:1502/1770 train_time:147499ms step_avg:98.86ms
step:1503/1770 train_time:147601ms step_avg:98.86ms
step:1504/1770 train_time:147704ms step_avg:98.86ms
step:1505/1770 train_time:147810ms step_avg:98.87ms
step:1506/1770 train_time:147913ms step_avg:98.87ms
step:1507/1770 train_time:148017ms step_avg:98.88ms
step:1508/1770 train_time:148122ms step_avg:98.88ms
step:1509/1770 train_time:148225ms step_avg:98.88ms
step:1510/1770 train_time:148328ms step_avg:98.89ms
step:1511/1770 train_time:148435ms step_avg:98.89ms
step:1512/1770 train_time:148539ms step_avg:98.89ms
step:1513/1770 train_time:148642ms step_avg:98.90ms
step:1514/1770 train_time:148745ms step_avg:98.90ms
step:1515/1770 train_time:148849ms step_avg:98.90ms
step:1516/1770 train_time:148952ms step_avg:98.91ms
step:1517/1770 train_time:149055ms step_avg:98.91ms
step:1518/1770 train_time:149160ms step_avg:98.91ms
step:1519/1770 train_time:149263ms step_avg:98.91ms
step:1520/1770 train_time:149367ms step_avg:98.92ms
step:1521/1770 train_time:149470ms step_avg:98.92ms
step:1522/1770 train_time:149574ms step_avg:98.92ms
step:1523/1770 train_time:149679ms step_avg:98.93ms
step:1524/1770 train_time:149781ms step_avg:98.93ms
step:1525/1770 train_time:149885ms step_avg:98.93ms
step:1526/1770 train_time:149989ms step_avg:98.94ms
step:1527/1770 train_time:150093ms step_avg:98.94ms
step:1528/1770 train_time:150198ms step_avg:98.94ms
step:1529/1770 train_time:150301ms step_avg:98.95ms
step:1530/1770 train_time:150403ms step_avg:98.95ms
step:1531/1770 train_time:150507ms step_avg:98.95ms
step:1532/1770 train_time:150612ms step_avg:98.96ms
step:1533/1770 train_time:150716ms step_avg:98.96ms
step:1534/1770 train_time:150820ms step_avg:98.96ms
step:1535/1770 train_time:150922ms step_avg:98.97ms
step:1536/1770 train_time:151026ms step_avg:98.97ms
step:1537/1770 train_time:151130ms step_avg:98.97ms
step:1538/1770 train_time:151235ms step_avg:98.98ms
step:1539/1770 train_time:151338ms step_avg:98.98ms
step:1540/1770 train_time:151444ms step_avg:98.98ms
step:1541/1770 train_time:151549ms step_avg:98.99ms
step:1542/1770 train_time:151653ms step_avg:98.99ms
step:1543/1770 train_time:151756ms step_avg:98.99ms
step:1544/1770 train_time:151862ms step_avg:99.00ms
step:1545/1770 train_time:151965ms step_avg:99.00ms
step:1546/1770 train_time:152068ms step_avg:99.00ms
step:1547/1770 train_time:152170ms step_avg:99.00ms
step:1548/1770 train_time:152273ms step_avg:99.01ms
step:1549/1770 train_time:152376ms step_avg:99.01ms
step:1550/1770 train_time:152480ms step_avg:99.01ms
step:1551/1770 train_time:152583ms step_avg:99.02ms
step:1552/1770 train_time:152688ms step_avg:99.02ms
step:1553/1770 train_time:152791ms step_avg:99.02ms
step:1554/1770 train_time:152893ms step_avg:99.02ms
step:1555/1770 train_time:152998ms step_avg:99.03ms
step:1556/1770 train_time:153101ms step_avg:99.03ms
step:1557/1770 train_time:153205ms step_avg:99.03ms
step:1558/1770 train_time:153308ms step_avg:99.04ms
step:1559/1770 train_time:153412ms step_avg:99.04ms
step:1560/1770 train_time:153514ms step_avg:99.04ms
step:1561/1770 train_time:153621ms step_avg:99.05ms
step:1562/1770 train_time:153724ms step_avg:99.05ms
step:1563/1770 train_time:153828ms step_avg:99.05ms
step:1564/1770 train_time:153931ms step_avg:99.05ms
step:1565/1770 train_time:154034ms step_avg:99.06ms
step:1566/1770 train_time:154137ms step_avg:99.06ms
step:1567/1770 train_time:154240ms step_avg:99.06ms
step:1568/1770 train_time:154342ms step_avg:99.06ms
step:1569/1770 train_time:154451ms step_avg:99.07ms
step:1570/1770 train_time:154553ms step_avg:99.07ms
step:1571/1770 train_time:154657ms step_avg:99.08ms
step:1572/1770 train_time:154761ms step_avg:99.08ms
step:1573/1770 train_time:154866ms step_avg:99.08ms
step:1574/1770 train_time:154969ms step_avg:99.09ms
step:1575/1770 train_time:155071ms step_avg:99.09ms
step:1576/1770 train_time:155175ms step_avg:99.09ms
step:1577/1770 train_time:155280ms step_avg:99.09ms
step:1578/1770 train_time:155384ms step_avg:99.10ms
step:1579/1770 train_time:155488ms step_avg:99.10ms
step:1580/1770 train_time:155591ms step_avg:99.10ms
step:1581/1770 train_time:155698ms step_avg:99.11ms
step:1582/1770 train_time:155803ms step_avg:99.11ms
step:1583/1770 train_time:155907ms step_avg:99.11ms
step:1584/1770 train_time:156012ms step_avg:99.12ms
step:1585/1770 train_time:156115ms step_avg:99.12ms
step:1586/1770 train_time:156223ms step_avg:99.13ms
step:1587/1770 train_time:156326ms step_avg:99.13ms
step:1588/1770 train_time:156429ms step_avg:99.13ms
step:1589/1770 train_time:156534ms step_avg:99.14ms
step:1590/1770 train_time:156637ms step_avg:99.14ms
step:1591/1770 train_time:156740ms step_avg:99.14ms
step:1592/1770 train_time:156844ms step_avg:99.14ms
step:1593/1770 train_time:156947ms step_avg:99.15ms
step:1594/1770 train_time:157050ms step_avg:99.15ms
step:1595/1770 train_time:157153ms step_avg:99.15ms
step:1596/1770 train_time:157258ms step_avg:99.15ms
step:1597/1770 train_time:157361ms step_avg:99.16ms
step:1598/1770 train_time:157464ms step_avg:99.16ms
step:1599/1770 train_time:157569ms step_avg:99.16ms
step:1600/1770 train_time:157675ms step_avg:99.17ms
step:1601/1770 train_time:157779ms step_avg:99.17ms
step:1602/1770 train_time:157884ms step_avg:99.17ms
step:1603/1770 train_time:157987ms step_avg:99.18ms
step:1604/1770 train_time:158091ms step_avg:99.18ms
step:1605/1770 train_time:158193ms step_avg:99.18ms
step:1606/1770 train_time:158296ms step_avg:99.18ms
step:1607/1770 train_time:158403ms step_avg:99.19ms
step:1608/1770 train_time:158506ms step_avg:99.19ms
step:1609/1770 train_time:158610ms step_avg:99.19ms
step:1610/1770 train_time:158715ms step_avg:99.20ms
step:1611/1770 train_time:158820ms step_avg:99.20ms
step:1612/1770 train_time:158925ms step_avg:99.20ms
step:1613/1770 train_time:159027ms step_avg:99.21ms
step:1614/1770 train_time:159131ms step_avg:99.21ms
step:1615/1770 train_time:159235ms step_avg:99.21ms
step:1616/1770 train_time:159338ms step_avg:99.21ms
step:1617/1770 train_time:159444ms step_avg:99.22ms
step:1618/1770 train_time:159548ms step_avg:99.22ms
step:1619/1770 train_time:159652ms step_avg:99.22ms
step:1620/1770 train_time:159756ms step_avg:99.23ms
step:1621/1770 train_time:159859ms step_avg:99.23ms
step:1622/1770 train_time:159963ms step_avg:99.23ms
step:1623/1770 train_time:160070ms step_avg:99.24ms
step:1624/1770 train_time:160172ms step_avg:99.24ms
step:1625/1770 train_time:160275ms step_avg:99.24ms
step:1625/1770 val_loss:3.3071 train_time:160377ms step_avg:99.30ms
step:1626/1770 train_time:160403ms step_avg:99.26ms
step:1627/1770 train_time:160487ms step_avg:99.25ms
step:1628/1770 train_time:160590ms step_avg:99.25ms
step:1629/1770 train_time:160692ms step_avg:99.25ms
step:1630/1770 train_time:160795ms step_avg:99.26ms
step:1631/1770 train_time:160897ms step_avg:99.26ms
step:1632/1770 train_time:161001ms step_avg:99.26ms
step:1633/1770 train_time:161105ms step_avg:99.26ms
step:1634/1770 train_time:161207ms step_avg:99.27ms
step:1635/1770 train_time:161310ms step_avg:99.27ms
step:1636/1770 train_time:161414ms step_avg:99.27ms
step:1637/1770 train_time:161518ms step_avg:99.27ms
step:1638/1770 train_time:161621ms step_avg:99.28ms
step:1639/1770 train_time:161728ms step_avg:99.28ms
step:1640/1770 train_time:161830ms step_avg:99.28ms
step:1641/1770 train_time:161933ms step_avg:99.28ms
step:1642/1770 train_time:162035ms step_avg:99.29ms
step:1643/1770 train_time:162138ms step_avg:99.29ms
step:1644/1770 train_time:162244ms step_avg:99.29ms
step:1645/1770 train_time:162349ms step_avg:99.30ms
step:1646/1770 train_time:162453ms step_avg:99.30ms
step:1647/1770 train_time:162557ms step_avg:99.30ms
step:1648/1770 train_time:162660ms step_avg:99.30ms
step:1649/1770 train_time:162764ms step_avg:99.31ms
step:1650/1770 train_time:162867ms step_avg:99.31ms
step:1651/1770 train_time:162970ms step_avg:99.31ms
step:1652/1770 train_time:163073ms step_avg:99.31ms
step:1653/1770 train_time:163176ms step_avg:99.32ms
step:1654/1770 train_time:163285ms step_avg:99.32ms
step:1655/1770 train_time:163390ms step_avg:99.33ms
step:1656/1770 train_time:163493ms step_avg:99.33ms
step:1657/1770 train_time:163598ms step_avg:99.33ms
step:1658/1770 train_time:163702ms step_avg:99.33ms
step:1659/1770 train_time:163808ms step_avg:99.34ms
step:1660/1770 train_time:163911ms step_avg:99.34ms
step:1661/1770 train_time:164015ms step_avg:99.34ms
step:1662/1770 train_time:164118ms step_avg:99.35ms
step:1663/1770 train_time:164221ms step_avg:99.35ms
step:1664/1770 train_time:164325ms step_avg:99.35ms
step:1665/1770 train_time:164428ms step_avg:99.35ms
step:1666/1770 train_time:164532ms step_avg:99.36ms
step:1667/1770 train_time:164635ms step_avg:99.36ms
step:1668/1770 train_time:164738ms step_avg:99.36ms
step:1669/1770 train_time:164841ms step_avg:99.36ms
step:1670/1770 train_time:164945ms step_avg:99.36ms
step:1671/1770 train_time:165049ms step_avg:99.37ms
step:1672/1770 train_time:165153ms step_avg:99.37ms
step:1673/1770 train_time:165259ms step_avg:99.37ms
step:1674/1770 train_time:165362ms step_avg:99.38ms
step:1675/1770 train_time:165465ms step_avg:99.38ms
step:1676/1770 train_time:165571ms step_avg:99.38ms
step:1677/1770 train_time:165678ms step_avg:99.39ms
step:1678/1770 train_time:165780ms step_avg:99.39ms
step:1679/1770 train_time:165884ms step_avg:99.39ms
step:1680/1770 train_time:165987ms step_avg:99.39ms
step:1681/1770 train_time:166091ms step_avg:99.40ms
step:1682/1770 train_time:166197ms step_avg:99.40ms
step:1683/1770 train_time:166299ms step_avg:99.40ms
step:1684/1770 train_time:166403ms step_avg:99.40ms
step:1685/1770 train_time:166508ms step_avg:99.41ms
step:1686/1770 train_time:166612ms step_avg:99.41ms
step:1687/1770 train_time:166716ms step_avg:99.41ms
step:1688/1770 train_time:166820ms step_avg:99.42ms
step:1689/1770 train_time:166924ms step_avg:99.42ms
step:1690/1770 train_time:167027ms step_avg:99.42ms
step:1691/1770 train_time:167130ms step_avg:99.42ms
step:1692/1770 train_time:167234ms step_avg:99.43ms
step:1693/1770 train_time:167339ms step_avg:99.43ms
step:1694/1770 train_time:167442ms step_avg:99.43ms
step:1695/1770 train_time:167546ms step_avg:99.43ms
step:1696/1770 train_time:167651ms step_avg:99.44ms
step:1697/1770 train_time:167757ms step_avg:99.44ms
step:1698/1770 train_time:167861ms step_avg:99.44ms
step:1699/1770 train_time:167965ms step_avg:99.45ms
step:1700/1770 train_time:168068ms step_avg:99.45ms
step:1701/1770 train_time:168173ms step_avg:99.45ms
step:1702/1770 train_time:168277ms step_avg:99.45ms
step:1703/1770 train_time:168380ms step_avg:99.46ms
step:1704/1770 train_time:168484ms step_avg:99.46ms
step:1705/1770 train_time:168588ms step_avg:99.46ms
step:1706/1770 train_time:168690ms step_avg:99.46ms
step:1707/1770 train_time:168794ms step_avg:99.47ms
step:1708/1770 train_time:168898ms step_avg:99.47ms
step:1709/1770 train_time:169003ms step_avg:99.47ms
step:1710/1770 train_time:169110ms step_avg:99.48ms
step:1711/1770 train_time:169216ms step_avg:99.48ms
step:1712/1770 train_time:169320ms step_avg:99.48ms
step:1713/1770 train_time:169424ms step_avg:99.49ms
step:1714/1770 train_time:169528ms step_avg:99.49ms
step:1715/1770 train_time:169631ms step_avg:99.49ms
step:1716/1770 train_time:169735ms step_avg:99.49ms
step:1717/1770 train_time:169838ms step_avg:99.50ms
step:1718/1770 train_time:169945ms step_avg:99.50ms
step:1719/1770 train_time:170049ms step_avg:99.50ms
step:1720/1770 train_time:170155ms step_avg:99.51ms
step:1721/1770 train_time:170258ms step_avg:99.51ms
step:1722/1770 train_time:170366ms step_avg:99.51ms
step:1723/1770 train_time:170471ms step_avg:99.52ms
step:1724/1770 train_time:170578ms step_avg:99.52ms
step:1725/1770 train_time:170684ms step_avg:99.52ms
step:1726/1770 train_time:170790ms step_avg:99.53ms
step:1727/1770 train_time:170893ms step_avg:99.53ms
step:1728/1770 train_time:170999ms step_avg:99.53ms
step:1729/1770 train_time:171103ms step_avg:99.54ms
step:1730/1770 train_time:171210ms step_avg:99.54ms
step:1731/1770 train_time:171315ms step_avg:99.54ms
step:1732/1770 train_time:171418ms step_avg:99.55ms
step:1733/1770 train_time:171524ms step_avg:99.55ms
step:1734/1770 train_time:171628ms step_avg:99.55ms
step:1735/1770 train_time:171733ms step_avg:99.56ms
step:1736/1770 train_time:171838ms step_avg:99.56ms
step:1737/1770 train_time:171942ms step_avg:99.56ms
step:1738/1770 train_time:172046ms step_avg:99.56ms
step:1739/1770 train_time:172151ms step_avg:99.57ms
step:1740/1770 train_time:172254ms step_avg:99.57ms
step:1741/1770 train_time:172361ms step_avg:99.57ms
step:1742/1770 train_time:172468ms step_avg:99.58ms
step:1743/1770 train_time:172573ms step_avg:99.58ms
step:1744/1770 train_time:172678ms step_avg:99.58ms
step:1745/1770 train_time:172782ms step_avg:99.59ms
step:1746/1770 train_time:172889ms step_avg:99.59ms
step:1747/1770 train_time:172992ms step_avg:99.59ms
step:1748/1770 train_time:173099ms step_avg:99.60ms
step:1749/1770 train_time:173204ms step_avg:99.60ms
step:1750/1770 train_time:173308ms step_avg:99.60ms
step:1750/1770 val_loss:3.2806 train_time:173410ms step_avg:99.66ms
step:1751/1770 train_time:173436ms step_avg:99.62ms
step:1752/1770 train_time:173522ms step_avg:99.61ms
step:1753/1770 train_time:173626ms step_avg:99.61ms
step:1754/1770 train_time:173730ms step_avg:99.62ms
step:1755/1770 train_time:173834ms step_avg:99.62ms
step:1756/1770 train_time:173939ms step_avg:99.62ms
step:1757/1770 train_time:174043ms step_avg:99.62ms
step:1758/1770 train_time:174147ms step_avg:99.63ms
step:1759/1770 train_time:174252ms step_avg:99.63ms
step:1760/1770 train_time:174356ms step_avg:99.63ms
step:1761/1770 train_time:174465ms step_avg:99.64ms
step:1762/1770 train_time:174571ms step_avg:99.64ms
step:1763/1770 train_time:174674ms step_avg:99.64ms
step:1764/1770 train_time:174778ms step_avg:99.65ms
step:1765/1770 train_time:174883ms step_avg:99.65ms
step:1766/1770 train_time:174991ms step_avg:99.65ms
step:1767/1770 train_time:175094ms step_avg:99.66ms
step:1768/1770 train_time:175198ms step_avg:99.66ms
step:1769/1770 train_time:175301ms step_avg:99.66ms
step:1770/1770 train_time:175405ms step_avg:99.66ms
step:1770/1770 val_loss:3.2774 train_time:175510ms step_avg:99.72ms
peak memory allocated: 29898 MiB reserved: 44552 MiB

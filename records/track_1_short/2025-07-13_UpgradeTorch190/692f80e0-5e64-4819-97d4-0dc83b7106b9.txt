import os
import sys
with open(sys.argv[0]) as f:
    code = f.read() # read the code of this file ASAP, for logging
import uuid
import time
import copy
import glob
from dataclasses import dataclass
from functools import lru_cache, partial # Added partial for hook registration
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch
torch.empty(1, device="cuda", requires_grad=True).backward() # prevents a bug on some systems
from torch import Tensor, nn, autocast
import torch.nn.functional as F
import torch.distributed as dist
# use of FlexAttention contributed by @KoszarskyB
from torch.nn.attention.flex_attention import BlockMask, flex_attention
#torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
#import wandb

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng

@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Muon optimizer

@torch.compile
def zeropower_via_newtonschulz5(G: Tensor, steps: int) -> Tensor:
    """
    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a
    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose
    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at
    zero even beyond the point where the iteration no longer converges all the way to one everywhere
    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T
    where S' is diagonal with S_{ii}' ~ Uniform(0.5, 1.5), which turns out not to hurt model
    performance at all relative to UV^T, where USV^T = G is the SVD.
    """
    assert G.ndim >= 2 # batched Muon implementation by @scottjmaddox, and put into practice in the record by @YouJiacheng
    a, b, c = (3.4445, -4.7750,  2.0315)
    X = G
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) + 1e-7)
    # Perform the NS iterations
    for _ in range(steps):
        A = X @ X.mT
        B = b * A + c * A @ A # quintic computation strategy adapted from suggestion by @jxbz, @leloykun, and @YouJiacheng
        X = a * X + B @ X

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, rank=0, world_size=1):
        self.rank = rank
        self.world_size = world_size
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)

        params = list(params)
        sizes = {p.shape for p in params}

        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params,))
        super().__init__(param_groups, defaults)

    @torch.no_grad()
    def step(self):
        futures: list[torch.Future] = []
        reduce_scatter_futures: list[torch.Future] = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            grad = torch.empty_like(params[-1])
            grad_pad = [param.grad for param in params] + [torch.zeros_like(params[-1])] * self.world_size
            for base_i in range(0, len(params), self.world_size):
                if base_i + self.rank < len(params):
                    grad = params[base_i + self.rank].grad
                # This gives strange dynamo warnings
                reduce_scatter_futures.append(dist.reduce_scatter(grad, grad_pad[base_i:base_i + self.world_size], op=dist.ReduceOp.AVG, async_op=True).get_future())

        idx = 0
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            params_pad = params + [torch.empty_like(params[-1])] * self.world_size
            momentum = group["momentum"]
            for base_i in range(0, len(params), self.world_size):
                reduce_scatter_futures[idx].wait()
                if base_i + self.rank < len(params):
                    p = params[base_i + self.rank]
                    grad = p.grad
                    eff_lr = group["lr"] * max(1, p.size(-2) / p.size(-1)) ** 0.5 * getattr(p, "lr_mul", 1.0)
                    eff_weight_decay = group["lr"] * group["weight_decay"] * getattr(p, "wd_mul", 1.0)
                    state = self.state[p]
                    if len(state) == 0:
                        state["momentum_buffer"] = torch.zeros_like(grad)
                    momentum_buffer = state["momentum_buffer"]
                    p.mul_(1 - eff_weight_decay)
                    momentum_buffer.lerp_(grad, 1 - momentum)
                    grad = grad.lerp_(momentum_buffer, momentum)
                    v = zeropower_via_newtonschulz5(grad.bfloat16(), 5)
                    p.add_(other=v, alpha=-eff_lr)
                idx += 1
                futures.append(dist.all_gather(params_pad[base_i:base_i + self.world_size], params_pad[base_i + self.rank], async_op=True).get_future())
        # TODO: Check if commenting it is dangerous
        torch.futures.collect_all(futures).wait()

class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01, rank: int = 0, world_size: int = 1):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        self.rank = rank
        self.world_size = world_size

        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(
                params=group_params,
            ))
        super().__init__(param_groups, defaults)

    @torch.compile
    @torch.no_grad()
    def step(self):
        futures: list[torch.Future] = []
        reduce_scatter_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            grad = torch.empty_like(params[-1])
            for base_i in range(len(params)):
                grad = params[base_i].grad
                rank_size = grad.shape[0] // self.world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for base in range(len(params)):
                reduce_scatter_futures[idx].wait()
                p = params[base]
                rank_size = p.shape[0] // self.world_size
                p_slice = p[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(p, "lr_mul", 1.0)
                state = self.state[p]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state['step'] = torch.tensor(0, dtype=torch.int64, device=p.device)
                    state['exp_avg'] = torch.zeros_like(p_slice)
                    state['exp_avg_sq'] = torch.zeros_like(p_slice)

                exp_avg = state['exp_avg']
                exp_avg_sq = state['exp_avg_sq']
                state['step'] += 1
                t = state['step']

                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(p, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)

                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t

                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                futures.append(dist.all_gather_into_tensor(p, p_slice, async_op=True).get_future())
        # TODO: Check if commenting it is dangerous
        torch.futures.collect_all(futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

class Rotary(nn.Module):
    def __init__(self, dim: int, max_seq_len: int):
        super().__init__()
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=dim//4, dtype=torch.float32)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(dim//4)])
        t = torch.arange(max_seq_len, dtype=torch.float32)
        theta = torch.einsum("i,j -> ij", t, angular_freq)
        self.cos = nn.Buffer(theta.cos(), persistent=False)
        self.sin = nn.Buffer(theta.sin(), persistent=False)

    def forward(self, x_BTHD: Tensor):
        assert self.cos.size(0) >= x_BTHD.size(-3)
        cos, sin = self.cos[None, :x_BTHD.size(-3), None, :], self.sin[None, :x_BTHD.size(-3), None, :]
        x1, x2 = x_BTHD.to(dtype=torch.float32).chunk(2, dim=-1)
        y1 = x1 * cos + x2 * sin
        y2 = x1 * (-sin) + x2 * cos
        return torch.cat((y1, y2), 3).type_as(x_BTHD)

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, num_heads: int, max_seq_len: int, head_dim=128):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        hdim = num_heads * head_dim
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        self.qkv_w = nn.Parameter(torch.empty(3, hdim, dim).uniform_(-bound, bound))
        self.rotary = Rotary(head_dim, max_seq_len)
        self.c_proj = CastedLinear(hdim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977
        # scale the attention logits by given constant, instead of the default head_dim**-0.5, by @leloykun
        # inspired by learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.12

    def forward(self, x: Tensor, ve: Tensor | None, lambdas: Tensor, block_mask: BlockMask):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "Must use batch size = 1 for FlexAttention"
        q, k, v = F.linear(x, self.qkv_w.flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = self.rotary(q), self.rotary(k)
        if ve is not None:
            v = lambdas[0] * v + lambdas[1] * ve.view_as(v) # @KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = lambdas[0] * v
        y = flex_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), block_mask=block_mask, scale=self.attn_scale).transpose(1, 2)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = self.c_proj(y)
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        self.c_fc = CastedLinear(dim, hdim)
        self.c_proj = CastedLinear(hdim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = self.c_fc(x)
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = self.c_proj(x)
        return x

class Block(nn.Module):
    def __init__(self, dim: int, num_heads: int, max_seq_len: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, num_heads, max_seq_len) if layer_idx != 7 else None
        self.mlp = MLP(dim)

    def forward(self, x: Tensor, ve: Tensor | None, x0: Tensor, lambdas: Tensor, sa_lambdas: Tensor, block_mask: BlockMask):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), ve, sa_lambdas, block_mask)
        x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, model_dim: int, max_seq_len: int):
        super().__init__()
        self.embed = nn.Embedding(vocab_size, model_dim)
        for param in self.embed.parameters():
            param.lr_mul = 75.
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        for embeds in self.value_embeds:
            for param in self.value_embeds.parameters():
                param.lr_mul = 75.
        self.blocks = nn.ModuleList([Block(model_dim, num_heads, max_seq_len, i) for i in range(num_layers)])
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        self.lm_head = CastedLinear(model_dim, next_multiple_of_n(vocab_size, n=128), use_fp8=True, x_s=(model_dim**0.5)/448, w_s=24/448, grad_s=1/448)
        self.lm_head.weight.lr_mul = 27.5
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5) % world_size
        self.scalars = nn.Parameter(torch.cat([
            torch.ones(num_layers), # skip_weights
            *[torch.tensor([1.0, 0.0]) for _ in range(num_layers)], # block lambdas
            *[torch.tensor([0.5, 0.5]) for _ in range(num_layers)], # SA lambdas
            torch.ones(pad),
        ]))
        self.scalars.lr_mul = 5.0

    def create_blockmasks(self, input_seq: Tensor, sliding_window_num_blocks: Tensor):
        BLOCK_SIZE = 128
        docs = (input_seq == 50256).cumsum(0)

        def document_causal(b, h, q_idx, kv_idx):
            causal_mask = q_idx >= kv_idx
            #return causal_mask
            document_mask = docs[q_idx] == docs[kv_idx]
            return causal_mask & document_mask

        def dense_to_ordered(dense_blockmask: Tensor):
            num_blocks = dense_blockmask.sum(dim=-1, dtype=torch.int32)
            indices = dense_blockmask.argsort(dim=-1, descending=False, stable=True).flip(-1).to(torch.int32)
            return num_blocks[None, None].contiguous(), indices[None, None].contiguous()

        # manual block mask creation by @YouJiacheng
        assert len(input_seq) % BLOCK_SIZE == 0
        NUM_BLOCKS = len(input_seq) // BLOCK_SIZE
        block_idx = torch.arange(NUM_BLOCKS, dtype=torch.int32, device="cuda")
        causal_blockmask_any = block_idx[:, None] >= block_idx
        causal_blockmask_all = block_idx[:, None] > block_idx
        docs_low = docs.view(-1, BLOCK_SIZE)[:, 0].contiguous()
        docs_high = docs.view(-1, BLOCK_SIZE)[:, -1].contiguous()
        document_blockmask_any = (docs_low[:, None] <= docs_high) & (docs_high[:, None] >= docs_low)
        document_blockmask_all = (docs_low[:, None] == docs_high) & (docs_high[:, None] == docs_low)
        blockmask_any = causal_blockmask_any & document_blockmask_any
        blockmask_all = causal_blockmask_all & document_blockmask_all
        partial_kv_num_blocks, partial_kv_indices = dense_to_ordered(blockmask_any & ~blockmask_all)
        full_kv_num_blocks, full_kv_indices = dense_to_ordered(blockmask_all)
        def build_bm(window_size_blocks: Tensor) -> BlockMask:
            return BlockMask.from_kv_blocks(
                torch.clamp_max(partial_kv_num_blocks, torch.clamp_min(window_size_blocks - full_kv_num_blocks, 1)),
                partial_kv_indices,
                torch.clamp_max(full_kv_num_blocks, window_size_blocks - 1),
                full_kv_indices,
                BLOCK_SIZE=BLOCK_SIZE,
                mask_mod=document_causal,
            )
        # Long-short SWA block masks by @leloykun & @YouJiacheng, adapated from suggestion by @Grad62304977, following Gemma 2 paper
        return build_bm(sliding_window_num_blocks), build_bm(sliding_window_num_blocks // 2)

    def forward(self, input_seq: Tensor, target_seq: Tensor, sliding_window_num_blocks: Tensor):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        ve = [ve[0], ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        long_bm, short_bm = self.create_blockmasks(input_seq, sliding_window_num_blocks)
        block_masks = [long_bm, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, long_bm, short_bm, short_bm, short_bm, long_bm]
        assert len(block_masks) == len(self.blocks)

        x = x0 = norm(self.embed(input_seq)[None]) # use of norm here by @Grad62304977

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)

        n = len(self.blocks) // 2

        for i in range(len(self.blocks)):
            if i >= n:
                x = x + skip_weights[i - n] * skip_connections.pop()
            x = self.blocks[i](x, ve[i], x0, lambdas[i], sa_lambdas[i], block_masks[i])
            if i < n:
                skip_connections.append(x)

        x = norm(x)
        logits = self.lm_head(x).float()
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / (7.5 * x.size(-1)**0.5))
        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), target_seq, reduction='sum' if self.training else 'mean')
        return loss

# -----------------------------------------------------------------------------
# Our own simple Distributed Data Loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

def distributed_data_generator(filename_pattern: str, batch_size: int, rank : int, world_size : int):
    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    assert batch_size % world_size == 0
    local_batch_size = batch_size // world_size
    file_iter = iter(files) # use itertools.cycle(files) instead if you want to do multi-epoch training
    tokens, pos = _load_data_shard(next(file_iter)), 0
    while True:
        if pos + batch_size + 1 >= len(tokens):
            tokens, pos = _load_data_shard(next(file_iter)), 0
        buf = tokens[pos + rank * local_batch_size:][:local_batch_size + 1]
        inputs = buf[:-1].to(device="cuda", dtype=torch.int32, non_blocking=True) # no sync on host side;
        targets = buf[1:].to(device="cuda", dtype=torch.int64, non_blocking=True) # H2D in another stream isn't helpful.
        pos += batch_size
        yield inputs, targets

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # optimization
    num_iterations = 1770 # number of iterations to run
    cooldown_frac = 0.4 # fraction of training spent cooling down the learning rate
    # evaluation and logging
    val_loss_every = 125 # every how many steps to evaluate val loss? 0 for only at the end
    # implementation
    seq_len = 48*1024 # FlexAttention sequence length
    val_seq_len = 4*64*1024 # FlexAttention sequence length for validation
    save_checkpoint = False
args = Hyperparameters()

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

#if master_process:
#    wandb.init(project="modded-nanogpt-tiny", name=f"run-{os.path.basename(__file__)}", save_code=True)

# begin logging
logfile = None
if master_process:
    run_id = uuid.uuid4()
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=True):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
if master_process:
    print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(vocab_size=next_multiple_of_n(50257, n=128), num_layers=12, num_heads=6, model_dim=768, max_seq_len=max(args.seq_len, args.val_seq_len)).cuda()
for m in model.modules():
    if isinstance(m, nn.Embedding):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094

optimizer1 = DistAdam(scalar_params + head_params + embed_params, lr=0.008, betas=(0.8, 0.95), eps=1e-10, weight_decay=0.0, rank=rank, world_size=world_size)
optimizer2 = Muon(hidden_matrix_params, lr=0.05, momentum=0.95, rank=rank, world_size=world_size, weight_decay=0.0)
optimizers = [optimizer1, optimizer2]

for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

for n, p in model.named_parameters():
    wd_mul = getattr(p, "wd_mul", 1.0)
    lr_mul = getattr(p, "lr_mul", 1.0)

    print0(f"{n}: {p.shape} {p.dtype} {wd_mul} {lr_mul}")

# Count parameters
total_params = sum(p.numel() for p in model.parameters())
embedding_params = sum(p.numel() for n, p in model.named_parameters() if "embed" in n)
non_embedding_params = total_params - embedding_params

print0(f"")
print0(f"Model parameters:")
print0(f"  Total parameters: {total_params:,}")
print0(f"  Embedding parameters: {embedding_params:,}")
print0(f"  Non-embedding parameters: {non_embedding_params:,}")

# learning rate schedule: stable then decay
def get_lr(step: int):
    x = step / args.num_iterations # progress in training
    assert 0 <= x <= 1
    w = min((1 - x) / args.cooldown_frac, 1.0) # 1 -> 0
    return w * 1.0 + (1 - w) * 0.1
@lru_cache(1)
def get_window_size_blocks_helper(window_size: int):
    return torch.tensor(window_size // 128, dtype=torch.int32, pin_memory=True).cuda(non_blocking=True)
def get_window_size_blocks(step: int):
    x = step / args.num_iterations # progress in training
    assert 0 <= x <= 1
    # Linearly increase the block-wise sliding window size over training 128 -> 1792
    # increase by @fernbear.bsky.social; block-wise by @YouJiacheng
    window_size = next_multiple_of_n(1728 * x, n=128)
    return get_window_size_blocks_helper(window_size)

model: nn.Module = torch.compile(model, dynamic=False)

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 10
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, world_size * args.seq_len, rank, world_size)
for _ in range(warmup_steps):
    inputs, targets = next(train_loader)
    model(inputs, targets, get_window_size_blocks(1)).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)

torch.cuda.synchronize()
dist.barrier()
with torch.profiler.profile() as prof:
    for _ in range(warmup_steps):
        inputs, targets = next(train_loader)
        model(inputs, targets, get_window_size_blocks(1)).backward()
        for opt in optimizers:
            opt.step()
    model.zero_grad(set_to_none=True)
    torch.cuda.synchronize()
    dist.barrier()
os.makedirs("traces", exist_ok=True)
prof.export_chrome_trace(f"traces/trace_{rank}.json")

model.load_state_dict(initial_state['model'])
for opt, opt_state in zip(optimizers, initial_state['optimizers']):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

train_loader = distributed_data_generator(args.train_files, world_size * args.seq_len, rank, world_size)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        val_batch_size = world_size * args.val_seq_len
        assert args.val_tokens % val_batch_size == 0
        val_steps = args.val_tokens // val_batch_size
        val_loader = distributed_data_generator(args.val_files, val_batch_size, rank, world_size)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets = next(val_loader)
                val_loss += model(inputs, targets, get_window_size_blocks(step))
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        #if master_process:
        #    wandb.log({"val/loss": val_loss}, step=step)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    inputs, targets = next(train_loader)
    model(inputs, targets, get_window_size_blocks(step)).backward()
    # set optimization hyperparameters
    for opt in optimizers:
        for group in opt.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)
    frac = min(step / 300, 1)
    for group in optimizer2.param_groups:
        group["momentum"] = (1 - frac) * 0.85 + frac * 0.95
    # step the optimizers and schedulers
    for opt in optimizers:
        opt.step()
    # null the gradients
    model.zero_grad(set_to_none=True)
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.12.7 (main, Jul 14 2025, 05:23:05) [GCC 13.2.0]
Running PyTorch 2.9.0.dev20250713+cu126 compiled for CUDA 12.6
Mon Jul 14 05:55:22 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.148.08             Driver Version: 570.148.08     CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:61:00.0 Off |                    0 |
| N/A   34C    P0            127W /  700W |    5858MiB /  81559MiB |      3%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:62:00.0 Off |                    0 |
| N/A   42C    P0            129W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:63:00.0 Off |                    0 |
| N/A   42C    P0            124W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:64:00.0 Off |                    0 |
| N/A   34C    P0            127W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:6A:00.0 Off |                    0 |
| N/A   32C    P0            118W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:6B:00.0 Off |                    0 |
| N/A   40C    P0            125W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:6C:00.0 Off |                    0 |
| N/A   40C    P0            131W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:6D:00.0 Off |                    0 |
| N/A   30C    P0            117W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A           39038      C   /usr/local/bin/python                  1510MiB |
|    0   N/A  N/A           39039      C   /usr/local/bin/python                   614MiB |
|    0   N/A  N/A           39040      C   /usr/local/bin/python                   614MiB |
|    0   N/A  N/A           39041      C   /usr/local/bin/python                   614MiB |
|    0   N/A  N/A           39042      C   /usr/local/bin/python                   614MiB |
|    0   N/A  N/A           39043      C   /usr/local/bin/python                   614MiB |
|    0   N/A  N/A           39044      C   /usr/local/bin/python                   614MiB |
|    0   N/A  N/A           39045      C   /usr/local/bin/python                   614MiB |
|    1   N/A  N/A           39039      C   /usr/local/bin/python                  1510MiB |
|    2   N/A  N/A           39040      C   /usr/local/bin/python                  1510MiB |
|    3   N/A  N/A           39041      C   /usr/local/bin/python                  1510MiB |
|    4   N/A  N/A           39042      C   /usr/local/bin/python                  1510MiB |
|    5   N/A  N/A           39043      C   /usr/local/bin/python                  1510MiB |
|    6   N/A  N/A           39044      C   /usr/local/bin/python                  1510MiB |
|    7   N/A  N/A           39045      C   /usr/local/bin/python                  1510MiB |
+-----------------------------------------------------------------------------------------+

====================================================================================================
scalars: torch.Size([64]) torch.float32 1.0 5.0
embed.weight: torch.Size([50304, 768]) torch.bfloat16 1.0 75.0
value_embeds.0.weight: torch.Size([50304, 768]) torch.bfloat16 1.0 75.0
value_embeds.1.weight: torch.Size([50304, 768]) torch.bfloat16 1.0 75.0
value_embeds.2.weight: torch.Size([50304, 768]) torch.bfloat16 1.0 75.0
blocks.0.attn.qkv_w: torch.Size([3, 768, 768]) torch.float32 1.0 1.0
blocks.0.attn.c_proj.weight: torch.Size([768, 768]) torch.float32 1.0 1.0
blocks.0.mlp.c_fc.weight: torch.Size([3072, 768]) torch.float32 1.0 1.0
blocks.0.mlp.c_proj.weight: torch.Size([768, 3072]) torch.float32 1.0 1.0
blocks.1.attn.qkv_w: torch.Size([3, 768, 768]) torch.float32 1.0 1.0
blocks.1.attn.c_proj.weight: torch.Size([768, 768]) torch.float32 1.0 1.0
blocks.1.mlp.c_fc.weight: torch.Size([3072, 768]) torch.float32 1.0 1.0
blocks.1.mlp.c_proj.weight: torch.Size([768, 3072]) torch.float32 1.0 1.0
blocks.2.attn.qkv_w: torch.Size([3, 768, 768]) torch.float32 1.0 1.0
blocks.2.attn.c_proj.weight: torch.Size([768, 768]) torch.float32 1.0 1.0
blocks.2.mlp.c_fc.weight: torch.Size([3072, 768]) torch.float32 1.0 1.0
blocks.2.mlp.c_proj.weight: torch.Size([768, 3072]) torch.float32 1.0 1.0
blocks.3.attn.qkv_w: torch.Size([3, 768, 768]) torch.float32 1.0 1.0
blocks.3.attn.c_proj.weight: torch.Size([768, 768]) torch.float32 1.0 1.0
blocks.3.mlp.c_fc.weight: torch.Size([3072, 768]) torch.float32 1.0 1.0
blocks.3.mlp.c_proj.weight: torch.Size([768, 3072]) torch.float32 1.0 1.0
blocks.4.attn.qkv_w: torch.Size([3, 768, 768]) torch.float32 1.0 1.0
blocks.4.attn.c_proj.weight: torch.Size([768, 768]) torch.float32 1.0 1.0
blocks.4.mlp.c_fc.weight: torch.Size([3072, 768]) torch.float32 1.0 1.0
blocks.4.mlp.c_proj.weight: torch.Size([768, 3072]) torch.float32 1.0 1.0
blocks.5.attn.qkv_w: torch.Size([3, 768, 768]) torch.float32 1.0 1.0
blocks.5.attn.c_proj.weight: torch.Size([768, 768]) torch.float32 1.0 1.0
blocks.5.mlp.c_fc.weight: torch.Size([3072, 768]) torch.float32 1.0 1.0
blocks.5.mlp.c_proj.weight: torch.Size([768, 3072]) torch.float32 1.0 1.0
blocks.6.attn.qkv_w: torch.Size([3, 768, 768]) torch.float32 1.0 1.0
blocks.6.attn.c_proj.weight: torch.Size([768, 768]) torch.float32 1.0 1.0
blocks.6.mlp.c_fc.weight: torch.Size([3072, 768]) torch.float32 1.0 1.0
blocks.6.mlp.c_proj.weight: torch.Size([768, 3072]) torch.float32 1.0 1.0
blocks.7.mlp.c_fc.weight: torch.Size([3072, 768]) torch.float32 1.0 1.0
blocks.7.mlp.c_proj.weight: torch.Size([768, 3072]) torch.float32 1.0 1.0
blocks.8.attn.qkv_w: torch.Size([3, 768, 768]) torch.float32 1.0 1.0
blocks.8.attn.c_proj.weight: torch.Size([768, 768]) torch.float32 1.0 1.0
blocks.8.mlp.c_fc.weight: torch.Size([3072, 768]) torch.float32 1.0 1.0
blocks.8.mlp.c_proj.weight: torch.Size([768, 3072]) torch.float32 1.0 1.0
blocks.9.attn.qkv_w: torch.Size([3, 768, 768]) torch.float32 1.0 1.0
blocks.9.attn.c_proj.weight: torch.Size([768, 768]) torch.float32 1.0 1.0
blocks.9.mlp.c_fc.weight: torch.Size([3072, 768]) torch.float32 1.0 1.0
blocks.9.mlp.c_proj.weight: torch.Size([768, 3072]) torch.float32 1.0 1.0
blocks.10.attn.qkv_w: torch.Size([3, 768, 768]) torch.float32 1.0 1.0
blocks.10.attn.c_proj.weight: torch.Size([768, 768]) torch.float32 1.0 1.0
blocks.10.mlp.c_fc.weight: torch.Size([3072, 768]) torch.float32 1.0 1.0
blocks.10.mlp.c_proj.weight: torch.Size([768, 3072]) torch.float32 1.0 1.0
blocks.11.attn.qkv_w: torch.Size([3, 768, 768]) torch.float32 1.0 1.0
blocks.11.attn.c_proj.weight: torch.Size([768, 768]) torch.float32 1.0 1.0
blocks.11.mlp.c_fc.weight: torch.Size([3072, 768]) torch.float32 1.0 1.0
blocks.11.mlp.c_proj.weight: torch.Size([768, 3072]) torch.float32 1.0 1.0
lm_head.weight: torch.Size([50304, 768]) torch.float32 1.0 27.5

Model parameters:
  Total parameters: 275,742,784
  Embedding parameters: 154,533,888
  Non-embedding parameters: 121,208,896
step:0/1770 val_loss:10.8258 train_time:0ms step_avg:0.02ms
step:1/1770 train_time:146ms step_avg:145.57ms
step:2/1770 train_time:181ms step_avg:90.64ms
step:3/1770 train_time:246ms step_avg:82.16ms
step:4/1770 train_time:338ms step_avg:84.59ms
step:5/1770 train_time:431ms step_avg:86.15ms
step:6/1770 train_time:523ms step_avg:87.12ms
step:7/1770 train_time:615ms step_avg:87.89ms
step:8/1770 train_time:708ms step_avg:88.46ms
step:9/1770 train_time:802ms step_avg:89.09ms
step:10/1770 train_time:895ms step_avg:89.45ms
step:11/1770 train_time:986ms step_avg:89.65ms
step:12/1770 train_time:1081ms step_avg:90.05ms
step:13/1770 train_time:1175ms step_avg:90.42ms
step:14/1770 train_time:1269ms step_avg:90.66ms
step:15/1770 train_time:1362ms step_avg:90.82ms
step:16/1770 train_time:1456ms step_avg:90.98ms
step:17/1770 train_time:1548ms step_avg:91.06ms
step:18/1770 train_time:1641ms step_avg:91.19ms
step:19/1770 train_time:1735ms step_avg:91.30ms
step:20/1770 train_time:1827ms step_avg:91.35ms
step:21/1770 train_time:1921ms step_avg:91.47ms
step:22/1770 train_time:2014ms step_avg:91.54ms
step:23/1770 train_time:2108ms step_avg:91.64ms
step:24/1770 train_time:2203ms step_avg:91.77ms
step:25/1770 train_time:2297ms step_avg:91.86ms
step:26/1770 train_time:2390ms step_avg:91.93ms
step:27/1770 train_time:2483ms step_avg:91.95ms
step:28/1770 train_time:2576ms step_avg:92.02ms
step:29/1770 train_time:2668ms step_avg:92.02ms
step:30/1770 train_time:2762ms step_avg:92.07ms
step:31/1770 train_time:2855ms step_avg:92.09ms
step:32/1770 train_time:2947ms step_avg:92.11ms
step:33/1770 train_time:3041ms step_avg:92.14ms
step:34/1770 train_time:3134ms step_avg:92.19ms
step:35/1770 train_time:3228ms step_avg:92.22ms
step:36/1770 train_time:3321ms step_avg:92.26ms
step:37/1770 train_time:3415ms step_avg:92.31ms
step:38/1770 train_time:3509ms step_avg:92.35ms
step:39/1770 train_time:3602ms step_avg:92.37ms
step:40/1770 train_time:3695ms step_avg:92.38ms
step:41/1770 train_time:3788ms step_avg:92.40ms
step:42/1770 train_time:3882ms step_avg:92.43ms
step:43/1770 train_time:3975ms step_avg:92.45ms
step:44/1770 train_time:4069ms step_avg:92.48ms
step:45/1770 train_time:4162ms step_avg:92.48ms
step:46/1770 train_time:4256ms step_avg:92.52ms
step:47/1770 train_time:4349ms step_avg:92.53ms
step:48/1770 train_time:4443ms step_avg:92.56ms
step:49/1770 train_time:4537ms step_avg:92.59ms
step:50/1770 train_time:4629ms step_avg:92.59ms
step:51/1770 train_time:4723ms step_avg:92.60ms
step:52/1770 train_time:4817ms step_avg:92.63ms
step:53/1770 train_time:4909ms step_avg:92.62ms
step:54/1770 train_time:5005ms step_avg:92.68ms
step:55/1770 train_time:5096ms step_avg:92.65ms
step:56/1770 train_time:5187ms step_avg:92.63ms
step:57/1770 train_time:5281ms step_avg:92.64ms
step:58/1770 train_time:5373ms step_avg:92.64ms
step:59/1770 train_time:5466ms step_avg:92.65ms
step:60/1770 train_time:5562ms step_avg:92.69ms
step:61/1770 train_time:5654ms step_avg:92.70ms
step:62/1770 train_time:5748ms step_avg:92.71ms
step:63/1770 train_time:5842ms step_avg:92.73ms
step:64/1770 train_time:5936ms step_avg:92.75ms
step:65/1770 train_time:6028ms step_avg:92.74ms
step:66/1770 train_time:6122ms step_avg:92.75ms
step:67/1770 train_time:6214ms step_avg:92.74ms
step:68/1770 train_time:6306ms step_avg:92.74ms
step:69/1770 train_time:6400ms step_avg:92.76ms
step:70/1770 train_time:6493ms step_avg:92.76ms
step:71/1770 train_time:6585ms step_avg:92.75ms
step:72/1770 train_time:6679ms step_avg:92.77ms
step:73/1770 train_time:6772ms step_avg:92.77ms
step:74/1770 train_time:6866ms step_avg:92.78ms
step:75/1770 train_time:6960ms step_avg:92.80ms
step:76/1770 train_time:7053ms step_avg:92.81ms
step:77/1770 train_time:7146ms step_avg:92.81ms
step:78/1770 train_time:7240ms step_avg:92.83ms
step:79/1770 train_time:7334ms step_avg:92.83ms
step:80/1770 train_time:7427ms step_avg:92.83ms
step:81/1770 train_time:7519ms step_avg:92.83ms
step:82/1770 train_time:7612ms step_avg:92.82ms
step:83/1770 train_time:7705ms step_avg:92.84ms
step:84/1770 train_time:7799ms step_avg:92.85ms
step:85/1770 train_time:7892ms step_avg:92.84ms
step:86/1770 train_time:7984ms step_avg:92.84ms
step:87/1770 train_time:8077ms step_avg:92.84ms
step:88/1770 train_time:8170ms step_avg:92.84ms
step:89/1770 train_time:8263ms step_avg:92.85ms
step:90/1770 train_time:8357ms step_avg:92.85ms
step:91/1770 train_time:8449ms step_avg:92.85ms
step:92/1770 train_time:8543ms step_avg:92.86ms
step:93/1770 train_time:8636ms step_avg:92.86ms
step:94/1770 train_time:8730ms step_avg:92.87ms
step:95/1770 train_time:8822ms step_avg:92.87ms
step:96/1770 train_time:8916ms step_avg:92.88ms
step:97/1770 train_time:9010ms step_avg:92.88ms
step:98/1770 train_time:9103ms step_avg:92.89ms
step:99/1770 train_time:9197ms step_avg:92.90ms
step:100/1770 train_time:9290ms step_avg:92.90ms
step:101/1770 train_time:9383ms step_avg:92.90ms
step:102/1770 train_time:9476ms step_avg:92.90ms
step:103/1770 train_time:9569ms step_avg:92.90ms
step:104/1770 train_time:9662ms step_avg:92.91ms
step:105/1770 train_time:9755ms step_avg:92.91ms
step:106/1770 train_time:9849ms step_avg:92.91ms
step:107/1770 train_time:9942ms step_avg:92.92ms
step:108/1770 train_time:10037ms step_avg:92.93ms
step:109/1770 train_time:10129ms step_avg:92.93ms
step:110/1770 train_time:10222ms step_avg:92.93ms
step:111/1770 train_time:10315ms step_avg:92.93ms
step:112/1770 train_time:10409ms step_avg:92.94ms
step:113/1770 train_time:10502ms step_avg:92.94ms
step:114/1770 train_time:10595ms step_avg:92.94ms
step:115/1770 train_time:10688ms step_avg:92.94ms
step:116/1770 train_time:10782ms step_avg:92.95ms
step:117/1770 train_time:10875ms step_avg:92.95ms
step:118/1770 train_time:10968ms step_avg:92.95ms
step:119/1770 train_time:11062ms step_avg:92.96ms
step:120/1770 train_time:11155ms step_avg:92.96ms
step:121/1770 train_time:11248ms step_avg:92.96ms
step:122/1770 train_time:11343ms step_avg:92.97ms
step:123/1770 train_time:11435ms step_avg:92.97ms
step:124/1770 train_time:11527ms step_avg:92.96ms
step:125/1770 train_time:11621ms step_avg:92.97ms
step:125/1770 val_loss:4.6448 train_time:11702ms step_avg:93.62ms
step:126/1770 train_time:11739ms step_avg:93.17ms
step:127/1770 train_time:11809ms step_avg:92.98ms
step:128/1770 train_time:11910ms step_avg:93.04ms
step:129/1770 train_time:12004ms step_avg:93.05ms
step:130/1770 train_time:12098ms step_avg:93.06ms
step:131/1770 train_time:12190ms step_avg:93.06ms
step:132/1770 train_time:12283ms step_avg:93.05ms
step:133/1770 train_time:12376ms step_avg:93.05ms
step:134/1770 train_time:12468ms step_avg:93.04ms
step:135/1770 train_time:12561ms step_avg:93.04ms
step:136/1770 train_time:12655ms step_avg:93.05ms
step:137/1770 train_time:12748ms step_avg:93.05ms
step:138/1770 train_time:12843ms step_avg:93.06ms
step:139/1770 train_time:12938ms step_avg:93.08ms
step:140/1770 train_time:13033ms step_avg:93.09ms
step:141/1770 train_time:13127ms step_avg:93.10ms
step:142/1770 train_time:13220ms step_avg:93.10ms
step:143/1770 train_time:13313ms step_avg:93.10ms
step:144/1770 train_time:13406ms step_avg:93.10ms
step:145/1770 train_time:13500ms step_avg:93.10ms
step:146/1770 train_time:13593ms step_avg:93.10ms
step:147/1770 train_time:13686ms step_avg:93.10ms
step:148/1770 train_time:13780ms step_avg:93.11ms
step:149/1770 train_time:13874ms step_avg:93.11ms
step:150/1770 train_time:13967ms step_avg:93.11ms
step:151/1770 train_time:14062ms step_avg:93.12ms
step:152/1770 train_time:14156ms step_avg:93.13ms
step:153/1770 train_time:14249ms step_avg:93.13ms
step:154/1770 train_time:14343ms step_avg:93.13ms
step:155/1770 train_time:14437ms step_avg:93.14ms
step:156/1770 train_time:14530ms step_avg:93.14ms
step:157/1770 train_time:14623ms step_avg:93.14ms
step:158/1770 train_time:14718ms step_avg:93.15ms
step:159/1770 train_time:14811ms step_avg:93.15ms
step:160/1770 train_time:14905ms step_avg:93.15ms
step:161/1770 train_time:14999ms step_avg:93.16ms
step:162/1770 train_time:15092ms step_avg:93.16ms
step:163/1770 train_time:15186ms step_avg:93.16ms
step:164/1770 train_time:15280ms step_avg:93.17ms
step:165/1770 train_time:15374ms step_avg:93.17ms
step:166/1770 train_time:15466ms step_avg:93.17ms
step:167/1770 train_time:15560ms step_avg:93.17ms
step:168/1770 train_time:15653ms step_avg:93.17ms
step:169/1770 train_time:15746ms step_avg:93.17ms
step:170/1770 train_time:15840ms step_avg:93.18ms
step:171/1770 train_time:15933ms step_avg:93.18ms
step:172/1770 train_time:16027ms step_avg:93.18ms
step:173/1770 train_time:16121ms step_avg:93.18ms
step:174/1770 train_time:16215ms step_avg:93.19ms
step:175/1770 train_time:16308ms step_avg:93.19ms
step:176/1770 train_time:16403ms step_avg:93.20ms
step:177/1770 train_time:16496ms step_avg:93.20ms
step:178/1770 train_time:16589ms step_avg:93.20ms
step:179/1770 train_time:16683ms step_avg:93.20ms
step:180/1770 train_time:16777ms step_avg:93.21ms
step:181/1770 train_time:16871ms step_avg:93.21ms
step:182/1770 train_time:16965ms step_avg:93.21ms
step:183/1770 train_time:17060ms step_avg:93.22ms
step:184/1770 train_time:17154ms step_avg:93.23ms
step:185/1770 train_time:17249ms step_avg:93.24ms
step:186/1770 train_time:17343ms step_avg:93.24ms
step:187/1770 train_time:17437ms step_avg:93.25ms
step:188/1770 train_time:17532ms step_avg:93.25ms
step:189/1770 train_time:17625ms step_avg:93.25ms
step:190/1770 train_time:17718ms step_avg:93.25ms
step:191/1770 train_time:17811ms step_avg:93.25ms
step:192/1770 train_time:17904ms step_avg:93.25ms
step:193/1770 train_time:17997ms step_avg:93.25ms
step:194/1770 train_time:18091ms step_avg:93.25ms
step:195/1770 train_time:18185ms step_avg:93.26ms
step:196/1770 train_time:18279ms step_avg:93.26ms
step:197/1770 train_time:18373ms step_avg:93.27ms
step:198/1770 train_time:18466ms step_avg:93.26ms
step:199/1770 train_time:18562ms step_avg:93.27ms
step:200/1770 train_time:18655ms step_avg:93.28ms
step:201/1770 train_time:18748ms step_avg:93.28ms
step:202/1770 train_time:18841ms step_avg:93.27ms
step:203/1770 train_time:18935ms step_avg:93.28ms
step:204/1770 train_time:19029ms step_avg:93.28ms
step:205/1770 train_time:19123ms step_avg:93.28ms
step:206/1770 train_time:19216ms step_avg:93.28ms
step:207/1770 train_time:19309ms step_avg:93.28ms
step:208/1770 train_time:19402ms step_avg:93.28ms
step:209/1770 train_time:19496ms step_avg:93.28ms
step:210/1770 train_time:19591ms step_avg:93.29ms
step:211/1770 train_time:19685ms step_avg:93.29ms
step:212/1770 train_time:19779ms step_avg:93.30ms
step:213/1770 train_time:19872ms step_avg:93.30ms
step:214/1770 train_time:19965ms step_avg:93.30ms
step:215/1770 train_time:20060ms step_avg:93.30ms
step:216/1770 train_time:20153ms step_avg:93.30ms
step:217/1770 train_time:20246ms step_avg:93.30ms
step:218/1770 train_time:20341ms step_avg:93.31ms
step:219/1770 train_time:20435ms step_avg:93.31ms
step:220/1770 train_time:20529ms step_avg:93.31ms
step:221/1770 train_time:20622ms step_avg:93.31ms
step:222/1770 train_time:20717ms step_avg:93.32ms
step:223/1770 train_time:20810ms step_avg:93.32ms
step:224/1770 train_time:20904ms step_avg:93.32ms
step:225/1770 train_time:20997ms step_avg:93.32ms
step:226/1770 train_time:21090ms step_avg:93.32ms
step:227/1770 train_time:21184ms step_avg:93.32ms
step:228/1770 train_time:21276ms step_avg:93.32ms
step:229/1770 train_time:21370ms step_avg:93.32ms
step:230/1770 train_time:21463ms step_avg:93.32ms
step:231/1770 train_time:21557ms step_avg:93.32ms
step:232/1770 train_time:21649ms step_avg:93.32ms
step:233/1770 train_time:21743ms step_avg:93.32ms
step:234/1770 train_time:21837ms step_avg:93.32ms
step:235/1770 train_time:21931ms step_avg:93.32ms
step:236/1770 train_time:22025ms step_avg:93.32ms
step:237/1770 train_time:22118ms step_avg:93.32ms
step:238/1770 train_time:22211ms step_avg:93.32ms
step:239/1770 train_time:22303ms step_avg:93.32ms
step:240/1770 train_time:22397ms step_avg:93.32ms
step:241/1770 train_time:22490ms step_avg:93.32ms
step:242/1770 train_time:22583ms step_avg:93.32ms
step:243/1770 train_time:22677ms step_avg:93.32ms
step:244/1770 train_time:22770ms step_avg:93.32ms
step:245/1770 train_time:22863ms step_avg:93.32ms
step:246/1770 train_time:22957ms step_avg:93.32ms
step:247/1770 train_time:23050ms step_avg:93.32ms
step:248/1770 train_time:23143ms step_avg:93.32ms
step:249/1770 train_time:23237ms step_avg:93.32ms
step:250/1770 train_time:23330ms step_avg:93.32ms
step:250/1770 val_loss:4.0973 train_time:23414ms step_avg:93.65ms
step:251/1770 train_time:23450ms step_avg:93.43ms
step:252/1770 train_time:23524ms step_avg:93.35ms
step:253/1770 train_time:23619ms step_avg:93.36ms
step:254/1770 train_time:23713ms step_avg:93.36ms
step:255/1770 train_time:23807ms step_avg:93.36ms
step:256/1770 train_time:23898ms step_avg:93.35ms
step:257/1770 train_time:23991ms step_avg:93.35ms
step:258/1770 train_time:24083ms step_avg:93.35ms
step:259/1770 train_time:24176ms step_avg:93.34ms
step:260/1770 train_time:24269ms step_avg:93.34ms
step:261/1770 train_time:24362ms step_avg:93.34ms
step:262/1770 train_time:24456ms step_avg:93.35ms
step:263/1770 train_time:24551ms step_avg:93.35ms
step:264/1770 train_time:24647ms step_avg:93.36ms
step:265/1770 train_time:24740ms step_avg:93.36ms
step:266/1770 train_time:24834ms step_avg:93.36ms
step:267/1770 train_time:24928ms step_avg:93.36ms
step:268/1770 train_time:25022ms step_avg:93.36ms
step:269/1770 train_time:25116ms step_avg:93.37ms
step:270/1770 train_time:25209ms step_avg:93.37ms
step:271/1770 train_time:25302ms step_avg:93.37ms
step:272/1770 train_time:25395ms step_avg:93.36ms
step:273/1770 train_time:25490ms step_avg:93.37ms
step:274/1770 train_time:25584ms step_avg:93.37ms
step:275/1770 train_time:25678ms step_avg:93.37ms
step:276/1770 train_time:25772ms step_avg:93.38ms
step:277/1770 train_time:25867ms step_avg:93.38ms
step:278/1770 train_time:25959ms step_avg:93.38ms
step:279/1770 train_time:26053ms step_avg:93.38ms
step:280/1770 train_time:26147ms step_avg:93.38ms
step:281/1770 train_time:26241ms step_avg:93.38ms
step:282/1770 train_time:26334ms step_avg:93.38ms
step:283/1770 train_time:26428ms step_avg:93.38ms
step:284/1770 train_time:26522ms step_avg:93.39ms
step:285/1770 train_time:26616ms step_avg:93.39ms
step:286/1770 train_time:26710ms step_avg:93.39ms
step:287/1770 train_time:26803ms step_avg:93.39ms
step:288/1770 train_time:26896ms step_avg:93.39ms
step:289/1770 train_time:26991ms step_avg:93.40ms
step:290/1770 train_time:27086ms step_avg:93.40ms
step:291/1770 train_time:27180ms step_avg:93.40ms
step:292/1770 train_time:27274ms step_avg:93.40ms
step:293/1770 train_time:27369ms step_avg:93.41ms
step:294/1770 train_time:27462ms step_avg:93.41ms
step:295/1770 train_time:27556ms step_avg:93.41ms
step:296/1770 train_time:27651ms step_avg:93.42ms
step:297/1770 train_time:27745ms step_avg:93.42ms
step:298/1770 train_time:27838ms step_avg:93.42ms
step:299/1770 train_time:27931ms step_avg:93.42ms
step:300/1770 train_time:28027ms step_avg:93.42ms
step:301/1770 train_time:28119ms step_avg:93.42ms
step:302/1770 train_time:28212ms step_avg:93.42ms
step:303/1770 train_time:28307ms step_avg:93.42ms
step:304/1770 train_time:28400ms step_avg:93.42ms
step:305/1770 train_time:28494ms step_avg:93.42ms
step:306/1770 train_time:28589ms step_avg:93.43ms
step:307/1770 train_time:28683ms step_avg:93.43ms
step:308/1770 train_time:28776ms step_avg:93.43ms
step:309/1770 train_time:28870ms step_avg:93.43ms
step:310/1770 train_time:28965ms step_avg:93.43ms
step:311/1770 train_time:29058ms step_avg:93.43ms
step:312/1770 train_time:29152ms step_avg:93.43ms
step:313/1770 train_time:29246ms step_avg:93.44ms
step:314/1770 train_time:29340ms step_avg:93.44ms
step:315/1770 train_time:29434ms step_avg:93.44ms
step:316/1770 train_time:29528ms step_avg:93.44ms
step:317/1770 train_time:29622ms step_avg:93.44ms
step:318/1770 train_time:29715ms step_avg:93.44ms
step:319/1770 train_time:29810ms step_avg:93.45ms
step:320/1770 train_time:29904ms step_avg:93.45ms
step:321/1770 train_time:29997ms step_avg:93.45ms
step:322/1770 train_time:30091ms step_avg:93.45ms
step:323/1770 train_time:30186ms step_avg:93.45ms
step:324/1770 train_time:30280ms step_avg:93.46ms
step:325/1770 train_time:30373ms step_avg:93.46ms
step:326/1770 train_time:30467ms step_avg:93.46ms
step:327/1770 train_time:30561ms step_avg:93.46ms
step:328/1770 train_time:30655ms step_avg:93.46ms
step:329/1770 train_time:30749ms step_avg:93.46ms
step:330/1770 train_time:30843ms step_avg:93.46ms
step:331/1770 train_time:30937ms step_avg:93.47ms
step:332/1770 train_time:31031ms step_avg:93.47ms
step:333/1770 train_time:31125ms step_avg:93.47ms
step:334/1770 train_time:31220ms step_avg:93.47ms
step:335/1770 train_time:31313ms step_avg:93.47ms
step:336/1770 train_time:31407ms step_avg:93.47ms
step:337/1770 train_time:31501ms step_avg:93.47ms
step:338/1770 train_time:31594ms step_avg:93.47ms
step:339/1770 train_time:31689ms step_avg:93.48ms
step:340/1770 train_time:31783ms step_avg:93.48ms
step:341/1770 train_time:31876ms step_avg:93.48ms
step:342/1770 train_time:31970ms step_avg:93.48ms
step:343/1770 train_time:32064ms step_avg:93.48ms
step:344/1770 train_time:32157ms step_avg:93.48ms
step:345/1770 train_time:32251ms step_avg:93.48ms
step:346/1770 train_time:32346ms step_avg:93.48ms
step:347/1770 train_time:32439ms step_avg:93.48ms
step:348/1770 train_time:32532ms step_avg:93.48ms
step:349/1770 train_time:32626ms step_avg:93.49ms
step:350/1770 train_time:32720ms step_avg:93.49ms
step:351/1770 train_time:32814ms step_avg:93.49ms
step:352/1770 train_time:32907ms step_avg:93.49ms
step:353/1770 train_time:33001ms step_avg:93.49ms
step:354/1770 train_time:33095ms step_avg:93.49ms
step:355/1770 train_time:33189ms step_avg:93.49ms
step:356/1770 train_time:33283ms step_avg:93.49ms
step:357/1770 train_time:33377ms step_avg:93.49ms
step:358/1770 train_time:33471ms step_avg:93.49ms
step:359/1770 train_time:33564ms step_avg:93.49ms
step:360/1770 train_time:33659ms step_avg:93.50ms
step:361/1770 train_time:33753ms step_avg:93.50ms
step:362/1770 train_time:33848ms step_avg:93.50ms
step:363/1770 train_time:33942ms step_avg:93.50ms
step:364/1770 train_time:34036ms step_avg:93.50ms
step:365/1770 train_time:34130ms step_avg:93.51ms
step:366/1770 train_time:34224ms step_avg:93.51ms
step:367/1770 train_time:34317ms step_avg:93.51ms
step:368/1770 train_time:34411ms step_avg:93.51ms
step:369/1770 train_time:34504ms step_avg:93.51ms
step:370/1770 train_time:34598ms step_avg:93.51ms
step:371/1770 train_time:34692ms step_avg:93.51ms
step:372/1770 train_time:34785ms step_avg:93.51ms
step:373/1770 train_time:34879ms step_avg:93.51ms
step:374/1770 train_time:34973ms step_avg:93.51ms
step:375/1770 train_time:35067ms step_avg:93.51ms
step:375/1770 val_loss:3.8929 train_time:35151ms step_avg:93.74ms
step:376/1770 train_time:35188ms step_avg:93.58ms
step:377/1770 train_time:35261ms step_avg:93.53ms
step:378/1770 train_time:35358ms step_avg:93.54ms
step:379/1770 train_time:35452ms step_avg:93.54ms
step:380/1770 train_time:35546ms step_avg:93.54ms
step:381/1770 train_time:35639ms step_avg:93.54ms
step:382/1770 train_time:35732ms step_avg:93.54ms
step:383/1770 train_time:35825ms step_avg:93.54ms
step:384/1770 train_time:35918ms step_avg:93.54ms
step:385/1770 train_time:36011ms step_avg:93.54ms
step:386/1770 train_time:36104ms step_avg:93.53ms
step:387/1770 train_time:36198ms step_avg:93.54ms
step:388/1770 train_time:36293ms step_avg:93.54ms
step:389/1770 train_time:36387ms step_avg:93.54ms
step:390/1770 train_time:36481ms step_avg:93.54ms
step:391/1770 train_time:36574ms step_avg:93.54ms
step:392/1770 train_time:36668ms step_avg:93.54ms
step:393/1770 train_time:36761ms step_avg:93.54ms
step:394/1770 train_time:36855ms step_avg:93.54ms
step:395/1770 train_time:36948ms step_avg:93.54ms
step:396/1770 train_time:37043ms step_avg:93.54ms
step:397/1770 train_time:37138ms step_avg:93.55ms
step:398/1770 train_time:37234ms step_avg:93.55ms
step:399/1770 train_time:37330ms step_avg:93.56ms
step:400/1770 train_time:37426ms step_avg:93.56ms
step:401/1770 train_time:37522ms step_avg:93.57ms
step:402/1770 train_time:37619ms step_avg:93.58ms
step:403/1770 train_time:37715ms step_avg:93.59ms
step:404/1770 train_time:37810ms step_avg:93.59ms
step:405/1770 train_time:37906ms step_avg:93.59ms
step:406/1770 train_time:38001ms step_avg:93.60ms
step:407/1770 train_time:38097ms step_avg:93.60ms
step:408/1770 train_time:38192ms step_avg:93.61ms
step:409/1770 train_time:38287ms step_avg:93.61ms
step:410/1770 train_time:38384ms step_avg:93.62ms
step:411/1770 train_time:38480ms step_avg:93.63ms
step:412/1770 train_time:38576ms step_avg:93.63ms
step:413/1770 train_time:38672ms step_avg:93.64ms
step:414/1770 train_time:38768ms step_avg:93.64ms
step:415/1770 train_time:38863ms step_avg:93.65ms
step:416/1770 train_time:38959ms step_avg:93.65ms
step:417/1770 train_time:39054ms step_avg:93.65ms
step:418/1770 train_time:39150ms step_avg:93.66ms
step:419/1770 train_time:39246ms step_avg:93.67ms
step:420/1770 train_time:39342ms step_avg:93.67ms
step:421/1770 train_time:39438ms step_avg:93.68ms
step:422/1770 train_time:39533ms step_avg:93.68ms
step:423/1770 train_time:39628ms step_avg:93.68ms
step:424/1770 train_time:39724ms step_avg:93.69ms
step:425/1770 train_time:39820ms step_avg:93.69ms
step:426/1770 train_time:39915ms step_avg:93.70ms
step:427/1770 train_time:40011ms step_avg:93.70ms
step:428/1770 train_time:40107ms step_avg:93.71ms
step:429/1770 train_time:40203ms step_avg:93.71ms
step:430/1770 train_time:40299ms step_avg:93.72ms
step:431/1770 train_time:40394ms step_avg:93.72ms
step:432/1770 train_time:40490ms step_avg:93.73ms
step:433/1770 train_time:40586ms step_avg:93.73ms
step:434/1770 train_time:40683ms step_avg:93.74ms
step:435/1770 train_time:40779ms step_avg:93.74ms
step:436/1770 train_time:40873ms step_avg:93.75ms
step:437/1770 train_time:40968ms step_avg:93.75ms
step:438/1770 train_time:41064ms step_avg:93.75ms
step:439/1770 train_time:41160ms step_avg:93.76ms
step:440/1770 train_time:41256ms step_avg:93.76ms
step:441/1770 train_time:41351ms step_avg:93.77ms
step:442/1770 train_time:41446ms step_avg:93.77ms
step:443/1770 train_time:41543ms step_avg:93.78ms
step:444/1770 train_time:41639ms step_avg:93.78ms
step:445/1770 train_time:41734ms step_avg:93.78ms
step:446/1770 train_time:41830ms step_avg:93.79ms
step:447/1770 train_time:41925ms step_avg:93.79ms
step:448/1770 train_time:42021ms step_avg:93.80ms
step:449/1770 train_time:42116ms step_avg:93.80ms
step:450/1770 train_time:42212ms step_avg:93.80ms
step:451/1770 train_time:42308ms step_avg:93.81ms
step:452/1770 train_time:42404ms step_avg:93.81ms
step:453/1770 train_time:42500ms step_avg:93.82ms
step:454/1770 train_time:42595ms step_avg:93.82ms
step:455/1770 train_time:42691ms step_avg:93.83ms
step:456/1770 train_time:42787ms step_avg:93.83ms
step:457/1770 train_time:42883ms step_avg:93.84ms
step:458/1770 train_time:42979ms step_avg:93.84ms
step:459/1770 train_time:43073ms step_avg:93.84ms
step:460/1770 train_time:43169ms step_avg:93.85ms
step:461/1770 train_time:43264ms step_avg:93.85ms
step:462/1770 train_time:43361ms step_avg:93.85ms
step:463/1770 train_time:43456ms step_avg:93.86ms
step:464/1770 train_time:43552ms step_avg:93.86ms
step:465/1770 train_time:43648ms step_avg:93.87ms
step:466/1770 train_time:43745ms step_avg:93.87ms
step:467/1770 train_time:43841ms step_avg:93.88ms
step:468/1770 train_time:43935ms step_avg:93.88ms
step:469/1770 train_time:44031ms step_avg:93.88ms
step:470/1770 train_time:44128ms step_avg:93.89ms
step:471/1770 train_time:44223ms step_avg:93.89ms
step:472/1770 train_time:44320ms step_avg:93.90ms
step:473/1770 train_time:44415ms step_avg:93.90ms
step:474/1770 train_time:44510ms step_avg:93.90ms
step:475/1770 train_time:44607ms step_avg:93.91ms
step:476/1770 train_time:44704ms step_avg:93.92ms
step:477/1770 train_time:44799ms step_avg:93.92ms
step:478/1770 train_time:44895ms step_avg:93.92ms
step:479/1770 train_time:44990ms step_avg:93.92ms
step:480/1770 train_time:45086ms step_avg:93.93ms
step:481/1770 train_time:45181ms step_avg:93.93ms
step:482/1770 train_time:45276ms step_avg:93.93ms
step:483/1770 train_time:45372ms step_avg:93.94ms
step:484/1770 train_time:45467ms step_avg:93.94ms
step:485/1770 train_time:45563ms step_avg:93.95ms
step:486/1770 train_time:45659ms step_avg:93.95ms
step:487/1770 train_time:45755ms step_avg:93.95ms
step:488/1770 train_time:45850ms step_avg:93.96ms
step:489/1770 train_time:45947ms step_avg:93.96ms
step:490/1770 train_time:46042ms step_avg:93.96ms
step:491/1770 train_time:46138ms step_avg:93.97ms
step:492/1770 train_time:46234ms step_avg:93.97ms
step:493/1770 train_time:46329ms step_avg:93.97ms
step:494/1770 train_time:46424ms step_avg:93.98ms
step:495/1770 train_time:46520ms step_avg:93.98ms
step:496/1770 train_time:46615ms step_avg:93.98ms
step:497/1770 train_time:46711ms step_avg:93.99ms
step:498/1770 train_time:46806ms step_avg:93.99ms
step:499/1770 train_time:46903ms step_avg:93.99ms
step:500/1770 train_time:46999ms step_avg:94.00ms
step:500/1770 val_loss:3.7469 train_time:47085ms step_avg:94.17ms
step:501/1770 train_time:47121ms step_avg:94.05ms
step:502/1770 train_time:47197ms step_avg:94.02ms
step:503/1770 train_time:47295ms step_avg:94.03ms
step:504/1770 train_time:47391ms step_avg:94.03ms
step:505/1770 train_time:47486ms step_avg:94.03ms
step:506/1770 train_time:47581ms step_avg:94.03ms
step:507/1770 train_time:47676ms step_avg:94.04ms
step:508/1770 train_time:47772ms step_avg:94.04ms
step:509/1770 train_time:47868ms step_avg:94.04ms
step:510/1770 train_time:47962ms step_avg:94.04ms
step:511/1770 train_time:48058ms step_avg:94.05ms
step:512/1770 train_time:48154ms step_avg:94.05ms
step:513/1770 train_time:48251ms step_avg:94.06ms
step:514/1770 train_time:48347ms step_avg:94.06ms
step:515/1770 train_time:48443ms step_avg:94.06ms
step:516/1770 train_time:48539ms step_avg:94.07ms
step:517/1770 train_time:48635ms step_avg:94.07ms
step:518/1770 train_time:48729ms step_avg:94.07ms
step:519/1770 train_time:48825ms step_avg:94.08ms
step:520/1770 train_time:48920ms step_avg:94.08ms
step:521/1770 train_time:49016ms step_avg:94.08ms
step:522/1770 train_time:49112ms step_avg:94.08ms
step:523/1770 train_time:49207ms step_avg:94.09ms
step:524/1770 train_time:49305ms step_avg:94.09ms
step:525/1770 train_time:49401ms step_avg:94.10ms
step:526/1770 train_time:49497ms step_avg:94.10ms
step:527/1770 train_time:49592ms step_avg:94.10ms
step:528/1770 train_time:49687ms step_avg:94.10ms
step:529/1770 train_time:49783ms step_avg:94.11ms
step:530/1770 train_time:49880ms step_avg:94.11ms
step:531/1770 train_time:49976ms step_avg:94.12ms
step:532/1770 train_time:50071ms step_avg:94.12ms
step:533/1770 train_time:50168ms step_avg:94.12ms
step:534/1770 train_time:50265ms step_avg:94.13ms
step:535/1770 train_time:50362ms step_avg:94.13ms
step:536/1770 train_time:50459ms step_avg:94.14ms
step:537/1770 train_time:50555ms step_avg:94.14ms
step:538/1770 train_time:50650ms step_avg:94.14ms
step:539/1770 train_time:50745ms step_avg:94.15ms
step:540/1770 train_time:50841ms step_avg:94.15ms
step:541/1770 train_time:50937ms step_avg:94.15ms
step:542/1770 train_time:51033ms step_avg:94.16ms
step:543/1770 train_time:51129ms step_avg:94.16ms
step:544/1770 train_time:51225ms step_avg:94.16ms
step:545/1770 train_time:51321ms step_avg:94.17ms
step:546/1770 train_time:51417ms step_avg:94.17ms
step:547/1770 train_time:51515ms step_avg:94.18ms
step:548/1770 train_time:51610ms step_avg:94.18ms
step:549/1770 train_time:51706ms step_avg:94.18ms
step:550/1770 train_time:51801ms step_avg:94.18ms
step:551/1770 train_time:51898ms step_avg:94.19ms
step:552/1770 train_time:51994ms step_avg:94.19ms
step:553/1770 train_time:52090ms step_avg:94.19ms
step:554/1770 train_time:52185ms step_avg:94.20ms
step:555/1770 train_time:52281ms step_avg:94.20ms
step:556/1770 train_time:52378ms step_avg:94.21ms
step:557/1770 train_time:52475ms step_avg:94.21ms
step:558/1770 train_time:52571ms step_avg:94.21ms
step:559/1770 train_time:52666ms step_avg:94.21ms
step:560/1770 train_time:52761ms step_avg:94.22ms
step:561/1770 train_time:52857ms step_avg:94.22ms
step:562/1770 train_time:52954ms step_avg:94.22ms
step:563/1770 train_time:53049ms step_avg:94.23ms
step:564/1770 train_time:53145ms step_avg:94.23ms
step:565/1770 train_time:53241ms step_avg:94.23ms
step:566/1770 train_time:53336ms step_avg:94.23ms
step:567/1770 train_time:53432ms step_avg:94.24ms
step:568/1770 train_time:53529ms step_avg:94.24ms
step:569/1770 train_time:53624ms step_avg:94.24ms
step:570/1770 train_time:53721ms step_avg:94.25ms
step:571/1770 train_time:53817ms step_avg:94.25ms
step:572/1770 train_time:53915ms step_avg:94.26ms
step:573/1770 train_time:54009ms step_avg:94.26ms
step:574/1770 train_time:54106ms step_avg:94.26ms
step:575/1770 train_time:54202ms step_avg:94.26ms
step:576/1770 train_time:54299ms step_avg:94.27ms
step:577/1770 train_time:54395ms step_avg:94.27ms
step:578/1770 train_time:54490ms step_avg:94.27ms
step:579/1770 train_time:54587ms step_avg:94.28ms
step:580/1770 train_time:54683ms step_avg:94.28ms
step:581/1770 train_time:54779ms step_avg:94.28ms
step:582/1770 train_time:54876ms step_avg:94.29ms
step:583/1770 train_time:54972ms step_avg:94.29ms
step:584/1770 train_time:55068ms step_avg:94.29ms
step:585/1770 train_time:55164ms step_avg:94.30ms
step:586/1770 train_time:55261ms step_avg:94.30ms
step:587/1770 train_time:55358ms step_avg:94.31ms
step:588/1770 train_time:55453ms step_avg:94.31ms
step:589/1770 train_time:55549ms step_avg:94.31ms
step:590/1770 train_time:55644ms step_avg:94.31ms
step:591/1770 train_time:55740ms step_avg:94.31ms
step:592/1770 train_time:55837ms step_avg:94.32ms
step:593/1770 train_time:55933ms step_avg:94.32ms
step:594/1770 train_time:56029ms step_avg:94.32ms
step:595/1770 train_time:56125ms step_avg:94.33ms
step:596/1770 train_time:56221ms step_avg:94.33ms
step:597/1770 train_time:56317ms step_avg:94.33ms
step:598/1770 train_time:56414ms step_avg:94.34ms
step:599/1770 train_time:56510ms step_avg:94.34ms
step:600/1770 train_time:56606ms step_avg:94.34ms
step:601/1770 train_time:56701ms step_avg:94.34ms
step:602/1770 train_time:56797ms step_avg:94.35ms
step:603/1770 train_time:56894ms step_avg:94.35ms
step:604/1770 train_time:56990ms step_avg:94.35ms
step:605/1770 train_time:57085ms step_avg:94.35ms
step:606/1770 train_time:57181ms step_avg:94.36ms
step:607/1770 train_time:57276ms step_avg:94.36ms
step:608/1770 train_time:57372ms step_avg:94.36ms
step:609/1770 train_time:57467ms step_avg:94.36ms
step:610/1770 train_time:57564ms step_avg:94.37ms
step:611/1770 train_time:57660ms step_avg:94.37ms
step:612/1770 train_time:57757ms step_avg:94.37ms
step:613/1770 train_time:57853ms step_avg:94.38ms
step:614/1770 train_time:57948ms step_avg:94.38ms
step:615/1770 train_time:58044ms step_avg:94.38ms
step:616/1770 train_time:58140ms step_avg:94.38ms
step:617/1770 train_time:58237ms step_avg:94.39ms
step:618/1770 train_time:58334ms step_avg:94.39ms
step:619/1770 train_time:58429ms step_avg:94.39ms
step:620/1770 train_time:58525ms step_avg:94.40ms
step:621/1770 train_time:58620ms step_avg:94.40ms
step:622/1770 train_time:58717ms step_avg:94.40ms
step:623/1770 train_time:58813ms step_avg:94.40ms
step:624/1770 train_time:58910ms step_avg:94.41ms
step:625/1770 train_time:59006ms step_avg:94.41ms
step:625/1770 val_loss:3.6617 train_time:59092ms step_avg:94.55ms
step:626/1770 train_time:59128ms step_avg:94.45ms
step:627/1770 train_time:59203ms step_avg:94.42ms
step:628/1770 train_time:59307ms step_avg:94.44ms
step:629/1770 train_time:59403ms step_avg:94.44ms
step:630/1770 train_time:59499ms step_avg:94.44ms
step:631/1770 train_time:59596ms step_avg:94.45ms
step:632/1770 train_time:59691ms step_avg:94.45ms
step:633/1770 train_time:59786ms step_avg:94.45ms
step:634/1770 train_time:59881ms step_avg:94.45ms
step:635/1770 train_time:59978ms step_avg:94.45ms
step:636/1770 train_time:60074ms step_avg:94.46ms
step:637/1770 train_time:60170ms step_avg:94.46ms
step:638/1770 train_time:60268ms step_avg:94.46ms
step:639/1770 train_time:60363ms step_avg:94.47ms
step:640/1770 train_time:60460ms step_avg:94.47ms
step:641/1770 train_time:60556ms step_avg:94.47ms
step:642/1770 train_time:60652ms step_avg:94.47ms
step:643/1770 train_time:60747ms step_avg:94.48ms
step:644/1770 train_time:60843ms step_avg:94.48ms
step:645/1770 train_time:60939ms step_avg:94.48ms
step:646/1770 train_time:61035ms step_avg:94.48ms
step:647/1770 train_time:61132ms step_avg:94.48ms
step:648/1770 train_time:61228ms step_avg:94.49ms
step:649/1770 train_time:61324ms step_avg:94.49ms
step:650/1770 train_time:61420ms step_avg:94.49ms
step:651/1770 train_time:61517ms step_avg:94.50ms
step:652/1770 train_time:61615ms step_avg:94.50ms
step:653/1770 train_time:61710ms step_avg:94.50ms
step:654/1770 train_time:61805ms step_avg:94.50ms
step:655/1770 train_time:61901ms step_avg:94.51ms
step:656/1770 train_time:61997ms step_avg:94.51ms
step:657/1770 train_time:62093ms step_avg:94.51ms
step:658/1770 train_time:62191ms step_avg:94.51ms
step:659/1770 train_time:62289ms step_avg:94.52ms
step:660/1770 train_time:62386ms step_avg:94.52ms
step:661/1770 train_time:62484ms step_avg:94.53ms
step:662/1770 train_time:62581ms step_avg:94.53ms
step:663/1770 train_time:62679ms step_avg:94.54ms
step:664/1770 train_time:62777ms step_avg:94.54ms
step:665/1770 train_time:62875ms step_avg:94.55ms
step:666/1770 train_time:62972ms step_avg:94.55ms
step:667/1770 train_time:63069ms step_avg:94.56ms
step:668/1770 train_time:63166ms step_avg:94.56ms
step:669/1770 train_time:63264ms step_avg:94.56ms
step:670/1770 train_time:63362ms step_avg:94.57ms
step:671/1770 train_time:63460ms step_avg:94.58ms
step:672/1770 train_time:63559ms step_avg:94.58ms
step:673/1770 train_time:63657ms step_avg:94.59ms
step:674/1770 train_time:63755ms step_avg:94.59ms
step:675/1770 train_time:63852ms step_avg:94.60ms
step:676/1770 train_time:63949ms step_avg:94.60ms
step:677/1770 train_time:64046ms step_avg:94.60ms
step:678/1770 train_time:64144ms step_avg:94.61ms
step:679/1770 train_time:64242ms step_avg:94.61ms
step:680/1770 train_time:64340ms step_avg:94.62ms
step:681/1770 train_time:64438ms step_avg:94.62ms
step:682/1770 train_time:64535ms step_avg:94.63ms
step:683/1770 train_time:64632ms step_avg:94.63ms
step:684/1770 train_time:64730ms step_avg:94.63ms
step:685/1770 train_time:64829ms step_avg:94.64ms
step:686/1770 train_time:64926ms step_avg:94.64ms
step:687/1770 train_time:65023ms step_avg:94.65ms
step:688/1770 train_time:65120ms step_avg:94.65ms
step:689/1770 train_time:65218ms step_avg:94.66ms
step:690/1770 train_time:65316ms step_avg:94.66ms
step:691/1770 train_time:65414ms step_avg:94.67ms
step:692/1770 train_time:65512ms step_avg:94.67ms
step:693/1770 train_time:65609ms step_avg:94.67ms
step:694/1770 train_time:65705ms step_avg:94.68ms
step:695/1770 train_time:65803ms step_avg:94.68ms
step:696/1770 train_time:65902ms step_avg:94.69ms
step:697/1770 train_time:66000ms step_avg:94.69ms
step:698/1770 train_time:66098ms step_avg:94.70ms
step:699/1770 train_time:66196ms step_avg:94.70ms
step:700/1770 train_time:66294ms step_avg:94.71ms
step:701/1770 train_time:66391ms step_avg:94.71ms
step:702/1770 train_time:66488ms step_avg:94.71ms
step:703/1770 train_time:66586ms step_avg:94.72ms
step:704/1770 train_time:66683ms step_avg:94.72ms
step:705/1770 train_time:66781ms step_avg:94.72ms
step:706/1770 train_time:66880ms step_avg:94.73ms
step:707/1770 train_time:66978ms step_avg:94.74ms
step:708/1770 train_time:67075ms step_avg:94.74ms
step:709/1770 train_time:67173ms step_avg:94.74ms
step:710/1770 train_time:67270ms step_avg:94.75ms
step:711/1770 train_time:67367ms step_avg:94.75ms
step:712/1770 train_time:67465ms step_avg:94.75ms
step:713/1770 train_time:67562ms step_avg:94.76ms
step:714/1770 train_time:67660ms step_avg:94.76ms
step:715/1770 train_time:67759ms step_avg:94.77ms
step:716/1770 train_time:67856ms step_avg:94.77ms
step:717/1770 train_time:67954ms step_avg:94.77ms
step:718/1770 train_time:68051ms step_avg:94.78ms
step:719/1770 train_time:68149ms step_avg:94.78ms
step:720/1770 train_time:68245ms step_avg:94.78ms
step:721/1770 train_time:68343ms step_avg:94.79ms
step:722/1770 train_time:68440ms step_avg:94.79ms
step:723/1770 train_time:68538ms step_avg:94.80ms
step:724/1770 train_time:68635ms step_avg:94.80ms
step:725/1770 train_time:68733ms step_avg:94.80ms
step:726/1770 train_time:68830ms step_avg:94.81ms
step:727/1770 train_time:68927ms step_avg:94.81ms
step:728/1770 train_time:69024ms step_avg:94.81ms
step:729/1770 train_time:69122ms step_avg:94.82ms
step:730/1770 train_time:69220ms step_avg:94.82ms
step:731/1770 train_time:69318ms step_avg:94.83ms
step:732/1770 train_time:69416ms step_avg:94.83ms
step:733/1770 train_time:69515ms step_avg:94.84ms
step:734/1770 train_time:69612ms step_avg:94.84ms
step:735/1770 train_time:69709ms step_avg:94.84ms
step:736/1770 train_time:69807ms step_avg:94.85ms
step:737/1770 train_time:69904ms step_avg:94.85ms
step:738/1770 train_time:70002ms step_avg:94.85ms
step:739/1770 train_time:70100ms step_avg:94.86ms
step:740/1770 train_time:70198ms step_avg:94.86ms
step:741/1770 train_time:70296ms step_avg:94.87ms
step:742/1770 train_time:70393ms step_avg:94.87ms
step:743/1770 train_time:70491ms step_avg:94.87ms
step:744/1770 train_time:70588ms step_avg:94.88ms
step:745/1770 train_time:70685ms step_avg:94.88ms
step:746/1770 train_time:70782ms step_avg:94.88ms
step:747/1770 train_time:70882ms step_avg:94.89ms
step:748/1770 train_time:70978ms step_avg:94.89ms
step:749/1770 train_time:71075ms step_avg:94.89ms
step:750/1770 train_time:71173ms step_avg:94.90ms
step:750/1770 val_loss:3.6002 train_time:71259ms step_avg:95.01ms
step:751/1770 train_time:71296ms step_avg:94.93ms
step:752/1770 train_time:71372ms step_avg:94.91ms
step:753/1770 train_time:71470ms step_avg:94.91ms
step:754/1770 train_time:71568ms step_avg:94.92ms
step:755/1770 train_time:71665ms step_avg:94.92ms
step:756/1770 train_time:71762ms step_avg:94.92ms
step:757/1770 train_time:71860ms step_avg:94.93ms
step:758/1770 train_time:71957ms step_avg:94.93ms
step:759/1770 train_time:72054ms step_avg:94.93ms
step:760/1770 train_time:72151ms step_avg:94.94ms
step:761/1770 train_time:72248ms step_avg:94.94ms
step:762/1770 train_time:72346ms step_avg:94.94ms
step:763/1770 train_time:72445ms step_avg:94.95ms
step:764/1770 train_time:72543ms step_avg:94.95ms
step:765/1770 train_time:72641ms step_avg:94.96ms
step:766/1770 train_time:72739ms step_avg:94.96ms
step:767/1770 train_time:72836ms step_avg:94.96ms
step:768/1770 train_time:72933ms step_avg:94.96ms
step:769/1770 train_time:73030ms step_avg:94.97ms
step:770/1770 train_time:73127ms step_avg:94.97ms
step:771/1770 train_time:73225ms step_avg:94.97ms
step:772/1770 train_time:73322ms step_avg:94.98ms
step:773/1770 train_time:73421ms step_avg:94.98ms
step:774/1770 train_time:73520ms step_avg:94.99ms
step:775/1770 train_time:73618ms step_avg:94.99ms
step:776/1770 train_time:73716ms step_avg:95.00ms
step:777/1770 train_time:73814ms step_avg:95.00ms
step:778/1770 train_time:73911ms step_avg:95.00ms
step:779/1770 train_time:74008ms step_avg:95.00ms
step:780/1770 train_time:74105ms step_avg:95.01ms
step:781/1770 train_time:74202ms step_avg:95.01ms
step:782/1770 train_time:74301ms step_avg:95.01ms
step:783/1770 train_time:74399ms step_avg:95.02ms
step:784/1770 train_time:74497ms step_avg:95.02ms
step:785/1770 train_time:74596ms step_avg:95.03ms
step:786/1770 train_time:74693ms step_avg:95.03ms
step:787/1770 train_time:74792ms step_avg:95.03ms
step:788/1770 train_time:74889ms step_avg:95.04ms
step:789/1770 train_time:74986ms step_avg:95.04ms
step:790/1770 train_time:75083ms step_avg:95.04ms
step:791/1770 train_time:75181ms step_avg:95.05ms
step:792/1770 train_time:75279ms step_avg:95.05ms
step:793/1770 train_time:75376ms step_avg:95.05ms
step:794/1770 train_time:75475ms step_avg:95.06ms
step:795/1770 train_time:75573ms step_avg:95.06ms
step:796/1770 train_time:75671ms step_avg:95.06ms
step:797/1770 train_time:75769ms step_avg:95.07ms
step:798/1770 train_time:75866ms step_avg:95.07ms
step:799/1770 train_time:75963ms step_avg:95.07ms
step:800/1770 train_time:76061ms step_avg:95.08ms
step:801/1770 train_time:76159ms step_avg:95.08ms
step:802/1770 train_time:76257ms step_avg:95.08ms
step:803/1770 train_time:76355ms step_avg:95.09ms
step:804/1770 train_time:76452ms step_avg:95.09ms
step:805/1770 train_time:76549ms step_avg:95.09ms
step:806/1770 train_time:76647ms step_avg:95.09ms
step:807/1770 train_time:76745ms step_avg:95.10ms
step:808/1770 train_time:76842ms step_avg:95.10ms
step:809/1770 train_time:76941ms step_avg:95.11ms
step:810/1770 train_time:77039ms step_avg:95.11ms
step:811/1770 train_time:77137ms step_avg:95.11ms
step:812/1770 train_time:77235ms step_avg:95.12ms
step:813/1770 train_time:77333ms step_avg:95.12ms
step:814/1770 train_time:77430ms step_avg:95.12ms
step:815/1770 train_time:77527ms step_avg:95.13ms
step:816/1770 train_time:77625ms step_avg:95.13ms
step:817/1770 train_time:77723ms step_avg:95.13ms
step:818/1770 train_time:77822ms step_avg:95.14ms
step:819/1770 train_time:77920ms step_avg:95.14ms
step:820/1770 train_time:78018ms step_avg:95.14ms
step:821/1770 train_time:78116ms step_avg:95.15ms
step:822/1770 train_time:78213ms step_avg:95.15ms
step:823/1770 train_time:78311ms step_avg:95.15ms
step:824/1770 train_time:78409ms step_avg:95.16ms
step:825/1770 train_time:78507ms step_avg:95.16ms
step:826/1770 train_time:78605ms step_avg:95.16ms
step:827/1770 train_time:78703ms step_avg:95.17ms
step:828/1770 train_time:78801ms step_avg:95.17ms
step:829/1770 train_time:78899ms step_avg:95.17ms
step:830/1770 train_time:78997ms step_avg:95.18ms
step:831/1770 train_time:79095ms step_avg:95.18ms
step:832/1770 train_time:79192ms step_avg:95.18ms
step:833/1770 train_time:79290ms step_avg:95.19ms
step:834/1770 train_time:79389ms step_avg:95.19ms
step:835/1770 train_time:79487ms step_avg:95.19ms
step:836/1770 train_time:79585ms step_avg:95.20ms
step:837/1770 train_time:79683ms step_avg:95.20ms
step:838/1770 train_time:79781ms step_avg:95.20ms
step:839/1770 train_time:79878ms step_avg:95.21ms
step:840/1770 train_time:79976ms step_avg:95.21ms
step:841/1770 train_time:80074ms step_avg:95.21ms
step:842/1770 train_time:80171ms step_avg:95.21ms
step:843/1770 train_time:80268ms step_avg:95.22ms
step:844/1770 train_time:80366ms step_avg:95.22ms
step:845/1770 train_time:80463ms step_avg:95.22ms
step:846/1770 train_time:80562ms step_avg:95.23ms
step:847/1770 train_time:80659ms step_avg:95.23ms
step:848/1770 train_time:80757ms step_avg:95.23ms
step:849/1770 train_time:80854ms step_avg:95.23ms
step:850/1770 train_time:80952ms step_avg:95.24ms
step:851/1770 train_time:81050ms step_avg:95.24ms
step:852/1770 train_time:81147ms step_avg:95.24ms
step:853/1770 train_time:81245ms step_avg:95.25ms
step:854/1770 train_time:81343ms step_avg:95.25ms
step:855/1770 train_time:81442ms step_avg:95.25ms
step:856/1770 train_time:81540ms step_avg:95.26ms
step:857/1770 train_time:81638ms step_avg:95.26ms
step:858/1770 train_time:81736ms step_avg:95.26ms
step:859/1770 train_time:81834ms step_avg:95.27ms
step:860/1770 train_time:81931ms step_avg:95.27ms
step:861/1770 train_time:82027ms step_avg:95.27ms
step:862/1770 train_time:82125ms step_avg:95.27ms
step:863/1770 train_time:82223ms step_avg:95.28ms
step:864/1770 train_time:82321ms step_avg:95.28ms
step:865/1770 train_time:82420ms step_avg:95.28ms
step:866/1770 train_time:82519ms step_avg:95.29ms
step:867/1770 train_time:82617ms step_avg:95.29ms
step:868/1770 train_time:82715ms step_avg:95.29ms
step:869/1770 train_time:82812ms step_avg:95.30ms
step:870/1770 train_time:82910ms step_avg:95.30ms
step:871/1770 train_time:83006ms step_avg:95.30ms
step:872/1770 train_time:83104ms step_avg:95.30ms
step:873/1770 train_time:83202ms step_avg:95.31ms
step:874/1770 train_time:83299ms step_avg:95.31ms
step:875/1770 train_time:83397ms step_avg:95.31ms
step:875/1770 val_loss:3.5514 train_time:83485ms step_avg:95.41ms
step:876/1770 train_time:83522ms step_avg:95.35ms
step:877/1770 train_time:83600ms step_avg:95.33ms
step:878/1770 train_time:83702ms step_avg:95.33ms
step:879/1770 train_time:83801ms step_avg:95.34ms
step:880/1770 train_time:83899ms step_avg:95.34ms
step:881/1770 train_time:83997ms step_avg:95.34ms
step:882/1770 train_time:84094ms step_avg:95.34ms
step:883/1770 train_time:84191ms step_avg:95.35ms
step:884/1770 train_time:84289ms step_avg:95.35ms
step:885/1770 train_time:84386ms step_avg:95.35ms
step:886/1770 train_time:84484ms step_avg:95.35ms
step:887/1770 train_time:84583ms step_avg:95.36ms
step:888/1770 train_time:84682ms step_avg:95.36ms
step:889/1770 train_time:84781ms step_avg:95.37ms
step:890/1770 train_time:84879ms step_avg:95.37ms
step:891/1770 train_time:84977ms step_avg:95.37ms
step:892/1770 train_time:85075ms step_avg:95.38ms
step:893/1770 train_time:85172ms step_avg:95.38ms
step:894/1770 train_time:85269ms step_avg:95.38ms
step:895/1770 train_time:85366ms step_avg:95.38ms
step:896/1770 train_time:85463ms step_avg:95.38ms
step:897/1770 train_time:85562ms step_avg:95.39ms
step:898/1770 train_time:85661ms step_avg:95.39ms
step:899/1770 train_time:85760ms step_avg:95.39ms
step:900/1770 train_time:85857ms step_avg:95.40ms
step:901/1770 train_time:85955ms step_avg:95.40ms
step:902/1770 train_time:86052ms step_avg:95.40ms
step:903/1770 train_time:86150ms step_avg:95.40ms
step:904/1770 train_time:86248ms step_avg:95.41ms
step:905/1770 train_time:86346ms step_avg:95.41ms
step:906/1770 train_time:86443ms step_avg:95.41ms
step:907/1770 train_time:86543ms step_avg:95.42ms
step:908/1770 train_time:86642ms step_avg:95.42ms
step:909/1770 train_time:86740ms step_avg:95.42ms
step:910/1770 train_time:86838ms step_avg:95.43ms
step:911/1770 train_time:86936ms step_avg:95.43ms
step:912/1770 train_time:87033ms step_avg:95.43ms
step:913/1770 train_time:87131ms step_avg:95.43ms
step:914/1770 train_time:87228ms step_avg:95.44ms
step:915/1770 train_time:87327ms step_avg:95.44ms
step:916/1770 train_time:87425ms step_avg:95.44ms
step:917/1770 train_time:87524ms step_avg:95.45ms
step:918/1770 train_time:87621ms step_avg:95.45ms
step:919/1770 train_time:87719ms step_avg:95.45ms
step:920/1770 train_time:87819ms step_avg:95.45ms
step:921/1770 train_time:87919ms step_avg:95.46ms
step:922/1770 train_time:88019ms step_avg:95.47ms
step:923/1770 train_time:88118ms step_avg:95.47ms
step:924/1770 train_time:88219ms step_avg:95.47ms
step:925/1770 train_time:88317ms step_avg:95.48ms
step:926/1770 train_time:88417ms step_avg:95.48ms
step:927/1770 train_time:88515ms step_avg:95.49ms
step:928/1770 train_time:88614ms step_avg:95.49ms
step:929/1770 train_time:88713ms step_avg:95.49ms
step:930/1770 train_time:88812ms step_avg:95.50ms
step:931/1770 train_time:88911ms step_avg:95.50ms
step:932/1770 train_time:89010ms step_avg:95.50ms
step:933/1770 train_time:89108ms step_avg:95.51ms
step:934/1770 train_time:89207ms step_avg:95.51ms
step:935/1770 train_time:89306ms step_avg:95.51ms
step:936/1770 train_time:89406ms step_avg:95.52ms
step:937/1770 train_time:89505ms step_avg:95.52ms
step:938/1770 train_time:89606ms step_avg:95.53ms
step:939/1770 train_time:89705ms step_avg:95.53ms
step:940/1770 train_time:89806ms step_avg:95.54ms
step:941/1770 train_time:89905ms step_avg:95.54ms
step:942/1770 train_time:90006ms step_avg:95.55ms
step:943/1770 train_time:90105ms step_avg:95.55ms
step:944/1770 train_time:90204ms step_avg:95.56ms
step:945/1770 train_time:90303ms step_avg:95.56ms
step:946/1770 train_time:90405ms step_avg:95.57ms
step:947/1770 train_time:90505ms step_avg:95.57ms
step:948/1770 train_time:90604ms step_avg:95.57ms
step:949/1770 train_time:90705ms step_avg:95.58ms
step:950/1770 train_time:90804ms step_avg:95.58ms
step:951/1770 train_time:90905ms step_avg:95.59ms
step:952/1770 train_time:91005ms step_avg:95.59ms
step:953/1770 train_time:91105ms step_avg:95.60ms
step:954/1770 train_time:91204ms step_avg:95.60ms
step:955/1770 train_time:91304ms step_avg:95.61ms
step:956/1770 train_time:91404ms step_avg:95.61ms
step:957/1770 train_time:91503ms step_avg:95.61ms
step:958/1770 train_time:91604ms step_avg:95.62ms
step:959/1770 train_time:91703ms step_avg:95.62ms
step:960/1770 train_time:91803ms step_avg:95.63ms
step:961/1770 train_time:91904ms step_avg:95.63ms
step:962/1770 train_time:92003ms step_avg:95.64ms
step:963/1770 train_time:92104ms step_avg:95.64ms
step:964/1770 train_time:92203ms step_avg:95.65ms
step:965/1770 train_time:92304ms step_avg:95.65ms
step:966/1770 train_time:92403ms step_avg:95.65ms
step:967/1770 train_time:92502ms step_avg:95.66ms
step:968/1770 train_time:92602ms step_avg:95.66ms
step:969/1770 train_time:92702ms step_avg:95.67ms
step:970/1770 train_time:92802ms step_avg:95.67ms
step:971/1770 train_time:92903ms step_avg:95.68ms
step:972/1770 train_time:93004ms step_avg:95.68ms
step:973/1770 train_time:93104ms step_avg:95.69ms
step:974/1770 train_time:93204ms step_avg:95.69ms
step:975/1770 train_time:93304ms step_avg:95.70ms
step:976/1770 train_time:93403ms step_avg:95.70ms
step:977/1770 train_time:93503ms step_avg:95.70ms
step:978/1770 train_time:93603ms step_avg:95.71ms
step:979/1770 train_time:93703ms step_avg:95.71ms
step:980/1770 train_time:93803ms step_avg:95.72ms
step:981/1770 train_time:93903ms step_avg:95.72ms
step:982/1770 train_time:94003ms step_avg:95.73ms
step:983/1770 train_time:94103ms step_avg:95.73ms
step:984/1770 train_time:94203ms step_avg:95.74ms
step:985/1770 train_time:94304ms step_avg:95.74ms
step:986/1770 train_time:94403ms step_avg:95.74ms
step:987/1770 train_time:94504ms step_avg:95.75ms
step:988/1770 train_time:94604ms step_avg:95.75ms
step:989/1770 train_time:94705ms step_avg:95.76ms
step:990/1770 train_time:94806ms step_avg:95.76ms
step:991/1770 train_time:94906ms step_avg:95.77ms
step:992/1770 train_time:95006ms step_avg:95.77ms
step:993/1770 train_time:95106ms step_avg:95.78ms
step:994/1770 train_time:95205ms step_avg:95.78ms
step:995/1770 train_time:95306ms step_avg:95.78ms
step:996/1770 train_time:95405ms step_avg:95.79ms
step:997/1770 train_time:95505ms step_avg:95.79ms
step:998/1770 train_time:95604ms step_avg:95.80ms
step:999/1770 train_time:95703ms step_avg:95.80ms
step:1000/1770 train_time:95804ms step_avg:95.80ms
step:1000/1770 val_loss:3.5119 train_time:95894ms step_avg:95.89ms
step:1001/1770 train_time:95931ms step_avg:95.84ms
step:1002/1770 train_time:96010ms step_avg:95.82ms
step:1003/1770 train_time:96114ms step_avg:95.83ms
step:1004/1770 train_time:96213ms step_avg:95.83ms
step:1005/1770 train_time:96312ms step_avg:95.83ms
step:1006/1770 train_time:96410ms step_avg:95.84ms
step:1007/1770 train_time:96508ms step_avg:95.84ms
step:1008/1770 train_time:96606ms step_avg:95.84ms
step:1009/1770 train_time:96704ms step_avg:95.84ms
step:1010/1770 train_time:96802ms step_avg:95.84ms
step:1011/1770 train_time:96902ms step_avg:95.85ms
step:1012/1770 train_time:97001ms step_avg:95.85ms
step:1013/1770 train_time:97101ms step_avg:95.86ms
step:1014/1770 train_time:97201ms step_avg:95.86ms
step:1015/1770 train_time:97300ms step_avg:95.86ms
step:1016/1770 train_time:97400ms step_avg:95.87ms
step:1017/1770 train_time:97501ms step_avg:95.87ms
step:1018/1770 train_time:97600ms step_avg:95.87ms
step:1019/1770 train_time:97699ms step_avg:95.88ms
step:1020/1770 train_time:97799ms step_avg:95.88ms
step:1021/1770 train_time:97899ms step_avg:95.88ms
step:1022/1770 train_time:97999ms step_avg:95.89ms
step:1023/1770 train_time:98099ms step_avg:95.89ms
step:1024/1770 train_time:98199ms step_avg:95.90ms
step:1025/1770 train_time:98299ms step_avg:95.90ms
step:1026/1770 train_time:98399ms step_avg:95.91ms
step:1027/1770 train_time:98499ms step_avg:95.91ms
step:1028/1770 train_time:98598ms step_avg:95.91ms
step:1029/1770 train_time:98697ms step_avg:95.92ms
step:1030/1770 train_time:98796ms step_avg:95.92ms
step:1031/1770 train_time:98895ms step_avg:95.92ms
step:1032/1770 train_time:98995ms step_avg:95.93ms
step:1033/1770 train_time:99095ms step_avg:95.93ms
step:1034/1770 train_time:99195ms step_avg:95.93ms
step:1035/1770 train_time:99294ms step_avg:95.94ms
step:1036/1770 train_time:99393ms step_avg:95.94ms
step:1037/1770 train_time:99493ms step_avg:95.94ms
step:1038/1770 train_time:99593ms step_avg:95.95ms
step:1039/1770 train_time:99693ms step_avg:95.95ms
step:1040/1770 train_time:99791ms step_avg:95.95ms
step:1041/1770 train_time:99890ms step_avg:95.96ms
step:1042/1770 train_time:99989ms step_avg:95.96ms
step:1043/1770 train_time:100089ms step_avg:95.96ms
step:1044/1770 train_time:100187ms step_avg:95.96ms
step:1045/1770 train_time:100285ms step_avg:95.97ms
step:1046/1770 train_time:100384ms step_avg:95.97ms
step:1047/1770 train_time:100482ms step_avg:95.97ms
step:1048/1770 train_time:100581ms step_avg:95.97ms
step:1049/1770 train_time:100680ms step_avg:95.98ms
step:1050/1770 train_time:100780ms step_avg:95.98ms
step:1051/1770 train_time:100880ms step_avg:95.98ms
step:1052/1770 train_time:100979ms step_avg:95.99ms
step:1053/1770 train_time:101079ms step_avg:95.99ms
step:1054/1770 train_time:101180ms step_avg:96.00ms
step:1055/1770 train_time:101279ms step_avg:96.00ms
step:1056/1770 train_time:101379ms step_avg:96.00ms
step:1057/1770 train_time:101478ms step_avg:96.01ms
step:1058/1770 train_time:101579ms step_avg:96.01ms
step:1059/1770 train_time:101679ms step_avg:96.01ms
step:1060/1770 train_time:101779ms step_avg:96.02ms
step:1061/1770 train_time:101879ms step_avg:96.02ms
step:1062/1770 train_time:101979ms step_avg:96.03ms
step:1063/1770 train_time:102082ms step_avg:96.03ms
step:1064/1770 train_time:102181ms step_avg:96.04ms
step:1065/1770 train_time:102281ms step_avg:96.04ms
step:1066/1770 train_time:102380ms step_avg:96.04ms
step:1067/1770 train_time:102481ms step_avg:96.05ms
step:1068/1770 train_time:102582ms step_avg:96.05ms
step:1069/1770 train_time:102681ms step_avg:96.05ms
step:1070/1770 train_time:102782ms step_avg:96.06ms
step:1071/1770 train_time:102881ms step_avg:96.06ms
step:1072/1770 train_time:102982ms step_avg:96.06ms
step:1073/1770 train_time:103079ms step_avg:96.07ms
step:1074/1770 train_time:103180ms step_avg:96.07ms
step:1075/1770 train_time:103280ms step_avg:96.07ms
step:1076/1770 train_time:103381ms step_avg:96.08ms
step:1077/1770 train_time:103480ms step_avg:96.08ms
step:1078/1770 train_time:103580ms step_avg:96.09ms
step:1079/1770 train_time:103679ms step_avg:96.09ms
step:1080/1770 train_time:103779ms step_avg:96.09ms
step:1081/1770 train_time:103878ms step_avg:96.09ms
step:1082/1770 train_time:103979ms step_avg:96.10ms
step:1083/1770 train_time:104079ms step_avg:96.10ms
step:1084/1770 train_time:104180ms step_avg:96.11ms
step:1085/1770 train_time:104280ms step_avg:96.11ms
step:1086/1770 train_time:104380ms step_avg:96.11ms
step:1087/1770 train_time:104478ms step_avg:96.12ms
step:1088/1770 train_time:104577ms step_avg:96.12ms
step:1089/1770 train_time:104677ms step_avg:96.12ms
step:1090/1770 train_time:104778ms step_avg:96.13ms
step:1091/1770 train_time:104878ms step_avg:96.13ms
step:1092/1770 train_time:104977ms step_avg:96.13ms
step:1093/1770 train_time:105078ms step_avg:96.14ms
step:1094/1770 train_time:105177ms step_avg:96.14ms
step:1095/1770 train_time:105278ms step_avg:96.14ms
step:1096/1770 train_time:105378ms step_avg:96.15ms
step:1097/1770 train_time:105478ms step_avg:96.15ms
step:1098/1770 train_time:105578ms step_avg:96.15ms
step:1099/1770 train_time:105677ms step_avg:96.16ms
step:1100/1770 train_time:105778ms step_avg:96.16ms
step:1101/1770 train_time:105878ms step_avg:96.16ms
step:1102/1770 train_time:105977ms step_avg:96.17ms
step:1103/1770 train_time:106078ms step_avg:96.17ms
step:1104/1770 train_time:106178ms step_avg:96.18ms
step:1105/1770 train_time:106277ms step_avg:96.18ms
step:1106/1770 train_time:106378ms step_avg:96.18ms
step:1107/1770 train_time:106477ms step_avg:96.19ms
step:1108/1770 train_time:106577ms step_avg:96.19ms
step:1109/1770 train_time:106678ms step_avg:96.19ms
step:1110/1770 train_time:106778ms step_avg:96.20ms
step:1111/1770 train_time:106877ms step_avg:96.20ms
step:1112/1770 train_time:106978ms step_avg:96.20ms
step:1113/1770 train_time:107078ms step_avg:96.21ms
step:1114/1770 train_time:107179ms step_avg:96.21ms
step:1115/1770 train_time:107280ms step_avg:96.21ms
step:1116/1770 train_time:107380ms step_avg:96.22ms
step:1117/1770 train_time:107479ms step_avg:96.22ms
step:1118/1770 train_time:107579ms step_avg:96.22ms
step:1119/1770 train_time:107678ms step_avg:96.23ms
step:1120/1770 train_time:107778ms step_avg:96.23ms
step:1121/1770 train_time:107877ms step_avg:96.23ms
step:1122/1770 train_time:107977ms step_avg:96.24ms
step:1123/1770 train_time:108078ms step_avg:96.24ms
step:1124/1770 train_time:108177ms step_avg:96.24ms
step:1125/1770 train_time:108279ms step_avg:96.25ms
step:1125/1770 val_loss:3.4715 train_time:108368ms step_avg:96.33ms
step:1126/1770 train_time:108405ms step_avg:96.27ms
step:1127/1770 train_time:108487ms step_avg:96.26ms
step:1128/1770 train_time:108587ms step_avg:96.27ms
step:1129/1770 train_time:108686ms step_avg:96.27ms
step:1130/1770 train_time:108785ms step_avg:96.27ms
step:1131/1770 train_time:108884ms step_avg:96.27ms
step:1132/1770 train_time:108983ms step_avg:96.27ms
step:1133/1770 train_time:109080ms step_avg:96.28ms
step:1134/1770 train_time:109180ms step_avg:96.28ms
step:1135/1770 train_time:109279ms step_avg:96.28ms
step:1136/1770 train_time:109378ms step_avg:96.28ms
step:1137/1770 train_time:109478ms step_avg:96.29ms
step:1138/1770 train_time:109577ms step_avg:96.29ms
step:1139/1770 train_time:109678ms step_avg:96.29ms
step:1140/1770 train_time:109777ms step_avg:96.30ms
step:1141/1770 train_time:109876ms step_avg:96.30ms
step:1142/1770 train_time:109976ms step_avg:96.30ms
step:1143/1770 train_time:110075ms step_avg:96.30ms
step:1144/1770 train_time:110174ms step_avg:96.31ms
step:1145/1770 train_time:110274ms step_avg:96.31ms
step:1146/1770 train_time:110374ms step_avg:96.31ms
step:1147/1770 train_time:110475ms step_avg:96.32ms
step:1148/1770 train_time:110575ms step_avg:96.32ms
step:1149/1770 train_time:110675ms step_avg:96.32ms
step:1150/1770 train_time:110775ms step_avg:96.33ms
step:1151/1770 train_time:110875ms step_avg:96.33ms
step:1152/1770 train_time:110975ms step_avg:96.33ms
step:1153/1770 train_time:111074ms step_avg:96.33ms
step:1154/1770 train_time:111174ms step_avg:96.34ms
step:1155/1770 train_time:111275ms step_avg:96.34ms
step:1156/1770 train_time:111374ms step_avg:96.34ms
step:1157/1770 train_time:111476ms step_avg:96.35ms
step:1158/1770 train_time:111575ms step_avg:96.35ms
step:1159/1770 train_time:111675ms step_avg:96.35ms
step:1160/1770 train_time:111774ms step_avg:96.36ms
step:1161/1770 train_time:111874ms step_avg:96.36ms
step:1162/1770 train_time:111973ms step_avg:96.36ms
step:1163/1770 train_time:112074ms step_avg:96.37ms
step:1164/1770 train_time:112174ms step_avg:96.37ms
step:1165/1770 train_time:112273ms step_avg:96.37ms
step:1166/1770 train_time:112374ms step_avg:96.38ms
step:1167/1770 train_time:112474ms step_avg:96.38ms
step:1168/1770 train_time:112573ms step_avg:96.38ms
step:1169/1770 train_time:112673ms step_avg:96.38ms
step:1170/1770 train_time:112773ms step_avg:96.39ms
step:1171/1770 train_time:112873ms step_avg:96.39ms
step:1172/1770 train_time:112973ms step_avg:96.39ms
step:1173/1770 train_time:113073ms step_avg:96.40ms
step:1174/1770 train_time:113174ms step_avg:96.40ms
step:1175/1770 train_time:113274ms step_avg:96.40ms
step:1176/1770 train_time:113374ms step_avg:96.41ms
step:1177/1770 train_time:113474ms step_avg:96.41ms
step:1178/1770 train_time:113574ms step_avg:96.41ms
step:1179/1770 train_time:113673ms step_avg:96.41ms
step:1180/1770 train_time:113774ms step_avg:96.42ms
step:1181/1770 train_time:113874ms step_avg:96.42ms
step:1182/1770 train_time:113975ms step_avg:96.43ms
step:1183/1770 train_time:114076ms step_avg:96.43ms
step:1184/1770 train_time:114177ms step_avg:96.43ms
step:1185/1770 train_time:114277ms step_avg:96.44ms
step:1186/1770 train_time:114379ms step_avg:96.44ms
step:1187/1770 train_time:114481ms step_avg:96.45ms
step:1188/1770 train_time:114580ms step_avg:96.45ms
step:1189/1770 train_time:114680ms step_avg:96.45ms
step:1190/1770 train_time:114779ms step_avg:96.45ms
step:1191/1770 train_time:114879ms step_avg:96.46ms
step:1192/1770 train_time:114979ms step_avg:96.46ms
step:1193/1770 train_time:115079ms step_avg:96.46ms
step:1194/1770 train_time:115179ms step_avg:96.46ms
step:1195/1770 train_time:115280ms step_avg:96.47ms
step:1196/1770 train_time:115381ms step_avg:96.47ms
step:1197/1770 train_time:115481ms step_avg:96.48ms
step:1198/1770 train_time:115581ms step_avg:96.48ms
step:1199/1770 train_time:115681ms step_avg:96.48ms
step:1200/1770 train_time:115782ms step_avg:96.48ms
step:1201/1770 train_time:115881ms step_avg:96.49ms
step:1202/1770 train_time:115982ms step_avg:96.49ms
step:1203/1770 train_time:116082ms step_avg:96.49ms
step:1204/1770 train_time:116182ms step_avg:96.50ms
step:1205/1770 train_time:116283ms step_avg:96.50ms
step:1206/1770 train_time:116385ms step_avg:96.50ms
step:1207/1770 train_time:116486ms step_avg:96.51ms
step:1208/1770 train_time:116586ms step_avg:96.51ms
step:1209/1770 train_time:116686ms step_avg:96.51ms
step:1210/1770 train_time:116787ms step_avg:96.52ms
step:1211/1770 train_time:116887ms step_avg:96.52ms
step:1212/1770 train_time:116990ms step_avg:96.53ms
step:1213/1770 train_time:117091ms step_avg:96.53ms
step:1214/1770 train_time:117192ms step_avg:96.53ms
step:1215/1770 train_time:117292ms step_avg:96.54ms
step:1216/1770 train_time:117396ms step_avg:96.54ms
step:1217/1770 train_time:117497ms step_avg:96.55ms
step:1218/1770 train_time:117597ms step_avg:96.55ms
step:1219/1770 train_time:117698ms step_avg:96.55ms
step:1220/1770 train_time:117798ms step_avg:96.56ms
step:1221/1770 train_time:117898ms step_avg:96.56ms
step:1222/1770 train_time:118000ms step_avg:96.56ms
step:1223/1770 train_time:118099ms step_avg:96.57ms
step:1224/1770 train_time:118200ms step_avg:96.57ms
step:1225/1770 train_time:118301ms step_avg:96.57ms
step:1226/1770 train_time:118402ms step_avg:96.58ms
step:1227/1770 train_time:118503ms step_avg:96.58ms
step:1228/1770 train_time:118605ms step_avg:96.58ms
step:1229/1770 train_time:118704ms step_avg:96.59ms
step:1230/1770 train_time:118805ms step_avg:96.59ms
step:1231/1770 train_time:118904ms step_avg:96.59ms
step:1232/1770 train_time:119005ms step_avg:96.59ms
step:1233/1770 train_time:119105ms step_avg:96.60ms
step:1234/1770 train_time:119206ms step_avg:96.60ms
step:1235/1770 train_time:119307ms step_avg:96.61ms
step:1236/1770 train_time:119408ms step_avg:96.61ms
step:1237/1770 train_time:119509ms step_avg:96.61ms
step:1238/1770 train_time:119611ms step_avg:96.62ms
step:1239/1770 train_time:119712ms step_avg:96.62ms
step:1240/1770 train_time:119813ms step_avg:96.62ms
step:1241/1770 train_time:119915ms step_avg:96.63ms
step:1242/1770 train_time:120016ms step_avg:96.63ms
step:1243/1770 train_time:120116ms step_avg:96.63ms
step:1244/1770 train_time:120217ms step_avg:96.64ms
step:1245/1770 train_time:120318ms step_avg:96.64ms
step:1246/1770 train_time:120420ms step_avg:96.65ms
step:1247/1770 train_time:120520ms step_avg:96.65ms
step:1248/1770 train_time:120619ms step_avg:96.65ms
step:1249/1770 train_time:120719ms step_avg:96.65ms
step:1250/1770 train_time:120819ms step_avg:96.66ms
step:1250/1770 val_loss:3.4241 train_time:120910ms step_avg:96.73ms
step:1251/1770 train_time:120947ms step_avg:96.68ms
step:1252/1770 train_time:121027ms step_avg:96.67ms
step:1253/1770 train_time:121129ms step_avg:96.67ms
step:1254/1770 train_time:121230ms step_avg:96.67ms
step:1255/1770 train_time:121333ms step_avg:96.68ms
step:1256/1770 train_time:121433ms step_avg:96.68ms
step:1257/1770 train_time:121533ms step_avg:96.69ms
step:1258/1770 train_time:121634ms step_avg:96.69ms
step:1259/1770 train_time:121734ms step_avg:96.69ms
step:1260/1770 train_time:121835ms step_avg:96.69ms
step:1261/1770 train_time:121937ms step_avg:96.70ms
step:1262/1770 train_time:122040ms step_avg:96.70ms
step:1263/1770 train_time:122140ms step_avg:96.71ms
step:1264/1770 train_time:122241ms step_avg:96.71ms
step:1265/1770 train_time:122341ms step_avg:96.71ms
step:1266/1770 train_time:122440ms step_avg:96.71ms
step:1267/1770 train_time:122541ms step_avg:96.72ms
step:1268/1770 train_time:122641ms step_avg:96.72ms
step:1269/1770 train_time:122740ms step_avg:96.72ms
step:1270/1770 train_time:122841ms step_avg:96.73ms
step:1271/1770 train_time:122942ms step_avg:96.73ms
step:1272/1770 train_time:123041ms step_avg:96.73ms
step:1273/1770 train_time:123142ms step_avg:96.73ms
step:1274/1770 train_time:123242ms step_avg:96.74ms
step:1275/1770 train_time:123342ms step_avg:96.74ms
step:1276/1770 train_time:123442ms step_avg:96.74ms
step:1277/1770 train_time:123541ms step_avg:96.74ms
step:1278/1770 train_time:123641ms step_avg:96.75ms
step:1279/1770 train_time:123741ms step_avg:96.75ms
step:1280/1770 train_time:123841ms step_avg:96.75ms
step:1281/1770 train_time:123941ms step_avg:96.75ms
step:1282/1770 train_time:124042ms step_avg:96.76ms
step:1283/1770 train_time:124143ms step_avg:96.76ms
step:1284/1770 train_time:124242ms step_avg:96.76ms
step:1285/1770 train_time:124343ms step_avg:96.76ms
step:1286/1770 train_time:124443ms step_avg:96.77ms
step:1287/1770 train_time:124545ms step_avg:96.77ms
step:1288/1770 train_time:124645ms step_avg:96.77ms
step:1289/1770 train_time:124745ms step_avg:96.78ms
step:1290/1770 train_time:124845ms step_avg:96.78ms
step:1291/1770 train_time:124945ms step_avg:96.78ms
step:1292/1770 train_time:125046ms step_avg:96.78ms
step:1293/1770 train_time:125147ms step_avg:96.79ms
step:1294/1770 train_time:125248ms step_avg:96.79ms
step:1295/1770 train_time:125348ms step_avg:96.79ms
step:1296/1770 train_time:125448ms step_avg:96.80ms
step:1297/1770 train_time:125549ms step_avg:96.80ms
step:1298/1770 train_time:125648ms step_avg:96.80ms
step:1299/1770 train_time:125749ms step_avg:96.80ms
step:1300/1770 train_time:125848ms step_avg:96.81ms
step:1301/1770 train_time:125950ms step_avg:96.81ms
step:1302/1770 train_time:126050ms step_avg:96.81ms
step:1303/1770 train_time:126153ms step_avg:96.82ms
step:1304/1770 train_time:126254ms step_avg:96.82ms
step:1305/1770 train_time:126355ms step_avg:96.82ms
step:1306/1770 train_time:126457ms step_avg:96.83ms
step:1307/1770 train_time:126557ms step_avg:96.83ms
step:1308/1770 train_time:126658ms step_avg:96.83ms
step:1309/1770 train_time:126758ms step_avg:96.84ms
step:1310/1770 train_time:126859ms step_avg:96.84ms
step:1311/1770 train_time:126959ms step_avg:96.84ms
step:1312/1770 train_time:127059ms step_avg:96.84ms
step:1313/1770 train_time:127159ms step_avg:96.85ms
step:1314/1770 train_time:127259ms step_avg:96.85ms
step:1315/1770 train_time:127359ms step_avg:96.85ms
step:1316/1770 train_time:127459ms step_avg:96.85ms
step:1317/1770 train_time:127560ms step_avg:96.86ms
step:1318/1770 train_time:127664ms step_avg:96.86ms
step:1319/1770 train_time:127763ms step_avg:96.86ms
step:1320/1770 train_time:127863ms step_avg:96.87ms
step:1321/1770 train_time:127964ms step_avg:96.87ms
step:1322/1770 train_time:128064ms step_avg:96.87ms
step:1323/1770 train_time:128164ms step_avg:96.87ms
step:1324/1770 train_time:128266ms step_avg:96.88ms
step:1325/1770 train_time:128366ms step_avg:96.88ms
step:1326/1770 train_time:128468ms step_avg:96.88ms
step:1327/1770 train_time:128571ms step_avg:96.89ms
step:1328/1770 train_time:128673ms step_avg:96.89ms
step:1329/1770 train_time:128775ms step_avg:96.90ms
step:1330/1770 train_time:128876ms step_avg:96.90ms
step:1331/1770 train_time:128977ms step_avg:96.90ms
step:1332/1770 train_time:129078ms step_avg:96.91ms
step:1333/1770 train_time:129178ms step_avg:96.91ms
step:1334/1770 train_time:129278ms step_avg:96.91ms
step:1335/1770 train_time:129378ms step_avg:96.91ms
step:1336/1770 train_time:129479ms step_avg:96.92ms
step:1337/1770 train_time:129579ms step_avg:96.92ms
step:1338/1770 train_time:129680ms step_avg:96.92ms
step:1339/1770 train_time:129781ms step_avg:96.92ms
step:1340/1770 train_time:129881ms step_avg:96.93ms
step:1341/1770 train_time:129981ms step_avg:96.93ms
step:1342/1770 train_time:130082ms step_avg:96.93ms
step:1343/1770 train_time:130182ms step_avg:96.93ms
step:1344/1770 train_time:130283ms step_avg:96.94ms
step:1345/1770 train_time:130383ms step_avg:96.94ms
step:1346/1770 train_time:130483ms step_avg:96.94ms
step:1347/1770 train_time:130583ms step_avg:96.94ms
step:1348/1770 train_time:130685ms step_avg:96.95ms
step:1349/1770 train_time:130786ms step_avg:96.95ms
step:1350/1770 train_time:130886ms step_avg:96.95ms
step:1351/1770 train_time:130987ms step_avg:96.96ms
step:1352/1770 train_time:131087ms step_avg:96.96ms
step:1353/1770 train_time:131187ms step_avg:96.96ms
step:1354/1770 train_time:131288ms step_avg:96.96ms
step:1355/1770 train_time:131389ms step_avg:96.97ms
step:1356/1770 train_time:131489ms step_avg:96.97ms
step:1357/1770 train_time:131590ms step_avg:96.97ms
step:1358/1770 train_time:131692ms step_avg:96.97ms
step:1359/1770 train_time:131793ms step_avg:96.98ms
step:1360/1770 train_time:131896ms step_avg:96.98ms
step:1361/1770 train_time:131997ms step_avg:96.99ms
step:1362/1770 train_time:132098ms step_avg:96.99ms
step:1363/1770 train_time:132199ms step_avg:96.99ms
step:1364/1770 train_time:132300ms step_avg:96.99ms
step:1365/1770 train_time:132400ms step_avg:97.00ms
step:1366/1770 train_time:132500ms step_avg:97.00ms
step:1367/1770 train_time:132600ms step_avg:97.00ms
step:1368/1770 train_time:132700ms step_avg:97.00ms
step:1369/1770 train_time:132801ms step_avg:97.01ms
step:1370/1770 train_time:132901ms step_avg:97.01ms
step:1371/1770 train_time:133001ms step_avg:97.01ms
step:1372/1770 train_time:133102ms step_avg:97.01ms
step:1373/1770 train_time:133202ms step_avg:97.02ms
step:1374/1770 train_time:133304ms step_avg:97.02ms
step:1375/1770 train_time:133405ms step_avg:97.02ms
step:1375/1770 val_loss:3.3812 train_time:133495ms step_avg:97.09ms
step:1376/1770 train_time:133532ms step_avg:97.04ms
step:1377/1770 train_time:133615ms step_avg:97.03ms
step:1378/1770 train_time:133717ms step_avg:97.04ms
step:1379/1770 train_time:133819ms step_avg:97.04ms
step:1380/1770 train_time:133920ms step_avg:97.04ms
step:1381/1770 train_time:134020ms step_avg:97.05ms
step:1382/1770 train_time:134120ms step_avg:97.05ms
step:1383/1770 train_time:134221ms step_avg:97.05ms
step:1384/1770 train_time:134321ms step_avg:97.05ms
step:1385/1770 train_time:134422ms step_avg:97.06ms
step:1386/1770 train_time:134525ms step_avg:97.06ms
step:1387/1770 train_time:134626ms step_avg:97.06ms
step:1388/1770 train_time:134726ms step_avg:97.07ms
step:1389/1770 train_time:134827ms step_avg:97.07ms
step:1390/1770 train_time:134927ms step_avg:97.07ms
step:1391/1770 train_time:135027ms step_avg:97.07ms
step:1392/1770 train_time:135127ms step_avg:97.07ms
step:1393/1770 train_time:135227ms step_avg:97.08ms
step:1394/1770 train_time:135328ms step_avg:97.08ms
step:1395/1770 train_time:135430ms step_avg:97.08ms
step:1396/1770 train_time:135530ms step_avg:97.08ms
step:1397/1770 train_time:135631ms step_avg:97.09ms
step:1398/1770 train_time:135732ms step_avg:97.09ms
step:1399/1770 train_time:135833ms step_avg:97.09ms
step:1400/1770 train_time:135935ms step_avg:97.10ms
step:1401/1770 train_time:136036ms step_avg:97.10ms
step:1402/1770 train_time:136138ms step_avg:97.10ms
step:1403/1770 train_time:136240ms step_avg:97.11ms
step:1404/1770 train_time:136341ms step_avg:97.11ms
step:1405/1770 train_time:136441ms step_avg:97.11ms
step:1406/1770 train_time:136542ms step_avg:97.11ms
step:1407/1770 train_time:136642ms step_avg:97.12ms
step:1408/1770 train_time:136743ms step_avg:97.12ms
step:1409/1770 train_time:136843ms step_avg:97.12ms
step:1410/1770 train_time:136944ms step_avg:97.12ms
step:1411/1770 train_time:137044ms step_avg:97.13ms
step:1412/1770 train_time:137144ms step_avg:97.13ms
step:1413/1770 train_time:137245ms step_avg:97.13ms
step:1414/1770 train_time:137346ms step_avg:97.13ms
step:1415/1770 train_time:137446ms step_avg:97.14ms
step:1416/1770 train_time:137547ms step_avg:97.14ms
step:1417/1770 train_time:137647ms step_avg:97.14ms
step:1418/1770 train_time:137747ms step_avg:97.14ms
step:1419/1770 train_time:137847ms step_avg:97.14ms
step:1420/1770 train_time:137948ms step_avg:97.15ms
step:1421/1770 train_time:138047ms step_avg:97.15ms
step:1422/1770 train_time:138148ms step_avg:97.15ms
step:1423/1770 train_time:138248ms step_avg:97.15ms
step:1424/1770 train_time:138349ms step_avg:97.16ms
step:1425/1770 train_time:138450ms step_avg:97.16ms
step:1426/1770 train_time:138552ms step_avg:97.16ms
step:1427/1770 train_time:138653ms step_avg:97.16ms
step:1428/1770 train_time:138754ms step_avg:97.17ms
step:1429/1770 train_time:138855ms step_avg:97.17ms
step:1430/1770 train_time:138955ms step_avg:97.17ms
step:1431/1770 train_time:139056ms step_avg:97.17ms
step:1432/1770 train_time:139157ms step_avg:97.18ms
step:1433/1770 train_time:139258ms step_avg:97.18ms
step:1434/1770 train_time:139358ms step_avg:97.18ms
step:1435/1770 train_time:139460ms step_avg:97.18ms
step:1436/1770 train_time:139562ms step_avg:97.19ms
step:1437/1770 train_time:139663ms step_avg:97.19ms
step:1438/1770 train_time:139763ms step_avg:97.19ms
step:1439/1770 train_time:139863ms step_avg:97.19ms
step:1440/1770 train_time:139963ms step_avg:97.20ms
step:1441/1770 train_time:140066ms step_avg:97.20ms
step:1442/1770 train_time:140165ms step_avg:97.20ms
step:1443/1770 train_time:140266ms step_avg:97.20ms
step:1444/1770 train_time:140368ms step_avg:97.21ms
step:1445/1770 train_time:140468ms step_avg:97.21ms
step:1446/1770 train_time:140569ms step_avg:97.21ms
step:1447/1770 train_time:140670ms step_avg:97.22ms
step:1448/1770 train_time:140771ms step_avg:97.22ms
step:1449/1770 train_time:140874ms step_avg:97.22ms
step:1450/1770 train_time:140975ms step_avg:97.22ms
step:1451/1770 train_time:141079ms step_avg:97.23ms
step:1452/1770 train_time:141182ms step_avg:97.23ms
step:1453/1770 train_time:141283ms step_avg:97.24ms
step:1454/1770 train_time:141386ms step_avg:97.24ms
step:1455/1770 train_time:141488ms step_avg:97.24ms
step:1456/1770 train_time:141589ms step_avg:97.25ms
step:1457/1770 train_time:141690ms step_avg:97.25ms
step:1458/1770 train_time:141792ms step_avg:97.25ms
step:1459/1770 train_time:141893ms step_avg:97.25ms
step:1460/1770 train_time:141993ms step_avg:97.26ms
step:1461/1770 train_time:142095ms step_avg:97.26ms
step:1462/1770 train_time:142197ms step_avg:97.26ms
step:1463/1770 train_time:142300ms step_avg:97.27ms
step:1464/1770 train_time:142403ms step_avg:97.27ms
step:1465/1770 train_time:142505ms step_avg:97.27ms
step:1466/1770 train_time:142607ms step_avg:97.28ms
step:1467/1770 train_time:142709ms step_avg:97.28ms
step:1468/1770 train_time:142810ms step_avg:97.28ms
step:1469/1770 train_time:142910ms step_avg:97.28ms
step:1470/1770 train_time:143011ms step_avg:97.29ms
step:1471/1770 train_time:143112ms step_avg:97.29ms
step:1472/1770 train_time:143213ms step_avg:97.29ms
step:1473/1770 train_time:143316ms step_avg:97.30ms
step:1474/1770 train_time:143421ms step_avg:97.30ms
step:1475/1770 train_time:143523ms step_avg:97.30ms
step:1476/1770 train_time:143625ms step_avg:97.31ms
step:1477/1770 train_time:143728ms step_avg:97.31ms
step:1478/1770 train_time:143830ms step_avg:97.31ms
step:1479/1770 train_time:143932ms step_avg:97.32ms
step:1480/1770 train_time:144033ms step_avg:97.32ms
step:1481/1770 train_time:144138ms step_avg:97.32ms
step:1482/1770 train_time:144239ms step_avg:97.33ms
step:1483/1770 train_time:144341ms step_avg:97.33ms
step:1484/1770 train_time:144443ms step_avg:97.33ms
step:1485/1770 train_time:144544ms step_avg:97.34ms
step:1486/1770 train_time:144646ms step_avg:97.34ms
step:1487/1770 train_time:144747ms step_avg:97.34ms
step:1488/1770 train_time:144849ms step_avg:97.34ms
step:1489/1770 train_time:144951ms step_avg:97.35ms
step:1490/1770 train_time:145052ms step_avg:97.35ms
step:1491/1770 train_time:145153ms step_avg:97.35ms
step:1492/1770 train_time:145255ms step_avg:97.36ms
step:1493/1770 train_time:145359ms step_avg:97.36ms
step:1494/1770 train_time:145463ms step_avg:97.36ms
step:1495/1770 train_time:145565ms step_avg:97.37ms
step:1496/1770 train_time:145666ms step_avg:97.37ms
step:1497/1770 train_time:145768ms step_avg:97.37ms
step:1498/1770 train_time:145868ms step_avg:97.38ms
step:1499/1770 train_time:145970ms step_avg:97.38ms
step:1500/1770 train_time:146070ms step_avg:97.38ms
step:1500/1770 val_loss:3.3434 train_time:146162ms step_avg:97.44ms
step:1501/1770 train_time:146199ms step_avg:97.40ms
step:1502/1770 train_time:146279ms step_avg:97.39ms
step:1503/1770 train_time:146380ms step_avg:97.39ms
step:1504/1770 train_time:146481ms step_avg:97.39ms
step:1505/1770 train_time:146585ms step_avg:97.40ms
step:1506/1770 train_time:146686ms step_avg:97.40ms
step:1507/1770 train_time:146788ms step_avg:97.40ms
step:1508/1770 train_time:146891ms step_avg:97.41ms
step:1509/1770 train_time:146993ms step_avg:97.41ms
step:1510/1770 train_time:147093ms step_avg:97.41ms
step:1511/1770 train_time:147198ms step_avg:97.42ms
step:1512/1770 train_time:147301ms step_avg:97.42ms
step:1513/1770 train_time:147404ms step_avg:97.42ms
step:1514/1770 train_time:147504ms step_avg:97.43ms
step:1515/1770 train_time:147606ms step_avg:97.43ms
step:1516/1770 train_time:147707ms step_avg:97.43ms
step:1517/1770 train_time:147808ms step_avg:97.43ms
step:1518/1770 train_time:147912ms step_avg:97.44ms
step:1519/1770 train_time:148013ms step_avg:97.44ms
step:1520/1770 train_time:148115ms step_avg:97.44ms
step:1521/1770 train_time:148217ms step_avg:97.45ms
step:1522/1770 train_time:148319ms step_avg:97.45ms
step:1523/1770 train_time:148421ms step_avg:97.45ms
step:1524/1770 train_time:148522ms step_avg:97.46ms
step:1525/1770 train_time:148623ms step_avg:97.46ms
step:1526/1770 train_time:148723ms step_avg:97.46ms
step:1527/1770 train_time:148824ms step_avg:97.46ms
step:1528/1770 train_time:148927ms step_avg:97.47ms
step:1529/1770 train_time:149029ms step_avg:97.47ms
step:1530/1770 train_time:149130ms step_avg:97.47ms
step:1531/1770 train_time:149233ms step_avg:97.47ms
step:1532/1770 train_time:149336ms step_avg:97.48ms
step:1533/1770 train_time:149437ms step_avg:97.48ms
step:1534/1770 train_time:149539ms step_avg:97.48ms
step:1535/1770 train_time:149640ms step_avg:97.49ms
step:1536/1770 train_time:149741ms step_avg:97.49ms
step:1537/1770 train_time:149843ms step_avg:97.49ms
step:1538/1770 train_time:149946ms step_avg:97.49ms
step:1539/1770 train_time:150048ms step_avg:97.50ms
step:1540/1770 train_time:150152ms step_avg:97.50ms
step:1541/1770 train_time:150255ms step_avg:97.50ms
step:1542/1770 train_time:150357ms step_avg:97.51ms
step:1543/1770 train_time:150458ms step_avg:97.51ms
step:1544/1770 train_time:150560ms step_avg:97.51ms
step:1545/1770 train_time:150662ms step_avg:97.52ms
step:1546/1770 train_time:150765ms step_avg:97.52ms
step:1547/1770 train_time:150866ms step_avg:97.52ms
step:1548/1770 train_time:150969ms step_avg:97.52ms
step:1549/1770 train_time:151070ms step_avg:97.53ms
step:1550/1770 train_time:151172ms step_avg:97.53ms
step:1551/1770 train_time:151273ms step_avg:97.53ms
step:1552/1770 train_time:151377ms step_avg:97.54ms
step:1553/1770 train_time:151479ms step_avg:97.54ms
step:1554/1770 train_time:151579ms step_avg:97.54ms
step:1555/1770 train_time:151681ms step_avg:97.54ms
step:1556/1770 train_time:151781ms step_avg:97.55ms
step:1557/1770 train_time:151883ms step_avg:97.55ms
step:1558/1770 train_time:151984ms step_avg:97.55ms
step:1559/1770 train_time:152085ms step_avg:97.55ms
step:1560/1770 train_time:152187ms step_avg:97.56ms
step:1561/1770 train_time:152291ms step_avg:97.56ms
step:1562/1770 train_time:152394ms step_avg:97.56ms
step:1563/1770 train_time:152496ms step_avg:97.57ms
step:1564/1770 train_time:152597ms step_avg:97.57ms
step:1565/1770 train_time:152698ms step_avg:97.57ms
step:1566/1770 train_time:152799ms step_avg:97.57ms
step:1567/1770 train_time:152900ms step_avg:97.57ms
step:1568/1770 train_time:153001ms step_avg:97.58ms
step:1569/1770 train_time:153106ms step_avg:97.58ms
step:1570/1770 train_time:153207ms step_avg:97.58ms
step:1571/1770 train_time:153309ms step_avg:97.59ms
step:1572/1770 train_time:153412ms step_avg:97.59ms
step:1573/1770 train_time:153516ms step_avg:97.59ms
step:1574/1770 train_time:153617ms step_avg:97.60ms
step:1575/1770 train_time:153717ms step_avg:97.60ms
step:1576/1770 train_time:153818ms step_avg:97.60ms
step:1577/1770 train_time:153920ms step_avg:97.60ms
step:1578/1770 train_time:154023ms step_avg:97.61ms
step:1579/1770 train_time:154124ms step_avg:97.61ms
step:1580/1770 train_time:154226ms step_avg:97.61ms
step:1581/1770 train_time:154329ms step_avg:97.62ms
step:1582/1770 train_time:154432ms step_avg:97.62ms
step:1583/1770 train_time:154535ms step_avg:97.62ms
step:1584/1770 train_time:154637ms step_avg:97.62ms
step:1585/1770 train_time:154738ms step_avg:97.63ms
step:1586/1770 train_time:154842ms step_avg:97.63ms
step:1587/1770 train_time:154945ms step_avg:97.63ms
step:1588/1770 train_time:155047ms step_avg:97.64ms
step:1589/1770 train_time:155151ms step_avg:97.64ms
step:1590/1770 train_time:155252ms step_avg:97.64ms
step:1591/1770 train_time:155353ms step_avg:97.64ms
step:1592/1770 train_time:155455ms step_avg:97.65ms
step:1593/1770 train_time:155557ms step_avg:97.65ms
step:1594/1770 train_time:155658ms step_avg:97.65ms
step:1595/1770 train_time:155760ms step_avg:97.65ms
step:1596/1770 train_time:155862ms step_avg:97.66ms
step:1597/1770 train_time:155962ms step_avg:97.66ms
step:1598/1770 train_time:156064ms step_avg:97.66ms
step:1599/1770 train_time:156165ms step_avg:97.66ms
step:1600/1770 train_time:156268ms step_avg:97.67ms
step:1601/1770 train_time:156370ms step_avg:97.67ms
step:1602/1770 train_time:156474ms step_avg:97.67ms
step:1603/1770 train_time:156576ms step_avg:97.68ms
step:1604/1770 train_time:156677ms step_avg:97.68ms
step:1605/1770 train_time:156778ms step_avg:97.68ms
step:1606/1770 train_time:156880ms step_avg:97.68ms
step:1607/1770 train_time:156985ms step_avg:97.69ms
step:1608/1770 train_time:157087ms step_avg:97.69ms
step:1609/1770 train_time:157188ms step_avg:97.69ms
step:1610/1770 train_time:157291ms step_avg:97.70ms
step:1611/1770 train_time:157394ms step_avg:97.70ms
step:1612/1770 train_time:157497ms step_avg:97.70ms
step:1613/1770 train_time:157598ms step_avg:97.70ms
step:1614/1770 train_time:157699ms step_avg:97.71ms
step:1615/1770 train_time:157802ms step_avg:97.71ms
step:1616/1770 train_time:157903ms step_avg:97.71ms
step:1617/1770 train_time:158008ms step_avg:97.72ms
step:1618/1770 train_time:158110ms step_avg:97.72ms
step:1619/1770 train_time:158213ms step_avg:97.72ms
step:1620/1770 train_time:158315ms step_avg:97.73ms
step:1621/1770 train_time:158416ms step_avg:97.73ms
step:1622/1770 train_time:158518ms step_avg:97.73ms
step:1623/1770 train_time:158622ms step_avg:97.73ms
step:1624/1770 train_time:158723ms step_avg:97.74ms
step:1625/1770 train_time:158824ms step_avg:97.74ms
step:1625/1770 val_loss:3.3089 train_time:158916ms step_avg:97.79ms
step:1626/1770 train_time:158953ms step_avg:97.76ms
step:1627/1770 train_time:159034ms step_avg:97.75ms
step:1628/1770 train_time:159136ms step_avg:97.75ms
step:1629/1770 train_time:159238ms step_avg:97.75ms
step:1630/1770 train_time:159340ms step_avg:97.75ms
step:1631/1770 train_time:159441ms step_avg:97.76ms
step:1632/1770 train_time:159541ms step_avg:97.76ms
step:1633/1770 train_time:159643ms step_avg:97.76ms
step:1634/1770 train_time:159744ms step_avg:97.76ms
step:1635/1770 train_time:159846ms step_avg:97.77ms
step:1636/1770 train_time:159948ms step_avg:97.77ms
step:1637/1770 train_time:160051ms step_avg:97.77ms
step:1638/1770 train_time:160152ms step_avg:97.77ms
step:1639/1770 train_time:160254ms step_avg:97.78ms
step:1640/1770 train_time:160356ms step_avg:97.78ms
step:1641/1770 train_time:160458ms step_avg:97.78ms
step:1642/1770 train_time:160560ms step_avg:97.78ms
step:1643/1770 train_time:160662ms step_avg:97.79ms
step:1644/1770 train_time:160766ms step_avg:97.79ms
step:1645/1770 train_time:160867ms step_avg:97.79ms
step:1646/1770 train_time:160970ms step_avg:97.79ms
step:1647/1770 train_time:161072ms step_avg:97.80ms
step:1648/1770 train_time:161173ms step_avg:97.80ms
step:1649/1770 train_time:161274ms step_avg:97.80ms
step:1650/1770 train_time:161376ms step_avg:97.80ms
step:1651/1770 train_time:161479ms step_avg:97.81ms
step:1652/1770 train_time:161581ms step_avg:97.81ms
step:1653/1770 train_time:161683ms step_avg:97.81ms
step:1654/1770 train_time:161787ms step_avg:97.82ms
step:1655/1770 train_time:161891ms step_avg:97.82ms
step:1656/1770 train_time:161993ms step_avg:97.82ms
step:1657/1770 train_time:162097ms step_avg:97.83ms
step:1658/1770 train_time:162199ms step_avg:97.83ms
step:1659/1770 train_time:162303ms step_avg:97.83ms
step:1660/1770 train_time:162404ms step_avg:97.83ms
step:1661/1770 train_time:162505ms step_avg:97.84ms
step:1662/1770 train_time:162606ms step_avg:97.84ms
step:1663/1770 train_time:162706ms step_avg:97.84ms
step:1664/1770 train_time:162808ms step_avg:97.84ms
step:1665/1770 train_time:162909ms step_avg:97.84ms
step:1666/1770 train_time:163010ms step_avg:97.85ms
step:1667/1770 train_time:163112ms step_avg:97.85ms
step:1668/1770 train_time:163214ms step_avg:97.85ms
step:1669/1770 train_time:163316ms step_avg:97.85ms
step:1670/1770 train_time:163417ms step_avg:97.85ms
step:1671/1770 train_time:163520ms step_avg:97.86ms
step:1672/1770 train_time:163622ms step_avg:97.86ms
step:1673/1770 train_time:163726ms step_avg:97.86ms
step:1674/1770 train_time:163827ms step_avg:97.87ms
step:1675/1770 train_time:163927ms step_avg:97.87ms
step:1676/1770 train_time:164029ms step_avg:97.87ms
step:1677/1770 train_time:164135ms step_avg:97.87ms
step:1678/1770 train_time:164235ms step_avg:97.88ms
step:1679/1770 train_time:164337ms step_avg:97.88ms
step:1680/1770 train_time:164438ms step_avg:97.88ms
step:1681/1770 train_time:164541ms step_avg:97.88ms
step:1682/1770 train_time:164646ms step_avg:97.89ms
step:1683/1770 train_time:164746ms step_avg:97.89ms
step:1684/1770 train_time:164848ms step_avg:97.89ms
step:1685/1770 train_time:164950ms step_avg:97.89ms
step:1686/1770 train_time:165052ms step_avg:97.90ms
step:1687/1770 train_time:165156ms step_avg:97.90ms
step:1688/1770 train_time:165257ms step_avg:97.90ms
step:1689/1770 train_time:165359ms step_avg:97.90ms
step:1690/1770 train_time:165461ms step_avg:97.91ms
step:1691/1770 train_time:165563ms step_avg:97.91ms
step:1692/1770 train_time:165665ms step_avg:97.91ms
step:1693/1770 train_time:165768ms step_avg:97.91ms
step:1694/1770 train_time:165870ms step_avg:97.92ms
step:1695/1770 train_time:165970ms step_avg:97.92ms
step:1696/1770 train_time:166073ms step_avg:97.92ms
step:1697/1770 train_time:166176ms step_avg:97.92ms
step:1698/1770 train_time:166277ms step_avg:97.93ms
step:1699/1770 train_time:166380ms step_avg:97.93ms
step:1700/1770 train_time:166482ms step_avg:97.93ms
step:1701/1770 train_time:166584ms step_avg:97.93ms
step:1702/1770 train_time:166686ms step_avg:97.94ms
step:1703/1770 train_time:166788ms step_avg:97.94ms
step:1704/1770 train_time:166890ms step_avg:97.94ms
step:1705/1770 train_time:166991ms step_avg:97.94ms
step:1706/1770 train_time:167092ms step_avg:97.94ms
step:1707/1770 train_time:167195ms step_avg:97.95ms
step:1708/1770 train_time:167298ms step_avg:97.95ms
step:1709/1770 train_time:167402ms step_avg:97.95ms
step:1710/1770 train_time:167508ms step_avg:97.96ms
step:1711/1770 train_time:167614ms step_avg:97.96ms
step:1712/1770 train_time:167715ms step_avg:97.96ms
step:1713/1770 train_time:167817ms step_avg:97.97ms
step:1714/1770 train_time:167921ms step_avg:97.97ms
step:1715/1770 train_time:168023ms step_avg:97.97ms
step:1716/1770 train_time:168126ms step_avg:97.98ms
step:1717/1770 train_time:168228ms step_avg:97.98ms
step:1718/1770 train_time:168330ms step_avg:97.98ms
step:1719/1770 train_time:168433ms step_avg:97.98ms
step:1720/1770 train_time:168537ms step_avg:97.99ms
step:1721/1770 train_time:168640ms step_avg:97.99ms
step:1722/1770 train_time:168745ms step_avg:97.99ms
step:1723/1770 train_time:168849ms step_avg:98.00ms
step:1724/1770 train_time:168953ms step_avg:98.00ms
step:1725/1770 train_time:169058ms step_avg:98.00ms
step:1726/1770 train_time:169164ms step_avg:98.01ms
step:1727/1770 train_time:169266ms step_avg:98.01ms
step:1728/1770 train_time:169369ms step_avg:98.01ms
step:1729/1770 train_time:169472ms step_avg:98.02ms
step:1730/1770 train_time:169575ms step_avg:98.02ms
step:1731/1770 train_time:169679ms step_avg:98.02ms
step:1732/1770 train_time:169781ms step_avg:98.03ms
step:1733/1770 train_time:169886ms step_avg:98.03ms
step:1734/1770 train_time:169987ms step_avg:98.03ms
step:1735/1770 train_time:170090ms step_avg:98.03ms
step:1736/1770 train_time:170192ms step_avg:98.04ms
step:1737/1770 train_time:170295ms step_avg:98.04ms
step:1738/1770 train_time:170397ms step_avg:98.04ms
step:1739/1770 train_time:170500ms step_avg:98.04ms
step:1740/1770 train_time:170603ms step_avg:98.05ms
step:1741/1770 train_time:170706ms step_avg:98.05ms
step:1742/1770 train_time:170811ms step_avg:98.05ms
step:1743/1770 train_time:170914ms step_avg:98.06ms
step:1744/1770 train_time:171017ms step_avg:98.06ms
step:1745/1770 train_time:171120ms step_avg:98.06ms
step:1746/1770 train_time:171226ms step_avg:98.07ms
step:1747/1770 train_time:171327ms step_avg:98.07ms
step:1748/1770 train_time:171430ms step_avg:98.07ms
step:1749/1770 train_time:171532ms step_avg:98.07ms
step:1750/1770 train_time:171634ms step_avg:98.08ms
step:1750/1770 val_loss:3.2824 train_time:171726ms step_avg:98.13ms
step:1751/1770 train_time:171763ms step_avg:98.09ms
step:1752/1770 train_time:171848ms step_avg:98.09ms
step:1753/1770 train_time:171950ms step_avg:98.09ms
step:1754/1770 train_time:172053ms step_avg:98.09ms
step:1755/1770 train_time:172155ms step_avg:98.09ms
step:1756/1770 train_time:172258ms step_avg:98.10ms
step:1757/1770 train_time:172360ms step_avg:98.10ms
step:1758/1770 train_time:172462ms step_avg:98.10ms
step:1759/1770 train_time:172564ms step_avg:98.10ms
step:1760/1770 train_time:172666ms step_avg:98.11ms
step:1761/1770 train_time:172770ms step_avg:98.11ms
step:1762/1770 train_time:172876ms step_avg:98.11ms
step:1763/1770 train_time:172977ms step_avg:98.12ms
step:1764/1770 train_time:173080ms step_avg:98.12ms
step:1765/1770 train_time:173182ms step_avg:98.12ms
step:1766/1770 train_time:173287ms step_avg:98.12ms
step:1767/1770 train_time:173389ms step_avg:98.13ms
step:1768/1770 train_time:173492ms step_avg:98.13ms
step:1769/1770 train_time:173594ms step_avg:98.13ms
step:1770/1770 train_time:173695ms step_avg:98.13ms
step:1770/1770 val_loss:3.2794 train_time:173789ms step_avg:98.19ms
peak memory allocated: 33278 MiB reserved: 48294 MiB

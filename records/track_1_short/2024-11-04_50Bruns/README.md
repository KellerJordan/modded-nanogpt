# 50B-token runs

This folder contains four runs generated by extending the 11/03/24 speedrun record to 50B FineWeb tokens.
The goal is to test how the speedrun generalizes to long durations, and especially how well Muon does.

We compare two things:
1. We compare Muon to Adam as the optimizer for the transformer body. (The head and embedding are always optimized by Adam.)
2. We compare training on 5 epochs of 10B tokens to training on 50B tokens. (Surprisingly this does about the same)

The four resulting runs are as follows:

* [Muon 50B tokens](./530f3ee1-8862-4d21-be2b-da10eb05e6a9.txt) (HellaSwag=35.82)
* [Adam 50B tokens](./69c33fc9-eabb-4a38-aa08-6922914eb405.txt) (HellaSwag=34.26)
* [Muon 5x10B tokens](./4fbe61ec-f79a-4c19-836d-46d599deecce.txt) (HellaSwag=36.17)
* [Adam 5x10B tokens](./3d715d41-453a-40d6-9506-421ba69766b2.txt) (HellaSwag=34.05)

To get a sense of what a good HellaSwag score would be for this scale of model, here are some baselines:
* Karpathy's baseline llm.c training (trained for 10B FineWeb tokens): 29.9
* OpenAI GPT-2 (124M): 29.4
* OpenAI GPT-3 (124M) (trained for 300B WebText tokens): 33.7
* Huggingface SmolLM2-135M (trained for 2T FineWeb/DCLM/etc tokens): 42.1

Note: I'm a little concerned that the learning rate schedule (WSD) and weight decay (zero), which are tuned for the speedrun duration,
might become undertuned/suboptimal for trainings of this duration.
It does look like the gap between Muon/Adam is too large to be closed by something like this, and the HellaSwag scores look quite reasonable, but you never know.


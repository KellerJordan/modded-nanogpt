import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from flash_attn_interface import flash_attn_varlen_func
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ns_line_1_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ns_line_1(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ns_line_1_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ns_line_2_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from ns_line_1_kernel, but also loads and adds a block of A
    # Performance is slightly slower than ns_line_1_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ns_line_2(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ns_line_2_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def newton_schulz_triton(G: torch.Tensor):
    a, b, c = (3.4445, -4.7750, 2.0315)
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) + 1e-7)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    ns_line_3 = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the NS iterations
    for _ in range(5):
        ns_line_1(X, out=A)  # A = X @ X.mT
        ns_line_2(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        ns_line_3(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    Though empirically small 1D params perform efficiently here:
        NS approximately performs a magnitude normalization of the grad
        This hyper-optimized class has faster execution time than the current impl of Adam for small params

    Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as 
    needed on the forward pass. This enables attn and mlp weights to be contained within the same 
    dist.reduce_scatter_tensor call(). The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on 1, then compute NS of 1 and schedule all gather
        6. wait on 2, then compute NS of 2 and schedule all gather
        7. wait on 3, then compute NS of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2ATTN, 2MLP, 2MLP, 2MLP]
            GPUs that receive params of type attn reshape before NS
        8. wait on 4, then compute NS of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params gives 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)
        if custom_sizing:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def generate_standard_param_groups(self, params):
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
    
    def generate_custom_param_groups(self, params):
        # implementation requires that a single GPU does not recieve both attn 
        # and mlp params when a param group is split across GPUs
        module_ranks = {
            'smear_gate': 1, # 1 param
            'attn_gate': 2, # 10 params
            'attn': 3, # 10 params
            'mlp': 4, # 22 params
        }
        params = list(params)
        params.sort(key=lambda x: module_ranks.get(x.module))
        idx = 0
        group_sizes = [1,10,16,16]
        assert len(params)==sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            group_params = params[idx:idx+size]
            param_groups.append(dict(params=group_params))
            idx += size
        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            num_params = len(params)
            padded_num_params = (
                (num_params + world_size - 1) // world_size * world_size
            )

            grads_to_stack = [p.grad for p in params]
            if padded_num_params > num_params:
                padding_grad = torch.zeros_like(params[0].grad)
                grads_to_stack.extend(
                    [padding_grad] * (padded_num_params - num_params)
                )

            stacked_grads = torch.stack(grads_to_stack)

            chunk_size = padded_num_params // world_size
            grad_chunk = torch.empty(
                (chunk_size, *params[0].grad.shape),
                dtype=stacked_grads.dtype,
                device=stacked_grads.device,
            )

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(
                {
                    "params": params,
                    "grad_chunk": grad_chunk,
                    "reduce_future": reduce_future,
                    "chunk_size": chunk_size,
                    "padded_num_params": padded_num_params,
                }
            )

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = info["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = info["chunk_size"]
            start_idx = rank * chunk_size

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            # This helps in vectorizing operations later.
            p_example = params[0]  # All params in a group have the same shape.
            eff_lr_val = (
                group["lr"]
                * max(1, p_example.size(-2) / p_example.size(-1)) ** 0.5
                * getattr(p_example, "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(p_example, "wd_mul", 1.0)
            )

            # Prepare a contiguous buffer for the updated parameters for this rank's chunk.
            # This buffer will serve as the input_tensor for dist.all_gather_into_tensor.
            updated_param_chunk = torch.empty(
                (chunk_size, *p_example.shape),
                dtype=p_example.dtype,
                device=p_example.device,
            )

            # List to collect update_grad tensors for batched zeropower computation.
            update_grads_for_zeropower = []

            # Process each parameter in this rank's chunk.
            for i in range(chunk_size):
                param_idx = start_idx + i

                if param_idx >= len(params):
                    # For padding: Fill the corresponding part of the updated_param_chunk with zeros.
                    # These padded entries will not be used by other ranks in the all_gather, but
                    # initializing them prevents uninitialized memory access issues.
                    updated_param_chunk[i].zero_()
                    # Also append a zero tensor for zeropower input if it must be padded.
                    update_grads_for_zeropower.append(
                        torch.zeros_like(p_example.grad)
                    )
                    continue
                p = params[param_idx]
                grad = grad_chunk[
                    i
                ]  # This gradient corresponds to the current parameter p.
                state = self.state[p]

                # Initialize momentum buffer if not present
                if not state:
                    state["momentum_buffer"] = torch.zeros_like(grad)

                momentum_buffer = state["momentum_buffer"]

                # Apply momentum update directly to the persistent momentum buffer in-place.
                momentum_buffer.lerp_(grad, 1 - group["momentum"])

                # Compute the actual `update_grad` for zeropower. This creates a new tensor.
                update_grad = grad.lerp(momentum_buffer, group["momentum"])
                update_grads_for_zeropower.append(update_grad)

                # Copy the current parameter value into the temporary buffer.
                updated_param_chunk[i].copy_(p)

                # Apply weight decay directly to the buffer.
                updated_param_chunk[i].mul_(1 - eff_weight_decay_val)

            # Stack the individual `update_grad` tensors for efficient batched zeropower computation.
            batched_update_grads = torch.stack(update_grads_for_zeropower)

            # Compute zeropower for the entire chunk in a single, batched call.
            original_shape = batched_update_grads.shape
            # Reshape attn params from [hdim, dim*4] to [4,hdim,dim] to apply NS indepedently to Q,K,V,O
            module_idx = start_idx if start_idx<len(params) else 0
            if getattr(params[module_idx],'module','none')=='attn':
                batch = 4 * original_shape[0]
                d1 = original_shape[1] 
                d2 = original_shape[2] // 4
                batched = batched_update_grads.view(batch, d1, d2)
                v_chunk = newton_schulz_triton(batched)
                v_chunk = v_chunk.view(original_shape)
            else:
                v_chunk = newton_schulz_triton(batched_update_grads)

            # Add the computed zeropower update to the parameters in the buffer.
            # This loop applies the zeropower output (v_chunk) to the `updated_param_chunk` buffer.
            for i in range(chunk_size):
                param_idx = start_idx + i
                if param_idx >= len(params):  # Skip padded entries again.
                    continue

                # Add the computed zeropower update to the parameter in the buffer.
                updated_param_chunk[i].add_(v_chunk[i], alpha=-eff_lr_val)

            stacked_params = torch.empty(
                (info["padded_num_params"], *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )
            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_param_chunk, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_gather_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for base_i in range(len(params)):
                grad = params[base_i].grad
                rank_size = grad.shape[0] // world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for base in range(len(params)):
                reduce_scatter_futures[idx].wait()
                p = params[base]
                rank_size = p.shape[0] // world_size
                p_slice = p[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(p, "lr_mul", 1.0)
                state = self.state[p]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state["step"] = torch.tensor(
                        0, dtype=torch.int64, device=p.device
                    )
                    state["exp_avg"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                    state["exp_avg_sq"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(p, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_gather_futures.append(dist.all_gather_into_tensor(p, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()
        
    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # shape to enable MegaBatchMuon
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        self.qkvo_w.module='attn'
        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        self.attn_gate.weight.module = 'attn_gate'
        self.attn_gate.weight.detach().zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4,self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens, max_seqlen_q=max_len, max_seqlen_k=max_len,
                                   causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4,self.hdim, self.dim)[3].type_as(y))
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make both matrices have the same shape because optimizer sorts params by shape
        # 2 matrices x 12 layers = 24 total, which is divisible by 8 GPU world size
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        self.c_fc.module='mlp'
        self.c_proj.module='mlp'
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.detach().zero_()
        self.smear_gate.weight.module = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 6) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(num_layers), #extra zeros params for smear_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        # smear token embed forward 1 position @classiclarryd
        smear_lambda = self.scalars[5 * len(self.blocks)]
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)

        n = len(self.blocks) // 2

        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            if i >= n and i<11:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)

        x = norm(x)
        logits = self.lm_head(x).float()
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), target_seq, reduction="sum" if self.training else "mean")
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()
    
    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 24 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_iterations: int = 1640 # number of iterations to run
    iteration_extension = 40 # number of iterations to continue training at final cooldown and window size
    cooldown_frac: int = 0.5 # fraction of training spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 125 # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_validate: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_long_validate: int = 20 # extend long windows out even further

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.8, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizer2 = Muon(hidden_matrix_params+gate_params, lr=0.05, momentum=0.95, weight_decay=0.0)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: stable then decay
def get_lr(step: int):
    x = min(0.9999,step / args.num_iterations)
    assert 0 <= x < 1
    lr = 1.0
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = w * 1.0 + (1 - w) * 0.1
    return lr

def get_ws(step: int):
    if step == args.num_iterations+args.iteration_extension:
        return args.ws_validate//2, args.ws_validate
    x = min(step / (1 + args.num_iterations),0.9999)
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx]//2, args.ws_schedule[ws_idx]

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
ws_long = args.ws_schedule[0]
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    new_ws_long = args.ws_schedule[step % len(args.ws_schedule)]  # each window size is a new graph, need to warm up each with YaRN params
    if new_ws_long > ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long = new_ws_long
    elif new_ws_long<ws_long:
        model.yarn.reset()
        ws_long = new_ws_long
    model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset()
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations + args.iteration_extension
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_long_validate
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for _ in range(grad_accum_steps):
        inputs, targets, cum_seqlens = next(train_loader)
        model(inputs, targets, cum_seqlens, ws_short, ws_long).backward()
    # set optimization hyperparameters
    for opt in optimizers:
        for group in opt.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)
    for group in optimizer2.param_groups:
        frac = min(step / 300, 1) # momentum warmup for muon
        group["momentum"] = (1 - frac) * 0.85 + frac * 0.95
    # step the optimizers
    for opt in optimizers:
        opt.step()
    # null the gradients
    model.zero_grad(set_to_none=True)
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.10.12 (main, May 27 2025, 17:12:29) [GCC 11.4.0]
Running PyTorch 2.9.0.dev20250726+cu126 compiled for CUDA 12.6
Running Triton version 3.4.0
Wed Sep 24 06:01:00 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.148.08             Driver Version: 570.148.08     CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:61:00.0 Off |                    0 |
| N/A   31C    P0            120W /  700W |    5858MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:62:00.0 Off |                    0 |
| N/A   33C    P0            117W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:63:00.0 Off |                    0 |
| N/A   36C    P0            118W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:64:00.0 Off |                    0 |
| N/A   30C    P0            119W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:6A:00.0 Off |                    0 |
| N/A   31C    P0            120W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:6B:00.0 Off |                    0 |
| N/A   37C    P0            121W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:6C:00.0 Off |                    0 |
| N/A   35C    P0            121W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:6D:00.0 Off |                    0 |
| N/A   30C    P0            120W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A          111355      C   /usr/bin/python3                       1510MiB |
|    0   N/A  N/A          111356      C   /usr/bin/python3                        614MiB |
|    0   N/A  N/A          111357      C   /usr/bin/python3                        614MiB |
|    0   N/A  N/A          111358      C   /usr/bin/python3                        614MiB |
|    0   N/A  N/A          111359      C   /usr/bin/python3                        614MiB |
|    0   N/A  N/A          111360      C   /usr/bin/python3                        614MiB |
|    0   N/A  N/A          111361      C   /usr/bin/python3                        614MiB |
|    0   N/A  N/A          111362      C   /usr/bin/python3                        614MiB |
|    1   N/A  N/A          111356      C   /usr/bin/python3                       1510MiB |
|    2   N/A  N/A          111357      C   /usr/bin/python3                       1510MiB |
|    3   N/A  N/A          111358      C   /usr/bin/python3                       1510MiB |
|    4   N/A  N/A          111359      C   /usr/bin/python3                       1510MiB |
|    5   N/A  N/A          111360      C   /usr/bin/python3                       1510MiB |
|    6   N/A  N/A          111361      C   /usr/bin/python3                       1510MiB |
|    7   N/A  N/A          111362      C   /usr/bin/python3                       1510MiB |
+-----------------------------------------------------------------------------------------+

====================================================================================================
step:0/1680 val_loss:10.8258 train_time:0ms step_avg:0.03ms
step:1/1680 train_time:150ms step_avg:149.59ms
step:2/1680 train_time:173ms step_avg:86.65ms
step:3/1680 train_time:236ms step_avg:78.63ms
step:4/1680 train_time:322ms step_avg:80.53ms
step:5/1680 train_time:409ms step_avg:81.88ms
step:6/1680 train_time:497ms step_avg:82.83ms
step:7/1680 train_time:584ms step_avg:83.47ms
step:8/1680 train_time:672ms step_avg:84.00ms
step:9/1680 train_time:759ms step_avg:84.36ms
step:10/1680 train_time:846ms step_avg:84.62ms
step:11/1680 train_time:934ms step_avg:84.88ms
step:12/1680 train_time:1021ms step_avg:85.12ms
step:13/1680 train_time:1113ms step_avg:85.62ms
step:14/1680 train_time:1204ms step_avg:85.99ms
step:15/1680 train_time:1293ms step_avg:86.21ms
step:16/1680 train_time:1381ms step_avg:86.32ms
step:17/1680 train_time:1469ms step_avg:86.43ms
step:18/1680 train_time:1557ms step_avg:86.52ms
step:19/1680 train_time:1645ms step_avg:86.59ms
step:20/1680 train_time:1733ms step_avg:86.67ms
step:21/1680 train_time:1821ms step_avg:86.72ms
step:22/1680 train_time:1909ms step_avg:86.79ms
step:23/1680 train_time:1998ms step_avg:86.88ms
step:24/1680 train_time:2087ms step_avg:86.97ms
step:25/1680 train_time:2177ms step_avg:87.06ms
step:26/1680 train_time:2267ms step_avg:87.18ms
step:27/1680 train_time:2355ms step_avg:87.24ms
step:28/1680 train_time:2444ms step_avg:87.29ms
step:29/1680 train_time:2532ms step_avg:87.31ms
step:30/1680 train_time:2620ms step_avg:87.33ms
step:31/1680 train_time:2708ms step_avg:87.36ms
step:32/1680 train_time:2796ms step_avg:87.37ms
step:33/1680 train_time:2883ms step_avg:87.37ms
step:34/1680 train_time:2971ms step_avg:87.40ms
step:35/1680 train_time:3060ms step_avg:87.43ms
step:36/1680 train_time:3149ms step_avg:87.46ms
step:37/1680 train_time:3237ms step_avg:87.50ms
step:38/1680 train_time:3326ms step_avg:87.53ms
step:39/1680 train_time:3414ms step_avg:87.55ms
step:40/1680 train_time:3502ms step_avg:87.55ms
step:41/1680 train_time:3590ms step_avg:87.57ms
step:42/1680 train_time:3678ms step_avg:87.58ms
step:43/1680 train_time:3766ms step_avg:87.59ms
step:44/1680 train_time:3854ms step_avg:87.58ms
step:45/1680 train_time:3942ms step_avg:87.59ms
step:46/1680 train_time:4029ms step_avg:87.59ms
step:47/1680 train_time:4117ms step_avg:87.60ms
step:48/1680 train_time:4205ms step_avg:87.61ms
step:49/1680 train_time:4293ms step_avg:87.62ms
step:50/1680 train_time:4381ms step_avg:87.63ms
step:51/1680 train_time:4470ms step_avg:87.65ms
step:52/1680 train_time:4559ms step_avg:87.66ms
step:53/1680 train_time:4646ms step_avg:87.67ms
step:54/1680 train_time:4735ms step_avg:87.69ms
step:55/1680 train_time:4823ms step_avg:87.69ms
step:56/1680 train_time:4911ms step_avg:87.70ms
step:57/1680 train_time:5000ms step_avg:87.71ms
step:58/1680 train_time:5088ms step_avg:87.72ms
step:59/1680 train_time:5176ms step_avg:87.73ms
step:60/1680 train_time:5263ms step_avg:87.72ms
step:61/1680 train_time:5352ms step_avg:87.73ms
step:62/1680 train_time:5440ms step_avg:87.74ms
step:63/1680 train_time:5528ms step_avg:87.74ms
step:64/1680 train_time:5616ms step_avg:87.75ms
step:65/1680 train_time:5704ms step_avg:87.75ms
step:66/1680 train_time:5792ms step_avg:87.76ms
step:67/1680 train_time:5880ms step_avg:87.76ms
step:68/1680 train_time:5968ms step_avg:87.77ms
step:69/1680 train_time:6056ms step_avg:87.77ms
step:70/1680 train_time:6145ms step_avg:87.79ms
step:71/1680 train_time:6233ms step_avg:87.79ms
step:72/1680 train_time:6321ms step_avg:87.79ms
step:73/1680 train_time:6409ms step_avg:87.79ms
step:74/1680 train_time:6497ms step_avg:87.80ms
step:75/1680 train_time:6585ms step_avg:87.80ms
step:76/1680 train_time:6673ms step_avg:87.80ms
step:77/1680 train_time:6761ms step_avg:87.80ms
step:78/1680 train_time:6849ms step_avg:87.81ms
step:79/1680 train_time:6938ms step_avg:87.82ms
step:80/1680 train_time:7026ms step_avg:87.83ms
step:81/1680 train_time:7115ms step_avg:87.84ms
step:82/1680 train_time:7203ms step_avg:87.84ms
step:83/1680 train_time:7290ms step_avg:87.83ms
step:84/1680 train_time:7378ms step_avg:87.83ms
step:85/1680 train_time:7465ms step_avg:87.83ms
step:86/1680 train_time:7553ms step_avg:87.82ms
step:87/1680 train_time:7641ms step_avg:87.83ms
step:88/1680 train_time:7729ms step_avg:87.83ms
step:89/1680 train_time:7817ms step_avg:87.83ms
step:90/1680 train_time:7905ms step_avg:87.84ms
step:91/1680 train_time:7994ms step_avg:87.85ms
step:92/1680 train_time:8082ms step_avg:87.85ms
step:93/1680 train_time:8171ms step_avg:87.86ms
step:94/1680 train_time:8258ms step_avg:87.85ms
step:95/1680 train_time:8346ms step_avg:87.85ms
step:96/1680 train_time:8434ms step_avg:87.86ms
step:97/1680 train_time:8522ms step_avg:87.86ms
step:98/1680 train_time:8610ms step_avg:87.85ms
step:99/1680 train_time:8698ms step_avg:87.86ms
step:100/1680 train_time:8786ms step_avg:87.86ms
step:101/1680 train_time:8876ms step_avg:87.88ms
step:102/1680 train_time:8964ms step_avg:87.89ms
step:103/1680 train_time:9054ms step_avg:87.90ms
step:104/1680 train_time:9142ms step_avg:87.90ms
step:105/1680 train_time:9229ms step_avg:87.90ms
step:106/1680 train_time:9317ms step_avg:87.90ms
step:107/1680 train_time:9406ms step_avg:87.90ms
step:108/1680 train_time:9495ms step_avg:87.92ms
step:109/1680 train_time:9582ms step_avg:87.91ms
step:110/1680 train_time:9670ms step_avg:87.90ms
step:111/1680 train_time:9757ms step_avg:87.90ms
step:112/1680 train_time:9845ms step_avg:87.90ms
step:113/1680 train_time:9932ms step_avg:87.90ms
step:114/1680 train_time:10020ms step_avg:87.90ms
step:115/1680 train_time:10109ms step_avg:87.91ms
step:116/1680 train_time:10197ms step_avg:87.91ms
step:117/1680 train_time:10285ms step_avg:87.91ms
step:118/1680 train_time:10373ms step_avg:87.91ms
step:119/1680 train_time:10461ms step_avg:87.91ms
step:120/1680 train_time:10550ms step_avg:87.92ms
step:121/1680 train_time:10638ms step_avg:87.92ms
step:122/1680 train_time:10726ms step_avg:87.92ms
step:123/1680 train_time:10814ms step_avg:87.92ms
step:124/1680 train_time:10902ms step_avg:87.92ms
step:125/1680 train_time:10989ms step_avg:87.91ms
step:125/1680 val_loss:4.3014 train_time:11078ms step_avg:88.62ms
step:126/1680 train_time:11101ms step_avg:88.11ms
step:127/1680 train_time:11167ms step_avg:87.93ms
step:128/1680 train_time:11266ms step_avg:88.02ms
step:129/1680 train_time:11360ms step_avg:88.06ms
step:130/1680 train_time:11448ms step_avg:88.06ms
step:131/1680 train_time:11535ms step_avg:88.05ms
step:132/1680 train_time:11622ms step_avg:88.05ms
step:133/1680 train_time:11709ms step_avg:88.04ms
step:134/1680 train_time:11796ms step_avg:88.03ms
step:135/1680 train_time:11883ms step_avg:88.02ms
step:136/1680 train_time:11970ms step_avg:88.02ms
step:137/1680 train_time:12057ms step_avg:88.01ms
step:138/1680 train_time:12145ms step_avg:88.01ms
step:139/1680 train_time:12236ms step_avg:88.03ms
step:140/1680 train_time:12326ms step_avg:88.04ms
step:141/1680 train_time:12415ms step_avg:88.05ms
step:142/1680 train_time:12503ms step_avg:88.05ms
step:143/1680 train_time:12590ms step_avg:88.04ms
step:144/1680 train_time:12678ms step_avg:88.04ms
step:145/1680 train_time:12766ms step_avg:88.04ms
step:146/1680 train_time:12853ms step_avg:88.03ms
step:147/1680 train_time:12941ms step_avg:88.03ms
step:148/1680 train_time:13028ms step_avg:88.03ms
step:149/1680 train_time:13116ms step_avg:88.03ms
step:150/1680 train_time:13205ms step_avg:88.04ms
step:151/1680 train_time:13295ms step_avg:88.05ms
step:152/1680 train_time:13384ms step_avg:88.05ms
step:153/1680 train_time:13472ms step_avg:88.05ms
step:154/1680 train_time:13561ms step_avg:88.06ms
step:155/1680 train_time:13649ms step_avg:88.06ms
step:156/1680 train_time:13737ms step_avg:88.06ms
step:157/1680 train_time:13825ms step_avg:88.06ms
step:158/1680 train_time:13912ms step_avg:88.05ms
step:159/1680 train_time:14000ms step_avg:88.05ms
step:160/1680 train_time:14087ms step_avg:88.04ms
step:161/1680 train_time:14176ms step_avg:88.05ms
step:162/1680 train_time:14264ms step_avg:88.05ms
step:163/1680 train_time:14353ms step_avg:88.06ms
step:164/1680 train_time:14441ms step_avg:88.06ms
step:165/1680 train_time:14529ms step_avg:88.05ms
step:166/1680 train_time:14617ms step_avg:88.06ms
step:167/1680 train_time:14705ms step_avg:88.06ms
step:168/1680 train_time:14793ms step_avg:88.05ms
step:169/1680 train_time:14882ms step_avg:88.06ms
step:170/1680 train_time:14969ms step_avg:88.05ms
step:171/1680 train_time:15057ms step_avg:88.05ms
step:172/1680 train_time:15145ms step_avg:88.06ms
step:173/1680 train_time:15234ms step_avg:88.06ms
step:174/1680 train_time:15323ms step_avg:88.06ms
step:175/1680 train_time:15411ms step_avg:88.06ms
step:176/1680 train_time:15499ms step_avg:88.06ms
step:177/1680 train_time:15588ms step_avg:88.07ms
step:178/1680 train_time:15676ms step_avg:88.07ms
step:179/1680 train_time:15765ms step_avg:88.07ms
step:180/1680 train_time:15852ms step_avg:88.07ms
step:181/1680 train_time:15940ms step_avg:88.06ms
step:182/1680 train_time:16027ms step_avg:88.06ms
step:183/1680 train_time:16115ms step_avg:88.06ms
step:184/1680 train_time:16204ms step_avg:88.06ms
step:185/1680 train_time:16292ms step_avg:88.06ms
step:186/1680 train_time:16380ms step_avg:88.06ms
step:187/1680 train_time:16468ms step_avg:88.07ms
step:188/1680 train_time:16557ms step_avg:88.07ms
step:189/1680 train_time:16645ms step_avg:88.07ms
step:190/1680 train_time:16733ms step_avg:88.07ms
step:191/1680 train_time:16821ms step_avg:88.07ms
step:192/1680 train_time:16909ms step_avg:88.07ms
step:193/1680 train_time:16997ms step_avg:88.07ms
step:194/1680 train_time:17085ms step_avg:88.07ms
step:195/1680 train_time:17173ms step_avg:88.07ms
step:196/1680 train_time:17261ms step_avg:88.07ms
step:197/1680 train_time:17349ms step_avg:88.06ms
step:198/1680 train_time:17437ms step_avg:88.07ms
step:199/1680 train_time:17525ms step_avg:88.07ms
step:200/1680 train_time:17613ms step_avg:88.07ms
step:201/1680 train_time:17702ms step_avg:88.07ms
step:202/1680 train_time:17790ms step_avg:88.07ms
step:203/1680 train_time:17878ms step_avg:88.07ms
step:204/1680 train_time:17965ms step_avg:88.07ms
step:205/1680 train_time:18053ms step_avg:88.06ms
step:206/1680 train_time:18141ms step_avg:88.06ms
step:207/1680 train_time:18229ms step_avg:88.06ms
step:208/1680 train_time:18317ms step_avg:88.06ms
step:209/1680 train_time:18405ms step_avg:88.06ms
step:210/1680 train_time:18493ms step_avg:88.06ms
step:211/1680 train_time:18581ms step_avg:88.06ms
step:212/1680 train_time:18669ms step_avg:88.06ms
step:213/1680 train_time:18757ms step_avg:88.06ms
step:214/1680 train_time:18846ms step_avg:88.06ms
step:215/1680 train_time:18933ms step_avg:88.06ms
step:216/1680 train_time:19021ms step_avg:88.06ms
step:217/1680 train_time:19109ms step_avg:88.06ms
step:218/1680 train_time:19196ms step_avg:88.06ms
step:219/1680 train_time:19286ms step_avg:88.06ms
step:220/1680 train_time:19374ms step_avg:88.06ms
step:221/1680 train_time:19462ms step_avg:88.06ms
step:222/1680 train_time:19550ms step_avg:88.06ms
step:223/1680 train_time:19637ms step_avg:88.06ms
step:224/1680 train_time:19725ms step_avg:88.06ms
step:225/1680 train_time:19813ms step_avg:88.06ms
step:226/1680 train_time:19902ms step_avg:88.06ms
step:227/1680 train_time:19990ms step_avg:88.06ms
step:228/1680 train_time:20078ms step_avg:88.06ms
step:229/1680 train_time:20166ms step_avg:88.06ms
step:230/1680 train_time:20254ms step_avg:88.06ms
step:231/1680 train_time:20342ms step_avg:88.06ms
step:232/1680 train_time:20430ms step_avg:88.06ms
step:233/1680 train_time:20518ms step_avg:88.06ms
step:234/1680 train_time:20607ms step_avg:88.06ms
step:235/1680 train_time:20695ms step_avg:88.06ms
step:236/1680 train_time:20783ms step_avg:88.06ms
step:237/1680 train_time:20871ms step_avg:88.06ms
step:238/1680 train_time:20959ms step_avg:88.06ms
step:239/1680 train_time:21047ms step_avg:88.06ms
step:240/1680 train_time:21135ms step_avg:88.06ms
step:241/1680 train_time:21223ms step_avg:88.06ms
step:242/1680 train_time:21310ms step_avg:88.06ms
step:243/1680 train_time:21399ms step_avg:88.06ms
step:244/1680 train_time:21487ms step_avg:88.06ms
step:245/1680 train_time:21574ms step_avg:88.06ms
step:246/1680 train_time:21662ms step_avg:88.06ms
step:247/1680 train_time:21750ms step_avg:88.06ms
step:248/1680 train_time:21838ms step_avg:88.06ms
step:249/1680 train_time:21926ms step_avg:88.06ms
step:250/1680 train_time:22015ms step_avg:88.06ms
step:250/1680 val_loss:3.9690 train_time:22103ms step_avg:88.41ms
step:251/1680 train_time:22127ms step_avg:88.15ms
step:252/1680 train_time:22194ms step_avg:88.07ms
step:253/1680 train_time:22285ms step_avg:88.08ms
step:254/1680 train_time:22375ms step_avg:88.09ms
step:255/1680 train_time:22462ms step_avg:88.08ms
step:256/1680 train_time:22550ms step_avg:88.08ms
step:257/1680 train_time:22637ms step_avg:88.08ms
step:258/1680 train_time:22724ms step_avg:88.08ms
step:259/1680 train_time:22811ms step_avg:88.07ms
step:260/1680 train_time:22897ms step_avg:88.07ms
step:261/1680 train_time:22985ms step_avg:88.07ms
step:262/1680 train_time:23074ms step_avg:88.07ms
step:263/1680 train_time:23164ms step_avg:88.08ms
step:264/1680 train_time:23254ms step_avg:88.09ms
step:265/1680 train_time:23343ms step_avg:88.09ms
step:266/1680 train_time:23431ms step_avg:88.08ms
step:267/1680 train_time:23518ms step_avg:88.08ms
step:268/1680 train_time:23605ms step_avg:88.08ms
step:269/1680 train_time:23693ms step_avg:88.08ms
step:270/1680 train_time:23781ms step_avg:88.08ms
step:271/1680 train_time:23868ms step_avg:88.07ms
step:272/1680 train_time:23955ms step_avg:88.07ms
step:273/1680 train_time:24042ms step_avg:88.07ms
step:274/1680 train_time:24131ms step_avg:88.07ms
step:275/1680 train_time:24219ms step_avg:88.07ms
step:276/1680 train_time:24309ms step_avg:88.07ms
step:277/1680 train_time:24396ms step_avg:88.07ms
step:278/1680 train_time:24485ms step_avg:88.07ms
step:279/1680 train_time:24573ms step_avg:88.07ms
step:280/1680 train_time:24660ms step_avg:88.07ms
step:281/1680 train_time:24748ms step_avg:88.07ms
step:282/1680 train_time:24835ms step_avg:88.07ms
step:283/1680 train_time:24922ms step_avg:88.06ms
step:284/1680 train_time:25009ms step_avg:88.06ms
step:285/1680 train_time:25097ms step_avg:88.06ms
step:286/1680 train_time:25186ms step_avg:88.06ms
step:287/1680 train_time:25274ms step_avg:88.06ms
step:288/1680 train_time:25362ms step_avg:88.06ms
step:289/1680 train_time:25451ms step_avg:88.07ms
step:290/1680 train_time:25539ms step_avg:88.06ms
step:291/1680 train_time:25627ms step_avg:88.07ms
step:292/1680 train_time:25716ms step_avg:88.07ms
step:293/1680 train_time:25803ms step_avg:88.07ms
step:294/1680 train_time:25890ms step_avg:88.06ms
step:295/1680 train_time:25978ms step_avg:88.06ms
step:296/1680 train_time:26065ms step_avg:88.06ms
step:297/1680 train_time:26154ms step_avg:88.06ms
step:298/1680 train_time:26241ms step_avg:88.06ms
step:299/1680 train_time:26330ms step_avg:88.06ms
step:300/1680 train_time:26419ms step_avg:88.06ms
step:301/1680 train_time:26507ms step_avg:88.06ms
step:302/1680 train_time:26594ms step_avg:88.06ms
step:303/1680 train_time:26682ms step_avg:88.06ms
step:304/1680 train_time:26770ms step_avg:88.06ms
step:305/1680 train_time:26857ms step_avg:88.06ms
step:306/1680 train_time:26944ms step_avg:88.05ms
step:307/1680 train_time:27032ms step_avg:88.05ms
step:308/1680 train_time:27119ms step_avg:88.05ms
step:309/1680 train_time:27207ms step_avg:88.05ms
step:310/1680 train_time:27295ms step_avg:88.05ms
step:311/1680 train_time:27384ms step_avg:88.05ms
step:312/1680 train_time:27472ms step_avg:88.05ms
step:313/1680 train_time:27559ms step_avg:88.05ms
step:314/1680 train_time:27647ms step_avg:88.05ms
step:315/1680 train_time:27735ms step_avg:88.05ms
step:316/1680 train_time:27823ms step_avg:88.05ms
step:317/1680 train_time:27911ms step_avg:88.05ms
step:318/1680 train_time:27999ms step_avg:88.05ms
step:319/1680 train_time:28087ms step_avg:88.05ms
step:320/1680 train_time:28176ms step_avg:88.05ms
step:321/1680 train_time:28263ms step_avg:88.05ms
step:322/1680 train_time:28352ms step_avg:88.05ms
step:323/1680 train_time:28440ms step_avg:88.05ms
step:324/1680 train_time:28528ms step_avg:88.05ms
step:325/1680 train_time:28616ms step_avg:88.05ms
step:326/1680 train_time:28704ms step_avg:88.05ms
step:327/1680 train_time:28792ms step_avg:88.05ms
step:328/1680 train_time:28880ms step_avg:88.05ms
step:329/1680 train_time:28968ms step_avg:88.05ms
step:330/1680 train_time:29057ms step_avg:88.05ms
step:331/1680 train_time:29145ms step_avg:88.05ms
step:332/1680 train_time:29233ms step_avg:88.05ms
step:333/1680 train_time:29321ms step_avg:88.05ms
step:334/1680 train_time:29409ms step_avg:88.05ms
step:335/1680 train_time:29496ms step_avg:88.05ms
step:336/1680 train_time:29584ms step_avg:88.05ms
step:337/1680 train_time:29672ms step_avg:88.05ms
step:338/1680 train_time:29760ms step_avg:88.05ms
step:339/1680 train_time:29848ms step_avg:88.05ms
step:340/1680 train_time:29936ms step_avg:88.05ms
step:341/1680 train_time:30023ms step_avg:88.04ms
step:342/1680 train_time:30112ms step_avg:88.05ms
step:343/1680 train_time:30199ms step_avg:88.04ms
step:344/1680 train_time:30287ms step_avg:88.05ms
step:345/1680 train_time:30375ms step_avg:88.04ms
step:346/1680 train_time:30463ms step_avg:88.04ms
step:347/1680 train_time:30551ms step_avg:88.04ms
step:348/1680 train_time:30638ms step_avg:88.04ms
step:349/1680 train_time:30726ms step_avg:88.04ms
step:350/1680 train_time:30814ms step_avg:88.04ms
step:351/1680 train_time:30901ms step_avg:88.04ms
step:352/1680 train_time:30990ms step_avg:88.04ms
step:353/1680 train_time:31077ms step_avg:88.04ms
step:354/1680 train_time:31165ms step_avg:88.04ms
step:355/1680 train_time:31253ms step_avg:88.04ms
step:356/1680 train_time:31340ms step_avg:88.04ms
step:357/1680 train_time:31428ms step_avg:88.03ms
step:358/1680 train_time:31516ms step_avg:88.03ms
step:359/1680 train_time:31604ms step_avg:88.03ms
step:360/1680 train_time:31693ms step_avg:88.04ms
step:361/1680 train_time:31780ms step_avg:88.03ms
step:362/1680 train_time:31868ms step_avg:88.03ms
step:363/1680 train_time:31956ms step_avg:88.03ms
step:364/1680 train_time:32044ms step_avg:88.03ms
step:365/1680 train_time:32132ms step_avg:88.03ms
step:366/1680 train_time:32220ms step_avg:88.03ms
step:367/1680 train_time:32307ms step_avg:88.03ms
step:368/1680 train_time:32395ms step_avg:88.03ms
step:369/1680 train_time:32483ms step_avg:88.03ms
step:370/1680 train_time:32571ms step_avg:88.03ms
step:371/1680 train_time:32659ms step_avg:88.03ms
step:372/1680 train_time:32747ms step_avg:88.03ms
step:373/1680 train_time:32835ms step_avg:88.03ms
step:374/1680 train_time:32923ms step_avg:88.03ms
step:375/1680 train_time:33011ms step_avg:88.03ms
step:375/1680 val_loss:3.8142 train_time:33100ms step_avg:88.27ms
step:376/1680 train_time:33123ms step_avg:88.09ms
step:377/1680 train_time:33192ms step_avg:88.04ms
step:378/1680 train_time:33285ms step_avg:88.06ms
step:379/1680 train_time:33374ms step_avg:88.06ms
step:380/1680 train_time:33462ms step_avg:88.06ms
step:381/1680 train_time:33549ms step_avg:88.05ms
step:382/1680 train_time:33636ms step_avg:88.05ms
step:383/1680 train_time:33723ms step_avg:88.05ms
step:384/1680 train_time:33810ms step_avg:88.05ms
step:385/1680 train_time:33897ms step_avg:88.04ms
step:386/1680 train_time:33985ms step_avg:88.04ms
step:387/1680 train_time:34072ms step_avg:88.04ms
step:388/1680 train_time:34162ms step_avg:88.05ms
step:389/1680 train_time:34252ms step_avg:88.05ms
step:390/1680 train_time:34342ms step_avg:88.06ms
step:391/1680 train_time:34430ms step_avg:88.06ms
step:392/1680 train_time:34518ms step_avg:88.06ms
step:393/1680 train_time:34605ms step_avg:88.05ms
step:394/1680 train_time:34692ms step_avg:88.05ms
step:395/1680 train_time:34780ms step_avg:88.05ms
step:396/1680 train_time:34866ms step_avg:88.05ms
step:397/1680 train_time:34954ms step_avg:88.04ms
step:398/1680 train_time:35041ms step_avg:88.04ms
step:399/1680 train_time:35128ms step_avg:88.04ms
step:400/1680 train_time:35217ms step_avg:88.04ms
step:401/1680 train_time:35306ms step_avg:88.05ms
step:402/1680 train_time:35395ms step_avg:88.05ms
step:403/1680 train_time:35484ms step_avg:88.05ms
step:404/1680 train_time:35572ms step_avg:88.05ms
step:405/1680 train_time:35660ms step_avg:88.05ms
step:406/1680 train_time:35747ms step_avg:88.05ms
step:407/1680 train_time:35835ms step_avg:88.05ms
step:408/1680 train_time:35923ms step_avg:88.05ms
step:409/1680 train_time:36010ms step_avg:88.04ms
step:410/1680 train_time:36099ms step_avg:88.05ms
step:411/1680 train_time:36187ms step_avg:88.05ms
step:412/1680 train_time:36276ms step_avg:88.05ms
step:413/1680 train_time:36364ms step_avg:88.05ms
step:414/1680 train_time:36452ms step_avg:88.05ms
step:415/1680 train_time:36541ms step_avg:88.05ms
step:416/1680 train_time:36628ms step_avg:88.05ms
step:417/1680 train_time:36716ms step_avg:88.05ms
step:418/1680 train_time:36803ms step_avg:88.05ms
step:419/1680 train_time:36890ms step_avg:88.04ms
step:420/1680 train_time:36978ms step_avg:88.04ms
step:421/1680 train_time:37066ms step_avg:88.04ms
step:422/1680 train_time:37154ms step_avg:88.04ms
step:423/1680 train_time:37242ms step_avg:88.04ms
step:424/1680 train_time:37330ms step_avg:88.04ms
step:425/1680 train_time:37419ms step_avg:88.04ms
step:426/1680 train_time:37507ms step_avg:88.05ms
step:427/1680 train_time:37596ms step_avg:88.05ms
step:428/1680 train_time:37683ms step_avg:88.05ms
step:429/1680 train_time:37771ms step_avg:88.04ms
step:430/1680 train_time:37859ms step_avg:88.04ms
step:431/1680 train_time:37947ms step_avg:88.04ms
step:432/1680 train_time:38034ms step_avg:88.04ms
step:433/1680 train_time:38122ms step_avg:88.04ms
step:434/1680 train_time:38209ms step_avg:88.04ms
step:435/1680 train_time:38297ms step_avg:88.04ms
step:436/1680 train_time:38386ms step_avg:88.04ms
step:437/1680 train_time:38474ms step_avg:88.04ms
step:438/1680 train_time:38563ms step_avg:88.04ms
step:439/1680 train_time:38651ms step_avg:88.04ms
step:440/1680 train_time:38739ms step_avg:88.04ms
step:441/1680 train_time:38826ms step_avg:88.04ms
step:442/1680 train_time:38914ms step_avg:88.04ms
step:443/1680 train_time:39002ms step_avg:88.04ms
step:444/1680 train_time:39089ms step_avg:88.04ms
step:445/1680 train_time:39177ms step_avg:88.04ms
step:446/1680 train_time:39265ms step_avg:88.04ms
step:447/1680 train_time:39353ms step_avg:88.04ms
step:448/1680 train_time:39441ms step_avg:88.04ms
step:449/1680 train_time:39529ms step_avg:88.04ms
step:450/1680 train_time:39618ms step_avg:88.04ms
step:451/1680 train_time:39706ms step_avg:88.04ms
step:452/1680 train_time:39794ms step_avg:88.04ms
step:453/1680 train_time:39882ms step_avg:88.04ms
step:454/1680 train_time:39969ms step_avg:88.04ms
step:455/1680 train_time:40057ms step_avg:88.04ms
step:456/1680 train_time:40145ms step_avg:88.04ms
step:457/1680 train_time:40232ms step_avg:88.04ms
step:458/1680 train_time:40321ms step_avg:88.04ms
step:459/1680 train_time:40409ms step_avg:88.04ms
step:460/1680 train_time:40496ms step_avg:88.04ms
step:461/1680 train_time:40584ms step_avg:88.04ms
step:462/1680 train_time:40672ms step_avg:88.03ms
step:463/1680 train_time:40760ms step_avg:88.03ms
step:464/1680 train_time:40848ms step_avg:88.03ms
step:465/1680 train_time:40936ms step_avg:88.03ms
step:466/1680 train_time:41024ms step_avg:88.03ms
step:467/1680 train_time:41112ms step_avg:88.03ms
step:468/1680 train_time:41200ms step_avg:88.03ms
step:469/1680 train_time:41288ms step_avg:88.03ms
step:470/1680 train_time:41375ms step_avg:88.03ms
step:471/1680 train_time:41463ms step_avg:88.03ms
step:472/1680 train_time:41551ms step_avg:88.03ms
step:473/1680 train_time:41639ms step_avg:88.03ms
step:474/1680 train_time:41727ms step_avg:88.03ms
step:475/1680 train_time:41815ms step_avg:88.03ms
step:476/1680 train_time:41903ms step_avg:88.03ms
step:477/1680 train_time:41990ms step_avg:88.03ms
step:478/1680 train_time:42079ms step_avg:88.03ms
step:479/1680 train_time:42166ms step_avg:88.03ms
step:480/1680 train_time:42254ms step_avg:88.03ms
step:481/1680 train_time:42343ms step_avg:88.03ms
step:482/1680 train_time:42430ms step_avg:88.03ms
step:483/1680 train_time:42518ms step_avg:88.03ms
step:484/1680 train_time:42607ms step_avg:88.03ms
step:485/1680 train_time:42695ms step_avg:88.03ms
step:486/1680 train_time:42783ms step_avg:88.03ms
step:487/1680 train_time:42870ms step_avg:88.03ms
step:488/1680 train_time:42958ms step_avg:88.03ms
step:489/1680 train_time:43046ms step_avg:88.03ms
step:490/1680 train_time:43133ms step_avg:88.03ms
step:491/1680 train_time:43222ms step_avg:88.03ms
step:492/1680 train_time:43309ms step_avg:88.03ms
step:493/1680 train_time:43397ms step_avg:88.03ms
step:494/1680 train_time:43485ms step_avg:88.03ms
step:495/1680 train_time:43572ms step_avg:88.03ms
step:496/1680 train_time:43661ms step_avg:88.03ms
step:497/1680 train_time:43748ms step_avg:88.02ms
step:498/1680 train_time:43836ms step_avg:88.02ms
step:499/1680 train_time:43924ms step_avg:88.02ms
step:500/1680 train_time:44012ms step_avg:88.02ms
step:500/1680 val_loss:3.7169 train_time:44101ms step_avg:88.20ms
step:501/1680 train_time:44124ms step_avg:88.07ms
step:502/1680 train_time:44192ms step_avg:88.03ms
step:503/1680 train_time:44286ms step_avg:88.04ms
step:504/1680 train_time:44375ms step_avg:88.05ms
step:505/1680 train_time:44462ms step_avg:88.04ms
step:506/1680 train_time:44549ms step_avg:88.04ms
step:507/1680 train_time:44636ms step_avg:88.04ms
step:508/1680 train_time:44723ms step_avg:88.04ms
step:509/1680 train_time:44810ms step_avg:88.04ms
step:510/1680 train_time:44897ms step_avg:88.03ms
step:511/1680 train_time:44985ms step_avg:88.03ms
step:512/1680 train_time:45073ms step_avg:88.03ms
step:513/1680 train_time:45162ms step_avg:88.03ms
step:514/1680 train_time:45253ms step_avg:88.04ms
step:515/1680 train_time:45341ms step_avg:88.04ms
step:516/1680 train_time:45429ms step_avg:88.04ms
step:517/1680 train_time:45517ms step_avg:88.04ms
step:518/1680 train_time:45604ms step_avg:88.04ms
step:519/1680 train_time:45690ms step_avg:88.04ms
step:520/1680 train_time:45777ms step_avg:88.03ms
step:521/1680 train_time:45865ms step_avg:88.03ms
step:522/1680 train_time:45952ms step_avg:88.03ms
step:523/1680 train_time:46039ms step_avg:88.03ms
step:524/1680 train_time:46128ms step_avg:88.03ms
step:525/1680 train_time:46217ms step_avg:88.03ms
step:526/1680 train_time:46306ms step_avg:88.03ms
step:527/1680 train_time:46395ms step_avg:88.04ms
step:528/1680 train_time:46482ms step_avg:88.03ms
step:529/1680 train_time:46570ms step_avg:88.03ms
step:530/1680 train_time:46658ms step_avg:88.03ms
step:531/1680 train_time:46746ms step_avg:88.03ms
step:532/1680 train_time:46833ms step_avg:88.03ms
step:533/1680 train_time:46921ms step_avg:88.03ms
step:534/1680 train_time:47009ms step_avg:88.03ms
step:535/1680 train_time:47096ms step_avg:88.03ms
step:536/1680 train_time:47185ms step_avg:88.03ms
step:537/1680 train_time:47274ms step_avg:88.03ms
step:538/1680 train_time:47362ms step_avg:88.03ms
step:539/1680 train_time:47450ms step_avg:88.03ms
step:540/1680 train_time:47537ms step_avg:88.03ms
step:541/1680 train_time:47625ms step_avg:88.03ms
step:542/1680 train_time:47713ms step_avg:88.03ms
step:543/1680 train_time:47800ms step_avg:88.03ms
step:544/1680 train_time:47887ms step_avg:88.03ms
step:545/1680 train_time:47975ms step_avg:88.03ms
step:546/1680 train_time:48063ms step_avg:88.03ms
step:547/1680 train_time:48150ms step_avg:88.03ms
step:548/1680 train_time:48238ms step_avg:88.03ms
step:549/1680 train_time:48328ms step_avg:88.03ms
step:550/1680 train_time:48418ms step_avg:88.03ms
step:551/1680 train_time:48507ms step_avg:88.04ms
step:552/1680 train_time:48596ms step_avg:88.04ms
step:553/1680 train_time:48686ms step_avg:88.04ms
step:554/1680 train_time:48775ms step_avg:88.04ms
step:555/1680 train_time:48864ms step_avg:88.04ms
step:556/1680 train_time:48953ms step_avg:88.04ms
step:557/1680 train_time:49041ms step_avg:88.05ms
step:558/1680 train_time:49130ms step_avg:88.05ms
step:559/1680 train_time:49219ms step_avg:88.05ms
step:560/1680 train_time:49309ms step_avg:88.05ms
step:561/1680 train_time:49398ms step_avg:88.05ms
step:562/1680 train_time:49487ms step_avg:88.05ms
step:563/1680 train_time:49576ms step_avg:88.06ms
step:564/1680 train_time:49666ms step_avg:88.06ms
step:565/1680 train_time:49756ms step_avg:88.06ms
step:566/1680 train_time:49845ms step_avg:88.07ms
step:567/1680 train_time:49934ms step_avg:88.07ms
step:568/1680 train_time:50023ms step_avg:88.07ms
step:569/1680 train_time:50112ms step_avg:88.07ms
step:570/1680 train_time:50201ms step_avg:88.07ms
step:571/1680 train_time:50290ms step_avg:88.07ms
step:572/1680 train_time:50379ms step_avg:88.08ms
step:573/1680 train_time:50468ms step_avg:88.08ms
step:574/1680 train_time:50558ms step_avg:88.08ms
step:575/1680 train_time:50647ms step_avg:88.08ms
step:576/1680 train_time:50735ms step_avg:88.08ms
step:577/1680 train_time:50824ms step_avg:88.08ms
step:578/1680 train_time:50916ms step_avg:88.09ms
step:579/1680 train_time:51003ms step_avg:88.09ms
step:580/1680 train_time:51092ms step_avg:88.09ms
step:581/1680 train_time:51181ms step_avg:88.09ms
step:582/1680 train_time:51271ms step_avg:88.09ms
step:583/1680 train_time:51360ms step_avg:88.10ms
step:584/1680 train_time:51449ms step_avg:88.10ms
step:585/1680 train_time:51538ms step_avg:88.10ms
step:586/1680 train_time:51627ms step_avg:88.10ms
step:587/1680 train_time:51716ms step_avg:88.10ms
step:588/1680 train_time:51805ms step_avg:88.10ms
step:589/1680 train_time:51894ms step_avg:88.11ms
step:590/1680 train_time:51983ms step_avg:88.11ms
step:591/1680 train_time:52073ms step_avg:88.11ms
step:592/1680 train_time:52163ms step_avg:88.11ms
step:593/1680 train_time:52252ms step_avg:88.11ms
step:594/1680 train_time:52341ms step_avg:88.12ms
step:595/1680 train_time:52430ms step_avg:88.12ms
step:596/1680 train_time:52519ms step_avg:88.12ms
step:597/1680 train_time:52608ms step_avg:88.12ms
step:598/1680 train_time:52697ms step_avg:88.12ms
step:599/1680 train_time:52787ms step_avg:88.12ms
step:600/1680 train_time:52877ms step_avg:88.13ms
step:601/1680 train_time:52966ms step_avg:88.13ms
step:602/1680 train_time:53056ms step_avg:88.13ms
step:603/1680 train_time:53145ms step_avg:88.13ms
step:604/1680 train_time:53235ms step_avg:88.14ms
step:605/1680 train_time:53325ms step_avg:88.14ms
step:606/1680 train_time:53414ms step_avg:88.14ms
step:607/1680 train_time:53503ms step_avg:88.14ms
step:608/1680 train_time:53591ms step_avg:88.14ms
step:609/1680 train_time:53680ms step_avg:88.14ms
step:610/1680 train_time:53769ms step_avg:88.15ms
step:611/1680 train_time:53858ms step_avg:88.15ms
step:612/1680 train_time:53948ms step_avg:88.15ms
step:613/1680 train_time:54037ms step_avg:88.15ms
step:614/1680 train_time:54126ms step_avg:88.15ms
step:615/1680 train_time:54216ms step_avg:88.16ms
step:616/1680 train_time:54305ms step_avg:88.16ms
step:617/1680 train_time:54394ms step_avg:88.16ms
step:618/1680 train_time:54483ms step_avg:88.16ms
step:619/1680 train_time:54572ms step_avg:88.16ms
step:620/1680 train_time:54661ms step_avg:88.16ms
step:621/1680 train_time:54750ms step_avg:88.16ms
step:622/1680 train_time:54839ms step_avg:88.17ms
step:623/1680 train_time:54928ms step_avg:88.17ms
step:624/1680 train_time:55017ms step_avg:88.17ms
step:625/1680 train_time:55106ms step_avg:88.17ms
step:625/1680 val_loss:3.6163 train_time:55197ms step_avg:88.31ms
step:626/1680 train_time:55221ms step_avg:88.21ms
step:627/1680 train_time:55287ms step_avg:88.18ms
step:628/1680 train_time:55387ms step_avg:88.20ms
step:629/1680 train_time:55484ms step_avg:88.21ms
step:630/1680 train_time:55573ms step_avg:88.21ms
step:631/1680 train_time:55661ms step_avg:88.21ms
step:632/1680 train_time:55750ms step_avg:88.21ms
step:633/1680 train_time:55838ms step_avg:88.21ms
step:634/1680 train_time:55925ms step_avg:88.21ms
step:635/1680 train_time:56014ms step_avg:88.21ms
step:636/1680 train_time:56101ms step_avg:88.21ms
step:637/1680 train_time:56189ms step_avg:88.21ms
step:638/1680 train_time:56278ms step_avg:88.21ms
step:639/1680 train_time:56369ms step_avg:88.21ms
step:640/1680 train_time:56461ms step_avg:88.22ms
step:641/1680 train_time:56552ms step_avg:88.22ms
step:642/1680 train_time:56642ms step_avg:88.23ms
step:643/1680 train_time:56730ms step_avg:88.23ms
step:644/1680 train_time:56819ms step_avg:88.23ms
step:645/1680 train_time:56907ms step_avg:88.23ms
step:646/1680 train_time:56995ms step_avg:88.23ms
step:647/1680 train_time:57082ms step_avg:88.23ms
step:648/1680 train_time:57170ms step_avg:88.23ms
step:649/1680 train_time:57259ms step_avg:88.23ms
step:650/1680 train_time:57349ms step_avg:88.23ms
step:651/1680 train_time:57439ms step_avg:88.23ms
step:652/1680 train_time:57529ms step_avg:88.24ms
step:653/1680 train_time:57619ms step_avg:88.24ms
step:654/1680 train_time:57708ms step_avg:88.24ms
step:655/1680 train_time:57796ms step_avg:88.24ms
step:656/1680 train_time:57885ms step_avg:88.24ms
step:657/1680 train_time:57973ms step_avg:88.24ms
step:658/1680 train_time:58062ms step_avg:88.24ms
step:659/1680 train_time:58150ms step_avg:88.24ms
step:660/1680 train_time:58240ms step_avg:88.24ms
step:661/1680 train_time:58329ms step_avg:88.24ms
step:662/1680 train_time:58419ms step_avg:88.25ms
step:663/1680 train_time:58509ms step_avg:88.25ms
step:664/1680 train_time:58598ms step_avg:88.25ms
step:665/1680 train_time:58687ms step_avg:88.25ms
step:666/1680 train_time:58776ms step_avg:88.25ms
step:667/1680 train_time:58864ms step_avg:88.25ms
step:668/1680 train_time:58954ms step_avg:88.25ms
step:669/1680 train_time:59042ms step_avg:88.25ms
step:670/1680 train_time:59131ms step_avg:88.26ms
step:671/1680 train_time:59220ms step_avg:88.26ms
step:672/1680 train_time:59309ms step_avg:88.26ms
step:673/1680 train_time:59398ms step_avg:88.26ms
step:674/1680 train_time:59487ms step_avg:88.26ms
step:675/1680 train_time:59577ms step_avg:88.26ms
step:676/1680 train_time:59665ms step_avg:88.26ms
step:677/1680 train_time:59755ms step_avg:88.26ms
step:678/1680 train_time:59844ms step_avg:88.27ms
step:679/1680 train_time:59933ms step_avg:88.27ms
step:680/1680 train_time:60021ms step_avg:88.27ms
step:681/1680 train_time:60110ms step_avg:88.27ms
step:682/1680 train_time:60199ms step_avg:88.27ms
step:683/1680 train_time:60287ms step_avg:88.27ms
step:684/1680 train_time:60376ms step_avg:88.27ms
step:685/1680 train_time:60465ms step_avg:88.27ms
step:686/1680 train_time:60554ms step_avg:88.27ms
step:687/1680 train_time:60643ms step_avg:88.27ms
step:688/1680 train_time:60732ms step_avg:88.27ms
step:689/1680 train_time:60822ms step_avg:88.28ms
step:690/1680 train_time:60911ms step_avg:88.28ms
step:691/1680 train_time:61001ms step_avg:88.28ms
step:692/1680 train_time:61090ms step_avg:88.28ms
step:693/1680 train_time:61179ms step_avg:88.28ms
step:694/1680 train_time:61267ms step_avg:88.28ms
step:695/1680 train_time:61356ms step_avg:88.28ms
step:696/1680 train_time:61446ms step_avg:88.28ms
step:697/1680 train_time:61535ms step_avg:88.29ms
step:698/1680 train_time:61624ms step_avg:88.29ms
step:699/1680 train_time:61714ms step_avg:88.29ms
step:700/1680 train_time:61803ms step_avg:88.29ms
step:701/1680 train_time:61891ms step_avg:88.29ms
step:702/1680 train_time:61981ms step_avg:88.29ms
step:703/1680 train_time:62069ms step_avg:88.29ms
step:704/1680 train_time:62159ms step_avg:88.29ms
step:705/1680 train_time:62247ms step_avg:88.29ms
step:706/1680 train_time:62336ms step_avg:88.29ms
step:707/1680 train_time:62424ms step_avg:88.29ms
step:708/1680 train_time:62514ms step_avg:88.30ms
step:709/1680 train_time:62603ms step_avg:88.30ms
step:710/1680 train_time:62692ms step_avg:88.30ms
step:711/1680 train_time:62781ms step_avg:88.30ms
step:712/1680 train_time:62870ms step_avg:88.30ms
step:713/1680 train_time:62960ms step_avg:88.30ms
step:714/1680 train_time:63048ms step_avg:88.30ms
step:715/1680 train_time:63138ms step_avg:88.30ms
step:716/1680 train_time:63226ms step_avg:88.30ms
step:717/1680 train_time:63316ms step_avg:88.31ms
step:718/1680 train_time:63404ms step_avg:88.31ms
step:719/1680 train_time:63493ms step_avg:88.31ms
step:720/1680 train_time:63582ms step_avg:88.31ms
step:721/1680 train_time:63671ms step_avg:88.31ms
step:722/1680 train_time:63760ms step_avg:88.31ms
step:723/1680 train_time:63849ms step_avg:88.31ms
step:724/1680 train_time:63939ms step_avg:88.31ms
step:725/1680 train_time:64028ms step_avg:88.31ms
step:726/1680 train_time:64118ms step_avg:88.32ms
step:727/1680 train_time:64206ms step_avg:88.32ms
step:728/1680 train_time:64296ms step_avg:88.32ms
step:729/1680 train_time:64384ms step_avg:88.32ms
step:730/1680 train_time:64473ms step_avg:88.32ms
step:731/1680 train_time:64562ms step_avg:88.32ms
step:732/1680 train_time:64651ms step_avg:88.32ms
step:733/1680 train_time:64741ms step_avg:88.32ms
step:734/1680 train_time:64831ms step_avg:88.33ms
step:735/1680 train_time:64919ms step_avg:88.33ms
step:736/1680 train_time:65008ms step_avg:88.33ms
step:737/1680 train_time:65098ms step_avg:88.33ms
step:738/1680 train_time:65186ms step_avg:88.33ms
step:739/1680 train_time:65276ms step_avg:88.33ms
step:740/1680 train_time:65364ms step_avg:88.33ms
step:741/1680 train_time:65454ms step_avg:88.33ms
step:742/1680 train_time:65542ms step_avg:88.33ms
step:743/1680 train_time:65631ms step_avg:88.33ms
step:744/1680 train_time:65720ms step_avg:88.33ms
step:745/1680 train_time:65809ms step_avg:88.33ms
step:746/1680 train_time:65898ms step_avg:88.34ms
step:747/1680 train_time:65987ms step_avg:88.34ms
step:748/1680 train_time:66076ms step_avg:88.34ms
step:749/1680 train_time:66165ms step_avg:88.34ms
step:750/1680 train_time:66255ms step_avg:88.34ms
step:750/1680 val_loss:3.5658 train_time:66345ms step_avg:88.46ms
step:751/1680 train_time:66368ms step_avg:88.37ms
step:752/1680 train_time:66437ms step_avg:88.35ms
step:753/1680 train_time:66530ms step_avg:88.35ms
step:754/1680 train_time:66622ms step_avg:88.36ms
step:755/1680 train_time:66710ms step_avg:88.36ms
step:756/1680 train_time:66799ms step_avg:88.36ms
step:757/1680 train_time:66887ms step_avg:88.36ms
step:758/1680 train_time:66975ms step_avg:88.36ms
step:759/1680 train_time:67063ms step_avg:88.36ms
step:760/1680 train_time:67151ms step_avg:88.36ms
step:761/1680 train_time:67239ms step_avg:88.36ms
step:762/1680 train_time:67329ms step_avg:88.36ms
step:763/1680 train_time:67419ms step_avg:88.36ms
step:764/1680 train_time:67509ms step_avg:88.36ms
step:765/1680 train_time:67600ms step_avg:88.37ms
step:766/1680 train_time:67689ms step_avg:88.37ms
step:767/1680 train_time:67779ms step_avg:88.37ms
step:768/1680 train_time:67868ms step_avg:88.37ms
step:769/1680 train_time:67956ms step_avg:88.37ms
step:770/1680 train_time:68043ms step_avg:88.37ms
step:771/1680 train_time:68132ms step_avg:88.37ms
step:772/1680 train_time:68220ms step_avg:88.37ms
step:773/1680 train_time:68309ms step_avg:88.37ms
step:774/1680 train_time:68398ms step_avg:88.37ms
step:775/1680 train_time:68488ms step_avg:88.37ms
step:776/1680 train_time:68578ms step_avg:88.37ms
step:777/1680 train_time:68667ms step_avg:88.37ms
step:778/1680 train_time:68756ms step_avg:88.38ms
step:779/1680 train_time:68845ms step_avg:88.38ms
step:780/1680 train_time:68933ms step_avg:88.38ms
step:781/1680 train_time:69022ms step_avg:88.38ms
step:782/1680 train_time:69110ms step_avg:88.38ms
step:783/1680 train_time:69199ms step_avg:88.38ms
step:784/1680 train_time:69288ms step_avg:88.38ms
step:785/1680 train_time:69377ms step_avg:88.38ms
step:786/1680 train_time:69467ms step_avg:88.38ms
step:787/1680 train_time:69556ms step_avg:88.38ms
step:788/1680 train_time:69646ms step_avg:88.38ms
step:789/1680 train_time:69736ms step_avg:88.39ms
step:790/1680 train_time:69825ms step_avg:88.39ms
step:791/1680 train_time:69914ms step_avg:88.39ms
step:792/1680 train_time:70002ms step_avg:88.39ms
step:793/1680 train_time:70091ms step_avg:88.39ms
step:794/1680 train_time:70179ms step_avg:88.39ms
step:795/1680 train_time:70268ms step_avg:88.39ms
step:796/1680 train_time:70356ms step_avg:88.39ms
step:797/1680 train_time:70445ms step_avg:88.39ms
step:798/1680 train_time:70535ms step_avg:88.39ms
step:799/1680 train_time:70624ms step_avg:88.39ms
step:800/1680 train_time:70714ms step_avg:88.39ms
step:801/1680 train_time:70803ms step_avg:88.39ms
step:802/1680 train_time:70892ms step_avg:88.39ms
step:803/1680 train_time:70981ms step_avg:88.39ms
step:804/1680 train_time:71070ms step_avg:88.40ms
step:805/1680 train_time:71161ms step_avg:88.40ms
step:806/1680 train_time:71249ms step_avg:88.40ms
step:807/1680 train_time:71338ms step_avg:88.40ms
step:808/1680 train_time:71427ms step_avg:88.40ms
step:809/1680 train_time:71517ms step_avg:88.40ms
step:810/1680 train_time:71606ms step_avg:88.40ms
step:811/1680 train_time:71696ms step_avg:88.40ms
step:812/1680 train_time:71785ms step_avg:88.40ms
step:813/1680 train_time:71874ms step_avg:88.41ms
step:814/1680 train_time:71963ms step_avg:88.41ms
step:815/1680 train_time:72052ms step_avg:88.41ms
step:816/1680 train_time:72141ms step_avg:88.41ms
step:817/1680 train_time:72230ms step_avg:88.41ms
step:818/1680 train_time:72319ms step_avg:88.41ms
step:819/1680 train_time:72407ms step_avg:88.41ms
step:820/1680 train_time:72497ms step_avg:88.41ms
step:821/1680 train_time:72586ms step_avg:88.41ms
step:822/1680 train_time:72676ms step_avg:88.41ms
step:823/1680 train_time:72765ms step_avg:88.41ms
step:824/1680 train_time:72854ms step_avg:88.41ms
step:825/1680 train_time:72943ms step_avg:88.42ms
step:826/1680 train_time:73031ms step_avg:88.42ms
step:827/1680 train_time:73120ms step_avg:88.42ms
step:828/1680 train_time:73209ms step_avg:88.42ms
step:829/1680 train_time:73298ms step_avg:88.42ms
step:830/1680 train_time:73387ms step_avg:88.42ms
step:831/1680 train_time:73475ms step_avg:88.42ms
step:832/1680 train_time:73564ms step_avg:88.42ms
step:833/1680 train_time:73653ms step_avg:88.42ms
step:834/1680 train_time:73742ms step_avg:88.42ms
step:835/1680 train_time:73831ms step_avg:88.42ms
step:836/1680 train_time:73920ms step_avg:88.42ms
step:837/1680 train_time:74009ms step_avg:88.42ms
step:838/1680 train_time:74099ms step_avg:88.42ms
step:839/1680 train_time:74188ms step_avg:88.42ms
step:840/1680 train_time:74276ms step_avg:88.42ms
step:841/1680 train_time:74365ms step_avg:88.42ms
step:842/1680 train_time:74454ms step_avg:88.43ms
step:843/1680 train_time:74543ms step_avg:88.43ms
step:844/1680 train_time:74632ms step_avg:88.43ms
step:845/1680 train_time:74722ms step_avg:88.43ms
step:846/1680 train_time:74811ms step_avg:88.43ms
step:847/1680 train_time:74901ms step_avg:88.43ms
step:848/1680 train_time:74990ms step_avg:88.43ms
step:849/1680 train_time:75079ms step_avg:88.43ms
step:850/1680 train_time:75168ms step_avg:88.43ms
step:851/1680 train_time:75257ms step_avg:88.43ms
step:852/1680 train_time:75345ms step_avg:88.43ms
step:853/1680 train_time:75434ms step_avg:88.43ms
step:854/1680 train_time:75523ms step_avg:88.43ms
step:855/1680 train_time:75612ms step_avg:88.44ms
step:856/1680 train_time:75702ms step_avg:88.44ms
step:857/1680 train_time:75791ms step_avg:88.44ms
step:858/1680 train_time:75880ms step_avg:88.44ms
step:859/1680 train_time:75969ms step_avg:88.44ms
step:860/1680 train_time:76059ms step_avg:88.44ms
step:861/1680 train_time:76148ms step_avg:88.44ms
step:862/1680 train_time:76237ms step_avg:88.44ms
step:863/1680 train_time:76325ms step_avg:88.44ms
step:864/1680 train_time:76415ms step_avg:88.44ms
step:865/1680 train_time:76503ms step_avg:88.44ms
step:866/1680 train_time:76592ms step_avg:88.44ms
step:867/1680 train_time:76681ms step_avg:88.44ms
step:868/1680 train_time:76770ms step_avg:88.44ms
step:869/1680 train_time:76859ms step_avg:88.45ms
step:870/1680 train_time:76948ms step_avg:88.45ms
step:871/1680 train_time:77037ms step_avg:88.45ms
step:872/1680 train_time:77125ms step_avg:88.45ms
step:873/1680 train_time:77215ms step_avg:88.45ms
step:874/1680 train_time:77304ms step_avg:88.45ms
step:875/1680 train_time:77394ms step_avg:88.45ms
step:875/1680 val_loss:3.5198 train_time:77483ms step_avg:88.55ms
step:876/1680 train_time:77506ms step_avg:88.48ms
step:877/1680 train_time:77578ms step_avg:88.46ms
step:878/1680 train_time:77672ms step_avg:88.46ms
step:879/1680 train_time:77761ms step_avg:88.47ms
step:880/1680 train_time:77850ms step_avg:88.47ms
step:881/1680 train_time:77938ms step_avg:88.47ms
step:882/1680 train_time:78026ms step_avg:88.47ms
step:883/1680 train_time:78114ms step_avg:88.46ms
step:884/1680 train_time:78202ms step_avg:88.46ms
step:885/1680 train_time:78290ms step_avg:88.46ms
step:886/1680 train_time:78378ms step_avg:88.46ms
step:887/1680 train_time:78469ms step_avg:88.47ms
step:888/1680 train_time:78559ms step_avg:88.47ms
step:889/1680 train_time:78650ms step_avg:88.47ms
step:890/1680 train_time:78740ms step_avg:88.47ms
step:891/1680 train_time:78829ms step_avg:88.47ms
step:892/1680 train_time:78918ms step_avg:88.47ms
step:893/1680 train_time:79006ms step_avg:88.47ms
step:894/1680 train_time:79095ms step_avg:88.47ms
step:895/1680 train_time:79183ms step_avg:88.47ms
step:896/1680 train_time:79272ms step_avg:88.47ms
step:897/1680 train_time:79360ms step_avg:88.47ms
step:898/1680 train_time:79449ms step_avg:88.47ms
step:899/1680 train_time:79538ms step_avg:88.47ms
step:900/1680 train_time:79628ms step_avg:88.48ms
step:901/1680 train_time:79718ms step_avg:88.48ms
step:902/1680 train_time:79807ms step_avg:88.48ms
step:903/1680 train_time:79897ms step_avg:88.48ms
step:904/1680 train_time:79985ms step_avg:88.48ms
step:905/1680 train_time:80075ms step_avg:88.48ms
step:906/1680 train_time:80163ms step_avg:88.48ms
step:907/1680 train_time:80252ms step_avg:88.48ms
step:908/1680 train_time:80341ms step_avg:88.48ms
step:909/1680 train_time:80430ms step_avg:88.48ms
step:910/1680 train_time:80519ms step_avg:88.48ms
step:911/1680 train_time:80608ms step_avg:88.48ms
step:912/1680 train_time:80698ms step_avg:88.48ms
step:913/1680 train_time:80788ms step_avg:88.49ms
step:914/1680 train_time:80878ms step_avg:88.49ms
step:915/1680 train_time:80967ms step_avg:88.49ms
step:916/1680 train_time:81057ms step_avg:88.49ms
step:917/1680 train_time:81146ms step_avg:88.49ms
step:918/1680 train_time:81235ms step_avg:88.49ms
step:919/1680 train_time:81324ms step_avg:88.49ms
step:920/1680 train_time:81413ms step_avg:88.49ms
step:921/1680 train_time:81502ms step_avg:88.49ms
step:922/1680 train_time:81591ms step_avg:88.49ms
step:923/1680 train_time:81680ms step_avg:88.49ms
step:924/1680 train_time:81769ms step_avg:88.49ms
step:925/1680 train_time:81858ms step_avg:88.50ms
step:926/1680 train_time:81947ms step_avg:88.50ms
step:927/1680 train_time:82036ms step_avg:88.50ms
step:928/1680 train_time:82126ms step_avg:88.50ms
step:929/1680 train_time:82215ms step_avg:88.50ms
step:930/1680 train_time:82303ms step_avg:88.50ms
step:931/1680 train_time:82392ms step_avg:88.50ms
step:932/1680 train_time:82481ms step_avg:88.50ms
step:933/1680 train_time:82570ms step_avg:88.50ms
step:934/1680 train_time:82659ms step_avg:88.50ms
step:935/1680 train_time:82749ms step_avg:88.50ms
step:936/1680 train_time:82838ms step_avg:88.50ms
step:937/1680 train_time:82928ms step_avg:88.50ms
step:938/1680 train_time:83017ms step_avg:88.50ms
step:939/1680 train_time:83106ms step_avg:88.50ms
step:940/1680 train_time:83196ms step_avg:88.51ms
step:941/1680 train_time:83285ms step_avg:88.51ms
step:942/1680 train_time:83374ms step_avg:88.51ms
step:943/1680 train_time:83462ms step_avg:88.51ms
step:944/1680 train_time:83552ms step_avg:88.51ms
step:945/1680 train_time:83641ms step_avg:88.51ms
step:946/1680 train_time:83730ms step_avg:88.51ms
step:947/1680 train_time:83819ms step_avg:88.51ms
step:948/1680 train_time:83908ms step_avg:88.51ms
step:949/1680 train_time:83997ms step_avg:88.51ms
step:950/1680 train_time:84086ms step_avg:88.51ms
step:951/1680 train_time:84175ms step_avg:88.51ms
step:952/1680 train_time:84264ms step_avg:88.51ms
step:953/1680 train_time:84354ms step_avg:88.51ms
step:954/1680 train_time:84442ms step_avg:88.51ms
step:955/1680 train_time:84531ms step_avg:88.51ms
step:956/1680 train_time:84620ms step_avg:88.51ms
step:957/1680 train_time:84709ms step_avg:88.52ms
step:958/1680 train_time:84798ms step_avg:88.52ms
step:959/1680 train_time:84887ms step_avg:88.52ms
step:960/1680 train_time:84977ms step_avg:88.52ms
step:961/1680 train_time:85066ms step_avg:88.52ms
step:962/1680 train_time:85156ms step_avg:88.52ms
step:963/1680 train_time:85245ms step_avg:88.52ms
step:964/1680 train_time:85334ms step_avg:88.52ms
step:965/1680 train_time:85423ms step_avg:88.52ms
step:966/1680 train_time:85512ms step_avg:88.52ms
step:967/1680 train_time:85600ms step_avg:88.52ms
step:968/1680 train_time:85689ms step_avg:88.52ms
step:969/1680 train_time:85777ms step_avg:88.52ms
step:970/1680 train_time:85867ms step_avg:88.52ms
step:971/1680 train_time:85957ms step_avg:88.52ms
step:972/1680 train_time:86046ms step_avg:88.52ms
step:973/1680 train_time:86135ms step_avg:88.52ms
step:974/1680 train_time:86224ms step_avg:88.53ms
step:975/1680 train_time:86313ms step_avg:88.53ms
step:976/1680 train_time:86402ms step_avg:88.53ms
step:977/1680 train_time:86492ms step_avg:88.53ms
step:978/1680 train_time:86580ms step_avg:88.53ms
step:979/1680 train_time:86670ms step_avg:88.53ms
step:980/1680 train_time:86759ms step_avg:88.53ms
step:981/1680 train_time:86848ms step_avg:88.53ms
step:982/1680 train_time:86937ms step_avg:88.53ms
step:983/1680 train_time:87027ms step_avg:88.53ms
step:984/1680 train_time:87116ms step_avg:88.53ms
step:985/1680 train_time:87205ms step_avg:88.53ms
step:986/1680 train_time:87294ms step_avg:88.53ms
step:987/1680 train_time:87383ms step_avg:88.53ms
step:988/1680 train_time:87472ms step_avg:88.53ms
step:989/1680 train_time:87561ms step_avg:88.53ms
step:990/1680 train_time:87650ms step_avg:88.54ms
step:991/1680 train_time:87739ms step_avg:88.54ms
step:992/1680 train_time:87828ms step_avg:88.54ms
step:993/1680 train_time:87917ms step_avg:88.54ms
step:994/1680 train_time:88006ms step_avg:88.54ms
step:995/1680 train_time:88095ms step_avg:88.54ms
step:996/1680 train_time:88185ms step_avg:88.54ms
step:997/1680 train_time:88274ms step_avg:88.54ms
step:998/1680 train_time:88364ms step_avg:88.54ms
step:999/1680 train_time:88454ms step_avg:88.54ms
step:1000/1680 train_time:88543ms step_avg:88.54ms
step:1000/1680 val_loss:3.4703 train_time:88633ms step_avg:88.63ms
step:1001/1680 train_time:88656ms step_avg:88.57ms
step:1002/1680 train_time:88725ms step_avg:88.55ms
step:1003/1680 train_time:88819ms step_avg:88.55ms
step:1004/1680 train_time:88910ms step_avg:88.56ms
step:1005/1680 train_time:88998ms step_avg:88.56ms
step:1006/1680 train_time:89087ms step_avg:88.56ms
step:1007/1680 train_time:89175ms step_avg:88.56ms
step:1008/1680 train_time:89263ms step_avg:88.55ms
step:1009/1680 train_time:89351ms step_avg:88.55ms
step:1010/1680 train_time:89439ms step_avg:88.55ms
step:1011/1680 train_time:89527ms step_avg:88.55ms
step:1012/1680 train_time:89618ms step_avg:88.55ms
step:1013/1680 train_time:89708ms step_avg:88.56ms
step:1014/1680 train_time:89800ms step_avg:88.56ms
step:1015/1680 train_time:89890ms step_avg:88.56ms
step:1016/1680 train_time:89979ms step_avg:88.56ms
step:1017/1680 train_time:90068ms step_avg:88.56ms
step:1018/1680 train_time:90157ms step_avg:88.56ms
step:1019/1680 train_time:90245ms step_avg:88.56ms
step:1020/1680 train_time:90333ms step_avg:88.56ms
step:1021/1680 train_time:90421ms step_avg:88.56ms
step:1022/1680 train_time:90510ms step_avg:88.56ms
step:1023/1680 train_time:90598ms step_avg:88.56ms
step:1024/1680 train_time:90688ms step_avg:88.56ms
step:1025/1680 train_time:90778ms step_avg:88.56ms
step:1026/1680 train_time:90868ms step_avg:88.57ms
step:1027/1680 train_time:90958ms step_avg:88.57ms
step:1028/1680 train_time:91047ms step_avg:88.57ms
step:1029/1680 train_time:91136ms step_avg:88.57ms
step:1030/1680 train_time:91225ms step_avg:88.57ms
step:1031/1680 train_time:91314ms step_avg:88.57ms
step:1032/1680 train_time:91402ms step_avg:88.57ms
step:1033/1680 train_time:91491ms step_avg:88.57ms
step:1034/1680 train_time:91579ms step_avg:88.57ms
step:1035/1680 train_time:91668ms step_avg:88.57ms
step:1036/1680 train_time:91757ms step_avg:88.57ms
step:1037/1680 train_time:91847ms step_avg:88.57ms
step:1038/1680 train_time:91937ms step_avg:88.57ms
step:1039/1680 train_time:92027ms step_avg:88.57ms
step:1040/1680 train_time:92118ms step_avg:88.57ms
step:1041/1680 train_time:92207ms step_avg:88.58ms
step:1042/1680 train_time:92296ms step_avg:88.58ms
step:1043/1680 train_time:92384ms step_avg:88.58ms
step:1044/1680 train_time:92473ms step_avg:88.58ms
step:1045/1680 train_time:92562ms step_avg:88.58ms
step:1046/1680 train_time:92651ms step_avg:88.58ms
step:1047/1680 train_time:92740ms step_avg:88.58ms
step:1048/1680 train_time:92829ms step_avg:88.58ms
step:1049/1680 train_time:92919ms step_avg:88.58ms
step:1050/1680 train_time:93008ms step_avg:88.58ms
step:1051/1680 train_time:93097ms step_avg:88.58ms
step:1052/1680 train_time:93187ms step_avg:88.58ms
step:1053/1680 train_time:93276ms step_avg:88.58ms
step:1054/1680 train_time:93364ms step_avg:88.58ms
step:1055/1680 train_time:93453ms step_avg:88.58ms
step:1056/1680 train_time:93542ms step_avg:88.58ms
step:1057/1680 train_time:93631ms step_avg:88.58ms
step:1058/1680 train_time:93720ms step_avg:88.58ms
step:1059/1680 train_time:93809ms step_avg:88.58ms
step:1060/1680 train_time:93898ms step_avg:88.58ms
step:1061/1680 train_time:93987ms step_avg:88.58ms
step:1062/1680 train_time:94077ms step_avg:88.58ms
step:1063/1680 train_time:94166ms step_avg:88.59ms
step:1064/1680 train_time:94256ms step_avg:88.59ms
step:1065/1680 train_time:94345ms step_avg:88.59ms
step:1066/1680 train_time:94434ms step_avg:88.59ms
step:1067/1680 train_time:94523ms step_avg:88.59ms
step:1068/1680 train_time:94612ms step_avg:88.59ms
step:1069/1680 train_time:94700ms step_avg:88.59ms
step:1070/1680 train_time:94789ms step_avg:88.59ms
step:1071/1680 train_time:94878ms step_avg:88.59ms
step:1072/1680 train_time:94967ms step_avg:88.59ms
step:1073/1680 train_time:95056ms step_avg:88.59ms
step:1074/1680 train_time:95145ms step_avg:88.59ms
step:1075/1680 train_time:95235ms step_avg:88.59ms
step:1076/1680 train_time:95323ms step_avg:88.59ms
step:1077/1680 train_time:95412ms step_avg:88.59ms
step:1078/1680 train_time:95500ms step_avg:88.59ms
step:1079/1680 train_time:95590ms step_avg:88.59ms
step:1080/1680 train_time:95678ms step_avg:88.59ms
step:1081/1680 train_time:95767ms step_avg:88.59ms
step:1082/1680 train_time:95857ms step_avg:88.59ms
step:1083/1680 train_time:95945ms step_avg:88.59ms
step:1084/1680 train_time:96035ms step_avg:88.59ms
step:1085/1680 train_time:96124ms step_avg:88.59ms
step:1086/1680 train_time:96215ms step_avg:88.60ms
step:1087/1680 train_time:96304ms step_avg:88.60ms
step:1088/1680 train_time:96393ms step_avg:88.60ms
step:1089/1680 train_time:96481ms step_avg:88.60ms
step:1090/1680 train_time:96570ms step_avg:88.60ms
step:1091/1680 train_time:96659ms step_avg:88.60ms
step:1092/1680 train_time:96748ms step_avg:88.60ms
step:1093/1680 train_time:96837ms step_avg:88.60ms
step:1094/1680 train_time:96927ms step_avg:88.60ms
step:1095/1680 train_time:97016ms step_avg:88.60ms
step:1096/1680 train_time:97105ms step_avg:88.60ms
step:1097/1680 train_time:97195ms step_avg:88.60ms
step:1098/1680 train_time:97284ms step_avg:88.60ms
step:1099/1680 train_time:97375ms step_avg:88.60ms
step:1100/1680 train_time:97465ms step_avg:88.60ms
step:1101/1680 train_time:97555ms step_avg:88.61ms
step:1102/1680 train_time:97644ms step_avg:88.61ms
step:1103/1680 train_time:97733ms step_avg:88.61ms
step:1104/1680 train_time:97823ms step_avg:88.61ms
step:1105/1680 train_time:97914ms step_avg:88.61ms
step:1106/1680 train_time:98004ms step_avg:88.61ms
step:1107/1680 train_time:98093ms step_avg:88.61ms
step:1108/1680 train_time:98184ms step_avg:88.61ms
step:1109/1680 train_time:98273ms step_avg:88.61ms
step:1110/1680 train_time:98362ms step_avg:88.61ms
step:1111/1680 train_time:98453ms step_avg:88.62ms
step:1112/1680 train_time:98543ms step_avg:88.62ms
step:1113/1680 train_time:98632ms step_avg:88.62ms
step:1114/1680 train_time:98721ms step_avg:88.62ms
step:1115/1680 train_time:98812ms step_avg:88.62ms
step:1116/1680 train_time:98902ms step_avg:88.62ms
step:1117/1680 train_time:98991ms step_avg:88.62ms
step:1118/1680 train_time:99081ms step_avg:88.62ms
step:1119/1680 train_time:99171ms step_avg:88.62ms
step:1120/1680 train_time:99260ms step_avg:88.62ms
step:1121/1680 train_time:99350ms step_avg:88.63ms
step:1122/1680 train_time:99439ms step_avg:88.63ms
step:1123/1680 train_time:99530ms step_avg:88.63ms
step:1124/1680 train_time:99619ms step_avg:88.63ms
step:1125/1680 train_time:99709ms step_avg:88.63ms
step:1125/1680 val_loss:3.4165 train_time:99799ms step_avg:88.71ms
step:1126/1680 train_time:99823ms step_avg:88.65ms
step:1127/1680 train_time:99890ms step_avg:88.63ms
step:1128/1680 train_time:99984ms step_avg:88.64ms
step:1129/1680 train_time:100079ms step_avg:88.64ms
step:1130/1680 train_time:100168ms step_avg:88.64ms
step:1131/1680 train_time:100257ms step_avg:88.64ms
step:1132/1680 train_time:100346ms step_avg:88.64ms
step:1133/1680 train_time:100434ms step_avg:88.64ms
step:1134/1680 train_time:100523ms step_avg:88.64ms
step:1135/1680 train_time:100611ms step_avg:88.64ms
step:1136/1680 train_time:100700ms step_avg:88.64ms
step:1137/1680 train_time:100792ms step_avg:88.65ms
step:1138/1680 train_time:100883ms step_avg:88.65ms
step:1139/1680 train_time:100974ms step_avg:88.65ms
step:1140/1680 train_time:101066ms step_avg:88.65ms
step:1141/1680 train_time:101157ms step_avg:88.66ms
step:1142/1680 train_time:101246ms step_avg:88.66ms
step:1143/1680 train_time:101336ms step_avg:88.66ms
step:1144/1680 train_time:101425ms step_avg:88.66ms
step:1145/1680 train_time:101514ms step_avg:88.66ms
step:1146/1680 train_time:101603ms step_avg:88.66ms
step:1147/1680 train_time:101691ms step_avg:88.66ms
step:1148/1680 train_time:101782ms step_avg:88.66ms
step:1149/1680 train_time:101872ms step_avg:88.66ms
step:1150/1680 train_time:101962ms step_avg:88.66ms
step:1151/1680 train_time:102053ms step_avg:88.66ms
step:1152/1680 train_time:102143ms step_avg:88.67ms
step:1153/1680 train_time:102232ms step_avg:88.67ms
step:1154/1680 train_time:102322ms step_avg:88.67ms
step:1155/1680 train_time:102411ms step_avg:88.67ms
step:1156/1680 train_time:102500ms step_avg:88.67ms
step:1157/1680 train_time:102589ms step_avg:88.67ms
step:1158/1680 train_time:102679ms step_avg:88.67ms
step:1159/1680 train_time:102768ms step_avg:88.67ms
step:1160/1680 train_time:102858ms step_avg:88.67ms
step:1161/1680 train_time:102948ms step_avg:88.67ms
step:1162/1680 train_time:103039ms step_avg:88.67ms
step:1163/1680 train_time:103128ms step_avg:88.67ms
step:1164/1680 train_time:103220ms step_avg:88.68ms
step:1165/1680 train_time:103309ms step_avg:88.68ms
step:1166/1680 train_time:103398ms step_avg:88.68ms
step:1167/1680 train_time:103487ms step_avg:88.68ms
step:1168/1680 train_time:103576ms step_avg:88.68ms
step:1169/1680 train_time:103665ms step_avg:88.68ms
step:1170/1680 train_time:103754ms step_avg:88.68ms
step:1171/1680 train_time:103843ms step_avg:88.68ms
step:1172/1680 train_time:103933ms step_avg:88.68ms
step:1173/1680 train_time:104024ms step_avg:88.68ms
step:1174/1680 train_time:104114ms step_avg:88.68ms
step:1175/1680 train_time:104205ms step_avg:88.68ms
step:1176/1680 train_time:104295ms step_avg:88.69ms
step:1177/1680 train_time:104385ms step_avg:88.69ms
step:1178/1680 train_time:104474ms step_avg:88.69ms
step:1179/1680 train_time:104564ms step_avg:88.69ms
step:1180/1680 train_time:104654ms step_avg:88.69ms
step:1181/1680 train_time:104744ms step_avg:88.69ms
step:1182/1680 train_time:104834ms step_avg:88.69ms
step:1183/1680 train_time:104924ms step_avg:88.69ms
step:1184/1680 train_time:105013ms step_avg:88.69ms
step:1185/1680 train_time:105103ms step_avg:88.69ms
step:1186/1680 train_time:105193ms step_avg:88.70ms
step:1187/1680 train_time:105283ms step_avg:88.70ms
step:1188/1680 train_time:105372ms step_avg:88.70ms
step:1189/1680 train_time:105462ms step_avg:88.70ms
step:1190/1680 train_time:105552ms step_avg:88.70ms
step:1191/1680 train_time:105642ms step_avg:88.70ms
step:1192/1680 train_time:105733ms step_avg:88.70ms
step:1193/1680 train_time:105822ms step_avg:88.70ms
step:1194/1680 train_time:105911ms step_avg:88.70ms
step:1195/1680 train_time:106001ms step_avg:88.70ms
step:1196/1680 train_time:106091ms step_avg:88.70ms
step:1197/1680 train_time:106181ms step_avg:88.71ms
step:1198/1680 train_time:106271ms step_avg:88.71ms
step:1199/1680 train_time:106360ms step_avg:88.71ms
step:1200/1680 train_time:106450ms step_avg:88.71ms
step:1201/1680 train_time:106540ms step_avg:88.71ms
step:1202/1680 train_time:106629ms step_avg:88.71ms
step:1203/1680 train_time:106719ms step_avg:88.71ms
step:1204/1680 train_time:106808ms step_avg:88.71ms
step:1205/1680 train_time:106897ms step_avg:88.71ms
step:1206/1680 train_time:106986ms step_avg:88.71ms
step:1207/1680 train_time:107077ms step_avg:88.71ms
step:1208/1680 train_time:107166ms step_avg:88.71ms
step:1209/1680 train_time:107256ms step_avg:88.71ms
step:1210/1680 train_time:107345ms step_avg:88.71ms
step:1211/1680 train_time:107436ms step_avg:88.72ms
step:1212/1680 train_time:107526ms step_avg:88.72ms
step:1213/1680 train_time:107616ms step_avg:88.72ms
step:1214/1680 train_time:107706ms step_avg:88.72ms
step:1215/1680 train_time:107796ms step_avg:88.72ms
step:1216/1680 train_time:107886ms step_avg:88.72ms
step:1217/1680 train_time:107976ms step_avg:88.72ms
step:1218/1680 train_time:108066ms step_avg:88.72ms
step:1219/1680 train_time:108156ms step_avg:88.73ms
step:1220/1680 train_time:108245ms step_avg:88.73ms
step:1221/1680 train_time:108335ms step_avg:88.73ms
step:1222/1680 train_time:108424ms step_avg:88.73ms
step:1223/1680 train_time:108514ms step_avg:88.73ms
step:1224/1680 train_time:108604ms step_avg:88.73ms
step:1225/1680 train_time:108695ms step_avg:88.73ms
step:1226/1680 train_time:108784ms step_avg:88.73ms
step:1227/1680 train_time:108874ms step_avg:88.73ms
step:1228/1680 train_time:108964ms step_avg:88.73ms
step:1229/1680 train_time:109053ms step_avg:88.73ms
step:1230/1680 train_time:109143ms step_avg:88.73ms
step:1231/1680 train_time:109234ms step_avg:88.74ms
step:1232/1680 train_time:109324ms step_avg:88.74ms
step:1233/1680 train_time:109413ms step_avg:88.74ms
step:1234/1680 train_time:109502ms step_avg:88.74ms
step:1235/1680 train_time:109592ms step_avg:88.74ms
step:1236/1680 train_time:109682ms step_avg:88.74ms
step:1237/1680 train_time:109771ms step_avg:88.74ms
step:1238/1680 train_time:109861ms step_avg:88.74ms
step:1239/1680 train_time:109951ms step_avg:88.74ms
step:1240/1680 train_time:110041ms step_avg:88.74ms
step:1241/1680 train_time:110130ms step_avg:88.74ms
step:1242/1680 train_time:110220ms step_avg:88.74ms
step:1243/1680 train_time:110309ms step_avg:88.74ms
step:1244/1680 train_time:110399ms step_avg:88.75ms
step:1245/1680 train_time:110489ms step_avg:88.75ms
step:1246/1680 train_time:110578ms step_avg:88.75ms
step:1247/1680 train_time:110667ms step_avg:88.75ms
step:1248/1680 train_time:110758ms step_avg:88.75ms
step:1249/1680 train_time:110847ms step_avg:88.75ms
step:1250/1680 train_time:110937ms step_avg:88.75ms
step:1250/1680 val_loss:3.3780 train_time:111027ms step_avg:88.82ms
step:1251/1680 train_time:111050ms step_avg:88.77ms
step:1252/1680 train_time:111121ms step_avg:88.76ms
step:1253/1680 train_time:111215ms step_avg:88.76ms
step:1254/1680 train_time:111304ms step_avg:88.76ms
step:1255/1680 train_time:111395ms step_avg:88.76ms
step:1256/1680 train_time:111483ms step_avg:88.76ms
step:1257/1680 train_time:111572ms step_avg:88.76ms
step:1258/1680 train_time:111662ms step_avg:88.76ms
step:1259/1680 train_time:111751ms step_avg:88.76ms
step:1260/1680 train_time:111840ms step_avg:88.76ms
step:1261/1680 train_time:111929ms step_avg:88.76ms
step:1262/1680 train_time:112020ms step_avg:88.76ms
step:1263/1680 train_time:112111ms step_avg:88.77ms
step:1264/1680 train_time:112202ms step_avg:88.77ms
step:1265/1680 train_time:112293ms step_avg:88.77ms
step:1266/1680 train_time:112382ms step_avg:88.77ms
step:1267/1680 train_time:112471ms step_avg:88.77ms
step:1268/1680 train_time:112561ms step_avg:88.77ms
step:1269/1680 train_time:112651ms step_avg:88.77ms
step:1270/1680 train_time:112739ms step_avg:88.77ms
step:1271/1680 train_time:112828ms step_avg:88.77ms
step:1272/1680 train_time:112918ms step_avg:88.77ms
step:1273/1680 train_time:113008ms step_avg:88.77ms
step:1274/1680 train_time:113099ms step_avg:88.78ms
step:1275/1680 train_time:113190ms step_avg:88.78ms
step:1276/1680 train_time:113281ms step_avg:88.78ms
step:1277/1680 train_time:113371ms step_avg:88.78ms
step:1278/1680 train_time:113461ms step_avg:88.78ms
step:1279/1680 train_time:113551ms step_avg:88.78ms
step:1280/1680 train_time:113640ms step_avg:88.78ms
step:1281/1680 train_time:113729ms step_avg:88.78ms
step:1282/1680 train_time:113818ms step_avg:88.78ms
step:1283/1680 train_time:113907ms step_avg:88.78ms
step:1284/1680 train_time:113997ms step_avg:88.78ms
step:1285/1680 train_time:114088ms step_avg:88.78ms
step:1286/1680 train_time:114178ms step_avg:88.79ms
step:1287/1680 train_time:114268ms step_avg:88.79ms
step:1288/1680 train_time:114358ms step_avg:88.79ms
step:1289/1680 train_time:114447ms step_avg:88.79ms
step:1290/1680 train_time:114538ms step_avg:88.79ms
step:1291/1680 train_time:114627ms step_avg:88.79ms
step:1292/1680 train_time:114717ms step_avg:88.79ms
step:1293/1680 train_time:114806ms step_avg:88.79ms
step:1294/1680 train_time:114895ms step_avg:88.79ms
step:1295/1680 train_time:114986ms step_avg:88.79ms
step:1296/1680 train_time:115076ms step_avg:88.79ms
step:1297/1680 train_time:115166ms step_avg:88.79ms
step:1298/1680 train_time:115256ms step_avg:88.80ms
step:1299/1680 train_time:115346ms step_avg:88.80ms
step:1300/1680 train_time:115435ms step_avg:88.80ms
step:1301/1680 train_time:115525ms step_avg:88.80ms
step:1302/1680 train_time:115615ms step_avg:88.80ms
step:1303/1680 train_time:115705ms step_avg:88.80ms
step:1304/1680 train_time:115794ms step_avg:88.80ms
step:1305/1680 train_time:115883ms step_avg:88.80ms
step:1306/1680 train_time:115973ms step_avg:88.80ms
step:1307/1680 train_time:116062ms step_avg:88.80ms
step:1308/1680 train_time:116152ms step_avg:88.80ms
step:1309/1680 train_time:116241ms step_avg:88.80ms
step:1310/1680 train_time:116332ms step_avg:88.80ms
step:1311/1680 train_time:116422ms step_avg:88.80ms
step:1312/1680 train_time:116514ms step_avg:88.81ms
step:1313/1680 train_time:116603ms step_avg:88.81ms
step:1314/1680 train_time:116693ms step_avg:88.81ms
step:1315/1680 train_time:116782ms step_avg:88.81ms
step:1316/1680 train_time:116872ms step_avg:88.81ms
step:1317/1680 train_time:116961ms step_avg:88.81ms
step:1318/1680 train_time:117051ms step_avg:88.81ms
step:1319/1680 train_time:117141ms step_avg:88.81ms
step:1320/1680 train_time:117230ms step_avg:88.81ms
step:1321/1680 train_time:117321ms step_avg:88.81ms
step:1322/1680 train_time:117411ms step_avg:88.81ms
step:1323/1680 train_time:117501ms step_avg:88.81ms
step:1324/1680 train_time:117591ms step_avg:88.81ms
step:1325/1680 train_time:117680ms step_avg:88.82ms
step:1326/1680 train_time:117770ms step_avg:88.82ms
step:1327/1680 train_time:117859ms step_avg:88.82ms
step:1328/1680 train_time:117948ms step_avg:88.82ms
step:1329/1680 train_time:118037ms step_avg:88.82ms
step:1330/1680 train_time:118126ms step_avg:88.82ms
step:1331/1680 train_time:118216ms step_avg:88.82ms
step:1332/1680 train_time:118306ms step_avg:88.82ms
step:1333/1680 train_time:118397ms step_avg:88.82ms
step:1334/1680 train_time:118487ms step_avg:88.82ms
step:1335/1680 train_time:118577ms step_avg:88.82ms
step:1336/1680 train_time:118667ms step_avg:88.82ms
step:1337/1680 train_time:118756ms step_avg:88.82ms
step:1338/1680 train_time:118845ms step_avg:88.82ms
step:1339/1680 train_time:118935ms step_avg:88.82ms
step:1340/1680 train_time:119025ms step_avg:88.82ms
step:1341/1680 train_time:119115ms step_avg:88.83ms
step:1342/1680 train_time:119204ms step_avg:88.83ms
step:1343/1680 train_time:119294ms step_avg:88.83ms
step:1344/1680 train_time:119384ms step_avg:88.83ms
step:1345/1680 train_time:119475ms step_avg:88.83ms
step:1346/1680 train_time:119564ms step_avg:88.83ms
step:1347/1680 train_time:119654ms step_avg:88.83ms
step:1348/1680 train_time:119743ms step_avg:88.83ms
step:1349/1680 train_time:119834ms step_avg:88.83ms
step:1350/1680 train_time:119923ms step_avg:88.83ms
step:1351/1680 train_time:120013ms step_avg:88.83ms
step:1352/1680 train_time:120102ms step_avg:88.83ms
step:1353/1680 train_time:120192ms step_avg:88.83ms
step:1354/1680 train_time:120282ms step_avg:88.83ms
step:1355/1680 train_time:120372ms step_avg:88.84ms
step:1356/1680 train_time:120462ms step_avg:88.84ms
step:1357/1680 train_time:120552ms step_avg:88.84ms
step:1358/1680 train_time:120642ms step_avg:88.84ms
step:1359/1680 train_time:120731ms step_avg:88.84ms
step:1360/1680 train_time:120821ms step_avg:88.84ms
step:1361/1680 train_time:120911ms step_avg:88.84ms
step:1362/1680 train_time:121000ms step_avg:88.84ms
step:1363/1680 train_time:121091ms step_avg:88.84ms
step:1364/1680 train_time:121181ms step_avg:88.84ms
step:1365/1680 train_time:121270ms step_avg:88.84ms
step:1366/1680 train_time:121360ms step_avg:88.84ms
step:1367/1680 train_time:121449ms step_avg:88.84ms
step:1368/1680 train_time:121539ms step_avg:88.84ms
step:1369/1680 train_time:121629ms step_avg:88.85ms
step:1370/1680 train_time:121720ms step_avg:88.85ms
step:1371/1680 train_time:121810ms step_avg:88.85ms
step:1372/1680 train_time:121900ms step_avg:88.85ms
step:1373/1680 train_time:121990ms step_avg:88.85ms
step:1374/1680 train_time:122080ms step_avg:88.85ms
step:1375/1680 train_time:122169ms step_avg:88.85ms
step:1375/1680 val_loss:3.3439 train_time:122260ms step_avg:88.92ms
step:1376/1680 train_time:122283ms step_avg:88.87ms
step:1377/1680 train_time:122354ms step_avg:88.86ms
step:1378/1680 train_time:122448ms step_avg:88.86ms
step:1379/1680 train_time:122538ms step_avg:88.86ms
step:1380/1680 train_time:122626ms step_avg:88.86ms
step:1381/1680 train_time:122716ms step_avg:88.86ms
step:1382/1680 train_time:122804ms step_avg:88.86ms
step:1383/1680 train_time:122893ms step_avg:88.86ms
step:1384/1680 train_time:122982ms step_avg:88.86ms
step:1385/1680 train_time:123072ms step_avg:88.86ms
step:1386/1680 train_time:123161ms step_avg:88.86ms
step:1387/1680 train_time:123252ms step_avg:88.86ms
step:1388/1680 train_time:123343ms step_avg:88.86ms
step:1389/1680 train_time:123434ms step_avg:88.87ms
step:1390/1680 train_time:123525ms step_avg:88.87ms
step:1391/1680 train_time:123614ms step_avg:88.87ms
step:1392/1680 train_time:123704ms step_avg:88.87ms
step:1393/1680 train_time:123794ms step_avg:88.87ms
step:1394/1680 train_time:123883ms step_avg:88.87ms
step:1395/1680 train_time:123973ms step_avg:88.87ms
step:1396/1680 train_time:124062ms step_avg:88.87ms
step:1397/1680 train_time:124152ms step_avg:88.87ms
step:1398/1680 train_time:124242ms step_avg:88.87ms
step:1399/1680 train_time:124333ms step_avg:88.87ms
step:1400/1680 train_time:124423ms step_avg:88.87ms
step:1401/1680 train_time:124513ms step_avg:88.87ms
step:1402/1680 train_time:124603ms step_avg:88.88ms
step:1403/1680 train_time:124693ms step_avg:88.88ms
step:1404/1680 train_time:124783ms step_avg:88.88ms
step:1405/1680 train_time:124872ms step_avg:88.88ms
step:1406/1680 train_time:124961ms step_avg:88.88ms
step:1407/1680 train_time:125051ms step_avg:88.88ms
step:1408/1680 train_time:125140ms step_avg:88.88ms
step:1409/1680 train_time:125230ms step_avg:88.88ms
step:1410/1680 train_time:125319ms step_avg:88.88ms
step:1411/1680 train_time:125410ms step_avg:88.88ms
step:1412/1680 train_time:125499ms step_avg:88.88ms
step:1413/1680 train_time:125589ms step_avg:88.88ms
step:1414/1680 train_time:125679ms step_avg:88.88ms
step:1415/1680 train_time:125769ms step_avg:88.88ms
step:1416/1680 train_time:125858ms step_avg:88.88ms
step:1417/1680 train_time:125947ms step_avg:88.88ms
step:1418/1680 train_time:126037ms step_avg:88.88ms
step:1419/1680 train_time:126126ms step_avg:88.88ms
step:1420/1680 train_time:126216ms step_avg:88.88ms
step:1421/1680 train_time:126305ms step_avg:88.88ms
step:1422/1680 train_time:126396ms step_avg:88.89ms
step:1423/1680 train_time:126486ms step_avg:88.89ms
step:1424/1680 train_time:126576ms step_avg:88.89ms
step:1425/1680 train_time:126668ms step_avg:88.89ms
step:1426/1680 train_time:126757ms step_avg:88.89ms
step:1427/1680 train_time:126847ms step_avg:88.89ms
step:1428/1680 train_time:126937ms step_avg:88.89ms
step:1429/1680 train_time:127027ms step_avg:88.89ms
step:1430/1680 train_time:127117ms step_avg:88.89ms
step:1431/1680 train_time:127207ms step_avg:88.89ms
step:1432/1680 train_time:127296ms step_avg:88.89ms
step:1433/1680 train_time:127387ms step_avg:88.90ms
step:1434/1680 train_time:127477ms step_avg:88.90ms
step:1435/1680 train_time:127567ms step_avg:88.90ms
step:1436/1680 train_time:127657ms step_avg:88.90ms
step:1437/1680 train_time:127747ms step_avg:88.90ms
step:1438/1680 train_time:127837ms step_avg:88.90ms
step:1439/1680 train_time:127927ms step_avg:88.90ms
step:1440/1680 train_time:128016ms step_avg:88.90ms
step:1441/1680 train_time:128106ms step_avg:88.90ms
step:1442/1680 train_time:128195ms step_avg:88.90ms
step:1443/1680 train_time:128284ms step_avg:88.90ms
step:1444/1680 train_time:128374ms step_avg:88.90ms
step:1445/1680 train_time:128464ms step_avg:88.90ms
step:1446/1680 train_time:128554ms step_avg:88.90ms
step:1447/1680 train_time:128644ms step_avg:88.90ms
step:1448/1680 train_time:128733ms step_avg:88.90ms
step:1449/1680 train_time:128824ms step_avg:88.91ms
step:1450/1680 train_time:128914ms step_avg:88.91ms
step:1451/1680 train_time:129003ms step_avg:88.91ms
step:1452/1680 train_time:129093ms step_avg:88.91ms
step:1453/1680 train_time:129183ms step_avg:88.91ms
step:1454/1680 train_time:129272ms step_avg:88.91ms
step:1455/1680 train_time:129362ms step_avg:88.91ms
step:1456/1680 train_time:129452ms step_avg:88.91ms
step:1457/1680 train_time:129541ms step_avg:88.91ms
step:1458/1680 train_time:129631ms step_avg:88.91ms
step:1459/1680 train_time:129720ms step_avg:88.91ms
step:1460/1680 train_time:129810ms step_avg:88.91ms
step:1461/1680 train_time:129900ms step_avg:88.91ms
step:1462/1680 train_time:129990ms step_avg:88.91ms
step:1463/1680 train_time:130079ms step_avg:88.91ms
step:1464/1680 train_time:130169ms step_avg:88.91ms
step:1465/1680 train_time:130259ms step_avg:88.91ms
step:1466/1680 train_time:130349ms step_avg:88.91ms
step:1467/1680 train_time:130438ms step_avg:88.91ms
step:1468/1680 train_time:130529ms step_avg:88.92ms
step:1469/1680 train_time:130618ms step_avg:88.92ms
step:1470/1680 train_time:130710ms step_avg:88.92ms
step:1471/1680 train_time:130798ms step_avg:88.92ms
step:1472/1680 train_time:130887ms step_avg:88.92ms
step:1473/1680 train_time:130978ms step_avg:88.92ms
step:1474/1680 train_time:131067ms step_avg:88.92ms
step:1475/1680 train_time:131157ms step_avg:88.92ms
step:1476/1680 train_time:131247ms step_avg:88.92ms
step:1477/1680 train_time:131337ms step_avg:88.92ms
step:1478/1680 train_time:131426ms step_avg:88.92ms
step:1479/1680 train_time:131516ms step_avg:88.92ms
step:1480/1680 train_time:131607ms step_avg:88.92ms
step:1481/1680 train_time:131696ms step_avg:88.92ms
step:1482/1680 train_time:131786ms step_avg:88.92ms
step:1483/1680 train_time:131876ms step_avg:88.92ms
step:1484/1680 train_time:131965ms step_avg:88.93ms
step:1485/1680 train_time:132055ms step_avg:88.93ms
step:1486/1680 train_time:132144ms step_avg:88.93ms
step:1487/1680 train_time:132234ms step_avg:88.93ms
step:1488/1680 train_time:132323ms step_avg:88.93ms
step:1489/1680 train_time:132413ms step_avg:88.93ms
step:1490/1680 train_time:132503ms step_avg:88.93ms
step:1491/1680 train_time:132594ms step_avg:88.93ms
step:1492/1680 train_time:132684ms step_avg:88.93ms
step:1493/1680 train_time:132774ms step_avg:88.93ms
step:1494/1680 train_time:132864ms step_avg:88.93ms
step:1495/1680 train_time:132953ms step_avg:88.93ms
step:1496/1680 train_time:133043ms step_avg:88.93ms
step:1497/1680 train_time:133133ms step_avg:88.93ms
step:1498/1680 train_time:133222ms step_avg:88.93ms
step:1499/1680 train_time:133312ms step_avg:88.93ms
step:1500/1680 train_time:133401ms step_avg:88.93ms
step:1500/1680 val_loss:3.3141 train_time:133492ms step_avg:88.99ms
step:1501/1680 train_time:133515ms step_avg:88.95ms
step:1502/1680 train_time:133584ms step_avg:88.94ms
step:1503/1680 train_time:133678ms step_avg:88.94ms
step:1504/1680 train_time:133769ms step_avg:88.94ms
step:1505/1680 train_time:133858ms step_avg:88.94ms
step:1506/1680 train_time:133946ms step_avg:88.94ms
step:1507/1680 train_time:134035ms step_avg:88.94ms
step:1508/1680 train_time:134125ms step_avg:88.94ms
step:1509/1680 train_time:134214ms step_avg:88.94ms
step:1510/1680 train_time:134304ms step_avg:88.94ms
step:1511/1680 train_time:134394ms step_avg:88.94ms
step:1512/1680 train_time:134484ms step_avg:88.94ms
step:1513/1680 train_time:134576ms step_avg:88.95ms
step:1514/1680 train_time:134667ms step_avg:88.95ms
step:1515/1680 train_time:134758ms step_avg:88.95ms
step:1516/1680 train_time:134847ms step_avg:88.95ms
step:1517/1680 train_time:134937ms step_avg:88.95ms
step:1518/1680 train_time:135026ms step_avg:88.95ms
step:1519/1680 train_time:135115ms step_avg:88.95ms
step:1520/1680 train_time:135204ms step_avg:88.95ms
step:1521/1680 train_time:135294ms step_avg:88.95ms
step:1522/1680 train_time:135383ms step_avg:88.95ms
step:1523/1680 train_time:135473ms step_avg:88.95ms
step:1524/1680 train_time:135563ms step_avg:88.95ms
step:1525/1680 train_time:135654ms step_avg:88.95ms
step:1526/1680 train_time:135746ms step_avg:88.96ms
step:1527/1680 train_time:135835ms step_avg:88.96ms
step:1528/1680 train_time:135925ms step_avg:88.96ms
step:1529/1680 train_time:136014ms step_avg:88.96ms
step:1530/1680 train_time:136103ms step_avg:88.96ms
step:1531/1680 train_time:136192ms step_avg:88.96ms
step:1532/1680 train_time:136282ms step_avg:88.96ms
step:1533/1680 train_time:136371ms step_avg:88.96ms
step:1534/1680 train_time:136460ms step_avg:88.96ms
step:1535/1680 train_time:136550ms step_avg:88.96ms
step:1536/1680 train_time:136643ms step_avg:88.96ms
step:1537/1680 train_time:136732ms step_avg:88.96ms
step:1538/1680 train_time:136823ms step_avg:88.96ms
step:1539/1680 train_time:136913ms step_avg:88.96ms
step:1540/1680 train_time:137003ms step_avg:88.96ms
step:1541/1680 train_time:137092ms step_avg:88.96ms
step:1542/1680 train_time:137182ms step_avg:88.96ms
step:1543/1680 train_time:137271ms step_avg:88.96ms
step:1544/1680 train_time:137361ms step_avg:88.96ms
step:1545/1680 train_time:137450ms step_avg:88.96ms
step:1546/1680 train_time:137540ms step_avg:88.97ms
step:1547/1680 train_time:137630ms step_avg:88.97ms
step:1548/1680 train_time:137721ms step_avg:88.97ms
step:1549/1680 train_time:137810ms step_avg:88.97ms
step:1550/1680 train_time:137900ms step_avg:88.97ms
step:1551/1680 train_time:137990ms step_avg:88.97ms
step:1552/1680 train_time:138079ms step_avg:88.97ms
step:1553/1680 train_time:138168ms step_avg:88.97ms
step:1554/1680 train_time:138256ms step_avg:88.97ms
step:1555/1680 train_time:138346ms step_avg:88.97ms
step:1556/1680 train_time:138435ms step_avg:88.97ms
step:1557/1680 train_time:138525ms step_avg:88.97ms
step:1558/1680 train_time:138616ms step_avg:88.97ms
step:1559/1680 train_time:138706ms step_avg:88.97ms
step:1560/1680 train_time:138796ms step_avg:88.97ms
step:1561/1680 train_time:138886ms step_avg:88.97ms
step:1562/1680 train_time:138975ms step_avg:88.97ms
step:1563/1680 train_time:139065ms step_avg:88.97ms
step:1564/1680 train_time:139155ms step_avg:88.97ms
step:1565/1680 train_time:139244ms step_avg:88.97ms
step:1566/1680 train_time:139334ms step_avg:88.97ms
step:1567/1680 train_time:139424ms step_avg:88.98ms
step:1568/1680 train_time:139514ms step_avg:88.98ms
step:1569/1680 train_time:139604ms step_avg:88.98ms
step:1570/1680 train_time:139693ms step_avg:88.98ms
step:1571/1680 train_time:139783ms step_avg:88.98ms
step:1572/1680 train_time:139873ms step_avg:88.98ms
step:1573/1680 train_time:139964ms step_avg:88.98ms
step:1574/1680 train_time:140054ms step_avg:88.98ms
step:1575/1680 train_time:140144ms step_avg:88.98ms
step:1576/1680 train_time:140234ms step_avg:88.98ms
step:1577/1680 train_time:140323ms step_avg:88.98ms
step:1578/1680 train_time:140413ms step_avg:88.98ms
step:1579/1680 train_time:140503ms step_avg:88.98ms
step:1580/1680 train_time:140593ms step_avg:88.98ms
step:1581/1680 train_time:140683ms step_avg:88.98ms
step:1582/1680 train_time:140772ms step_avg:88.98ms
step:1583/1680 train_time:140862ms step_avg:88.98ms
step:1584/1680 train_time:140952ms step_avg:88.98ms
step:1585/1680 train_time:141043ms step_avg:88.99ms
step:1586/1680 train_time:141132ms step_avg:88.99ms
step:1587/1680 train_time:141222ms step_avg:88.99ms
step:1588/1680 train_time:141311ms step_avg:88.99ms
step:1589/1680 train_time:141401ms step_avg:88.99ms
step:1590/1680 train_time:141490ms step_avg:88.99ms
step:1591/1680 train_time:141581ms step_avg:88.99ms
step:1592/1680 train_time:141670ms step_avg:88.99ms
step:1593/1680 train_time:141760ms step_avg:88.99ms
step:1594/1680 train_time:141850ms step_avg:88.99ms
step:1595/1680 train_time:141939ms step_avg:88.99ms
step:1596/1680 train_time:142029ms step_avg:88.99ms
step:1597/1680 train_time:142119ms step_avg:88.99ms
step:1598/1680 train_time:142208ms step_avg:88.99ms
step:1599/1680 train_time:142298ms step_avg:88.99ms
step:1600/1680 train_time:142388ms step_avg:88.99ms
step:1601/1680 train_time:142478ms step_avg:88.99ms
step:1602/1680 train_time:142568ms step_avg:88.99ms
step:1603/1680 train_time:142658ms step_avg:88.99ms
step:1604/1680 train_time:142748ms step_avg:89.00ms
step:1605/1680 train_time:142839ms step_avg:89.00ms
step:1606/1680 train_time:142929ms step_avg:89.00ms
step:1607/1680 train_time:143019ms step_avg:89.00ms
step:1608/1680 train_time:143108ms step_avg:89.00ms
step:1609/1680 train_time:143197ms step_avg:89.00ms
step:1610/1680 train_time:143287ms step_avg:89.00ms
step:1611/1680 train_time:143377ms step_avg:89.00ms
step:1612/1680 train_time:143467ms step_avg:89.00ms
step:1613/1680 train_time:143558ms step_avg:89.00ms
step:1614/1680 train_time:143647ms step_avg:89.00ms
step:1615/1680 train_time:143737ms step_avg:89.00ms
step:1616/1680 train_time:143827ms step_avg:89.00ms
step:1617/1680 train_time:143917ms step_avg:89.00ms
step:1618/1680 train_time:144006ms step_avg:89.00ms
step:1619/1680 train_time:144097ms step_avg:89.00ms
step:1620/1680 train_time:144186ms step_avg:89.00ms
step:1621/1680 train_time:144276ms step_avg:89.00ms
step:1622/1680 train_time:144366ms step_avg:89.00ms
step:1623/1680 train_time:144456ms step_avg:89.01ms
step:1624/1680 train_time:144546ms step_avg:89.01ms
step:1625/1680 train_time:144636ms step_avg:89.01ms
step:1625/1680 val_loss:3.2903 train_time:144727ms step_avg:89.06ms
step:1626/1680 train_time:144751ms step_avg:89.02ms
step:1627/1680 train_time:144821ms step_avg:89.01ms
step:1628/1680 train_time:144917ms step_avg:89.02ms
step:1629/1680 train_time:145007ms step_avg:89.02ms
step:1630/1680 train_time:145097ms step_avg:89.02ms
step:1631/1680 train_time:145186ms step_avg:89.02ms
step:1632/1680 train_time:145276ms step_avg:89.02ms
step:1633/1680 train_time:145364ms step_avg:89.02ms
step:1634/1680 train_time:145453ms step_avg:89.02ms
step:1635/1680 train_time:145542ms step_avg:89.02ms
step:1636/1680 train_time:145631ms step_avg:89.02ms
step:1637/1680 train_time:145721ms step_avg:89.02ms
step:1638/1680 train_time:145813ms step_avg:89.02ms
step:1639/1680 train_time:145906ms step_avg:89.02ms
step:1640/1680 train_time:145998ms step_avg:89.02ms
step:1641/1680 train_time:146087ms step_avg:89.02ms
step:1642/1680 train_time:146177ms step_avg:89.02ms
step:1643/1680 train_time:146266ms step_avg:89.02ms
step:1644/1680 train_time:146356ms step_avg:89.02ms
step:1645/1680 train_time:146445ms step_avg:89.02ms
step:1646/1680 train_time:146533ms step_avg:89.02ms
step:1647/1680 train_time:146622ms step_avg:89.02ms
step:1648/1680 train_time:146713ms step_avg:89.02ms
step:1649/1680 train_time:146804ms step_avg:89.03ms
step:1650/1680 train_time:146894ms step_avg:89.03ms
step:1651/1680 train_time:146984ms step_avg:89.03ms
step:1652/1680 train_time:147075ms step_avg:89.03ms
step:1653/1680 train_time:147163ms step_avg:89.03ms
step:1654/1680 train_time:147253ms step_avg:89.03ms
step:1655/1680 train_time:147342ms step_avg:89.03ms
step:1656/1680 train_time:147432ms step_avg:89.03ms
step:1657/1680 train_time:147521ms step_avg:89.03ms
step:1658/1680 train_time:147610ms step_avg:89.03ms
step:1659/1680 train_time:147701ms step_avg:89.03ms
step:1660/1680 train_time:147791ms step_avg:89.03ms
step:1661/1680 train_time:147882ms step_avg:89.03ms
step:1662/1680 train_time:147972ms step_avg:89.03ms
step:1663/1680 train_time:148061ms step_avg:89.03ms
step:1664/1680 train_time:148151ms step_avg:89.03ms
step:1665/1680 train_time:148240ms step_avg:89.03ms
step:1666/1680 train_time:148330ms step_avg:89.03ms
step:1667/1680 train_time:148419ms step_avg:89.03ms
step:1668/1680 train_time:148508ms step_avg:89.03ms
step:1669/1680 train_time:148597ms step_avg:89.03ms
step:1670/1680 train_time:148687ms step_avg:89.03ms
step:1671/1680 train_time:148778ms step_avg:89.04ms
step:1672/1680 train_time:148868ms step_avg:89.04ms
step:1673/1680 train_time:148959ms step_avg:89.04ms
step:1674/1680 train_time:149049ms step_avg:89.04ms
step:1675/1680 train_time:149139ms step_avg:89.04ms
step:1676/1680 train_time:149228ms step_avg:89.04ms
step:1677/1680 train_time:149319ms step_avg:89.04ms
step:1678/1680 train_time:149408ms step_avg:89.04ms
step:1679/1680 train_time:149497ms step_avg:89.04ms
step:1680/1680 train_time:149587ms step_avg:89.04ms
step:1680/1680 val_loss:3.2792 train_time:149678ms step_avg:89.09ms
peak memory allocated: 30760 MiB reserved: 46054 MiB

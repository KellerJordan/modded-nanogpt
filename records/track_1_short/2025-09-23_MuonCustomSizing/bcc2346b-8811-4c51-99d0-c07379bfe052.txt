import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from flash_attn_interface import flash_attn_varlen_func
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ns_line_1_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ns_line_1(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ns_line_1_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ns_line_2_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from ns_line_1_kernel, but also loads and adds a block of A
    # Performance is slightly slower than ns_line_1_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ns_line_2(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ns_line_2_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def newton_schulz_triton(G: torch.Tensor):
    a, b, c = (3.4445, -4.7750, 2.0315)
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) + 1e-7)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    ns_line_3 = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the NS iterations
    for _ in range(5):
        ns_line_1(X, out=A)  # A = X @ X.mT
        ns_line_2(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        ns_line_3(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    Though empirically small 1D params perform efficiently here:
        NS approximately performs a magnitude normalization of the grad
        This hyper-optimized class has faster execution time than the current impl of Adam for small params

    Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as 
    needed on the forward pass. This enables attn and mlp weights to be contained within the same 
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable 
    (n_attn_layers+n_mlp_layers*2)%4==0 for batching across 8 GPUs with zero padding. The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute NS of 1 and schedule all gather
        6. wait on step 2, then compute NS of 2 and schedule all gather
        7. wait on step 3, then compute NS of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before NS
        8. wait on 4, then compute NS of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)
        if custom_sizing:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def generate_standard_param_groups(self, params):
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
    
    def generate_custom_param_groups(self, params):
        # implementation requires that a single GPU does not recieve both attn 
        # and mlp params when a param group is split across GPUs
        module_ranks = {
            'smear_gate': 1, # 1 param
            'attn_gate': 2, # 10 params
            'attn': 3, # 10 params
            'mlp': 4, # 22 params
        }
        params = list(params)
        params.sort(key=lambda x: module_ranks.get(x.module))
        idx = 0
        group_sizes = [1,10,16,16]
        assert len(params)==sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            group_params = params[idx:idx+size]
            param_groups.append(dict(params=group_params))
            idx += size
        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            num_params = len(params)
            padded_num_params = (
                (num_params + world_size - 1) // world_size * world_size
            )

            grads_to_stack = [p.grad for p in params]
            if padded_num_params > num_params:
                padding_grad = torch.zeros_like(params[0].grad)
                grads_to_stack.extend(
                    [padding_grad] * (padded_num_params - num_params)
                )

            stacked_grads = torch.stack(grads_to_stack)

            chunk_size = padded_num_params // world_size
            grad_chunk = torch.empty(
                (chunk_size, *params[0].grad.shape),
                dtype=stacked_grads.dtype,
                device=stacked_grads.device,
            )

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(
                {
                    "params": params,
                    "grad_chunk": grad_chunk,
                    "reduce_future": reduce_future,
                    "chunk_size": chunk_size,
                    "padded_num_params": padded_num_params,
                }
            )

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = info["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = info["chunk_size"]
            start_idx = rank * chunk_size

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            # This helps in vectorizing operations later.
            p_example = params[0]  # All params in a group have the same shape.
            eff_lr_val = (
                group["lr"]
                * max(1, p_example.size(-2) / p_example.size(-1)) ** 0.5
                * getattr(p_example, "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(p_example, "wd_mul", 1.0)
            )

            # Prepare a contiguous buffer for the updated parameters for this rank's chunk.
            # This buffer will serve as the input_tensor for dist.all_gather_into_tensor.
            updated_param_chunk = torch.empty(
                (chunk_size, *p_example.shape),
                dtype=p_example.dtype,
                device=p_example.device,
            )

            # List to collect update_grad tensors for batched zeropower computation.
            update_grads_for_zeropower = []

            # Process each parameter in this rank's chunk.
            for i in range(chunk_size):
                param_idx = start_idx + i

                if param_idx >= len(params):
                    # For padding: Fill the corresponding part of the updated_param_chunk with zeros.
                    # These padded entries will not be used by other ranks in the all_gather, but
                    # initializing them prevents uninitialized memory access issues.
                    updated_param_chunk[i].zero_()
                    # Also append a zero tensor for zeropower input if it must be padded.
                    update_grads_for_zeropower.append(
                        torch.zeros_like(p_example.grad)
                    )
                    continue
                p = params[param_idx]
                grad = grad_chunk[
                    i
                ]  # This gradient corresponds to the current parameter p.
                state = self.state[p]

                # Initialize momentum buffer if not present
                if not state:
                    state["momentum_buffer"] = torch.zeros_like(grad)

                momentum_buffer = state["momentum_buffer"]

                # Apply momentum update directly to the persistent momentum buffer in-place.
                momentum_buffer.lerp_(grad, 1 - group["momentum"])

                # Compute the actual `update_grad` for zeropower. This creates a new tensor.
                update_grad = grad.lerp(momentum_buffer, group["momentum"])
                update_grads_for_zeropower.append(update_grad)

                # Copy the current parameter value into the temporary buffer.
                updated_param_chunk[i].copy_(p)

                # Apply weight decay directly to the buffer.
                updated_param_chunk[i].mul_(1 - eff_weight_decay_val)

            # Stack the individual `update_grad` tensors for efficient batched zeropower computation.
            batched_update_grads = torch.stack(update_grads_for_zeropower)

            # Compute zeropower for the entire chunk in a single, batched call.
            original_shape = batched_update_grads.shape
            # Reshape attn params from [hdim, dim*4] to [4,hdim,dim] to apply NS indepedently to Q,K,V,O
            module_idx = start_idx if start_idx<len(params) else 0
            if getattr(params[module_idx],'module','none')=='attn':
                batch = 4 * original_shape[0]
                d1 = original_shape[1] 
                d2 = original_shape[2] // 4
                batched = batched_update_grads.view(batch, d1, d2)
                v_chunk = newton_schulz_triton(batched)
                v_chunk = v_chunk.view(original_shape)
            else:
                v_chunk = newton_schulz_triton(batched_update_grads)

            # Add the computed zeropower update to the parameters in the buffer.
            # This loop applies the zeropower output (v_chunk) to the `updated_param_chunk` buffer.
            for i in range(chunk_size):
                param_idx = start_idx + i
                if param_idx >= len(params):  # Skip padded entries again.
                    continue

                # Add the computed zeropower update to the parameter in the buffer.
                updated_param_chunk[i].add_(v_chunk[i], alpha=-eff_lr_val)

            stacked_params = torch.empty(
                (info["padded_num_params"], *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )
            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_param_chunk, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_gather_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for base_i in range(len(params)):
                grad = params[base_i].grad
                rank_size = grad.shape[0] // world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for base in range(len(params)):
                reduce_scatter_futures[idx].wait()
                p = params[base]
                rank_size = p.shape[0] // world_size
                p_slice = p[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(p, "lr_mul", 1.0)
                state = self.state[p]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state["step"] = torch.tensor(
                        0, dtype=torch.int64, device=p.device
                    )
                    state["exp_avg"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                    state["exp_avg_sq"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(p, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_gather_futures.append(dist.all_gather_into_tensor(p, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()
        
    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.module='attn'
        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.module = 'attn_gate'
        self.attn_gate.weight.detach().zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4,self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens, max_seqlen_q=max_len, max_seqlen_k=max_len,
                                   causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4,self.hdim, self.dim)[3].type_as(y))
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.module='mlp'
        self.c_proj.module='mlp'
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.detach().zero_()
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.module = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 6) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(num_layers), #extra zeros params for smear_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        # smear token embed forward 1 position @classiclarryd
        smear_lambda = self.scalars[5 * len(self.blocks)]
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)

        n = len(self.blocks) // 2

        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            if i >= n and i<11:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)

        x = norm(x)
        logits = self.lm_head(x).float()
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), target_seq, reduction="sum" if self.training else "mean")
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()
    
    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 24 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_iterations: int = 1640 # number of iterations to run
    iteration_extension = 40 # number of iterations to continue training at final cooldown and window size
    cooldown_frac: int = 0.5 # fraction of training spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 125 # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_validate: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_long_validate: int = 20 # extend long windows out even further

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.8, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizer2 = Muon(hidden_matrix_params + gate_params, lr=0.05, momentum=0.95, weight_decay=0.0)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: stable then decay
def get_lr(step: int):
    x = min(0.9999,step / args.num_iterations)
    assert 0 <= x < 1
    lr = 1.0
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = w * 1.0 + (1 - w) * 0.1
    return lr

def get_ws(step: int):
    if step == args.num_iterations+args.iteration_extension:
        return args.ws_validate//2, args.ws_validate
    x = min(step / (1 + args.num_iterations),0.9999)
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx]//2, args.ws_schedule[ws_idx]

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
ws_long = args.ws_schedule[0]
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    new_ws_long = args.ws_schedule[step % len(args.ws_schedule)]  # each window size is a new graph, need to warm up each with YaRN params
    if new_ws_long > ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long = new_ws_long
    elif new_ws_long<ws_long:
        model.yarn.reset()
        ws_long = new_ws_long
    model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset()
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations + args.iteration_extension
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_long_validate
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for _ in range(grad_accum_steps):
        inputs, targets, cum_seqlens = next(train_loader)
        model(inputs, targets, cum_seqlens, ws_short, ws_long).backward()
    # set optimization hyperparameters
    for opt in optimizers:
        for group in opt.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)
    for group in optimizer2.param_groups:
        frac = min(step / 300, 1) # momentum warmup for muon
        group["momentum"] = (1 - frac) * 0.85 + frac * 0.95
    # step the optimizers
    for opt in optimizers:
        opt.step()
    # null the gradients
    model.zero_grad(set_to_none=True)
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.10.12 (main, May 27 2025, 17:12:29) [GCC 11.4.0]
Running PyTorch 2.9.0.dev20250726+cu126 compiled for CUDA 12.6
Running Triton version 3.4.0
Wed Sep 24 06:10:55 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.148.08             Driver Version: 570.148.08     CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:61:00.0 Off |                    0 |
| N/A   27C    P0            117W /  700W |    5858MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:62:00.0 Off |                    0 |
| N/A   27C    P0            113W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:63:00.0 Off |                    0 |
| N/A   29C    P0            115W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:64:00.0 Off |                    0 |
| N/A   26C    P0            115W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:6A:00.0 Off |                    0 |
| N/A   27C    P0            118W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:6B:00.0 Off |                    0 |
| N/A   30C    P0            118W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:6C:00.0 Off |                    0 |
| N/A   29C    P0            117W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:6D:00.0 Off |                    0 |
| N/A   26C    P0            117W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A          114913      C   /usr/bin/python3                       1510MiB |
|    0   N/A  N/A          114914      C   /usr/bin/python3                        614MiB |
|    0   N/A  N/A          114915      C   /usr/bin/python3                        614MiB |
|    0   N/A  N/A          114916      C   /usr/bin/python3                        614MiB |
|    0   N/A  N/A          114917      C   /usr/bin/python3                        614MiB |
|    0   N/A  N/A          114918      C   /usr/bin/python3                        614MiB |
|    0   N/A  N/A          114919      C   /usr/bin/python3                        614MiB |
|    0   N/A  N/A          114920      C   /usr/bin/python3                        614MiB |
|    1   N/A  N/A          114914      C   /usr/bin/python3                       1510MiB |
|    2   N/A  N/A          114915      C   /usr/bin/python3                       1510MiB |
|    3   N/A  N/A          114916      C   /usr/bin/python3                       1510MiB |
|    4   N/A  N/A          114917      C   /usr/bin/python3                       1510MiB |
|    5   N/A  N/A          114918      C   /usr/bin/python3                       1510MiB |
|    6   N/A  N/A          114919      C   /usr/bin/python3                       1510MiB |
|    7   N/A  N/A          114920      C   /usr/bin/python3                       1510MiB |
+-----------------------------------------------------------------------------------------+

====================================================================================================
step:0/1680 val_loss:10.8258 train_time:0ms step_avg:0.03ms
step:1/1680 train_time:154ms step_avg:153.97ms
step:2/1680 train_time:179ms step_avg:89.29ms
step:3/1680 train_time:239ms step_avg:79.51ms
step:4/1680 train_time:324ms step_avg:81.08ms
step:5/1680 train_time:412ms step_avg:82.44ms
step:6/1680 train_time:500ms step_avg:83.31ms
step:7/1680 train_time:587ms step_avg:83.86ms
step:8/1680 train_time:674ms step_avg:84.31ms
step:9/1680 train_time:761ms step_avg:84.58ms
step:10/1680 train_time:849ms step_avg:84.86ms
step:11/1680 train_time:936ms step_avg:85.05ms
step:12/1680 train_time:1023ms step_avg:85.26ms
step:13/1680 train_time:1113ms step_avg:85.64ms
step:14/1680 train_time:1204ms step_avg:85.99ms
step:15/1680 train_time:1293ms step_avg:86.20ms
step:16/1680 train_time:1381ms step_avg:86.33ms
step:17/1680 train_time:1470ms step_avg:86.46ms
step:18/1680 train_time:1557ms step_avg:86.52ms
step:19/1680 train_time:1645ms step_avg:86.59ms
step:20/1680 train_time:1733ms step_avg:86.64ms
step:21/1680 train_time:1820ms step_avg:86.66ms
step:22/1680 train_time:1907ms step_avg:86.69ms
step:23/1680 train_time:1995ms step_avg:86.76ms
step:24/1680 train_time:2084ms step_avg:86.84ms
step:25/1680 train_time:2174ms step_avg:86.95ms
step:26/1680 train_time:2263ms step_avg:87.03ms
step:27/1680 train_time:2351ms step_avg:87.08ms
step:28/1680 train_time:2440ms step_avg:87.15ms
step:29/1680 train_time:2529ms step_avg:87.20ms
step:30/1680 train_time:2618ms step_avg:87.25ms
step:31/1680 train_time:2705ms step_avg:87.25ms
step:32/1680 train_time:2793ms step_avg:87.29ms
step:33/1680 train_time:2882ms step_avg:87.32ms
step:34/1680 train_time:2969ms step_avg:87.34ms
step:35/1680 train_time:3058ms step_avg:87.37ms
step:36/1680 train_time:3146ms step_avg:87.39ms
step:37/1680 train_time:3235ms step_avg:87.43ms
step:38/1680 train_time:3322ms step_avg:87.43ms
step:39/1680 train_time:3410ms step_avg:87.45ms
step:40/1680 train_time:3499ms step_avg:87.47ms
step:41/1680 train_time:3587ms step_avg:87.49ms
step:42/1680 train_time:3675ms step_avg:87.50ms
step:43/1680 train_time:3763ms step_avg:87.51ms
step:44/1680 train_time:3852ms step_avg:87.54ms
step:45/1680 train_time:3940ms step_avg:87.55ms
step:46/1680 train_time:4028ms step_avg:87.56ms
step:47/1680 train_time:4116ms step_avg:87.57ms
step:48/1680 train_time:4204ms step_avg:87.57ms
step:49/1680 train_time:4292ms step_avg:87.60ms
step:50/1680 train_time:4382ms step_avg:87.63ms
step:51/1680 train_time:4470ms step_avg:87.64ms
step:52/1680 train_time:4558ms step_avg:87.66ms
step:53/1680 train_time:4646ms step_avg:87.67ms
step:54/1680 train_time:4734ms step_avg:87.67ms
step:55/1680 train_time:4822ms step_avg:87.67ms
step:56/1680 train_time:4910ms step_avg:87.69ms
step:57/1680 train_time:4999ms step_avg:87.70ms
step:58/1680 train_time:5087ms step_avg:87.70ms
step:59/1680 train_time:5175ms step_avg:87.71ms
step:60/1680 train_time:5262ms step_avg:87.71ms
step:61/1680 train_time:5351ms step_avg:87.72ms
step:62/1680 train_time:5439ms step_avg:87.73ms
step:63/1680 train_time:5527ms step_avg:87.73ms
step:64/1680 train_time:5616ms step_avg:87.76ms
step:65/1680 train_time:5704ms step_avg:87.76ms
step:66/1680 train_time:5793ms step_avg:87.77ms
step:67/1680 train_time:5880ms step_avg:87.77ms
step:68/1680 train_time:5968ms step_avg:87.77ms
step:69/1680 train_time:6057ms step_avg:87.78ms
step:70/1680 train_time:6144ms step_avg:87.78ms
step:71/1680 train_time:6233ms step_avg:87.79ms
step:72/1680 train_time:6321ms step_avg:87.80ms
step:73/1680 train_time:6409ms step_avg:87.80ms
step:74/1680 train_time:6497ms step_avg:87.80ms
step:75/1680 train_time:6585ms step_avg:87.80ms
step:76/1680 train_time:6673ms step_avg:87.80ms
step:77/1680 train_time:6762ms step_avg:87.81ms
step:78/1680 train_time:6850ms step_avg:87.82ms
step:79/1680 train_time:6938ms step_avg:87.83ms
step:80/1680 train_time:7026ms step_avg:87.82ms
step:81/1680 train_time:7114ms step_avg:87.82ms
step:82/1680 train_time:7202ms step_avg:87.83ms
step:83/1680 train_time:7290ms step_avg:87.83ms
step:84/1680 train_time:7378ms step_avg:87.84ms
step:85/1680 train_time:7467ms step_avg:87.85ms
step:86/1680 train_time:7555ms step_avg:87.85ms
step:87/1680 train_time:7642ms step_avg:87.84ms
step:88/1680 train_time:7731ms step_avg:87.85ms
step:89/1680 train_time:7819ms step_avg:87.86ms
step:90/1680 train_time:7907ms step_avg:87.86ms
step:91/1680 train_time:7996ms step_avg:87.87ms
step:92/1680 train_time:8084ms step_avg:87.87ms
step:93/1680 train_time:8172ms step_avg:87.87ms
step:94/1680 train_time:8260ms step_avg:87.87ms
step:95/1680 train_time:8347ms step_avg:87.87ms
step:96/1680 train_time:8436ms step_avg:87.87ms
step:97/1680 train_time:8523ms step_avg:87.87ms
step:98/1680 train_time:8611ms step_avg:87.87ms
step:99/1680 train_time:8699ms step_avg:87.87ms
step:100/1680 train_time:8788ms step_avg:87.88ms
step:101/1680 train_time:8876ms step_avg:87.88ms
step:102/1680 train_time:8964ms step_avg:87.88ms
step:103/1680 train_time:9052ms step_avg:87.88ms
step:104/1680 train_time:9139ms step_avg:87.88ms
step:105/1680 train_time:9227ms step_avg:87.87ms
step:106/1680 train_time:9315ms step_avg:87.88ms
step:107/1680 train_time:9403ms step_avg:87.88ms
step:108/1680 train_time:9492ms step_avg:87.88ms
step:109/1680 train_time:9580ms step_avg:87.89ms
step:110/1680 train_time:9668ms step_avg:87.89ms
step:111/1680 train_time:9756ms step_avg:87.89ms
step:112/1680 train_time:9844ms step_avg:87.89ms
step:113/1680 train_time:9932ms step_avg:87.89ms
step:114/1680 train_time:10021ms step_avg:87.91ms
step:115/1680 train_time:10109ms step_avg:87.91ms
step:116/1680 train_time:10197ms step_avg:87.91ms
step:117/1680 train_time:10284ms step_avg:87.90ms
step:118/1680 train_time:10373ms step_avg:87.90ms
step:119/1680 train_time:10461ms step_avg:87.90ms
step:120/1680 train_time:10548ms step_avg:87.90ms
step:121/1680 train_time:10636ms step_avg:87.90ms
step:122/1680 train_time:10724ms step_avg:87.90ms
step:123/1680 train_time:10813ms step_avg:87.91ms
step:124/1680 train_time:10901ms step_avg:87.91ms
step:125/1680 train_time:10989ms step_avg:87.92ms
step:125/1680 val_loss:4.2947 train_time:11078ms step_avg:88.63ms
step:126/1680 train_time:11101ms step_avg:88.10ms
step:127/1680 train_time:11167ms step_avg:87.93ms
step:128/1680 train_time:11264ms step_avg:88.00ms
step:129/1680 train_time:11357ms step_avg:88.04ms
step:130/1680 train_time:11448ms step_avg:88.06ms
step:131/1680 train_time:11536ms step_avg:88.06ms
step:132/1680 train_time:11623ms step_avg:88.06ms
step:133/1680 train_time:11710ms step_avg:88.05ms
step:134/1680 train_time:11797ms step_avg:88.04ms
step:135/1680 train_time:11884ms step_avg:88.03ms
step:136/1680 train_time:11971ms step_avg:88.02ms
step:137/1680 train_time:12058ms step_avg:88.02ms
step:138/1680 train_time:12146ms step_avg:88.01ms
step:139/1680 train_time:12236ms step_avg:88.03ms
step:140/1680 train_time:12327ms step_avg:88.05ms
step:141/1680 train_time:12417ms step_avg:88.06ms
step:142/1680 train_time:12506ms step_avg:88.07ms
step:143/1680 train_time:12593ms step_avg:88.07ms
step:144/1680 train_time:12681ms step_avg:88.06ms
step:145/1680 train_time:12768ms step_avg:88.06ms
step:146/1680 train_time:12856ms step_avg:88.05ms
step:147/1680 train_time:12943ms step_avg:88.05ms
step:148/1680 train_time:13030ms step_avg:88.04ms
step:149/1680 train_time:13118ms step_avg:88.04ms
step:150/1680 train_time:13207ms step_avg:88.04ms
step:151/1680 train_time:13296ms step_avg:88.05ms
step:152/1680 train_time:13385ms step_avg:88.06ms
step:153/1680 train_time:13474ms step_avg:88.06ms
step:154/1680 train_time:13562ms step_avg:88.06ms
step:155/1680 train_time:13650ms step_avg:88.06ms
step:156/1680 train_time:13738ms step_avg:88.06ms
step:157/1680 train_time:13825ms step_avg:88.06ms
step:158/1680 train_time:13913ms step_avg:88.06ms
step:159/1680 train_time:14001ms step_avg:88.05ms
step:160/1680 train_time:14088ms step_avg:88.05ms
step:161/1680 train_time:14176ms step_avg:88.05ms
step:162/1680 train_time:14265ms step_avg:88.05ms
step:163/1680 train_time:14354ms step_avg:88.06ms
step:164/1680 train_time:14443ms step_avg:88.07ms
step:165/1680 train_time:14531ms step_avg:88.07ms
step:166/1680 train_time:14619ms step_avg:88.07ms
step:167/1680 train_time:14707ms step_avg:88.07ms
step:168/1680 train_time:14795ms step_avg:88.07ms
step:169/1680 train_time:14883ms step_avg:88.06ms
step:170/1680 train_time:14970ms step_avg:88.06ms
step:171/1680 train_time:15058ms step_avg:88.06ms
step:172/1680 train_time:15145ms step_avg:88.05ms
step:173/1680 train_time:15234ms step_avg:88.06ms
step:174/1680 train_time:15323ms step_avg:88.06ms
step:175/1680 train_time:15412ms step_avg:88.07ms
step:176/1680 train_time:15500ms step_avg:88.07ms
step:177/1680 train_time:15588ms step_avg:88.07ms
step:178/1680 train_time:15676ms step_avg:88.07ms
step:179/1680 train_time:15764ms step_avg:88.06ms
step:180/1680 train_time:15851ms step_avg:88.06ms
step:181/1680 train_time:15939ms step_avg:88.06ms
step:182/1680 train_time:16027ms step_avg:88.06ms
step:183/1680 train_time:16114ms step_avg:88.06ms
step:184/1680 train_time:16202ms step_avg:88.06ms
step:185/1680 train_time:16290ms step_avg:88.06ms
step:186/1680 train_time:16378ms step_avg:88.06ms
step:187/1680 train_time:16466ms step_avg:88.06ms
step:188/1680 train_time:16555ms step_avg:88.06ms
step:189/1680 train_time:16643ms step_avg:88.06ms
step:190/1680 train_time:16731ms step_avg:88.06ms
step:191/1680 train_time:16818ms step_avg:88.05ms
step:192/1680 train_time:16906ms step_avg:88.05ms
step:193/1680 train_time:16994ms step_avg:88.05ms
step:194/1680 train_time:17081ms step_avg:88.05ms
step:195/1680 train_time:17169ms step_avg:88.05ms
step:196/1680 train_time:17257ms step_avg:88.04ms
step:197/1680 train_time:17345ms step_avg:88.04ms
step:198/1680 train_time:17432ms step_avg:88.04ms
step:199/1680 train_time:17521ms step_avg:88.04ms
step:200/1680 train_time:17610ms step_avg:88.05ms
step:201/1680 train_time:17697ms step_avg:88.05ms
step:202/1680 train_time:17785ms step_avg:88.05ms
step:203/1680 train_time:17873ms step_avg:88.04ms
step:204/1680 train_time:17960ms step_avg:88.04ms
step:205/1680 train_time:18048ms step_avg:88.04ms
step:206/1680 train_time:18137ms step_avg:88.04ms
step:207/1680 train_time:18225ms step_avg:88.04ms
step:208/1680 train_time:18312ms step_avg:88.04ms
step:209/1680 train_time:18401ms step_avg:88.04ms
step:210/1680 train_time:18489ms step_avg:88.04ms
step:211/1680 train_time:18578ms step_avg:88.05ms
step:212/1680 train_time:18666ms step_avg:88.05ms
step:213/1680 train_time:18754ms step_avg:88.05ms
step:214/1680 train_time:18841ms step_avg:88.04ms
step:215/1680 train_time:18929ms step_avg:88.04ms
step:216/1680 train_time:19016ms step_avg:88.04ms
step:217/1680 train_time:19103ms step_avg:88.03ms
step:218/1680 train_time:19192ms step_avg:88.04ms
step:219/1680 train_time:19279ms step_avg:88.03ms
step:220/1680 train_time:19368ms step_avg:88.03ms
step:221/1680 train_time:19456ms step_avg:88.04ms
step:222/1680 train_time:19545ms step_avg:88.04ms
step:223/1680 train_time:19633ms step_avg:88.04ms
step:224/1680 train_time:19720ms step_avg:88.04ms
step:225/1680 train_time:19809ms step_avg:88.04ms
step:226/1680 train_time:19897ms step_avg:88.04ms
step:227/1680 train_time:19985ms step_avg:88.04ms
step:228/1680 train_time:20074ms step_avg:88.04ms
step:229/1680 train_time:20161ms step_avg:88.04ms
step:230/1680 train_time:20249ms step_avg:88.04ms
step:231/1680 train_time:20337ms step_avg:88.04ms
step:232/1680 train_time:20425ms step_avg:88.04ms
step:233/1680 train_time:20513ms step_avg:88.04ms
step:234/1680 train_time:20601ms step_avg:88.04ms
step:235/1680 train_time:20689ms step_avg:88.04ms
step:236/1680 train_time:20777ms step_avg:88.04ms
step:237/1680 train_time:20864ms step_avg:88.04ms
step:238/1680 train_time:20952ms step_avg:88.04ms
step:239/1680 train_time:21040ms step_avg:88.03ms
step:240/1680 train_time:21128ms step_avg:88.03ms
step:241/1680 train_time:21215ms step_avg:88.03ms
step:242/1680 train_time:21303ms step_avg:88.03ms
step:243/1680 train_time:21391ms step_avg:88.03ms
step:244/1680 train_time:21479ms step_avg:88.03ms
step:245/1680 train_time:21567ms step_avg:88.03ms
step:246/1680 train_time:21654ms step_avg:88.03ms
step:247/1680 train_time:21742ms step_avg:88.03ms
step:248/1680 train_time:21830ms step_avg:88.03ms
step:249/1680 train_time:21918ms step_avg:88.03ms
step:250/1680 train_time:22006ms step_avg:88.02ms
step:250/1680 val_loss:3.9605 train_time:22095ms step_avg:88.38ms
step:251/1680 train_time:22117ms step_avg:88.12ms
step:252/1680 train_time:22187ms step_avg:88.04ms
step:253/1680 train_time:22279ms step_avg:88.06ms
step:254/1680 train_time:22368ms step_avg:88.06ms
step:255/1680 train_time:22455ms step_avg:88.06ms
step:256/1680 train_time:22542ms step_avg:88.06ms
step:257/1680 train_time:22629ms step_avg:88.05ms
step:258/1680 train_time:22717ms step_avg:88.05ms
step:259/1680 train_time:22804ms step_avg:88.05ms
step:260/1680 train_time:22891ms step_avg:88.04ms
step:261/1680 train_time:22978ms step_avg:88.04ms
step:262/1680 train_time:23067ms step_avg:88.04ms
step:263/1680 train_time:23156ms step_avg:88.04ms
step:264/1680 train_time:23246ms step_avg:88.05ms
step:265/1680 train_time:23335ms step_avg:88.06ms
step:266/1680 train_time:23423ms step_avg:88.06ms
step:267/1680 train_time:23511ms step_avg:88.06ms
step:268/1680 train_time:23599ms step_avg:88.06ms
step:269/1680 train_time:23686ms step_avg:88.05ms
step:270/1680 train_time:23774ms step_avg:88.05ms
step:271/1680 train_time:23861ms step_avg:88.05ms
step:272/1680 train_time:23948ms step_avg:88.05ms
step:273/1680 train_time:24036ms step_avg:88.04ms
step:274/1680 train_time:24125ms step_avg:88.05ms
step:275/1680 train_time:24213ms step_avg:88.05ms
step:276/1680 train_time:24302ms step_avg:88.05ms
step:277/1680 train_time:24391ms step_avg:88.05ms
step:278/1680 train_time:24479ms step_avg:88.05ms
step:279/1680 train_time:24567ms step_avg:88.05ms
step:280/1680 train_time:24655ms step_avg:88.05ms
step:281/1680 train_time:24743ms step_avg:88.05ms
step:282/1680 train_time:24830ms step_avg:88.05ms
step:283/1680 train_time:24917ms step_avg:88.05ms
step:284/1680 train_time:25005ms step_avg:88.05ms
step:285/1680 train_time:25093ms step_avg:88.05ms
step:286/1680 train_time:25181ms step_avg:88.05ms
step:287/1680 train_time:25270ms step_avg:88.05ms
step:288/1680 train_time:25359ms step_avg:88.05ms
step:289/1680 train_time:25447ms step_avg:88.05ms
step:290/1680 train_time:25535ms step_avg:88.05ms
step:291/1680 train_time:25623ms step_avg:88.05ms
step:292/1680 train_time:25711ms step_avg:88.05ms
step:293/1680 train_time:25798ms step_avg:88.05ms
step:294/1680 train_time:25886ms step_avg:88.05ms
step:295/1680 train_time:25974ms step_avg:88.05ms
step:296/1680 train_time:26061ms step_avg:88.04ms
step:297/1680 train_time:26149ms step_avg:88.04ms
step:298/1680 train_time:26237ms step_avg:88.04ms
step:299/1680 train_time:26326ms step_avg:88.05ms
step:300/1680 train_time:26414ms step_avg:88.05ms
step:301/1680 train_time:26502ms step_avg:88.05ms
step:302/1680 train_time:26590ms step_avg:88.05ms
step:303/1680 train_time:26678ms step_avg:88.05ms
step:304/1680 train_time:26766ms step_avg:88.05ms
step:305/1680 train_time:26854ms step_avg:88.05ms
step:306/1680 train_time:26942ms step_avg:88.05ms
step:307/1680 train_time:27030ms step_avg:88.04ms
step:308/1680 train_time:27118ms step_avg:88.05ms
step:309/1680 train_time:27207ms step_avg:88.05ms
step:310/1680 train_time:27295ms step_avg:88.05ms
step:311/1680 train_time:27383ms step_avg:88.05ms
step:312/1680 train_time:27471ms step_avg:88.05ms
step:313/1680 train_time:27559ms step_avg:88.05ms
step:314/1680 train_time:27648ms step_avg:88.05ms
step:315/1680 train_time:27736ms step_avg:88.05ms
step:316/1680 train_time:27823ms step_avg:88.05ms
step:317/1680 train_time:27910ms step_avg:88.05ms
step:318/1680 train_time:27998ms step_avg:88.04ms
step:319/1680 train_time:28086ms step_avg:88.04ms
step:320/1680 train_time:28173ms step_avg:88.04ms
step:321/1680 train_time:28262ms step_avg:88.04ms
step:322/1680 train_time:28349ms step_avg:88.04ms
step:323/1680 train_time:28437ms step_avg:88.04ms
step:324/1680 train_time:28525ms step_avg:88.04ms
step:325/1680 train_time:28613ms step_avg:88.04ms
step:326/1680 train_time:28701ms step_avg:88.04ms
step:327/1680 train_time:28789ms step_avg:88.04ms
step:328/1680 train_time:28877ms step_avg:88.04ms
step:329/1680 train_time:28965ms step_avg:88.04ms
step:330/1680 train_time:29052ms step_avg:88.04ms
step:331/1680 train_time:29140ms step_avg:88.04ms
step:332/1680 train_time:29228ms step_avg:88.04ms
step:333/1680 train_time:29317ms step_avg:88.04ms
step:334/1680 train_time:29406ms step_avg:88.04ms
step:335/1680 train_time:29493ms step_avg:88.04ms
step:336/1680 train_time:29581ms step_avg:88.04ms
step:337/1680 train_time:29669ms step_avg:88.04ms
step:338/1680 train_time:29757ms step_avg:88.04ms
step:339/1680 train_time:29845ms step_avg:88.04ms
step:340/1680 train_time:29932ms step_avg:88.04ms
step:341/1680 train_time:30020ms step_avg:88.04ms
step:342/1680 train_time:30109ms step_avg:88.04ms
step:343/1680 train_time:30196ms step_avg:88.04ms
step:344/1680 train_time:30285ms step_avg:88.04ms
step:345/1680 train_time:30372ms step_avg:88.04ms
step:346/1680 train_time:30460ms step_avg:88.04ms
step:347/1680 train_time:30548ms step_avg:88.03ms
step:348/1680 train_time:30636ms step_avg:88.04ms
step:349/1680 train_time:30725ms step_avg:88.04ms
step:350/1680 train_time:30812ms step_avg:88.03ms
step:351/1680 train_time:30900ms step_avg:88.03ms
step:352/1680 train_time:30988ms step_avg:88.03ms
step:353/1680 train_time:31075ms step_avg:88.03ms
step:354/1680 train_time:31163ms step_avg:88.03ms
step:355/1680 train_time:31250ms step_avg:88.03ms
step:356/1680 train_time:31339ms step_avg:88.03ms
step:357/1680 train_time:31426ms step_avg:88.03ms
step:358/1680 train_time:31514ms step_avg:88.03ms
step:359/1680 train_time:31601ms step_avg:88.03ms
step:360/1680 train_time:31689ms step_avg:88.03ms
step:361/1680 train_time:31777ms step_avg:88.02ms
step:362/1680 train_time:31865ms step_avg:88.02ms
step:363/1680 train_time:31953ms step_avg:88.02ms
step:364/1680 train_time:32041ms step_avg:88.02ms
step:365/1680 train_time:32129ms step_avg:88.02ms
step:366/1680 train_time:32217ms step_avg:88.02ms
step:367/1680 train_time:32305ms step_avg:88.03ms
step:368/1680 train_time:32394ms step_avg:88.03ms
step:369/1680 train_time:32481ms step_avg:88.03ms
step:370/1680 train_time:32569ms step_avg:88.02ms
step:371/1680 train_time:32656ms step_avg:88.02ms
step:372/1680 train_time:32745ms step_avg:88.02ms
step:373/1680 train_time:32832ms step_avg:88.02ms
step:374/1680 train_time:32920ms step_avg:88.02ms
step:375/1680 train_time:33008ms step_avg:88.02ms
step:375/1680 val_loss:3.8142 train_time:33097ms step_avg:88.26ms
step:376/1680 train_time:33120ms step_avg:88.09ms
step:377/1680 train_time:33187ms step_avg:88.03ms
step:378/1680 train_time:33281ms step_avg:88.05ms
step:379/1680 train_time:33371ms step_avg:88.05ms
step:380/1680 train_time:33459ms step_avg:88.05ms
step:381/1680 train_time:33546ms step_avg:88.05ms
step:382/1680 train_time:33633ms step_avg:88.04ms
step:383/1680 train_time:33720ms step_avg:88.04ms
step:384/1680 train_time:33807ms step_avg:88.04ms
step:385/1680 train_time:33894ms step_avg:88.04ms
step:386/1680 train_time:33981ms step_avg:88.03ms
step:387/1680 train_time:34070ms step_avg:88.04ms
step:388/1680 train_time:34158ms step_avg:88.04ms
step:389/1680 train_time:34248ms step_avg:88.04ms
step:390/1680 train_time:34337ms step_avg:88.04ms
step:391/1680 train_time:34425ms step_avg:88.04ms
step:392/1680 train_time:34513ms step_avg:88.04ms
step:393/1680 train_time:34600ms step_avg:88.04ms
step:394/1680 train_time:34688ms step_avg:88.04ms
step:395/1680 train_time:34775ms step_avg:88.04ms
step:396/1680 train_time:34862ms step_avg:88.03ms
step:397/1680 train_time:34948ms step_avg:88.03ms
step:398/1680 train_time:35036ms step_avg:88.03ms
step:399/1680 train_time:35124ms step_avg:88.03ms
step:400/1680 train_time:35212ms step_avg:88.03ms
step:401/1680 train_time:35301ms step_avg:88.03ms
step:402/1680 train_time:35390ms step_avg:88.03ms
step:403/1680 train_time:35478ms step_avg:88.04ms
step:404/1680 train_time:35566ms step_avg:88.03ms
step:405/1680 train_time:35654ms step_avg:88.03ms
step:406/1680 train_time:35741ms step_avg:88.03ms
step:407/1680 train_time:35828ms step_avg:88.03ms
step:408/1680 train_time:35916ms step_avg:88.03ms
step:409/1680 train_time:36004ms step_avg:88.03ms
step:410/1680 train_time:36091ms step_avg:88.03ms
step:411/1680 train_time:36180ms step_avg:88.03ms
step:412/1680 train_time:36268ms step_avg:88.03ms
step:413/1680 train_time:36357ms step_avg:88.03ms
step:414/1680 train_time:36445ms step_avg:88.03ms
step:415/1680 train_time:36533ms step_avg:88.03ms
step:416/1680 train_time:36621ms step_avg:88.03ms
step:417/1680 train_time:36709ms step_avg:88.03ms
step:418/1680 train_time:36797ms step_avg:88.03ms
step:419/1680 train_time:36884ms step_avg:88.03ms
step:420/1680 train_time:36972ms step_avg:88.03ms
step:421/1680 train_time:37060ms step_avg:88.03ms
step:422/1680 train_time:37148ms step_avg:88.03ms
step:423/1680 train_time:37236ms step_avg:88.03ms
step:424/1680 train_time:37324ms step_avg:88.03ms
step:425/1680 train_time:37412ms step_avg:88.03ms
step:426/1680 train_time:37500ms step_avg:88.03ms
step:427/1680 train_time:37588ms step_avg:88.03ms
step:428/1680 train_time:37676ms step_avg:88.03ms
step:429/1680 train_time:37763ms step_avg:88.02ms
step:430/1680 train_time:37850ms step_avg:88.02ms
step:431/1680 train_time:37938ms step_avg:88.02ms
step:432/1680 train_time:38025ms step_avg:88.02ms
step:433/1680 train_time:38113ms step_avg:88.02ms
step:434/1680 train_time:38201ms step_avg:88.02ms
step:435/1680 train_time:38289ms step_avg:88.02ms
step:436/1680 train_time:38377ms step_avg:88.02ms
step:437/1680 train_time:38466ms step_avg:88.02ms
step:438/1680 train_time:38554ms step_avg:88.02ms
step:439/1680 train_time:38642ms step_avg:88.02ms
step:440/1680 train_time:38730ms step_avg:88.02ms
step:441/1680 train_time:38819ms step_avg:88.03ms
step:442/1680 train_time:38907ms step_avg:88.02ms
step:443/1680 train_time:38994ms step_avg:88.02ms
step:444/1680 train_time:39082ms step_avg:88.02ms
step:445/1680 train_time:39170ms step_avg:88.02ms
step:446/1680 train_time:39258ms step_avg:88.02ms
step:447/1680 train_time:39346ms step_avg:88.02ms
step:448/1680 train_time:39433ms step_avg:88.02ms
step:449/1680 train_time:39522ms step_avg:88.02ms
step:450/1680 train_time:39610ms step_avg:88.02ms
step:451/1680 train_time:39698ms step_avg:88.02ms
step:452/1680 train_time:39785ms step_avg:88.02ms
step:453/1680 train_time:39873ms step_avg:88.02ms
step:454/1680 train_time:39961ms step_avg:88.02ms
step:455/1680 train_time:40049ms step_avg:88.02ms
step:456/1680 train_time:40137ms step_avg:88.02ms
step:457/1680 train_time:40224ms step_avg:88.02ms
step:458/1680 train_time:40312ms step_avg:88.02ms
step:459/1680 train_time:40400ms step_avg:88.02ms
step:460/1680 train_time:40488ms step_avg:88.02ms
step:461/1680 train_time:40576ms step_avg:88.02ms
step:462/1680 train_time:40664ms step_avg:88.02ms
step:463/1680 train_time:40753ms step_avg:88.02ms
step:464/1680 train_time:40840ms step_avg:88.02ms
step:465/1680 train_time:40927ms step_avg:88.02ms
step:466/1680 train_time:41015ms step_avg:88.01ms
step:467/1680 train_time:41102ms step_avg:88.01ms
step:468/1680 train_time:41191ms step_avg:88.01ms
step:469/1680 train_time:41279ms step_avg:88.02ms
step:470/1680 train_time:41367ms step_avg:88.01ms
step:471/1680 train_time:41455ms step_avg:88.02ms
step:472/1680 train_time:41543ms step_avg:88.01ms
step:473/1680 train_time:41631ms step_avg:88.01ms
step:474/1680 train_time:41720ms step_avg:88.02ms
step:475/1680 train_time:41807ms step_avg:88.02ms
step:476/1680 train_time:41895ms step_avg:88.01ms
step:477/1680 train_time:41982ms step_avg:88.01ms
step:478/1680 train_time:42070ms step_avg:88.01ms
step:479/1680 train_time:42157ms step_avg:88.01ms
step:480/1680 train_time:42245ms step_avg:88.01ms
step:481/1680 train_time:42334ms step_avg:88.01ms
step:482/1680 train_time:42422ms step_avg:88.01ms
step:483/1680 train_time:42510ms step_avg:88.01ms
step:484/1680 train_time:42599ms step_avg:88.01ms
step:485/1680 train_time:42686ms step_avg:88.01ms
step:486/1680 train_time:42774ms step_avg:88.01ms
step:487/1680 train_time:42862ms step_avg:88.01ms
step:488/1680 train_time:42950ms step_avg:88.01ms
step:489/1680 train_time:43038ms step_avg:88.01ms
step:490/1680 train_time:43126ms step_avg:88.01ms
step:491/1680 train_time:43214ms step_avg:88.01ms
step:492/1680 train_time:43301ms step_avg:88.01ms
step:493/1680 train_time:43389ms step_avg:88.01ms
step:494/1680 train_time:43477ms step_avg:88.01ms
step:495/1680 train_time:43564ms step_avg:88.01ms
step:496/1680 train_time:43653ms step_avg:88.01ms
step:497/1680 train_time:43741ms step_avg:88.01ms
step:498/1680 train_time:43829ms step_avg:88.01ms
step:499/1680 train_time:43917ms step_avg:88.01ms
step:500/1680 train_time:44005ms step_avg:88.01ms
step:500/1680 val_loss:3.7169 train_time:44094ms step_avg:88.19ms
step:501/1680 train_time:44117ms step_avg:88.06ms
step:502/1680 train_time:44184ms step_avg:88.01ms
step:503/1680 train_time:44276ms step_avg:88.02ms
step:504/1680 train_time:44366ms step_avg:88.03ms
step:505/1680 train_time:44453ms step_avg:88.03ms
step:506/1680 train_time:44541ms step_avg:88.03ms
step:507/1680 train_time:44628ms step_avg:88.02ms
step:508/1680 train_time:44715ms step_avg:88.02ms
step:509/1680 train_time:44802ms step_avg:88.02ms
step:510/1680 train_time:44889ms step_avg:88.02ms
step:511/1680 train_time:44976ms step_avg:88.02ms
step:512/1680 train_time:45064ms step_avg:88.02ms
step:513/1680 train_time:45152ms step_avg:88.02ms
step:514/1680 train_time:45242ms step_avg:88.02ms
step:515/1680 train_time:45331ms step_avg:88.02ms
step:516/1680 train_time:45420ms step_avg:88.02ms
step:517/1680 train_time:45508ms step_avg:88.02ms
step:518/1680 train_time:45595ms step_avg:88.02ms
step:519/1680 train_time:45683ms step_avg:88.02ms
step:520/1680 train_time:45770ms step_avg:88.02ms
step:521/1680 train_time:45857ms step_avg:88.02ms
step:522/1680 train_time:45944ms step_avg:88.02ms
step:523/1680 train_time:46032ms step_avg:88.01ms
step:524/1680 train_time:46120ms step_avg:88.01ms
step:525/1680 train_time:46208ms step_avg:88.02ms
step:526/1680 train_time:46297ms step_avg:88.02ms
step:527/1680 train_time:46386ms step_avg:88.02ms
step:528/1680 train_time:46474ms step_avg:88.02ms
step:529/1680 train_time:46563ms step_avg:88.02ms
step:530/1680 train_time:46651ms step_avg:88.02ms
step:531/1680 train_time:46739ms step_avg:88.02ms
step:532/1680 train_time:46827ms step_avg:88.02ms
step:533/1680 train_time:46913ms step_avg:88.02ms
step:534/1680 train_time:47001ms step_avg:88.02ms
step:535/1680 train_time:47089ms step_avg:88.02ms
step:536/1680 train_time:47176ms step_avg:88.02ms
step:537/1680 train_time:47266ms step_avg:88.02ms
step:538/1680 train_time:47354ms step_avg:88.02ms
step:539/1680 train_time:47442ms step_avg:88.02ms
step:540/1680 train_time:47530ms step_avg:88.02ms
step:541/1680 train_time:47618ms step_avg:88.02ms
step:542/1680 train_time:47705ms step_avg:88.02ms
step:543/1680 train_time:47793ms step_avg:88.02ms
step:544/1680 train_time:47880ms step_avg:88.01ms
step:545/1680 train_time:47968ms step_avg:88.01ms
step:546/1680 train_time:48055ms step_avg:88.01ms
step:547/1680 train_time:48143ms step_avg:88.01ms
step:548/1680 train_time:48231ms step_avg:88.01ms
step:549/1680 train_time:48321ms step_avg:88.02ms
step:550/1680 train_time:48410ms step_avg:88.02ms
step:551/1680 train_time:48499ms step_avg:88.02ms
step:552/1680 train_time:48589ms step_avg:88.02ms
step:553/1680 train_time:48677ms step_avg:88.02ms
step:554/1680 train_time:48767ms step_avg:88.03ms
step:555/1680 train_time:48855ms step_avg:88.03ms
step:556/1680 train_time:48944ms step_avg:88.03ms
step:557/1680 train_time:49033ms step_avg:88.03ms
step:558/1680 train_time:49123ms step_avg:88.03ms
step:559/1680 train_time:49212ms step_avg:88.04ms
step:560/1680 train_time:49302ms step_avg:88.04ms
step:561/1680 train_time:49390ms step_avg:88.04ms
step:562/1680 train_time:49480ms step_avg:88.04ms
step:563/1680 train_time:49572ms step_avg:88.05ms
step:564/1680 train_time:49661ms step_avg:88.05ms
step:565/1680 train_time:49750ms step_avg:88.05ms
step:566/1680 train_time:49839ms step_avg:88.06ms
step:567/1680 train_time:49928ms step_avg:88.06ms
step:568/1680 train_time:50017ms step_avg:88.06ms
step:569/1680 train_time:50107ms step_avg:88.06ms
step:570/1680 train_time:50196ms step_avg:88.06ms
step:571/1680 train_time:50285ms step_avg:88.07ms
step:572/1680 train_time:50374ms step_avg:88.07ms
step:573/1680 train_time:50463ms step_avg:88.07ms
step:574/1680 train_time:50552ms step_avg:88.07ms
step:575/1680 train_time:50641ms step_avg:88.07ms
step:576/1680 train_time:50731ms step_avg:88.07ms
step:577/1680 train_time:50820ms step_avg:88.08ms
step:578/1680 train_time:50911ms step_avg:88.08ms
step:579/1680 train_time:50999ms step_avg:88.08ms
step:580/1680 train_time:51088ms step_avg:88.08ms
step:581/1680 train_time:51177ms step_avg:88.09ms
step:582/1680 train_time:51266ms step_avg:88.09ms
step:583/1680 train_time:51354ms step_avg:88.09ms
step:584/1680 train_time:51444ms step_avg:88.09ms
step:585/1680 train_time:51534ms step_avg:88.09ms
step:586/1680 train_time:51623ms step_avg:88.09ms
step:587/1680 train_time:51712ms step_avg:88.10ms
step:588/1680 train_time:51801ms step_avg:88.10ms
step:589/1680 train_time:51890ms step_avg:88.10ms
step:590/1680 train_time:51979ms step_avg:88.10ms
step:591/1680 train_time:52069ms step_avg:88.10ms
step:592/1680 train_time:52157ms step_avg:88.10ms
step:593/1680 train_time:52247ms step_avg:88.11ms
step:594/1680 train_time:52335ms step_avg:88.11ms
step:595/1680 train_time:52424ms step_avg:88.11ms
step:596/1680 train_time:52513ms step_avg:88.11ms
step:597/1680 train_time:52603ms step_avg:88.11ms
step:598/1680 train_time:52692ms step_avg:88.11ms
step:599/1680 train_time:52781ms step_avg:88.11ms
step:600/1680 train_time:52870ms step_avg:88.12ms
step:601/1680 train_time:52958ms step_avg:88.12ms
step:602/1680 train_time:53048ms step_avg:88.12ms
step:603/1680 train_time:53136ms step_avg:88.12ms
step:604/1680 train_time:53226ms step_avg:88.12ms
step:605/1680 train_time:53315ms step_avg:88.12ms
step:606/1680 train_time:53404ms step_avg:88.13ms
step:607/1680 train_time:53492ms step_avg:88.13ms
step:608/1680 train_time:53581ms step_avg:88.13ms
step:609/1680 train_time:53671ms step_avg:88.13ms
step:610/1680 train_time:53760ms step_avg:88.13ms
step:611/1680 train_time:53850ms step_avg:88.13ms
step:612/1680 train_time:53939ms step_avg:88.14ms
step:613/1680 train_time:54028ms step_avg:88.14ms
step:614/1680 train_time:54117ms step_avg:88.14ms
step:615/1680 train_time:54207ms step_avg:88.14ms
step:616/1680 train_time:54295ms step_avg:88.14ms
step:617/1680 train_time:54385ms step_avg:88.14ms
step:618/1680 train_time:54473ms step_avg:88.14ms
step:619/1680 train_time:54562ms step_avg:88.15ms
step:620/1680 train_time:54651ms step_avg:88.15ms
step:621/1680 train_time:54741ms step_avg:88.15ms
step:622/1680 train_time:54830ms step_avg:88.15ms
step:623/1680 train_time:54919ms step_avg:88.15ms
step:624/1680 train_time:55007ms step_avg:88.15ms
step:625/1680 train_time:55096ms step_avg:88.15ms
step:625/1680 val_loss:3.6157 train_time:55186ms step_avg:88.30ms
step:626/1680 train_time:55209ms step_avg:88.19ms
step:627/1680 train_time:55277ms step_avg:88.16ms
step:628/1680 train_time:55374ms step_avg:88.18ms
step:629/1680 train_time:55468ms step_avg:88.18ms
step:630/1680 train_time:55556ms step_avg:88.18ms
step:631/1680 train_time:55644ms step_avg:88.18ms
step:632/1680 train_time:55732ms step_avg:88.18ms
step:633/1680 train_time:55819ms step_avg:88.18ms
step:634/1680 train_time:55907ms step_avg:88.18ms
step:635/1680 train_time:55995ms step_avg:88.18ms
step:636/1680 train_time:56084ms step_avg:88.18ms
step:637/1680 train_time:56175ms step_avg:88.19ms
step:638/1680 train_time:56266ms step_avg:88.19ms
step:639/1680 train_time:56357ms step_avg:88.20ms
step:640/1680 train_time:56447ms step_avg:88.20ms
step:641/1680 train_time:56536ms step_avg:88.20ms
step:642/1680 train_time:56625ms step_avg:88.20ms
step:643/1680 train_time:56713ms step_avg:88.20ms
step:644/1680 train_time:56802ms step_avg:88.20ms
step:645/1680 train_time:56890ms step_avg:88.20ms
step:646/1680 train_time:56978ms step_avg:88.20ms
step:647/1680 train_time:57066ms step_avg:88.20ms
step:648/1680 train_time:57155ms step_avg:88.20ms
step:649/1680 train_time:57245ms step_avg:88.21ms
step:650/1680 train_time:57335ms step_avg:88.21ms
step:651/1680 train_time:57426ms step_avg:88.21ms
step:652/1680 train_time:57515ms step_avg:88.21ms
step:653/1680 train_time:57605ms step_avg:88.22ms
step:654/1680 train_time:57693ms step_avg:88.22ms
step:655/1680 train_time:57782ms step_avg:88.22ms
step:656/1680 train_time:57870ms step_avg:88.22ms
step:657/1680 train_time:57958ms step_avg:88.22ms
step:658/1680 train_time:58047ms step_avg:88.22ms
step:659/1680 train_time:58136ms step_avg:88.22ms
step:660/1680 train_time:58225ms step_avg:88.22ms
step:661/1680 train_time:58315ms step_avg:88.22ms
step:662/1680 train_time:58405ms step_avg:88.23ms
step:663/1680 train_time:58494ms step_avg:88.23ms
step:664/1680 train_time:58584ms step_avg:88.23ms
step:665/1680 train_time:58673ms step_avg:88.23ms
step:666/1680 train_time:58762ms step_avg:88.23ms
step:667/1680 train_time:58850ms step_avg:88.23ms
step:668/1680 train_time:58939ms step_avg:88.23ms
step:669/1680 train_time:59027ms step_avg:88.23ms
step:670/1680 train_time:59116ms step_avg:88.23ms
step:671/1680 train_time:59205ms step_avg:88.23ms
step:672/1680 train_time:59294ms step_avg:88.23ms
step:673/1680 train_time:59383ms step_avg:88.24ms
step:674/1680 train_time:59473ms step_avg:88.24ms
step:675/1680 train_time:59563ms step_avg:88.24ms
step:676/1680 train_time:59651ms step_avg:88.24ms
step:677/1680 train_time:59741ms step_avg:88.24ms
step:678/1680 train_time:59829ms step_avg:88.24ms
step:679/1680 train_time:59918ms step_avg:88.24ms
step:680/1680 train_time:60006ms step_avg:88.24ms
step:681/1680 train_time:60096ms step_avg:88.25ms
step:682/1680 train_time:60185ms step_avg:88.25ms
step:683/1680 train_time:60274ms step_avg:88.25ms
step:684/1680 train_time:60363ms step_avg:88.25ms
step:685/1680 train_time:60453ms step_avg:88.25ms
step:686/1680 train_time:60542ms step_avg:88.25ms
step:687/1680 train_time:60632ms step_avg:88.26ms
step:688/1680 train_time:60720ms step_avg:88.26ms
step:689/1680 train_time:60809ms step_avg:88.26ms
step:690/1680 train_time:60898ms step_avg:88.26ms
step:691/1680 train_time:60986ms step_avg:88.26ms
step:692/1680 train_time:61075ms step_avg:88.26ms
step:693/1680 train_time:61165ms step_avg:88.26ms
step:694/1680 train_time:61254ms step_avg:88.26ms
step:695/1680 train_time:61343ms step_avg:88.26ms
step:696/1680 train_time:61433ms step_avg:88.27ms
step:697/1680 train_time:61522ms step_avg:88.27ms
step:698/1680 train_time:61611ms step_avg:88.27ms
step:699/1680 train_time:61700ms step_avg:88.27ms
step:700/1680 train_time:61789ms step_avg:88.27ms
step:701/1680 train_time:61878ms step_avg:88.27ms
step:702/1680 train_time:61967ms step_avg:88.27ms
step:703/1680 train_time:62057ms step_avg:88.27ms
step:704/1680 train_time:62145ms step_avg:88.27ms
step:705/1680 train_time:62234ms step_avg:88.28ms
step:706/1680 train_time:62323ms step_avg:88.28ms
step:707/1680 train_time:62412ms step_avg:88.28ms
step:708/1680 train_time:62502ms step_avg:88.28ms
step:709/1680 train_time:62590ms step_avg:88.28ms
step:710/1680 train_time:62679ms step_avg:88.28ms
step:711/1680 train_time:62767ms step_avg:88.28ms
step:712/1680 train_time:62857ms step_avg:88.28ms
step:713/1680 train_time:62947ms step_avg:88.28ms
step:714/1680 train_time:63035ms step_avg:88.28ms
step:715/1680 train_time:63125ms step_avg:88.29ms
step:716/1680 train_time:63213ms step_avg:88.29ms
step:717/1680 train_time:63303ms step_avg:88.29ms
step:718/1680 train_time:63391ms step_avg:88.29ms
step:719/1680 train_time:63481ms step_avg:88.29ms
step:720/1680 train_time:63570ms step_avg:88.29ms
step:721/1680 train_time:63659ms step_avg:88.29ms
step:722/1680 train_time:63748ms step_avg:88.29ms
step:723/1680 train_time:63837ms step_avg:88.29ms
step:724/1680 train_time:63927ms step_avg:88.30ms
step:725/1680 train_time:64016ms step_avg:88.30ms
step:726/1680 train_time:64106ms step_avg:88.30ms
step:727/1680 train_time:64194ms step_avg:88.30ms
step:728/1680 train_time:64283ms step_avg:88.30ms
step:729/1680 train_time:64372ms step_avg:88.30ms
step:730/1680 train_time:64462ms step_avg:88.30ms
step:731/1680 train_time:64551ms step_avg:88.31ms
step:732/1680 train_time:64641ms step_avg:88.31ms
step:733/1680 train_time:64729ms step_avg:88.31ms
step:734/1680 train_time:64818ms step_avg:88.31ms
step:735/1680 train_time:64907ms step_avg:88.31ms
step:736/1680 train_time:64996ms step_avg:88.31ms
step:737/1680 train_time:65086ms step_avg:88.31ms
step:738/1680 train_time:65176ms step_avg:88.31ms
step:739/1680 train_time:65266ms step_avg:88.32ms
step:740/1680 train_time:65354ms step_avg:88.32ms
step:741/1680 train_time:65444ms step_avg:88.32ms
step:742/1680 train_time:65533ms step_avg:88.32ms
step:743/1680 train_time:65622ms step_avg:88.32ms
step:744/1680 train_time:65711ms step_avg:88.32ms
step:745/1680 train_time:65800ms step_avg:88.32ms
step:746/1680 train_time:65889ms step_avg:88.32ms
step:747/1680 train_time:65979ms step_avg:88.33ms
step:748/1680 train_time:66068ms step_avg:88.33ms
step:749/1680 train_time:66158ms step_avg:88.33ms
step:750/1680 train_time:66247ms step_avg:88.33ms
step:750/1680 val_loss:3.5665 train_time:66336ms step_avg:88.45ms
step:751/1680 train_time:66359ms step_avg:88.36ms
step:752/1680 train_time:66428ms step_avg:88.33ms
step:753/1680 train_time:66521ms step_avg:88.34ms
step:754/1680 train_time:66609ms step_avg:88.34ms
step:755/1680 train_time:66698ms step_avg:88.34ms
step:756/1680 train_time:66786ms step_avg:88.34ms
step:757/1680 train_time:66874ms step_avg:88.34ms
step:758/1680 train_time:66962ms step_avg:88.34ms
step:759/1680 train_time:67051ms step_avg:88.34ms
step:760/1680 train_time:67139ms step_avg:88.34ms
step:761/1680 train_time:67228ms step_avg:88.34ms
step:762/1680 train_time:67318ms step_avg:88.34ms
step:763/1680 train_time:67408ms step_avg:88.35ms
step:764/1680 train_time:67499ms step_avg:88.35ms
step:765/1680 train_time:67589ms step_avg:88.35ms
step:766/1680 train_time:67678ms step_avg:88.35ms
step:767/1680 train_time:67767ms step_avg:88.35ms
step:768/1680 train_time:67855ms step_avg:88.35ms
step:769/1680 train_time:67945ms step_avg:88.35ms
step:770/1680 train_time:68032ms step_avg:88.35ms
step:771/1680 train_time:68121ms step_avg:88.35ms
step:772/1680 train_time:68209ms step_avg:88.35ms
step:773/1680 train_time:68298ms step_avg:88.35ms
step:774/1680 train_time:68387ms step_avg:88.36ms
step:775/1680 train_time:68477ms step_avg:88.36ms
step:776/1680 train_time:68567ms step_avg:88.36ms
step:777/1680 train_time:68657ms step_avg:88.36ms
step:778/1680 train_time:68746ms step_avg:88.36ms
step:779/1680 train_time:68836ms step_avg:88.36ms
step:780/1680 train_time:68925ms step_avg:88.37ms
step:781/1680 train_time:69013ms step_avg:88.37ms
step:782/1680 train_time:69102ms step_avg:88.37ms
step:783/1680 train_time:69191ms step_avg:88.37ms
step:784/1680 train_time:69280ms step_avg:88.37ms
step:785/1680 train_time:69369ms step_avg:88.37ms
step:786/1680 train_time:69459ms step_avg:88.37ms
step:787/1680 train_time:69549ms step_avg:88.37ms
step:788/1680 train_time:69638ms step_avg:88.37ms
step:789/1680 train_time:69727ms step_avg:88.37ms
step:790/1680 train_time:69816ms step_avg:88.37ms
step:791/1680 train_time:69905ms step_avg:88.38ms
step:792/1680 train_time:69994ms step_avg:88.38ms
step:793/1680 train_time:70082ms step_avg:88.38ms
step:794/1680 train_time:70171ms step_avg:88.38ms
step:795/1680 train_time:70260ms step_avg:88.38ms
step:796/1680 train_time:70349ms step_avg:88.38ms
step:797/1680 train_time:70439ms step_avg:88.38ms
step:798/1680 train_time:70528ms step_avg:88.38ms
step:799/1680 train_time:70619ms step_avg:88.38ms
step:800/1680 train_time:70707ms step_avg:88.38ms
step:801/1680 train_time:70796ms step_avg:88.38ms
step:802/1680 train_time:70886ms step_avg:88.39ms
step:803/1680 train_time:70974ms step_avg:88.39ms
step:804/1680 train_time:71064ms step_avg:88.39ms
step:805/1680 train_time:71152ms step_avg:88.39ms
step:806/1680 train_time:71241ms step_avg:88.39ms
step:807/1680 train_time:71331ms step_avg:88.39ms
step:808/1680 train_time:71421ms step_avg:88.39ms
step:809/1680 train_time:71511ms step_avg:88.39ms
step:810/1680 train_time:71600ms step_avg:88.40ms
step:811/1680 train_time:71689ms step_avg:88.40ms
step:812/1680 train_time:71779ms step_avg:88.40ms
step:813/1680 train_time:71867ms step_avg:88.40ms
step:814/1680 train_time:71956ms step_avg:88.40ms
step:815/1680 train_time:72046ms step_avg:88.40ms
step:816/1680 train_time:72134ms step_avg:88.40ms
step:817/1680 train_time:72223ms step_avg:88.40ms
step:818/1680 train_time:72312ms step_avg:88.40ms
step:819/1680 train_time:72402ms step_avg:88.40ms
step:820/1680 train_time:72490ms step_avg:88.40ms
step:821/1680 train_time:72579ms step_avg:88.40ms
step:822/1680 train_time:72668ms step_avg:88.40ms
step:823/1680 train_time:72757ms step_avg:88.40ms
step:824/1680 train_time:72847ms step_avg:88.41ms
step:825/1680 train_time:72935ms step_avg:88.41ms
step:826/1680 train_time:73025ms step_avg:88.41ms
step:827/1680 train_time:73113ms step_avg:88.41ms
step:828/1680 train_time:73202ms step_avg:88.41ms
step:829/1680 train_time:73291ms step_avg:88.41ms
step:830/1680 train_time:73381ms step_avg:88.41ms
step:831/1680 train_time:73470ms step_avg:88.41ms
step:832/1680 train_time:73560ms step_avg:88.41ms
step:833/1680 train_time:73648ms step_avg:88.41ms
step:834/1680 train_time:73737ms step_avg:88.41ms
step:835/1680 train_time:73826ms step_avg:88.41ms
step:836/1680 train_time:73916ms step_avg:88.42ms
step:837/1680 train_time:74005ms step_avg:88.42ms
step:838/1680 train_time:74094ms step_avg:88.42ms
step:839/1680 train_time:74183ms step_avg:88.42ms
step:840/1680 train_time:74272ms step_avg:88.42ms
step:841/1680 train_time:74361ms step_avg:88.42ms
step:842/1680 train_time:74450ms step_avg:88.42ms
step:843/1680 train_time:74539ms step_avg:88.42ms
step:844/1680 train_time:74628ms step_avg:88.42ms
step:845/1680 train_time:74716ms step_avg:88.42ms
step:846/1680 train_time:74806ms step_avg:88.42ms
step:847/1680 train_time:74895ms step_avg:88.42ms
step:848/1680 train_time:74984ms step_avg:88.42ms
step:849/1680 train_time:75072ms step_avg:88.42ms
step:850/1680 train_time:75161ms step_avg:88.42ms
step:851/1680 train_time:75250ms step_avg:88.43ms
step:852/1680 train_time:75339ms step_avg:88.43ms
step:853/1680 train_time:75428ms step_avg:88.43ms
step:854/1680 train_time:75518ms step_avg:88.43ms
step:855/1680 train_time:75607ms step_avg:88.43ms
step:856/1680 train_time:75697ms step_avg:88.43ms
step:857/1680 train_time:75786ms step_avg:88.43ms
step:858/1680 train_time:75875ms step_avg:88.43ms
step:859/1680 train_time:75964ms step_avg:88.43ms
step:860/1680 train_time:76053ms step_avg:88.43ms
step:861/1680 train_time:76142ms step_avg:88.43ms
step:862/1680 train_time:76230ms step_avg:88.43ms
step:863/1680 train_time:76319ms step_avg:88.43ms
step:864/1680 train_time:76408ms step_avg:88.44ms
step:865/1680 train_time:76498ms step_avg:88.44ms
step:866/1680 train_time:76587ms step_avg:88.44ms
step:867/1680 train_time:76676ms step_avg:88.44ms
step:868/1680 train_time:76765ms step_avg:88.44ms
step:869/1680 train_time:76854ms step_avg:88.44ms
step:870/1680 train_time:76943ms step_avg:88.44ms
step:871/1680 train_time:77031ms step_avg:88.44ms
step:872/1680 train_time:77121ms step_avg:88.44ms
step:873/1680 train_time:77210ms step_avg:88.44ms
step:874/1680 train_time:77299ms step_avg:88.44ms
step:875/1680 train_time:77388ms step_avg:88.44ms
step:875/1680 val_loss:3.5206 train_time:77478ms step_avg:88.55ms
step:876/1680 train_time:77500ms step_avg:88.47ms
step:877/1680 train_time:77570ms step_avg:88.45ms
step:878/1680 train_time:77663ms step_avg:88.45ms
step:879/1680 train_time:77753ms step_avg:88.46ms
step:880/1680 train_time:77841ms step_avg:88.46ms
step:881/1680 train_time:77929ms step_avg:88.46ms
step:882/1680 train_time:78017ms step_avg:88.45ms
step:883/1680 train_time:78105ms step_avg:88.45ms
step:884/1680 train_time:78193ms step_avg:88.45ms
step:885/1680 train_time:78281ms step_avg:88.45ms
step:886/1680 train_time:78370ms step_avg:88.45ms
step:887/1680 train_time:78460ms step_avg:88.46ms
step:888/1680 train_time:78550ms step_avg:88.46ms
step:889/1680 train_time:78642ms step_avg:88.46ms
step:890/1680 train_time:78731ms step_avg:88.46ms
step:891/1680 train_time:78821ms step_avg:88.46ms
step:892/1680 train_time:78909ms step_avg:88.46ms
step:893/1680 train_time:78998ms step_avg:88.46ms
step:894/1680 train_time:79087ms step_avg:88.46ms
step:895/1680 train_time:79175ms step_avg:88.46ms
step:896/1680 train_time:79264ms step_avg:88.46ms
step:897/1680 train_time:79353ms step_avg:88.46ms
step:898/1680 train_time:79442ms step_avg:88.47ms
step:899/1680 train_time:79531ms step_avg:88.47ms
step:900/1680 train_time:79621ms step_avg:88.47ms
step:901/1680 train_time:79710ms step_avg:88.47ms
step:902/1680 train_time:79800ms step_avg:88.47ms
step:903/1680 train_time:79889ms step_avg:88.47ms
step:904/1680 train_time:79978ms step_avg:88.47ms
step:905/1680 train_time:80067ms step_avg:88.47ms
step:906/1680 train_time:80155ms step_avg:88.47ms
step:907/1680 train_time:80244ms step_avg:88.47ms
step:908/1680 train_time:80332ms step_avg:88.47ms
step:909/1680 train_time:80421ms step_avg:88.47ms
step:910/1680 train_time:80510ms step_avg:88.47ms
step:911/1680 train_time:80600ms step_avg:88.47ms
step:912/1680 train_time:80689ms step_avg:88.48ms
step:913/1680 train_time:80780ms step_avg:88.48ms
step:914/1680 train_time:80869ms step_avg:88.48ms
step:915/1680 train_time:80958ms step_avg:88.48ms
step:916/1680 train_time:81046ms step_avg:88.48ms
step:917/1680 train_time:81135ms step_avg:88.48ms
step:918/1680 train_time:81224ms step_avg:88.48ms
step:919/1680 train_time:81312ms step_avg:88.48ms
step:920/1680 train_time:81402ms step_avg:88.48ms
step:921/1680 train_time:81490ms step_avg:88.48ms
step:922/1680 train_time:81580ms step_avg:88.48ms
step:923/1680 train_time:81669ms step_avg:88.48ms
step:924/1680 train_time:81758ms step_avg:88.48ms
step:925/1680 train_time:81847ms step_avg:88.48ms
step:926/1680 train_time:81936ms step_avg:88.48ms
step:927/1680 train_time:82025ms step_avg:88.48ms
step:928/1680 train_time:82114ms step_avg:88.48ms
step:929/1680 train_time:82205ms step_avg:88.49ms
step:930/1680 train_time:82292ms step_avg:88.49ms
step:931/1680 train_time:82382ms step_avg:88.49ms
step:932/1680 train_time:82470ms step_avg:88.49ms
step:933/1680 train_time:82559ms step_avg:88.49ms
step:934/1680 train_time:82648ms step_avg:88.49ms
step:935/1680 train_time:82738ms step_avg:88.49ms
step:936/1680 train_time:82827ms step_avg:88.49ms
step:937/1680 train_time:82916ms step_avg:88.49ms
step:938/1680 train_time:83006ms step_avg:88.49ms
step:939/1680 train_time:83096ms step_avg:88.49ms
step:940/1680 train_time:83185ms step_avg:88.49ms
step:941/1680 train_time:83273ms step_avg:88.49ms
step:942/1680 train_time:83362ms step_avg:88.50ms
step:943/1680 train_time:83451ms step_avg:88.50ms
step:944/1680 train_time:83539ms step_avg:88.50ms
step:945/1680 train_time:83628ms step_avg:88.50ms
step:946/1680 train_time:83718ms step_avg:88.50ms
step:947/1680 train_time:83807ms step_avg:88.50ms
step:948/1680 train_time:83896ms step_avg:88.50ms
step:949/1680 train_time:83985ms step_avg:88.50ms
step:950/1680 train_time:84074ms step_avg:88.50ms
step:951/1680 train_time:84163ms step_avg:88.50ms
step:952/1680 train_time:84252ms step_avg:88.50ms
step:953/1680 train_time:84341ms step_avg:88.50ms
step:954/1680 train_time:84430ms step_avg:88.50ms
step:955/1680 train_time:84520ms step_avg:88.50ms
step:956/1680 train_time:84608ms step_avg:88.50ms
step:957/1680 train_time:84698ms step_avg:88.50ms
step:958/1680 train_time:84787ms step_avg:88.50ms
step:959/1680 train_time:84876ms step_avg:88.50ms
step:960/1680 train_time:84965ms step_avg:88.51ms
step:961/1680 train_time:85054ms step_avg:88.51ms
step:962/1680 train_time:85143ms step_avg:88.51ms
step:963/1680 train_time:85232ms step_avg:88.51ms
step:964/1680 train_time:85321ms step_avg:88.51ms
step:965/1680 train_time:85410ms step_avg:88.51ms
step:966/1680 train_time:85500ms step_avg:88.51ms
step:967/1680 train_time:85588ms step_avg:88.51ms
step:968/1680 train_time:85677ms step_avg:88.51ms
step:969/1680 train_time:85766ms step_avg:88.51ms
step:970/1680 train_time:85856ms step_avg:88.51ms
step:971/1680 train_time:85945ms step_avg:88.51ms
step:972/1680 train_time:86034ms step_avg:88.51ms
step:973/1680 train_time:86123ms step_avg:88.51ms
step:974/1680 train_time:86212ms step_avg:88.51ms
step:975/1680 train_time:86302ms step_avg:88.51ms
step:976/1680 train_time:86391ms step_avg:88.51ms
step:977/1680 train_time:86479ms step_avg:88.52ms
step:978/1680 train_time:86567ms step_avg:88.51ms
step:979/1680 train_time:86656ms step_avg:88.52ms
step:980/1680 train_time:86746ms step_avg:88.52ms
step:981/1680 train_time:86835ms step_avg:88.52ms
step:982/1680 train_time:86924ms step_avg:88.52ms
step:983/1680 train_time:87013ms step_avg:88.52ms
step:984/1680 train_time:87103ms step_avg:88.52ms
step:985/1680 train_time:87192ms step_avg:88.52ms
step:986/1680 train_time:87281ms step_avg:88.52ms
step:987/1680 train_time:87370ms step_avg:88.52ms
step:988/1680 train_time:87459ms step_avg:88.52ms
step:989/1680 train_time:87547ms step_avg:88.52ms
step:990/1680 train_time:87636ms step_avg:88.52ms
step:991/1680 train_time:87725ms step_avg:88.52ms
step:992/1680 train_time:87815ms step_avg:88.52ms
step:993/1680 train_time:87904ms step_avg:88.52ms
step:994/1680 train_time:87993ms step_avg:88.52ms
step:995/1680 train_time:88083ms step_avg:88.53ms
step:996/1680 train_time:88171ms step_avg:88.53ms
step:997/1680 train_time:88261ms step_avg:88.53ms
step:998/1680 train_time:88349ms step_avg:88.53ms
step:999/1680 train_time:88439ms step_avg:88.53ms
step:1000/1680 train_time:88528ms step_avg:88.53ms
step:1000/1680 val_loss:3.4706 train_time:88618ms step_avg:88.62ms
step:1001/1680 train_time:88640ms step_avg:88.55ms
step:1002/1680 train_time:88710ms step_avg:88.53ms
step:1003/1680 train_time:88805ms step_avg:88.54ms
step:1004/1680 train_time:88895ms step_avg:88.54ms
step:1005/1680 train_time:88983ms step_avg:88.54ms
step:1006/1680 train_time:89071ms step_avg:88.54ms
step:1007/1680 train_time:89159ms step_avg:88.54ms
step:1008/1680 train_time:89247ms step_avg:88.54ms
step:1009/1680 train_time:89335ms step_avg:88.54ms
step:1010/1680 train_time:89424ms step_avg:88.54ms
step:1011/1680 train_time:89513ms step_avg:88.54ms
step:1012/1680 train_time:89602ms step_avg:88.54ms
step:1013/1680 train_time:89693ms step_avg:88.54ms
step:1014/1680 train_time:89784ms step_avg:88.54ms
step:1015/1680 train_time:89874ms step_avg:88.55ms
step:1016/1680 train_time:89963ms step_avg:88.55ms
step:1017/1680 train_time:90052ms step_avg:88.55ms
step:1018/1680 train_time:90140ms step_avg:88.55ms
step:1019/1680 train_time:90229ms step_avg:88.55ms
step:1020/1680 train_time:90316ms step_avg:88.55ms
step:1021/1680 train_time:90404ms step_avg:88.54ms
step:1022/1680 train_time:90493ms step_avg:88.55ms
step:1023/1680 train_time:90582ms step_avg:88.55ms
step:1024/1680 train_time:90672ms step_avg:88.55ms
step:1025/1680 train_time:90764ms step_avg:88.55ms
step:1026/1680 train_time:90853ms step_avg:88.55ms
step:1027/1680 train_time:90942ms step_avg:88.55ms
step:1028/1680 train_time:91032ms step_avg:88.55ms
step:1029/1680 train_time:91120ms step_avg:88.55ms
step:1030/1680 train_time:91209ms step_avg:88.55ms
step:1031/1680 train_time:91297ms step_avg:88.55ms
step:1032/1680 train_time:91385ms step_avg:88.55ms
step:1033/1680 train_time:91474ms step_avg:88.55ms
step:1034/1680 train_time:91563ms step_avg:88.55ms
step:1035/1680 train_time:91652ms step_avg:88.55ms
step:1036/1680 train_time:91742ms step_avg:88.55ms
step:1037/1680 train_time:91832ms step_avg:88.56ms
step:1038/1680 train_time:91920ms step_avg:88.56ms
step:1039/1680 train_time:92009ms step_avg:88.56ms
step:1040/1680 train_time:92098ms step_avg:88.56ms
step:1041/1680 train_time:92186ms step_avg:88.56ms
step:1042/1680 train_time:92274ms step_avg:88.55ms
step:1043/1680 train_time:92363ms step_avg:88.55ms
step:1044/1680 train_time:92451ms step_avg:88.56ms
step:1045/1680 train_time:92541ms step_avg:88.56ms
step:1046/1680 train_time:92631ms step_avg:88.56ms
step:1047/1680 train_time:92720ms step_avg:88.56ms
step:1048/1680 train_time:92809ms step_avg:88.56ms
step:1049/1680 train_time:92899ms step_avg:88.56ms
step:1050/1680 train_time:92988ms step_avg:88.56ms
step:1051/1680 train_time:93078ms step_avg:88.56ms
step:1052/1680 train_time:93166ms step_avg:88.56ms
step:1053/1680 train_time:93255ms step_avg:88.56ms
step:1054/1680 train_time:93343ms step_avg:88.56ms
step:1055/1680 train_time:93432ms step_avg:88.56ms
step:1056/1680 train_time:93522ms step_avg:88.56ms
step:1057/1680 train_time:93610ms step_avg:88.56ms
step:1058/1680 train_time:93699ms step_avg:88.56ms
step:1059/1680 train_time:93788ms step_avg:88.56ms
step:1060/1680 train_time:93877ms step_avg:88.56ms
step:1061/1680 train_time:93966ms step_avg:88.56ms
step:1062/1680 train_time:94056ms step_avg:88.56ms
step:1063/1680 train_time:94144ms step_avg:88.56ms
step:1064/1680 train_time:94233ms step_avg:88.56ms
step:1065/1680 train_time:94322ms step_avg:88.57ms
step:1066/1680 train_time:94410ms step_avg:88.57ms
step:1067/1680 train_time:94500ms step_avg:88.57ms
step:1068/1680 train_time:94588ms step_avg:88.57ms
step:1069/1680 train_time:94677ms step_avg:88.57ms
step:1070/1680 train_time:94766ms step_avg:88.57ms
step:1071/1680 train_time:94855ms step_avg:88.57ms
step:1072/1680 train_time:94944ms step_avg:88.57ms
step:1073/1680 train_time:95033ms step_avg:88.57ms
step:1074/1680 train_time:95123ms step_avg:88.57ms
step:1075/1680 train_time:95213ms step_avg:88.57ms
step:1076/1680 train_time:95302ms step_avg:88.57ms
step:1077/1680 train_time:95390ms step_avg:88.57ms
step:1078/1680 train_time:95479ms step_avg:88.57ms
step:1079/1680 train_time:95568ms step_avg:88.57ms
step:1080/1680 train_time:95657ms step_avg:88.57ms
step:1081/1680 train_time:95745ms step_avg:88.57ms
step:1082/1680 train_time:95835ms step_avg:88.57ms
step:1083/1680 train_time:95924ms step_avg:88.57ms
step:1084/1680 train_time:96013ms step_avg:88.57ms
step:1085/1680 train_time:96102ms step_avg:88.57ms
step:1086/1680 train_time:96191ms step_avg:88.57ms
step:1087/1680 train_time:96280ms step_avg:88.57ms
step:1088/1680 train_time:96368ms step_avg:88.57ms
step:1089/1680 train_time:96458ms step_avg:88.57ms
step:1090/1680 train_time:96546ms step_avg:88.57ms
step:1091/1680 train_time:96636ms step_avg:88.58ms
step:1092/1680 train_time:96724ms step_avg:88.58ms
step:1093/1680 train_time:96813ms step_avg:88.58ms
step:1094/1680 train_time:96903ms step_avg:88.58ms
step:1095/1680 train_time:96992ms step_avg:88.58ms
step:1096/1680 train_time:97082ms step_avg:88.58ms
step:1097/1680 train_time:97171ms step_avg:88.58ms
step:1098/1680 train_time:97261ms step_avg:88.58ms
step:1099/1680 train_time:97350ms step_avg:88.58ms
step:1100/1680 train_time:97440ms step_avg:88.58ms
step:1101/1680 train_time:97530ms step_avg:88.58ms
step:1102/1680 train_time:97620ms step_avg:88.58ms
step:1103/1680 train_time:97710ms step_avg:88.59ms
step:1104/1680 train_time:97800ms step_avg:88.59ms
step:1105/1680 train_time:97890ms step_avg:88.59ms
step:1106/1680 train_time:97980ms step_avg:88.59ms
step:1107/1680 train_time:98069ms step_avg:88.59ms
step:1108/1680 train_time:98159ms step_avg:88.59ms
step:1109/1680 train_time:98248ms step_avg:88.59ms
step:1110/1680 train_time:98338ms step_avg:88.59ms
step:1111/1680 train_time:98428ms step_avg:88.59ms
step:1112/1680 train_time:98518ms step_avg:88.60ms
step:1113/1680 train_time:98608ms step_avg:88.60ms
step:1114/1680 train_time:98697ms step_avg:88.60ms
step:1115/1680 train_time:98786ms step_avg:88.60ms
step:1116/1680 train_time:98876ms step_avg:88.60ms
step:1117/1680 train_time:98966ms step_avg:88.60ms
step:1118/1680 train_time:99057ms step_avg:88.60ms
step:1119/1680 train_time:99146ms step_avg:88.60ms
step:1120/1680 train_time:99236ms step_avg:88.60ms
step:1121/1680 train_time:99325ms step_avg:88.60ms
step:1122/1680 train_time:99414ms step_avg:88.60ms
step:1123/1680 train_time:99504ms step_avg:88.61ms
step:1124/1680 train_time:99593ms step_avg:88.61ms
step:1125/1680 train_time:99683ms step_avg:88.61ms
step:1125/1680 val_loss:3.4167 train_time:99774ms step_avg:88.69ms
step:1126/1680 train_time:99797ms step_avg:88.63ms
step:1127/1680 train_time:99865ms step_avg:88.61ms
step:1128/1680 train_time:99963ms step_avg:88.62ms
step:1129/1680 train_time:100056ms step_avg:88.62ms
step:1130/1680 train_time:100144ms step_avg:88.62ms
step:1131/1680 train_time:100233ms step_avg:88.62ms
step:1132/1680 train_time:100321ms step_avg:88.62ms
step:1133/1680 train_time:100409ms step_avg:88.62ms
step:1134/1680 train_time:100498ms step_avg:88.62ms
step:1135/1680 train_time:100587ms step_avg:88.62ms
step:1136/1680 train_time:100679ms step_avg:88.63ms
step:1137/1680 train_time:100772ms step_avg:88.63ms
step:1138/1680 train_time:100863ms step_avg:88.63ms
step:1139/1680 train_time:100956ms step_avg:88.64ms
step:1140/1680 train_time:101046ms step_avg:88.64ms
step:1141/1680 train_time:101135ms step_avg:88.64ms
step:1142/1680 train_time:101224ms step_avg:88.64ms
step:1143/1680 train_time:101314ms step_avg:88.64ms
step:1144/1680 train_time:101403ms step_avg:88.64ms
step:1145/1680 train_time:101492ms step_avg:88.64ms
step:1146/1680 train_time:101580ms step_avg:88.64ms
step:1147/1680 train_time:101671ms step_avg:88.64ms
step:1148/1680 train_time:101761ms step_avg:88.64ms
step:1149/1680 train_time:101853ms step_avg:88.64ms
step:1150/1680 train_time:101943ms step_avg:88.65ms
step:1151/1680 train_time:102034ms step_avg:88.65ms
step:1152/1680 train_time:102123ms step_avg:88.65ms
step:1153/1680 train_time:102212ms step_avg:88.65ms
step:1154/1680 train_time:102301ms step_avg:88.65ms
step:1155/1680 train_time:102390ms step_avg:88.65ms
step:1156/1680 train_time:102479ms step_avg:88.65ms
step:1157/1680 train_time:102569ms step_avg:88.65ms
step:1158/1680 train_time:102658ms step_avg:88.65ms
step:1159/1680 train_time:102748ms step_avg:88.65ms
step:1160/1680 train_time:102840ms step_avg:88.65ms
step:1161/1680 train_time:102930ms step_avg:88.66ms
step:1162/1680 train_time:103020ms step_avg:88.66ms
step:1163/1680 train_time:103111ms step_avg:88.66ms
step:1164/1680 train_time:103202ms step_avg:88.66ms
step:1165/1680 train_time:103291ms step_avg:88.66ms
step:1166/1680 train_time:103380ms step_avg:88.66ms
step:1167/1680 train_time:103469ms step_avg:88.66ms
step:1168/1680 train_time:103558ms step_avg:88.66ms
step:1169/1680 train_time:103648ms step_avg:88.66ms
step:1170/1680 train_time:103737ms step_avg:88.66ms
step:1171/1680 train_time:103827ms step_avg:88.67ms
step:1172/1680 train_time:103918ms step_avg:88.67ms
step:1173/1680 train_time:104008ms step_avg:88.67ms
step:1174/1680 train_time:104099ms step_avg:88.67ms
step:1175/1680 train_time:104190ms step_avg:88.67ms
step:1176/1680 train_time:104280ms step_avg:88.67ms
step:1177/1680 train_time:104370ms step_avg:88.67ms
step:1178/1680 train_time:104458ms step_avg:88.67ms
step:1179/1680 train_time:104547ms step_avg:88.67ms
step:1180/1680 train_time:104637ms step_avg:88.68ms
step:1181/1680 train_time:104726ms step_avg:88.68ms
step:1182/1680 train_time:104816ms step_avg:88.68ms
step:1183/1680 train_time:104906ms step_avg:88.68ms
step:1184/1680 train_time:104997ms step_avg:88.68ms
step:1185/1680 train_time:105087ms step_avg:88.68ms
step:1186/1680 train_time:105178ms step_avg:88.68ms
step:1187/1680 train_time:105268ms step_avg:88.68ms
step:1188/1680 train_time:105357ms step_avg:88.68ms
step:1189/1680 train_time:105446ms step_avg:88.68ms
step:1190/1680 train_time:105536ms step_avg:88.69ms
step:1191/1680 train_time:105626ms step_avg:88.69ms
step:1192/1680 train_time:105715ms step_avg:88.69ms
step:1193/1680 train_time:105804ms step_avg:88.69ms
step:1194/1680 train_time:105894ms step_avg:88.69ms
step:1195/1680 train_time:105985ms step_avg:88.69ms
step:1196/1680 train_time:106074ms step_avg:88.69ms
step:1197/1680 train_time:106164ms step_avg:88.69ms
step:1198/1680 train_time:106254ms step_avg:88.69ms
step:1199/1680 train_time:106344ms step_avg:88.69ms
step:1200/1680 train_time:106433ms step_avg:88.69ms
step:1201/1680 train_time:106522ms step_avg:88.69ms
step:1202/1680 train_time:106611ms step_avg:88.69ms
step:1203/1680 train_time:106700ms step_avg:88.69ms
step:1204/1680 train_time:106789ms step_avg:88.70ms
step:1205/1680 train_time:106879ms step_avg:88.70ms
step:1206/1680 train_time:106969ms step_avg:88.70ms
step:1207/1680 train_time:107060ms step_avg:88.70ms
step:1208/1680 train_time:107150ms step_avg:88.70ms
step:1209/1680 train_time:107241ms step_avg:88.70ms
step:1210/1680 train_time:107330ms step_avg:88.70ms
step:1211/1680 train_time:107420ms step_avg:88.70ms
step:1212/1680 train_time:107509ms step_avg:88.70ms
step:1213/1680 train_time:107599ms step_avg:88.71ms
step:1214/1680 train_time:107689ms step_avg:88.71ms
step:1215/1680 train_time:107778ms step_avg:88.71ms
step:1216/1680 train_time:107868ms step_avg:88.71ms
step:1217/1680 train_time:107960ms step_avg:88.71ms
step:1218/1680 train_time:108049ms step_avg:88.71ms
step:1219/1680 train_time:108140ms step_avg:88.71ms
step:1220/1680 train_time:108230ms step_avg:88.71ms
step:1221/1680 train_time:108320ms step_avg:88.71ms
step:1222/1680 train_time:108410ms step_avg:88.71ms
step:1223/1680 train_time:108500ms step_avg:88.72ms
step:1224/1680 train_time:108590ms step_avg:88.72ms
step:1225/1680 train_time:108680ms step_avg:88.72ms
step:1226/1680 train_time:108769ms step_avg:88.72ms
step:1227/1680 train_time:108859ms step_avg:88.72ms
step:1228/1680 train_time:108948ms step_avg:88.72ms
step:1229/1680 train_time:109038ms step_avg:88.72ms
step:1230/1680 train_time:109128ms step_avg:88.72ms
step:1231/1680 train_time:109219ms step_avg:88.72ms
step:1232/1680 train_time:109309ms step_avg:88.72ms
step:1233/1680 train_time:109400ms step_avg:88.73ms
step:1234/1680 train_time:109489ms step_avg:88.73ms
step:1235/1680 train_time:109579ms step_avg:88.73ms
step:1236/1680 train_time:109670ms step_avg:88.73ms
step:1237/1680 train_time:109759ms step_avg:88.73ms
step:1238/1680 train_time:109849ms step_avg:88.73ms
step:1239/1680 train_time:109939ms step_avg:88.73ms
step:1240/1680 train_time:110029ms step_avg:88.73ms
step:1241/1680 train_time:110119ms step_avg:88.73ms
step:1242/1680 train_time:110209ms step_avg:88.74ms
step:1243/1680 train_time:110299ms step_avg:88.74ms
step:1244/1680 train_time:110390ms step_avg:88.74ms
step:1245/1680 train_time:110479ms step_avg:88.74ms
step:1246/1680 train_time:110569ms step_avg:88.74ms
step:1247/1680 train_time:110659ms step_avg:88.74ms
step:1248/1680 train_time:110749ms step_avg:88.74ms
step:1249/1680 train_time:110839ms step_avg:88.74ms
step:1250/1680 train_time:110929ms step_avg:88.74ms
step:1250/1680 val_loss:3.3781 train_time:111020ms step_avg:88.82ms
step:1251/1680 train_time:111043ms step_avg:88.76ms
step:1252/1680 train_time:111115ms step_avg:88.75ms
step:1253/1680 train_time:111209ms step_avg:88.75ms
step:1254/1680 train_time:111300ms step_avg:88.76ms
step:1255/1680 train_time:111389ms step_avg:88.76ms
step:1256/1680 train_time:111478ms step_avg:88.76ms
step:1257/1680 train_time:111567ms step_avg:88.76ms
step:1258/1680 train_time:111655ms step_avg:88.76ms
step:1259/1680 train_time:111745ms step_avg:88.76ms
step:1260/1680 train_time:111834ms step_avg:88.76ms
step:1261/1680 train_time:111923ms step_avg:88.76ms
step:1262/1680 train_time:112013ms step_avg:88.76ms
step:1263/1680 train_time:112105ms step_avg:88.76ms
step:1264/1680 train_time:112196ms step_avg:88.76ms
step:1265/1680 train_time:112288ms step_avg:88.77ms
step:1266/1680 train_time:112379ms step_avg:88.77ms
step:1267/1680 train_time:112469ms step_avg:88.77ms
step:1268/1680 train_time:112558ms step_avg:88.77ms
step:1269/1680 train_time:112647ms step_avg:88.77ms
step:1270/1680 train_time:112735ms step_avg:88.77ms
step:1271/1680 train_time:112824ms step_avg:88.77ms
step:1272/1680 train_time:112913ms step_avg:88.77ms
step:1273/1680 train_time:113003ms step_avg:88.77ms
step:1274/1680 train_time:113093ms step_avg:88.77ms
step:1275/1680 train_time:113185ms step_avg:88.77ms
step:1276/1680 train_time:113276ms step_avg:88.77ms
step:1277/1680 train_time:113366ms step_avg:88.78ms
step:1278/1680 train_time:113456ms step_avg:88.78ms
step:1279/1680 train_time:113546ms step_avg:88.78ms
step:1280/1680 train_time:113635ms step_avg:88.78ms
step:1281/1680 train_time:113723ms step_avg:88.78ms
step:1282/1680 train_time:113812ms step_avg:88.78ms
step:1283/1680 train_time:113902ms step_avg:88.78ms
step:1284/1680 train_time:113992ms step_avg:88.78ms
step:1285/1680 train_time:114082ms step_avg:88.78ms
step:1286/1680 train_time:114173ms step_avg:88.78ms
step:1287/1680 train_time:114264ms step_avg:88.78ms
step:1288/1680 train_time:114354ms step_avg:88.78ms
step:1289/1680 train_time:114445ms step_avg:88.79ms
step:1290/1680 train_time:114535ms step_avg:88.79ms
step:1291/1680 train_time:114625ms step_avg:88.79ms
step:1292/1680 train_time:114714ms step_avg:88.79ms
step:1293/1680 train_time:114803ms step_avg:88.79ms
step:1294/1680 train_time:114892ms step_avg:88.79ms
step:1295/1680 train_time:114981ms step_avg:88.79ms
step:1296/1680 train_time:115072ms step_avg:88.79ms
step:1297/1680 train_time:115163ms step_avg:88.79ms
step:1298/1680 train_time:115254ms step_avg:88.79ms
step:1299/1680 train_time:115345ms step_avg:88.80ms
step:1300/1680 train_time:115435ms step_avg:88.80ms
step:1301/1680 train_time:115525ms step_avg:88.80ms
step:1302/1680 train_time:115616ms step_avg:88.80ms
step:1303/1680 train_time:115705ms step_avg:88.80ms
step:1304/1680 train_time:115794ms step_avg:88.80ms
step:1305/1680 train_time:115883ms step_avg:88.80ms
step:1306/1680 train_time:115973ms step_avg:88.80ms
step:1307/1680 train_time:116063ms step_avg:88.80ms
step:1308/1680 train_time:116153ms step_avg:88.80ms
step:1309/1680 train_time:116243ms step_avg:88.80ms
step:1310/1680 train_time:116333ms step_avg:88.80ms
step:1311/1680 train_time:116424ms step_avg:88.81ms
step:1312/1680 train_time:116514ms step_avg:88.81ms
step:1313/1680 train_time:116604ms step_avg:88.81ms
step:1314/1680 train_time:116694ms step_avg:88.81ms
step:1315/1680 train_time:116783ms step_avg:88.81ms
step:1316/1680 train_time:116872ms step_avg:88.81ms
step:1317/1680 train_time:116961ms step_avg:88.81ms
step:1318/1680 train_time:117052ms step_avg:88.81ms
step:1319/1680 train_time:117142ms step_avg:88.81ms
step:1320/1680 train_time:117231ms step_avg:88.81ms
step:1321/1680 train_time:117320ms step_avg:88.81ms
step:1322/1680 train_time:117410ms step_avg:88.81ms
step:1323/1680 train_time:117501ms step_avg:88.81ms
step:1324/1680 train_time:117591ms step_avg:88.82ms
step:1325/1680 train_time:117681ms step_avg:88.82ms
step:1326/1680 train_time:117770ms step_avg:88.82ms
step:1327/1680 train_time:117860ms step_avg:88.82ms
step:1328/1680 train_time:117950ms step_avg:88.82ms
step:1329/1680 train_time:118040ms step_avg:88.82ms
step:1330/1680 train_time:118130ms step_avg:88.82ms
step:1331/1680 train_time:118220ms step_avg:88.82ms
step:1332/1680 train_time:118309ms step_avg:88.82ms
step:1333/1680 train_time:118399ms step_avg:88.82ms
step:1334/1680 train_time:118489ms step_avg:88.82ms
step:1335/1680 train_time:118578ms step_avg:88.82ms
step:1336/1680 train_time:118669ms step_avg:88.82ms
step:1337/1680 train_time:118758ms step_avg:88.82ms
step:1338/1680 train_time:118847ms step_avg:88.82ms
step:1339/1680 train_time:118936ms step_avg:88.82ms
step:1340/1680 train_time:119026ms step_avg:88.83ms
step:1341/1680 train_time:119116ms step_avg:88.83ms
step:1342/1680 train_time:119205ms step_avg:88.83ms
step:1343/1680 train_time:119294ms step_avg:88.83ms
step:1344/1680 train_time:119384ms step_avg:88.83ms
step:1345/1680 train_time:119474ms step_avg:88.83ms
step:1346/1680 train_time:119563ms step_avg:88.83ms
step:1347/1680 train_time:119654ms step_avg:88.83ms
step:1348/1680 train_time:119744ms step_avg:88.83ms
step:1349/1680 train_time:119833ms step_avg:88.83ms
step:1350/1680 train_time:119923ms step_avg:88.83ms
step:1351/1680 train_time:120013ms step_avg:88.83ms
step:1352/1680 train_time:120103ms step_avg:88.83ms
step:1353/1680 train_time:120193ms step_avg:88.83ms
step:1354/1680 train_time:120284ms step_avg:88.84ms
step:1355/1680 train_time:120374ms step_avg:88.84ms
step:1356/1680 train_time:120464ms step_avg:88.84ms
step:1357/1680 train_time:120555ms step_avg:88.84ms
step:1358/1680 train_time:120644ms step_avg:88.84ms
step:1359/1680 train_time:120733ms step_avg:88.84ms
step:1360/1680 train_time:120823ms step_avg:88.84ms
step:1361/1680 train_time:120913ms step_avg:88.84ms
step:1362/1680 train_time:121003ms step_avg:88.84ms
step:1363/1680 train_time:121093ms step_avg:88.84ms
step:1364/1680 train_time:121182ms step_avg:88.84ms
step:1365/1680 train_time:121271ms step_avg:88.84ms
step:1366/1680 train_time:121361ms step_avg:88.84ms
step:1367/1680 train_time:121451ms step_avg:88.85ms
step:1368/1680 train_time:121542ms step_avg:88.85ms
step:1369/1680 train_time:121632ms step_avg:88.85ms
step:1370/1680 train_time:121722ms step_avg:88.85ms
step:1371/1680 train_time:121813ms step_avg:88.85ms
step:1372/1680 train_time:121903ms step_avg:88.85ms
step:1373/1680 train_time:121993ms step_avg:88.85ms
step:1374/1680 train_time:122083ms step_avg:88.85ms
step:1375/1680 train_time:122172ms step_avg:88.85ms
step:1375/1680 val_loss:3.3436 train_time:122263ms step_avg:88.92ms
step:1376/1680 train_time:122286ms step_avg:88.87ms
step:1377/1680 train_time:122357ms step_avg:88.86ms
step:1378/1680 train_time:122452ms step_avg:88.86ms
step:1379/1680 train_time:122547ms step_avg:88.87ms
step:1380/1680 train_time:122636ms step_avg:88.87ms
step:1381/1680 train_time:122725ms step_avg:88.87ms
step:1382/1680 train_time:122814ms step_avg:88.87ms
step:1383/1680 train_time:122902ms step_avg:88.87ms
step:1384/1680 train_time:122991ms step_avg:88.87ms
step:1385/1680 train_time:123080ms step_avg:88.87ms
step:1386/1680 train_time:123168ms step_avg:88.87ms
step:1387/1680 train_time:123259ms step_avg:88.87ms
step:1388/1680 train_time:123349ms step_avg:88.87ms
step:1389/1680 train_time:123441ms step_avg:88.87ms
step:1390/1680 train_time:123532ms step_avg:88.87ms
step:1391/1680 train_time:123622ms step_avg:88.87ms
step:1392/1680 train_time:123712ms step_avg:88.87ms
step:1393/1680 train_time:123802ms step_avg:88.87ms
step:1394/1680 train_time:123890ms step_avg:88.87ms
step:1395/1680 train_time:123980ms step_avg:88.87ms
step:1396/1680 train_time:124069ms step_avg:88.87ms
step:1397/1680 train_time:124158ms step_avg:88.87ms
step:1398/1680 train_time:124246ms step_avg:88.87ms
step:1399/1680 train_time:124337ms step_avg:88.88ms
step:1400/1680 train_time:124428ms step_avg:88.88ms
step:1401/1680 train_time:124520ms step_avg:88.88ms
step:1402/1680 train_time:124610ms step_avg:88.88ms
step:1403/1680 train_time:124700ms step_avg:88.88ms
step:1404/1680 train_time:124789ms step_avg:88.88ms
step:1405/1680 train_time:124878ms step_avg:88.88ms
step:1406/1680 train_time:124967ms step_avg:88.88ms
step:1407/1680 train_time:125056ms step_avg:88.88ms
step:1408/1680 train_time:125145ms step_avg:88.88ms
step:1409/1680 train_time:125235ms step_avg:88.88ms
step:1410/1680 train_time:125325ms step_avg:88.88ms
step:1411/1680 train_time:125415ms step_avg:88.88ms
step:1412/1680 train_time:125506ms step_avg:88.89ms
step:1413/1680 train_time:125597ms step_avg:88.89ms
step:1414/1680 train_time:125687ms step_avg:88.89ms
step:1415/1680 train_time:125777ms step_avg:88.89ms
step:1416/1680 train_time:125866ms step_avg:88.89ms
step:1417/1680 train_time:125955ms step_avg:88.89ms
step:1418/1680 train_time:126044ms step_avg:88.89ms
step:1419/1680 train_time:126133ms step_avg:88.89ms
step:1420/1680 train_time:126222ms step_avg:88.89ms
step:1421/1680 train_time:126312ms step_avg:88.89ms
step:1422/1680 train_time:126403ms step_avg:88.89ms
step:1423/1680 train_time:126493ms step_avg:88.89ms
step:1424/1680 train_time:126583ms step_avg:88.89ms
step:1425/1680 train_time:126673ms step_avg:88.89ms
step:1426/1680 train_time:126762ms step_avg:88.89ms
step:1427/1680 train_time:126852ms step_avg:88.89ms
step:1428/1680 train_time:126941ms step_avg:88.89ms
step:1429/1680 train_time:127030ms step_avg:88.89ms
step:1430/1680 train_time:127120ms step_avg:88.89ms
step:1431/1680 train_time:127209ms step_avg:88.90ms
step:1432/1680 train_time:127299ms step_avg:88.90ms
step:1433/1680 train_time:127389ms step_avg:88.90ms
step:1434/1680 train_time:127480ms step_avg:88.90ms
step:1435/1680 train_time:127569ms step_avg:88.90ms
step:1436/1680 train_time:127659ms step_avg:88.90ms
step:1437/1680 train_time:127749ms step_avg:88.90ms
step:1438/1680 train_time:127839ms step_avg:88.90ms
step:1439/1680 train_time:127928ms step_avg:88.90ms
step:1440/1680 train_time:128019ms step_avg:88.90ms
step:1441/1680 train_time:128108ms step_avg:88.90ms
step:1442/1680 train_time:128197ms step_avg:88.90ms
step:1443/1680 train_time:128287ms step_avg:88.90ms
step:1444/1680 train_time:128376ms step_avg:88.90ms
step:1445/1680 train_time:128466ms step_avg:88.90ms
step:1446/1680 train_time:128557ms step_avg:88.91ms
step:1447/1680 train_time:128647ms step_avg:88.91ms
step:1448/1680 train_time:128737ms step_avg:88.91ms
step:1449/1680 train_time:128827ms step_avg:88.91ms
step:1450/1680 train_time:128916ms step_avg:88.91ms
step:1451/1680 train_time:129008ms step_avg:88.91ms
step:1452/1680 train_time:129097ms step_avg:88.91ms
step:1453/1680 train_time:129186ms step_avg:88.91ms
step:1454/1680 train_time:129277ms step_avg:88.91ms
step:1455/1680 train_time:129367ms step_avg:88.91ms
step:1456/1680 train_time:129457ms step_avg:88.91ms
step:1457/1680 train_time:129546ms step_avg:88.91ms
step:1458/1680 train_time:129636ms step_avg:88.91ms
step:1459/1680 train_time:129726ms step_avg:88.91ms
step:1460/1680 train_time:129816ms step_avg:88.91ms
step:1461/1680 train_time:129907ms step_avg:88.92ms
step:1462/1680 train_time:129998ms step_avg:88.92ms
step:1463/1680 train_time:130088ms step_avg:88.92ms
step:1464/1680 train_time:130177ms step_avg:88.92ms
step:1465/1680 train_time:130267ms step_avg:88.92ms
step:1466/1680 train_time:130356ms step_avg:88.92ms
step:1467/1680 train_time:130446ms step_avg:88.92ms
step:1468/1680 train_time:130536ms step_avg:88.92ms
step:1469/1680 train_time:130626ms step_avg:88.92ms
step:1470/1680 train_time:130716ms step_avg:88.92ms
step:1471/1680 train_time:130807ms step_avg:88.92ms
step:1472/1680 train_time:130897ms step_avg:88.92ms
step:1473/1680 train_time:130987ms step_avg:88.93ms
step:1474/1680 train_time:131076ms step_avg:88.93ms
step:1475/1680 train_time:131166ms step_avg:88.93ms
step:1476/1680 train_time:131255ms step_avg:88.93ms
step:1477/1680 train_time:131345ms step_avg:88.93ms
step:1478/1680 train_time:131435ms step_avg:88.93ms
step:1479/1680 train_time:131525ms step_avg:88.93ms
step:1480/1680 train_time:131614ms step_avg:88.93ms
step:1481/1680 train_time:131705ms step_avg:88.93ms
step:1482/1680 train_time:131795ms step_avg:88.93ms
step:1483/1680 train_time:131885ms step_avg:88.93ms
step:1484/1680 train_time:131975ms step_avg:88.93ms
step:1485/1680 train_time:132065ms step_avg:88.93ms
step:1486/1680 train_time:132154ms step_avg:88.93ms
step:1487/1680 train_time:132245ms step_avg:88.93ms
step:1488/1680 train_time:132334ms step_avg:88.93ms
step:1489/1680 train_time:132424ms step_avg:88.93ms
step:1490/1680 train_time:132513ms step_avg:88.93ms
step:1491/1680 train_time:132604ms step_avg:88.94ms
step:1492/1680 train_time:132694ms step_avg:88.94ms
step:1493/1680 train_time:132784ms step_avg:88.94ms
step:1494/1680 train_time:132873ms step_avg:88.94ms
step:1495/1680 train_time:132963ms step_avg:88.94ms
step:1496/1680 train_time:133053ms step_avg:88.94ms
step:1497/1680 train_time:133142ms step_avg:88.94ms
step:1498/1680 train_time:133232ms step_avg:88.94ms
step:1499/1680 train_time:133321ms step_avg:88.94ms
step:1500/1680 train_time:133411ms step_avg:88.94ms
step:1500/1680 val_loss:3.3141 train_time:133502ms step_avg:89.00ms
step:1501/1680 train_time:133525ms step_avg:88.96ms
step:1502/1680 train_time:133595ms step_avg:88.94ms
step:1503/1680 train_time:133689ms step_avg:88.95ms
step:1504/1680 train_time:133780ms step_avg:88.95ms
step:1505/1680 train_time:133871ms step_avg:88.95ms
step:1506/1680 train_time:133960ms step_avg:88.95ms
step:1507/1680 train_time:134048ms step_avg:88.95ms
step:1508/1680 train_time:134138ms step_avg:88.95ms
step:1509/1680 train_time:134226ms step_avg:88.95ms
step:1510/1680 train_time:134315ms step_avg:88.95ms
step:1511/1680 train_time:134404ms step_avg:88.95ms
step:1512/1680 train_time:134494ms step_avg:88.95ms
step:1513/1680 train_time:134586ms step_avg:88.95ms
step:1514/1680 train_time:134677ms step_avg:88.95ms
step:1515/1680 train_time:134768ms step_avg:88.96ms
step:1516/1680 train_time:134859ms step_avg:88.96ms
step:1517/1680 train_time:134949ms step_avg:88.96ms
step:1518/1680 train_time:135038ms step_avg:88.96ms
step:1519/1680 train_time:135127ms step_avg:88.96ms
step:1520/1680 train_time:135216ms step_avg:88.96ms
step:1521/1680 train_time:135305ms step_avg:88.96ms
step:1522/1680 train_time:135394ms step_avg:88.96ms
step:1523/1680 train_time:135483ms step_avg:88.96ms
step:1524/1680 train_time:135574ms step_avg:88.96ms
step:1525/1680 train_time:135664ms step_avg:88.96ms
step:1526/1680 train_time:135756ms step_avg:88.96ms
step:1527/1680 train_time:135847ms step_avg:88.96ms
step:1528/1680 train_time:135937ms step_avg:88.96ms
step:1529/1680 train_time:136026ms step_avg:88.96ms
step:1530/1680 train_time:136115ms step_avg:88.96ms
step:1531/1680 train_time:136203ms step_avg:88.96ms
step:1532/1680 train_time:136294ms step_avg:88.96ms
step:1533/1680 train_time:136382ms step_avg:88.96ms
step:1534/1680 train_time:136472ms step_avg:88.96ms
step:1535/1680 train_time:136563ms step_avg:88.97ms
step:1536/1680 train_time:136654ms step_avg:88.97ms
step:1537/1680 train_time:136744ms step_avg:88.97ms
step:1538/1680 train_time:136836ms step_avg:88.97ms
step:1539/1680 train_time:136925ms step_avg:88.97ms
step:1540/1680 train_time:137016ms step_avg:88.97ms
step:1541/1680 train_time:137105ms step_avg:88.97ms
step:1542/1680 train_time:137194ms step_avg:88.97ms
step:1543/1680 train_time:137283ms step_avg:88.97ms
step:1544/1680 train_time:137372ms step_avg:88.97ms
step:1545/1680 train_time:137461ms step_avg:88.97ms
step:1546/1680 train_time:137552ms step_avg:88.97ms
step:1547/1680 train_time:137641ms step_avg:88.97ms
step:1548/1680 train_time:137733ms step_avg:88.97ms
step:1549/1680 train_time:137823ms step_avg:88.98ms
step:1550/1680 train_time:137913ms step_avg:88.98ms
step:1551/1680 train_time:138002ms step_avg:88.98ms
step:1552/1680 train_time:138092ms step_avg:88.98ms
step:1553/1680 train_time:138181ms step_avg:88.98ms
step:1554/1680 train_time:138271ms step_avg:88.98ms
step:1555/1680 train_time:138360ms step_avg:88.98ms
step:1556/1680 train_time:138450ms step_avg:88.98ms
step:1557/1680 train_time:138540ms step_avg:88.98ms
step:1558/1680 train_time:138630ms step_avg:88.98ms
step:1559/1680 train_time:138720ms step_avg:88.98ms
step:1560/1680 train_time:138811ms step_avg:88.98ms
step:1561/1680 train_time:138900ms step_avg:88.98ms
step:1562/1680 train_time:138990ms step_avg:88.98ms
step:1563/1680 train_time:139080ms step_avg:88.98ms
step:1564/1680 train_time:139170ms step_avg:88.98ms
step:1565/1680 train_time:139259ms step_avg:88.98ms
step:1566/1680 train_time:139349ms step_avg:88.98ms
step:1567/1680 train_time:139440ms step_avg:88.99ms
step:1568/1680 train_time:139530ms step_avg:88.99ms
step:1569/1680 train_time:139620ms step_avg:88.99ms
step:1570/1680 train_time:139710ms step_avg:88.99ms
step:1571/1680 train_time:139800ms step_avg:88.99ms
step:1572/1680 train_time:139889ms step_avg:88.99ms
step:1573/1680 train_time:139979ms step_avg:88.99ms
step:1574/1680 train_time:140070ms step_avg:88.99ms
step:1575/1680 train_time:140160ms step_avg:88.99ms
step:1576/1680 train_time:140249ms step_avg:88.99ms
step:1577/1680 train_time:140339ms step_avg:88.99ms
step:1578/1680 train_time:140429ms step_avg:88.99ms
step:1579/1680 train_time:140518ms step_avg:88.99ms
step:1580/1680 train_time:140608ms step_avg:88.99ms
step:1581/1680 train_time:140698ms step_avg:88.99ms
step:1582/1680 train_time:140787ms step_avg:88.99ms
step:1583/1680 train_time:140878ms step_avg:88.99ms
step:1584/1680 train_time:140968ms step_avg:88.99ms
step:1585/1680 train_time:141058ms step_avg:89.00ms
step:1586/1680 train_time:141148ms step_avg:89.00ms
step:1587/1680 train_time:141238ms step_avg:89.00ms
step:1588/1680 train_time:141327ms step_avg:89.00ms
step:1589/1680 train_time:141417ms step_avg:89.00ms
step:1590/1680 train_time:141506ms step_avg:89.00ms
step:1591/1680 train_time:141596ms step_avg:89.00ms
step:1592/1680 train_time:141686ms step_avg:89.00ms
step:1593/1680 train_time:141776ms step_avg:89.00ms
step:1594/1680 train_time:141865ms step_avg:89.00ms
step:1595/1680 train_time:141955ms step_avg:89.00ms
step:1596/1680 train_time:142045ms step_avg:89.00ms
step:1597/1680 train_time:142135ms step_avg:89.00ms
step:1598/1680 train_time:142225ms step_avg:89.00ms
step:1599/1680 train_time:142316ms step_avg:89.00ms
step:1600/1680 train_time:142406ms step_avg:89.00ms
step:1601/1680 train_time:142495ms step_avg:89.00ms
step:1602/1680 train_time:142584ms step_avg:89.00ms
step:1603/1680 train_time:142674ms step_avg:89.00ms
step:1604/1680 train_time:142763ms step_avg:89.00ms
step:1605/1680 train_time:142853ms step_avg:89.01ms
step:1606/1680 train_time:142942ms step_avg:89.01ms
step:1607/1680 train_time:143032ms step_avg:89.01ms
step:1608/1680 train_time:143122ms step_avg:89.01ms
step:1609/1680 train_time:143213ms step_avg:89.01ms
step:1610/1680 train_time:143303ms step_avg:89.01ms
step:1611/1680 train_time:143393ms step_avg:89.01ms
step:1612/1680 train_time:143482ms step_avg:89.01ms
step:1613/1680 train_time:143572ms step_avg:89.01ms
step:1614/1680 train_time:143662ms step_avg:89.01ms
step:1615/1680 train_time:143752ms step_avg:89.01ms
step:1616/1680 train_time:143842ms step_avg:89.01ms
step:1617/1680 train_time:143932ms step_avg:89.01ms
step:1618/1680 train_time:144022ms step_avg:89.01ms
step:1619/1680 train_time:144111ms step_avg:89.01ms
step:1620/1680 train_time:144201ms step_avg:89.01ms
step:1621/1680 train_time:144290ms step_avg:89.01ms
step:1622/1680 train_time:144380ms step_avg:89.01ms
step:1623/1680 train_time:144470ms step_avg:89.01ms
step:1624/1680 train_time:144560ms step_avg:89.02ms
step:1625/1680 train_time:144651ms step_avg:89.02ms
step:1625/1680 val_loss:3.2905 train_time:144741ms step_avg:89.07ms
step:1626/1680 train_time:144767ms step_avg:89.03ms
step:1627/1680 train_time:144831ms step_avg:89.02ms
step:1628/1680 train_time:144926ms step_avg:89.02ms
step:1629/1680 train_time:145017ms step_avg:89.02ms
step:1630/1680 train_time:145107ms step_avg:89.02ms
step:1631/1680 train_time:145195ms step_avg:89.02ms
step:1632/1680 train_time:145284ms step_avg:89.02ms
step:1633/1680 train_time:145373ms step_avg:89.02ms
step:1634/1680 train_time:145462ms step_avg:89.02ms
step:1635/1680 train_time:145550ms step_avg:89.02ms
step:1636/1680 train_time:145640ms step_avg:89.02ms
step:1637/1680 train_time:145731ms step_avg:89.02ms
step:1638/1680 train_time:145822ms step_avg:89.02ms
step:1639/1680 train_time:145913ms step_avg:89.03ms
step:1640/1680 train_time:146004ms step_avg:89.03ms
step:1641/1680 train_time:146093ms step_avg:89.03ms
step:1642/1680 train_time:146183ms step_avg:89.03ms
step:1643/1680 train_time:146272ms step_avg:89.03ms
step:1644/1680 train_time:146361ms step_avg:89.03ms
step:1645/1680 train_time:146449ms step_avg:89.03ms
step:1646/1680 train_time:146539ms step_avg:89.03ms
step:1647/1680 train_time:146628ms step_avg:89.03ms
step:1648/1680 train_time:146718ms step_avg:89.03ms
step:1649/1680 train_time:146810ms step_avg:89.03ms
step:1650/1680 train_time:146901ms step_avg:89.03ms
step:1651/1680 train_time:146992ms step_avg:89.03ms
step:1652/1680 train_time:147082ms step_avg:89.03ms
step:1653/1680 train_time:147171ms step_avg:89.03ms
step:1654/1680 train_time:147261ms step_avg:89.03ms
step:1655/1680 train_time:147350ms step_avg:89.03ms
step:1656/1680 train_time:147438ms step_avg:89.03ms
step:1657/1680 train_time:147528ms step_avg:89.03ms
step:1658/1680 train_time:147617ms step_avg:89.03ms
step:1659/1680 train_time:147707ms step_avg:89.03ms
step:1660/1680 train_time:147798ms step_avg:89.03ms
step:1661/1680 train_time:147888ms step_avg:89.04ms
step:1662/1680 train_time:147979ms step_avg:89.04ms
step:1663/1680 train_time:148069ms step_avg:89.04ms
step:1664/1680 train_time:148159ms step_avg:89.04ms
step:1665/1680 train_time:148248ms step_avg:89.04ms
step:1666/1680 train_time:148338ms step_avg:89.04ms
step:1667/1680 train_time:148427ms step_avg:89.04ms
step:1668/1680 train_time:148517ms step_avg:89.04ms
step:1669/1680 train_time:148607ms step_avg:89.04ms
step:1670/1680 train_time:148696ms step_avg:89.04ms
step:1671/1680 train_time:148786ms step_avg:89.04ms
step:1672/1680 train_time:148877ms step_avg:89.04ms
step:1673/1680 train_time:148967ms step_avg:89.04ms
step:1674/1680 train_time:149057ms step_avg:89.04ms
step:1675/1680 train_time:149147ms step_avg:89.04ms
step:1676/1680 train_time:149237ms step_avg:89.04ms
step:1677/1680 train_time:149326ms step_avg:89.04ms
step:1678/1680 train_time:149416ms step_avg:89.04ms
step:1679/1680 train_time:149506ms step_avg:89.04ms
step:1680/1680 train_time:149595ms step_avg:89.04ms
step:1680/1680 val_loss:3.2801 train_time:149686ms step_avg:89.10ms
peak memory allocated: 30760 MiB reserved: 45554 MiB

import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from flash_attn_interface import flash_attn_varlen_func
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ns_line_1_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ns_line_1(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ns_line_1_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ns_line_2_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from ns_line_1_kernel, but also loads and adds a block of A
    # Performance is slightly slower than ns_line_1_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ns_line_2(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ns_line_2_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def newton_schulz_triton(G: torch.Tensor):
    a, b, c = (3.4445, -4.7750, 2.0315)
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) + 1e-7)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    ns_line_3 = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the NS iterations
    for _ in range(5):
        ns_line_1(X, out=A)  # A = X @ X.mT
        ns_line_2(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        ns_line_3(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    Though empirically small 1D params perform efficiently here:
        NS approximately performs a magnitude normalization of the grad
        This hyper-optimized function has faster execution time than DistAdam for small params

    Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as 
    needed on the forward pass. This enables attn and mlp weights to be contained within the same 
    dist.reduce_scatter_tensor call(). The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. once 1 completes, compute NS of 1 and schedule all gather
        6. once 2 completes, compute NS of 2 and schedule all gather
        7. once 3 completes, compute NS of 3 and schedule all gather
            GPUs recieve [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2ATTN, 2MLP, 2MLP, 2MLP]
            GPUs that receive params of type attn reshape before NS
        8. once 4 completes, compute NS of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params gives 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)
        if custom_sizing:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def generate_standard_param_groups(self, params):
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
    
    def generate_custom_param_groups(self, params):
        # implementation requires that a single GPU does not recieve both attn 
        # and mlp params when a param group is split across GPUs
        module_ranks = {
            'smear_gate': 1, # 1 param
            'attn_gate': 2, # 10 params
            'attn': 3, # 10 params
            'mlp': 4, # 22 params
        }
        params = list(params)
        params.sort(key=lambda x: module_ranks.get(x.module))
        idx = 0
        group_sizes = [1,10,16,16]
        assert len(params)==sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            group_params = params[idx:idx+size]
            param_groups.append(dict(params=group_params))
            idx += size
        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            num_params = len(params)
            padded_num_params = (
                (num_params + world_size - 1) // world_size * world_size
            )

            grads_to_stack = [p.grad for p in params]
            if padded_num_params > num_params:
                padding_grad = torch.zeros_like(params[0].grad)
                grads_to_stack.extend(
                    [padding_grad] * (padded_num_params - num_params)
                )

            stacked_grads = torch.stack(grads_to_stack)

            chunk_size = padded_num_params // world_size
            grad_chunk = torch.empty(
                (chunk_size, *params[0].grad.shape),
                dtype=stacked_grads.dtype,
                device=stacked_grads.device,
            )

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(
                {
                    "params": params,
                    "grad_chunk": grad_chunk,
                    "reduce_future": reduce_future,
                    "chunk_size": chunk_size,
                    "padded_num_params": padded_num_params,
                }
            )

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = info["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = info["chunk_size"]
            start_idx = rank * chunk_size

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            # This helps in vectorizing operations later.
            p_example = params[0]  # All params in a group have the same shape.
            eff_lr_val = (
                group["lr"]
                * max(1, p_example.size(-2) / p_example.size(-1)) ** 0.5
                * getattr(p_example, "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(p_example, "wd_mul", 1.0)
            )

            # Prepare a contiguous buffer for the updated parameters for this rank's chunk.
            # This buffer will serve as the input_tensor for dist.all_gather_into_tensor.
            updated_param_chunk = torch.empty(
                (chunk_size, *p_example.shape),
                dtype=p_example.dtype,
                device=p_example.device,
            )

            # List to collect update_grad tensors for batched zeropower computation.
            update_grads_for_zeropower = []

            # Process each parameter in this rank's chunk.
            for i in range(chunk_size):
                param_idx = start_idx + i

                if param_idx >= len(params):
                    # For padding: Fill the corresponding part of the updated_param_chunk with zeros.
                    # These padded entries will not be used by other ranks in the all_gather, but
                    # initializing them prevents uninitialized memory access issues.
                    updated_param_chunk[i].zero_()
                    # Also append a zero tensor for zeropower input if it must be padded.
                    update_grads_for_zeropower.append(
                        torch.zeros_like(p_example.grad)
                    )
                    continue
                p = params[param_idx]
                grad = grad_chunk[
                    i
                ]  # This gradient corresponds to the current parameter p.
                state = self.state[p]

                # Initialize momentum buffer if not present
                if not state:
                    state["momentum_buffer"] = torch.zeros_like(grad)

                momentum_buffer = state["momentum_buffer"]

                # Apply momentum update directly to the persistent momentum buffer in-place.
                momentum_buffer.lerp_(grad, 1 - group["momentum"])

                # Compute the actual `update_grad` for zeropower. This creates a new tensor.
                update_grad = grad.lerp(momentum_buffer, group["momentum"])
                update_grads_for_zeropower.append(update_grad)

                # Copy the current parameter value into the temporary buffer.
                updated_param_chunk[i].copy_(p)

                # Apply weight decay directly to the buffer.
                updated_param_chunk[i].mul_(1 - eff_weight_decay_val)

            # Stack the individual `update_grad` tensors for efficient batched zeropower computation.
            batched_update_grads = torch.stack(update_grads_for_zeropower)

            # Compute zeropower for the entire chunk in a single, batched call.
            original_shape = batched_update_grads.shape
            # Reshape attn params from [hdim, dim*4] to [4,hdim,dim] to apply NS indepedently to Q,K,V,O
            module_idx = start_idx if start_idx<len(params) else 0
            if getattr(params[module_idx],'module','none')=='attn':
                batch = 4 * original_shape[0]
                d1 = original_shape[1] 
                d2 = original_shape[2] // 4
                batched = batched_update_grads.view(batch, d1, d2)
                v_chunk = newton_schulz_triton(batched)
                v_chunk = v_chunk.view(original_shape)
            else:
                v_chunk = newton_schulz_triton(batched_update_grads)

            # Add the computed zeropower update to the parameters in the buffer.
            # This loop applies the zeropower output (v_chunk) to the `updated_param_chunk` buffer.
            for i in range(chunk_size):
                param_idx = start_idx + i
                if param_idx >= len(params):  # Skip padded entries again.
                    continue

                # Add the computed zeropower update to the parameter in the buffer.
                updated_param_chunk[i].add_(v_chunk[i], alpha=-eff_lr_val)

            stacked_params = torch.empty(
                (info["padded_num_params"], *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )
            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_param_chunk, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_gather_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for base_i in range(len(params)):
                grad = params[base_i].grad
                rank_size = grad.shape[0] // world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for base in range(len(params)):
                reduce_scatter_futures[idx].wait()
                p = params[base]
                rank_size = p.shape[0] // world_size
                p_slice = p[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(p, "lr_mul", 1.0)
                state = self.state[p]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state["step"] = torch.tensor(
                        0, dtype=torch.int64, device=p.device
                    )
                    state["exp_avg"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                    state["exp_avg_sq"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(p, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_gather_futures.append(dist.all_gather_into_tensor(p, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()
        
    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # shape to enable MegaBatchMuon
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        self.qkvo_w.module='attn'
        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        self.attn_gate.weight.module = 'attn_gate'
        self.attn_gate.weight.detach().zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4,self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens, max_seqlen_q=max_len, max_seqlen_k=max_len,
                                   causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4,self.hdim, self.dim)[3].type_as(y))
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make both matrices have the same shape because optimizer sorts params by shape
        # 2 matrices x 12 layers = 24 total, which is divisible by 8 GPU world size
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        self.c_fc.module='mlp'
        self.c_proj.module='mlp'
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.detach().zero_()
        self.smear_gate.weight.module = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 6) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(num_layers), #extra zeros params for smear_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        # smear token embed forward 1 position @classiclarryd
        smear_lambda = self.scalars[5 * len(self.blocks)]
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)

        n = len(self.blocks) // 2

        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            if i >= n and i<11:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)

        x = norm(x)
        logits = self.lm_head(x).float()
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), target_seq, reduction="sum" if self.training else "mean")
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()
    
    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 24 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_iterations: int = 1640 # number of iterations to run
    iteration_extension = 40 # number of iterations to continue training at final cooldown and window size
    cooldown_frac: int = 0.5 # fraction of training spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 125 # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_validate: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_long_validate: int = 20 # extend long windows out even further

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.8, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizer2 = Muon(hidden_matrix_params+gate_params, lr=0.05, momentum=0.95, weight_decay=0.0)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: stable then decay
def get_lr(step: int):
    x = min(0.9999,step / args.num_iterations)
    assert 0 <= x < 1
    lr = 1.0
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = w * 1.0 + (1 - w) * 0.1
    return lr

def get_ws(step: int):
    if step == args.num_iterations+args.iteration_extension:
        return args.ws_validate//2, args.ws_validate
    x = min(step / (1 + args.num_iterations),0.9999)
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx]//2, args.ws_schedule[ws_idx]

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
ws_long = args.ws_schedule[0]
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    new_ws_long = args.ws_schedule[step % len(args.ws_schedule)]  # each window size is a new graph, need to warm up each with YaRN params
    if new_ws_long > ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long = new_ws_long
    elif new_ws_long<ws_long:
        model.yarn.reset()
        ws_long = new_ws_long
    model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset()
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations + args.iteration_extension
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_long_validate
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for _ in range(grad_accum_steps):
        inputs, targets, cum_seqlens = next(train_loader)
        model(inputs, targets, cum_seqlens, ws_short, ws_long).backward()
    # set optimization hyperparameters
    for opt in optimizers:
        for group in opt.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)
    for group in optimizer2.param_groups:
        frac = min(step / 300, 1) # momentum warmup for muon
        group["momentum"] = (1 - frac) * 0.85 + frac * 0.95
    # step the optimizers
    for opt in optimizers:
        opt.step()
    # null the gradients
    model.zero_grad(set_to_none=True)
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.10.12 (main, May 27 2025, 17:12:29) [GCC 11.4.0]
Running PyTorch 2.9.0.dev20250726+cu126 compiled for CUDA 12.6
Running Triton version 3.4.0
Wed Sep 24 05:57:12 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.148.08             Driver Version: 570.148.08     CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:61:00.0 Off |                    0 |
| N/A   27C    P0            117W /  700W |    5858MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:62:00.0 Off |                    0 |
| N/A   26C    P0            112W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:63:00.0 Off |                    0 |
| N/A   27C    P0            113W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:64:00.0 Off |                    0 |
| N/A   25C    P0            114W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:6A:00.0 Off |                    0 |
| N/A   26C    P0            117W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:6B:00.0 Off |                    0 |
| N/A   28C    P0            118W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:6C:00.0 Off |                    0 |
| N/A   28C    P0            116W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:6D:00.0 Off |                    0 |
| N/A   25C    P0            117W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A          109683      C   /usr/bin/python3                       1510MiB |
|    0   N/A  N/A          109684      C   /usr/bin/python3                        614MiB |
|    0   N/A  N/A          109685      C   /usr/bin/python3                        614MiB |
|    0   N/A  N/A          109686      C   /usr/bin/python3                        614MiB |
|    0   N/A  N/A          109687      C   /usr/bin/python3                        614MiB |
|    0   N/A  N/A          109688      C   /usr/bin/python3                        614MiB |
|    0   N/A  N/A          109689      C   /usr/bin/python3                        614MiB |
|    0   N/A  N/A          109690      C   /usr/bin/python3                        614MiB |
|    1   N/A  N/A          109684      C   /usr/bin/python3                       1510MiB |
|    2   N/A  N/A          109685      C   /usr/bin/python3                       1510MiB |
|    3   N/A  N/A          109686      C   /usr/bin/python3                       1510MiB |
|    4   N/A  N/A          109687      C   /usr/bin/python3                       1510MiB |
|    5   N/A  N/A          109688      C   /usr/bin/python3                       1510MiB |
|    6   N/A  N/A          109689      C   /usr/bin/python3                       1510MiB |
|    7   N/A  N/A          109690      C   /usr/bin/python3                       1510MiB |
+-----------------------------------------------------------------------------------------+

====================================================================================================
step:0/1680 val_loss:10.8258 train_time:0ms step_avg:0.03ms
step:1/1680 train_time:153ms step_avg:153.12ms
step:2/1680 train_time:178ms step_avg:89.23ms
step:3/1680 train_time:238ms step_avg:79.26ms
step:4/1680 train_time:324ms step_avg:80.89ms
step:5/1680 train_time:410ms step_avg:82.05ms
step:6/1680 train_time:497ms step_avg:82.91ms
step:7/1680 train_time:586ms step_avg:83.69ms
step:8/1680 train_time:673ms step_avg:84.09ms
step:9/1680 train_time:760ms step_avg:84.44ms
step:10/1680 train_time:848ms step_avg:84.79ms
step:11/1680 train_time:935ms step_avg:84.98ms
step:12/1680 train_time:1023ms step_avg:85.23ms
step:13/1680 train_time:1113ms step_avg:85.60ms
step:14/1680 train_time:1203ms step_avg:85.95ms
step:15/1680 train_time:1291ms step_avg:86.08ms
step:16/1680 train_time:1379ms step_avg:86.20ms
step:17/1680 train_time:1466ms step_avg:86.26ms
step:18/1680 train_time:1554ms step_avg:86.36ms
step:19/1680 train_time:1642ms step_avg:86.45ms
step:20/1680 train_time:1730ms step_avg:86.49ms
step:21/1680 train_time:1817ms step_avg:86.55ms
step:22/1680 train_time:1905ms step_avg:86.61ms
step:23/1680 train_time:1993ms step_avg:86.66ms
step:24/1680 train_time:2082ms step_avg:86.76ms
step:25/1680 train_time:2171ms step_avg:86.82ms
step:26/1680 train_time:2260ms step_avg:86.92ms
step:27/1680 train_time:2349ms step_avg:87.02ms
step:28/1680 train_time:2438ms step_avg:87.07ms
step:29/1680 train_time:2526ms step_avg:87.12ms
step:30/1680 train_time:2615ms step_avg:87.16ms
step:31/1680 train_time:2703ms step_avg:87.18ms
step:32/1680 train_time:2790ms step_avg:87.20ms
step:33/1680 train_time:2878ms step_avg:87.20ms
step:34/1680 train_time:2965ms step_avg:87.22ms
step:35/1680 train_time:3053ms step_avg:87.23ms
step:36/1680 train_time:3141ms step_avg:87.26ms
step:37/1680 train_time:3230ms step_avg:87.29ms
step:38/1680 train_time:3318ms step_avg:87.33ms
step:39/1680 train_time:3407ms step_avg:87.36ms
step:40/1680 train_time:3495ms step_avg:87.37ms
step:41/1680 train_time:3583ms step_avg:87.39ms
step:42/1680 train_time:3670ms step_avg:87.39ms
step:43/1680 train_time:3759ms step_avg:87.42ms
step:44/1680 train_time:3848ms step_avg:87.45ms
step:45/1680 train_time:3935ms step_avg:87.45ms
step:46/1680 train_time:4023ms step_avg:87.46ms
step:47/1680 train_time:4110ms step_avg:87.45ms
step:48/1680 train_time:4198ms step_avg:87.47ms
step:49/1680 train_time:4286ms step_avg:87.47ms
step:50/1680 train_time:4375ms step_avg:87.49ms
step:51/1680 train_time:4462ms step_avg:87.50ms
step:52/1680 train_time:4550ms step_avg:87.51ms
step:53/1680 train_time:4639ms step_avg:87.52ms
step:54/1680 train_time:4726ms step_avg:87.53ms
step:55/1680 train_time:4816ms step_avg:87.56ms
step:56/1680 train_time:4903ms step_avg:87.55ms
step:57/1680 train_time:4991ms step_avg:87.57ms
step:58/1680 train_time:5079ms step_avg:87.57ms
step:59/1680 train_time:5167ms step_avg:87.58ms
step:60/1680 train_time:5255ms step_avg:87.59ms
step:61/1680 train_time:5343ms step_avg:87.59ms
step:62/1680 train_time:5431ms step_avg:87.60ms
step:63/1680 train_time:5519ms step_avg:87.60ms
step:64/1680 train_time:5607ms step_avg:87.62ms
step:65/1680 train_time:5695ms step_avg:87.62ms
step:66/1680 train_time:5783ms step_avg:87.62ms
step:67/1680 train_time:5870ms step_avg:87.62ms
step:68/1680 train_time:5958ms step_avg:87.62ms
step:69/1680 train_time:6047ms step_avg:87.63ms
step:70/1680 train_time:6135ms step_avg:87.64ms
step:71/1680 train_time:6223ms step_avg:87.65ms
step:72/1680 train_time:6312ms step_avg:87.66ms
step:73/1680 train_time:6400ms step_avg:87.67ms
step:74/1680 train_time:6489ms step_avg:87.69ms
step:75/1680 train_time:6578ms step_avg:87.70ms
step:76/1680 train_time:6666ms step_avg:87.71ms
step:77/1680 train_time:6754ms step_avg:87.72ms
step:78/1680 train_time:6842ms step_avg:87.72ms
step:79/1680 train_time:6929ms step_avg:87.71ms
step:80/1680 train_time:7017ms step_avg:87.71ms
step:81/1680 train_time:7104ms step_avg:87.71ms
step:82/1680 train_time:7192ms step_avg:87.70ms
step:83/1680 train_time:7280ms step_avg:87.71ms
step:84/1680 train_time:7368ms step_avg:87.72ms
step:85/1680 train_time:7456ms step_avg:87.72ms
step:86/1680 train_time:7545ms step_avg:87.73ms
step:87/1680 train_time:7632ms step_avg:87.73ms
step:88/1680 train_time:7721ms step_avg:87.74ms
step:89/1680 train_time:7809ms step_avg:87.75ms
step:90/1680 train_time:7897ms step_avg:87.74ms
step:91/1680 train_time:7986ms step_avg:87.75ms
step:92/1680 train_time:8073ms step_avg:87.76ms
step:93/1680 train_time:8162ms step_avg:87.76ms
step:94/1680 train_time:8250ms step_avg:87.76ms
step:95/1680 train_time:8338ms step_avg:87.76ms
step:96/1680 train_time:8426ms step_avg:87.77ms
step:97/1680 train_time:8514ms step_avg:87.77ms
step:98/1680 train_time:8602ms step_avg:87.78ms
step:99/1680 train_time:8690ms step_avg:87.78ms
step:100/1680 train_time:8778ms step_avg:87.78ms
step:101/1680 train_time:8866ms step_avg:87.78ms
step:102/1680 train_time:8954ms step_avg:87.79ms
step:103/1680 train_time:9043ms step_avg:87.79ms
step:104/1680 train_time:9130ms step_avg:87.79ms
step:105/1680 train_time:9218ms step_avg:87.79ms
step:106/1680 train_time:9306ms step_avg:87.80ms
step:107/1680 train_time:9394ms step_avg:87.80ms
step:108/1680 train_time:9482ms step_avg:87.80ms
step:109/1680 train_time:9570ms step_avg:87.80ms
step:110/1680 train_time:9658ms step_avg:87.80ms
step:111/1680 train_time:9746ms step_avg:87.80ms
step:112/1680 train_time:9833ms step_avg:87.80ms
step:113/1680 train_time:9921ms step_avg:87.80ms
step:114/1680 train_time:10010ms step_avg:87.80ms
step:115/1680 train_time:10098ms step_avg:87.81ms
step:116/1680 train_time:10186ms step_avg:87.81ms
step:117/1680 train_time:10274ms step_avg:87.81ms
step:118/1680 train_time:10362ms step_avg:87.82ms
step:119/1680 train_time:10450ms step_avg:87.82ms
step:120/1680 train_time:10539ms step_avg:87.83ms
step:121/1680 train_time:10627ms step_avg:87.83ms
step:122/1680 train_time:10716ms step_avg:87.84ms
step:123/1680 train_time:10803ms step_avg:87.83ms
step:124/1680 train_time:10891ms step_avg:87.83ms
step:125/1680 train_time:10979ms step_avg:87.83ms
step:125/1680 val_loss:4.3068 train_time:11068ms step_avg:88.54ms
step:126/1680 train_time:11093ms step_avg:88.04ms
step:127/1680 train_time:11158ms step_avg:87.85ms
step:128/1680 train_time:11256ms step_avg:87.94ms
step:129/1680 train_time:11349ms step_avg:87.97ms
step:130/1680 train_time:11438ms step_avg:87.98ms
step:131/1680 train_time:11526ms step_avg:87.99ms
step:132/1680 train_time:11613ms step_avg:87.98ms
step:133/1680 train_time:11700ms step_avg:87.97ms
step:134/1680 train_time:11787ms step_avg:87.97ms
step:135/1680 train_time:11875ms step_avg:87.96ms
step:136/1680 train_time:11961ms step_avg:87.95ms
step:137/1680 train_time:12048ms step_avg:87.94ms
step:138/1680 train_time:12136ms step_avg:87.94ms
step:139/1680 train_time:12224ms step_avg:87.94ms
step:140/1680 train_time:12315ms step_avg:87.96ms
step:141/1680 train_time:12404ms step_avg:87.97ms
step:142/1680 train_time:12493ms step_avg:87.98ms
step:143/1680 train_time:12580ms step_avg:87.97ms
step:144/1680 train_time:12668ms step_avg:87.97ms
step:145/1680 train_time:12756ms step_avg:87.97ms
step:146/1680 train_time:12842ms step_avg:87.96ms
step:147/1680 train_time:12930ms step_avg:87.96ms
step:148/1680 train_time:13017ms step_avg:87.95ms
step:149/1680 train_time:13104ms step_avg:87.95ms
step:150/1680 train_time:13193ms step_avg:87.95ms
step:151/1680 train_time:13281ms step_avg:87.95ms
step:152/1680 train_time:13370ms step_avg:87.96ms
step:153/1680 train_time:13458ms step_avg:87.96ms
step:154/1680 train_time:13547ms step_avg:87.97ms
step:155/1680 train_time:13634ms step_avg:87.96ms
step:156/1680 train_time:13722ms step_avg:87.96ms
step:157/1680 train_time:13809ms step_avg:87.96ms
step:158/1680 train_time:13898ms step_avg:87.96ms
step:159/1680 train_time:13986ms step_avg:87.96ms
step:160/1680 train_time:14073ms step_avg:87.96ms
step:161/1680 train_time:14161ms step_avg:87.95ms
step:162/1680 train_time:14248ms step_avg:87.95ms
step:163/1680 train_time:14338ms step_avg:87.97ms
step:164/1680 train_time:14426ms step_avg:87.97ms
step:165/1680 train_time:14515ms step_avg:87.97ms
step:166/1680 train_time:14603ms step_avg:87.97ms
step:167/1680 train_time:14691ms step_avg:87.97ms
step:168/1680 train_time:14778ms step_avg:87.97ms
step:169/1680 train_time:14866ms step_avg:87.97ms
step:170/1680 train_time:14953ms step_avg:87.96ms
step:171/1680 train_time:15041ms step_avg:87.96ms
step:172/1680 train_time:15129ms step_avg:87.96ms
step:173/1680 train_time:15216ms step_avg:87.96ms
step:174/1680 train_time:15305ms step_avg:87.96ms
step:175/1680 train_time:15393ms step_avg:87.96ms
step:176/1680 train_time:15480ms step_avg:87.96ms
step:177/1680 train_time:15569ms step_avg:87.96ms
step:178/1680 train_time:15657ms step_avg:87.96ms
step:179/1680 train_time:15744ms step_avg:87.95ms
step:180/1680 train_time:15831ms step_avg:87.95ms
step:181/1680 train_time:15919ms step_avg:87.95ms
step:182/1680 train_time:16007ms step_avg:87.95ms
step:183/1680 train_time:16095ms step_avg:87.95ms
step:184/1680 train_time:16184ms step_avg:87.95ms
step:185/1680 train_time:16271ms step_avg:87.95ms
step:186/1680 train_time:16359ms step_avg:87.95ms
step:187/1680 train_time:16447ms step_avg:87.95ms
step:188/1680 train_time:16535ms step_avg:87.95ms
step:189/1680 train_time:16623ms step_avg:87.95ms
step:190/1680 train_time:16711ms step_avg:87.95ms
step:191/1680 train_time:16800ms step_avg:87.96ms
step:192/1680 train_time:16887ms step_avg:87.96ms
step:193/1680 train_time:16975ms step_avg:87.95ms
step:194/1680 train_time:17063ms step_avg:87.95ms
step:195/1680 train_time:17150ms step_avg:87.95ms
step:196/1680 train_time:17239ms step_avg:87.96ms
step:197/1680 train_time:17327ms step_avg:87.96ms
step:198/1680 train_time:17415ms step_avg:87.95ms
step:199/1680 train_time:17502ms step_avg:87.95ms
step:200/1680 train_time:17590ms step_avg:87.95ms
step:201/1680 train_time:17678ms step_avg:87.95ms
step:202/1680 train_time:17766ms step_avg:87.95ms
step:203/1680 train_time:17853ms step_avg:87.95ms
step:204/1680 train_time:17941ms step_avg:87.94ms
step:205/1680 train_time:18029ms step_avg:87.94ms
step:206/1680 train_time:18116ms step_avg:87.94ms
step:207/1680 train_time:18205ms step_avg:87.94ms
step:208/1680 train_time:18293ms step_avg:87.95ms
step:209/1680 train_time:18380ms step_avg:87.94ms
step:210/1680 train_time:18468ms step_avg:87.94ms
step:211/1680 train_time:18555ms step_avg:87.94ms
step:212/1680 train_time:18643ms step_avg:87.94ms
step:213/1680 train_time:18731ms step_avg:87.94ms
step:214/1680 train_time:18819ms step_avg:87.94ms
step:215/1680 train_time:18906ms step_avg:87.94ms
step:216/1680 train_time:18994ms step_avg:87.94ms
step:217/1680 train_time:19081ms step_avg:87.93ms
step:218/1680 train_time:19169ms step_avg:87.93ms
step:219/1680 train_time:19257ms step_avg:87.93ms
step:220/1680 train_time:19344ms step_avg:87.93ms
step:221/1680 train_time:19432ms step_avg:87.93ms
step:222/1680 train_time:19520ms step_avg:87.93ms
step:223/1680 train_time:19608ms step_avg:87.93ms
step:224/1680 train_time:19696ms step_avg:87.93ms
step:225/1680 train_time:19784ms step_avg:87.93ms
step:226/1680 train_time:19873ms step_avg:87.93ms
step:227/1680 train_time:19961ms step_avg:87.93ms
step:228/1680 train_time:20049ms step_avg:87.93ms
step:229/1680 train_time:20138ms step_avg:87.94ms
step:230/1680 train_time:20225ms step_avg:87.94ms
step:231/1680 train_time:20313ms step_avg:87.94ms
step:232/1680 train_time:20401ms step_avg:87.93ms
step:233/1680 train_time:20489ms step_avg:87.93ms
step:234/1680 train_time:20577ms step_avg:87.93ms
step:235/1680 train_time:20664ms step_avg:87.93ms
step:236/1680 train_time:20753ms step_avg:87.94ms
step:237/1680 train_time:20840ms step_avg:87.93ms
step:238/1680 train_time:20928ms step_avg:87.93ms
step:239/1680 train_time:21016ms step_avg:87.93ms
step:240/1680 train_time:21105ms step_avg:87.94ms
step:241/1680 train_time:21193ms step_avg:87.94ms
step:242/1680 train_time:21281ms step_avg:87.94ms
step:243/1680 train_time:21369ms step_avg:87.94ms
step:244/1680 train_time:21456ms step_avg:87.94ms
step:245/1680 train_time:21543ms step_avg:87.93ms
step:246/1680 train_time:21632ms step_avg:87.93ms
step:247/1680 train_time:21720ms step_avg:87.94ms
step:248/1680 train_time:21808ms step_avg:87.94ms
step:249/1680 train_time:21896ms step_avg:87.94ms
step:250/1680 train_time:21984ms step_avg:87.94ms
step:250/1680 val_loss:3.9723 train_time:22073ms step_avg:88.29ms
step:251/1680 train_time:22098ms step_avg:88.04ms
step:252/1680 train_time:22164ms step_avg:87.95ms
step:253/1680 train_time:22258ms step_avg:87.98ms
step:254/1680 train_time:22348ms step_avg:87.98ms
step:255/1680 train_time:22436ms step_avg:87.98ms
step:256/1680 train_time:22524ms step_avg:87.98ms
step:257/1680 train_time:22611ms step_avg:87.98ms
step:258/1680 train_time:22698ms step_avg:87.98ms
step:259/1680 train_time:22785ms step_avg:87.97ms
step:260/1680 train_time:22872ms step_avg:87.97ms
step:261/1680 train_time:22959ms step_avg:87.97ms
step:262/1680 train_time:23048ms step_avg:87.97ms
step:263/1680 train_time:23136ms step_avg:87.97ms
step:264/1680 train_time:23226ms step_avg:87.98ms
step:265/1680 train_time:23315ms step_avg:87.98ms
step:266/1680 train_time:23404ms step_avg:87.98ms
step:267/1680 train_time:23492ms step_avg:87.98ms
step:268/1680 train_time:23579ms step_avg:87.98ms
step:269/1680 train_time:23666ms step_avg:87.98ms
step:270/1680 train_time:23753ms step_avg:87.98ms
step:271/1680 train_time:23841ms step_avg:87.97ms
step:272/1680 train_time:23928ms step_avg:87.97ms
step:273/1680 train_time:24015ms step_avg:87.97ms
step:274/1680 train_time:24103ms step_avg:87.97ms
step:275/1680 train_time:24192ms step_avg:87.97ms
step:276/1680 train_time:24280ms step_avg:87.97ms
step:277/1680 train_time:24368ms step_avg:87.97ms
step:278/1680 train_time:24456ms step_avg:87.97ms
step:279/1680 train_time:24544ms step_avg:87.97ms
step:280/1680 train_time:24632ms step_avg:87.97ms
step:281/1680 train_time:24720ms step_avg:87.97ms
step:282/1680 train_time:24808ms step_avg:87.97ms
step:283/1680 train_time:24895ms step_avg:87.97ms
step:284/1680 train_time:24982ms step_avg:87.96ms
step:285/1680 train_time:25069ms step_avg:87.96ms
step:286/1680 train_time:25158ms step_avg:87.97ms
step:287/1680 train_time:25247ms step_avg:87.97ms
step:288/1680 train_time:25335ms step_avg:87.97ms
step:289/1680 train_time:25423ms step_avg:87.97ms
step:290/1680 train_time:25512ms step_avg:87.97ms
step:291/1680 train_time:25599ms step_avg:87.97ms
step:292/1680 train_time:25687ms step_avg:87.97ms
step:293/1680 train_time:25774ms step_avg:87.97ms
step:294/1680 train_time:25862ms step_avg:87.97ms
step:295/1680 train_time:25950ms step_avg:87.97ms
step:296/1680 train_time:26038ms step_avg:87.97ms
step:297/1680 train_time:26125ms step_avg:87.96ms
step:298/1680 train_time:26213ms step_avg:87.96ms
step:299/1680 train_time:26301ms step_avg:87.96ms
step:300/1680 train_time:26389ms step_avg:87.96ms
step:301/1680 train_time:26478ms step_avg:87.97ms
step:302/1680 train_time:26566ms step_avg:87.97ms
step:303/1680 train_time:26654ms step_avg:87.97ms
step:304/1680 train_time:26741ms step_avg:87.96ms
step:305/1680 train_time:26829ms step_avg:87.96ms
step:306/1680 train_time:26917ms step_avg:87.96ms
step:307/1680 train_time:27005ms step_avg:87.96ms
step:308/1680 train_time:27093ms step_avg:87.96ms
step:309/1680 train_time:27181ms step_avg:87.96ms
step:310/1680 train_time:27268ms step_avg:87.96ms
step:311/1680 train_time:27356ms step_avg:87.96ms
step:312/1680 train_time:27445ms step_avg:87.96ms
step:313/1680 train_time:27532ms step_avg:87.96ms
step:314/1680 train_time:27621ms step_avg:87.96ms
step:315/1680 train_time:27708ms step_avg:87.96ms
step:316/1680 train_time:27795ms step_avg:87.96ms
step:317/1680 train_time:27883ms step_avg:87.96ms
step:318/1680 train_time:27972ms step_avg:87.96ms
step:319/1680 train_time:28060ms step_avg:87.96ms
step:320/1680 train_time:28148ms step_avg:87.96ms
step:321/1680 train_time:28236ms step_avg:87.96ms
step:322/1680 train_time:28324ms step_avg:87.96ms
step:323/1680 train_time:28412ms step_avg:87.96ms
step:324/1680 train_time:28500ms step_avg:87.96ms
step:325/1680 train_time:28588ms step_avg:87.96ms
step:326/1680 train_time:28676ms step_avg:87.96ms
step:327/1680 train_time:28764ms step_avg:87.96ms
step:328/1680 train_time:28851ms step_avg:87.96ms
step:329/1680 train_time:28939ms step_avg:87.96ms
step:330/1680 train_time:29027ms step_avg:87.96ms
step:331/1680 train_time:29115ms step_avg:87.96ms
step:332/1680 train_time:29204ms step_avg:87.96ms
step:333/1680 train_time:29292ms step_avg:87.96ms
step:334/1680 train_time:29380ms step_avg:87.96ms
step:335/1680 train_time:29468ms step_avg:87.97ms
step:336/1680 train_time:29556ms step_avg:87.97ms
step:337/1680 train_time:29644ms step_avg:87.97ms
step:338/1680 train_time:29732ms step_avg:87.96ms
step:339/1680 train_time:29820ms step_avg:87.96ms
step:340/1680 train_time:29908ms step_avg:87.96ms
step:341/1680 train_time:29996ms step_avg:87.96ms
step:342/1680 train_time:30084ms step_avg:87.96ms
step:343/1680 train_time:30172ms step_avg:87.96ms
step:344/1680 train_time:30260ms step_avg:87.96ms
step:345/1680 train_time:30348ms step_avg:87.97ms
step:346/1680 train_time:30437ms step_avg:87.97ms
step:347/1680 train_time:30525ms step_avg:87.97ms
step:348/1680 train_time:30613ms step_avg:87.97ms
step:349/1680 train_time:30700ms step_avg:87.97ms
step:350/1680 train_time:30788ms step_avg:87.97ms
step:351/1680 train_time:30875ms step_avg:87.96ms
step:352/1680 train_time:30963ms step_avg:87.96ms
step:353/1680 train_time:31051ms step_avg:87.96ms
step:354/1680 train_time:31139ms step_avg:87.96ms
step:355/1680 train_time:31227ms step_avg:87.96ms
step:356/1680 train_time:31314ms step_avg:87.96ms
step:357/1680 train_time:31402ms step_avg:87.96ms
step:358/1680 train_time:31491ms step_avg:87.96ms
step:359/1680 train_time:31579ms step_avg:87.96ms
step:360/1680 train_time:31668ms step_avg:87.97ms
step:361/1680 train_time:31755ms step_avg:87.96ms
step:362/1680 train_time:31843ms step_avg:87.96ms
step:363/1680 train_time:31930ms step_avg:87.96ms
step:364/1680 train_time:32018ms step_avg:87.96ms
step:365/1680 train_time:32106ms step_avg:87.96ms
step:366/1680 train_time:32194ms step_avg:87.96ms
step:367/1680 train_time:32281ms step_avg:87.96ms
step:368/1680 train_time:32369ms step_avg:87.96ms
step:369/1680 train_time:32457ms step_avg:87.96ms
step:370/1680 train_time:32546ms step_avg:87.96ms
step:371/1680 train_time:32633ms step_avg:87.96ms
step:372/1680 train_time:32721ms step_avg:87.96ms
step:373/1680 train_time:32809ms step_avg:87.96ms
step:374/1680 train_time:32897ms step_avg:87.96ms
step:375/1680 train_time:32985ms step_avg:87.96ms
step:375/1680 val_loss:3.8198 train_time:33074ms step_avg:88.20ms
step:376/1680 train_time:33099ms step_avg:88.03ms
step:377/1680 train_time:33163ms step_avg:87.97ms
step:378/1680 train_time:33257ms step_avg:87.98ms
step:379/1680 train_time:33349ms step_avg:87.99ms
step:380/1680 train_time:33438ms step_avg:87.99ms
step:381/1680 train_time:33524ms step_avg:87.99ms
step:382/1680 train_time:33612ms step_avg:87.99ms
step:383/1680 train_time:33699ms step_avg:87.99ms
step:384/1680 train_time:33785ms step_avg:87.98ms
step:385/1680 train_time:33872ms step_avg:87.98ms
step:386/1680 train_time:33959ms step_avg:87.98ms
step:387/1680 train_time:34046ms step_avg:87.98ms
step:388/1680 train_time:34136ms step_avg:87.98ms
step:389/1680 train_time:34226ms step_avg:87.98ms
step:390/1680 train_time:34315ms step_avg:87.99ms
step:391/1680 train_time:34405ms step_avg:87.99ms
step:392/1680 train_time:34492ms step_avg:87.99ms
step:393/1680 train_time:34580ms step_avg:87.99ms
step:394/1680 train_time:34668ms step_avg:87.99ms
step:395/1680 train_time:34756ms step_avg:87.99ms
step:396/1680 train_time:34844ms step_avg:87.99ms
step:397/1680 train_time:34931ms step_avg:87.99ms
step:398/1680 train_time:35018ms step_avg:87.98ms
step:399/1680 train_time:35106ms step_avg:87.98ms
step:400/1680 train_time:35194ms step_avg:87.99ms
step:401/1680 train_time:35284ms step_avg:87.99ms
step:402/1680 train_time:35373ms step_avg:87.99ms
step:403/1680 train_time:35460ms step_avg:87.99ms
step:404/1680 train_time:35548ms step_avg:87.99ms
step:405/1680 train_time:35636ms step_avg:87.99ms
step:406/1680 train_time:35723ms step_avg:87.99ms
step:407/1680 train_time:35811ms step_avg:87.99ms
step:408/1680 train_time:35898ms step_avg:87.99ms
step:409/1680 train_time:35985ms step_avg:87.98ms
step:410/1680 train_time:36072ms step_avg:87.98ms
step:411/1680 train_time:36161ms step_avg:87.98ms
step:412/1680 train_time:36249ms step_avg:87.98ms
step:413/1680 train_time:36338ms step_avg:87.98ms
step:414/1680 train_time:36425ms step_avg:87.98ms
step:415/1680 train_time:36513ms step_avg:87.98ms
step:416/1680 train_time:36602ms step_avg:87.99ms
step:417/1680 train_time:36690ms step_avg:87.98ms
step:418/1680 train_time:36777ms step_avg:87.98ms
step:419/1680 train_time:36865ms step_avg:87.98ms
step:420/1680 train_time:36954ms step_avg:87.98ms
step:421/1680 train_time:37042ms step_avg:87.99ms
step:422/1680 train_time:37130ms step_avg:87.99ms
step:423/1680 train_time:37218ms step_avg:87.98ms
step:424/1680 train_time:37305ms step_avg:87.98ms
step:425/1680 train_time:37393ms step_avg:87.98ms
step:426/1680 train_time:37482ms step_avg:87.98ms
step:427/1680 train_time:37569ms step_avg:87.98ms
step:428/1680 train_time:37657ms step_avg:87.98ms
step:429/1680 train_time:37745ms step_avg:87.98ms
step:430/1680 train_time:37832ms step_avg:87.98ms
step:431/1680 train_time:37920ms step_avg:87.98ms
step:432/1680 train_time:38009ms step_avg:87.98ms
step:433/1680 train_time:38096ms step_avg:87.98ms
step:434/1680 train_time:38184ms step_avg:87.98ms
step:435/1680 train_time:38272ms step_avg:87.98ms
step:436/1680 train_time:38361ms step_avg:87.98ms
step:437/1680 train_time:38449ms step_avg:87.98ms
step:438/1680 train_time:38537ms step_avg:87.98ms
step:439/1680 train_time:38625ms step_avg:87.98ms
step:440/1680 train_time:38712ms step_avg:87.98ms
step:441/1680 train_time:38800ms step_avg:87.98ms
step:442/1680 train_time:38887ms step_avg:87.98ms
step:443/1680 train_time:38975ms step_avg:87.98ms
step:444/1680 train_time:39063ms step_avg:87.98ms
step:445/1680 train_time:39152ms step_avg:87.98ms
step:446/1680 train_time:39240ms step_avg:87.98ms
step:447/1680 train_time:39328ms step_avg:87.98ms
step:448/1680 train_time:39415ms step_avg:87.98ms
step:449/1680 train_time:39503ms step_avg:87.98ms
step:450/1680 train_time:39591ms step_avg:87.98ms
step:451/1680 train_time:39678ms step_avg:87.98ms
step:452/1680 train_time:39765ms step_avg:87.98ms
step:453/1680 train_time:39853ms step_avg:87.98ms
step:454/1680 train_time:39941ms step_avg:87.98ms
step:455/1680 train_time:40029ms step_avg:87.98ms
step:456/1680 train_time:40118ms step_avg:87.98ms
step:457/1680 train_time:40206ms step_avg:87.98ms
step:458/1680 train_time:40294ms step_avg:87.98ms
step:459/1680 train_time:40384ms step_avg:87.98ms
step:460/1680 train_time:40471ms step_avg:87.98ms
step:461/1680 train_time:40559ms step_avg:87.98ms
step:462/1680 train_time:40646ms step_avg:87.98ms
step:463/1680 train_time:40734ms step_avg:87.98ms
step:464/1680 train_time:40821ms step_avg:87.98ms
step:465/1680 train_time:40909ms step_avg:87.98ms
step:466/1680 train_time:40997ms step_avg:87.98ms
step:467/1680 train_time:41086ms step_avg:87.98ms
step:468/1680 train_time:41174ms step_avg:87.98ms
step:469/1680 train_time:41262ms step_avg:87.98ms
step:470/1680 train_time:41351ms step_avg:87.98ms
step:471/1680 train_time:41439ms step_avg:87.98ms
step:472/1680 train_time:41527ms step_avg:87.98ms
step:473/1680 train_time:41615ms step_avg:87.98ms
step:474/1680 train_time:41703ms step_avg:87.98ms
step:475/1680 train_time:41790ms step_avg:87.98ms
step:476/1680 train_time:41877ms step_avg:87.98ms
step:477/1680 train_time:41965ms step_avg:87.98ms
step:478/1680 train_time:42052ms step_avg:87.98ms
step:479/1680 train_time:42141ms step_avg:87.98ms
step:480/1680 train_time:42229ms step_avg:87.98ms
step:481/1680 train_time:42317ms step_avg:87.98ms
step:482/1680 train_time:42406ms step_avg:87.98ms
step:483/1680 train_time:42494ms step_avg:87.98ms
step:484/1680 train_time:42582ms step_avg:87.98ms
step:485/1680 train_time:42670ms step_avg:87.98ms
step:486/1680 train_time:42758ms step_avg:87.98ms
step:487/1680 train_time:42846ms step_avg:87.98ms
step:488/1680 train_time:42933ms step_avg:87.98ms
step:489/1680 train_time:43021ms step_avg:87.98ms
step:490/1680 train_time:43109ms step_avg:87.98ms
step:491/1680 train_time:43197ms step_avg:87.98ms
step:492/1680 train_time:43285ms step_avg:87.98ms
step:493/1680 train_time:43373ms step_avg:87.98ms
step:494/1680 train_time:43461ms step_avg:87.98ms
step:495/1680 train_time:43549ms step_avg:87.98ms
step:496/1680 train_time:43637ms step_avg:87.98ms
step:497/1680 train_time:43724ms step_avg:87.98ms
step:498/1680 train_time:43812ms step_avg:87.98ms
step:499/1680 train_time:43900ms step_avg:87.98ms
step:500/1680 train_time:43988ms step_avg:87.98ms
step:500/1680 val_loss:3.7212 train_time:44076ms step_avg:88.15ms
step:501/1680 train_time:44101ms step_avg:88.03ms
step:502/1680 train_time:44166ms step_avg:87.98ms
step:503/1680 train_time:44260ms step_avg:87.99ms
step:504/1680 train_time:44352ms step_avg:88.00ms
step:505/1680 train_time:44439ms step_avg:88.00ms
step:506/1680 train_time:44526ms step_avg:88.00ms
step:507/1680 train_time:44614ms step_avg:88.00ms
step:508/1680 train_time:44700ms step_avg:87.99ms
step:509/1680 train_time:44788ms step_avg:87.99ms
step:510/1680 train_time:44875ms step_avg:87.99ms
step:511/1680 train_time:44961ms step_avg:87.99ms
step:512/1680 train_time:45050ms step_avg:87.99ms
step:513/1680 train_time:45138ms step_avg:87.99ms
step:514/1680 train_time:45228ms step_avg:87.99ms
step:515/1680 train_time:45317ms step_avg:87.99ms
step:516/1680 train_time:45405ms step_avg:87.99ms
step:517/1680 train_time:45493ms step_avg:87.99ms
step:518/1680 train_time:45581ms step_avg:87.99ms
step:519/1680 train_time:45668ms step_avg:87.99ms
step:520/1680 train_time:45755ms step_avg:87.99ms
step:521/1680 train_time:45842ms step_avg:87.99ms
step:522/1680 train_time:45929ms step_avg:87.99ms
step:523/1680 train_time:46016ms step_avg:87.99ms
step:524/1680 train_time:46105ms step_avg:87.99ms
step:525/1680 train_time:46194ms step_avg:87.99ms
step:526/1680 train_time:46282ms step_avg:87.99ms
step:527/1680 train_time:46370ms step_avg:87.99ms
step:528/1680 train_time:46458ms step_avg:87.99ms
step:529/1680 train_time:46546ms step_avg:87.99ms
step:530/1680 train_time:46634ms step_avg:87.99ms
step:531/1680 train_time:46722ms step_avg:87.99ms
step:532/1680 train_time:46809ms step_avg:87.99ms
step:533/1680 train_time:46896ms step_avg:87.99ms
step:534/1680 train_time:46984ms step_avg:87.98ms
step:535/1680 train_time:47072ms step_avg:87.98ms
step:536/1680 train_time:47160ms step_avg:87.98ms
step:537/1680 train_time:47248ms step_avg:87.99ms
step:538/1680 train_time:47336ms step_avg:87.99ms
step:539/1680 train_time:47424ms step_avg:87.98ms
step:540/1680 train_time:47512ms step_avg:87.99ms
step:541/1680 train_time:47600ms step_avg:87.99ms
step:542/1680 train_time:47687ms step_avg:87.98ms
step:543/1680 train_time:47775ms step_avg:87.98ms
step:544/1680 train_time:47862ms step_avg:87.98ms
step:545/1680 train_time:47950ms step_avg:87.98ms
step:546/1680 train_time:48037ms step_avg:87.98ms
step:547/1680 train_time:48125ms step_avg:87.98ms
step:548/1680 train_time:48214ms step_avg:87.98ms
step:549/1680 train_time:48303ms step_avg:87.98ms
step:550/1680 train_time:48392ms step_avg:87.99ms
step:551/1680 train_time:48482ms step_avg:87.99ms
step:552/1680 train_time:48570ms step_avg:87.99ms
step:553/1680 train_time:48660ms step_avg:87.99ms
step:554/1680 train_time:48748ms step_avg:87.99ms
step:555/1680 train_time:48837ms step_avg:87.99ms
step:556/1680 train_time:48926ms step_avg:88.00ms
step:557/1680 train_time:49014ms step_avg:88.00ms
step:558/1680 train_time:49103ms step_avg:88.00ms
step:559/1680 train_time:49193ms step_avg:88.00ms
step:560/1680 train_time:49282ms step_avg:88.00ms
step:561/1680 train_time:49371ms step_avg:88.01ms
step:562/1680 train_time:49460ms step_avg:88.01ms
step:563/1680 train_time:49549ms step_avg:88.01ms
step:564/1680 train_time:49638ms step_avg:88.01ms
step:565/1680 train_time:49727ms step_avg:88.01ms
step:566/1680 train_time:49816ms step_avg:88.01ms
step:567/1680 train_time:49905ms step_avg:88.02ms
step:568/1680 train_time:49994ms step_avg:88.02ms
step:569/1680 train_time:50083ms step_avg:88.02ms
step:570/1680 train_time:50172ms step_avg:88.02ms
step:571/1680 train_time:50261ms step_avg:88.02ms
step:572/1680 train_time:50350ms step_avg:88.02ms
step:573/1680 train_time:50439ms step_avg:88.03ms
step:574/1680 train_time:50528ms step_avg:88.03ms
step:575/1680 train_time:50617ms step_avg:88.03ms
step:576/1680 train_time:50706ms step_avg:88.03ms
step:577/1680 train_time:50795ms step_avg:88.03ms
step:578/1680 train_time:50883ms step_avg:88.03ms
step:579/1680 train_time:50972ms step_avg:88.03ms
step:580/1680 train_time:51061ms step_avg:88.04ms
step:581/1680 train_time:51149ms step_avg:88.04ms
step:582/1680 train_time:51238ms step_avg:88.04ms
step:583/1680 train_time:51328ms step_avg:88.04ms
step:584/1680 train_time:51417ms step_avg:88.04ms
step:585/1680 train_time:51506ms step_avg:88.04ms
step:586/1680 train_time:51596ms step_avg:88.05ms
step:587/1680 train_time:51685ms step_avg:88.05ms
step:588/1680 train_time:51774ms step_avg:88.05ms
step:589/1680 train_time:51863ms step_avg:88.05ms
step:590/1680 train_time:51952ms step_avg:88.05ms
step:591/1680 train_time:52040ms step_avg:88.05ms
step:592/1680 train_time:52129ms step_avg:88.06ms
step:593/1680 train_time:52218ms step_avg:88.06ms
step:594/1680 train_time:52308ms step_avg:88.06ms
step:595/1680 train_time:52396ms step_avg:88.06ms
step:596/1680 train_time:52485ms step_avg:88.06ms
step:597/1680 train_time:52574ms step_avg:88.06ms
step:598/1680 train_time:52664ms step_avg:88.07ms
step:599/1680 train_time:52754ms step_avg:88.07ms
step:600/1680 train_time:52843ms step_avg:88.07ms
step:601/1680 train_time:52932ms step_avg:88.07ms
step:602/1680 train_time:53021ms step_avg:88.08ms
step:603/1680 train_time:53110ms step_avg:88.08ms
step:604/1680 train_time:53199ms step_avg:88.08ms
step:605/1680 train_time:53289ms step_avg:88.08ms
step:606/1680 train_time:53378ms step_avg:88.08ms
step:607/1680 train_time:53467ms step_avg:88.08ms
step:608/1680 train_time:53556ms step_avg:88.09ms
step:609/1680 train_time:53645ms step_avg:88.09ms
step:610/1680 train_time:53735ms step_avg:88.09ms
step:611/1680 train_time:53824ms step_avg:88.09ms
step:612/1680 train_time:53914ms step_avg:88.09ms
step:613/1680 train_time:54003ms step_avg:88.10ms
step:614/1680 train_time:54092ms step_avg:88.10ms
step:615/1680 train_time:54181ms step_avg:88.10ms
step:616/1680 train_time:54270ms step_avg:88.10ms
step:617/1680 train_time:54358ms step_avg:88.10ms
step:618/1680 train_time:54447ms step_avg:88.10ms
step:619/1680 train_time:54537ms step_avg:88.11ms
step:620/1680 train_time:54626ms step_avg:88.11ms
step:621/1680 train_time:54716ms step_avg:88.11ms
step:622/1680 train_time:54805ms step_avg:88.11ms
step:623/1680 train_time:54895ms step_avg:88.11ms
step:624/1680 train_time:54984ms step_avg:88.12ms
step:625/1680 train_time:55072ms step_avg:88.12ms
step:625/1680 val_loss:3.6174 train_time:55163ms step_avg:88.26ms
step:626/1680 train_time:55188ms step_avg:88.16ms
step:627/1680 train_time:55252ms step_avg:88.12ms
step:628/1680 train_time:55348ms step_avg:88.13ms
step:629/1680 train_time:55441ms step_avg:88.14ms
step:630/1680 train_time:55529ms step_avg:88.14ms
step:631/1680 train_time:55618ms step_avg:88.14ms
step:632/1680 train_time:55706ms step_avg:88.14ms
step:633/1680 train_time:55795ms step_avg:88.14ms
step:634/1680 train_time:55883ms step_avg:88.14ms
step:635/1680 train_time:55971ms step_avg:88.14ms
step:636/1680 train_time:56058ms step_avg:88.14ms
step:637/1680 train_time:56146ms step_avg:88.14ms
step:638/1680 train_time:56236ms step_avg:88.14ms
step:639/1680 train_time:56328ms step_avg:88.15ms
step:640/1680 train_time:56418ms step_avg:88.15ms
step:641/1680 train_time:56508ms step_avg:88.16ms
step:642/1680 train_time:56598ms step_avg:88.16ms
step:643/1680 train_time:56686ms step_avg:88.16ms
step:644/1680 train_time:56776ms step_avg:88.16ms
step:645/1680 train_time:56864ms step_avg:88.16ms
step:646/1680 train_time:56952ms step_avg:88.16ms
step:647/1680 train_time:57041ms step_avg:88.16ms
step:648/1680 train_time:57130ms step_avg:88.16ms
step:649/1680 train_time:57219ms step_avg:88.16ms
step:650/1680 train_time:57308ms step_avg:88.17ms
step:651/1680 train_time:57399ms step_avg:88.17ms
step:652/1680 train_time:57489ms step_avg:88.17ms
step:653/1680 train_time:57578ms step_avg:88.17ms
step:654/1680 train_time:57667ms step_avg:88.18ms
step:655/1680 train_time:57756ms step_avg:88.18ms
step:656/1680 train_time:57844ms step_avg:88.18ms
step:657/1680 train_time:57933ms step_avg:88.18ms
step:658/1680 train_time:58021ms step_avg:88.18ms
step:659/1680 train_time:58109ms step_avg:88.18ms
step:660/1680 train_time:58198ms step_avg:88.18ms
step:661/1680 train_time:58288ms step_avg:88.18ms
step:662/1680 train_time:58378ms step_avg:88.18ms
step:663/1680 train_time:58469ms step_avg:88.19ms
step:664/1680 train_time:58557ms step_avg:88.19ms
step:665/1680 train_time:58646ms step_avg:88.19ms
step:666/1680 train_time:58735ms step_avg:88.19ms
step:667/1680 train_time:58823ms step_avg:88.19ms
step:668/1680 train_time:58912ms step_avg:88.19ms
step:669/1680 train_time:59001ms step_avg:88.19ms
step:670/1680 train_time:59090ms step_avg:88.19ms
step:671/1680 train_time:59179ms step_avg:88.19ms
step:672/1680 train_time:59269ms step_avg:88.20ms
step:673/1680 train_time:59358ms step_avg:88.20ms
step:674/1680 train_time:59448ms step_avg:88.20ms
step:675/1680 train_time:59538ms step_avg:88.20ms
step:676/1680 train_time:59626ms step_avg:88.20ms
step:677/1680 train_time:59716ms step_avg:88.21ms
step:678/1680 train_time:59804ms step_avg:88.21ms
step:679/1680 train_time:59893ms step_avg:88.21ms
step:680/1680 train_time:59981ms step_avg:88.21ms
step:681/1680 train_time:60070ms step_avg:88.21ms
step:682/1680 train_time:60159ms step_avg:88.21ms
step:683/1680 train_time:60249ms step_avg:88.21ms
step:684/1680 train_time:60338ms step_avg:88.21ms
step:685/1680 train_time:60427ms step_avg:88.21ms
step:686/1680 train_time:60517ms step_avg:88.22ms
step:687/1680 train_time:60606ms step_avg:88.22ms
step:688/1680 train_time:60696ms step_avg:88.22ms
step:689/1680 train_time:60784ms step_avg:88.22ms
step:690/1680 train_time:60873ms step_avg:88.22ms
step:691/1680 train_time:60962ms step_avg:88.22ms
step:692/1680 train_time:61050ms step_avg:88.22ms
step:693/1680 train_time:61139ms step_avg:88.22ms
step:694/1680 train_time:61229ms step_avg:88.23ms
step:695/1680 train_time:61318ms step_avg:88.23ms
step:696/1680 train_time:61407ms step_avg:88.23ms
step:697/1680 train_time:61497ms step_avg:88.23ms
step:698/1680 train_time:61586ms step_avg:88.23ms
step:699/1680 train_time:61676ms step_avg:88.23ms
step:700/1680 train_time:61765ms step_avg:88.24ms
step:701/1680 train_time:61855ms step_avg:88.24ms
step:702/1680 train_time:61943ms step_avg:88.24ms
step:703/1680 train_time:62032ms step_avg:88.24ms
step:704/1680 train_time:62121ms step_avg:88.24ms
step:705/1680 train_time:62210ms step_avg:88.24ms
step:706/1680 train_time:62299ms step_avg:88.24ms
step:707/1680 train_time:62387ms step_avg:88.24ms
step:708/1680 train_time:62477ms step_avg:88.24ms
step:709/1680 train_time:62566ms step_avg:88.25ms
step:710/1680 train_time:62655ms step_avg:88.25ms
step:711/1680 train_time:62744ms step_avg:88.25ms
step:712/1680 train_time:62833ms step_avg:88.25ms
step:713/1680 train_time:62922ms step_avg:88.25ms
step:714/1680 train_time:63010ms step_avg:88.25ms
step:715/1680 train_time:63099ms step_avg:88.25ms
step:716/1680 train_time:63187ms step_avg:88.25ms
step:717/1680 train_time:63277ms step_avg:88.25ms
step:718/1680 train_time:63365ms step_avg:88.25ms
step:719/1680 train_time:63454ms step_avg:88.25ms
step:720/1680 train_time:63542ms step_avg:88.25ms
step:721/1680 train_time:63632ms step_avg:88.25ms
step:722/1680 train_time:63722ms step_avg:88.26ms
step:723/1680 train_time:63811ms step_avg:88.26ms
step:724/1680 train_time:63900ms step_avg:88.26ms
step:725/1680 train_time:63989ms step_avg:88.26ms
step:726/1680 train_time:64078ms step_avg:88.26ms
step:727/1680 train_time:64167ms step_avg:88.26ms
step:728/1680 train_time:64256ms step_avg:88.26ms
step:729/1680 train_time:64344ms step_avg:88.26ms
step:730/1680 train_time:64433ms step_avg:88.27ms
step:731/1680 train_time:64522ms step_avg:88.27ms
step:732/1680 train_time:64612ms step_avg:88.27ms
step:733/1680 train_time:64702ms step_avg:88.27ms
step:734/1680 train_time:64791ms step_avg:88.27ms
step:735/1680 train_time:64880ms step_avg:88.27ms
step:736/1680 train_time:64969ms step_avg:88.27ms
step:737/1680 train_time:65058ms step_avg:88.27ms
step:738/1680 train_time:65147ms step_avg:88.27ms
step:739/1680 train_time:65236ms step_avg:88.28ms
step:740/1680 train_time:65325ms step_avg:88.28ms
step:741/1680 train_time:65414ms step_avg:88.28ms
step:742/1680 train_time:65503ms step_avg:88.28ms
step:743/1680 train_time:65592ms step_avg:88.28ms
step:744/1680 train_time:65681ms step_avg:88.28ms
step:745/1680 train_time:65770ms step_avg:88.28ms
step:746/1680 train_time:65859ms step_avg:88.28ms
step:747/1680 train_time:65948ms step_avg:88.28ms
step:748/1680 train_time:66037ms step_avg:88.28ms
step:749/1680 train_time:66127ms step_avg:88.29ms
step:750/1680 train_time:66216ms step_avg:88.29ms
step:750/1680 val_loss:3.5664 train_time:66306ms step_avg:88.41ms
step:751/1680 train_time:66330ms step_avg:88.32ms
step:752/1680 train_time:66398ms step_avg:88.30ms
step:753/1680 train_time:66491ms step_avg:88.30ms
step:754/1680 train_time:66582ms step_avg:88.31ms
step:755/1680 train_time:66671ms step_avg:88.31ms
step:756/1680 train_time:66759ms step_avg:88.31ms
step:757/1680 train_time:66847ms step_avg:88.31ms
step:758/1680 train_time:66935ms step_avg:88.30ms
step:759/1680 train_time:67023ms step_avg:88.30ms
step:760/1680 train_time:67111ms step_avg:88.30ms
step:761/1680 train_time:67198ms step_avg:88.30ms
step:762/1680 train_time:67287ms step_avg:88.30ms
step:763/1680 train_time:67378ms step_avg:88.31ms
step:764/1680 train_time:67469ms step_avg:88.31ms
step:765/1680 train_time:67560ms step_avg:88.31ms
step:766/1680 train_time:67650ms step_avg:88.32ms
step:767/1680 train_time:67739ms step_avg:88.32ms
step:768/1680 train_time:67827ms step_avg:88.32ms
step:769/1680 train_time:67915ms step_avg:88.32ms
step:770/1680 train_time:68004ms step_avg:88.32ms
step:771/1680 train_time:68092ms step_avg:88.32ms
step:772/1680 train_time:68181ms step_avg:88.32ms
step:773/1680 train_time:68269ms step_avg:88.32ms
step:774/1680 train_time:68359ms step_avg:88.32ms
step:775/1680 train_time:68449ms step_avg:88.32ms
step:776/1680 train_time:68539ms step_avg:88.32ms
step:777/1680 train_time:68628ms step_avg:88.32ms
step:778/1680 train_time:68717ms step_avg:88.32ms
step:779/1680 train_time:68805ms step_avg:88.32ms
step:780/1680 train_time:68894ms step_avg:88.33ms
step:781/1680 train_time:68983ms step_avg:88.33ms
step:782/1680 train_time:69071ms step_avg:88.33ms
step:783/1680 train_time:69160ms step_avg:88.33ms
step:784/1680 train_time:69249ms step_avg:88.33ms
step:785/1680 train_time:69338ms step_avg:88.33ms
step:786/1680 train_time:69429ms step_avg:88.33ms
step:787/1680 train_time:69518ms step_avg:88.33ms
step:788/1680 train_time:69607ms step_avg:88.33ms
step:789/1680 train_time:69697ms step_avg:88.34ms
step:790/1680 train_time:69785ms step_avg:88.34ms
step:791/1680 train_time:69874ms step_avg:88.34ms
step:792/1680 train_time:69963ms step_avg:88.34ms
step:793/1680 train_time:70052ms step_avg:88.34ms
step:794/1680 train_time:70140ms step_avg:88.34ms
step:795/1680 train_time:70229ms step_avg:88.34ms
step:796/1680 train_time:70318ms step_avg:88.34ms
step:797/1680 train_time:70407ms step_avg:88.34ms
step:798/1680 train_time:70497ms step_avg:88.34ms
step:799/1680 train_time:70586ms step_avg:88.34ms
step:800/1680 train_time:70676ms step_avg:88.34ms
step:801/1680 train_time:70765ms step_avg:88.35ms
step:802/1680 train_time:70854ms step_avg:88.35ms
step:803/1680 train_time:70942ms step_avg:88.35ms
step:804/1680 train_time:71031ms step_avg:88.35ms
step:805/1680 train_time:71120ms step_avg:88.35ms
step:806/1680 train_time:71208ms step_avg:88.35ms
step:807/1680 train_time:71298ms step_avg:88.35ms
step:808/1680 train_time:71386ms step_avg:88.35ms
step:809/1680 train_time:71476ms step_avg:88.35ms
step:810/1680 train_time:71565ms step_avg:88.35ms
step:811/1680 train_time:71654ms step_avg:88.35ms
step:812/1680 train_time:71743ms step_avg:88.35ms
step:813/1680 train_time:71832ms step_avg:88.35ms
step:814/1680 train_time:71922ms step_avg:88.36ms
step:815/1680 train_time:72011ms step_avg:88.36ms
step:816/1680 train_time:72100ms step_avg:88.36ms
step:817/1680 train_time:72188ms step_avg:88.36ms
step:818/1680 train_time:72278ms step_avg:88.36ms
step:819/1680 train_time:72366ms step_avg:88.36ms
step:820/1680 train_time:72456ms step_avg:88.36ms
step:821/1680 train_time:72545ms step_avg:88.36ms
step:822/1680 train_time:72634ms step_avg:88.36ms
step:823/1680 train_time:72723ms step_avg:88.36ms
step:824/1680 train_time:72811ms step_avg:88.36ms
step:825/1680 train_time:72900ms step_avg:88.36ms
step:826/1680 train_time:72989ms step_avg:88.36ms
step:827/1680 train_time:73079ms step_avg:88.37ms
step:828/1680 train_time:73168ms step_avg:88.37ms
step:829/1680 train_time:73258ms step_avg:88.37ms
step:830/1680 train_time:73346ms step_avg:88.37ms
step:831/1680 train_time:73434ms step_avg:88.37ms
step:832/1680 train_time:73523ms step_avg:88.37ms
step:833/1680 train_time:73612ms step_avg:88.37ms
step:834/1680 train_time:73701ms step_avg:88.37ms
step:835/1680 train_time:73791ms step_avg:88.37ms
step:836/1680 train_time:73879ms step_avg:88.37ms
step:837/1680 train_time:73968ms step_avg:88.37ms
step:838/1680 train_time:74057ms step_avg:88.37ms
step:839/1680 train_time:74146ms step_avg:88.37ms
step:840/1680 train_time:74236ms step_avg:88.38ms
step:841/1680 train_time:74324ms step_avg:88.38ms
step:842/1680 train_time:74413ms step_avg:88.38ms
step:843/1680 train_time:74502ms step_avg:88.38ms
step:844/1680 train_time:74590ms step_avg:88.38ms
step:845/1680 train_time:74680ms step_avg:88.38ms
step:846/1680 train_time:74769ms step_avg:88.38ms
step:847/1680 train_time:74858ms step_avg:88.38ms
step:848/1680 train_time:74947ms step_avg:88.38ms
step:849/1680 train_time:75036ms step_avg:88.38ms
step:850/1680 train_time:75124ms step_avg:88.38ms
step:851/1680 train_time:75213ms step_avg:88.38ms
step:852/1680 train_time:75303ms step_avg:88.38ms
step:853/1680 train_time:75392ms step_avg:88.38ms
step:854/1680 train_time:75481ms step_avg:88.39ms
step:855/1680 train_time:75570ms step_avg:88.39ms
step:856/1680 train_time:75660ms step_avg:88.39ms
step:857/1680 train_time:75749ms step_avg:88.39ms
step:858/1680 train_time:75838ms step_avg:88.39ms
step:859/1680 train_time:75927ms step_avg:88.39ms
step:860/1680 train_time:76017ms step_avg:88.39ms
step:861/1680 train_time:76106ms step_avg:88.39ms
step:862/1680 train_time:76195ms step_avg:88.39ms
step:863/1680 train_time:76284ms step_avg:88.39ms
step:864/1680 train_time:76372ms step_avg:88.39ms
step:865/1680 train_time:76462ms step_avg:88.40ms
step:866/1680 train_time:76551ms step_avg:88.40ms
step:867/1680 train_time:76639ms step_avg:88.40ms
step:868/1680 train_time:76728ms step_avg:88.40ms
step:869/1680 train_time:76817ms step_avg:88.40ms
step:870/1680 train_time:76905ms step_avg:88.40ms
step:871/1680 train_time:76995ms step_avg:88.40ms
step:872/1680 train_time:77083ms step_avg:88.40ms
step:873/1680 train_time:77172ms step_avg:88.40ms
step:874/1680 train_time:77262ms step_avg:88.40ms
step:875/1680 train_time:77351ms step_avg:88.40ms
step:875/1680 val_loss:3.5194 train_time:77441ms step_avg:88.50ms
step:876/1680 train_time:77466ms step_avg:88.43ms
step:877/1680 train_time:77532ms step_avg:88.41ms
step:878/1680 train_time:77627ms step_avg:88.41ms
step:879/1680 train_time:77720ms step_avg:88.42ms
step:880/1680 train_time:77808ms step_avg:88.42ms
step:881/1680 train_time:77897ms step_avg:88.42ms
step:882/1680 train_time:77985ms step_avg:88.42ms
step:883/1680 train_time:78073ms step_avg:88.42ms
step:884/1680 train_time:78161ms step_avg:88.42ms
step:885/1680 train_time:78249ms step_avg:88.42ms
step:886/1680 train_time:78337ms step_avg:88.42ms
step:887/1680 train_time:78426ms step_avg:88.42ms
step:888/1680 train_time:78517ms step_avg:88.42ms
step:889/1680 train_time:78608ms step_avg:88.42ms
step:890/1680 train_time:78698ms step_avg:88.43ms
step:891/1680 train_time:78788ms step_avg:88.43ms
step:892/1680 train_time:78877ms step_avg:88.43ms
step:893/1680 train_time:78965ms step_avg:88.43ms
step:894/1680 train_time:79054ms step_avg:88.43ms
step:895/1680 train_time:79142ms step_avg:88.43ms
step:896/1680 train_time:79230ms step_avg:88.43ms
step:897/1680 train_time:79318ms step_avg:88.43ms
step:898/1680 train_time:79407ms step_avg:88.43ms
step:899/1680 train_time:79497ms step_avg:88.43ms
step:900/1680 train_time:79587ms step_avg:88.43ms
step:901/1680 train_time:79676ms step_avg:88.43ms
step:902/1680 train_time:79765ms step_avg:88.43ms
step:903/1680 train_time:79854ms step_avg:88.43ms
step:904/1680 train_time:79943ms step_avg:88.43ms
step:905/1680 train_time:80032ms step_avg:88.43ms
step:906/1680 train_time:80122ms step_avg:88.44ms
step:907/1680 train_time:80210ms step_avg:88.43ms
step:908/1680 train_time:80299ms step_avg:88.43ms
step:909/1680 train_time:80388ms step_avg:88.44ms
step:910/1680 train_time:80476ms step_avg:88.44ms
step:911/1680 train_time:80566ms step_avg:88.44ms
step:912/1680 train_time:80655ms step_avg:88.44ms
step:913/1680 train_time:80744ms step_avg:88.44ms
step:914/1680 train_time:80833ms step_avg:88.44ms
step:915/1680 train_time:80922ms step_avg:88.44ms
step:916/1680 train_time:81011ms step_avg:88.44ms
step:917/1680 train_time:81100ms step_avg:88.44ms
step:918/1680 train_time:81189ms step_avg:88.44ms
step:919/1680 train_time:81278ms step_avg:88.44ms
step:920/1680 train_time:81367ms step_avg:88.44ms
step:921/1680 train_time:81456ms step_avg:88.44ms
step:922/1680 train_time:81545ms step_avg:88.44ms
step:923/1680 train_time:81635ms step_avg:88.44ms
step:924/1680 train_time:81723ms step_avg:88.45ms
step:925/1680 train_time:81812ms step_avg:88.45ms
step:926/1680 train_time:81901ms step_avg:88.45ms
step:927/1680 train_time:81990ms step_avg:88.45ms
step:928/1680 train_time:82080ms step_avg:88.45ms
step:929/1680 train_time:82169ms step_avg:88.45ms
step:930/1680 train_time:82258ms step_avg:88.45ms
step:931/1680 train_time:82347ms step_avg:88.45ms
step:932/1680 train_time:82436ms step_avg:88.45ms
step:933/1680 train_time:82525ms step_avg:88.45ms
step:934/1680 train_time:82615ms step_avg:88.45ms
step:935/1680 train_time:82704ms step_avg:88.45ms
step:936/1680 train_time:82793ms step_avg:88.45ms
step:937/1680 train_time:82882ms step_avg:88.45ms
step:938/1680 train_time:82971ms step_avg:88.46ms
step:939/1680 train_time:83060ms step_avg:88.46ms
step:940/1680 train_time:83149ms step_avg:88.46ms
step:941/1680 train_time:83237ms step_avg:88.46ms
step:942/1680 train_time:83327ms step_avg:88.46ms
step:943/1680 train_time:83415ms step_avg:88.46ms
step:944/1680 train_time:83504ms step_avg:88.46ms
step:945/1680 train_time:83594ms step_avg:88.46ms
step:946/1680 train_time:83683ms step_avg:88.46ms
step:947/1680 train_time:83772ms step_avg:88.46ms
step:948/1680 train_time:83863ms step_avg:88.46ms
step:949/1680 train_time:83951ms step_avg:88.46ms
step:950/1680 train_time:84040ms step_avg:88.46ms
step:951/1680 train_time:84128ms step_avg:88.46ms
step:952/1680 train_time:84217ms step_avg:88.46ms
step:953/1680 train_time:84307ms step_avg:88.46ms
step:954/1680 train_time:84396ms step_avg:88.47ms
step:955/1680 train_time:84485ms step_avg:88.47ms
step:956/1680 train_time:84573ms step_avg:88.47ms
step:957/1680 train_time:84663ms step_avg:88.47ms
step:958/1680 train_time:84752ms step_avg:88.47ms
step:959/1680 train_time:84841ms step_avg:88.47ms
step:960/1680 train_time:84930ms step_avg:88.47ms
step:961/1680 train_time:85020ms step_avg:88.47ms
step:962/1680 train_time:85109ms step_avg:88.47ms
step:963/1680 train_time:85198ms step_avg:88.47ms
step:964/1680 train_time:85288ms step_avg:88.47ms
step:965/1680 train_time:85377ms step_avg:88.47ms
step:966/1680 train_time:85465ms step_avg:88.47ms
step:967/1680 train_time:85554ms step_avg:88.47ms
step:968/1680 train_time:85643ms step_avg:88.47ms
step:969/1680 train_time:85732ms step_avg:88.47ms
step:970/1680 train_time:85821ms step_avg:88.48ms
step:971/1680 train_time:85910ms step_avg:88.48ms
step:972/1680 train_time:85999ms step_avg:88.48ms
step:973/1680 train_time:86089ms step_avg:88.48ms
step:974/1680 train_time:86178ms step_avg:88.48ms
step:975/1680 train_time:86267ms step_avg:88.48ms
step:976/1680 train_time:86356ms step_avg:88.48ms
step:977/1680 train_time:86444ms step_avg:88.48ms
step:978/1680 train_time:86533ms step_avg:88.48ms
step:979/1680 train_time:86622ms step_avg:88.48ms
step:980/1680 train_time:86711ms step_avg:88.48ms
step:981/1680 train_time:86801ms step_avg:88.48ms
step:982/1680 train_time:86890ms step_avg:88.48ms
step:983/1680 train_time:86979ms step_avg:88.48ms
step:984/1680 train_time:87068ms step_avg:88.48ms
step:985/1680 train_time:87157ms step_avg:88.48ms
step:986/1680 train_time:87246ms step_avg:88.48ms
step:987/1680 train_time:87336ms step_avg:88.49ms
step:988/1680 train_time:87424ms step_avg:88.49ms
step:989/1680 train_time:87514ms step_avg:88.49ms
step:990/1680 train_time:87603ms step_avg:88.49ms
step:991/1680 train_time:87692ms step_avg:88.49ms
step:992/1680 train_time:87782ms step_avg:88.49ms
step:993/1680 train_time:87871ms step_avg:88.49ms
step:994/1680 train_time:87960ms step_avg:88.49ms
step:995/1680 train_time:88049ms step_avg:88.49ms
step:996/1680 train_time:88138ms step_avg:88.49ms
step:997/1680 train_time:88228ms step_avg:88.49ms
step:998/1680 train_time:88318ms step_avg:88.49ms
step:999/1680 train_time:88406ms step_avg:88.49ms
step:1000/1680 train_time:88496ms step_avg:88.50ms
step:1000/1680 val_loss:3.4711 train_time:88585ms step_avg:88.59ms
step:1001/1680 train_time:88610ms step_avg:88.52ms
step:1002/1680 train_time:88677ms step_avg:88.50ms
step:1003/1680 train_time:88771ms step_avg:88.51ms
step:1004/1680 train_time:88863ms step_avg:88.51ms
step:1005/1680 train_time:88951ms step_avg:88.51ms
step:1006/1680 train_time:89039ms step_avg:88.51ms
step:1007/1680 train_time:89127ms step_avg:88.51ms
step:1008/1680 train_time:89214ms step_avg:88.51ms
step:1009/1680 train_time:89302ms step_avg:88.51ms
step:1010/1680 train_time:89390ms step_avg:88.51ms
step:1011/1680 train_time:89479ms step_avg:88.51ms
step:1012/1680 train_time:89569ms step_avg:88.51ms
step:1013/1680 train_time:89659ms step_avg:88.51ms
step:1014/1680 train_time:89750ms step_avg:88.51ms
step:1015/1680 train_time:89840ms step_avg:88.51ms
step:1016/1680 train_time:89929ms step_avg:88.51ms
step:1017/1680 train_time:90018ms step_avg:88.51ms
step:1018/1680 train_time:90107ms step_avg:88.51ms
step:1019/1680 train_time:90196ms step_avg:88.51ms
step:1020/1680 train_time:90284ms step_avg:88.51ms
step:1021/1680 train_time:90372ms step_avg:88.51ms
step:1022/1680 train_time:90461ms step_avg:88.51ms
step:1023/1680 train_time:90549ms step_avg:88.51ms
step:1024/1680 train_time:90640ms step_avg:88.52ms
step:1025/1680 train_time:90729ms step_avg:88.52ms
step:1026/1680 train_time:90819ms step_avg:88.52ms
step:1027/1680 train_time:90909ms step_avg:88.52ms
step:1028/1680 train_time:90998ms step_avg:88.52ms
step:1029/1680 train_time:91087ms step_avg:88.52ms
step:1030/1680 train_time:91176ms step_avg:88.52ms
step:1031/1680 train_time:91264ms step_avg:88.52ms
step:1032/1680 train_time:91352ms step_avg:88.52ms
step:1033/1680 train_time:91441ms step_avg:88.52ms
step:1034/1680 train_time:91529ms step_avg:88.52ms
step:1035/1680 train_time:91620ms step_avg:88.52ms
step:1036/1680 train_time:91710ms step_avg:88.52ms
step:1037/1680 train_time:91799ms step_avg:88.52ms
step:1038/1680 train_time:91888ms step_avg:88.52ms
step:1039/1680 train_time:91978ms step_avg:88.53ms
step:1040/1680 train_time:92067ms step_avg:88.53ms
step:1041/1680 train_time:92156ms step_avg:88.53ms
step:1042/1680 train_time:92245ms step_avg:88.53ms
step:1043/1680 train_time:92333ms step_avg:88.53ms
step:1044/1680 train_time:92422ms step_avg:88.53ms
step:1045/1680 train_time:92511ms step_avg:88.53ms
step:1046/1680 train_time:92601ms step_avg:88.53ms
step:1047/1680 train_time:92690ms step_avg:88.53ms
step:1048/1680 train_time:92780ms step_avg:88.53ms
step:1049/1680 train_time:92869ms step_avg:88.53ms
step:1050/1680 train_time:92959ms step_avg:88.53ms
step:1051/1680 train_time:93048ms step_avg:88.53ms
step:1052/1680 train_time:93137ms step_avg:88.53ms
step:1053/1680 train_time:93225ms step_avg:88.53ms
step:1054/1680 train_time:93314ms step_avg:88.53ms
step:1055/1680 train_time:93403ms step_avg:88.53ms
step:1056/1680 train_time:93492ms step_avg:88.53ms
step:1057/1680 train_time:93581ms step_avg:88.53ms
step:1058/1680 train_time:93670ms step_avg:88.53ms
step:1059/1680 train_time:93759ms step_avg:88.54ms
step:1060/1680 train_time:93849ms step_avg:88.54ms
step:1061/1680 train_time:93938ms step_avg:88.54ms
step:1062/1680 train_time:94027ms step_avg:88.54ms
step:1063/1680 train_time:94117ms step_avg:88.54ms
step:1064/1680 train_time:94205ms step_avg:88.54ms
step:1065/1680 train_time:94294ms step_avg:88.54ms
step:1066/1680 train_time:94382ms step_avg:88.54ms
step:1067/1680 train_time:94472ms step_avg:88.54ms
step:1068/1680 train_time:94560ms step_avg:88.54ms
step:1069/1680 train_time:94649ms step_avg:88.54ms
step:1070/1680 train_time:94739ms step_avg:88.54ms
step:1071/1680 train_time:94827ms step_avg:88.54ms
step:1072/1680 train_time:94917ms step_avg:88.54ms
step:1073/1680 train_time:95006ms step_avg:88.54ms
step:1074/1680 train_time:95095ms step_avg:88.54ms
step:1075/1680 train_time:95185ms step_avg:88.54ms
step:1076/1680 train_time:95274ms step_avg:88.54ms
step:1077/1680 train_time:95363ms step_avg:88.55ms
step:1078/1680 train_time:95453ms step_avg:88.55ms
step:1079/1680 train_time:95541ms step_avg:88.55ms
step:1080/1680 train_time:95630ms step_avg:88.55ms
step:1081/1680 train_time:95721ms step_avg:88.55ms
step:1082/1680 train_time:95810ms step_avg:88.55ms
step:1083/1680 train_time:95898ms step_avg:88.55ms
step:1084/1680 train_time:95987ms step_avg:88.55ms
step:1085/1680 train_time:96076ms step_avg:88.55ms
step:1086/1680 train_time:96166ms step_avg:88.55ms
step:1087/1680 train_time:96255ms step_avg:88.55ms
step:1088/1680 train_time:96344ms step_avg:88.55ms
step:1089/1680 train_time:96433ms step_avg:88.55ms
step:1090/1680 train_time:96522ms step_avg:88.55ms
step:1091/1680 train_time:96611ms step_avg:88.55ms
step:1092/1680 train_time:96700ms step_avg:88.55ms
step:1093/1680 train_time:96789ms step_avg:88.55ms
step:1094/1680 train_time:96879ms step_avg:88.55ms
step:1095/1680 train_time:96968ms step_avg:88.56ms
step:1096/1680 train_time:97058ms step_avg:88.56ms
step:1097/1680 train_time:97148ms step_avg:88.56ms
step:1098/1680 train_time:97237ms step_avg:88.56ms
step:1099/1680 train_time:97327ms step_avg:88.56ms
step:1100/1680 train_time:97418ms step_avg:88.56ms
step:1101/1680 train_time:97507ms step_avg:88.56ms
step:1102/1680 train_time:97597ms step_avg:88.56ms
step:1103/1680 train_time:97686ms step_avg:88.56ms
step:1104/1680 train_time:97776ms step_avg:88.57ms
step:1105/1680 train_time:97865ms step_avg:88.57ms
step:1106/1680 train_time:97955ms step_avg:88.57ms
step:1107/1680 train_time:98046ms step_avg:88.57ms
step:1108/1680 train_time:98135ms step_avg:88.57ms
step:1109/1680 train_time:98225ms step_avg:88.57ms
step:1110/1680 train_time:98315ms step_avg:88.57ms
step:1111/1680 train_time:98404ms step_avg:88.57ms
step:1112/1680 train_time:98494ms step_avg:88.57ms
step:1113/1680 train_time:98583ms step_avg:88.57ms
step:1114/1680 train_time:98673ms step_avg:88.57ms
step:1115/1680 train_time:98762ms step_avg:88.58ms
step:1116/1680 train_time:98852ms step_avg:88.58ms
step:1117/1680 train_time:98941ms step_avg:88.58ms
step:1118/1680 train_time:99031ms step_avg:88.58ms
step:1119/1680 train_time:99122ms step_avg:88.58ms
step:1120/1680 train_time:99211ms step_avg:88.58ms
step:1121/1680 train_time:99300ms step_avg:88.58ms
step:1122/1680 train_time:99390ms step_avg:88.58ms
step:1123/1680 train_time:99481ms step_avg:88.58ms
step:1124/1680 train_time:99570ms step_avg:88.59ms
step:1125/1680 train_time:99660ms step_avg:88.59ms
step:1125/1680 val_loss:3.4169 train_time:99751ms step_avg:88.67ms
step:1126/1680 train_time:99776ms step_avg:88.61ms
step:1127/1680 train_time:99841ms step_avg:88.59ms
step:1128/1680 train_time:99936ms step_avg:88.60ms
step:1129/1680 train_time:100031ms step_avg:88.60ms
step:1130/1680 train_time:100120ms step_avg:88.60ms
step:1131/1680 train_time:100208ms step_avg:88.60ms
step:1132/1680 train_time:100297ms step_avg:88.60ms
step:1133/1680 train_time:100385ms step_avg:88.60ms
step:1134/1680 train_time:100473ms step_avg:88.60ms
step:1135/1680 train_time:100562ms step_avg:88.60ms
step:1136/1680 train_time:100652ms step_avg:88.60ms
step:1137/1680 train_time:100743ms step_avg:88.60ms
step:1138/1680 train_time:100834ms step_avg:88.61ms
step:1139/1680 train_time:100924ms step_avg:88.61ms
step:1140/1680 train_time:101015ms step_avg:88.61ms
step:1141/1680 train_time:101104ms step_avg:88.61ms
step:1142/1680 train_time:101194ms step_avg:88.61ms
step:1143/1680 train_time:101282ms step_avg:88.61ms
step:1144/1680 train_time:101371ms step_avg:88.61ms
step:1145/1680 train_time:101460ms step_avg:88.61ms
step:1146/1680 train_time:101549ms step_avg:88.61ms
step:1147/1680 train_time:101638ms step_avg:88.61ms
step:1148/1680 train_time:101729ms step_avg:88.61ms
step:1149/1680 train_time:101819ms step_avg:88.62ms
step:1150/1680 train_time:101911ms step_avg:88.62ms
step:1151/1680 train_time:102002ms step_avg:88.62ms
step:1152/1680 train_time:102092ms step_avg:88.62ms
step:1153/1680 train_time:102181ms step_avg:88.62ms
step:1154/1680 train_time:102271ms step_avg:88.62ms
step:1155/1680 train_time:102360ms step_avg:88.62ms
step:1156/1680 train_time:102449ms step_avg:88.62ms
step:1157/1680 train_time:102538ms step_avg:88.62ms
step:1158/1680 train_time:102627ms step_avg:88.62ms
step:1159/1680 train_time:102717ms step_avg:88.63ms
step:1160/1680 train_time:102808ms step_avg:88.63ms
step:1161/1680 train_time:102898ms step_avg:88.63ms
step:1162/1680 train_time:102988ms step_avg:88.63ms
step:1163/1680 train_time:103079ms step_avg:88.63ms
step:1164/1680 train_time:103169ms step_avg:88.63ms
step:1165/1680 train_time:103260ms step_avg:88.64ms
step:1166/1680 train_time:103349ms step_avg:88.64ms
step:1167/1680 train_time:103438ms step_avg:88.64ms
step:1168/1680 train_time:103527ms step_avg:88.64ms
step:1169/1680 train_time:103617ms step_avg:88.64ms
step:1170/1680 train_time:103707ms step_avg:88.64ms
step:1171/1680 train_time:103796ms step_avg:88.64ms
step:1172/1680 train_time:103887ms step_avg:88.64ms
step:1173/1680 train_time:103977ms step_avg:88.64ms
step:1174/1680 train_time:104067ms step_avg:88.64ms
step:1175/1680 train_time:104157ms step_avg:88.64ms
step:1176/1680 train_time:104246ms step_avg:88.64ms
step:1177/1680 train_time:104336ms step_avg:88.65ms
step:1178/1680 train_time:104425ms step_avg:88.65ms
step:1179/1680 train_time:104514ms step_avg:88.65ms
step:1180/1680 train_time:104604ms step_avg:88.65ms
step:1181/1680 train_time:104693ms step_avg:88.65ms
step:1182/1680 train_time:104782ms step_avg:88.65ms
step:1183/1680 train_time:104872ms step_avg:88.65ms
step:1184/1680 train_time:104961ms step_avg:88.65ms
step:1185/1680 train_time:105051ms step_avg:88.65ms
step:1186/1680 train_time:105142ms step_avg:88.65ms
step:1187/1680 train_time:105232ms step_avg:88.65ms
step:1188/1680 train_time:105322ms step_avg:88.65ms
step:1189/1680 train_time:105411ms step_avg:88.66ms
step:1190/1680 train_time:105501ms step_avg:88.66ms
step:1191/1680 train_time:105591ms step_avg:88.66ms
step:1192/1680 train_time:105680ms step_avg:88.66ms
step:1193/1680 train_time:105770ms step_avg:88.66ms
step:1194/1680 train_time:105861ms step_avg:88.66ms
step:1195/1680 train_time:105950ms step_avg:88.66ms
step:1196/1680 train_time:106041ms step_avg:88.66ms
step:1197/1680 train_time:106131ms step_avg:88.66ms
step:1198/1680 train_time:106221ms step_avg:88.66ms
step:1199/1680 train_time:106311ms step_avg:88.67ms
step:1200/1680 train_time:106400ms step_avg:88.67ms
step:1201/1680 train_time:106489ms step_avg:88.67ms
step:1202/1680 train_time:106578ms step_avg:88.67ms
step:1203/1680 train_time:106669ms step_avg:88.67ms
step:1204/1680 train_time:106758ms step_avg:88.67ms
step:1205/1680 train_time:106848ms step_avg:88.67ms
step:1206/1680 train_time:106938ms step_avg:88.67ms
step:1207/1680 train_time:107028ms step_avg:88.67ms
step:1208/1680 train_time:107117ms step_avg:88.67ms
step:1209/1680 train_time:107207ms step_avg:88.67ms
step:1210/1680 train_time:107296ms step_avg:88.67ms
step:1211/1680 train_time:107386ms step_avg:88.68ms
step:1212/1680 train_time:107475ms step_avg:88.68ms
step:1213/1680 train_time:107565ms step_avg:88.68ms
step:1214/1680 train_time:107655ms step_avg:88.68ms
step:1215/1680 train_time:107744ms step_avg:88.68ms
step:1216/1680 train_time:107834ms step_avg:88.68ms
step:1217/1680 train_time:107923ms step_avg:88.68ms
step:1218/1680 train_time:108013ms step_avg:88.68ms
step:1219/1680 train_time:108104ms step_avg:88.68ms
step:1220/1680 train_time:108193ms step_avg:88.68ms
step:1221/1680 train_time:108283ms step_avg:88.68ms
step:1222/1680 train_time:108373ms step_avg:88.68ms
step:1223/1680 train_time:108462ms step_avg:88.69ms
step:1224/1680 train_time:108551ms step_avg:88.69ms
step:1225/1680 train_time:108641ms step_avg:88.69ms
step:1226/1680 train_time:108731ms step_avg:88.69ms
step:1227/1680 train_time:108821ms step_avg:88.69ms
step:1228/1680 train_time:108911ms step_avg:88.69ms
step:1229/1680 train_time:109000ms step_avg:88.69ms
step:1230/1680 train_time:109090ms step_avg:88.69ms
step:1231/1680 train_time:109179ms step_avg:88.69ms
step:1232/1680 train_time:109269ms step_avg:88.69ms
step:1233/1680 train_time:109359ms step_avg:88.69ms
step:1234/1680 train_time:109449ms step_avg:88.69ms
step:1235/1680 train_time:109539ms step_avg:88.70ms
step:1236/1680 train_time:109628ms step_avg:88.70ms
step:1237/1680 train_time:109717ms step_avg:88.70ms
step:1238/1680 train_time:109807ms step_avg:88.70ms
step:1239/1680 train_time:109897ms step_avg:88.70ms
step:1240/1680 train_time:109988ms step_avg:88.70ms
step:1241/1680 train_time:110078ms step_avg:88.70ms
step:1242/1680 train_time:110168ms step_avg:88.70ms
step:1243/1680 train_time:110258ms step_avg:88.70ms
step:1244/1680 train_time:110347ms step_avg:88.70ms
step:1245/1680 train_time:110438ms step_avg:88.70ms
step:1246/1680 train_time:110528ms step_avg:88.71ms
step:1247/1680 train_time:110618ms step_avg:88.71ms
step:1248/1680 train_time:110708ms step_avg:88.71ms
step:1249/1680 train_time:110797ms step_avg:88.71ms
step:1250/1680 train_time:110891ms step_avg:88.71ms
step:1250/1680 val_loss:3.3788 train_time:110977ms step_avg:88.78ms
step:1251/1680 train_time:111002ms step_avg:88.73ms
step:1252/1680 train_time:111069ms step_avg:88.71ms
step:1253/1680 train_time:111164ms step_avg:88.72ms
step:1254/1680 train_time:111254ms step_avg:88.72ms
step:1255/1680 train_time:111344ms step_avg:88.72ms
step:1256/1680 train_time:111433ms step_avg:88.72ms
step:1257/1680 train_time:111521ms step_avg:88.72ms
step:1258/1680 train_time:111610ms step_avg:88.72ms
step:1259/1680 train_time:111699ms step_avg:88.72ms
step:1260/1680 train_time:111789ms step_avg:88.72ms
step:1261/1680 train_time:111878ms step_avg:88.72ms
step:1262/1680 train_time:111968ms step_avg:88.72ms
step:1263/1680 train_time:112059ms step_avg:88.72ms
step:1264/1680 train_time:112152ms step_avg:88.73ms
step:1265/1680 train_time:112242ms step_avg:88.73ms
step:1266/1680 train_time:112333ms step_avg:88.73ms
step:1267/1680 train_time:112422ms step_avg:88.73ms
step:1268/1680 train_time:112511ms step_avg:88.73ms
step:1269/1680 train_time:112600ms step_avg:88.73ms
step:1270/1680 train_time:112689ms step_avg:88.73ms
step:1271/1680 train_time:112778ms step_avg:88.73ms
step:1272/1680 train_time:112867ms step_avg:88.73ms
step:1273/1680 train_time:112958ms step_avg:88.73ms
step:1274/1680 train_time:113050ms step_avg:88.74ms
step:1275/1680 train_time:113141ms step_avg:88.74ms
step:1276/1680 train_time:113232ms step_avg:88.74ms
step:1277/1680 train_time:113324ms step_avg:88.74ms
step:1278/1680 train_time:113413ms step_avg:88.74ms
step:1279/1680 train_time:113502ms step_avg:88.74ms
step:1280/1680 train_time:113592ms step_avg:88.74ms
step:1281/1680 train_time:113681ms step_avg:88.74ms
step:1282/1680 train_time:113770ms step_avg:88.74ms
step:1283/1680 train_time:113859ms step_avg:88.74ms
step:1284/1680 train_time:113949ms step_avg:88.75ms
step:1285/1680 train_time:114039ms step_avg:88.75ms
step:1286/1680 train_time:114129ms step_avg:88.75ms
step:1287/1680 train_time:114220ms step_avg:88.75ms
step:1288/1680 train_time:114311ms step_avg:88.75ms
step:1289/1680 train_time:114401ms step_avg:88.75ms
step:1290/1680 train_time:114491ms step_avg:88.75ms
step:1291/1680 train_time:114581ms step_avg:88.75ms
step:1292/1680 train_time:114671ms step_avg:88.75ms
step:1293/1680 train_time:114761ms step_avg:88.76ms
step:1294/1680 train_time:114850ms step_avg:88.76ms
step:1295/1680 train_time:114939ms step_avg:88.76ms
step:1296/1680 train_time:115029ms step_avg:88.76ms
step:1297/1680 train_time:115119ms step_avg:88.76ms
step:1298/1680 train_time:115209ms step_avg:88.76ms
step:1299/1680 train_time:115299ms step_avg:88.76ms
step:1300/1680 train_time:115389ms step_avg:88.76ms
step:1301/1680 train_time:115479ms step_avg:88.76ms
step:1302/1680 train_time:115569ms step_avg:88.76ms
step:1303/1680 train_time:115658ms step_avg:88.76ms
step:1304/1680 train_time:115748ms step_avg:88.76ms
step:1305/1680 train_time:115837ms step_avg:88.76ms
step:1306/1680 train_time:115928ms step_avg:88.77ms
step:1307/1680 train_time:116018ms step_avg:88.77ms
step:1308/1680 train_time:116107ms step_avg:88.77ms
step:1309/1680 train_time:116197ms step_avg:88.77ms
step:1310/1680 train_time:116287ms step_avg:88.77ms
step:1311/1680 train_time:116377ms step_avg:88.77ms
step:1312/1680 train_time:116467ms step_avg:88.77ms
step:1313/1680 train_time:116557ms step_avg:88.77ms
step:1314/1680 train_time:116646ms step_avg:88.77ms
step:1315/1680 train_time:116736ms step_avg:88.77ms
step:1316/1680 train_time:116825ms step_avg:88.77ms
step:1317/1680 train_time:116914ms step_avg:88.77ms
step:1318/1680 train_time:117004ms step_avg:88.77ms
step:1319/1680 train_time:117093ms step_avg:88.77ms
step:1320/1680 train_time:117183ms step_avg:88.78ms
step:1321/1680 train_time:117272ms step_avg:88.78ms
step:1322/1680 train_time:117362ms step_avg:88.78ms
step:1323/1680 train_time:117452ms step_avg:88.78ms
step:1324/1680 train_time:117542ms step_avg:88.78ms
step:1325/1680 train_time:117632ms step_avg:88.78ms
step:1326/1680 train_time:117722ms step_avg:88.78ms
step:1327/1680 train_time:117811ms step_avg:88.78ms
step:1328/1680 train_time:117902ms step_avg:88.78ms
step:1329/1680 train_time:117992ms step_avg:88.78ms
step:1330/1680 train_time:118082ms step_avg:88.78ms
step:1331/1680 train_time:118171ms step_avg:88.78ms
step:1332/1680 train_time:118261ms step_avg:88.78ms
step:1333/1680 train_time:118351ms step_avg:88.79ms
step:1334/1680 train_time:118442ms step_avg:88.79ms
step:1335/1680 train_time:118531ms step_avg:88.79ms
step:1336/1680 train_time:118621ms step_avg:88.79ms
step:1337/1680 train_time:118711ms step_avg:88.79ms
step:1338/1680 train_time:118801ms step_avg:88.79ms
step:1339/1680 train_time:118891ms step_avg:88.79ms
step:1340/1680 train_time:118981ms step_avg:88.79ms
step:1341/1680 train_time:119071ms step_avg:88.79ms
step:1342/1680 train_time:119161ms step_avg:88.79ms
step:1343/1680 train_time:119251ms step_avg:88.79ms
step:1344/1680 train_time:119340ms step_avg:88.79ms
step:1345/1680 train_time:119430ms step_avg:88.80ms
step:1346/1680 train_time:119520ms step_avg:88.80ms
step:1347/1680 train_time:119610ms step_avg:88.80ms
step:1348/1680 train_time:119699ms step_avg:88.80ms
step:1349/1680 train_time:119789ms step_avg:88.80ms
step:1350/1680 train_time:119878ms step_avg:88.80ms
step:1351/1680 train_time:119968ms step_avg:88.80ms
step:1352/1680 train_time:120058ms step_avg:88.80ms
step:1353/1680 train_time:120149ms step_avg:88.80ms
step:1354/1680 train_time:120239ms step_avg:88.80ms
step:1355/1680 train_time:120328ms step_avg:88.80ms
step:1356/1680 train_time:120418ms step_avg:88.80ms
step:1357/1680 train_time:120508ms step_avg:88.80ms
step:1358/1680 train_time:120597ms step_avg:88.80ms
step:1359/1680 train_time:120687ms step_avg:88.81ms
step:1360/1680 train_time:120777ms step_avg:88.81ms
step:1361/1680 train_time:120866ms step_avg:88.81ms
step:1362/1680 train_time:120957ms step_avg:88.81ms
step:1363/1680 train_time:121046ms step_avg:88.81ms
step:1364/1680 train_time:121136ms step_avg:88.81ms
step:1365/1680 train_time:121227ms step_avg:88.81ms
step:1366/1680 train_time:121316ms step_avg:88.81ms
step:1367/1680 train_time:121406ms step_avg:88.81ms
step:1368/1680 train_time:121496ms step_avg:88.81ms
step:1369/1680 train_time:121585ms step_avg:88.81ms
step:1370/1680 train_time:121675ms step_avg:88.81ms
step:1371/1680 train_time:121765ms step_avg:88.81ms
step:1372/1680 train_time:121854ms step_avg:88.81ms
step:1373/1680 train_time:121943ms step_avg:88.82ms
step:1374/1680 train_time:122033ms step_avg:88.82ms
step:1375/1680 train_time:122123ms step_avg:88.82ms
step:1375/1680 val_loss:3.3439 train_time:122213ms step_avg:88.88ms
step:1376/1680 train_time:122238ms step_avg:88.84ms
step:1377/1680 train_time:122305ms step_avg:88.82ms
step:1378/1680 train_time:122399ms step_avg:88.82ms
step:1379/1680 train_time:122490ms step_avg:88.82ms
step:1380/1680 train_time:122579ms step_avg:88.83ms
step:1381/1680 train_time:122668ms step_avg:88.83ms
step:1382/1680 train_time:122756ms step_avg:88.83ms
step:1383/1680 train_time:122845ms step_avg:88.82ms
step:1384/1680 train_time:122933ms step_avg:88.82ms
step:1385/1680 train_time:123022ms step_avg:88.82ms
step:1386/1680 train_time:123111ms step_avg:88.82ms
step:1387/1680 train_time:123202ms step_avg:88.83ms
step:1388/1680 train_time:123293ms step_avg:88.83ms
step:1389/1680 train_time:123384ms step_avg:88.83ms
step:1390/1680 train_time:123475ms step_avg:88.83ms
step:1391/1680 train_time:123564ms step_avg:88.83ms
step:1392/1680 train_time:123654ms step_avg:88.83ms
step:1393/1680 train_time:123744ms step_avg:88.83ms
step:1394/1680 train_time:123832ms step_avg:88.83ms
step:1395/1680 train_time:123922ms step_avg:88.83ms
step:1396/1680 train_time:124011ms step_avg:88.83ms
step:1397/1680 train_time:124100ms step_avg:88.83ms
step:1398/1680 train_time:124189ms step_avg:88.83ms
step:1399/1680 train_time:124280ms step_avg:88.83ms
step:1400/1680 train_time:124370ms step_avg:88.84ms
step:1401/1680 train_time:124461ms step_avg:88.84ms
step:1402/1680 train_time:124551ms step_avg:88.84ms
step:1403/1680 train_time:124641ms step_avg:88.84ms
step:1404/1680 train_time:124730ms step_avg:88.84ms
step:1405/1680 train_time:124819ms step_avg:88.84ms
step:1406/1680 train_time:124908ms step_avg:88.84ms
step:1407/1680 train_time:124997ms step_avg:88.84ms
step:1408/1680 train_time:125087ms step_avg:88.84ms
step:1409/1680 train_time:125176ms step_avg:88.84ms
step:1410/1680 train_time:125267ms step_avg:88.84ms
step:1411/1680 train_time:125357ms step_avg:88.84ms
step:1412/1680 train_time:125448ms step_avg:88.84ms
step:1413/1680 train_time:125539ms step_avg:88.85ms
step:1414/1680 train_time:125628ms step_avg:88.85ms
step:1415/1680 train_time:125718ms step_avg:88.85ms
step:1416/1680 train_time:125807ms step_avg:88.85ms
step:1417/1680 train_time:125896ms step_avg:88.85ms
step:1418/1680 train_time:125985ms step_avg:88.85ms
step:1419/1680 train_time:126074ms step_avg:88.85ms
step:1420/1680 train_time:126163ms step_avg:88.85ms
step:1421/1680 train_time:126253ms step_avg:88.85ms
step:1422/1680 train_time:126343ms step_avg:88.85ms
step:1423/1680 train_time:126433ms step_avg:88.85ms
step:1424/1680 train_time:126523ms step_avg:88.85ms
step:1425/1680 train_time:126613ms step_avg:88.85ms
step:1426/1680 train_time:126704ms step_avg:88.85ms
step:1427/1680 train_time:126793ms step_avg:88.85ms
step:1428/1680 train_time:126882ms step_avg:88.85ms
step:1429/1680 train_time:126971ms step_avg:88.85ms
step:1430/1680 train_time:127060ms step_avg:88.85ms
step:1431/1680 train_time:127149ms step_avg:88.85ms
step:1432/1680 train_time:127239ms step_avg:88.85ms
step:1433/1680 train_time:127329ms step_avg:88.85ms
step:1434/1680 train_time:127419ms step_avg:88.86ms
step:1435/1680 train_time:127508ms step_avg:88.86ms
step:1436/1680 train_time:127598ms step_avg:88.86ms
step:1437/1680 train_time:127689ms step_avg:88.86ms
step:1438/1680 train_time:127780ms step_avg:88.86ms
step:1439/1680 train_time:127868ms step_avg:88.86ms
step:1440/1680 train_time:127958ms step_avg:88.86ms
step:1441/1680 train_time:128047ms step_avg:88.86ms
step:1442/1680 train_time:128137ms step_avg:88.86ms
step:1443/1680 train_time:128226ms step_avg:88.86ms
step:1444/1680 train_time:128316ms step_avg:88.86ms
step:1445/1680 train_time:128407ms step_avg:88.86ms
step:1446/1680 train_time:128497ms step_avg:88.86ms
step:1447/1680 train_time:128586ms step_avg:88.86ms
step:1448/1680 train_time:128676ms step_avg:88.86ms
step:1449/1680 train_time:128767ms step_avg:88.87ms
step:1450/1680 train_time:128856ms step_avg:88.87ms
step:1451/1680 train_time:128947ms step_avg:88.87ms
step:1452/1680 train_time:129037ms step_avg:88.87ms
step:1453/1680 train_time:129127ms step_avg:88.87ms
step:1454/1680 train_time:129217ms step_avg:88.87ms
step:1455/1680 train_time:129307ms step_avg:88.87ms
step:1456/1680 train_time:129397ms step_avg:88.87ms
step:1457/1680 train_time:129486ms step_avg:88.87ms
step:1458/1680 train_time:129576ms step_avg:88.87ms
step:1459/1680 train_time:129666ms step_avg:88.87ms
step:1460/1680 train_time:129756ms step_avg:88.87ms
step:1461/1680 train_time:129847ms step_avg:88.88ms
step:1462/1680 train_time:129937ms step_avg:88.88ms
step:1463/1680 train_time:130026ms step_avg:88.88ms
step:1464/1680 train_time:130115ms step_avg:88.88ms
step:1465/1680 train_time:130206ms step_avg:88.88ms
step:1466/1680 train_time:130295ms step_avg:88.88ms
step:1467/1680 train_time:130386ms step_avg:88.88ms
step:1468/1680 train_time:130475ms step_avg:88.88ms
step:1469/1680 train_time:130564ms step_avg:88.88ms
step:1470/1680 train_time:130654ms step_avg:88.88ms
step:1471/1680 train_time:130743ms step_avg:88.88ms
step:1472/1680 train_time:130833ms step_avg:88.88ms
step:1473/1680 train_time:130923ms step_avg:88.88ms
step:1474/1680 train_time:131013ms step_avg:88.88ms
step:1475/1680 train_time:131103ms step_avg:88.88ms
step:1476/1680 train_time:131192ms step_avg:88.88ms
step:1477/1680 train_time:131282ms step_avg:88.88ms
step:1478/1680 train_time:131371ms step_avg:88.88ms
step:1479/1680 train_time:131461ms step_avg:88.88ms
step:1480/1680 train_time:131550ms step_avg:88.89ms
step:1481/1680 train_time:131640ms step_avg:88.89ms
step:1482/1680 train_time:131729ms step_avg:88.89ms
step:1483/1680 train_time:131819ms step_avg:88.89ms
step:1484/1680 train_time:131908ms step_avg:88.89ms
step:1485/1680 train_time:131999ms step_avg:88.89ms
step:1486/1680 train_time:132088ms step_avg:88.89ms
step:1487/1680 train_time:132178ms step_avg:88.89ms
step:1488/1680 train_time:132268ms step_avg:88.89ms
step:1489/1680 train_time:132358ms step_avg:88.89ms
step:1490/1680 train_time:132448ms step_avg:88.89ms
step:1491/1680 train_time:132538ms step_avg:88.89ms
step:1492/1680 train_time:132627ms step_avg:88.89ms
step:1493/1680 train_time:132718ms step_avg:88.89ms
step:1494/1680 train_time:132808ms step_avg:88.89ms
step:1495/1680 train_time:132898ms step_avg:88.89ms
step:1496/1680 train_time:132988ms step_avg:88.90ms
step:1497/1680 train_time:133077ms step_avg:88.90ms
step:1498/1680 train_time:133167ms step_avg:88.90ms
step:1499/1680 train_time:133257ms step_avg:88.90ms
step:1500/1680 train_time:133347ms step_avg:88.90ms
step:1500/1680 val_loss:3.3139 train_time:133438ms step_avg:88.96ms
step:1501/1680 train_time:133463ms step_avg:88.92ms
step:1502/1680 train_time:133531ms step_avg:88.90ms
step:1503/1680 train_time:133625ms step_avg:88.91ms
step:1504/1680 train_time:133716ms step_avg:88.91ms
step:1505/1680 train_time:133806ms step_avg:88.91ms
step:1506/1680 train_time:133894ms step_avg:88.91ms
step:1507/1680 train_time:133982ms step_avg:88.91ms
step:1508/1680 train_time:134071ms step_avg:88.91ms
step:1509/1680 train_time:134159ms step_avg:88.91ms
step:1510/1680 train_time:134248ms step_avg:88.91ms
step:1511/1680 train_time:134337ms step_avg:88.91ms
step:1512/1680 train_time:134427ms step_avg:88.91ms
step:1513/1680 train_time:134520ms step_avg:88.91ms
step:1514/1680 train_time:134612ms step_avg:88.91ms
step:1515/1680 train_time:134703ms step_avg:88.91ms
step:1516/1680 train_time:134793ms step_avg:88.91ms
step:1517/1680 train_time:134883ms step_avg:88.91ms
step:1518/1680 train_time:134974ms step_avg:88.92ms
step:1519/1680 train_time:135063ms step_avg:88.92ms
step:1520/1680 train_time:135153ms step_avg:88.92ms
step:1521/1680 train_time:135242ms step_avg:88.92ms
step:1522/1680 train_time:135331ms step_avg:88.92ms
step:1523/1680 train_time:135420ms step_avg:88.92ms
step:1524/1680 train_time:135510ms step_avg:88.92ms
step:1525/1680 train_time:135600ms step_avg:88.92ms
step:1526/1680 train_time:135691ms step_avg:88.92ms
step:1527/1680 train_time:135781ms step_avg:88.92ms
step:1528/1680 train_time:135870ms step_avg:88.92ms
step:1529/1680 train_time:135960ms step_avg:88.92ms
step:1530/1680 train_time:136049ms step_avg:88.92ms
step:1531/1680 train_time:136139ms step_avg:88.92ms
step:1532/1680 train_time:136228ms step_avg:88.92ms
step:1533/1680 train_time:136317ms step_avg:88.92ms
step:1534/1680 train_time:136406ms step_avg:88.92ms
step:1535/1680 train_time:136497ms step_avg:88.92ms
step:1536/1680 train_time:136588ms step_avg:88.92ms
step:1537/1680 train_time:136677ms step_avg:88.92ms
step:1538/1680 train_time:136767ms step_avg:88.93ms
step:1539/1680 train_time:136857ms step_avg:88.93ms
step:1540/1680 train_time:136947ms step_avg:88.93ms
step:1541/1680 train_time:137038ms step_avg:88.93ms
step:1542/1680 train_time:137127ms step_avg:88.93ms
step:1543/1680 train_time:137217ms step_avg:88.93ms
step:1544/1680 train_time:137306ms step_avg:88.93ms
step:1545/1680 train_time:137395ms step_avg:88.93ms
step:1546/1680 train_time:137485ms step_avg:88.93ms
step:1547/1680 train_time:137574ms step_avg:88.93ms
step:1548/1680 train_time:137665ms step_avg:88.93ms
step:1549/1680 train_time:137755ms step_avg:88.93ms
step:1550/1680 train_time:137844ms step_avg:88.93ms
step:1551/1680 train_time:137934ms step_avg:88.93ms
step:1552/1680 train_time:138023ms step_avg:88.93ms
step:1553/1680 train_time:138113ms step_avg:88.93ms
step:1554/1680 train_time:138202ms step_avg:88.93ms
step:1555/1680 train_time:138292ms step_avg:88.93ms
step:1556/1680 train_time:138381ms step_avg:88.93ms
step:1557/1680 train_time:138471ms step_avg:88.93ms
step:1558/1680 train_time:138561ms step_avg:88.93ms
step:1559/1680 train_time:138651ms step_avg:88.94ms
step:1560/1680 train_time:138740ms step_avg:88.94ms
step:1561/1680 train_time:138830ms step_avg:88.94ms
step:1562/1680 train_time:138919ms step_avg:88.94ms
step:1563/1680 train_time:139010ms step_avg:88.94ms
step:1564/1680 train_time:139100ms step_avg:88.94ms
step:1565/1680 train_time:139190ms step_avg:88.94ms
step:1566/1680 train_time:139279ms step_avg:88.94ms
step:1567/1680 train_time:139368ms step_avg:88.94ms
step:1568/1680 train_time:139459ms step_avg:88.94ms
step:1569/1680 train_time:139548ms step_avg:88.94ms
step:1570/1680 train_time:139638ms step_avg:88.94ms
step:1571/1680 train_time:139727ms step_avg:88.94ms
step:1572/1680 train_time:139817ms step_avg:88.94ms
step:1573/1680 train_time:139906ms step_avg:88.94ms
step:1574/1680 train_time:139995ms step_avg:88.94ms
step:1575/1680 train_time:140086ms step_avg:88.94ms
step:1576/1680 train_time:140175ms step_avg:88.94ms
step:1577/1680 train_time:140265ms step_avg:88.94ms
step:1578/1680 train_time:140354ms step_avg:88.94ms
step:1579/1680 train_time:140444ms step_avg:88.94ms
step:1580/1680 train_time:140534ms step_avg:88.95ms
step:1581/1680 train_time:140623ms step_avg:88.95ms
step:1582/1680 train_time:140713ms step_avg:88.95ms
step:1583/1680 train_time:140802ms step_avg:88.95ms
step:1584/1680 train_time:140892ms step_avg:88.95ms
step:1585/1680 train_time:140981ms step_avg:88.95ms
step:1586/1680 train_time:141071ms step_avg:88.95ms
step:1587/1680 train_time:141161ms step_avg:88.95ms
step:1588/1680 train_time:141251ms step_avg:88.95ms
step:1589/1680 train_time:141340ms step_avg:88.95ms
step:1590/1680 train_time:141429ms step_avg:88.95ms
step:1591/1680 train_time:141518ms step_avg:88.95ms
step:1592/1680 train_time:141608ms step_avg:88.95ms
step:1593/1680 train_time:141699ms step_avg:88.95ms
step:1594/1680 train_time:141789ms step_avg:88.95ms
step:1595/1680 train_time:141879ms step_avg:88.95ms
step:1596/1680 train_time:141969ms step_avg:88.95ms
step:1597/1680 train_time:142059ms step_avg:88.95ms
step:1598/1680 train_time:142148ms step_avg:88.95ms
step:1599/1680 train_time:142239ms step_avg:88.95ms
step:1600/1680 train_time:142329ms step_avg:88.96ms
step:1601/1680 train_time:142419ms step_avg:88.96ms
step:1602/1680 train_time:142508ms step_avg:88.96ms
step:1603/1680 train_time:142598ms step_avg:88.96ms
step:1604/1680 train_time:142689ms step_avg:88.96ms
step:1605/1680 train_time:142778ms step_avg:88.96ms
step:1606/1680 train_time:142868ms step_avg:88.96ms
step:1607/1680 train_time:142957ms step_avg:88.96ms
step:1608/1680 train_time:143047ms step_avg:88.96ms
step:1609/1680 train_time:143137ms step_avg:88.96ms
step:1610/1680 train_time:143227ms step_avg:88.96ms
step:1611/1680 train_time:143317ms step_avg:88.96ms
step:1612/1680 train_time:143407ms step_avg:88.96ms
step:1613/1680 train_time:143498ms step_avg:88.96ms
step:1614/1680 train_time:143588ms step_avg:88.96ms
step:1615/1680 train_time:143677ms step_avg:88.96ms
step:1616/1680 train_time:143767ms step_avg:88.96ms
step:1617/1680 train_time:143859ms step_avg:88.97ms
step:1618/1680 train_time:143948ms step_avg:88.97ms
step:1619/1680 train_time:144039ms step_avg:88.97ms
step:1620/1680 train_time:144128ms step_avg:88.97ms
step:1621/1680 train_time:144219ms step_avg:88.97ms
step:1622/1680 train_time:144309ms step_avg:88.97ms
step:1623/1680 train_time:144399ms step_avg:88.97ms
step:1624/1680 train_time:144489ms step_avg:88.97ms
step:1625/1680 train_time:144578ms step_avg:88.97ms
step:1625/1680 val_loss:3.2901 train_time:144669ms step_avg:89.03ms
step:1626/1680 train_time:144694ms step_avg:88.99ms
step:1627/1680 train_time:144764ms step_avg:88.98ms
step:1628/1680 train_time:144859ms step_avg:88.98ms
step:1629/1680 train_time:144950ms step_avg:88.98ms
step:1630/1680 train_time:145039ms step_avg:88.98ms
step:1631/1680 train_time:145128ms step_avg:88.98ms
step:1632/1680 train_time:145216ms step_avg:88.98ms
step:1633/1680 train_time:145306ms step_avg:88.98ms
step:1634/1680 train_time:145393ms step_avg:88.98ms
step:1635/1680 train_time:145482ms step_avg:88.98ms
step:1636/1680 train_time:145571ms step_avg:88.98ms
step:1637/1680 train_time:145663ms step_avg:88.98ms
step:1638/1680 train_time:145754ms step_avg:88.98ms
step:1639/1680 train_time:145846ms step_avg:88.98ms
step:1640/1680 train_time:145938ms step_avg:88.99ms
step:1641/1680 train_time:146027ms step_avg:88.99ms
step:1642/1680 train_time:146116ms step_avg:88.99ms
step:1643/1680 train_time:146206ms step_avg:88.99ms
step:1644/1680 train_time:146296ms step_avg:88.99ms
step:1645/1680 train_time:146385ms step_avg:88.99ms
step:1646/1680 train_time:146474ms step_avg:88.99ms
step:1647/1680 train_time:146563ms step_avg:88.99ms
step:1648/1680 train_time:146653ms step_avg:88.99ms
step:1649/1680 train_time:146745ms step_avg:88.99ms
step:1650/1680 train_time:146836ms step_avg:88.99ms
step:1651/1680 train_time:146927ms step_avg:88.99ms
step:1652/1680 train_time:147016ms step_avg:88.99ms
step:1653/1680 train_time:147106ms step_avg:88.99ms
step:1654/1680 train_time:147195ms step_avg:88.99ms
step:1655/1680 train_time:147284ms step_avg:88.99ms
step:1656/1680 train_time:147374ms step_avg:88.99ms
step:1657/1680 train_time:147463ms step_avg:88.99ms
step:1658/1680 train_time:147553ms step_avg:88.99ms
step:1659/1680 train_time:147644ms step_avg:89.00ms
step:1660/1680 train_time:147734ms step_avg:89.00ms
step:1661/1680 train_time:147825ms step_avg:89.00ms
step:1662/1680 train_time:147915ms step_avg:89.00ms
step:1663/1680 train_time:148006ms step_avg:89.00ms
step:1664/1680 train_time:148096ms step_avg:89.00ms
step:1665/1680 train_time:148185ms step_avg:89.00ms
step:1666/1680 train_time:148275ms step_avg:89.00ms
step:1667/1680 train_time:148364ms step_avg:89.00ms
step:1668/1680 train_time:148453ms step_avg:89.00ms
step:1669/1680 train_time:148543ms step_avg:89.00ms
step:1670/1680 train_time:148632ms step_avg:89.00ms
step:1671/1680 train_time:148722ms step_avg:89.00ms
step:1672/1680 train_time:148812ms step_avg:89.00ms
step:1673/1680 train_time:148903ms step_avg:89.00ms
step:1674/1680 train_time:148993ms step_avg:89.00ms
step:1675/1680 train_time:149083ms step_avg:89.00ms
step:1676/1680 train_time:149173ms step_avg:89.01ms
step:1677/1680 train_time:149263ms step_avg:89.01ms
step:1678/1680 train_time:149352ms step_avg:89.01ms
step:1679/1680 train_time:149442ms step_avg:89.01ms
step:1680/1680 train_time:149531ms step_avg:89.01ms
step:1680/1680 val_loss:3.2799 train_time:149623ms step_avg:89.06ms
peak memory allocated: 30760 MiB reserved: 45694 MiB

import os
import sys
with open(sys.argv[0]) as f:
    code = f.read() # read the code of this file ASAP, for logging
import uuid
import time
import glob
import subprocess
import contextlib
from dataclasses import dataclass

import torch
torch.empty(1, device='cuda', requires_grad=True).backward()
from torch import nn
import torch.nn.functional as F
import torch.distributed as dist
from torch.nn.parallel import DistributedDataParallel as DDP
# use of FlexAttention contributed by @KoszarskyB
from torch.nn.attention.flex_attention import BlockMask, flex_attention

# -----------------------------------------------------------------------------
# Muon optimizer

@torch.compile
def zeropower_via_newtonschulz5(G, steps):
    """
    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a
    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose
    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at
    zero even beyond the point where the iteration no longer converges all the way to one everywhere
    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T
    where S' is diagonal with S_{ii}' ~ Uniform(0.5, 1.5), which turns out not to hurt model
    performance at all relative to UV^T, where USV^T = G is the SVD.
    """
    assert len(G.shape) == 2
    a, b, c = (3.4445, -4.7750,  2.0315)
    X = G.bfloat16()
    if G.size(0) > G.size(1):
        X = X.T

    # Ensure spectral norm is at most 1
    X = X / (X.norm() + 1e-7)
    # Perform the NS iterations
    for _ in range(steps):
        A = X @ X.T
        B = b * A + c * A @ A # adapted from suggestion by @jxbz, @leloykun, and @YouJiacheng
        X = a * X + B @ X
    
    if G.size(0) > G.size(1):
        X = X.T
    return X

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Some warnings:
    - This optimizer assumes that all parameters passed in are 2D.
    - It should not be used for the embedding layer, the final fully connected layer, or any {0,1}-D
    parameters; those should all be optimized by a standard method (e.g., AdamW).
    - To use it with 4D convolutional filters, it works well to just flatten their last 3 dimensions.
    - We believe it is unlikely to work well for training with small batch size.
    - We believe it may not work well for finetuning pretrained models, but we haven't tested this.
    - We have not yet tried this optimizer for training scenarios larger than NanoGPT (124M).

    Arguments:
        lr: The learning rate used by the internal SGD.
        momentum: The momentum used by the internal SGD.
        nesterov: Whether to use Nesterov-style momentum in the internal SGD. (recommended)
        ns_steps: The number of Newton-Schulz iteration steps to use.
    """
    def __init__(self, params, lr=0.02, momentum=0.95, nesterov=True, ns_steps=5):
        self.world_size = int(os.environ['WORLD_SIZE'])
        self.rank = int(os.environ['RANK'])
        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, ns_steps=ns_steps)
        assert all(isinstance(p, torch.Tensor) for p in params)
        sizes = {p.numel() for p in params}
        param_groups = [dict(params=[p for p in params if p.numel() == size],
                             update_buffer=[torch.empty(size, device='cuda', dtype=torch.bfloat16) for _ in range(self.world_size)])
                        for size in sizes]
        super().__init__(param_groups, defaults)

    def step(self):

        for group in self.param_groups:

            lr = group['lr']
            momentum = group['momentum']
            nesterov = group['nesterov']
            ns_steps = group['ns_steps']
            update_buffers = group['update_buffer']
            # generate weight updates in distributed fashion
            params = group['params']
            handle = None
            params_world = None
            def update_prev():
                if params_world is None:
                    return
                assert handle is not None
                handle.wait()
                for p_world, g_world in zip(params_world, update_buffers):
                    p_world.data.add_(
                        g_world.view_as(p_world),
                        alpha=-lr * max(1, p_world.size(0) / p_world.size(1)) ** 0.5,
                    )
            for base_i in range(len(params))[::self.world_size]:
                if base_i + rank < len(params):
                    p = params[base_i + self.rank]
                    g = p.grad
                    assert g is not None
                    state = self.state[p]
                    if 'momentum_buffer' not in state:
                        state['momentum_buffer'] = torch.zeros_like(g)
                    buf = state['momentum_buffer']
                    buf.lerp_(g, 1 - momentum)
                    g = g.lerp_(buf, momentum) if nesterov else buf
                    g = zeropower_via_newtonschulz5(g, steps=ns_steps).flatten()
                else:
                    g = update_buffers[rank]
                update_prev() # async all_gather instead of sync all_reduce by @YouJiacheng
                handle = dist.all_gather(update_buffers, g, async_op=True)
                params_world = params[base_i : base_i + self.world_size]
            update_prev()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the GPT-2 model

def norm(x):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):

    def __init__(self, in_features, out_features):
        super().__init__(in_features, out_features, bias=False)

    def forward(self, x):
        return F.linear(x, self.weight.type_as(x))

class Rotary(nn.Module):

    def __init__(self, dim, max_seq_len=65536):
        super().__init__()
        # half-truncate RoPE by @YouJiacheng
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=dim//4, dtype=torch.float32)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(dim//4)])
        t = torch.arange(max_seq_len, dtype=torch.float32)
        theta = torch.einsum('i,j -> ij', t, angular_freq)
        self.cos = nn.Buffer(theta.cos(), persistent=False)
        self.sin = nn.Buffer(theta.sin(), persistent=False)

    def forward(self, x):
        cos, sin = self.cos[None, :x.size(-3), None, :], self.sin[None, :x.size(-3), None, :]
        x1, x2 = x.float().chunk(2, dim=-1)
        y1 = x1 * cos + x2 * sin
        y2 = x1 * (-sin) + x2 * cos
        return torch.cat((y1, y2), 3).type_as(x)

class CausalSelfAttention(nn.Module):

    def __init__(self, dim, num_heads):
        super().__init__()
        assert dim % num_heads == 0
        self.num_heads = num_heads
        self.c_q = CastedLinear(dim, dim)
        self.c_k = CastedLinear(dim, dim)
        self.c_v = CastedLinear(dim, dim)
        self.lambdas = nn.Parameter(torch.tensor([0.5, 0.5]))
        self.rotary = Rotary(dim // num_heads) # dim // num_heads = head_dim
        self.c_proj = CastedLinear(dim, dim)
        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977
        self.attn_scale = nn.Parameter(torch.tensor(1.0 / (dim // num_heads) ** 0.5))

    def forward(self, x, ve, block_mask):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, 'Must use batch size = 1 for FlexAttention'
        q = self.c_q(x).view(B, T, self.num_heads, -1)
        k = self.c_k(x).view(B, T, self.num_heads, -1)
        v = self.c_v(x).view(B, T, self.num_heads, -1)
        if ve is not None:
            v = self.lambdas[0] * v + self.lambdas[1] * ve.view_as(v) # @KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = self.lambdas[0] * v
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = self.rotary(q), self.rotary(k)
        y = flex_attention(q.transpose(1, 2) * self.attn_scale, k.transpose(1, 2), v.transpose(1, 2), block_mask=block_mask, scale=1.)
        y = y.transpose(1, 2).contiguous().view_as(x) # re-assemble all head outputs side by side
        y = self.c_proj(y)
        return y

class MLP(nn.Module):

    def __init__(self, dim):
        super().__init__()
        self.c_fc = CastedLinear(dim, 4 * dim)
        self.c_proj = CastedLinear(4 * dim, dim)
        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977

    def forward(self, x):
        x = self.c_fc(x)
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = self.c_proj(x)
        return x

class Block(nn.Module):

    def __init__(self, model_dim, num_heads, use_attn=True):
        super().__init__()
        self.attn = CausalSelfAttention(model_dim, num_heads) if use_attn else None
        self.mlp = MLP(model_dim)
        self.lambdas = nn.Parameter(torch.tensor([1., 0.]))

    def forward(self, x, ve, x0, block_mask):
        x = self.lambdas[0] * x + self.lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), ve, block_mask)
        x = x + self.mlp(norm(x))
        return x

class ValueEmbedding(nn.Module):
    def __init__(self, vocab_size, model_dim):
        super().__init__()
        self.embed = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])

    def forward(self, inputs):
        ve = [emb(inputs).bfloat16() for emb in self.embed]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        ve = [ve[0], ve[1], ve[2], None, None, None, None, None, None, ve[0], ve[1], ve[2]]
        return ve

# -----------------------------------------------------------------------------
# The main GPT-2 model

class GPT(nn.Module):

    def __init__(self, vocab_size, num_layers, num_heads, model_dim):
        super().__init__()
        self.embed = nn.Embedding(vocab_size, model_dim)
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.blocks = nn.ModuleList([Block(model_dim, num_heads, use_attn=(i != 7))
                                     for i in range(num_layers)])
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual learning
        # U-net structure on token value embeddings by @leloykun
        self.value_embeds = ValueEmbedding(vocab_size, model_dim)
        self.lm_head = CastedLinear(model_dim, vocab_size)
        self.lm_head.weight.data.zero_() # @Grad62304977
        # U-net design by @brendanh0gan
        self.num_encoder_layers = num_layers // 2 # Half of the layers for encoder
        self.num_decoder_layers = num_layers - self.num_encoder_layers # Remaining for decoder
        # Add learnable skip connection weights for decoder layers
        self.skip_weights = nn.Parameter(torch.ones(self.num_decoder_layers))

    def forward(self, inputs, targets, sliding_window_num_blocks):
        BLOCK_SIZE = 128
        seq_len = len(inputs)
        assert seq_len % BLOCK_SIZE == 0
        total_num_blocks = seq_len // BLOCK_SIZE
        assert inputs.ndim == 1
        docs = (inputs == 50256).cumsum(0)
        docs_low = docs.view(-1, BLOCK_SIZE)[:, 0].contiguous()
        docs_high = docs.view(-1, BLOCK_SIZE)[:, -1].contiguous()

        def document_causal(b, h, q_idx, kv_idx):
            causal_mask = q_idx >= kv_idx
            document_mask = docs[q_idx] == docs[kv_idx]
            return causal_mask & document_mask

        def dense_to_ordered(dense_mask):
            num_blocks = dense_mask.sum(dim=-1, dtype=torch.int32)
            indices = dense_mask.argsort(dim=-1, descending=True, stable=True).to(torch.int32)
            return num_blocks[None, None].contiguous(), indices[None, None].contiguous()

        def create_doc_swc_block_mask(sliding_window_num_blocks):
            kv_idx = block_idx = torch.arange(total_num_blocks, dtype=torch.int32, device='cuda')
            q_idx = block_idx[:, None]
            causal_bm = q_idx >= kv_idx
            causal_full_bm = q_idx > kv_idx
            window_bm = q_idx - kv_idx < sliding_window_num_blocks
            window_full_bm = window_bm # block-wise sliding window by @YouJiacheng
            # document_bm = (docs_low[q_idx] <= docs_high[kv_idx]) & (docs_low[kv_idx] <= docs_high[q_idx])
            document_bm = (docs_low[:, None] <= docs_high) & (docs_low <= docs_high[:, None])
            document_full_bm = (docs_low[:, None] == docs_high) & (docs_low == docs_high[:, None])
            nonzero_bm = causal_bm & window_bm & document_bm
            full_bm  = causal_full_bm & window_full_bm & document_full_bm
            kv_num_blocks, kv_indices = dense_to_ordered(nonzero_bm & ~full_bm)
            full_kv_num_blocks, full_kv_indices = dense_to_ordered(full_bm)
            return BlockMask.from_kv_blocks(
                kv_num_blocks,
                kv_indices,
                full_kv_num_blocks,
                full_kv_indices,
                BLOCK_SIZE=BLOCK_SIZE,
                mask_mod=document_causal,
            )

        block_mask = create_doc_swc_block_mask(sliding_window_num_blocks)

        x0 = norm(self.embed(inputs[None]).bfloat16()) # use of norm here by @Grad62304977
        x = x0
        ve = self.value_embeds(inputs)
        assert len(ve) == len(self.blocks)
        ve_enc, ve_dec = ve[:self.num_encoder_layers], ve[self.num_encoder_layers:]

        # Store outputs for U-Net skip connections
        skip_connections = []
        # Encoder pass - process only the first half of the blocks
        for i in range(self.num_encoder_layers):
            x = self.blocks[i](x, ve_enc[i], x0, block_mask)
            skip_connections.append(x)
        # Decoder pass - process the remaining blocks with weighted skip connections
        for i in range(self.num_decoder_layers):
            x = x + self.skip_weights[i] * skip_connections.pop()
            # U-net structure on token value embeddings by @leloykun
            x = self.blocks[self.num_encoder_layers + i](x, ve_dec[i], x0, block_mask)

        x = norm(x)
        logits = self.lm_head(x)
        logits = 15 * torch.tanh(logits / 15) # @Grad62304977 added tanh softcapping, @KoszarskyB reduced it from 30 to 15
        logits = logits.float()
        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets)
        return loss

# -----------------------------------------------------------------------------
# Our own simple Distributed Data Loader

def _load_data_shard(path):
    # only reads the header, returns header data
    # header is 256 int32
    header = torch.from_file(path, False, 256, dtype=torch.int32)
    assert header[0] == 20240520, 'magic number mismatch in the data .bin file'
    assert header[1] == 1, 'unsupported version'
    num_tokens = int(header[2]) # number of tokens (claimed)
    with open(path, 'rb', buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, 'number of tokens read does not match header'
    return tokens

class DistributedDataLoader:

    def __init__(self, filename_pattern):
        self.rank = int(os.environ['RANK'])
        self.world_size = int(os.environ['WORLD_SIZE'])
        self.files = sorted(glob.glob(filename_pattern))
        self.reset()

    def reset(self):
        self.current_shard = -1
        self.advance()

    def advance(self):
        self.current_shard = (self.current_shard + 1) % len(self.files)
        self.current_position = 0
        self.tokens = _load_data_shard(self.files[self.current_shard])

    def next_batch(self, batch_size):
        assert batch_size % self.world_size == 0
        device_batch_size = batch_size // self.world_size
        # load next shard if necessary
        if self.current_position + batch_size + 1 >= len(self.tokens):
            self.advance()
        pos = self.current_position + self.rank * device_batch_size
        device_batch_tokens = self.tokens[pos:pos+device_batch_size+1]
        # advance current position
        self.current_position += batch_size
        inputs = device_batch_tokens[:-1].to(device='cuda', dtype=torch.int32, non_blocking=True)
        targets = device_batch_tokens[1:].to(device='cuda', dtype=torch.int64, non_blocking=True)
        return inputs, targets

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files = 'data/fineweb10B/fineweb_train_*.bin' # input .bin to train on
    val_files = 'data/fineweb10B/fineweb_val_*.bin' # input .bin to eval validation loss on
    val_tokens = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # optimization
    batch_size = 8*64*1024 # batch size in tokens
    num_iterations = 1370 # number of iterations to run
    cooldown_frac = 0.4 # fraction of training spent cooling down the learning rate
    bf16_embeds = True
    # evaluation and logging
    val_loss_every = 125 # every how many steps to evaluate val loss? 0 for only at the end
    # implementation
    max_device_batch_size = 64*1024 # batch size per device in tokens
    save_checkpoint = False
args = Hyperparameters()

micro_bs = args.max_device_batch_size

# set up DDP (distributed data parallel). torchrun sets this env variable
rank = int(os.environ['RANK'])
local_rank = int(os.environ['LOCAL_RANK'])
world_size = int(os.environ['WORLD_SIZE'])
assert torch.cuda.is_available()
torch.cuda.set_device(local_rank)
dist.init_process_group(backend='nccl', device_id=torch.device(local_rank))
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = uuid.uuid4()
    os.makedirs('logs', exist_ok=True)
    logfile = f'logs/{run_id}.txt'
    print(logfile)

def print0(s, console=False):
    if master_process:
        with open(logfile, 'a') as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0('='*100)
# log information about the hardware/software environment this is running on
print0(f'Running Python {sys.version}')
print0(f'Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}')
print0(subprocess.run(['nvidia-smi'], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout)
print0('='*100)

# load data
train_loader = DistributedDataLoader(args.train_files)
val_loader = DistributedDataLoader(args.val_files)
print0(f'Training dataloader files: {train_loader.files}')
print0(f'Validation dataloader files: {val_loader.files}')
print0('='*100)

# there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency. suggested to me by @Grad62304977.
# this originates from Karpathy's experiments.
model = GPT(vocab_size=50304, num_layers=12, num_heads=6, model_dim=768)
model = model.cuda()
if args.bf16_embeds:
    for m in model.modules():
        if isinstance(m, nn.Embedding):
            m.bfloat16()
model: GPT = torch.compile(model)
ddp_model = DDP(model, device_ids=[local_rank], broadcast_buffers=False, gradient_as_bucket_view=True)

# collect the parameters to optimize
hidden_matrix_params = [p for p in model.blocks.parameters() if p.ndim == 2]
embed_params = [model.embed.weight, *model.value_embeds.parameters()]
scalar_params = [p for n, p in model.named_parameters() if p.ndim < 2 and "attn_scale" not in n]
attn_scale_params = [p for n, p in model.named_parameters() if p.ndim < 2 and "attn_scale" in n]
head_params = [model.lm_head.weight]

# init the optimizer(s)
optimizer1 = torch.optim.Adam([dict(params=embed_params, lr=0.6),
                               dict(params=head_params, lr=0.008),
                               dict(params=scalar_params, lr=0.04),
                               dict(params=attn_scale_params, lr=0.01)],
                              betas=(0.8, 0.95), fused=True)
optimizer2 = Muon(hidden_matrix_params, lr=0.05, momentum=0.95)
optimizers = [optimizer1, optimizer2]

# learning rate schedule: stable then decay
def get_lr(it):
    t = 1 - it / args.num_iterations # time remaining in training
    assert 1 >= t > 0
    # 1) constant lr for first part of training
    if t >= args.cooldown_frac:
        return 1.0
    # 2) then linear cooldown
    else:
        return t / args.cooldown_frac
schedulers = [torch.optim.lr_scheduler.LambdaLR(opt, get_lr) for opt in optimizers]

# sliding window size schedule: linear increase over training in chunks of 128 from 128 -> 1792. By @fernbear.bsky.social
def get_sliding_window_blocks(it):
    x = it / args.num_iterations # training progress
    assert 0 <= x <= 1
    return int(((1 - x) * 128 + x * 1856) // 128)
sliding_window_num_blocks = torch.tensor(1, dtype=torch.int32, device='cuda')

# Start training loop
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    # This effectively ignores timing first 10 steps, which are slower for weird reasons.
    # Alternately, and slightly more correctly in terms of benchmarking, we could do 10
    # steps with dummy data first, and then re-initialize the model and reset the loader.
    if step == 10:
        training_time_ms = 0
        t0 = time.perf_counter()
    timed_steps = float('nan') if step <= 11 else (step - 10) + 1 # <= 11 to avoid bug in val

    sliding_window_num_blocks.copy_(get_sliding_window_blocks(step))

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        # run validation batches
        model.eval()
        val_loader.reset()
        val_loss = 0.0
        # calculate the number of steps to take in the val loop.
        val_batch_size = world_size * micro_bs
        assert args.val_tokens % val_batch_size == 0
        val_steps = args.val_tokens // val_batch_size
        for _ in range(val_steps):
            with torch.no_grad():
                inputs_val, targets_val = val_loader.next_batch(val_batch_size)
                val_loss += ddp_model(inputs_val, targets_val, sliding_window_num_blocks)
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        val_loss /= val_steps
        # logging
        print0(f'step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/(timed_steps-1):.2f}ms', console=True)
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f'logs/{run_id}', exist_ok=True)
            torch.save(log, f'logs/{run_id}/state_step{step:06d}.pt')
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    model.train()
    batch_size = args.batch_size
    assert batch_size % world_size == 0
    inputs_train, targets_train = train_loader.next_batch(batch_size)
    assert len(inputs_train) <= micro_bs or len(inputs_train) % micro_bs == 0
    for micro_inputs_train, micro_targets_train in zip(inputs_train.split(micro_bs), targets_train.split(micro_bs)):
        ddp_model(micro_inputs_train, micro_targets_train, sliding_window_num_blocks).backward()
    # momentum warmup for Muon
    frac = min(step/300, 1)
    for group in optimizer2.param_groups:
        group['momentum'] = (1 - frac) * 0.85 + frac * 0.95
    # step the optimizers and schedulers
    for opt, sched in zip(optimizers, schedulers):
        opt.step()
        if step != train_steps-1:
            sched.step()
    # null the gradients
    model.zero_grad(set_to_none=True)
    # logging
    approx_time = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f'step:{step+1}/{train_steps} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms', console=True)

print0(f'peak memory consumption: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB')
dist.destroy_process_group()

====================================================================================================
Running Python 3.10.12 (main, Sep 11 2024, 15:47:36) [GCC 11.4.0]
Running PyTorch 2.6.0.dev20241231+cu126 compiled for CUDA 12.6
Sat Jan 11 23:13:52 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.183.06             Driver Version: 535.183.06   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:19:00.0 Off |                    0 |
| N/A   39C    P0             127W / 700W |   7713MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:3B:00.0 Off |                    0 |
| N/A   32C    P0             114W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:4C:00.0 Off |                    0 |
| N/A   30C    P0             118W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   38C    P0             130W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:9B:00.0 Off |                    0 |
| N/A   39C    P0             124W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:BB:00.0 Off |                    0 |
| N/A   31C    P0             115W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:CB:00.0 Off |                    0 |
| N/A   38C    P0             117W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:DB:00.0 Off |                    0 |
| N/A   30C    P0             118W / 700W |   3211MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
Training dataloader files: ['data/fineweb10B/fineweb_train_000001.bin', 'data/fineweb10B/fineweb_train_000002.bin', 'data/fineweb10B/fineweb_train_000003.bin', 'data/fineweb10B/fineweb_train_000004.bin', 'data/fineweb10B/fineweb_train_000005.bin', 'data/fineweb10B/fineweb_train_000006.bin', 'data/fineweb10B/fineweb_train_000007.bin', 'data/fineweb10B/fineweb_train_000008.bin', 'data/fineweb10B/fineweb_train_000009.bin']
Validation dataloader files: ['data/fineweb10B/fineweb_val_000000.bin']
====================================================================================================
step:0/1370 val_loss:10.8258 train_time:0ms step_avg:nanms
step:1/1370 train_time:29447ms step_avg:nanms
step:2/1370 train_time:29523ms step_avg:nanms
step:3/1370 train_time:29707ms step_avg:nanms
step:4/1370 train_time:29841ms step_avg:nanms
step:5/1370 train_time:29974ms step_avg:nanms
step:6/1370 train_time:30110ms step_avg:nanms
step:7/1370 train_time:30246ms step_avg:nanms
step:8/1370 train_time:30379ms step_avg:nanms
step:9/1370 train_time:30512ms step_avg:nanms
step:10/1370 train_time:30654ms step_avg:nanms
step:11/1370 train_time:137ms step_avg:nanms
step:12/1370 train_time:273ms step_avg:nanms
step:13/1370 train_time:409ms step_avg:136.34ms
step:14/1370 train_time:545ms step_avg:136.22ms
step:15/1370 train_time:679ms step_avg:135.90ms
step:16/1370 train_time:815ms step_avg:135.86ms
step:17/1370 train_time:952ms step_avg:135.96ms
step:18/1370 train_time:1089ms step_avg:136.09ms
step:19/1370 train_time:1224ms step_avg:135.96ms
step:20/1370 train_time:1361ms step_avg:136.08ms
step:21/1370 train_time:1498ms step_avg:136.16ms
step:22/1370 train_time:1632ms step_avg:136.01ms
step:23/1370 train_time:1768ms step_avg:136.03ms
step:24/1370 train_time:1904ms step_avg:135.99ms
step:25/1370 train_time:2039ms step_avg:135.95ms
step:26/1370 train_time:2175ms step_avg:135.96ms
step:27/1370 train_time:2312ms step_avg:135.98ms
step:28/1370 train_time:2449ms step_avg:136.07ms
step:29/1370 train_time:2586ms step_avg:136.13ms
step:30/1370 train_time:2722ms step_avg:136.08ms
step:31/1370 train_time:2858ms step_avg:136.10ms
step:32/1370 train_time:2995ms step_avg:136.14ms
step:33/1370 train_time:3130ms step_avg:136.08ms
step:34/1370 train_time:3268ms step_avg:136.15ms
step:35/1370 train_time:3403ms step_avg:136.12ms
step:36/1370 train_time:3539ms step_avg:136.12ms
step:37/1370 train_time:3675ms step_avg:136.12ms
step:38/1370 train_time:3812ms step_avg:136.14ms
step:39/1370 train_time:3949ms step_avg:136.16ms
step:40/1370 train_time:4085ms step_avg:136.15ms
step:41/1370 train_time:4221ms step_avg:136.17ms
step:42/1370 train_time:4358ms step_avg:136.19ms
step:43/1370 train_time:4495ms step_avg:136.22ms
step:44/1370 train_time:4632ms step_avg:136.24ms
step:45/1370 train_time:4770ms step_avg:136.27ms
step:46/1370 train_time:4907ms step_avg:136.30ms
step:47/1370 train_time:5042ms step_avg:136.27ms
step:48/1370 train_time:5179ms step_avg:136.29ms
step:49/1370 train_time:5314ms step_avg:136.26ms
step:50/1370 train_time:5450ms step_avg:136.26ms
step:51/1370 train_time:5587ms step_avg:136.26ms
step:52/1370 train_time:5722ms step_avg:136.24ms
step:53/1370 train_time:5857ms step_avg:136.21ms
step:54/1370 train_time:5995ms step_avg:136.25ms
step:55/1370 train_time:6131ms step_avg:136.24ms
step:56/1370 train_time:6268ms step_avg:136.26ms
step:57/1370 train_time:6404ms step_avg:136.25ms
step:58/1370 train_time:6539ms step_avg:136.24ms
step:59/1370 train_time:6676ms step_avg:136.24ms
step:60/1370 train_time:6813ms step_avg:136.26ms
step:61/1370 train_time:6949ms step_avg:136.25ms
step:62/1370 train_time:7086ms step_avg:136.27ms
step:63/1370 train_time:7221ms step_avg:136.25ms
step:64/1370 train_time:7358ms step_avg:136.27ms
step:65/1370 train_time:7496ms step_avg:136.29ms
step:66/1370 train_time:7632ms step_avg:136.29ms
step:67/1370 train_time:7769ms step_avg:136.30ms
step:68/1370 train_time:7906ms step_avg:136.30ms
step:69/1370 train_time:8041ms step_avg:136.28ms
step:70/1370 train_time:8176ms step_avg:136.27ms
step:71/1370 train_time:8313ms step_avg:136.27ms
step:72/1370 train_time:8449ms step_avg:136.28ms
step:73/1370 train_time:8586ms step_avg:136.29ms
step:74/1370 train_time:8722ms step_avg:136.28ms
step:75/1370 train_time:8859ms step_avg:136.29ms
step:76/1370 train_time:8997ms step_avg:136.32ms
step:77/1370 train_time:9133ms step_avg:136.31ms
step:78/1370 train_time:9269ms step_avg:136.31ms
step:79/1370 train_time:9406ms step_avg:136.32ms
step:80/1370 train_time:9541ms step_avg:136.30ms
step:81/1370 train_time:9677ms step_avg:136.29ms
step:82/1370 train_time:9813ms step_avg:136.29ms
step:83/1370 train_time:9950ms step_avg:136.30ms
step:84/1370 train_time:10087ms step_avg:136.31ms
step:85/1370 train_time:10223ms step_avg:136.30ms
step:86/1370 train_time:10360ms step_avg:136.31ms
step:87/1370 train_time:10496ms step_avg:136.32ms
step:88/1370 train_time:10632ms step_avg:136.30ms
step:89/1370 train_time:10770ms step_avg:136.33ms
step:90/1370 train_time:10907ms step_avg:136.34ms
step:91/1370 train_time:11043ms step_avg:136.33ms
step:92/1370 train_time:11180ms step_avg:136.34ms
step:93/1370 train_time:11318ms step_avg:136.36ms
step:94/1370 train_time:11454ms step_avg:136.36ms
step:95/1370 train_time:11590ms step_avg:136.36ms
step:96/1370 train_time:11727ms step_avg:136.35ms
step:97/1370 train_time:11863ms step_avg:136.36ms
step:98/1370 train_time:12001ms step_avg:136.37ms
step:99/1370 train_time:12139ms step_avg:136.39ms
step:100/1370 train_time:12274ms step_avg:136.38ms
step:101/1370 train_time:12412ms step_avg:136.40ms
step:102/1370 train_time:12549ms step_avg:136.40ms
step:103/1370 train_time:12688ms step_avg:136.42ms
step:104/1370 train_time:12826ms step_avg:136.45ms
step:105/1370 train_time:12967ms step_avg:136.49ms
step:106/1370 train_time:13107ms step_avg:136.53ms
step:107/1370 train_time:13246ms step_avg:136.55ms
step:108/1370 train_time:13386ms step_avg:136.59ms
step:109/1370 train_time:13524ms step_avg:136.61ms
step:110/1370 train_time:13663ms step_avg:136.63ms
step:111/1370 train_time:13804ms step_avg:136.67ms
step:112/1370 train_time:13943ms step_avg:136.70ms
step:113/1370 train_time:14084ms step_avg:136.73ms
step:114/1370 train_time:14223ms step_avg:136.76ms
step:115/1370 train_time:14363ms step_avg:136.79ms
step:116/1370 train_time:14504ms step_avg:136.83ms
step:117/1370 train_time:14643ms step_avg:136.85ms
step:118/1370 train_time:14783ms step_avg:136.88ms
step:119/1370 train_time:14922ms step_avg:136.90ms
step:120/1370 train_time:15064ms step_avg:136.94ms
step:121/1370 train_time:15204ms step_avg:136.98ms
step:122/1370 train_time:15345ms step_avg:137.01ms
step:123/1370 train_time:15486ms step_avg:137.04ms
step:124/1370 train_time:15625ms step_avg:137.06ms
step:125/1370 train_time:15765ms step_avg:137.08ms
step:125/1370 val_loss:4.3706 train_time:15827ms step_avg:137.63ms
step:126/1370 train_time:15908ms step_avg:137.13ms
step:127/1370 train_time:16051ms step_avg:137.19ms
step:128/1370 train_time:16191ms step_avg:137.22ms
step:129/1370 train_time:16330ms step_avg:137.23ms
step:130/1370 train_time:16469ms step_avg:137.25ms
step:131/1370 train_time:16607ms step_avg:137.25ms
step:132/1370 train_time:16746ms step_avg:137.26ms
step:133/1370 train_time:16889ms step_avg:137.31ms
step:134/1370 train_time:17031ms step_avg:137.35ms
step:135/1370 train_time:17172ms step_avg:137.37ms
step:136/1370 train_time:17311ms step_avg:137.39ms
step:137/1370 train_time:17450ms step_avg:137.40ms
step:138/1370 train_time:17590ms step_avg:137.42ms
step:139/1370 train_time:17730ms step_avg:137.45ms
step:140/1370 train_time:17870ms step_avg:137.46ms
step:141/1370 train_time:18012ms step_avg:137.50ms
step:142/1370 train_time:18150ms step_avg:137.50ms
step:143/1370 train_time:18292ms step_avg:137.53ms
step:144/1370 train_time:18431ms step_avg:137.55ms
step:145/1370 train_time:18571ms step_avg:137.56ms
step:146/1370 train_time:18709ms step_avg:137.57ms
step:147/1370 train_time:18850ms step_avg:137.59ms
step:148/1370 train_time:18990ms step_avg:137.61ms
step:149/1370 train_time:19130ms step_avg:137.63ms
step:150/1370 train_time:19271ms step_avg:137.65ms
step:151/1370 train_time:19412ms step_avg:137.67ms
step:152/1370 train_time:19552ms step_avg:137.69ms
step:153/1370 train_time:19692ms step_avg:137.70ms
step:154/1370 train_time:19832ms step_avg:137.72ms
step:155/1370 train_time:19973ms step_avg:137.74ms
step:156/1370 train_time:20114ms step_avg:137.77ms
step:157/1370 train_time:20252ms step_avg:137.77ms
step:158/1370 train_time:20393ms step_avg:137.79ms
step:159/1370 train_time:20534ms step_avg:137.82ms
step:160/1370 train_time:20676ms step_avg:137.84ms
step:161/1370 train_time:20818ms step_avg:137.86ms
step:162/1370 train_time:20957ms step_avg:137.87ms
step:163/1370 train_time:21098ms step_avg:137.90ms
step:164/1370 train_time:21239ms step_avg:137.92ms
step:165/1370 train_time:21378ms step_avg:137.92ms
step:166/1370 train_time:21520ms step_avg:137.95ms
step:167/1370 train_time:21659ms step_avg:137.95ms
step:168/1370 train_time:21798ms step_avg:137.96ms
step:169/1370 train_time:21939ms step_avg:137.98ms
step:170/1370 train_time:22079ms step_avg:137.99ms
step:171/1370 train_time:22220ms step_avg:138.01ms
step:172/1370 train_time:22361ms step_avg:138.03ms
step:173/1370 train_time:22501ms step_avg:138.04ms
step:174/1370 train_time:22641ms step_avg:138.06ms
step:175/1370 train_time:22781ms step_avg:138.07ms
step:176/1370 train_time:22922ms step_avg:138.08ms
step:177/1370 train_time:23061ms step_avg:138.09ms
step:178/1370 train_time:23202ms step_avg:138.11ms
step:179/1370 train_time:23343ms step_avg:138.12ms
step:180/1370 train_time:23483ms step_avg:138.13ms
step:181/1370 train_time:23624ms step_avg:138.15ms
step:182/1370 train_time:23764ms step_avg:138.16ms
step:183/1370 train_time:23904ms step_avg:138.17ms
step:184/1370 train_time:24046ms step_avg:138.20ms
step:185/1370 train_time:24187ms step_avg:138.21ms
step:186/1370 train_time:24328ms step_avg:138.23ms
step:187/1370 train_time:24469ms step_avg:138.24ms
step:188/1370 train_time:24610ms step_avg:138.26ms
step:189/1370 train_time:24750ms step_avg:138.27ms
step:190/1370 train_time:24889ms step_avg:138.27ms
step:191/1370 train_time:25063ms step_avg:138.47ms
step:192/1370 train_time:25203ms step_avg:138.48ms
step:193/1370 train_time:25342ms step_avg:138.48ms
step:194/1370 train_time:25481ms step_avg:138.48ms
step:195/1370 train_time:25620ms step_avg:138.49ms
step:196/1370 train_time:25758ms step_avg:138.48ms
step:197/1370 train_time:25901ms step_avg:138.51ms
step:198/1370 train_time:26045ms step_avg:138.54ms
step:199/1370 train_time:26184ms step_avg:138.54ms
step:200/1370 train_time:26325ms step_avg:138.55ms
step:201/1370 train_time:26464ms step_avg:138.55ms
step:202/1370 train_time:26603ms step_avg:138.56ms
step:203/1370 train_time:26743ms step_avg:138.56ms
step:204/1370 train_time:26884ms step_avg:138.58ms
step:205/1370 train_time:27030ms step_avg:138.62ms
step:206/1370 train_time:27174ms step_avg:138.64ms
step:207/1370 train_time:27318ms step_avg:138.67ms
step:208/1370 train_time:27459ms step_avg:138.68ms
step:209/1370 train_time:27600ms step_avg:138.69ms
step:210/1370 train_time:27741ms step_avg:138.70ms
step:211/1370 train_time:27882ms step_avg:138.72ms
step:212/1370 train_time:28026ms step_avg:138.74ms
step:213/1370 train_time:28168ms step_avg:138.76ms
step:214/1370 train_time:28312ms step_avg:138.78ms
step:215/1370 train_time:28454ms step_avg:138.80ms
step:216/1370 train_time:28595ms step_avg:138.81ms
step:217/1370 train_time:28737ms step_avg:138.83ms
step:218/1370 train_time:28879ms step_avg:138.84ms
step:219/1370 train_time:29022ms step_avg:138.86ms
step:220/1370 train_time:29164ms step_avg:138.88ms
step:221/1370 train_time:29308ms step_avg:138.90ms
step:222/1370 train_time:29451ms step_avg:138.92ms
step:223/1370 train_time:29594ms step_avg:138.94ms
step:224/1370 train_time:29737ms step_avg:138.96ms
step:225/1370 train_time:29879ms step_avg:138.97ms
step:226/1370 train_time:30022ms step_avg:138.99ms
step:227/1370 train_time:30164ms step_avg:139.01ms
step:228/1370 train_time:30308ms step_avg:139.03ms
step:229/1370 train_time:30451ms step_avg:139.05ms
step:230/1370 train_time:30594ms step_avg:139.06ms
step:231/1370 train_time:30737ms step_avg:139.08ms
step:232/1370 train_time:30879ms step_avg:139.10ms
step:233/1370 train_time:31022ms step_avg:139.11ms
step:234/1370 train_time:31164ms step_avg:139.12ms
step:235/1370 train_time:31307ms step_avg:139.14ms
step:236/1370 train_time:31450ms step_avg:139.16ms
step:237/1370 train_time:31594ms step_avg:139.18ms
step:238/1370 train_time:31737ms step_avg:139.20ms
step:239/1370 train_time:31878ms step_avg:139.21ms
step:240/1370 train_time:32021ms step_avg:139.22ms
step:241/1370 train_time:32164ms step_avg:139.24ms
step:242/1370 train_time:32307ms step_avg:139.25ms
step:243/1370 train_time:32449ms step_avg:139.27ms
step:244/1370 train_time:32593ms step_avg:139.29ms
step:245/1370 train_time:32736ms step_avg:139.30ms
step:246/1370 train_time:32877ms step_avg:139.31ms
step:247/1370 train_time:33020ms step_avg:139.33ms
step:248/1370 train_time:33162ms step_avg:139.33ms
step:249/1370 train_time:33305ms step_avg:139.35ms
step:250/1370 train_time:33447ms step_avg:139.36ms
step:250/1370 val_loss:3.9547 train_time:33513ms step_avg:139.64ms
step:251/1370 train_time:33593ms step_avg:139.39ms
step:252/1370 train_time:33739ms step_avg:139.42ms
step:253/1370 train_time:33880ms step_avg:139.42ms
step:254/1370 train_time:34022ms step_avg:139.43ms
step:255/1370 train_time:34162ms step_avg:139.44ms
step:256/1370 train_time:34304ms step_avg:139.45ms
step:257/1370 train_time:34445ms step_avg:139.45ms
step:258/1370 train_time:34592ms step_avg:139.49ms
step:259/1370 train_time:34738ms step_avg:139.51ms
step:260/1370 train_time:34880ms step_avg:139.52ms
step:261/1370 train_time:35022ms step_avg:139.53ms
step:262/1370 train_time:35163ms step_avg:139.54ms
step:263/1370 train_time:35306ms step_avg:139.55ms
step:264/1370 train_time:35448ms step_avg:139.56ms
step:265/1370 train_time:35592ms step_avg:139.58ms
step:266/1370 train_time:35736ms step_avg:139.59ms
step:267/1370 train_time:35879ms step_avg:139.61ms
step:268/1370 train_time:36022ms step_avg:139.62ms
step:269/1370 train_time:36162ms step_avg:139.62ms
step:270/1370 train_time:36304ms step_avg:139.63ms
step:271/1370 train_time:36447ms step_avg:139.64ms
step:272/1370 train_time:36592ms step_avg:139.66ms
step:273/1370 train_time:36736ms step_avg:139.68ms
step:274/1370 train_time:36878ms step_avg:139.69ms
step:275/1370 train_time:37020ms step_avg:139.70ms
step:276/1370 train_time:37162ms step_avg:139.71ms
step:277/1370 train_time:37303ms step_avg:139.71ms
step:278/1370 train_time:37446ms step_avg:139.72ms
step:279/1370 train_time:37591ms step_avg:139.74ms
step:280/1370 train_time:37735ms step_avg:139.76ms
step:281/1370 train_time:37877ms step_avg:139.77ms
step:282/1370 train_time:38021ms step_avg:139.78ms
step:283/1370 train_time:38163ms step_avg:139.79ms
step:284/1370 train_time:38305ms step_avg:139.80ms
step:285/1370 train_time:38448ms step_avg:139.81ms
step:286/1370 train_time:38593ms step_avg:139.83ms
step:287/1370 train_time:38736ms step_avg:139.84ms
step:288/1370 train_time:38877ms step_avg:139.85ms
step:289/1370 train_time:39020ms step_avg:139.86ms
step:290/1370 train_time:39162ms step_avg:139.87ms
step:291/1370 train_time:39305ms step_avg:139.87ms
step:292/1370 train_time:39448ms step_avg:139.89ms
step:293/1370 train_time:39593ms step_avg:139.91ms
step:294/1370 train_time:39736ms step_avg:139.92ms
step:295/1370 train_time:39877ms step_avg:139.92ms
step:296/1370 train_time:40020ms step_avg:139.93ms
step:297/1370 train_time:40161ms step_avg:139.93ms
step:298/1370 train_time:40304ms step_avg:139.95ms
step:299/1370 train_time:40449ms step_avg:139.96ms
step:300/1370 train_time:40593ms step_avg:139.98ms
step:301/1370 train_time:40737ms step_avg:139.99ms
step:302/1370 train_time:40879ms step_avg:140.00ms
step:303/1370 train_time:41020ms step_avg:140.00ms
step:304/1370 train_time:41163ms step_avg:140.01ms
step:305/1370 train_time:41306ms step_avg:140.02ms
step:306/1370 train_time:41450ms step_avg:140.03ms
step:307/1370 train_time:41597ms step_avg:140.06ms
step:308/1370 train_time:41742ms step_avg:140.07ms
step:309/1370 train_time:41884ms step_avg:140.08ms
step:310/1370 train_time:42031ms step_avg:140.10ms
step:311/1370 train_time:42176ms step_avg:140.12ms
step:312/1370 train_time:42321ms step_avg:140.14ms
step:313/1370 train_time:42465ms step_avg:140.15ms
step:314/1370 train_time:42612ms step_avg:140.17ms
step:315/1370 train_time:42758ms step_avg:140.19ms
step:316/1370 train_time:42902ms step_avg:140.20ms
step:317/1370 train_time:43046ms step_avg:140.22ms
step:318/1370 train_time:43191ms step_avg:140.23ms
step:319/1370 train_time:43337ms step_avg:140.25ms
step:320/1370 train_time:43480ms step_avg:140.26ms
step:321/1370 train_time:43625ms step_avg:140.27ms
step:322/1370 train_time:43769ms step_avg:140.29ms
step:323/1370 train_time:43916ms step_avg:140.31ms
step:324/1370 train_time:44060ms step_avg:140.32ms
step:325/1370 train_time:44204ms step_avg:140.33ms
step:326/1370 train_time:44349ms step_avg:140.35ms
step:327/1370 train_time:44495ms step_avg:140.36ms
step:328/1370 train_time:44640ms step_avg:140.38ms
step:329/1370 train_time:44785ms step_avg:140.39ms
step:330/1370 train_time:44930ms step_avg:140.41ms
step:331/1370 train_time:45075ms step_avg:140.42ms
step:332/1370 train_time:45219ms step_avg:140.43ms
step:333/1370 train_time:45363ms step_avg:140.44ms
step:334/1370 train_time:45507ms step_avg:140.45ms
step:335/1370 train_time:45653ms step_avg:140.47ms
step:336/1370 train_time:45798ms step_avg:140.48ms
step:337/1370 train_time:45942ms step_avg:140.50ms
step:338/1370 train_time:46086ms step_avg:140.51ms
step:339/1370 train_time:46234ms step_avg:140.53ms
step:340/1370 train_time:46378ms step_avg:140.54ms
step:341/1370 train_time:46523ms step_avg:140.55ms
step:342/1370 train_time:46668ms step_avg:140.57ms
step:343/1370 train_time:46815ms step_avg:140.59ms
step:344/1370 train_time:46959ms step_avg:140.60ms
step:345/1370 train_time:47104ms step_avg:140.61ms
step:346/1370 train_time:47250ms step_avg:140.63ms
step:347/1370 train_time:47396ms step_avg:140.64ms
step:348/1370 train_time:47540ms step_avg:140.65ms
step:349/1370 train_time:47683ms step_avg:140.66ms
step:350/1370 train_time:47830ms step_avg:140.68ms
step:351/1370 train_time:47974ms step_avg:140.69ms
step:352/1370 train_time:48119ms step_avg:140.70ms
step:353/1370 train_time:48262ms step_avg:140.71ms
step:354/1370 train_time:48407ms step_avg:140.72ms
step:355/1370 train_time:48552ms step_avg:140.73ms
step:356/1370 train_time:48699ms step_avg:140.75ms
step:357/1370 train_time:48843ms step_avg:140.76ms
step:358/1370 train_time:48987ms step_avg:140.77ms
step:359/1370 train_time:49133ms step_avg:140.78ms
step:360/1370 train_time:49277ms step_avg:140.79ms
step:361/1370 train_time:49422ms step_avg:140.80ms
step:362/1370 train_time:49567ms step_avg:140.81ms
step:363/1370 train_time:49712ms step_avg:140.83ms
step:364/1370 train_time:49858ms step_avg:140.84ms
step:365/1370 train_time:50003ms step_avg:140.85ms
step:366/1370 train_time:50150ms step_avg:140.87ms
step:367/1370 train_time:50295ms step_avg:140.88ms
step:368/1370 train_time:50439ms step_avg:140.89ms
step:369/1370 train_time:50582ms step_avg:140.90ms
step:370/1370 train_time:50730ms step_avg:140.92ms
step:371/1370 train_time:50875ms step_avg:140.93ms
step:372/1370 train_time:51021ms step_avg:140.94ms
step:373/1370 train_time:51164ms step_avg:140.95ms
step:374/1370 train_time:51310ms step_avg:140.96ms
step:375/1370 train_time:51454ms step_avg:140.97ms
step:375/1370 val_loss:3.7722 train_time:51519ms step_avg:141.15ms
step:376/1370 train_time:51600ms step_avg:140.98ms
step:377/1370 train_time:51749ms step_avg:141.00ms
step:378/1370 train_time:51896ms step_avg:141.02ms
step:379/1370 train_time:52041ms step_avg:141.03ms
step:380/1370 train_time:52185ms step_avg:141.04ms
step:381/1370 train_time:52361ms step_avg:141.14ms
step:382/1370 train_time:52517ms step_avg:141.17ms
step:383/1370 train_time:52660ms step_avg:141.18ms
step:384/1370 train_time:52804ms step_avg:141.19ms
step:385/1370 train_time:52948ms step_avg:141.19ms
step:386/1370 train_time:53090ms step_avg:141.20ms
step:387/1370 train_time:53236ms step_avg:141.21ms
step:388/1370 train_time:53386ms step_avg:141.23ms
step:389/1370 train_time:53531ms step_avg:141.24ms
step:390/1370 train_time:53677ms step_avg:141.26ms
step:391/1370 train_time:53822ms step_avg:141.27ms
step:392/1370 train_time:53965ms step_avg:141.27ms
step:393/1370 train_time:54109ms step_avg:141.28ms
step:394/1370 train_time:54254ms step_avg:141.29ms
step:395/1370 train_time:54402ms step_avg:141.30ms
step:396/1370 train_time:54547ms step_avg:141.31ms
step:397/1370 train_time:54690ms step_avg:141.32ms
step:398/1370 train_time:54836ms step_avg:141.33ms
step:399/1370 train_time:54981ms step_avg:141.34ms
step:400/1370 train_time:55125ms step_avg:141.35ms
step:401/1370 train_time:55268ms step_avg:141.35ms
step:402/1370 train_time:55414ms step_avg:141.36ms
step:403/1370 train_time:55560ms step_avg:141.37ms
step:404/1370 train_time:55706ms step_avg:141.39ms
step:405/1370 train_time:55850ms step_avg:141.39ms
step:406/1370 train_time:55994ms step_avg:141.40ms
step:407/1370 train_time:56139ms step_avg:141.41ms
step:408/1370 train_time:56285ms step_avg:141.42ms
step:409/1370 train_time:56430ms step_avg:141.43ms
step:410/1370 train_time:56577ms step_avg:141.44ms
step:411/1370 train_time:56725ms step_avg:141.46ms
step:412/1370 train_time:56870ms step_avg:141.47ms
step:413/1370 train_time:57018ms step_avg:141.48ms
step:414/1370 train_time:57163ms step_avg:141.49ms
step:415/1370 train_time:57309ms step_avg:141.50ms
step:416/1370 train_time:57453ms step_avg:141.51ms
step:417/1370 train_time:57602ms step_avg:141.53ms
step:418/1370 train_time:57749ms step_avg:141.54ms
step:419/1370 train_time:57898ms step_avg:141.56ms
step:420/1370 train_time:58045ms step_avg:141.57ms
step:421/1370 train_time:58191ms step_avg:141.58ms
step:422/1370 train_time:58337ms step_avg:141.59ms
step:423/1370 train_time:58484ms step_avg:141.61ms
step:424/1370 train_time:58630ms step_avg:141.62ms
step:425/1370 train_time:58779ms step_avg:141.64ms
step:426/1370 train_time:58926ms step_avg:141.65ms
step:427/1370 train_time:59072ms step_avg:141.66ms
step:428/1370 train_time:59219ms step_avg:141.67ms
step:429/1370 train_time:59365ms step_avg:141.68ms
step:430/1370 train_time:59510ms step_avg:141.69ms
step:431/1370 train_time:59657ms step_avg:141.70ms
step:432/1370 train_time:59805ms step_avg:141.72ms
step:433/1370 train_time:59951ms step_avg:141.73ms
step:434/1370 train_time:60099ms step_avg:141.74ms
step:435/1370 train_time:60247ms step_avg:141.76ms
step:436/1370 train_time:60392ms step_avg:141.77ms
step:437/1370 train_time:60539ms step_avg:141.78ms
step:438/1370 train_time:60686ms step_avg:141.79ms
step:439/1370 train_time:60832ms step_avg:141.80ms
step:440/1370 train_time:60980ms step_avg:141.81ms
step:441/1370 train_time:61127ms step_avg:141.83ms
step:442/1370 train_time:61275ms step_avg:141.84ms
step:443/1370 train_time:61423ms step_avg:141.85ms
step:444/1370 train_time:61567ms step_avg:141.86ms
step:445/1370 train_time:61713ms step_avg:141.87ms
step:446/1370 train_time:61860ms step_avg:141.88ms
step:447/1370 train_time:62006ms step_avg:141.89ms
step:448/1370 train_time:62151ms step_avg:141.90ms
step:449/1370 train_time:62299ms step_avg:141.91ms
step:450/1370 train_time:62445ms step_avg:141.92ms
step:451/1370 train_time:62591ms step_avg:141.93ms
step:452/1370 train_time:62738ms step_avg:141.94ms
step:453/1370 train_time:62886ms step_avg:141.95ms
step:454/1370 train_time:63032ms step_avg:141.96ms
step:455/1370 train_time:63182ms step_avg:141.98ms
step:456/1370 train_time:63328ms step_avg:141.99ms
step:457/1370 train_time:63474ms step_avg:142.00ms
step:458/1370 train_time:63622ms step_avg:142.01ms
step:459/1370 train_time:63767ms step_avg:142.02ms
step:460/1370 train_time:63915ms step_avg:142.03ms
step:461/1370 train_time:64062ms step_avg:142.04ms
step:462/1370 train_time:64210ms step_avg:142.06ms
step:463/1370 train_time:64357ms step_avg:142.07ms
step:464/1370 train_time:64504ms step_avg:142.08ms
step:465/1370 train_time:64649ms step_avg:142.09ms
step:466/1370 train_time:64797ms step_avg:142.10ms
step:467/1370 train_time:64944ms step_avg:142.11ms
step:468/1370 train_time:65089ms step_avg:142.12ms
step:469/1370 train_time:65237ms step_avg:142.13ms
step:470/1370 train_time:65385ms step_avg:142.14ms
step:471/1370 train_time:65532ms step_avg:142.15ms
step:472/1370 train_time:65679ms step_avg:142.16ms
step:473/1370 train_time:65826ms step_avg:142.17ms
step:474/1370 train_time:65971ms step_avg:142.18ms
step:475/1370 train_time:66119ms step_avg:142.19ms
step:476/1370 train_time:66266ms step_avg:142.20ms
step:477/1370 train_time:66412ms step_avg:142.21ms
step:478/1370 train_time:66559ms step_avg:142.22ms
step:479/1370 train_time:66706ms step_avg:142.23ms
step:480/1370 train_time:66852ms step_avg:142.24ms
step:481/1370 train_time:66999ms step_avg:142.25ms
step:482/1370 train_time:67146ms step_avg:142.26ms
step:483/1370 train_time:67293ms step_avg:142.27ms
step:484/1370 train_time:67440ms step_avg:142.28ms
step:485/1370 train_time:67587ms step_avg:142.29ms
step:486/1370 train_time:67734ms step_avg:142.30ms
step:487/1370 train_time:67883ms step_avg:142.31ms
step:488/1370 train_time:68029ms step_avg:142.32ms
step:489/1370 train_time:68174ms step_avg:142.33ms
step:490/1370 train_time:68323ms step_avg:142.34ms
step:491/1370 train_time:68468ms step_avg:142.34ms
step:492/1370 train_time:68616ms step_avg:142.36ms
step:493/1370 train_time:68763ms step_avg:142.37ms
step:494/1370 train_time:68909ms step_avg:142.37ms
step:495/1370 train_time:69055ms step_avg:142.38ms
step:496/1370 train_time:69203ms step_avg:142.39ms
step:497/1370 train_time:69348ms step_avg:142.40ms
step:498/1370 train_time:69495ms step_avg:142.41ms
step:499/1370 train_time:69642ms step_avg:142.42ms
step:500/1370 train_time:69788ms step_avg:142.42ms
step:500/1370 val_loss:3.6552 train_time:69855ms step_avg:142.56ms
step:501/1370 train_time:69937ms step_avg:142.44ms
step:502/1370 train_time:70086ms step_avg:142.45ms
step:503/1370 train_time:70230ms step_avg:142.46ms
step:504/1370 train_time:70376ms step_avg:142.46ms
step:505/1370 train_time:70522ms step_avg:142.47ms
step:506/1370 train_time:70668ms step_avg:142.48ms
step:507/1370 train_time:70816ms step_avg:142.49ms
step:508/1370 train_time:70967ms step_avg:142.50ms
step:509/1370 train_time:71113ms step_avg:142.51ms
step:510/1370 train_time:71262ms step_avg:142.52ms
step:511/1370 train_time:71409ms step_avg:142.53ms
step:512/1370 train_time:71559ms step_avg:142.55ms
step:513/1370 train_time:71709ms step_avg:142.56ms
step:514/1370 train_time:71859ms step_avg:142.58ms
step:515/1370 train_time:72009ms step_avg:142.59ms
step:516/1370 train_time:72158ms step_avg:142.61ms
step:517/1370 train_time:72308ms step_avg:142.62ms
step:518/1370 train_time:72456ms step_avg:142.63ms
step:519/1370 train_time:72605ms step_avg:142.64ms
step:520/1370 train_time:72752ms step_avg:142.65ms
step:521/1370 train_time:72901ms step_avg:142.66ms
step:522/1370 train_time:73050ms step_avg:142.68ms
step:523/1370 train_time:73199ms step_avg:142.69ms
step:524/1370 train_time:73348ms step_avg:142.70ms
step:525/1370 train_time:73496ms step_avg:142.71ms
step:526/1370 train_time:73645ms step_avg:142.72ms
step:527/1370 train_time:73792ms step_avg:142.73ms
step:528/1370 train_time:73941ms step_avg:142.74ms
step:529/1370 train_time:74090ms step_avg:142.76ms
step:530/1370 train_time:74239ms step_avg:142.77ms
step:531/1370 train_time:74388ms step_avg:142.78ms
step:532/1370 train_time:74535ms step_avg:142.79ms
step:533/1370 train_time:74686ms step_avg:142.80ms
step:534/1370 train_time:74833ms step_avg:142.81ms
step:535/1370 train_time:74982ms step_avg:142.82ms
step:536/1370 train_time:75131ms step_avg:142.84ms
step:537/1370 train_time:75281ms step_avg:142.85ms
step:538/1370 train_time:75429ms step_avg:142.86ms
step:539/1370 train_time:75578ms step_avg:142.87ms
step:540/1370 train_time:75728ms step_avg:142.88ms
step:541/1370 train_time:75874ms step_avg:142.89ms
step:542/1370 train_time:76022ms step_avg:142.90ms
step:543/1370 train_time:76170ms step_avg:142.91ms
step:544/1370 train_time:76319ms step_avg:142.92ms
step:545/1370 train_time:76469ms step_avg:142.93ms
step:546/1370 train_time:76616ms step_avg:142.94ms
step:547/1370 train_time:76765ms step_avg:142.95ms
step:548/1370 train_time:76913ms step_avg:142.96ms
step:549/1370 train_time:77063ms step_avg:142.97ms
step:550/1370 train_time:77211ms step_avg:142.98ms
step:551/1370 train_time:77359ms step_avg:142.99ms
step:552/1370 train_time:77509ms step_avg:143.00ms
step:553/1370 train_time:77656ms step_avg:143.01ms
step:554/1370 train_time:77805ms step_avg:143.02ms
step:555/1370 train_time:77953ms step_avg:143.03ms
step:556/1370 train_time:78103ms step_avg:143.05ms
step:557/1370 train_time:78251ms step_avg:143.06ms
step:558/1370 train_time:78401ms step_avg:143.07ms
step:559/1370 train_time:78548ms step_avg:143.08ms
step:560/1370 train_time:78698ms step_avg:143.09ms
step:561/1370 train_time:78845ms step_avg:143.09ms
step:562/1370 train_time:78993ms step_avg:143.10ms
step:563/1370 train_time:79139ms step_avg:143.11ms
step:564/1370 train_time:79289ms step_avg:143.12ms
step:565/1370 train_time:79438ms step_avg:143.13ms
step:566/1370 train_time:79589ms step_avg:143.15ms
step:567/1370 train_time:79736ms step_avg:143.15ms
step:568/1370 train_time:79887ms step_avg:143.17ms
step:569/1370 train_time:80033ms step_avg:143.17ms
step:570/1370 train_time:80182ms step_avg:143.18ms
step:571/1370 train_time:80363ms step_avg:143.25ms
step:572/1370 train_time:80521ms step_avg:143.28ms
step:573/1370 train_time:80669ms step_avg:143.28ms
step:574/1370 train_time:80818ms step_avg:143.29ms
step:575/1370 train_time:80966ms step_avg:143.30ms
step:576/1370 train_time:81112ms step_avg:143.31ms
step:577/1370 train_time:81261ms step_avg:143.32ms
step:578/1370 train_time:81412ms step_avg:143.33ms
step:579/1370 train_time:81559ms step_avg:143.34ms
step:580/1370 train_time:81709ms step_avg:143.35ms
step:581/1370 train_time:81857ms step_avg:143.36ms
step:582/1370 train_time:82006ms step_avg:143.37ms
step:583/1370 train_time:82151ms step_avg:143.37ms
step:584/1370 train_time:82302ms step_avg:143.38ms
step:585/1370 train_time:82450ms step_avg:143.39ms
step:586/1370 train_time:82601ms step_avg:143.40ms
step:587/1370 train_time:82750ms step_avg:143.41ms
step:588/1370 train_time:82899ms step_avg:143.42ms
step:589/1370 train_time:83047ms step_avg:143.43ms
step:590/1370 train_time:83195ms step_avg:143.44ms
step:591/1370 train_time:83345ms step_avg:143.45ms
step:592/1370 train_time:83493ms step_avg:143.46ms
step:593/1370 train_time:83642ms step_avg:143.47ms
step:594/1370 train_time:83790ms step_avg:143.48ms
step:595/1370 train_time:83938ms step_avg:143.48ms
step:596/1370 train_time:84087ms step_avg:143.49ms
step:597/1370 train_time:84233ms step_avg:143.50ms
step:598/1370 train_time:84384ms step_avg:143.51ms
step:599/1370 train_time:84531ms step_avg:143.52ms
step:600/1370 train_time:84679ms step_avg:143.52ms
step:601/1370 train_time:84829ms step_avg:143.53ms
step:602/1370 train_time:84977ms step_avg:143.54ms
step:603/1370 train_time:85126ms step_avg:143.55ms
step:604/1370 train_time:85272ms step_avg:143.56ms
step:605/1370 train_time:85421ms step_avg:143.56ms
step:606/1370 train_time:85572ms step_avg:143.58ms
step:607/1370 train_time:85720ms step_avg:143.58ms
step:608/1370 train_time:85870ms step_avg:143.60ms
step:609/1370 train_time:86016ms step_avg:143.60ms
step:610/1370 train_time:86167ms step_avg:143.61ms
step:611/1370 train_time:86315ms step_avg:143.62ms
step:612/1370 train_time:86466ms step_avg:143.63ms
step:613/1370 train_time:86615ms step_avg:143.64ms
step:614/1370 train_time:86768ms step_avg:143.66ms
step:615/1370 train_time:86916ms step_avg:143.66ms
step:616/1370 train_time:87068ms step_avg:143.68ms
step:617/1370 train_time:87218ms step_avg:143.69ms
step:618/1370 train_time:87369ms step_avg:143.70ms
step:619/1370 train_time:87518ms step_avg:143.71ms
step:620/1370 train_time:87669ms step_avg:143.72ms
step:621/1370 train_time:87818ms step_avg:143.73ms
step:622/1370 train_time:87971ms step_avg:143.74ms
step:623/1370 train_time:88120ms step_avg:143.75ms
step:624/1370 train_time:88270ms step_avg:143.76ms
step:625/1370 train_time:88420ms step_avg:143.77ms
step:625/1370 val_loss:3.5747 train_time:88490ms step_avg:143.89ms
step:626/1370 train_time:88571ms step_avg:143.78ms
step:627/1370 train_time:88725ms step_avg:143.80ms
step:628/1370 train_time:88872ms step_avg:143.81ms
step:629/1370 train_time:89022ms step_avg:143.82ms
step:630/1370 train_time:89169ms step_avg:143.82ms
step:631/1370 train_time:89319ms step_avg:143.83ms
step:632/1370 train_time:89469ms step_avg:143.84ms
step:633/1370 train_time:89622ms step_avg:143.86ms
step:634/1370 train_time:89772ms step_avg:143.87ms
step:635/1370 train_time:89923ms step_avg:143.88ms
step:636/1370 train_time:90072ms step_avg:143.89ms
step:637/1370 train_time:90223ms step_avg:143.90ms
step:638/1370 train_time:90371ms step_avg:143.90ms
step:639/1370 train_time:90524ms step_avg:143.92ms
step:640/1370 train_time:90672ms step_avg:143.92ms
step:641/1370 train_time:90823ms step_avg:143.94ms
step:642/1370 train_time:90972ms step_avg:143.94ms
step:643/1370 train_time:91124ms step_avg:143.96ms
step:644/1370 train_time:91272ms step_avg:143.96ms
step:645/1370 train_time:91424ms step_avg:143.98ms
step:646/1370 train_time:91574ms step_avg:143.98ms
step:647/1370 train_time:91725ms step_avg:144.00ms
step:648/1370 train_time:91876ms step_avg:144.01ms
step:649/1370 train_time:92027ms step_avg:144.02ms
step:650/1370 train_time:92178ms step_avg:144.03ms
step:651/1370 train_time:92327ms step_avg:144.04ms
step:652/1370 train_time:92477ms step_avg:144.04ms
step:653/1370 train_time:92627ms step_avg:144.05ms
step:654/1370 train_time:92779ms step_avg:144.07ms
step:655/1370 train_time:92927ms step_avg:144.07ms
step:656/1370 train_time:93077ms step_avg:144.08ms
step:657/1370 train_time:93227ms step_avg:144.09ms
step:658/1370 train_time:93377ms step_avg:144.10ms
step:659/1370 train_time:93527ms step_avg:144.11ms
step:660/1370 train_time:93676ms step_avg:144.12ms
step:661/1370 train_time:93827ms step_avg:144.13ms
step:662/1370 train_time:93978ms step_avg:144.14ms
step:663/1370 train_time:94127ms step_avg:144.15ms
step:664/1370 train_time:94279ms step_avg:144.16ms
step:665/1370 train_time:94428ms step_avg:144.17ms
step:666/1370 train_time:94577ms step_avg:144.17ms
step:667/1370 train_time:94727ms step_avg:144.18ms
step:668/1370 train_time:94877ms step_avg:144.19ms
step:669/1370 train_time:95028ms step_avg:144.20ms
step:670/1370 train_time:95180ms step_avg:144.21ms
step:671/1370 train_time:95329ms step_avg:144.22ms
step:672/1370 train_time:95481ms step_avg:144.23ms
step:673/1370 train_time:95628ms step_avg:144.24ms
step:674/1370 train_time:95779ms step_avg:144.24ms
step:675/1370 train_time:95928ms step_avg:144.25ms
step:676/1370 train_time:96080ms step_avg:144.26ms
step:677/1370 train_time:96227ms step_avg:144.27ms
step:678/1370 train_time:96378ms step_avg:144.28ms
step:679/1370 train_time:96529ms step_avg:144.29ms
step:680/1370 train_time:96679ms step_avg:144.30ms
step:681/1370 train_time:96827ms step_avg:144.30ms
step:682/1370 train_time:96979ms step_avg:144.31ms
step:683/1370 train_time:97128ms step_avg:144.32ms
step:684/1370 train_time:97277ms step_avg:144.33ms
step:685/1370 train_time:97428ms step_avg:144.34ms
step:686/1370 train_time:97576ms step_avg:144.34ms
step:687/1370 train_time:97726ms step_avg:144.35ms
step:688/1370 train_time:97876ms step_avg:144.36ms
step:689/1370 train_time:98027ms step_avg:144.37ms
step:690/1370 train_time:98177ms step_avg:144.38ms
step:691/1370 train_time:98327ms step_avg:144.39ms
step:692/1370 train_time:98478ms step_avg:144.40ms
step:693/1370 train_time:98627ms step_avg:144.40ms
step:694/1370 train_time:98776ms step_avg:144.41ms
step:695/1370 train_time:98926ms step_avg:144.42ms
step:696/1370 train_time:99076ms step_avg:144.43ms
step:697/1370 train_time:99226ms step_avg:144.43ms
step:698/1370 train_time:99374ms step_avg:144.44ms
step:699/1370 train_time:99526ms step_avg:144.45ms
step:700/1370 train_time:99674ms step_avg:144.46ms
step:701/1370 train_time:99827ms step_avg:144.47ms
step:702/1370 train_time:99978ms step_avg:144.48ms
step:703/1370 train_time:100127ms step_avg:144.48ms
step:704/1370 train_time:100276ms step_avg:144.49ms
step:705/1370 train_time:100427ms step_avg:144.50ms
step:706/1370 train_time:100579ms step_avg:144.51ms
step:707/1370 train_time:100728ms step_avg:144.52ms
step:708/1370 train_time:100876ms step_avg:144.52ms
step:709/1370 train_time:101028ms step_avg:144.53ms
step:710/1370 train_time:101178ms step_avg:144.54ms
step:711/1370 train_time:101329ms step_avg:144.55ms
step:712/1370 train_time:101482ms step_avg:144.56ms
step:713/1370 train_time:101634ms step_avg:144.57ms
step:714/1370 train_time:101787ms step_avg:144.58ms
step:715/1370 train_time:101938ms step_avg:144.59ms
step:716/1370 train_time:102089ms step_avg:144.60ms
step:717/1370 train_time:102241ms step_avg:144.61ms
step:718/1370 train_time:102390ms step_avg:144.62ms
step:719/1370 train_time:102541ms step_avg:144.63ms
step:720/1370 train_time:102692ms step_avg:144.64ms
step:721/1370 train_time:102845ms step_avg:144.65ms
step:722/1370 train_time:102996ms step_avg:144.66ms
step:723/1370 train_time:103147ms step_avg:144.67ms
step:724/1370 train_time:103298ms step_avg:144.67ms
step:725/1370 train_time:103449ms step_avg:144.68ms
step:726/1370 train_time:103601ms step_avg:144.69ms
step:727/1370 train_time:103753ms step_avg:144.70ms
step:728/1370 train_time:103904ms step_avg:144.71ms
step:729/1370 train_time:104055ms step_avg:144.72ms
step:730/1370 train_time:104207ms step_avg:144.73ms
step:731/1370 train_time:104359ms step_avg:144.74ms
step:732/1370 train_time:104508ms step_avg:144.75ms
step:733/1370 train_time:104661ms step_avg:144.76ms
step:734/1370 train_time:104811ms step_avg:144.77ms
step:735/1370 train_time:104965ms step_avg:144.78ms
step:736/1370 train_time:105116ms step_avg:144.79ms
step:737/1370 train_time:105268ms step_avg:144.80ms
step:738/1370 train_time:105419ms step_avg:144.81ms
step:739/1370 train_time:105569ms step_avg:144.81ms
step:740/1370 train_time:105723ms step_avg:144.83ms
step:741/1370 train_time:105874ms step_avg:144.83ms
step:742/1370 train_time:106026ms step_avg:144.84ms
step:743/1370 train_time:106175ms step_avg:144.85ms
step:744/1370 train_time:106326ms step_avg:144.86ms
step:745/1370 train_time:106479ms step_avg:144.87ms
step:746/1370 train_time:106628ms step_avg:144.88ms
step:747/1370 train_time:106779ms step_avg:144.88ms
step:748/1370 train_time:106931ms step_avg:144.89ms
step:749/1370 train_time:107084ms step_avg:144.90ms
step:750/1370 train_time:107234ms step_avg:144.91ms
step:750/1370 val_loss:3.5212 train_time:107306ms step_avg:145.01ms
step:751/1370 train_time:107389ms step_avg:144.92ms
step:752/1370 train_time:107542ms step_avg:144.94ms
step:753/1370 train_time:107691ms step_avg:144.94ms
step:754/1370 train_time:107840ms step_avg:144.95ms
step:755/1370 train_time:107989ms step_avg:144.95ms
step:756/1370 train_time:108140ms step_avg:144.96ms
step:757/1370 train_time:108292ms step_avg:144.97ms
step:758/1370 train_time:108445ms step_avg:144.98ms
step:759/1370 train_time:108598ms step_avg:144.99ms
step:760/1370 train_time:108746ms step_avg:144.99ms
step:761/1370 train_time:108930ms step_avg:145.05ms
step:762/1370 train_time:109091ms step_avg:145.07ms
step:763/1370 train_time:109243ms step_avg:145.08ms
step:764/1370 train_time:109394ms step_avg:145.09ms
step:765/1370 train_time:109544ms step_avg:145.09ms
step:766/1370 train_time:109697ms step_avg:145.10ms
step:767/1370 train_time:109849ms step_avg:145.11ms
step:768/1370 train_time:110004ms step_avg:145.12ms
step:769/1370 train_time:110155ms step_avg:145.13ms
step:770/1370 train_time:110307ms step_avg:145.14ms
step:771/1370 train_time:110458ms step_avg:145.15ms
step:772/1370 train_time:110608ms step_avg:145.15ms
step:773/1370 train_time:110761ms step_avg:145.17ms
step:774/1370 train_time:110915ms step_avg:145.18ms
step:775/1370 train_time:111066ms step_avg:145.18ms
step:776/1370 train_time:111219ms step_avg:145.19ms
step:777/1370 train_time:111370ms step_avg:145.20ms
step:778/1370 train_time:111522ms step_avg:145.21ms
step:779/1370 train_time:111672ms step_avg:145.22ms
step:780/1370 train_time:111824ms step_avg:145.23ms
step:781/1370 train_time:111976ms step_avg:145.23ms
step:782/1370 train_time:112125ms step_avg:145.24ms
step:783/1370 train_time:112276ms step_avg:145.25ms
step:784/1370 train_time:112427ms step_avg:145.25ms
step:785/1370 train_time:112582ms step_avg:145.27ms
step:786/1370 train_time:112732ms step_avg:145.27ms
step:787/1370 train_time:112884ms step_avg:145.28ms
step:788/1370 train_time:113036ms step_avg:145.29ms
step:789/1370 train_time:113185ms step_avg:145.30ms
step:790/1370 train_time:113336ms step_avg:145.30ms
step:791/1370 train_time:113486ms step_avg:145.31ms
step:792/1370 train_time:113639ms step_avg:145.32ms
step:793/1370 train_time:113788ms step_avg:145.32ms
step:794/1370 train_time:113943ms step_avg:145.34ms
step:795/1370 train_time:114097ms step_avg:145.35ms
step:796/1370 train_time:114246ms step_avg:145.35ms
step:797/1370 train_time:114398ms step_avg:145.36ms
step:798/1370 train_time:114550ms step_avg:145.37ms
step:799/1370 train_time:114705ms step_avg:145.38ms
step:800/1370 train_time:114855ms step_avg:145.39ms
step:801/1370 train_time:115006ms step_avg:145.39ms
step:802/1370 train_time:115159ms step_avg:145.40ms
step:803/1370 train_time:115308ms step_avg:145.41ms
step:804/1370 train_time:115459ms step_avg:145.41ms
step:805/1370 train_time:115614ms step_avg:145.43ms
step:806/1370 train_time:115766ms step_avg:145.43ms
step:807/1370 train_time:115918ms step_avg:145.44ms
step:808/1370 train_time:116069ms step_avg:145.45ms
step:809/1370 train_time:116221ms step_avg:145.46ms
step:810/1370 train_time:116370ms step_avg:145.46ms
step:811/1370 train_time:116523ms step_avg:145.47ms
step:812/1370 train_time:116674ms step_avg:145.48ms
step:813/1370 train_time:116824ms step_avg:145.48ms
step:814/1370 train_time:116977ms step_avg:145.49ms
step:815/1370 train_time:117128ms step_avg:145.50ms
step:816/1370 train_time:117284ms step_avg:145.51ms
step:817/1370 train_time:117437ms step_avg:145.52ms
step:818/1370 train_time:117587ms step_avg:145.53ms
step:819/1370 train_time:117741ms step_avg:145.54ms
step:820/1370 train_time:117896ms step_avg:145.55ms
step:821/1370 train_time:118046ms step_avg:145.56ms
step:822/1370 train_time:118201ms step_avg:145.57ms
step:823/1370 train_time:118354ms step_avg:145.58ms
step:824/1370 train_time:118506ms step_avg:145.59ms
step:825/1370 train_time:118661ms step_avg:145.60ms
step:826/1370 train_time:118815ms step_avg:145.61ms
step:827/1370 train_time:118967ms step_avg:145.61ms
step:828/1370 train_time:119122ms step_avg:145.63ms
step:829/1370 train_time:119274ms step_avg:145.63ms
step:830/1370 train_time:119427ms step_avg:145.64ms
step:831/1370 train_time:119581ms step_avg:145.65ms
step:832/1370 train_time:119732ms step_avg:145.66ms
step:833/1370 train_time:119884ms step_avg:145.67ms
step:834/1370 train_time:120036ms step_avg:145.67ms
step:835/1370 train_time:120189ms step_avg:145.68ms
step:836/1370 train_time:120344ms step_avg:145.69ms
step:837/1370 train_time:120497ms step_avg:145.70ms
step:838/1370 train_time:120648ms step_avg:145.71ms
step:839/1370 train_time:120802ms step_avg:145.72ms
step:840/1370 train_time:120953ms step_avg:145.73ms
step:841/1370 train_time:121105ms step_avg:145.73ms
step:842/1370 train_time:121257ms step_avg:145.74ms
step:843/1370 train_time:121409ms step_avg:145.75ms
step:844/1370 train_time:121561ms step_avg:145.76ms
step:845/1370 train_time:121713ms step_avg:145.76ms
step:846/1370 train_time:121865ms step_avg:145.77ms
step:847/1370 train_time:122021ms step_avg:145.78ms
step:848/1370 train_time:122171ms step_avg:145.79ms
step:849/1370 train_time:122325ms step_avg:145.80ms
step:850/1370 train_time:122480ms step_avg:145.81ms
step:851/1370 train_time:122632ms step_avg:145.82ms
step:852/1370 train_time:122785ms step_avg:145.83ms
step:853/1370 train_time:122936ms step_avg:145.83ms
step:854/1370 train_time:123088ms step_avg:145.84ms
step:855/1370 train_time:123240ms step_avg:145.85ms
step:856/1370 train_time:123390ms step_avg:145.85ms
step:857/1370 train_time:123545ms step_avg:145.86ms
step:858/1370 train_time:123702ms step_avg:145.87ms
step:859/1370 train_time:123855ms step_avg:145.88ms
step:860/1370 train_time:124006ms step_avg:145.89ms
step:861/1370 train_time:124160ms step_avg:145.90ms
step:862/1370 train_time:124312ms step_avg:145.91ms
step:863/1370 train_time:124465ms step_avg:145.91ms
step:864/1370 train_time:124618ms step_avg:145.92ms
step:865/1370 train_time:124768ms step_avg:145.93ms
step:866/1370 train_time:124924ms step_avg:145.94ms
step:867/1370 train_time:125077ms step_avg:145.95ms
step:868/1370 train_time:125227ms step_avg:145.95ms
step:869/1370 train_time:125382ms step_avg:145.96ms
step:870/1370 train_time:125536ms step_avg:145.97ms
step:871/1370 train_time:125688ms step_avg:145.98ms
step:872/1370 train_time:125840ms step_avg:145.99ms
step:873/1370 train_time:125992ms step_avg:145.99ms
step:874/1370 train_time:126145ms step_avg:146.00ms
step:875/1370 train_time:126298ms step_avg:146.01ms
step:875/1370 val_loss:3.4675 train_time:126370ms step_avg:146.09ms
step:876/1370 train_time:126453ms step_avg:146.02ms
step:877/1370 train_time:126607ms step_avg:146.03ms
step:878/1370 train_time:126761ms step_avg:146.04ms
step:879/1370 train_time:126913ms step_avg:146.04ms
step:880/1370 train_time:127065ms step_avg:146.05ms
step:881/1370 train_time:127217ms step_avg:146.06ms
step:882/1370 train_time:127373ms step_avg:146.07ms
step:883/1370 train_time:127527ms step_avg:146.08ms
step:884/1370 train_time:127683ms step_avg:146.09ms
step:885/1370 train_time:127835ms step_avg:146.10ms
step:886/1370 train_time:127988ms step_avg:146.11ms
step:887/1370 train_time:128141ms step_avg:146.11ms
step:888/1370 train_time:128295ms step_avg:146.12ms
step:889/1370 train_time:128451ms step_avg:146.13ms
step:890/1370 train_time:128602ms step_avg:146.14ms
step:891/1370 train_time:128755ms step_avg:146.15ms
step:892/1370 train_time:128906ms step_avg:146.15ms
step:893/1370 train_time:129061ms step_avg:146.16ms
step:894/1370 train_time:129214ms step_avg:146.17ms
step:895/1370 train_time:129369ms step_avg:146.18ms
step:896/1370 train_time:129523ms step_avg:146.19ms
step:897/1370 train_time:129676ms step_avg:146.20ms
step:898/1370 train_time:129830ms step_avg:146.21ms
step:899/1370 train_time:129983ms step_avg:146.21ms
step:900/1370 train_time:130135ms step_avg:146.22ms
step:901/1370 train_time:130287ms step_avg:146.23ms
step:902/1370 train_time:130440ms step_avg:146.23ms
step:903/1370 train_time:130593ms step_avg:146.24ms
step:904/1370 train_time:130745ms step_avg:146.25ms
step:905/1370 train_time:130898ms step_avg:146.25ms
step:906/1370 train_time:131052ms step_avg:146.26ms
step:907/1370 train_time:131207ms step_avg:146.27ms
step:908/1370 train_time:131361ms step_avg:146.28ms
step:909/1370 train_time:131514ms step_avg:146.29ms
step:910/1370 train_time:131669ms step_avg:146.30ms
step:911/1370 train_time:131823ms step_avg:146.31ms
step:912/1370 train_time:131974ms step_avg:146.31ms
step:913/1370 train_time:132127ms step_avg:146.32ms
step:914/1370 train_time:132281ms step_avg:146.33ms
step:915/1370 train_time:132435ms step_avg:146.34ms
step:916/1370 train_time:132589ms step_avg:146.35ms
step:917/1370 train_time:132743ms step_avg:146.35ms
step:918/1370 train_time:132897ms step_avg:146.36ms
step:919/1370 train_time:133055ms step_avg:146.37ms
step:920/1370 train_time:133209ms step_avg:146.38ms
step:921/1370 train_time:133365ms step_avg:146.39ms
step:922/1370 train_time:133524ms step_avg:146.41ms
step:923/1370 train_time:133678ms step_avg:146.42ms
step:924/1370 train_time:133830ms step_avg:146.42ms
step:925/1370 train_time:133988ms step_avg:146.43ms
step:926/1370 train_time:134143ms step_avg:146.44ms
step:927/1370 train_time:134296ms step_avg:146.45ms
step:928/1370 train_time:134450ms step_avg:146.46ms
step:929/1370 train_time:134606ms step_avg:146.47ms
step:930/1370 train_time:134762ms step_avg:146.48ms
step:931/1370 train_time:134914ms step_avg:146.49ms
step:932/1370 train_time:135066ms step_avg:146.49ms
step:933/1370 train_time:135221ms step_avg:146.50ms
step:934/1370 train_time:135374ms step_avg:146.51ms
step:935/1370 train_time:135528ms step_avg:146.52ms
step:936/1370 train_time:135684ms step_avg:146.53ms
step:937/1370 train_time:135840ms step_avg:146.54ms
step:938/1370 train_time:135994ms step_avg:146.55ms
step:939/1370 train_time:136150ms step_avg:146.56ms
step:940/1370 train_time:136304ms step_avg:146.56ms
step:941/1370 train_time:136459ms step_avg:146.57ms
step:942/1370 train_time:136612ms step_avg:146.58ms
step:943/1370 train_time:136767ms step_avg:146.59ms
step:944/1370 train_time:136924ms step_avg:146.60ms
step:945/1370 train_time:137078ms step_avg:146.61ms
step:946/1370 train_time:137233ms step_avg:146.62ms
step:947/1370 train_time:137388ms step_avg:146.63ms
step:948/1370 train_time:137542ms step_avg:146.63ms
step:949/1370 train_time:137695ms step_avg:146.64ms
step:950/1370 train_time:137850ms step_avg:146.65ms
step:951/1370 train_time:138043ms step_avg:146.70ms
step:952/1370 train_time:138196ms step_avg:146.71ms
step:953/1370 train_time:138348ms step_avg:146.71ms
step:954/1370 train_time:138501ms step_avg:146.72ms
step:955/1370 train_time:138653ms step_avg:146.72ms
step:956/1370 train_time:138806ms step_avg:146.73ms
step:957/1370 train_time:138963ms step_avg:146.74ms
step:958/1370 train_time:139121ms step_avg:146.75ms
step:959/1370 train_time:139276ms step_avg:146.76ms
step:960/1370 train_time:139432ms step_avg:146.77ms
step:961/1370 train_time:139585ms step_avg:146.78ms
step:962/1370 train_time:139739ms step_avg:146.78ms
step:963/1370 train_time:139896ms step_avg:146.80ms
step:964/1370 train_time:140051ms step_avg:146.80ms
step:965/1370 train_time:140205ms step_avg:146.81ms
step:966/1370 train_time:140360ms step_avg:146.82ms
step:967/1370 train_time:140513ms step_avg:146.83ms
step:968/1370 train_time:140664ms step_avg:146.83ms
step:969/1370 train_time:140817ms step_avg:146.84ms
step:970/1370 train_time:140970ms step_avg:146.84ms
step:971/1370 train_time:141128ms step_avg:146.86ms
step:972/1370 train_time:141282ms step_avg:146.86ms
step:973/1370 train_time:141435ms step_avg:146.87ms
step:974/1370 train_time:141588ms step_avg:146.88ms
step:975/1370 train_time:141744ms step_avg:146.88ms
step:976/1370 train_time:141895ms step_avg:146.89ms
step:977/1370 train_time:142047ms step_avg:146.90ms
step:978/1370 train_time:142201ms step_avg:146.90ms
step:979/1370 train_time:142355ms step_avg:146.91ms
step:980/1370 train_time:142507ms step_avg:146.91ms
step:981/1370 train_time:142660ms step_avg:146.92ms
step:982/1370 train_time:142813ms step_avg:146.93ms
step:983/1370 train_time:142965ms step_avg:146.93ms
step:984/1370 train_time:143118ms step_avg:146.94ms
step:985/1370 train_time:143270ms step_avg:146.94ms
step:986/1370 train_time:143428ms step_avg:146.95ms
step:987/1370 train_time:143579ms step_avg:146.96ms
step:988/1370 train_time:143733ms step_avg:146.97ms
step:989/1370 train_time:143884ms step_avg:146.97ms
step:990/1370 train_time:144040ms step_avg:146.98ms
step:991/1370 train_time:144192ms step_avg:146.99ms
step:992/1370 train_time:144350ms step_avg:147.00ms
step:993/1370 train_time:144515ms step_avg:147.01ms
step:994/1370 train_time:144668ms step_avg:147.02ms
step:995/1370 train_time:144823ms step_avg:147.03ms
step:996/1370 train_time:144976ms step_avg:147.03ms
step:997/1370 train_time:145127ms step_avg:147.04ms
step:998/1370 train_time:145281ms step_avg:147.05ms
step:999/1370 train_time:145436ms step_avg:147.05ms
step:1000/1370 train_time:145589ms step_avg:147.06ms
step:1000/1370 val_loss:3.4028 train_time:145663ms step_avg:147.13ms
step:1001/1370 train_time:145746ms step_avg:147.07ms
step:1002/1370 train_time:145903ms step_avg:147.08ms
step:1003/1370 train_time:146059ms step_avg:147.09ms
step:1004/1370 train_time:146214ms step_avg:147.10ms
step:1005/1370 train_time:146366ms step_avg:147.10ms
step:1006/1370 train_time:146519ms step_avg:147.11ms
step:1007/1370 train_time:146674ms step_avg:147.12ms
step:1008/1370 train_time:146830ms step_avg:147.12ms
step:1009/1370 train_time:146991ms step_avg:147.14ms
step:1010/1370 train_time:147144ms step_avg:147.14ms
step:1011/1370 train_time:147299ms step_avg:147.15ms
step:1012/1370 train_time:147450ms step_avg:147.16ms
step:1013/1370 train_time:147603ms step_avg:147.16ms
step:1014/1370 train_time:147758ms step_avg:147.17ms
step:1015/1370 train_time:147911ms step_avg:147.17ms
step:1016/1370 train_time:148066ms step_avg:147.18ms
step:1017/1370 train_time:148221ms step_avg:147.19ms
step:1018/1370 train_time:148375ms step_avg:147.20ms
step:1019/1370 train_time:148530ms step_avg:147.21ms
step:1020/1370 train_time:148687ms step_avg:147.21ms
step:1021/1370 train_time:148841ms step_avg:147.22ms
step:1022/1370 train_time:148995ms step_avg:147.23ms
step:1023/1370 train_time:149149ms step_avg:147.24ms
step:1024/1370 train_time:149303ms step_avg:147.24ms
step:1025/1370 train_time:149460ms step_avg:147.25ms
step:1026/1370 train_time:149612ms step_avg:147.26ms
step:1027/1370 train_time:149769ms step_avg:147.27ms
step:1028/1370 train_time:149925ms step_avg:147.27ms
step:1029/1370 train_time:150082ms step_avg:147.28ms
step:1030/1370 train_time:150236ms step_avg:147.29ms
step:1031/1370 train_time:150388ms step_avg:147.29ms
step:1032/1370 train_time:150541ms step_avg:147.30ms
step:1033/1370 train_time:150696ms step_avg:147.31ms
step:1034/1370 train_time:150850ms step_avg:147.31ms
step:1035/1370 train_time:151007ms step_avg:147.32ms
step:1036/1370 train_time:151163ms step_avg:147.33ms
step:1037/1370 train_time:151321ms step_avg:147.34ms
step:1038/1370 train_time:151478ms step_avg:147.35ms
step:1039/1370 train_time:151631ms step_avg:147.36ms
step:1040/1370 train_time:151783ms step_avg:147.36ms
step:1041/1370 train_time:151938ms step_avg:147.37ms
step:1042/1370 train_time:152091ms step_avg:147.38ms
step:1043/1370 train_time:152244ms step_avg:147.38ms
step:1044/1370 train_time:152400ms step_avg:147.39ms
step:1045/1370 train_time:152557ms step_avg:147.40ms
step:1046/1370 train_time:152709ms step_avg:147.40ms
step:1047/1370 train_time:152865ms step_avg:147.41ms
step:1048/1370 train_time:153021ms step_avg:147.42ms
step:1049/1370 train_time:153176ms step_avg:147.43ms
step:1050/1370 train_time:153333ms step_avg:147.44ms
step:1051/1370 train_time:153491ms step_avg:147.45ms
step:1052/1370 train_time:153645ms step_avg:147.45ms
step:1053/1370 train_time:153800ms step_avg:147.46ms
step:1054/1370 train_time:153955ms step_avg:147.47ms
step:1055/1370 train_time:154108ms step_avg:147.47ms
step:1056/1370 train_time:154264ms step_avg:147.48ms
step:1057/1370 train_time:154421ms step_avg:147.49ms
step:1058/1370 train_time:154577ms step_avg:147.50ms
step:1059/1370 train_time:154733ms step_avg:147.51ms
step:1060/1370 train_time:154888ms step_avg:147.51ms
step:1061/1370 train_time:155040ms step_avg:147.52ms
step:1062/1370 train_time:155198ms step_avg:147.53ms
step:1063/1370 train_time:155353ms step_avg:147.53ms
step:1064/1370 train_time:155507ms step_avg:147.54ms
step:1065/1370 train_time:155663ms step_avg:147.55ms
step:1066/1370 train_time:155823ms step_avg:147.56ms
step:1067/1370 train_time:155979ms step_avg:147.57ms
step:1068/1370 train_time:156132ms step_avg:147.57ms
step:1069/1370 train_time:156292ms step_avg:147.58ms
step:1070/1370 train_time:156445ms step_avg:147.59ms
step:1071/1370 train_time:156601ms step_avg:147.60ms
step:1072/1370 train_time:156755ms step_avg:147.60ms
step:1073/1370 train_time:156907ms step_avg:147.61ms
step:1074/1370 train_time:157061ms step_avg:147.61ms
step:1075/1370 train_time:157215ms step_avg:147.62ms
step:1076/1370 train_time:157368ms step_avg:147.62ms
step:1077/1370 train_time:157523ms step_avg:147.63ms
step:1078/1370 train_time:157683ms step_avg:147.64ms
step:1079/1370 train_time:157841ms step_avg:147.65ms
step:1080/1370 train_time:157999ms step_avg:147.66ms
step:1081/1370 train_time:158152ms step_avg:147.67ms
step:1082/1370 train_time:158305ms step_avg:147.67ms
step:1083/1370 train_time:158460ms step_avg:147.68ms
step:1084/1370 train_time:158620ms step_avg:147.69ms
step:1085/1370 train_time:158774ms step_avg:147.70ms
step:1086/1370 train_time:158930ms step_avg:147.70ms
step:1087/1370 train_time:159087ms step_avg:147.71ms
step:1088/1370 train_time:159241ms step_avg:147.72ms
step:1089/1370 train_time:159402ms step_avg:147.73ms
step:1090/1370 train_time:159562ms step_avg:147.74ms
step:1091/1370 train_time:159717ms step_avg:147.75ms
step:1092/1370 train_time:159870ms step_avg:147.75ms
step:1093/1370 train_time:160026ms step_avg:147.76ms
step:1094/1370 train_time:160180ms step_avg:147.77ms
step:1095/1370 train_time:160335ms step_avg:147.77ms
step:1096/1370 train_time:160496ms step_avg:147.79ms
step:1097/1370 train_time:160651ms step_avg:147.79ms
step:1098/1370 train_time:160804ms step_avg:147.80ms
step:1099/1370 train_time:160958ms step_avg:147.80ms
step:1100/1370 train_time:161110ms step_avg:147.81ms
step:1101/1370 train_time:161266ms step_avg:147.81ms
step:1102/1370 train_time:161423ms step_avg:147.82ms
step:1103/1370 train_time:161580ms step_avg:147.83ms
step:1104/1370 train_time:161733ms step_avg:147.84ms
step:1105/1370 train_time:161888ms step_avg:147.84ms
step:1106/1370 train_time:162043ms step_avg:147.85ms
step:1107/1370 train_time:162200ms step_avg:147.86ms
step:1108/1370 train_time:162357ms step_avg:147.87ms
step:1109/1370 train_time:162511ms step_avg:147.87ms
step:1110/1370 train_time:162668ms step_avg:147.88ms
step:1111/1370 train_time:162825ms step_avg:147.89ms
step:1112/1370 train_time:162980ms step_avg:147.89ms
step:1113/1370 train_time:163133ms step_avg:147.90ms
step:1114/1370 train_time:163290ms step_avg:147.91ms
step:1115/1370 train_time:163444ms step_avg:147.91ms
step:1116/1370 train_time:163599ms step_avg:147.92ms
step:1117/1370 train_time:163757ms step_avg:147.93ms
step:1118/1370 train_time:163916ms step_avg:147.94ms
step:1119/1370 train_time:164071ms step_avg:147.95ms
step:1120/1370 train_time:164227ms step_avg:147.95ms
step:1121/1370 train_time:164383ms step_avg:147.96ms
step:1122/1370 train_time:164538ms step_avg:147.97ms
step:1123/1370 train_time:164695ms step_avg:147.97ms
step:1124/1370 train_time:164855ms step_avg:147.98ms
step:1125/1370 train_time:165010ms step_avg:147.99ms
step:1125/1370 val_loss:3.3489 train_time:165082ms step_avg:148.06ms
step:1126/1370 train_time:165165ms step_avg:148.00ms
step:1127/1370 train_time:165321ms step_avg:148.00ms
step:1128/1370 train_time:165477ms step_avg:148.01ms
step:1129/1370 train_time:165636ms step_avg:148.02ms
step:1130/1370 train_time:165791ms step_avg:148.03ms
step:1131/1370 train_time:165949ms step_avg:148.04ms
step:1132/1370 train_time:166105ms step_avg:148.04ms
step:1133/1370 train_time:166261ms step_avg:148.05ms
step:1134/1370 train_time:166420ms step_avg:148.06ms
step:1135/1370 train_time:166574ms step_avg:148.07ms
step:1136/1370 train_time:166731ms step_avg:148.07ms
step:1137/1370 train_time:166885ms step_avg:148.08ms
step:1138/1370 train_time:167042ms step_avg:148.09ms
step:1139/1370 train_time:167199ms step_avg:148.09ms
step:1140/1370 train_time:167354ms step_avg:148.10ms
step:1141/1370 train_time:167545ms step_avg:148.14ms
step:1142/1370 train_time:167707ms step_avg:148.15ms
step:1143/1370 train_time:167865ms step_avg:148.16ms
step:1144/1370 train_time:168022ms step_avg:148.17ms
step:1145/1370 train_time:168176ms step_avg:148.17ms
step:1146/1370 train_time:168332ms step_avg:148.18ms
step:1147/1370 train_time:168491ms step_avg:148.19ms
step:1148/1370 train_time:168646ms step_avg:148.19ms
step:1149/1370 train_time:168801ms step_avg:148.20ms
step:1150/1370 train_time:168955ms step_avg:148.21ms
step:1151/1370 train_time:169110ms step_avg:148.21ms
step:1152/1370 train_time:169266ms step_avg:148.22ms
step:1153/1370 train_time:169425ms step_avg:148.23ms
step:1154/1370 train_time:169579ms step_avg:148.23ms
step:1155/1370 train_time:169736ms step_avg:148.24ms
step:1156/1370 train_time:169896ms step_avg:148.25ms
step:1157/1370 train_time:170053ms step_avg:148.26ms
step:1158/1370 train_time:170209ms step_avg:148.27ms
step:1159/1370 train_time:170365ms step_avg:148.27ms
step:1160/1370 train_time:170521ms step_avg:148.28ms
step:1161/1370 train_time:170677ms step_avg:148.29ms
step:1162/1370 train_time:170833ms step_avg:148.29ms
step:1163/1370 train_time:170987ms step_avg:148.30ms
step:1164/1370 train_time:171143ms step_avg:148.30ms
step:1165/1370 train_time:171298ms step_avg:148.31ms
step:1166/1370 train_time:171455ms step_avg:148.32ms
step:1167/1370 train_time:171611ms step_avg:148.32ms
step:1168/1370 train_time:171767ms step_avg:148.33ms
step:1169/1370 train_time:171924ms step_avg:148.34ms
step:1170/1370 train_time:172082ms step_avg:148.35ms
step:1171/1370 train_time:172237ms step_avg:148.35ms
step:1172/1370 train_time:172393ms step_avg:148.36ms
step:1173/1370 train_time:172549ms step_avg:148.37ms
step:1174/1370 train_time:172716ms step_avg:148.38ms
step:1175/1370 train_time:172877ms step_avg:148.39ms
step:1176/1370 train_time:173038ms step_avg:148.40ms
step:1177/1370 train_time:173201ms step_avg:148.42ms
step:1178/1370 train_time:173355ms step_avg:148.42ms
step:1179/1370 train_time:173510ms step_avg:148.43ms
step:1180/1370 train_time:173674ms step_avg:148.44ms
step:1181/1370 train_time:173829ms step_avg:148.45ms
step:1182/1370 train_time:173984ms step_avg:148.45ms
step:1183/1370 train_time:174141ms step_avg:148.46ms
step:1184/1370 train_time:174298ms step_avg:148.46ms
step:1185/1370 train_time:174455ms step_avg:148.47ms
step:1186/1370 train_time:174611ms step_avg:148.48ms
step:1187/1370 train_time:174774ms step_avg:148.49ms
step:1188/1370 train_time:174928ms step_avg:148.50ms
step:1189/1370 train_time:175088ms step_avg:148.51ms
step:1190/1370 train_time:175244ms step_avg:148.51ms
step:1191/1370 train_time:175402ms step_avg:148.52ms
step:1192/1370 train_time:175554ms step_avg:148.52ms
step:1193/1370 train_time:175709ms step_avg:148.53ms
step:1194/1370 train_time:175866ms step_avg:148.54ms
step:1195/1370 train_time:176022ms step_avg:148.54ms
step:1196/1370 train_time:176180ms step_avg:148.55ms
step:1197/1370 train_time:176337ms step_avg:148.56ms
step:1198/1370 train_time:176497ms step_avg:148.57ms
step:1199/1370 train_time:176651ms step_avg:148.57ms
step:1200/1370 train_time:176806ms step_avg:148.58ms
step:1201/1370 train_time:176962ms step_avg:148.58ms
step:1202/1370 train_time:177131ms step_avg:148.60ms
step:1203/1370 train_time:177290ms step_avg:148.61ms
step:1204/1370 train_time:177447ms step_avg:148.62ms
step:1205/1370 train_time:177602ms step_avg:148.62ms
step:1206/1370 train_time:177758ms step_avg:148.63ms
step:1207/1370 train_time:177914ms step_avg:148.63ms
step:1208/1370 train_time:178071ms step_avg:148.64ms
step:1209/1370 train_time:178226ms step_avg:148.65ms
step:1210/1370 train_time:178388ms step_avg:148.66ms
step:1211/1370 train_time:178544ms step_avg:148.66ms
step:1212/1370 train_time:178701ms step_avg:148.67ms
step:1213/1370 train_time:178856ms step_avg:148.67ms
step:1214/1370 train_time:179014ms step_avg:148.68ms
step:1215/1370 train_time:179170ms step_avg:148.69ms
step:1216/1370 train_time:179325ms step_avg:148.69ms
step:1217/1370 train_time:179482ms step_avg:148.70ms
step:1218/1370 train_time:179635ms step_avg:148.70ms
step:1219/1370 train_time:179789ms step_avg:148.71ms
step:1220/1370 train_time:179944ms step_avg:148.71ms
step:1221/1370 train_time:180100ms step_avg:148.72ms
step:1222/1370 train_time:180254ms step_avg:148.72ms
step:1223/1370 train_time:180414ms step_avg:148.73ms
step:1224/1370 train_time:180576ms step_avg:148.74ms
step:1225/1370 train_time:180731ms step_avg:148.75ms
step:1226/1370 train_time:180888ms step_avg:148.76ms
step:1227/1370 train_time:181047ms step_avg:148.76ms
step:1228/1370 train_time:181202ms step_avg:148.77ms
step:1229/1370 train_time:181358ms step_avg:148.78ms
step:1230/1370 train_time:181520ms step_avg:148.79ms
step:1231/1370 train_time:181679ms step_avg:148.80ms
step:1232/1370 train_time:181839ms step_avg:148.80ms
step:1233/1370 train_time:181996ms step_avg:148.81ms
step:1234/1370 train_time:182153ms step_avg:148.82ms
step:1235/1370 train_time:182308ms step_avg:148.82ms
step:1236/1370 train_time:182463ms step_avg:148.83ms
step:1237/1370 train_time:182619ms step_avg:148.83ms
step:1238/1370 train_time:182783ms step_avg:148.85ms
step:1239/1370 train_time:182941ms step_avg:148.85ms
step:1240/1370 train_time:183099ms step_avg:148.86ms
step:1241/1370 train_time:183259ms step_avg:148.87ms
step:1242/1370 train_time:183416ms step_avg:148.88ms
step:1243/1370 train_time:183578ms step_avg:148.89ms
step:1244/1370 train_time:183733ms step_avg:148.89ms
step:1245/1370 train_time:183893ms step_avg:148.90ms
step:1246/1370 train_time:184050ms step_avg:148.91ms
step:1247/1370 train_time:184208ms step_avg:148.92ms
step:1248/1370 train_time:184361ms step_avg:148.92ms
step:1249/1370 train_time:184517ms step_avg:148.92ms
step:1250/1370 train_time:184673ms step_avg:148.93ms
step:1250/1370 val_loss:3.3037 train_time:184749ms step_avg:148.99ms
step:1251/1370 train_time:184835ms step_avg:148.94ms
step:1252/1370 train_time:184989ms step_avg:148.94ms
step:1253/1370 train_time:185145ms step_avg:148.95ms
step:1254/1370 train_time:185299ms step_avg:148.95ms
step:1255/1370 train_time:185466ms step_avg:148.97ms
step:1256/1370 train_time:185620ms step_avg:148.97ms
step:1257/1370 train_time:185779ms step_avg:148.98ms
step:1258/1370 train_time:185938ms step_avg:148.99ms
step:1259/1370 train_time:186094ms step_avg:148.99ms
step:1260/1370 train_time:186248ms step_avg:149.00ms
step:1261/1370 train_time:186404ms step_avg:149.00ms
step:1262/1370 train_time:186562ms step_avg:149.01ms
step:1263/1370 train_time:186719ms step_avg:149.02ms
step:1264/1370 train_time:186874ms step_avg:149.02ms
step:1265/1370 train_time:187031ms step_avg:149.03ms
step:1266/1370 train_time:187187ms step_avg:149.03ms
step:1267/1370 train_time:187345ms step_avg:149.04ms
step:1268/1370 train_time:187501ms step_avg:149.05ms
step:1269/1370 train_time:187661ms step_avg:149.06ms
step:1270/1370 train_time:187819ms step_avg:149.06ms
step:1271/1370 train_time:187977ms step_avg:149.07ms
step:1272/1370 train_time:188134ms step_avg:149.08ms
step:1273/1370 train_time:188289ms step_avg:149.08ms
step:1274/1370 train_time:188445ms step_avg:149.09ms
step:1275/1370 train_time:188601ms step_avg:149.09ms
step:1276/1370 train_time:188755ms step_avg:149.10ms
step:1277/1370 train_time:188913ms step_avg:149.10ms
step:1278/1370 train_time:189067ms step_avg:149.11ms
step:1279/1370 train_time:189224ms step_avg:149.11ms
step:1280/1370 train_time:189387ms step_avg:149.12ms
step:1281/1370 train_time:189546ms step_avg:149.13ms
step:1282/1370 train_time:189700ms step_avg:149.13ms
step:1283/1370 train_time:189858ms step_avg:149.14ms
step:1284/1370 train_time:190019ms step_avg:149.15ms
step:1285/1370 train_time:190173ms step_avg:149.16ms
step:1286/1370 train_time:190329ms step_avg:149.16ms
step:1287/1370 train_time:190485ms step_avg:149.17ms
step:1288/1370 train_time:190641ms step_avg:149.17ms
step:1289/1370 train_time:190803ms step_avg:149.18ms
step:1290/1370 train_time:190964ms step_avg:149.19ms
step:1291/1370 train_time:191126ms step_avg:149.20ms
step:1292/1370 train_time:191282ms step_avg:149.21ms
step:1293/1370 train_time:191440ms step_avg:149.21ms
step:1294/1370 train_time:191595ms step_avg:149.22ms
step:1295/1370 train_time:191752ms step_avg:149.22ms
step:1296/1370 train_time:191910ms step_avg:149.23ms
step:1297/1370 train_time:192067ms step_avg:149.24ms
step:1298/1370 train_time:192224ms step_avg:149.24ms
step:1299/1370 train_time:192381ms step_avg:149.25ms
step:1300/1370 train_time:192537ms step_avg:149.25ms
step:1301/1370 train_time:192692ms step_avg:149.26ms
step:1302/1370 train_time:192851ms step_avg:149.27ms
step:1303/1370 train_time:193011ms step_avg:149.27ms
step:1304/1370 train_time:193170ms step_avg:149.28ms
step:1305/1370 train_time:193325ms step_avg:149.29ms
step:1306/1370 train_time:193483ms step_avg:149.29ms
step:1307/1370 train_time:193639ms step_avg:149.30ms
step:1308/1370 train_time:193796ms step_avg:149.30ms
step:1309/1370 train_time:193952ms step_avg:149.31ms
step:1310/1370 train_time:194110ms step_avg:149.32ms
step:1311/1370 train_time:194263ms step_avg:149.32ms
step:1312/1370 train_time:194416ms step_avg:149.32ms
step:1313/1370 train_time:194572ms step_avg:149.33ms
step:1314/1370 train_time:194731ms step_avg:149.33ms
step:1315/1370 train_time:194890ms step_avg:149.34ms
step:1316/1370 train_time:195045ms step_avg:149.35ms
step:1317/1370 train_time:195201ms step_avg:149.35ms
step:1318/1370 train_time:195362ms step_avg:149.36ms
step:1319/1370 train_time:195520ms step_avg:149.37ms
step:1320/1370 train_time:195676ms step_avg:149.37ms
step:1321/1370 train_time:195834ms step_avg:149.38ms
step:1322/1370 train_time:195995ms step_avg:149.39ms
step:1323/1370 train_time:196152ms step_avg:149.39ms
step:1324/1370 train_time:196308ms step_avg:149.40ms
step:1325/1370 train_time:196465ms step_avg:149.40ms
step:1326/1370 train_time:196625ms step_avg:149.41ms
step:1327/1370 train_time:196782ms step_avg:149.42ms
step:1328/1370 train_time:196940ms step_avg:149.42ms
step:1329/1370 train_time:197115ms step_avg:149.44ms
step:1330/1370 train_time:197276ms step_avg:149.45ms
step:1331/1370 train_time:197470ms step_avg:149.49ms
step:1332/1370 train_time:197645ms step_avg:149.50ms
step:1333/1370 train_time:197803ms step_avg:149.51ms
step:1334/1370 train_time:197958ms step_avg:149.51ms
step:1335/1370 train_time:198113ms step_avg:149.52ms
step:1336/1370 train_time:198277ms step_avg:149.53ms
step:1337/1370 train_time:198436ms step_avg:149.54ms
step:1338/1370 train_time:198593ms step_avg:149.54ms
step:1339/1370 train_time:198752ms step_avg:149.55ms
step:1340/1370 train_time:198912ms step_avg:149.56ms
step:1341/1370 train_time:199068ms step_avg:149.56ms
step:1342/1370 train_time:199227ms step_avg:149.57ms
step:1343/1370 train_time:199383ms step_avg:149.57ms
step:1344/1370 train_time:199539ms step_avg:149.58ms
step:1345/1370 train_time:199696ms step_avg:149.59ms
step:1346/1370 train_time:199857ms step_avg:149.59ms
step:1347/1370 train_time:200015ms step_avg:149.60ms
step:1348/1370 train_time:200173ms step_avg:149.61ms
step:1349/1370 train_time:200331ms step_avg:149.61ms
step:1350/1370 train_time:200486ms step_avg:149.62ms
step:1351/1370 train_time:200644ms step_avg:149.62ms
step:1352/1370 train_time:200807ms step_avg:149.63ms
step:1353/1370 train_time:200971ms step_avg:149.64ms
step:1354/1370 train_time:201130ms step_avg:149.65ms
step:1355/1370 train_time:201287ms step_avg:149.66ms
step:1356/1370 train_time:201443ms step_avg:149.66ms
step:1357/1370 train_time:201600ms step_avg:149.67ms
step:1358/1370 train_time:201760ms step_avg:149.67ms
step:1359/1370 train_time:201919ms step_avg:149.68ms
step:1360/1370 train_time:202077ms step_avg:149.69ms
step:1361/1370 train_time:202237ms step_avg:149.69ms
step:1362/1370 train_time:202396ms step_avg:149.70ms
step:1363/1370 train_time:202558ms step_avg:149.71ms
step:1364/1370 train_time:202714ms step_avg:149.71ms
step:1365/1370 train_time:202870ms step_avg:149.72ms
step:1366/1370 train_time:203029ms step_avg:149.73ms
step:1367/1370 train_time:203186ms step_avg:149.73ms
step:1368/1370 train_time:203345ms step_avg:149.74ms
step:1369/1370 train_time:203508ms step_avg:149.75ms
step:1370/1370 train_time:203669ms step_avg:149.76ms
step:1370/1370 val_loss:3.2795 train_time:203742ms step_avg:149.81ms
peak memory consumption: 32619 MiB

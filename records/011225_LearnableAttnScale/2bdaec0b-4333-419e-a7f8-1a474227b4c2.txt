import os
import sys
with open(sys.argv[0]) as f:
    code = f.read() # read the code of this file ASAP, for logging
import uuid
import time
import glob
import subprocess
import contextlib
from dataclasses import dataclass

import torch
torch.empty(1, device='cuda', requires_grad=True).backward()
from torch import nn
import torch.nn.functional as F
import torch.distributed as dist
from torch.nn.parallel import DistributedDataParallel as DDP
# use of FlexAttention contributed by @KoszarskyB
from torch.nn.attention.flex_attention import BlockMask, flex_attention

# -----------------------------------------------------------------------------
# Muon optimizer

@torch.compile
def zeropower_via_newtonschulz5(G, steps):
    """
    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a
    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose
    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at
    zero even beyond the point where the iteration no longer converges all the way to one everywhere
    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T
    where S' is diagonal with S_{ii}' ~ Uniform(0.5, 1.5), which turns out not to hurt model
    performance at all relative to UV^T, where USV^T = G is the SVD.
    """
    assert len(G.shape) == 2
    a, b, c = (3.4445, -4.7750,  2.0315)
    X = G.bfloat16()
    if G.size(0) > G.size(1):
        X = X.T

    # Ensure spectral norm is at most 1
    X = X / (X.norm() + 1e-7)
    # Perform the NS iterations
    for _ in range(steps):
        A = X @ X.T
        B = b * A + c * A @ A # adapted from suggestion by @jxbz, @leloykun, and @YouJiacheng
        X = a * X + B @ X
    
    if G.size(0) > G.size(1):
        X = X.T
    return X

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Some warnings:
    - This optimizer assumes that all parameters passed in are 2D.
    - It should not be used for the embedding layer, the final fully connected layer, or any {0,1}-D
    parameters; those should all be optimized by a standard method (e.g., AdamW).
    - To use it with 4D convolutional filters, it works well to just flatten their last 3 dimensions.
    - We believe it is unlikely to work well for training with small batch size.
    - We believe it may not work well for finetuning pretrained models, but we haven't tested this.
    - We have not yet tried this optimizer for training scenarios larger than NanoGPT (124M).

    Arguments:
        lr: The learning rate used by the internal SGD.
        momentum: The momentum used by the internal SGD.
        nesterov: Whether to use Nesterov-style momentum in the internal SGD. (recommended)
        ns_steps: The number of Newton-Schulz iteration steps to use.
    """
    def __init__(self, params, lr=0.02, momentum=0.95, nesterov=True, ns_steps=5):
        self.world_size = int(os.environ['WORLD_SIZE'])
        self.rank = int(os.environ['RANK'])
        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, ns_steps=ns_steps)
        assert all(isinstance(p, torch.Tensor) for p in params)
        sizes = {p.numel() for p in params}
        param_groups = [dict(params=[p for p in params if p.numel() == size],
                             update_buffer=[torch.empty(size, device='cuda', dtype=torch.bfloat16) for _ in range(self.world_size)])
                        for size in sizes]
        super().__init__(param_groups, defaults)

    def step(self):

        for group in self.param_groups:

            lr = group['lr']
            momentum = group['momentum']
            nesterov = group['nesterov']
            ns_steps = group['ns_steps']
            update_buffers = group['update_buffer']
            # generate weight updates in distributed fashion
            params = group['params']
            handle = None
            params_world = None
            def update_prev():
                if params_world is None:
                    return
                assert handle is not None
                handle.wait()
                for p_world, g_world in zip(params_world, update_buffers):
                    p_world.data.add_(
                        g_world.view_as(p_world),
                        alpha=-lr * max(1, p_world.size(0) / p_world.size(1)) ** 0.5,
                    )
            for base_i in range(len(params))[::self.world_size]:
                if base_i + rank < len(params):
                    p = params[base_i + self.rank]
                    g = p.grad
                    assert g is not None
                    state = self.state[p]
                    if 'momentum_buffer' not in state:
                        state['momentum_buffer'] = torch.zeros_like(g)
                    buf = state['momentum_buffer']
                    buf.lerp_(g, 1 - momentum)
                    g = g.lerp_(buf, momentum) if nesterov else buf
                    g = zeropower_via_newtonschulz5(g, steps=ns_steps).flatten()
                else:
                    g = update_buffers[rank]
                update_prev() # async all_gather instead of sync all_reduce by @YouJiacheng
                handle = dist.all_gather(update_buffers, g, async_op=True)
                params_world = params[base_i : base_i + self.world_size]
            update_prev()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the GPT-2 model

def norm(x):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):

    def __init__(self, in_features, out_features):
        super().__init__(in_features, out_features, bias=False)

    def forward(self, x):
        return F.linear(x, self.weight.type_as(x))

class Rotary(nn.Module):

    def __init__(self, dim, max_seq_len=65536):
        super().__init__()
        # half-truncate RoPE by @YouJiacheng
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=dim//4, dtype=torch.float32)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(dim//4)])
        t = torch.arange(max_seq_len, dtype=torch.float32)
        theta = torch.einsum('i,j -> ij', t, angular_freq)
        self.cos = nn.Buffer(theta.cos(), persistent=False)
        self.sin = nn.Buffer(theta.sin(), persistent=False)

    def forward(self, x):
        cos, sin = self.cos[None, :x.size(-3), None, :], self.sin[None, :x.size(-3), None, :]
        x1, x2 = x.float().chunk(2, dim=-1)
        y1 = x1 * cos + x2 * sin
        y2 = x1 * (-sin) + x2 * cos
        return torch.cat((y1, y2), 3).type_as(x)

class CausalSelfAttention(nn.Module):

    def __init__(self, dim, num_heads):
        super().__init__()
        assert dim % num_heads == 0
        self.num_heads = num_heads
        self.c_q = CastedLinear(dim, dim)
        self.c_k = CastedLinear(dim, dim)
        self.c_v = CastedLinear(dim, dim)
        self.lambdas = nn.Parameter(torch.tensor([0.5, 0.5]))
        self.rotary = Rotary(dim // num_heads) # dim // num_heads = head_dim
        self.c_proj = CastedLinear(dim, dim)
        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977
        self.attn_scale = nn.Parameter(torch.tensor(1.0 / (dim // num_heads) ** 0.5))

    def forward(self, x, ve, block_mask):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, 'Must use batch size = 1 for FlexAttention'
        q = self.c_q(x).view(B, T, self.num_heads, -1)
        k = self.c_k(x).view(B, T, self.num_heads, -1)
        v = self.c_v(x).view(B, T, self.num_heads, -1)
        if ve is not None:
            v = self.lambdas[0] * v + self.lambdas[1] * ve.view_as(v) # @KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = self.lambdas[0] * v
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = self.rotary(q), self.rotary(k)
        y = flex_attention(q.transpose(1, 2) * self.attn_scale, k.transpose(1, 2), v.transpose(1, 2), block_mask=block_mask, scale=1.)
        y = y.transpose(1, 2).contiguous().view_as(x) # re-assemble all head outputs side by side
        y = self.c_proj(y)
        return y

class MLP(nn.Module):

    def __init__(self, dim):
        super().__init__()
        self.c_fc = CastedLinear(dim, 4 * dim)
        self.c_proj = CastedLinear(4 * dim, dim)
        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977

    def forward(self, x):
        x = self.c_fc(x)
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = self.c_proj(x)
        return x

class Block(nn.Module):

    def __init__(self, model_dim, num_heads, use_attn=True):
        super().__init__()
        self.attn = CausalSelfAttention(model_dim, num_heads) if use_attn else None
        self.mlp = MLP(model_dim)
        self.lambdas = nn.Parameter(torch.tensor([1., 0.]))

    def forward(self, x, ve, x0, block_mask):
        x = self.lambdas[0] * x + self.lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), ve, block_mask)
        x = x + self.mlp(norm(x))
        return x

class ValueEmbedding(nn.Module):
    def __init__(self, vocab_size, model_dim):
        super().__init__()
        self.embed = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])

    def forward(self, inputs):
        ve = [emb(inputs).bfloat16() for emb in self.embed]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        ve = [ve[0], ve[1], ve[2], None, None, None, None, None, None, ve[0], ve[1], ve[2]]
        return ve

# -----------------------------------------------------------------------------
# The main GPT-2 model

class GPT(nn.Module):

    def __init__(self, vocab_size, num_layers, num_heads, model_dim):
        super().__init__()
        self.embed = nn.Embedding(vocab_size, model_dim)
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.blocks = nn.ModuleList([Block(model_dim, num_heads, use_attn=(i != 7))
                                     for i in range(num_layers)])
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual learning
        # U-net structure on token value embeddings by @leloykun
        self.value_embeds = ValueEmbedding(vocab_size, model_dim)
        self.lm_head = CastedLinear(model_dim, vocab_size)
        self.lm_head.weight.data.zero_() # @Grad62304977
        # U-net design by @brendanh0gan
        self.num_encoder_layers = num_layers // 2 # Half of the layers for encoder
        self.num_decoder_layers = num_layers - self.num_encoder_layers # Remaining for decoder
        # Add learnable skip connection weights for decoder layers
        self.skip_weights = nn.Parameter(torch.ones(self.num_decoder_layers))

    def forward(self, inputs, targets, sliding_window_num_blocks):
        BLOCK_SIZE = 128
        seq_len = len(inputs)
        assert seq_len % BLOCK_SIZE == 0
        total_num_blocks = seq_len // BLOCK_SIZE
        assert inputs.ndim == 1
        docs = (inputs == 50256).cumsum(0)
        docs_low = docs.view(-1, BLOCK_SIZE)[:, 0].contiguous()
        docs_high = docs.view(-1, BLOCK_SIZE)[:, -1].contiguous()

        def document_causal(b, h, q_idx, kv_idx):
            causal_mask = q_idx >= kv_idx
            document_mask = docs[q_idx] == docs[kv_idx]
            return causal_mask & document_mask

        def dense_to_ordered(dense_mask):
            num_blocks = dense_mask.sum(dim=-1, dtype=torch.int32)
            indices = dense_mask.argsort(dim=-1, descending=True, stable=True).to(torch.int32)
            return num_blocks[None, None].contiguous(), indices[None, None].contiguous()

        def create_doc_swc_block_mask(sliding_window_num_blocks):
            kv_idx = block_idx = torch.arange(total_num_blocks, dtype=torch.int32, device='cuda')
            q_idx = block_idx[:, None]
            causal_bm = q_idx >= kv_idx
            causal_full_bm = q_idx > kv_idx
            window_bm = q_idx - kv_idx < sliding_window_num_blocks
            window_full_bm = window_bm # block-wise sliding window by @YouJiacheng
            # document_bm = (docs_low[q_idx] <= docs_high[kv_idx]) & (docs_low[kv_idx] <= docs_high[q_idx])
            document_bm = (docs_low[:, None] <= docs_high) & (docs_low <= docs_high[:, None])
            document_full_bm = (docs_low[:, None] == docs_high) & (docs_low == docs_high[:, None])
            nonzero_bm = causal_bm & window_bm & document_bm
            full_bm  = causal_full_bm & window_full_bm & document_full_bm
            kv_num_blocks, kv_indices = dense_to_ordered(nonzero_bm & ~full_bm)
            full_kv_num_blocks, full_kv_indices = dense_to_ordered(full_bm)
            return BlockMask.from_kv_blocks(
                kv_num_blocks,
                kv_indices,
                full_kv_num_blocks,
                full_kv_indices,
                BLOCK_SIZE=BLOCK_SIZE,
                mask_mod=document_causal,
            )

        block_mask = create_doc_swc_block_mask(sliding_window_num_blocks)

        x0 = norm(self.embed(inputs[None]).bfloat16()) # use of norm here by @Grad62304977
        x = x0
        ve = self.value_embeds(inputs)
        assert len(ve) == len(self.blocks)
        ve_enc, ve_dec = ve[:self.num_encoder_layers], ve[self.num_encoder_layers:]

        # Store outputs for U-Net skip connections
        skip_connections = []
        # Encoder pass - process only the first half of the blocks
        for i in range(self.num_encoder_layers):
            x = self.blocks[i](x, ve_enc[i], x0, block_mask)
            skip_connections.append(x)
        # Decoder pass - process the remaining blocks with weighted skip connections
        for i in range(self.num_decoder_layers):
            x = x + self.skip_weights[i] * skip_connections.pop()
            # U-net structure on token value embeddings by @leloykun
            x = self.blocks[self.num_encoder_layers + i](x, ve_dec[i], x0, block_mask)

        x = norm(x)
        logits = self.lm_head(x)
        logits = 15 * torch.tanh(logits / 15) # @Grad62304977 added tanh softcapping, @KoszarskyB reduced it from 30 to 15
        logits = logits.float()
        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets)
        return loss

# -----------------------------------------------------------------------------
# Our own simple Distributed Data Loader

def _load_data_shard(path):
    # only reads the header, returns header data
    # header is 256 int32
    header = torch.from_file(path, False, 256, dtype=torch.int32)
    assert header[0] == 20240520, 'magic number mismatch in the data .bin file'
    assert header[1] == 1, 'unsupported version'
    num_tokens = int(header[2]) # number of tokens (claimed)
    with open(path, 'rb', buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, 'number of tokens read does not match header'
    return tokens

class DistributedDataLoader:

    def __init__(self, filename_pattern):
        self.rank = int(os.environ['RANK'])
        self.world_size = int(os.environ['WORLD_SIZE'])
        self.files = sorted(glob.glob(filename_pattern))
        self.reset()

    def reset(self):
        self.current_shard = -1
        self.advance()

    def advance(self):
        self.current_shard = (self.current_shard + 1) % len(self.files)
        self.current_position = 0
        self.tokens = _load_data_shard(self.files[self.current_shard])

    def next_batch(self, batch_size):
        assert batch_size % self.world_size == 0
        device_batch_size = batch_size // self.world_size
        # load next shard if necessary
        if self.current_position + batch_size + 1 >= len(self.tokens):
            self.advance()
        pos = self.current_position + self.rank * device_batch_size
        device_batch_tokens = self.tokens[pos:pos+device_batch_size+1]
        # advance current position
        self.current_position += batch_size
        inputs = device_batch_tokens[:-1].to(device='cuda', dtype=torch.int32, non_blocking=True)
        targets = device_batch_tokens[1:].to(device='cuda', dtype=torch.int64, non_blocking=True)
        return inputs, targets

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files = 'data/fineweb10B/fineweb_train_*.bin' # input .bin to train on
    val_files = 'data/fineweb10B/fineweb_val_*.bin' # input .bin to eval validation loss on
    val_tokens = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # optimization
    batch_size = 8*64*1024 # batch size in tokens
    num_iterations = 1370 # number of iterations to run
    cooldown_frac = 0.4 # fraction of training spent cooling down the learning rate
    bf16_embeds = True
    # evaluation and logging
    val_loss_every = 125 # every how many steps to evaluate val loss? 0 for only at the end
    # implementation
    max_device_batch_size = 64*1024 # batch size per device in tokens
    save_checkpoint = False
args = Hyperparameters()

micro_bs = args.max_device_batch_size

# set up DDP (distributed data parallel). torchrun sets this env variable
rank = int(os.environ['RANK'])
local_rank = int(os.environ['LOCAL_RANK'])
world_size = int(os.environ['WORLD_SIZE'])
assert torch.cuda.is_available()
torch.cuda.set_device(local_rank)
dist.init_process_group(backend='nccl', device_id=torch.device(local_rank))
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = uuid.uuid4()
    os.makedirs('logs', exist_ok=True)
    logfile = f'logs/{run_id}.txt'
    print(logfile)

def print0(s, console=False):
    if master_process:
        with open(logfile, 'a') as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0('='*100)
# log information about the hardware/software environment this is running on
print0(f'Running Python {sys.version}')
print0(f'Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}')
print0(subprocess.run(['nvidia-smi'], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout)
print0('='*100)

# load data
train_loader = DistributedDataLoader(args.train_files)
val_loader = DistributedDataLoader(args.val_files)
print0(f'Training dataloader files: {train_loader.files}')
print0(f'Validation dataloader files: {val_loader.files}')
print0('='*100)

# there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency. suggested to me by @Grad62304977.
# this originates from Karpathy's experiments.
model = GPT(vocab_size=50304, num_layers=12, num_heads=6, model_dim=768)
model = model.cuda()
if args.bf16_embeds:
    for m in model.modules():
        if isinstance(m, nn.Embedding):
            m.bfloat16()
model: GPT = torch.compile(model)
ddp_model = DDP(model, device_ids=[local_rank], broadcast_buffers=False, gradient_as_bucket_view=True)

# collect the parameters to optimize
hidden_matrix_params = [p for p in model.blocks.parameters() if p.ndim == 2]
embed_params = [model.embed.weight, *model.value_embeds.parameters()]
scalar_params = [p for n, p in model.named_parameters() if p.ndim < 2 and "attn_scale" not in n]
attn_scale_params = [p for n, p in model.named_parameters() if p.ndim < 2 and "attn_scale" in n]
head_params = [model.lm_head.weight]

# init the optimizer(s)
optimizer1 = torch.optim.Adam([dict(params=embed_params, lr=0.6),
                               dict(params=head_params, lr=0.008),
                               dict(params=scalar_params, lr=0.04),
                               dict(params=attn_scale_params, lr=0.01)],
                              betas=(0.8, 0.95), fused=True)
optimizer2 = Muon(hidden_matrix_params, lr=0.05, momentum=0.95)
optimizers = [optimizer1, optimizer2]

# learning rate schedule: stable then decay
def get_lr(it):
    t = 1 - it / args.num_iterations # time remaining in training
    assert 1 >= t > 0
    # 1) constant lr for first part of training
    if t >= args.cooldown_frac:
        return 1.0
    # 2) then linear cooldown
    else:
        return t / args.cooldown_frac
schedulers = [torch.optim.lr_scheduler.LambdaLR(opt, get_lr) for opt in optimizers]

# sliding window size schedule: linear increase over training in chunks of 128 from 128 -> 1792. By @fernbear.bsky.social
def get_sliding_window_blocks(it):
    x = it / args.num_iterations # training progress
    assert 0 <= x <= 1
    return int(((1 - x) * 128 + x * 1856) // 128)
sliding_window_num_blocks = torch.tensor(1, dtype=torch.int32, device='cuda')

# Start training loop
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    # This effectively ignores timing first 10 steps, which are slower for weird reasons.
    # Alternately, and slightly more correctly in terms of benchmarking, we could do 10
    # steps with dummy data first, and then re-initialize the model and reset the loader.
    if step == 10:
        training_time_ms = 0
        t0 = time.perf_counter()
    timed_steps = float('nan') if step <= 11 else (step - 10) + 1 # <= 11 to avoid bug in val

    sliding_window_num_blocks.copy_(get_sliding_window_blocks(step))

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        # run validation batches
        model.eval()
        val_loader.reset()
        val_loss = 0.0
        # calculate the number of steps to take in the val loop.
        val_batch_size = world_size * micro_bs
        assert args.val_tokens % val_batch_size == 0
        val_steps = args.val_tokens // val_batch_size
        for _ in range(val_steps):
            with torch.no_grad():
                inputs_val, targets_val = val_loader.next_batch(val_batch_size)
                val_loss += ddp_model(inputs_val, targets_val, sliding_window_num_blocks)
        # Print attention scales
        for n, p in model.named_parameters():
            if p.ndim < 2 and "attn_scale" in n:
                print0(f'{n}: {p.item()}', console=True)
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        val_loss /= val_steps
        # logging
        print0(f'step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/(timed_steps-1):.2f}ms', console=True)
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f'logs/{run_id}', exist_ok=True)
            torch.save(log, f'logs/{run_id}/state_step{step:06d}.pt')
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    model.train()
    batch_size = args.batch_size
    assert batch_size % world_size == 0
    inputs_train, targets_train = train_loader.next_batch(batch_size)
    assert len(inputs_train) <= micro_bs or len(inputs_train) % micro_bs == 0
    for micro_inputs_train, micro_targets_train in zip(inputs_train.split(micro_bs), targets_train.split(micro_bs)):
        ddp_model(micro_inputs_train, micro_targets_train, sliding_window_num_blocks).backward()
    # momentum warmup for Muon
    frac = min(step/300, 1)
    for group in optimizer2.param_groups:
        group['momentum'] = (1 - frac) * 0.85 + frac * 0.95
    # step the optimizers and schedulers
    for opt, sched in zip(optimizers, schedulers):
        opt.step()
        if step != train_steps-1:
            sched.step()
    # null the gradients
    model.zero_grad(set_to_none=True)
    # logging
    approx_time = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f'step:{step+1}/{train_steps} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms', console=True)

print0(f'peak memory consumption: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB')
dist.destroy_process_group()

====================================================================================================
Running Python 3.10.12 (main, Sep 11 2024, 15:47:36) [GCC 11.4.0]
Running PyTorch 2.7.0.dev20250110+cu126 compiled for CUDA 12.6
Wed Jan 15 21:03:51 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.183.06             Driver Version: 535.183.06   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:19:00.0 Off |                    0 |
| N/A   38C    P0             121W / 700W |   7713MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:3B:00.0 Off |                    0 |
| N/A   30C    P0             113W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:4C:00.0 Off |                    0 |
| N/A   29C    P0             115W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   37C    P0             117W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:9B:00.0 Off |                    0 |
| N/A   38C    P0             119W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:BB:00.0 Off |                    0 |
| N/A   30C    P0             114W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:CB:00.0 Off |                    0 |
| N/A   37C    P0             116W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:DB:00.0 Off |                    0 |
| N/A   29C    P0             113W / 700W |   3211MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
Training dataloader files: ['data/fineweb10B/fineweb_train_000001.bin', 'data/fineweb10B/fineweb_train_000002.bin', 'data/fineweb10B/fineweb_train_000003.bin', 'data/fineweb10B/fineweb_train_000004.bin', 'data/fineweb10B/fineweb_train_000005.bin', 'data/fineweb10B/fineweb_train_000006.bin', 'data/fineweb10B/fineweb_train_000007.bin', 'data/fineweb10B/fineweb_train_000008.bin', 'data/fineweb10B/fineweb_train_000009.bin']
Validation dataloader files: ['data/fineweb10B/fineweb_val_000000.bin']
====================================================================================================
_orig_mod.blocks.0.attn.attn_scale: 0.0883883461356163
_orig_mod.blocks.1.attn.attn_scale: 0.0883883461356163
_orig_mod.blocks.2.attn.attn_scale: 0.0883883461356163
_orig_mod.blocks.3.attn.attn_scale: 0.0883883461356163
_orig_mod.blocks.4.attn.attn_scale: 0.0883883461356163
_orig_mod.blocks.5.attn.attn_scale: 0.0883883461356163
_orig_mod.blocks.6.attn.attn_scale: 0.0883883461356163
_orig_mod.blocks.8.attn.attn_scale: 0.0883883461356163
_orig_mod.blocks.9.attn.attn_scale: 0.0883883461356163
_orig_mod.blocks.10.attn.attn_scale: 0.0883883461356163
_orig_mod.blocks.11.attn.attn_scale: 0.0883883461356163
step:0/1370 val_loss:10.8258 train_time:0ms step_avg:nanms
step:1/1370 train_time:30354ms step_avg:nanms
step:2/1370 train_time:30427ms step_avg:nanms
step:3/1370 train_time:30612ms step_avg:nanms
step:4/1370 train_time:30746ms step_avg:nanms
step:5/1370 train_time:30879ms step_avg:nanms
step:6/1370 train_time:31012ms step_avg:nanms
step:7/1370 train_time:31144ms step_avg:nanms
step:8/1370 train_time:31277ms step_avg:nanms
step:9/1370 train_time:31410ms step_avg:nanms
step:10/1370 train_time:31551ms step_avg:nanms
step:11/1370 train_time:136ms step_avg:nanms
step:12/1370 train_time:270ms step_avg:nanms
step:13/1370 train_time:404ms step_avg:134.78ms
step:14/1370 train_time:538ms step_avg:134.54ms
step:15/1370 train_time:672ms step_avg:134.34ms
step:16/1370 train_time:806ms step_avg:134.34ms
step:17/1370 train_time:944ms step_avg:134.83ms
step:18/1370 train_time:1078ms step_avg:134.76ms
step:19/1370 train_time:1213ms step_avg:134.76ms
step:20/1370 train_time:1348ms step_avg:134.85ms
step:21/1370 train_time:1483ms step_avg:134.82ms
step:22/1370 train_time:1617ms step_avg:134.75ms
step:23/1370 train_time:1752ms step_avg:134.75ms
step:24/1370 train_time:1887ms step_avg:134.79ms
step:25/1370 train_time:2022ms step_avg:134.82ms
step:26/1370 train_time:2158ms step_avg:134.86ms
step:27/1370 train_time:2293ms step_avg:134.91ms
step:28/1370 train_time:2429ms step_avg:134.95ms
step:29/1370 train_time:2564ms step_avg:134.96ms
step:30/1370 train_time:2700ms step_avg:134.99ms
step:31/1370 train_time:2836ms step_avg:135.05ms
step:32/1370 train_time:2971ms step_avg:135.05ms
step:33/1370 train_time:3107ms step_avg:135.07ms
step:34/1370 train_time:3243ms step_avg:135.12ms
step:35/1370 train_time:3378ms step_avg:135.12ms
step:36/1370 train_time:3512ms step_avg:135.09ms
step:37/1370 train_time:3648ms step_avg:135.12ms
step:38/1370 train_time:3782ms step_avg:135.08ms
step:39/1370 train_time:3917ms step_avg:135.06ms
step:40/1370 train_time:4051ms step_avg:135.04ms
step:41/1370 train_time:4187ms step_avg:135.07ms
step:42/1370 train_time:4321ms step_avg:135.04ms
step:43/1370 train_time:4456ms step_avg:135.03ms
step:44/1370 train_time:4591ms step_avg:135.02ms
step:45/1370 train_time:4727ms step_avg:135.07ms
step:46/1370 train_time:4862ms step_avg:135.06ms
step:47/1370 train_time:4996ms step_avg:135.04ms
step:48/1370 train_time:5131ms step_avg:135.02ms
step:49/1370 train_time:5268ms step_avg:135.08ms
step:50/1370 train_time:5402ms step_avg:135.05ms
step:51/1370 train_time:5537ms step_avg:135.04ms
step:52/1370 train_time:5672ms step_avg:135.04ms
step:53/1370 train_time:5808ms step_avg:135.06ms
step:54/1370 train_time:5941ms step_avg:135.03ms
step:55/1370 train_time:6076ms step_avg:135.03ms
step:56/1370 train_time:6212ms step_avg:135.04ms
step:57/1370 train_time:6347ms step_avg:135.05ms
step:58/1370 train_time:6481ms step_avg:135.03ms
step:59/1370 train_time:6616ms step_avg:135.02ms
step:60/1370 train_time:6751ms step_avg:135.02ms
step:61/1370 train_time:6887ms step_avg:135.05ms
step:62/1370 train_time:7022ms step_avg:135.04ms
step:63/1370 train_time:7157ms step_avg:135.03ms
step:64/1370 train_time:7292ms step_avg:135.04ms
step:65/1370 train_time:7428ms step_avg:135.05ms
step:66/1370 train_time:7562ms step_avg:135.03ms
step:67/1370 train_time:7698ms step_avg:135.05ms
step:68/1370 train_time:7833ms step_avg:135.05ms
step:69/1370 train_time:7968ms step_avg:135.05ms
step:70/1370 train_time:8104ms step_avg:135.06ms
step:71/1370 train_time:8240ms step_avg:135.09ms
step:72/1370 train_time:8375ms step_avg:135.09ms
step:73/1370 train_time:8510ms step_avg:135.08ms
step:74/1370 train_time:8646ms step_avg:135.10ms
step:75/1370 train_time:8781ms step_avg:135.09ms
step:76/1370 train_time:8916ms step_avg:135.09ms
step:77/1370 train_time:9051ms step_avg:135.09ms
step:78/1370 train_time:9187ms step_avg:135.10ms
step:79/1370 train_time:9322ms step_avg:135.10ms
step:80/1370 train_time:9458ms step_avg:135.11ms
step:81/1370 train_time:9592ms step_avg:135.10ms
step:82/1370 train_time:9728ms step_avg:135.11ms
step:83/1370 train_time:9862ms step_avg:135.10ms
step:84/1370 train_time:9999ms step_avg:135.12ms
step:85/1370 train_time:10133ms step_avg:135.11ms
step:86/1370 train_time:10269ms step_avg:135.12ms
step:87/1370 train_time:10403ms step_avg:135.11ms
step:88/1370 train_time:10539ms step_avg:135.12ms
step:89/1370 train_time:10674ms step_avg:135.12ms
step:90/1370 train_time:10809ms step_avg:135.11ms
step:91/1370 train_time:10944ms step_avg:135.12ms
step:92/1370 train_time:11080ms step_avg:135.13ms
step:93/1370 train_time:11216ms step_avg:135.14ms
step:94/1370 train_time:11352ms step_avg:135.14ms
step:95/1370 train_time:11488ms step_avg:135.15ms
step:96/1370 train_time:11624ms step_avg:135.16ms
step:97/1370 train_time:11760ms step_avg:135.17ms
step:98/1370 train_time:11894ms step_avg:135.16ms
step:99/1370 train_time:12029ms step_avg:135.16ms
step:100/1370 train_time:12165ms step_avg:135.17ms
step:101/1370 train_time:12300ms step_avg:135.17ms
step:102/1370 train_time:12436ms step_avg:135.17ms
step:103/1370 train_time:12572ms step_avg:135.18ms
step:104/1370 train_time:12711ms step_avg:135.23ms
step:105/1370 train_time:12850ms step_avg:135.27ms
step:106/1370 train_time:12988ms step_avg:135.29ms
step:107/1370 train_time:13127ms step_avg:135.33ms
step:108/1370 train_time:13266ms step_avg:135.37ms
step:109/1370 train_time:13404ms step_avg:135.39ms
step:110/1370 train_time:13543ms step_avg:135.43ms
step:111/1370 train_time:13681ms step_avg:135.45ms
step:112/1370 train_time:13819ms step_avg:135.48ms
step:113/1370 train_time:13956ms step_avg:135.50ms
step:114/1370 train_time:14095ms step_avg:135.53ms
step:115/1370 train_time:14233ms step_avg:135.55ms
step:116/1370 train_time:14372ms step_avg:135.59ms
step:117/1370 train_time:14511ms step_avg:135.61ms
step:118/1370 train_time:14650ms step_avg:135.65ms
step:119/1370 train_time:14787ms step_avg:135.66ms
step:120/1370 train_time:14925ms step_avg:135.68ms
step:121/1370 train_time:15064ms step_avg:135.71ms
step:122/1370 train_time:15202ms step_avg:135.73ms
step:123/1370 train_time:15339ms step_avg:135.75ms
step:124/1370 train_time:15477ms step_avg:135.76ms
step:125/1370 train_time:15616ms step_avg:135.79ms
_orig_mod.blocks.0.attn.attn_scale: 0.10929735749959946
_orig_mod.blocks.1.attn.attn_scale: 0.13294784724712372
_orig_mod.blocks.2.attn.attn_scale: 0.13855984807014465
_orig_mod.blocks.3.attn.attn_scale: 0.1792960911989212
_orig_mod.blocks.4.attn.attn_scale: 0.19472771883010864
_orig_mod.blocks.5.attn.attn_scale: 0.1975816786289215
_orig_mod.blocks.6.attn.attn_scale: 0.22341547906398773
_orig_mod.blocks.8.attn.attn_scale: 0.20815463364124298
_orig_mod.blocks.9.attn.attn_scale: 0.13338665664196014
_orig_mod.blocks.10.attn.attn_scale: 0.13603341579437256
_orig_mod.blocks.11.attn.attn_scale: 0.13646472990512848
step:125/1370 val_loss:4.3926 train_time:15678ms step_avg:136.33ms
step:126/1370 train_time:15757ms step_avg:135.84ms
step:127/1370 train_time:15896ms step_avg:135.86ms
step:128/1370 train_time:16035ms step_avg:135.89ms
step:129/1370 train_time:16173ms step_avg:135.91ms
step:130/1370 train_time:16311ms step_avg:135.92ms
step:131/1370 train_time:16448ms step_avg:135.93ms
step:132/1370 train_time:16585ms step_avg:135.95ms
step:133/1370 train_time:16726ms step_avg:135.99ms
step:134/1370 train_time:16866ms step_avg:136.02ms
step:135/1370 train_time:17005ms step_avg:136.04ms
step:136/1370 train_time:17142ms step_avg:136.05ms
step:137/1370 train_time:17282ms step_avg:136.08ms
step:138/1370 train_time:17419ms step_avg:136.09ms
step:139/1370 train_time:17557ms step_avg:136.10ms
step:140/1370 train_time:17697ms step_avg:136.13ms
step:141/1370 train_time:17836ms step_avg:136.15ms
step:142/1370 train_time:17975ms step_avg:136.18ms
step:143/1370 train_time:18114ms step_avg:136.19ms
step:144/1370 train_time:18253ms step_avg:136.22ms
step:145/1370 train_time:18391ms step_avg:136.23ms
step:146/1370 train_time:18529ms step_avg:136.24ms
step:147/1370 train_time:18668ms step_avg:136.26ms
step:148/1370 train_time:18808ms step_avg:136.29ms
step:149/1370 train_time:18948ms step_avg:136.31ms
step:150/1370 train_time:19087ms step_avg:136.34ms
step:151/1370 train_time:19226ms step_avg:136.35ms
step:152/1370 train_time:19365ms step_avg:136.37ms
step:153/1370 train_time:19504ms step_avg:136.39ms
step:154/1370 train_time:19643ms step_avg:136.41ms
step:155/1370 train_time:19780ms step_avg:136.41ms
step:156/1370 train_time:19919ms step_avg:136.43ms
step:157/1370 train_time:20059ms step_avg:136.45ms
step:158/1370 train_time:20198ms step_avg:136.47ms
step:159/1370 train_time:20337ms step_avg:136.49ms
step:160/1370 train_time:20476ms step_avg:136.51ms
step:161/1370 train_time:20615ms step_avg:136.53ms
step:162/1370 train_time:20754ms step_avg:136.54ms
step:163/1370 train_time:20893ms step_avg:136.55ms
step:164/1370 train_time:21032ms step_avg:136.57ms
step:165/1370 train_time:21172ms step_avg:136.59ms
step:166/1370 train_time:21310ms step_avg:136.60ms
step:167/1370 train_time:21448ms step_avg:136.61ms
step:168/1370 train_time:21588ms step_avg:136.63ms
step:169/1370 train_time:21725ms step_avg:136.64ms
step:170/1370 train_time:21864ms step_avg:136.65ms
step:171/1370 train_time:22002ms step_avg:136.66ms
step:172/1370 train_time:22141ms step_avg:136.67ms
step:173/1370 train_time:22279ms step_avg:136.68ms
step:174/1370 train_time:22417ms step_avg:136.69ms
step:175/1370 train_time:22556ms step_avg:136.70ms
step:176/1370 train_time:22695ms step_avg:136.72ms
step:177/1370 train_time:22833ms step_avg:136.72ms
step:178/1370 train_time:22971ms step_avg:136.73ms
step:179/1370 train_time:23109ms step_avg:136.74ms
step:180/1370 train_time:23248ms step_avg:136.75ms
step:181/1370 train_time:23387ms step_avg:136.77ms
step:182/1370 train_time:23525ms step_avg:136.78ms
step:183/1370 train_time:23664ms step_avg:136.79ms
step:184/1370 train_time:23802ms step_avg:136.79ms
step:185/1370 train_time:23940ms step_avg:136.80ms
step:186/1370 train_time:24080ms step_avg:136.82ms
step:187/1370 train_time:24218ms step_avg:136.83ms
step:188/1370 train_time:24358ms step_avg:136.84ms
step:189/1370 train_time:24497ms step_avg:136.86ms
step:190/1370 train_time:24636ms step_avg:136.87ms
step:191/1370 train_time:24813ms step_avg:137.09ms
step:192/1370 train_time:24951ms step_avg:137.09ms
step:193/1370 train_time:25089ms step_avg:137.10ms
step:194/1370 train_time:25227ms step_avg:137.10ms
step:195/1370 train_time:25365ms step_avg:137.11ms
step:196/1370 train_time:25502ms step_avg:137.11ms
step:197/1370 train_time:25643ms step_avg:137.13ms
step:198/1370 train_time:25784ms step_avg:137.15ms
step:199/1370 train_time:25925ms step_avg:137.17ms
step:200/1370 train_time:26063ms step_avg:137.17ms
step:201/1370 train_time:26200ms step_avg:137.17ms
step:202/1370 train_time:26339ms step_avg:137.18ms
step:203/1370 train_time:26478ms step_avg:137.19ms
step:204/1370 train_time:26618ms step_avg:137.21ms
step:205/1370 train_time:26759ms step_avg:137.23ms
step:206/1370 train_time:26902ms step_avg:137.25ms
step:207/1370 train_time:27045ms step_avg:137.28ms
step:208/1370 train_time:27187ms step_avg:137.31ms
step:209/1370 train_time:27329ms step_avg:137.33ms
step:210/1370 train_time:27469ms step_avg:137.35ms
step:211/1370 train_time:27610ms step_avg:137.36ms
step:212/1370 train_time:27752ms step_avg:137.38ms
step:213/1370 train_time:27894ms step_avg:137.41ms
step:214/1370 train_time:28037ms step_avg:137.43ms
step:215/1370 train_time:28178ms step_avg:137.45ms
step:216/1370 train_time:28318ms step_avg:137.47ms
step:217/1370 train_time:28460ms step_avg:137.49ms
step:218/1370 train_time:28601ms step_avg:137.50ms
step:219/1370 train_time:28740ms step_avg:137.51ms
step:220/1370 train_time:28881ms step_avg:137.53ms
step:221/1370 train_time:29023ms step_avg:137.55ms
step:222/1370 train_time:29166ms step_avg:137.57ms
step:223/1370 train_time:29308ms step_avg:137.60ms
step:224/1370 train_time:29449ms step_avg:137.61ms
step:225/1370 train_time:29591ms step_avg:137.63ms
step:226/1370 train_time:29732ms step_avg:137.65ms
step:227/1370 train_time:29872ms step_avg:137.66ms
step:228/1370 train_time:30014ms step_avg:137.68ms
step:229/1370 train_time:30156ms step_avg:137.70ms
step:230/1370 train_time:30297ms step_avg:137.71ms
step:231/1370 train_time:30438ms step_avg:137.73ms
step:232/1370 train_time:30579ms step_avg:137.74ms
step:233/1370 train_time:30720ms step_avg:137.76ms
step:234/1370 train_time:30860ms step_avg:137.77ms
step:235/1370 train_time:31001ms step_avg:137.78ms
step:236/1370 train_time:31142ms step_avg:137.79ms
step:237/1370 train_time:31283ms step_avg:137.81ms
step:238/1370 train_time:31426ms step_avg:137.83ms
step:239/1370 train_time:31567ms step_avg:137.85ms
step:240/1370 train_time:31709ms step_avg:137.86ms
step:241/1370 train_time:31850ms step_avg:137.88ms
step:242/1370 train_time:31991ms step_avg:137.89ms
step:243/1370 train_time:32132ms step_avg:137.91ms
step:244/1370 train_time:32273ms step_avg:137.92ms
step:245/1370 train_time:32414ms step_avg:137.93ms
step:246/1370 train_time:32555ms step_avg:137.95ms
step:247/1370 train_time:32696ms step_avg:137.96ms
step:248/1370 train_time:32837ms step_avg:137.97ms
step:249/1370 train_time:32978ms step_avg:137.98ms
step:250/1370 train_time:33119ms step_avg:137.99ms
_orig_mod.blocks.0.attn.attn_scale: 0.11240368336439133
_orig_mod.blocks.1.attn.attn_scale: 0.14901094138622284
_orig_mod.blocks.2.attn.attn_scale: 0.14926663041114807
_orig_mod.blocks.3.attn.attn_scale: 0.1697719693183899
_orig_mod.blocks.4.attn.attn_scale: 0.17261448502540588
_orig_mod.blocks.5.attn.attn_scale: 0.16922923922538757
_orig_mod.blocks.6.attn.attn_scale: 0.17178362607955933
_orig_mod.blocks.8.attn.attn_scale: 0.15639258921146393
_orig_mod.blocks.9.attn.attn_scale: 0.11297104507684708
_orig_mod.blocks.10.attn.attn_scale: 0.12686534225940704
_orig_mod.blocks.11.attn.attn_scale: 0.1200389489531517
step:250/1370 val_loss:3.9563 train_time:33182ms step_avg:138.26ms
step:251/1370 train_time:33264ms step_avg:138.02ms
step:252/1370 train_time:33407ms step_avg:138.05ms
step:253/1370 train_time:33549ms step_avg:138.06ms
step:254/1370 train_time:33689ms step_avg:138.07ms
step:255/1370 train_time:33828ms step_avg:138.08ms
step:256/1370 train_time:33969ms step_avg:138.08ms
step:257/1370 train_time:34110ms step_avg:138.10ms
step:258/1370 train_time:34254ms step_avg:138.12ms
step:259/1370 train_time:34395ms step_avg:138.13ms
step:260/1370 train_time:34536ms step_avg:138.14ms
step:261/1370 train_time:34676ms step_avg:138.15ms
step:262/1370 train_time:34816ms step_avg:138.16ms
step:263/1370 train_time:34955ms step_avg:138.16ms
step:264/1370 train_time:35095ms step_avg:138.17ms
step:265/1370 train_time:35238ms step_avg:138.19ms
step:266/1370 train_time:35380ms step_avg:138.20ms
step:267/1370 train_time:35522ms step_avg:138.22ms
step:268/1370 train_time:35664ms step_avg:138.23ms
step:269/1370 train_time:35805ms step_avg:138.24ms
step:270/1370 train_time:35945ms step_avg:138.25ms
step:271/1370 train_time:36087ms step_avg:138.26ms
step:272/1370 train_time:36228ms step_avg:138.27ms
step:273/1370 train_time:36370ms step_avg:138.29ms
step:274/1370 train_time:36512ms step_avg:138.30ms
step:275/1370 train_time:36654ms step_avg:138.32ms
step:276/1370 train_time:36795ms step_avg:138.33ms
step:277/1370 train_time:36936ms step_avg:138.34ms
step:278/1370 train_time:37076ms step_avg:138.34ms
step:279/1370 train_time:37217ms step_avg:138.35ms
step:280/1370 train_time:37359ms step_avg:138.37ms
step:281/1370 train_time:37502ms step_avg:138.38ms
step:282/1370 train_time:37644ms step_avg:138.40ms
step:283/1370 train_time:37785ms step_avg:138.41ms
step:284/1370 train_time:37926ms step_avg:138.42ms
step:285/1370 train_time:38067ms step_avg:138.43ms
step:286/1370 train_time:38208ms step_avg:138.43ms
step:287/1370 train_time:38349ms step_avg:138.44ms
step:288/1370 train_time:38491ms step_avg:138.46ms
step:289/1370 train_time:38633ms step_avg:138.47ms
step:290/1370 train_time:38774ms step_avg:138.48ms
step:291/1370 train_time:38914ms step_avg:138.49ms
step:292/1370 train_time:39056ms step_avg:138.50ms
step:293/1370 train_time:39196ms step_avg:138.50ms
step:294/1370 train_time:39336ms step_avg:138.51ms
step:295/1370 train_time:39478ms step_avg:138.52ms
step:296/1370 train_time:39620ms step_avg:138.53ms
step:297/1370 train_time:39761ms step_avg:138.54ms
step:298/1370 train_time:39904ms step_avg:138.56ms
step:299/1370 train_time:40047ms step_avg:138.57ms
step:300/1370 train_time:40187ms step_avg:138.58ms
step:301/1370 train_time:40328ms step_avg:138.58ms
step:302/1370 train_time:40470ms step_avg:138.60ms
step:303/1370 train_time:40611ms step_avg:138.61ms
step:304/1370 train_time:40753ms step_avg:138.62ms
step:305/1370 train_time:40894ms step_avg:138.62ms
step:306/1370 train_time:41037ms step_avg:138.64ms
step:307/1370 train_time:41180ms step_avg:138.65ms
step:308/1370 train_time:41324ms step_avg:138.67ms
step:309/1370 train_time:41468ms step_avg:138.69ms
step:310/1370 train_time:41611ms step_avg:138.70ms
step:311/1370 train_time:41754ms step_avg:138.72ms
step:312/1370 train_time:41899ms step_avg:138.74ms
step:313/1370 train_time:42044ms step_avg:138.76ms
step:314/1370 train_time:42188ms step_avg:138.78ms
step:315/1370 train_time:42332ms step_avg:138.79ms
step:316/1370 train_time:42474ms step_avg:138.81ms
step:317/1370 train_time:42619ms step_avg:138.82ms
step:318/1370 train_time:42763ms step_avg:138.84ms
step:319/1370 train_time:42907ms step_avg:138.86ms
step:320/1370 train_time:43052ms step_avg:138.88ms
step:321/1370 train_time:43193ms step_avg:138.89ms
step:322/1370 train_time:43336ms step_avg:138.90ms
step:323/1370 train_time:43478ms step_avg:138.91ms
step:324/1370 train_time:43622ms step_avg:138.92ms
step:325/1370 train_time:43767ms step_avg:138.94ms
step:326/1370 train_time:43911ms step_avg:138.96ms
step:327/1370 train_time:44054ms step_avg:138.97ms
step:328/1370 train_time:44197ms step_avg:138.98ms
step:329/1370 train_time:44341ms step_avg:139.00ms
step:330/1370 train_time:44484ms step_avg:139.01ms
step:331/1370 train_time:44627ms step_avg:139.02ms
step:332/1370 train_time:44771ms step_avg:139.04ms
step:333/1370 train_time:44915ms step_avg:139.05ms
step:334/1370 train_time:45058ms step_avg:139.07ms
step:335/1370 train_time:45201ms step_avg:139.08ms
step:336/1370 train_time:45345ms step_avg:139.10ms
step:337/1370 train_time:45488ms step_avg:139.11ms
step:338/1370 train_time:45631ms step_avg:139.12ms
step:339/1370 train_time:45773ms step_avg:139.13ms
step:340/1370 train_time:45916ms step_avg:139.14ms
step:341/1370 train_time:46059ms step_avg:139.15ms
step:342/1370 train_time:46204ms step_avg:139.17ms
step:343/1370 train_time:46348ms step_avg:139.18ms
step:344/1370 train_time:46491ms step_avg:139.19ms
step:345/1370 train_time:46635ms step_avg:139.21ms
step:346/1370 train_time:46778ms step_avg:139.22ms
step:347/1370 train_time:46922ms step_avg:139.23ms
step:348/1370 train_time:47065ms step_avg:139.25ms
step:349/1370 train_time:47208ms step_avg:139.26ms
step:350/1370 train_time:47352ms step_avg:139.27ms
step:351/1370 train_time:47494ms step_avg:139.28ms
step:352/1370 train_time:47638ms step_avg:139.29ms
step:353/1370 train_time:47782ms step_avg:139.31ms
step:354/1370 train_time:47925ms step_avg:139.32ms
step:355/1370 train_time:48069ms step_avg:139.33ms
step:356/1370 train_time:48212ms step_avg:139.34ms
step:357/1370 train_time:48355ms step_avg:139.35ms
step:358/1370 train_time:48498ms step_avg:139.36ms
step:359/1370 train_time:48642ms step_avg:139.38ms
step:360/1370 train_time:48787ms step_avg:139.39ms
step:361/1370 train_time:48931ms step_avg:139.40ms
step:362/1370 train_time:49074ms step_avg:139.42ms
step:363/1370 train_time:49219ms step_avg:139.43ms
step:364/1370 train_time:49363ms step_avg:139.44ms
step:365/1370 train_time:49506ms step_avg:139.45ms
step:366/1370 train_time:49650ms step_avg:139.47ms
step:367/1370 train_time:49793ms step_avg:139.48ms
step:368/1370 train_time:49936ms step_avg:139.49ms
step:369/1370 train_time:50080ms step_avg:139.50ms
step:370/1370 train_time:50224ms step_avg:139.51ms
step:371/1370 train_time:50367ms step_avg:139.52ms
step:372/1370 train_time:50511ms step_avg:139.53ms
step:373/1370 train_time:50654ms step_avg:139.54ms
step:374/1370 train_time:50797ms step_avg:139.55ms
step:375/1370 train_time:50940ms step_avg:139.56ms
_orig_mod.blocks.0.attn.attn_scale: 0.11503627151250839
_orig_mod.blocks.1.attn.attn_scale: 0.16050532460212708
_orig_mod.blocks.2.attn.attn_scale: 0.15913155674934387
_orig_mod.blocks.3.attn.attn_scale: 0.1690029352903366
_orig_mod.blocks.4.attn.attn_scale: 0.1750437170267105
_orig_mod.blocks.5.attn.attn_scale: 0.16408374905586243
_orig_mod.blocks.6.attn.attn_scale: 0.1611815243959427
_orig_mod.blocks.8.attn.attn_scale: 0.14992275834083557
_orig_mod.blocks.9.attn.attn_scale: 0.12487296760082245
_orig_mod.blocks.10.attn.attn_scale: 0.13725480437278748
_orig_mod.blocks.11.attn.attn_scale: 0.12665578722953796
step:375/1370 val_loss:3.7748 train_time:51004ms step_avg:139.74ms
step:376/1370 train_time:51084ms step_avg:139.57ms
step:377/1370 train_time:51228ms step_avg:139.59ms
step:378/1370 train_time:51371ms step_avg:139.60ms
step:379/1370 train_time:51514ms step_avg:139.60ms
step:380/1370 train_time:51657ms step_avg:139.61ms
step:381/1370 train_time:51842ms step_avg:139.74ms
step:382/1370 train_time:51985ms step_avg:139.74ms
step:383/1370 train_time:52127ms step_avg:139.75ms
step:384/1370 train_time:52270ms step_avg:139.76ms
step:385/1370 train_time:52413ms step_avg:139.77ms
step:386/1370 train_time:52555ms step_avg:139.77ms
step:387/1370 train_time:52700ms step_avg:139.79ms
step:388/1370 train_time:52844ms step_avg:139.80ms
step:389/1370 train_time:52989ms step_avg:139.81ms
step:390/1370 train_time:53133ms step_avg:139.82ms
step:391/1370 train_time:53275ms step_avg:139.83ms
step:392/1370 train_time:53418ms step_avg:139.84ms
step:393/1370 train_time:53561ms step_avg:139.85ms
step:394/1370 train_time:53704ms step_avg:139.86ms
step:395/1370 train_time:53850ms step_avg:139.87ms
step:396/1370 train_time:53993ms step_avg:139.88ms
step:397/1370 train_time:54136ms step_avg:139.89ms
step:398/1370 train_time:54280ms step_avg:139.90ms
step:399/1370 train_time:54422ms step_avg:139.90ms
step:400/1370 train_time:54565ms step_avg:139.91ms
step:401/1370 train_time:54708ms step_avg:139.92ms
step:402/1370 train_time:54852ms step_avg:139.93ms
step:403/1370 train_time:54997ms step_avg:139.94ms
step:404/1370 train_time:55141ms step_avg:139.95ms
step:405/1370 train_time:55284ms step_avg:139.96ms
step:406/1370 train_time:55427ms step_avg:139.97ms
step:407/1370 train_time:55571ms step_avg:139.98ms
step:408/1370 train_time:55715ms step_avg:139.99ms
step:409/1370 train_time:55860ms step_avg:140.00ms
step:410/1370 train_time:56005ms step_avg:140.01ms
step:411/1370 train_time:56151ms step_avg:140.03ms
step:412/1370 train_time:56295ms step_avg:140.04ms
step:413/1370 train_time:56441ms step_avg:140.05ms
step:414/1370 train_time:56584ms step_avg:140.06ms
step:415/1370 train_time:56730ms step_avg:140.07ms
step:416/1370 train_time:56876ms step_avg:140.09ms
step:417/1370 train_time:57022ms step_avg:140.10ms
step:418/1370 train_time:57168ms step_avg:140.12ms
step:419/1370 train_time:57313ms step_avg:140.13ms
step:420/1370 train_time:57458ms step_avg:140.14ms
step:421/1370 train_time:57602ms step_avg:140.15ms
step:422/1370 train_time:57747ms step_avg:140.16ms
step:423/1370 train_time:57892ms step_avg:140.17ms
step:424/1370 train_time:58038ms step_avg:140.19ms
step:425/1370 train_time:58184ms step_avg:140.20ms
step:426/1370 train_time:58329ms step_avg:140.21ms
step:427/1370 train_time:58474ms step_avg:140.23ms
step:428/1370 train_time:58620ms step_avg:140.24ms
step:429/1370 train_time:58764ms step_avg:140.25ms
step:430/1370 train_time:58911ms step_avg:140.26ms
step:431/1370 train_time:59057ms step_avg:140.28ms
step:432/1370 train_time:59203ms step_avg:140.29ms
step:433/1370 train_time:59347ms step_avg:140.30ms
step:434/1370 train_time:59492ms step_avg:140.31ms
step:435/1370 train_time:59637ms step_avg:140.32ms
step:436/1370 train_time:59781ms step_avg:140.33ms
step:437/1370 train_time:59927ms step_avg:140.34ms
step:438/1370 train_time:60072ms step_avg:140.35ms
step:439/1370 train_time:60216ms step_avg:140.36ms
step:440/1370 train_time:60362ms step_avg:140.38ms
step:441/1370 train_time:60507ms step_avg:140.39ms
step:442/1370 train_time:60652ms step_avg:140.40ms
step:443/1370 train_time:60798ms step_avg:140.41ms
step:444/1370 train_time:60943ms step_avg:140.42ms
step:445/1370 train_time:61088ms step_avg:140.43ms
step:446/1370 train_time:61232ms step_avg:140.44ms
step:447/1370 train_time:61378ms step_avg:140.45ms
step:448/1370 train_time:61523ms step_avg:140.46ms
step:449/1370 train_time:61669ms step_avg:140.48ms
step:450/1370 train_time:61815ms step_avg:140.49ms
step:451/1370 train_time:61960ms step_avg:140.50ms
step:452/1370 train_time:62104ms step_avg:140.51ms
step:453/1370 train_time:62250ms step_avg:140.52ms
step:454/1370 train_time:62397ms step_avg:140.53ms
step:455/1370 train_time:62541ms step_avg:140.54ms
step:456/1370 train_time:62686ms step_avg:140.55ms
step:457/1370 train_time:62831ms step_avg:140.56ms
step:458/1370 train_time:62977ms step_avg:140.57ms
step:459/1370 train_time:63122ms step_avg:140.58ms
step:460/1370 train_time:63267ms step_avg:140.59ms
step:461/1370 train_time:63413ms step_avg:140.60ms
step:462/1370 train_time:63559ms step_avg:140.62ms
step:463/1370 train_time:63703ms step_avg:140.63ms
step:464/1370 train_time:63849ms step_avg:140.64ms
step:465/1370 train_time:63995ms step_avg:140.65ms
step:466/1370 train_time:64140ms step_avg:140.66ms
step:467/1370 train_time:64285ms step_avg:140.67ms
step:468/1370 train_time:64430ms step_avg:140.68ms
step:469/1370 train_time:64576ms step_avg:140.69ms
step:470/1370 train_time:64720ms step_avg:140.70ms
step:471/1370 train_time:64865ms step_avg:140.70ms
step:472/1370 train_time:65011ms step_avg:140.72ms
step:473/1370 train_time:65158ms step_avg:140.73ms
step:474/1370 train_time:65303ms step_avg:140.74ms
step:475/1370 train_time:65447ms step_avg:140.75ms
step:476/1370 train_time:65592ms step_avg:140.76ms
step:477/1370 train_time:65738ms step_avg:140.77ms
step:478/1370 train_time:65883ms step_avg:140.78ms
step:479/1370 train_time:66028ms step_avg:140.78ms
step:480/1370 train_time:66174ms step_avg:140.80ms
step:481/1370 train_time:66320ms step_avg:140.81ms
step:482/1370 train_time:66464ms step_avg:140.81ms
step:483/1370 train_time:66611ms step_avg:140.83ms
step:484/1370 train_time:66756ms step_avg:140.83ms
step:485/1370 train_time:66901ms step_avg:140.84ms
step:486/1370 train_time:67046ms step_avg:140.85ms
step:487/1370 train_time:67192ms step_avg:140.86ms
step:488/1370 train_time:67338ms step_avg:140.87ms
step:489/1370 train_time:67483ms step_avg:140.88ms
step:490/1370 train_time:67628ms step_avg:140.89ms
step:491/1370 train_time:67773ms step_avg:140.90ms
step:492/1370 train_time:67918ms step_avg:140.91ms
step:493/1370 train_time:68063ms step_avg:140.92ms
step:494/1370 train_time:68209ms step_avg:140.93ms
step:495/1370 train_time:68355ms step_avg:140.94ms
step:496/1370 train_time:68500ms step_avg:140.95ms
step:497/1370 train_time:68645ms step_avg:140.95ms
step:498/1370 train_time:68791ms step_avg:140.96ms
step:499/1370 train_time:68936ms step_avg:140.97ms
step:500/1370 train_time:69084ms step_avg:140.99ms
_orig_mod.blocks.0.attn.attn_scale: 0.11938557028770447
_orig_mod.blocks.1.attn.attn_scale: 0.1645306944847107
_orig_mod.blocks.2.attn.attn_scale: 0.15903468430042267
_orig_mod.blocks.3.attn.attn_scale: 0.1689232587814331
_orig_mod.blocks.4.attn.attn_scale: 0.173727348446846
_orig_mod.blocks.5.attn.attn_scale: 0.1662413477897644
_orig_mod.blocks.6.attn.attn_scale: 0.16214242577552795
_orig_mod.blocks.8.attn.attn_scale: 0.1505853235721588
_orig_mod.blocks.9.attn.attn_scale: 0.13207298517227173
_orig_mod.blocks.10.attn.attn_scale: 0.14435088634490967
_orig_mod.blocks.11.attn.attn_scale: 0.12881353497505188
step:500/1370 val_loss:3.6578 train_time:69149ms step_avg:141.12ms
step:501/1370 train_time:69230ms step_avg:141.00ms
step:502/1370 train_time:69377ms step_avg:141.01ms
step:503/1370 train_time:69522ms step_avg:141.02ms
step:504/1370 train_time:69666ms step_avg:141.02ms
step:505/1370 train_time:69810ms step_avg:141.03ms
step:506/1370 train_time:69955ms step_avg:141.04ms
step:507/1370 train_time:70100ms step_avg:141.05ms
step:508/1370 train_time:70248ms step_avg:141.06ms
step:509/1370 train_time:70393ms step_avg:141.07ms
step:510/1370 train_time:70539ms step_avg:141.08ms
step:511/1370 train_time:70686ms step_avg:141.09ms
step:512/1370 train_time:70832ms step_avg:141.10ms
step:513/1370 train_time:70979ms step_avg:141.11ms
step:514/1370 train_time:71128ms step_avg:141.13ms
step:515/1370 train_time:71277ms step_avg:141.14ms
step:516/1370 train_time:71425ms step_avg:141.16ms
step:517/1370 train_time:71571ms step_avg:141.16ms
step:518/1370 train_time:71717ms step_avg:141.17ms
step:519/1370 train_time:71864ms step_avg:141.19ms
step:520/1370 train_time:72010ms step_avg:141.20ms
step:521/1370 train_time:72158ms step_avg:141.21ms
step:522/1370 train_time:72306ms step_avg:141.22ms
step:523/1370 train_time:72452ms step_avg:141.23ms
step:524/1370 train_time:72600ms step_avg:141.24ms
step:525/1370 train_time:72746ms step_avg:141.25ms
step:526/1370 train_time:72891ms step_avg:141.26ms
step:527/1370 train_time:73039ms step_avg:141.27ms
step:528/1370 train_time:73186ms step_avg:141.28ms
step:529/1370 train_time:73332ms step_avg:141.30ms
step:530/1370 train_time:73481ms step_avg:141.31ms
step:531/1370 train_time:73627ms step_avg:141.32ms
step:532/1370 train_time:73773ms step_avg:141.33ms
step:533/1370 train_time:73920ms step_avg:141.34ms
step:534/1370 train_time:74067ms step_avg:141.35ms
step:535/1370 train_time:74214ms step_avg:141.36ms
step:536/1370 train_time:74361ms step_avg:141.37ms
step:537/1370 train_time:74508ms step_avg:141.38ms
step:538/1370 train_time:74654ms step_avg:141.39ms
step:539/1370 train_time:74802ms step_avg:141.40ms
step:540/1370 train_time:74949ms step_avg:141.41ms
step:541/1370 train_time:75095ms step_avg:141.42ms
step:542/1370 train_time:75241ms step_avg:141.43ms
step:543/1370 train_time:75387ms step_avg:141.44ms
step:544/1370 train_time:75533ms step_avg:141.45ms
step:545/1370 train_time:75680ms step_avg:141.46ms
step:546/1370 train_time:75827ms step_avg:141.47ms
step:547/1370 train_time:75973ms step_avg:141.48ms
step:548/1370 train_time:76121ms step_avg:141.49ms
step:549/1370 train_time:76269ms step_avg:141.50ms
step:550/1370 train_time:76415ms step_avg:141.51ms
step:551/1370 train_time:76563ms step_avg:141.52ms
step:552/1370 train_time:76710ms step_avg:141.53ms
step:553/1370 train_time:76857ms step_avg:141.54ms
step:554/1370 train_time:77004ms step_avg:141.55ms
step:555/1370 train_time:77150ms step_avg:141.56ms
step:556/1370 train_time:77296ms step_avg:141.57ms
step:557/1370 train_time:77442ms step_avg:141.58ms
step:558/1370 train_time:77589ms step_avg:141.59ms
step:559/1370 train_time:77735ms step_avg:141.59ms
step:560/1370 train_time:77883ms step_avg:141.60ms
step:561/1370 train_time:78029ms step_avg:141.61ms
step:562/1370 train_time:78177ms step_avg:141.63ms
step:563/1370 train_time:78324ms step_avg:141.63ms
step:564/1370 train_time:78470ms step_avg:141.64ms
step:565/1370 train_time:78616ms step_avg:141.65ms
step:566/1370 train_time:78763ms step_avg:141.66ms
step:567/1370 train_time:78909ms step_avg:141.67ms
step:568/1370 train_time:79055ms step_avg:141.68ms
step:569/1370 train_time:79203ms step_avg:141.69ms
step:570/1370 train_time:79350ms step_avg:141.70ms
step:571/1370 train_time:79540ms step_avg:141.78ms
step:572/1370 train_time:79685ms step_avg:141.79ms
step:573/1370 train_time:79832ms step_avg:141.80ms
step:574/1370 train_time:79980ms step_avg:141.81ms
step:575/1370 train_time:80126ms step_avg:141.82ms
step:576/1370 train_time:80271ms step_avg:141.82ms
step:577/1370 train_time:80418ms step_avg:141.83ms
step:578/1370 train_time:80568ms step_avg:141.84ms
step:579/1370 train_time:80713ms step_avg:141.85ms
step:580/1370 train_time:80859ms step_avg:141.86ms
step:581/1370 train_time:81007ms step_avg:141.87ms
step:582/1370 train_time:81152ms step_avg:141.87ms
step:583/1370 train_time:81299ms step_avg:141.88ms
step:584/1370 train_time:81446ms step_avg:141.89ms
step:585/1370 train_time:81592ms step_avg:141.90ms
step:586/1370 train_time:81739ms step_avg:141.91ms
step:587/1370 train_time:81887ms step_avg:141.92ms
step:588/1370 train_time:82032ms step_avg:141.92ms
step:589/1370 train_time:82178ms step_avg:141.93ms
step:590/1370 train_time:82325ms step_avg:141.94ms
step:591/1370 train_time:82472ms step_avg:141.95ms
step:592/1370 train_time:82619ms step_avg:141.96ms
step:593/1370 train_time:82766ms step_avg:141.97ms
step:594/1370 train_time:82914ms step_avg:141.98ms
step:595/1370 train_time:83061ms step_avg:141.99ms
step:596/1370 train_time:83209ms step_avg:141.99ms
step:597/1370 train_time:83355ms step_avg:142.00ms
step:598/1370 train_time:83503ms step_avg:142.01ms
step:599/1370 train_time:83650ms step_avg:142.02ms
step:600/1370 train_time:83796ms step_avg:142.03ms
step:601/1370 train_time:83943ms step_avg:142.04ms
step:602/1370 train_time:84089ms step_avg:142.04ms
step:603/1370 train_time:84236ms step_avg:142.05ms
step:604/1370 train_time:84383ms step_avg:142.06ms
step:605/1370 train_time:84530ms step_avg:142.07ms
step:606/1370 train_time:84678ms step_avg:142.08ms
step:607/1370 train_time:84826ms step_avg:142.09ms
step:608/1370 train_time:84971ms step_avg:142.09ms
step:609/1370 train_time:85118ms step_avg:142.10ms
step:610/1370 train_time:85266ms step_avg:142.11ms
step:611/1370 train_time:85412ms step_avg:142.12ms
step:612/1370 train_time:85561ms step_avg:142.13ms
step:613/1370 train_time:85709ms step_avg:142.14ms
step:614/1370 train_time:85858ms step_avg:142.15ms
step:615/1370 train_time:86007ms step_avg:142.16ms
step:616/1370 train_time:86154ms step_avg:142.17ms
step:617/1370 train_time:86303ms step_avg:142.18ms
step:618/1370 train_time:86449ms step_avg:142.19ms
step:619/1370 train_time:86598ms step_avg:142.20ms
step:620/1370 train_time:86747ms step_avg:142.21ms
step:621/1370 train_time:86896ms step_avg:142.22ms
step:622/1370 train_time:87045ms step_avg:142.23ms
step:623/1370 train_time:87192ms step_avg:142.24ms
step:624/1370 train_time:87341ms step_avg:142.25ms
step:625/1370 train_time:87488ms step_avg:142.26ms
_orig_mod.blocks.0.attn.attn_scale: 0.1279841512441635
_orig_mod.blocks.1.attn.attn_scale: 0.16685470938682556
_orig_mod.blocks.2.attn.attn_scale: 0.1651824712753296
_orig_mod.blocks.3.attn.attn_scale: 0.1720069795846939
_orig_mod.blocks.4.attn.attn_scale: 0.17696720361709595
_orig_mod.blocks.5.attn.attn_scale: 0.16329973936080933
_orig_mod.blocks.6.attn.attn_scale: 0.16121038794517517
_orig_mod.blocks.8.attn.attn_scale: 0.15143489837646484
_orig_mod.blocks.9.attn.attn_scale: 0.13861984014511108
_orig_mod.blocks.10.attn.attn_scale: 0.14972199499607086
_orig_mod.blocks.11.attn.attn_scale: 0.13190637528896332
step:625/1370 val_loss:3.5761 train_time:87557ms step_avg:142.37ms
step:626/1370 train_time:87640ms step_avg:142.27ms
step:627/1370 train_time:87790ms step_avg:142.28ms
step:628/1370 train_time:87937ms step_avg:142.29ms
step:629/1370 train_time:88084ms step_avg:142.30ms
step:630/1370 train_time:88230ms step_avg:142.31ms
step:631/1370 train_time:88378ms step_avg:142.32ms
step:632/1370 train_time:88526ms step_avg:142.33ms
step:633/1370 train_time:88675ms step_avg:142.34ms
step:634/1370 train_time:88825ms step_avg:142.35ms
step:635/1370 train_time:88973ms step_avg:142.36ms
step:636/1370 train_time:89122ms step_avg:142.37ms
step:637/1370 train_time:89270ms step_avg:142.38ms
step:638/1370 train_time:89417ms step_avg:142.38ms
step:639/1370 train_time:89564ms step_avg:142.39ms
step:640/1370 train_time:89712ms step_avg:142.40ms
step:641/1370 train_time:89861ms step_avg:142.41ms
step:642/1370 train_time:90009ms step_avg:142.42ms
step:643/1370 train_time:90157ms step_avg:142.43ms
step:644/1370 train_time:90304ms step_avg:142.44ms
step:645/1370 train_time:90451ms step_avg:142.44ms
step:646/1370 train_time:90599ms step_avg:142.45ms
step:647/1370 train_time:90748ms step_avg:142.46ms
step:648/1370 train_time:90899ms step_avg:142.48ms
step:649/1370 train_time:91048ms step_avg:142.48ms
step:650/1370 train_time:91197ms step_avg:142.49ms
step:651/1370 train_time:91345ms step_avg:142.50ms
step:652/1370 train_time:91492ms step_avg:142.51ms
step:653/1370 train_time:91641ms step_avg:142.52ms
step:654/1370 train_time:91789ms step_avg:142.53ms
step:655/1370 train_time:91937ms step_avg:142.54ms
step:656/1370 train_time:92085ms step_avg:142.55ms
step:657/1370 train_time:92232ms step_avg:142.55ms
step:658/1370 train_time:92381ms step_avg:142.56ms
step:659/1370 train_time:92528ms step_avg:142.57ms
step:660/1370 train_time:92677ms step_avg:142.58ms
step:661/1370 train_time:92824ms step_avg:142.59ms
step:662/1370 train_time:92972ms step_avg:142.59ms
step:663/1370 train_time:93120ms step_avg:142.60ms
step:664/1370 train_time:93269ms step_avg:142.61ms
step:665/1370 train_time:93417ms step_avg:142.62ms
step:666/1370 train_time:93565ms step_avg:142.63ms
step:667/1370 train_time:93713ms step_avg:142.64ms
step:668/1370 train_time:93862ms step_avg:142.65ms
step:669/1370 train_time:94012ms step_avg:142.66ms
step:670/1370 train_time:94161ms step_avg:142.67ms
step:671/1370 train_time:94309ms step_avg:142.68ms
step:672/1370 train_time:94457ms step_avg:142.68ms
step:673/1370 train_time:94604ms step_avg:142.69ms
step:674/1370 train_time:94752ms step_avg:142.70ms
step:675/1370 train_time:94900ms step_avg:142.71ms
step:676/1370 train_time:95050ms step_avg:142.72ms
step:677/1370 train_time:95198ms step_avg:142.73ms
step:678/1370 train_time:95346ms step_avg:142.73ms
step:679/1370 train_time:95493ms step_avg:142.74ms
step:680/1370 train_time:95643ms step_avg:142.75ms
step:681/1370 train_time:95791ms step_avg:142.76ms
step:682/1370 train_time:95939ms step_avg:142.77ms
step:683/1370 train_time:96088ms step_avg:142.78ms
step:684/1370 train_time:96237ms step_avg:142.78ms
step:685/1370 train_time:96386ms step_avg:142.79ms
step:686/1370 train_time:96533ms step_avg:142.80ms
step:687/1370 train_time:96681ms step_avg:142.81ms
step:688/1370 train_time:96830ms step_avg:142.82ms
step:689/1370 train_time:96980ms step_avg:142.83ms
step:690/1370 train_time:97129ms step_avg:142.84ms
step:691/1370 train_time:97277ms step_avg:142.84ms
step:692/1370 train_time:97425ms step_avg:142.85ms
step:693/1370 train_time:97572ms step_avg:142.86ms
step:694/1370 train_time:97720ms step_avg:142.87ms
step:695/1370 train_time:97868ms step_avg:142.87ms
step:696/1370 train_time:98016ms step_avg:142.88ms
step:697/1370 train_time:98164ms step_avg:142.89ms
step:698/1370 train_time:98311ms step_avg:142.89ms
step:699/1370 train_time:98461ms step_avg:142.90ms
step:700/1370 train_time:98609ms step_avg:142.91ms
step:701/1370 train_time:98757ms step_avg:142.92ms
step:702/1370 train_time:98906ms step_avg:142.93ms
step:703/1370 train_time:99053ms step_avg:142.93ms
step:704/1370 train_time:99202ms step_avg:142.94ms
step:705/1370 train_time:99350ms step_avg:142.95ms
step:706/1370 train_time:99500ms step_avg:142.96ms
step:707/1370 train_time:99649ms step_avg:142.97ms
step:708/1370 train_time:99798ms step_avg:142.98ms
step:709/1370 train_time:99948ms step_avg:142.99ms
step:710/1370 train_time:100096ms step_avg:142.99ms
step:711/1370 train_time:100244ms step_avg:143.00ms
step:712/1370 train_time:100392ms step_avg:143.01ms
step:713/1370 train_time:100543ms step_avg:143.02ms
step:714/1370 train_time:100691ms step_avg:143.03ms
step:715/1370 train_time:100842ms step_avg:143.04ms
step:716/1370 train_time:100991ms step_avg:143.05ms
step:717/1370 train_time:101142ms step_avg:143.06ms
step:718/1370 train_time:101290ms step_avg:143.06ms
step:719/1370 train_time:101439ms step_avg:143.07ms
step:720/1370 train_time:101590ms step_avg:143.08ms
step:721/1370 train_time:101739ms step_avg:143.09ms
step:722/1370 train_time:101889ms step_avg:143.10ms
step:723/1370 train_time:102039ms step_avg:143.11ms
step:724/1370 train_time:102189ms step_avg:143.12ms
step:725/1370 train_time:102340ms step_avg:143.13ms
step:726/1370 train_time:102490ms step_avg:143.14ms
step:727/1370 train_time:102641ms step_avg:143.15ms
step:728/1370 train_time:102789ms step_avg:143.16ms
step:729/1370 train_time:102938ms step_avg:143.17ms
step:730/1370 train_time:103089ms step_avg:143.18ms
step:731/1370 train_time:103239ms step_avg:143.19ms
step:732/1370 train_time:103388ms step_avg:143.20ms
step:733/1370 train_time:103537ms step_avg:143.20ms
step:734/1370 train_time:103687ms step_avg:143.21ms
step:735/1370 train_time:103836ms step_avg:143.22ms
step:736/1370 train_time:103986ms step_avg:143.23ms
step:737/1370 train_time:104134ms step_avg:143.24ms
step:738/1370 train_time:104283ms step_avg:143.25ms
step:739/1370 train_time:104432ms step_avg:143.25ms
step:740/1370 train_time:104584ms step_avg:143.27ms
step:741/1370 train_time:104736ms step_avg:143.28ms
step:742/1370 train_time:104885ms step_avg:143.29ms
step:743/1370 train_time:105034ms step_avg:143.29ms
step:744/1370 train_time:105183ms step_avg:143.30ms
step:745/1370 train_time:105333ms step_avg:143.31ms
step:746/1370 train_time:105482ms step_avg:143.32ms
step:747/1370 train_time:105631ms step_avg:143.33ms
step:748/1370 train_time:105781ms step_avg:143.33ms
step:749/1370 train_time:105931ms step_avg:143.34ms
step:750/1370 train_time:106081ms step_avg:143.35ms
_orig_mod.blocks.0.attn.attn_scale: 0.13264305889606476
_orig_mod.blocks.1.attn.attn_scale: 0.1701008826494217
_orig_mod.blocks.2.attn.attn_scale: 0.16961432993412018
_orig_mod.blocks.3.attn.attn_scale: 0.17470122873783112
_orig_mod.blocks.4.attn.attn_scale: 0.18042410910129547
_orig_mod.blocks.5.attn.attn_scale: 0.16806301474571228
_orig_mod.blocks.6.attn.attn_scale: 0.16453242301940918
_orig_mod.blocks.8.attn.attn_scale: 0.15549616515636444
_orig_mod.blocks.9.attn.attn_scale: 0.14233151078224182
_orig_mod.blocks.10.attn.attn_scale: 0.1561250239610672
_orig_mod.blocks.11.attn.attn_scale: 0.13297982513904572
step:750/1370 val_loss:3.5199 train_time:106152ms step_avg:143.45ms
step:751/1370 train_time:106235ms step_avg:143.37ms
step:752/1370 train_time:106383ms step_avg:143.37ms
step:753/1370 train_time:106531ms step_avg:143.38ms
step:754/1370 train_time:106681ms step_avg:143.39ms
step:755/1370 train_time:106828ms step_avg:143.39ms
step:756/1370 train_time:106977ms step_avg:143.40ms
step:757/1370 train_time:107128ms step_avg:143.41ms
step:758/1370 train_time:107279ms step_avg:143.42ms
step:759/1370 train_time:107430ms step_avg:143.43ms
step:760/1370 train_time:107580ms step_avg:143.44ms
step:761/1370 train_time:107774ms step_avg:143.51ms
step:762/1370 train_time:107923ms step_avg:143.51ms
step:763/1370 train_time:108071ms step_avg:143.52ms
step:764/1370 train_time:108222ms step_avg:143.53ms
step:765/1370 train_time:108371ms step_avg:143.54ms
step:766/1370 train_time:108521ms step_avg:143.55ms
step:767/1370 train_time:108672ms step_avg:143.56ms
step:768/1370 train_time:108822ms step_avg:143.57ms
step:769/1370 train_time:108971ms step_avg:143.57ms
step:770/1370 train_time:109121ms step_avg:143.58ms
step:771/1370 train_time:109269ms step_avg:143.59ms
step:772/1370 train_time:109417ms step_avg:143.59ms
step:773/1370 train_time:109566ms step_avg:143.60ms
step:774/1370 train_time:109716ms step_avg:143.61ms
step:775/1370 train_time:109867ms step_avg:143.62ms
step:776/1370 train_time:110017ms step_avg:143.63ms
step:777/1370 train_time:110168ms step_avg:143.64ms
step:778/1370 train_time:110317ms step_avg:143.64ms
step:779/1370 train_time:110467ms step_avg:143.65ms
step:780/1370 train_time:110618ms step_avg:143.66ms
step:781/1370 train_time:110768ms step_avg:143.67ms
step:782/1370 train_time:110917ms step_avg:143.68ms
step:783/1370 train_time:111067ms step_avg:143.68ms
step:784/1370 train_time:111217ms step_avg:143.69ms
step:785/1370 train_time:111367ms step_avg:143.70ms
step:786/1370 train_time:111515ms step_avg:143.71ms
step:787/1370 train_time:111665ms step_avg:143.71ms
step:788/1370 train_time:111813ms step_avg:143.72ms
step:789/1370 train_time:111962ms step_avg:143.73ms
step:790/1370 train_time:112111ms step_avg:143.73ms
step:791/1370 train_time:112261ms step_avg:143.74ms
step:792/1370 train_time:112411ms step_avg:143.75ms
step:793/1370 train_time:112559ms step_avg:143.75ms
step:794/1370 train_time:112709ms step_avg:143.76ms
step:795/1370 train_time:112859ms step_avg:143.77ms
step:796/1370 train_time:113011ms step_avg:143.78ms
step:797/1370 train_time:113162ms step_avg:143.79ms
step:798/1370 train_time:113312ms step_avg:143.80ms
step:799/1370 train_time:113465ms step_avg:143.81ms
step:800/1370 train_time:113613ms step_avg:143.81ms
step:801/1370 train_time:113762ms step_avg:143.82ms
step:802/1370 train_time:113912ms step_avg:143.83ms
step:803/1370 train_time:114061ms step_avg:143.83ms
step:804/1370 train_time:114209ms step_avg:143.84ms
step:805/1370 train_time:114361ms step_avg:143.85ms
step:806/1370 train_time:114509ms step_avg:143.86ms
step:807/1370 train_time:114657ms step_avg:143.86ms
step:808/1370 train_time:114806ms step_avg:143.87ms
step:809/1370 train_time:114955ms step_avg:143.87ms
step:810/1370 train_time:115104ms step_avg:143.88ms
step:811/1370 train_time:115252ms step_avg:143.89ms
step:812/1370 train_time:115403ms step_avg:143.89ms
step:813/1370 train_time:115551ms step_avg:143.90ms
step:814/1370 train_time:115702ms step_avg:143.91ms
step:815/1370 train_time:115851ms step_avg:143.91ms
step:816/1370 train_time:116004ms step_avg:143.93ms
step:817/1370 train_time:116153ms step_avg:143.93ms
step:818/1370 train_time:116303ms step_avg:143.94ms
step:819/1370 train_time:116455ms step_avg:143.95ms
step:820/1370 train_time:116606ms step_avg:143.96ms
step:821/1370 train_time:116755ms step_avg:143.96ms
step:822/1370 train_time:116905ms step_avg:143.97ms
step:823/1370 train_time:117055ms step_avg:143.98ms
step:824/1370 train_time:117206ms step_avg:143.99ms
step:825/1370 train_time:117357ms step_avg:144.00ms
step:826/1370 train_time:117510ms step_avg:144.01ms
step:827/1370 train_time:117662ms step_avg:144.02ms
step:828/1370 train_time:117812ms step_avg:144.03ms
step:829/1370 train_time:117964ms step_avg:144.03ms
step:830/1370 train_time:118116ms step_avg:144.04ms
step:831/1370 train_time:118267ms step_avg:144.05ms
step:832/1370 train_time:118417ms step_avg:144.06ms
step:833/1370 train_time:118567ms step_avg:144.07ms
step:834/1370 train_time:118719ms step_avg:144.08ms
step:835/1370 train_time:118869ms step_avg:144.08ms
step:836/1370 train_time:119021ms step_avg:144.09ms
step:837/1370 train_time:119171ms step_avg:144.10ms
step:838/1370 train_time:119321ms step_avg:144.11ms
step:839/1370 train_time:119471ms step_avg:144.11ms
step:840/1370 train_time:119621ms step_avg:144.12ms
step:841/1370 train_time:119771ms step_avg:144.13ms
step:842/1370 train_time:119921ms step_avg:144.14ms
step:843/1370 train_time:120071ms step_avg:144.14ms
step:844/1370 train_time:120221ms step_avg:144.15ms
step:845/1370 train_time:120371ms step_avg:144.16ms
step:846/1370 train_time:120522ms step_avg:144.17ms
step:847/1370 train_time:120673ms step_avg:144.17ms
step:848/1370 train_time:120823ms step_avg:144.18ms
step:849/1370 train_time:120974ms step_avg:144.19ms
step:850/1370 train_time:121125ms step_avg:144.20ms
step:851/1370 train_time:121276ms step_avg:144.21ms
step:852/1370 train_time:121428ms step_avg:144.21ms
step:853/1370 train_time:121576ms step_avg:144.22ms
step:854/1370 train_time:121727ms step_avg:144.23ms
step:855/1370 train_time:121877ms step_avg:144.23ms
step:856/1370 train_time:122029ms step_avg:144.24ms
step:857/1370 train_time:122179ms step_avg:144.25ms
step:858/1370 train_time:122331ms step_avg:144.26ms
step:859/1370 train_time:122482ms step_avg:144.27ms
step:860/1370 train_time:122632ms step_avg:144.27ms
step:861/1370 train_time:122783ms step_avg:144.28ms
step:862/1370 train_time:122935ms step_avg:144.29ms
step:863/1370 train_time:123088ms step_avg:144.30ms
step:864/1370 train_time:123238ms step_avg:144.31ms
step:865/1370 train_time:123388ms step_avg:144.31ms
step:866/1370 train_time:123545ms step_avg:144.33ms
step:867/1370 train_time:123697ms step_avg:144.34ms
step:868/1370 train_time:123847ms step_avg:144.34ms
step:869/1370 train_time:123996ms step_avg:144.35ms
step:870/1370 train_time:124151ms step_avg:144.36ms
step:871/1370 train_time:124300ms step_avg:144.37ms
step:872/1370 train_time:124450ms step_avg:144.37ms
step:873/1370 train_time:124600ms step_avg:144.38ms
step:874/1370 train_time:124752ms step_avg:144.39ms
step:875/1370 train_time:124904ms step_avg:144.40ms
_orig_mod.blocks.0.attn.attn_scale: 0.13714797794818878
_orig_mod.blocks.1.attn.attn_scale: 0.17006681859493256
_orig_mod.blocks.2.attn.attn_scale: 0.17124207317829132
_orig_mod.blocks.3.attn.attn_scale: 0.173948273062706
_orig_mod.blocks.4.attn.attn_scale: 0.17959368228912354
_orig_mod.blocks.5.attn.attn_scale: 0.16841953992843628
_orig_mod.blocks.6.attn.attn_scale: 0.169583261013031
_orig_mod.blocks.8.attn.attn_scale: 0.1558426171541214
_orig_mod.blocks.9.attn.attn_scale: 0.14816637337207794
_orig_mod.blocks.10.attn.attn_scale: 0.16131283342838287
_orig_mod.blocks.11.attn.attn_scale: 0.1342315375804901
step:875/1370 val_loss:3.4667 train_time:124974ms step_avg:144.48ms
step:876/1370 train_time:125057ms step_avg:144.41ms
step:877/1370 train_time:125206ms step_avg:144.41ms
step:878/1370 train_time:125356ms step_avg:144.42ms
step:879/1370 train_time:125506ms step_avg:144.43ms
step:880/1370 train_time:125656ms step_avg:144.43ms
step:881/1370 train_time:125806ms step_avg:144.44ms
step:882/1370 train_time:125957ms step_avg:144.45ms
step:883/1370 train_time:126109ms step_avg:144.45ms
step:884/1370 train_time:126260ms step_avg:144.46ms
step:885/1370 train_time:126410ms step_avg:144.47ms
step:886/1370 train_time:126562ms step_avg:144.48ms
step:887/1370 train_time:126712ms step_avg:144.48ms
step:888/1370 train_time:126864ms step_avg:144.49ms
step:889/1370 train_time:127017ms step_avg:144.50ms
step:890/1370 train_time:127165ms step_avg:144.51ms
step:891/1370 train_time:127317ms step_avg:144.51ms
step:892/1370 train_time:127467ms step_avg:144.52ms
step:893/1370 train_time:127617ms step_avg:144.53ms
step:894/1370 train_time:127768ms step_avg:144.53ms
step:895/1370 train_time:127919ms step_avg:144.54ms
step:896/1370 train_time:128069ms step_avg:144.55ms
step:897/1370 train_time:128220ms step_avg:144.55ms
step:898/1370 train_time:128371ms step_avg:144.56ms
step:899/1370 train_time:128523ms step_avg:144.57ms
step:900/1370 train_time:128672ms step_avg:144.58ms
step:901/1370 train_time:128821ms step_avg:144.58ms
step:902/1370 train_time:128969ms step_avg:144.58ms
step:903/1370 train_time:129120ms step_avg:144.59ms
step:904/1370 train_time:129271ms step_avg:144.60ms
step:905/1370 train_time:129420ms step_avg:144.60ms
step:906/1370 train_time:129572ms step_avg:144.61ms
step:907/1370 train_time:129725ms step_avg:144.62ms
step:908/1370 train_time:129875ms step_avg:144.63ms
step:909/1370 train_time:130024ms step_avg:144.63ms
step:910/1370 train_time:130179ms step_avg:144.64ms
step:911/1370 train_time:130329ms step_avg:144.65ms
step:912/1370 train_time:130479ms step_avg:144.66ms
step:913/1370 train_time:130631ms step_avg:144.66ms
step:914/1370 train_time:130782ms step_avg:144.67ms
step:915/1370 train_time:130935ms step_avg:144.68ms
step:916/1370 train_time:131086ms step_avg:144.69ms
step:917/1370 train_time:131237ms step_avg:144.69ms
step:918/1370 train_time:131391ms step_avg:144.70ms
step:919/1370 train_time:131548ms step_avg:144.72ms
step:920/1370 train_time:131700ms step_avg:144.73ms
step:921/1370 train_time:131852ms step_avg:144.73ms
step:922/1370 train_time:132006ms step_avg:144.74ms
step:923/1370 train_time:132155ms step_avg:144.75ms
step:924/1370 train_time:132308ms step_avg:144.76ms
step:925/1370 train_time:132461ms step_avg:144.77ms
step:926/1370 train_time:132613ms step_avg:144.77ms
step:927/1370 train_time:132763ms step_avg:144.78ms
step:928/1370 train_time:132915ms step_avg:144.79ms
step:929/1370 train_time:133068ms step_avg:144.80ms
step:930/1370 train_time:133219ms step_avg:144.80ms
step:931/1370 train_time:133370ms step_avg:144.81ms
step:932/1370 train_time:133521ms step_avg:144.82ms
step:933/1370 train_time:133675ms step_avg:144.83ms
step:934/1370 train_time:133825ms step_avg:144.83ms
step:935/1370 train_time:133977ms step_avg:144.84ms
step:936/1370 train_time:134130ms step_avg:144.85ms
step:937/1370 train_time:134282ms step_avg:144.86ms
step:938/1370 train_time:134436ms step_avg:144.87ms
step:939/1370 train_time:134589ms step_avg:144.88ms
step:940/1370 train_time:134744ms step_avg:144.89ms
step:941/1370 train_time:134895ms step_avg:144.89ms
step:942/1370 train_time:135045ms step_avg:144.90ms
step:943/1370 train_time:135199ms step_avg:144.91ms
step:944/1370 train_time:135353ms step_avg:144.92ms
step:945/1370 train_time:135503ms step_avg:144.92ms
step:946/1370 train_time:135658ms step_avg:144.93ms
step:947/1370 train_time:135811ms step_avg:144.94ms
step:948/1370 train_time:135963ms step_avg:144.95ms
step:949/1370 train_time:136116ms step_avg:144.96ms
step:950/1370 train_time:136268ms step_avg:144.97ms
step:951/1370 train_time:136457ms step_avg:145.01ms
step:952/1370 train_time:136607ms step_avg:145.02ms
step:953/1370 train_time:136758ms step_avg:145.02ms
step:954/1370 train_time:136909ms step_avg:145.03ms
step:955/1370 train_time:137059ms step_avg:145.04ms
step:956/1370 train_time:137212ms step_avg:145.04ms
step:957/1370 train_time:137364ms step_avg:145.05ms
step:958/1370 train_time:137519ms step_avg:145.06ms
step:959/1370 train_time:137675ms step_avg:145.07ms
step:960/1370 train_time:137826ms step_avg:145.08ms
step:961/1370 train_time:137978ms step_avg:145.09ms
step:962/1370 train_time:138129ms step_avg:145.09ms
step:963/1370 train_time:138284ms step_avg:145.10ms
step:964/1370 train_time:138439ms step_avg:145.11ms
step:965/1370 train_time:138592ms step_avg:145.12ms
step:966/1370 train_time:138742ms step_avg:145.13ms
step:967/1370 train_time:138896ms step_avg:145.14ms
step:968/1370 train_time:139044ms step_avg:145.14ms
step:969/1370 train_time:139199ms step_avg:145.15ms
step:970/1370 train_time:139351ms step_avg:145.16ms
step:971/1370 train_time:139502ms step_avg:145.16ms
step:972/1370 train_time:139654ms step_avg:145.17ms
step:973/1370 train_time:139805ms step_avg:145.18ms
step:974/1370 train_time:139957ms step_avg:145.18ms
step:975/1370 train_time:140109ms step_avg:145.19ms
step:976/1370 train_time:140261ms step_avg:145.20ms
step:977/1370 train_time:140411ms step_avg:145.20ms
step:978/1370 train_time:140561ms step_avg:145.21ms
step:979/1370 train_time:140713ms step_avg:145.22ms
step:980/1370 train_time:140865ms step_avg:145.22ms
step:981/1370 train_time:141016ms step_avg:145.23ms
step:982/1370 train_time:141166ms step_avg:145.23ms
step:983/1370 train_time:141317ms step_avg:145.24ms
step:984/1370 train_time:141469ms step_avg:145.25ms
step:985/1370 train_time:141621ms step_avg:145.25ms
step:986/1370 train_time:141777ms step_avg:145.26ms
step:987/1370 train_time:141928ms step_avg:145.27ms
step:988/1370 train_time:142079ms step_avg:145.28ms
step:989/1370 train_time:142232ms step_avg:145.28ms
step:990/1370 train_time:142385ms step_avg:145.29ms
step:991/1370 train_time:142537ms step_avg:145.30ms
step:992/1370 train_time:142693ms step_avg:145.31ms
step:993/1370 train_time:142850ms step_avg:145.32ms
step:994/1370 train_time:143001ms step_avg:145.33ms
step:995/1370 train_time:143152ms step_avg:145.33ms
step:996/1370 train_time:143301ms step_avg:145.34ms
step:997/1370 train_time:143452ms step_avg:145.34ms
step:998/1370 train_time:143601ms step_avg:145.34ms
step:999/1370 train_time:143753ms step_avg:145.35ms
step:1000/1370 train_time:143903ms step_avg:145.36ms
_orig_mod.blocks.0.attn.attn_scale: 0.1398625522851944
_orig_mod.blocks.1.attn.attn_scale: 0.1675691157579422
_orig_mod.blocks.2.attn.attn_scale: 0.17185701429843903
_orig_mod.blocks.3.attn.attn_scale: 0.16902683675289154
_orig_mod.blocks.4.attn.attn_scale: 0.1794145703315735
_orig_mod.blocks.5.attn.attn_scale: 0.16754265129566193
_orig_mod.blocks.6.attn.attn_scale: 0.16741034388542175
_orig_mod.blocks.8.attn.attn_scale: 0.15660402178764343
_orig_mod.blocks.9.attn.attn_scale: 0.15420332551002502
_orig_mod.blocks.10.attn.attn_scale: 0.16751345992088318
_orig_mod.blocks.11.attn.attn_scale: 0.1369180828332901
step:1000/1370 val_loss:3.4003 train_time:143971ms step_avg:145.43ms
step:1001/1370 train_time:144058ms step_avg:145.37ms
step:1002/1370 train_time:144208ms step_avg:145.37ms
step:1003/1370 train_time:144362ms step_avg:145.38ms
step:1004/1370 train_time:144514ms step_avg:145.39ms
step:1005/1370 train_time:144665ms step_avg:145.39ms
step:1006/1370 train_time:144814ms step_avg:145.40ms
step:1007/1370 train_time:144966ms step_avg:145.40ms
step:1008/1370 train_time:145121ms step_avg:145.41ms
step:1009/1370 train_time:145276ms step_avg:145.42ms
step:1010/1370 train_time:145428ms step_avg:145.43ms
step:1011/1370 train_time:145580ms step_avg:145.43ms
step:1012/1370 train_time:145730ms step_avg:145.44ms
step:1013/1370 train_time:145883ms step_avg:145.45ms
step:1014/1370 train_time:146033ms step_avg:145.45ms
step:1015/1370 train_time:146186ms step_avg:145.46ms
step:1016/1370 train_time:146338ms step_avg:145.47ms
step:1017/1370 train_time:146491ms step_avg:145.47ms
step:1018/1370 train_time:146644ms step_avg:145.48ms
step:1019/1370 train_time:146798ms step_avg:145.49ms
step:1020/1370 train_time:146954ms step_avg:145.50ms
step:1021/1370 train_time:147105ms step_avg:145.50ms
step:1022/1370 train_time:147257ms step_avg:145.51ms
step:1023/1370 train_time:147412ms step_avg:145.52ms
step:1024/1370 train_time:147568ms step_avg:145.53ms
step:1025/1370 train_time:147721ms step_avg:145.54ms
step:1026/1370 train_time:147873ms step_avg:145.54ms
step:1027/1370 train_time:148026ms step_avg:145.55ms
step:1028/1370 train_time:148180ms step_avg:145.56ms
step:1029/1370 train_time:148336ms step_avg:145.57ms
step:1030/1370 train_time:148490ms step_avg:145.58ms
step:1031/1370 train_time:148641ms step_avg:145.58ms
step:1032/1370 train_time:148793ms step_avg:145.59ms
step:1033/1370 train_time:148945ms step_avg:145.60ms
step:1034/1370 train_time:149098ms step_avg:145.60ms
step:1035/1370 train_time:149252ms step_avg:145.61ms
step:1036/1370 train_time:149405ms step_avg:145.62ms
step:1037/1370 train_time:149561ms step_avg:145.63ms
step:1038/1370 train_time:149713ms step_avg:145.64ms
step:1039/1370 train_time:149866ms step_avg:145.64ms
step:1040/1370 train_time:150018ms step_avg:145.65ms
step:1041/1370 train_time:150170ms step_avg:145.65ms
step:1042/1370 train_time:150322ms step_avg:145.66ms
step:1043/1370 train_time:150477ms step_avg:145.67ms
step:1044/1370 train_time:150633ms step_avg:145.68ms
step:1045/1370 train_time:150787ms step_avg:145.69ms
step:1046/1370 train_time:150938ms step_avg:145.69ms
step:1047/1370 train_time:151090ms step_avg:145.70ms
step:1048/1370 train_time:151244ms step_avg:145.71ms
step:1049/1370 train_time:151397ms step_avg:145.71ms
step:1050/1370 train_time:151550ms step_avg:145.72ms
step:1051/1370 train_time:151704ms step_avg:145.73ms
step:1052/1370 train_time:151856ms step_avg:145.74ms
step:1053/1370 train_time:152008ms step_avg:145.74ms
step:1054/1370 train_time:152163ms step_avg:145.75ms
step:1055/1370 train_time:152316ms step_avg:145.76ms
step:1056/1370 train_time:152468ms step_avg:145.76ms
step:1057/1370 train_time:152622ms step_avg:145.77ms
step:1058/1370 train_time:152777ms step_avg:145.78ms
step:1059/1370 train_time:152932ms step_avg:145.79ms
step:1060/1370 train_time:153086ms step_avg:145.80ms
step:1061/1370 train_time:153237ms step_avg:145.80ms
step:1062/1370 train_time:153391ms step_avg:145.81ms
step:1063/1370 train_time:153545ms step_avg:145.82ms
step:1064/1370 train_time:153696ms step_avg:145.82ms
step:1065/1370 train_time:153850ms step_avg:145.83ms
step:1066/1370 train_time:154006ms step_avg:145.84ms
step:1067/1370 train_time:154160ms step_avg:145.85ms
step:1068/1370 train_time:154314ms step_avg:145.85ms
step:1069/1370 train_time:154469ms step_avg:145.86ms
step:1070/1370 train_time:154621ms step_avg:145.87ms
step:1071/1370 train_time:154778ms step_avg:145.88ms
step:1072/1370 train_time:154930ms step_avg:145.88ms
step:1073/1370 train_time:155081ms step_avg:145.89ms
step:1074/1370 train_time:155232ms step_avg:145.89ms
step:1075/1370 train_time:155387ms step_avg:145.90ms
step:1076/1370 train_time:155540ms step_avg:145.91ms
step:1077/1370 train_time:155690ms step_avg:145.91ms
step:1078/1370 train_time:155848ms step_avg:145.93ms
step:1079/1370 train_time:156005ms step_avg:145.94ms
step:1080/1370 train_time:156158ms step_avg:145.94ms
step:1081/1370 train_time:156310ms step_avg:145.95ms
step:1082/1370 train_time:156464ms step_avg:145.96ms
step:1083/1370 train_time:156617ms step_avg:145.96ms
step:1084/1370 train_time:156772ms step_avg:145.97ms
step:1085/1370 train_time:156925ms step_avg:145.98ms
step:1086/1370 train_time:157077ms step_avg:145.98ms
step:1087/1370 train_time:157229ms step_avg:145.99ms
step:1088/1370 train_time:157384ms step_avg:146.00ms
step:1089/1370 train_time:157541ms step_avg:146.01ms
step:1090/1370 train_time:157697ms step_avg:146.02ms
step:1091/1370 train_time:157849ms step_avg:146.02ms
step:1092/1370 train_time:158001ms step_avg:146.03ms
step:1093/1370 train_time:158153ms step_avg:146.03ms
step:1094/1370 train_time:158306ms step_avg:146.04ms
step:1095/1370 train_time:158459ms step_avg:146.05ms
step:1096/1370 train_time:158615ms step_avg:146.05ms
step:1097/1370 train_time:158769ms step_avg:146.06ms
step:1098/1370 train_time:158924ms step_avg:146.07ms
step:1099/1370 train_time:159076ms step_avg:146.07ms
step:1100/1370 train_time:159227ms step_avg:146.08ms
step:1101/1370 train_time:159380ms step_avg:146.09ms
step:1102/1370 train_time:159534ms step_avg:146.09ms
step:1103/1370 train_time:159687ms step_avg:146.10ms
step:1104/1370 train_time:159840ms step_avg:146.11ms
step:1105/1370 train_time:159996ms step_avg:146.12ms
step:1106/1370 train_time:160150ms step_avg:146.12ms
step:1107/1370 train_time:160305ms step_avg:146.13ms
step:1108/1370 train_time:160461ms step_avg:146.14ms
step:1109/1370 train_time:160614ms step_avg:146.15ms
step:1110/1370 train_time:160766ms step_avg:146.15ms
step:1111/1370 train_time:160920ms step_avg:146.16ms
step:1112/1370 train_time:161073ms step_avg:146.16ms
step:1113/1370 train_time:161224ms step_avg:146.17ms
step:1114/1370 train_time:161378ms step_avg:146.18ms
step:1115/1370 train_time:161532ms step_avg:146.18ms
step:1116/1370 train_time:161685ms step_avg:146.19ms
step:1117/1370 train_time:161840ms step_avg:146.20ms
step:1118/1370 train_time:161997ms step_avg:146.21ms
step:1119/1370 train_time:162149ms step_avg:146.21ms
step:1120/1370 train_time:162303ms step_avg:146.22ms
step:1121/1370 train_time:162454ms step_avg:146.22ms
step:1122/1370 train_time:162607ms step_avg:146.23ms
step:1123/1370 train_time:162761ms step_avg:146.24ms
step:1124/1370 train_time:162917ms step_avg:146.24ms
step:1125/1370 train_time:163069ms step_avg:146.25ms
_orig_mod.blocks.0.attn.attn_scale: 0.1434386819601059
_orig_mod.blocks.1.attn.attn_scale: 0.1691802740097046
_orig_mod.blocks.2.attn.attn_scale: 0.17452523112297058
_orig_mod.blocks.3.attn.attn_scale: 0.17249695956707
_orig_mod.blocks.4.attn.attn_scale: 0.17846889793872833
_orig_mod.blocks.5.attn.attn_scale: 0.16803497076034546
_orig_mod.blocks.6.attn.attn_scale: 0.16647207736968994
_orig_mod.blocks.8.attn.attn_scale: 0.15661472082138062
_orig_mod.blocks.9.attn.attn_scale: 0.15890060365200043
_orig_mod.blocks.10.attn.attn_scale: 0.1714271903038025
_orig_mod.blocks.11.attn.attn_scale: 0.14287430047988892
step:1125/1370 val_loss:3.3467 train_time:163142ms step_avg:146.32ms
step:1126/1370 train_time:163226ms step_avg:146.26ms
step:1127/1370 train_time:163378ms step_avg:146.27ms
step:1128/1370 train_time:163533ms step_avg:146.27ms
step:1129/1370 train_time:163689ms step_avg:146.28ms
step:1130/1370 train_time:163840ms step_avg:146.29ms
step:1131/1370 train_time:163997ms step_avg:146.29ms
step:1132/1370 train_time:164150ms step_avg:146.30ms
step:1133/1370 train_time:164305ms step_avg:146.31ms
step:1134/1370 train_time:164462ms step_avg:146.32ms
step:1135/1370 train_time:164617ms step_avg:146.33ms
step:1136/1370 train_time:164775ms step_avg:146.34ms
step:1137/1370 train_time:164928ms step_avg:146.34ms
step:1138/1370 train_time:165085ms step_avg:146.35ms
step:1139/1370 train_time:165239ms step_avg:146.36ms
step:1140/1370 train_time:165394ms step_avg:146.37ms
step:1141/1370 train_time:165590ms step_avg:146.41ms
step:1142/1370 train_time:165745ms step_avg:146.42ms
step:1143/1370 train_time:165901ms step_avg:146.43ms
step:1144/1370 train_time:166055ms step_avg:146.43ms
step:1145/1370 train_time:166207ms step_avg:146.44ms
step:1146/1370 train_time:166363ms step_avg:146.45ms
step:1147/1370 train_time:166518ms step_avg:146.45ms
step:1148/1370 train_time:166671ms step_avg:146.46ms
step:1149/1370 train_time:166827ms step_avg:146.47ms
step:1150/1370 train_time:166981ms step_avg:146.47ms
step:1151/1370 train_time:167137ms step_avg:146.48ms
step:1152/1370 train_time:167292ms step_avg:146.49ms
step:1153/1370 train_time:167448ms step_avg:146.50ms
step:1154/1370 train_time:167601ms step_avg:146.50ms
step:1155/1370 train_time:167756ms step_avg:146.51ms
step:1156/1370 train_time:167915ms step_avg:146.52ms
step:1157/1370 train_time:168071ms step_avg:146.53ms
step:1158/1370 train_time:168224ms step_avg:146.54ms
step:1159/1370 train_time:168377ms step_avg:146.54ms
step:1160/1370 train_time:168530ms step_avg:146.55ms
step:1161/1370 train_time:168685ms step_avg:146.56ms
step:1162/1370 train_time:168840ms step_avg:146.56ms
step:1163/1370 train_time:168995ms step_avg:146.57ms
step:1164/1370 train_time:169149ms step_avg:146.58ms
step:1165/1370 train_time:169301ms step_avg:146.58ms
step:1166/1370 train_time:169456ms step_avg:146.59ms
step:1167/1370 train_time:169608ms step_avg:146.59ms
step:1168/1370 train_time:169763ms step_avg:146.60ms
step:1169/1370 train_time:169919ms step_avg:146.61ms
step:1170/1370 train_time:170072ms step_avg:146.61ms
step:1171/1370 train_time:170227ms step_avg:146.62ms
step:1172/1370 train_time:170382ms step_avg:146.63ms
step:1173/1370 train_time:170538ms step_avg:146.64ms
step:1174/1370 train_time:170699ms step_avg:146.65ms
step:1175/1370 train_time:170854ms step_avg:146.66ms
step:1176/1370 train_time:171011ms step_avg:146.66ms
step:1177/1370 train_time:171170ms step_avg:146.68ms
step:1178/1370 train_time:171323ms step_avg:146.68ms
step:1179/1370 train_time:171476ms step_avg:146.69ms
step:1180/1370 train_time:171636ms step_avg:146.70ms
step:1181/1370 train_time:171791ms step_avg:146.70ms
step:1182/1370 train_time:171943ms step_avg:146.71ms
step:1183/1370 train_time:172098ms step_avg:146.72ms
step:1184/1370 train_time:172252ms step_avg:146.72ms
step:1185/1370 train_time:172408ms step_avg:146.73ms
step:1186/1370 train_time:172564ms step_avg:146.74ms
step:1187/1370 train_time:172730ms step_avg:146.75ms
step:1188/1370 train_time:172881ms step_avg:146.76ms
step:1189/1370 train_time:173037ms step_avg:146.77ms
step:1190/1370 train_time:173191ms step_avg:146.77ms
step:1191/1370 train_time:173346ms step_avg:146.78ms
step:1192/1370 train_time:173499ms step_avg:146.78ms
step:1193/1370 train_time:173654ms step_avg:146.79ms
step:1194/1370 train_time:173807ms step_avg:146.80ms
step:1195/1370 train_time:173960ms step_avg:146.80ms
step:1196/1370 train_time:174115ms step_avg:146.81ms
step:1197/1370 train_time:174271ms step_avg:146.82ms
step:1198/1370 train_time:174430ms step_avg:146.83ms
step:1199/1370 train_time:174585ms step_avg:146.83ms
step:1200/1370 train_time:174737ms step_avg:146.84ms
step:1201/1370 train_time:174892ms step_avg:146.84ms
step:1202/1370 train_time:175059ms step_avg:146.86ms
step:1203/1370 train_time:175218ms step_avg:146.87ms
step:1204/1370 train_time:175374ms step_avg:146.88ms
step:1205/1370 train_time:175529ms step_avg:146.89ms
step:1206/1370 train_time:175684ms step_avg:146.89ms
step:1207/1370 train_time:175837ms step_avg:146.90ms
step:1208/1370 train_time:175994ms step_avg:146.91ms
step:1209/1370 train_time:176150ms step_avg:146.91ms
step:1210/1370 train_time:176305ms step_avg:146.92ms
step:1211/1370 train_time:176460ms step_avg:146.93ms
step:1212/1370 train_time:176615ms step_avg:146.93ms
step:1213/1370 train_time:176769ms step_avg:146.94ms
step:1214/1370 train_time:176928ms step_avg:146.95ms
step:1215/1370 train_time:177082ms step_avg:146.96ms
step:1216/1370 train_time:177236ms step_avg:146.96ms
step:1217/1370 train_time:177390ms step_avg:146.97ms
step:1218/1370 train_time:177540ms step_avg:146.97ms
step:1219/1370 train_time:177692ms step_avg:146.97ms
step:1220/1370 train_time:177847ms step_avg:146.98ms
step:1221/1370 train_time:178000ms step_avg:146.99ms
step:1222/1370 train_time:178155ms step_avg:146.99ms
step:1223/1370 train_time:178310ms step_avg:147.00ms
step:1224/1370 train_time:178468ms step_avg:147.01ms
step:1225/1370 train_time:178627ms step_avg:147.02ms
step:1226/1370 train_time:178784ms step_avg:147.03ms
step:1227/1370 train_time:178939ms step_avg:147.03ms
step:1228/1370 train_time:179095ms step_avg:147.04ms
step:1229/1370 train_time:179248ms step_avg:147.05ms
step:1230/1370 train_time:179409ms step_avg:147.06ms
step:1231/1370 train_time:179568ms step_avg:147.07ms
step:1232/1370 train_time:179725ms step_avg:147.07ms
step:1233/1370 train_time:179879ms step_avg:147.08ms
step:1234/1370 train_time:180032ms step_avg:147.09ms
step:1235/1370 train_time:180185ms step_avg:147.09ms
step:1236/1370 train_time:180341ms step_avg:147.10ms
step:1237/1370 train_time:180495ms step_avg:147.10ms
step:1238/1370 train_time:180659ms step_avg:147.12ms
step:1239/1370 train_time:180813ms step_avg:147.12ms
step:1240/1370 train_time:180970ms step_avg:147.13ms
step:1241/1370 train_time:181128ms step_avg:147.14ms
step:1242/1370 train_time:181282ms step_avg:147.14ms
step:1243/1370 train_time:181438ms step_avg:147.15ms
step:1244/1370 train_time:181592ms step_avg:147.16ms
step:1245/1370 train_time:181746ms step_avg:147.16ms
step:1246/1370 train_time:181900ms step_avg:147.17ms
step:1247/1370 train_time:182055ms step_avg:147.17ms
step:1248/1370 train_time:182208ms step_avg:147.18ms
step:1249/1370 train_time:182360ms step_avg:147.18ms
step:1250/1370 train_time:182515ms step_avg:147.19ms
_orig_mod.blocks.0.attn.attn_scale: 0.14646564424037933
_orig_mod.blocks.1.attn.attn_scale: 0.16916057467460632
_orig_mod.blocks.2.attn.attn_scale: 0.17300447821617126
_orig_mod.blocks.3.attn.attn_scale: 0.17101074755191803
_orig_mod.blocks.4.attn.attn_scale: 0.1778104305267334
_orig_mod.blocks.5.attn.attn_scale: 0.1668989360332489
_orig_mod.blocks.6.attn.attn_scale: 0.16693855822086334
_orig_mod.blocks.8.attn.attn_scale: 0.15606951713562012
_orig_mod.blocks.9.attn.attn_scale: 0.16166910529136658
_orig_mod.blocks.10.attn.attn_scale: 0.17507414519786835
_orig_mod.blocks.11.attn.attn_scale: 0.14317873120307922
step:1250/1370 val_loss:3.3015 train_time:182587ms step_avg:147.25ms
step:1251/1370 train_time:182675ms step_avg:147.20ms
step:1252/1370 train_time:182831ms step_avg:147.21ms
step:1253/1370 train_time:182984ms step_avg:147.21ms
step:1254/1370 train_time:183136ms step_avg:147.22ms
step:1255/1370 train_time:183300ms step_avg:147.23ms
step:1256/1370 train_time:183454ms step_avg:147.23ms
step:1257/1370 train_time:183608ms step_avg:147.24ms
step:1258/1370 train_time:183765ms step_avg:147.25ms
step:1259/1370 train_time:183921ms step_avg:147.25ms
step:1260/1370 train_time:184073ms step_avg:147.26ms
step:1261/1370 train_time:184227ms step_avg:147.26ms
step:1262/1370 train_time:184384ms step_avg:147.27ms
step:1263/1370 train_time:184539ms step_avg:147.28ms
step:1264/1370 train_time:184692ms step_avg:147.28ms
step:1265/1370 train_time:184847ms step_avg:147.29ms
step:1266/1370 train_time:185002ms step_avg:147.29ms
step:1267/1370 train_time:185155ms step_avg:147.30ms
step:1268/1370 train_time:185311ms step_avg:147.31ms
step:1269/1370 train_time:185472ms step_avg:147.32ms
step:1270/1370 train_time:185627ms step_avg:147.32ms
step:1271/1370 train_time:185782ms step_avg:147.33ms
step:1272/1370 train_time:185934ms step_avg:147.33ms
step:1273/1370 train_time:186087ms step_avg:147.34ms
step:1274/1370 train_time:186242ms step_avg:147.34ms
step:1275/1370 train_time:186399ms step_avg:147.35ms
step:1276/1370 train_time:186551ms step_avg:147.35ms
step:1277/1370 train_time:186707ms step_avg:147.36ms
step:1278/1370 train_time:186861ms step_avg:147.37ms
step:1279/1370 train_time:187018ms step_avg:147.37ms
step:1280/1370 train_time:187177ms step_avg:147.38ms
step:1281/1370 train_time:187331ms step_avg:147.39ms
step:1282/1370 train_time:187484ms step_avg:147.39ms
step:1283/1370 train_time:187639ms step_avg:147.40ms
step:1284/1370 train_time:187793ms step_avg:147.40ms
step:1285/1370 train_time:187948ms step_avg:147.41ms
step:1286/1370 train_time:188101ms step_avg:147.41ms
step:1287/1370 train_time:188255ms step_avg:147.42ms
step:1288/1370 train_time:188410ms step_avg:147.43ms
step:1289/1370 train_time:188571ms step_avg:147.44ms
step:1290/1370 train_time:188732ms step_avg:147.45ms
step:1291/1370 train_time:188888ms step_avg:147.45ms
step:1292/1370 train_time:189044ms step_avg:147.46ms
step:1293/1370 train_time:189203ms step_avg:147.47ms
step:1294/1370 train_time:189358ms step_avg:147.47ms
step:1295/1370 train_time:189516ms step_avg:147.48ms
step:1296/1370 train_time:189670ms step_avg:147.49ms
step:1297/1370 train_time:189825ms step_avg:147.49ms
step:1298/1370 train_time:189980ms step_avg:147.50ms
step:1299/1370 train_time:190136ms step_avg:147.51ms
step:1300/1370 train_time:190287ms step_avg:147.51ms
step:1301/1370 train_time:190443ms step_avg:147.52ms
step:1302/1370 train_time:190599ms step_avg:147.52ms
step:1303/1370 train_time:190758ms step_avg:147.53ms
step:1304/1370 train_time:190915ms step_avg:147.54ms
step:1305/1370 train_time:191070ms step_avg:147.54ms
step:1306/1370 train_time:191226ms step_avg:147.55ms
step:1307/1370 train_time:191381ms step_avg:147.56ms
step:1308/1370 train_time:191540ms step_avg:147.57ms
step:1309/1370 train_time:191697ms step_avg:147.57ms
step:1310/1370 train_time:191854ms step_avg:147.58ms
step:1311/1370 train_time:192007ms step_avg:147.58ms
step:1312/1370 train_time:192160ms step_avg:147.59ms
step:1313/1370 train_time:192314ms step_avg:147.59ms
step:1314/1370 train_time:192468ms step_avg:147.60ms
step:1315/1370 train_time:192624ms step_avg:147.60ms
step:1316/1370 train_time:192777ms step_avg:147.61ms
step:1317/1370 train_time:192930ms step_avg:147.61ms
step:1318/1370 train_time:193092ms step_avg:147.62ms
step:1319/1370 train_time:193249ms step_avg:147.63ms
step:1320/1370 train_time:193403ms step_avg:147.64ms
step:1321/1370 train_time:193558ms step_avg:147.64ms
step:1322/1370 train_time:193720ms step_avg:147.65ms
step:1323/1370 train_time:193876ms step_avg:147.66ms
step:1324/1370 train_time:194030ms step_avg:147.66ms
step:1325/1370 train_time:194188ms step_avg:147.67ms
step:1326/1370 train_time:194349ms step_avg:147.68ms
step:1327/1370 train_time:194504ms step_avg:147.69ms
step:1328/1370 train_time:194657ms step_avg:147.69ms
step:1329/1370 train_time:194829ms step_avg:147.71ms
step:1330/1370 train_time:194989ms step_avg:147.72ms
step:1331/1370 train_time:195190ms step_avg:147.76ms
step:1332/1370 train_time:195355ms step_avg:147.77ms
step:1333/1370 train_time:195510ms step_avg:147.78ms
step:1334/1370 train_time:195665ms step_avg:147.78ms
step:1335/1370 train_time:195818ms step_avg:147.79ms
step:1336/1370 train_time:195980ms step_avg:147.80ms
step:1337/1370 train_time:196138ms step_avg:147.81ms
step:1338/1370 train_time:196294ms step_avg:147.81ms
step:1339/1370 train_time:196450ms step_avg:147.82ms
step:1340/1370 train_time:196611ms step_avg:147.83ms
step:1341/1370 train_time:196765ms step_avg:147.83ms
step:1342/1370 train_time:196921ms step_avg:147.84ms
step:1343/1370 train_time:197074ms step_avg:147.84ms
step:1344/1370 train_time:197228ms step_avg:147.85ms
step:1345/1370 train_time:197383ms step_avg:147.85ms
step:1346/1370 train_time:197540ms step_avg:147.86ms
step:1347/1370 train_time:197698ms step_avg:147.87ms
step:1348/1370 train_time:197851ms step_avg:147.87ms
step:1349/1370 train_time:198005ms step_avg:147.88ms
step:1350/1370 train_time:198158ms step_avg:147.88ms
step:1351/1370 train_time:198314ms step_avg:147.88ms
step:1352/1370 train_time:198476ms step_avg:147.90ms
step:1353/1370 train_time:198633ms step_avg:147.90ms
step:1354/1370 train_time:198788ms step_avg:147.91ms
step:1355/1370 train_time:198943ms step_avg:147.91ms
step:1356/1370 train_time:199097ms step_avg:147.92ms
step:1357/1370 train_time:199254ms step_avg:147.92ms
step:1358/1370 train_time:199410ms step_avg:147.93ms
step:1359/1370 train_time:199565ms step_avg:147.94ms
step:1360/1370 train_time:199723ms step_avg:147.94ms
step:1361/1370 train_time:199884ms step_avg:147.95ms
step:1362/1370 train_time:200042ms step_avg:147.96ms
step:1363/1370 train_time:200201ms step_avg:147.97ms
step:1364/1370 train_time:200357ms step_avg:147.97ms
step:1365/1370 train_time:200510ms step_avg:147.98ms
step:1366/1370 train_time:200665ms step_avg:147.98ms
step:1367/1370 train_time:200821ms step_avg:147.99ms
step:1368/1370 train_time:200976ms step_avg:147.99ms
step:1369/1370 train_time:201140ms step_avg:148.01ms
step:1370/1370 train_time:201299ms step_avg:148.01ms
_orig_mod.blocks.0.attn.attn_scale: 0.1463543325662613
_orig_mod.blocks.1.attn.attn_scale: 0.16650094091892242
_orig_mod.blocks.2.attn.attn_scale: 0.174552321434021
_orig_mod.blocks.3.attn.attn_scale: 0.1710779219865799
_orig_mod.blocks.4.attn.attn_scale: 0.17667727172374725
_orig_mod.blocks.5.attn.attn_scale: 0.1661936640739441
_orig_mod.blocks.6.attn.attn_scale: 0.16601943969726562
_orig_mod.blocks.8.attn.attn_scale: 0.15610100328922272
_orig_mod.blocks.9.attn.attn_scale: 0.16336509585380554
_orig_mod.blocks.10.attn.attn_scale: 0.17628510296344757
_orig_mod.blocks.11.attn.attn_scale: 0.1447836011648178
step:1370/1370 val_loss:3.2772 train_time:201373ms step_avg:148.07ms
peak memory consumption: 32619 MiB

import os
import sys
with open(sys.argv[0]) as f:
    code = f.read() # read the code of this file ASAP, for logging
import uuid
import time
import glob
import subprocess
import contextlib
from dataclasses import dataclass

import torch
torch.empty(1, device='cuda', requires_grad=True).backward()
from torch import nn
import torch.nn.functional as F
import torch.distributed as dist
from torch.nn.parallel import DistributedDataParallel as DDP
# use of FlexAttention contributed by @KoszarskyB
from torch.nn.attention.flex_attention import BlockMask, flex_attention

# -----------------------------------------------------------------------------
# Muon optimizer

@torch.compile
def zeropower_via_newtonschulz5(G, steps):
    """
    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a
    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose
    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at
    zero even beyond the point where the iteration no longer converges all the way to one everywhere
    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T
    where S' is diagonal with S_{ii}' ~ Uniform(0.5, 1.5), which turns out not to hurt model
    performance at all relative to UV^T, where USV^T = G is the SVD.
    """
    assert len(G.shape) == 2
    a, b, c = (3.4445, -4.7750,  2.0315)
    X = G.bfloat16()
    if G.size(0) > G.size(1):
        X = X.T

    # Ensure spectral norm is at most 1
    X = X / (X.norm() + 1e-7)
    # Perform the NS iterations
    for _ in range(steps):
        A = X @ X.T
        B = b * A + c * A @ A # adapted from suggestion by @jxbz, @leloykun, and @YouJiacheng
        X = a * X + B @ X
    
    if G.size(0) > G.size(1):
        X = X.T
    return X

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Some warnings:
    - This optimizer assumes that all parameters passed in are 2D.
    - It should not be used for the embedding layer, the final fully connected layer, or any {0,1}-D
    parameters; those should all be optimized by a standard method (e.g., AdamW).
    - To use it with 4D convolutional filters, it works well to just flatten their last 3 dimensions.
    - We believe it is unlikely to work well for training with small batch size.
    - We believe it may not work well for finetuning pretrained models, but we haven't tested this.
    - We have not yet tried this optimizer for training scenarios larger than NanoGPT (124M).

    Arguments:
        lr: The learning rate used by the internal SGD.
        momentum: The momentum used by the internal SGD.
        nesterov: Whether to use Nesterov-style momentum in the internal SGD. (recommended)
        ns_steps: The number of Newton-Schulz iteration steps to use.
    """
    def __init__(self, params, lr=0.02, momentum=0.95, nesterov=True, ns_steps=5):
        self.world_size = int(os.environ['WORLD_SIZE'])
        self.rank = int(os.environ['RANK'])
        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, ns_steps=ns_steps)
        assert all(isinstance(p, torch.Tensor) for p in params)
        sizes = {p.numel() for p in params}
        param_groups = [dict(params=[p for p in params if p.numel() == size],
                             update_buffer=[torch.empty(size, device='cuda', dtype=torch.bfloat16) for _ in range(self.world_size)])
                        for size in sizes]
        super().__init__(param_groups, defaults)

    def step(self):

        for group in self.param_groups:

            lr = group['lr']
            momentum = group['momentum']
            nesterov = group['nesterov']
            ns_steps = group['ns_steps']
            update_buffers = group['update_buffer']
            # generate weight updates in distributed fashion
            params = group['params']
            handle = None
            params_world = None
            def update_prev():
                if params_world is None:
                    return
                assert handle is not None
                handle.wait()
                for p_world, g_world in zip(params_world, update_buffers):
                    p_world.data.add_(
                        g_world.view_as(p_world),
                        alpha=-lr * max(1, p_world.size(0) / p_world.size(1)) ** 0.5,
                    )
            for base_i in range(len(params))[::self.world_size]:
                if base_i + rank < len(params):
                    p = params[base_i + self.rank]
                    g = p.grad
                    assert g is not None
                    state = self.state[p]
                    if 'momentum_buffer' not in state:
                        state['momentum_buffer'] = torch.zeros_like(g)
                    buf = state['momentum_buffer']
                    buf.lerp_(g, 1 - momentum)
                    g = g.lerp_(buf, momentum) if nesterov else buf
                    g = zeropower_via_newtonschulz5(g, steps=ns_steps).flatten()
                else:
                    g = update_buffers[rank]
                update_prev() # async all_gather instead of sync all_reduce by @YouJiacheng
                handle = dist.all_gather(update_buffers, g, async_op=True)
                params_world = params[base_i : base_i + self.world_size]
            update_prev()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the GPT-2 model

def norm(x):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):

    def __init__(self, in_features, out_features):
        super().__init__(in_features, out_features, bias=False)

    def forward(self, x):
        return F.linear(x, self.weight.type_as(x))

class Rotary(nn.Module):

    def __init__(self, dim, max_seq_len=65536):
        super().__init__()
        # half-truncate RoPE by @YouJiacheng
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=dim//4, dtype=torch.float32)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(dim//4)])
        t = torch.arange(max_seq_len, dtype=torch.float32)
        theta = torch.einsum('i,j -> ij', t, angular_freq)
        self.cos = nn.Buffer(theta.cos(), persistent=False)
        self.sin = nn.Buffer(theta.sin(), persistent=False)

    def forward(self, x):
        cos, sin = self.cos[None, :x.size(-3), None, :], self.sin[None, :x.size(-3), None, :]
        x1, x2 = x.float().chunk(2, dim=-1)
        y1 = x1 * cos + x2 * sin
        y2 = x1 * (-sin) + x2 * cos
        return torch.cat((y1, y2), 3).type_as(x)

class CausalSelfAttention(nn.Module):

    def __init__(self, dim, num_heads):
        super().__init__()
        assert dim % num_heads == 0
        self.num_heads = num_heads
        self.c_q = CastedLinear(dim, dim)
        self.c_k = CastedLinear(dim, dim)
        self.c_v = CastedLinear(dim, dim)
        self.lambdas = nn.Parameter(torch.tensor([0.5, 0.5]))
        self.rotary = Rotary(dim // num_heads) # dim // num_heads = head_dim
        self.c_proj = CastedLinear(dim, dim)
        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977
        self.attn_scale = nn.Parameter(torch.tensor(1.0 / (dim // num_heads) ** 0.5))

    def forward(self, x, ve, block_mask):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, 'Must use batch size = 1 for FlexAttention'
        q = self.c_q(x).view(B, T, self.num_heads, -1)
        k = self.c_k(x).view(B, T, self.num_heads, -1)
        v = self.c_v(x).view(B, T, self.num_heads, -1)
        if ve is not None:
            v = self.lambdas[0] * v + self.lambdas[1] * ve.view_as(v) # @KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = self.lambdas[0] * v
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = self.rotary(q), self.rotary(k)
        y = flex_attention(q.transpose(1, 2) * self.attn_scale, k.transpose(1, 2), v.transpose(1, 2), block_mask=block_mask, scale=1.)
        y = y.transpose(1, 2).contiguous().view_as(x) # re-assemble all head outputs side by side
        y = self.c_proj(y)
        return y

class MLP(nn.Module):

    def __init__(self, dim):
        super().__init__()
        self.c_fc = CastedLinear(dim, 4 * dim)
        self.c_proj = CastedLinear(4 * dim, dim)
        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977

    def forward(self, x):
        x = self.c_fc(x)
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = self.c_proj(x)
        return x

class Block(nn.Module):

    def __init__(self, model_dim, num_heads, use_attn=True):
        super().__init__()
        self.attn = CausalSelfAttention(model_dim, num_heads) if use_attn else None
        self.mlp = MLP(model_dim)
        self.lambdas = nn.Parameter(torch.tensor([1., 0.]))

    def forward(self, x, ve, x0, block_mask):
        x = self.lambdas[0] * x + self.lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), ve, block_mask)
        x = x + self.mlp(norm(x))
        return x

class ValueEmbedding(nn.Module):
    def __init__(self, vocab_size, model_dim):
        super().__init__()
        self.embed = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])

    def forward(self, inputs):
        ve = [emb(inputs).bfloat16() for emb in self.embed]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        ve = [ve[0], ve[1], ve[2], None, None, None, None, None, None, ve[0], ve[1], ve[2]]
        return ve

# -----------------------------------------------------------------------------
# The main GPT-2 model

class GPT(nn.Module):

    def __init__(self, vocab_size, num_layers, num_heads, model_dim):
        super().__init__()
        self.embed = nn.Embedding(vocab_size, model_dim)
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.blocks = nn.ModuleList([Block(model_dim, num_heads, use_attn=(i != 7))
                                     for i in range(num_layers)])
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual learning
        # U-net structure on token value embeddings by @leloykun
        self.value_embeds = ValueEmbedding(vocab_size, model_dim)
        self.lm_head = CastedLinear(model_dim, vocab_size)
        self.lm_head.weight.data.zero_() # @Grad62304977
        # U-net design by @brendanh0gan
        self.num_encoder_layers = num_layers // 2 # Half of the layers for encoder
        self.num_decoder_layers = num_layers - self.num_encoder_layers # Remaining for decoder
        # Add learnable skip connection weights for decoder layers
        self.skip_weights = nn.Parameter(torch.ones(self.num_decoder_layers))

    def forward(self, inputs, targets, sliding_window_num_blocks):
        BLOCK_SIZE = 128
        seq_len = len(inputs)
        assert seq_len % BLOCK_SIZE == 0
        total_num_blocks = seq_len // BLOCK_SIZE
        assert inputs.ndim == 1
        docs = (inputs == 50256).cumsum(0)
        docs_low = docs.view(-1, BLOCK_SIZE)[:, 0].contiguous()
        docs_high = docs.view(-1, BLOCK_SIZE)[:, -1].contiguous()

        def document_causal(b, h, q_idx, kv_idx):
            causal_mask = q_idx >= kv_idx
            document_mask = docs[q_idx] == docs[kv_idx]
            return causal_mask & document_mask

        def dense_to_ordered(dense_mask):
            num_blocks = dense_mask.sum(dim=-1, dtype=torch.int32)
            indices = dense_mask.argsort(dim=-1, descending=True, stable=True).to(torch.int32)
            return num_blocks[None, None].contiguous(), indices[None, None].contiguous()

        def create_doc_swc_block_mask(sliding_window_num_blocks):
            kv_idx = block_idx = torch.arange(total_num_blocks, dtype=torch.int32, device='cuda')
            q_idx = block_idx[:, None]
            causal_bm = q_idx >= kv_idx
            causal_full_bm = q_idx > kv_idx
            window_bm = q_idx - kv_idx < sliding_window_num_blocks
            window_full_bm = window_bm # block-wise sliding window by @YouJiacheng
            # document_bm = (docs_low[q_idx] <= docs_high[kv_idx]) & (docs_low[kv_idx] <= docs_high[q_idx])
            document_bm = (docs_low[:, None] <= docs_high) & (docs_low <= docs_high[:, None])
            document_full_bm = (docs_low[:, None] == docs_high) & (docs_low == docs_high[:, None])
            nonzero_bm = causal_bm & window_bm & document_bm
            full_bm  = causal_full_bm & window_full_bm & document_full_bm
            kv_num_blocks, kv_indices = dense_to_ordered(nonzero_bm & ~full_bm)
            full_kv_num_blocks, full_kv_indices = dense_to_ordered(full_bm)
            return BlockMask.from_kv_blocks(
                kv_num_blocks,
                kv_indices,
                full_kv_num_blocks,
                full_kv_indices,
                BLOCK_SIZE=BLOCK_SIZE,
                mask_mod=document_causal,
            )

        block_mask = create_doc_swc_block_mask(sliding_window_num_blocks)

        x0 = norm(self.embed(inputs[None]).bfloat16()) # use of norm here by @Grad62304977
        x = x0
        ve = self.value_embeds(inputs)
        assert len(ve) == len(self.blocks)
        ve_enc, ve_dec = ve[:self.num_encoder_layers], ve[self.num_encoder_layers:]

        # Store outputs for U-Net skip connections
        skip_connections = []
        # Encoder pass - process only the first half of the blocks
        for i in range(self.num_encoder_layers):
            x = self.blocks[i](x, ve_enc[i], x0, block_mask)
            skip_connections.append(x)
        # Decoder pass - process the remaining blocks with weighted skip connections
        for i in range(self.num_decoder_layers):
            x = x + self.skip_weights[i] * skip_connections.pop()
            # U-net structure on token value embeddings by @leloykun
            x = self.blocks[self.num_encoder_layers + i](x, ve_dec[i], x0, block_mask)

        x = norm(x)
        logits = self.lm_head(x)
        logits = 15 * torch.tanh(logits / 15) # @Grad62304977 added tanh softcapping, @KoszarskyB reduced it from 30 to 15
        logits = logits.float()
        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets)
        return loss

# -----------------------------------------------------------------------------
# Our own simple Distributed Data Loader

def _load_data_shard(path):
    # only reads the header, returns header data
    # header is 256 int32
    header = torch.from_file(path, False, 256, dtype=torch.int32)
    assert header[0] == 20240520, 'magic number mismatch in the data .bin file'
    assert header[1] == 1, 'unsupported version'
    num_tokens = int(header[2]) # number of tokens (claimed)
    with open(path, 'rb', buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, 'number of tokens read does not match header'
    return tokens

class DistributedDataLoader:

    def __init__(self, filename_pattern):
        self.rank = int(os.environ['RANK'])
        self.world_size = int(os.environ['WORLD_SIZE'])
        self.files = sorted(glob.glob(filename_pattern))
        self.reset()

    def reset(self):
        self.current_shard = -1
        self.advance()

    def advance(self):
        self.current_shard = (self.current_shard + 1) % len(self.files)
        self.current_position = 0
        self.tokens = _load_data_shard(self.files[self.current_shard])

    def next_batch(self, batch_size):
        assert batch_size % self.world_size == 0
        device_batch_size = batch_size // self.world_size
        # load next shard if necessary
        if self.current_position + batch_size + 1 >= len(self.tokens):
            self.advance()
        pos = self.current_position + self.rank * device_batch_size
        device_batch_tokens = self.tokens[pos:pos+device_batch_size+1]
        # advance current position
        self.current_position += batch_size
        inputs = device_batch_tokens[:-1].to(device='cuda', dtype=torch.int32, non_blocking=True)
        targets = device_batch_tokens[1:].to(device='cuda', dtype=torch.int64, non_blocking=True)
        return inputs, targets

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files = 'data/fineweb10B/fineweb_train_*.bin' # input .bin to train on
    val_files = 'data/fineweb10B/fineweb_val_*.bin' # input .bin to eval validation loss on
    val_tokens = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # optimization
    batch_size = 8*64*1024 # batch size in tokens
    num_iterations = 1370 # number of iterations to run
    cooldown_frac = 0.4 # fraction of training spent cooling down the learning rate
    bf16_embeds = True
    # evaluation and logging
    val_loss_every = 125 # every how many steps to evaluate val loss? 0 for only at the end
    # implementation
    max_device_batch_size = 64*1024 # batch size per device in tokens
    save_checkpoint = False
args = Hyperparameters()

micro_bs = args.max_device_batch_size

# set up DDP (distributed data parallel). torchrun sets this env variable
rank = int(os.environ['RANK'])
local_rank = int(os.environ['LOCAL_RANK'])
world_size = int(os.environ['WORLD_SIZE'])
assert torch.cuda.is_available()
torch.cuda.set_device(local_rank)
dist.init_process_group(backend='nccl', device_id=torch.device(local_rank))
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = uuid.uuid4()
    os.makedirs('logs', exist_ok=True)
    logfile = f'logs/{run_id}.txt'
    print(logfile)

def print0(s, console=False):
    if master_process:
        with open(logfile, 'a') as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0('='*100)
# log information about the hardware/software environment this is running on
print0(f'Running Python {sys.version}')
print0(f'Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}')
print0(subprocess.run(['nvidia-smi'], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout)
print0('='*100)

# load data
train_loader = DistributedDataLoader(args.train_files)
val_loader = DistributedDataLoader(args.val_files)
print0(f'Training dataloader files: {train_loader.files}')
print0(f'Validation dataloader files: {val_loader.files}')
print0('='*100)

# there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency. suggested to me by @Grad62304977.
# this originates from Karpathy's experiments.
model = GPT(vocab_size=50304, num_layers=12, num_heads=6, model_dim=768)
model = model.cuda()
if args.bf16_embeds:
    for m in model.modules():
        if isinstance(m, nn.Embedding):
            m.bfloat16()
model: GPT = torch.compile(model)
ddp_model = DDP(model, device_ids=[local_rank], broadcast_buffers=False, gradient_as_bucket_view=True)

# collect the parameters to optimize
hidden_matrix_params = [p for p in model.blocks.parameters() if p.ndim == 2]
embed_params = [model.embed.weight, *model.value_embeds.parameters()]
scalar_params = [p for n, p in model.named_parameters() if p.ndim < 2 and "attn_scale" not in n]
attn_scale_params = [p for n, p in model.named_parameters() if p.ndim < 2 and "attn_scale" in n]
head_params = [model.lm_head.weight]

# init the optimizer(s)
optimizer1 = torch.optim.Adam([dict(params=embed_params, lr=0.6),
                               dict(params=head_params, lr=0.008),
                               dict(params=scalar_params, lr=0.04),
                               dict(params=attn_scale_params, lr=0.01)],
                              betas=(0.8, 0.95), fused=True)
optimizer2 = Muon(hidden_matrix_params, lr=0.05, momentum=0.95)
optimizers = [optimizer1, optimizer2]

# learning rate schedule: stable then decay
def get_lr(it):
    t = 1 - it / args.num_iterations # time remaining in training
    assert 1 >= t > 0
    # 1) constant lr for first part of training
    if t >= args.cooldown_frac:
        return 1.0
    # 2) then linear cooldown
    else:
        return t / args.cooldown_frac
schedulers = [torch.optim.lr_scheduler.LambdaLR(opt, get_lr) for opt in optimizers]

# sliding window size schedule: linear increase over training in chunks of 128 from 128 -> 1792. By @fernbear.bsky.social
def get_sliding_window_blocks(it):
    x = it / args.num_iterations # training progress
    assert 0 <= x <= 1
    return int(((1 - x) * 128 + x * 1856) // 128)
sliding_window_num_blocks = torch.tensor(1, dtype=torch.int32, device='cuda')

# Start training loop
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    # This effectively ignores timing first 10 steps, which are slower for weird reasons.
    # Alternately, and slightly more correctly in terms of benchmarking, we could do 10
    # steps with dummy data first, and then re-initialize the model and reset the loader.
    if step == 10:
        training_time_ms = 0
        t0 = time.perf_counter()
    timed_steps = float('nan') if step <= 11 else (step - 10) + 1 # <= 11 to avoid bug in val

    sliding_window_num_blocks.copy_(get_sliding_window_blocks(step))

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        # run validation batches
        model.eval()
        val_loader.reset()
        val_loss = 0.0
        # calculate the number of steps to take in the val loop.
        val_batch_size = world_size * micro_bs
        assert args.val_tokens % val_batch_size == 0
        val_steps = args.val_tokens // val_batch_size
        for _ in range(val_steps):
            with torch.no_grad():
                inputs_val, targets_val = val_loader.next_batch(val_batch_size)
                val_loss += ddp_model(inputs_val, targets_val, sliding_window_num_blocks)
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        val_loss /= val_steps
        # logging
        print0(f'step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/(timed_steps-1):.2f}ms', console=True)
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f'logs/{run_id}', exist_ok=True)
            torch.save(log, f'logs/{run_id}/state_step{step:06d}.pt')
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    model.train()
    batch_size = args.batch_size
    assert batch_size % world_size == 0
    inputs_train, targets_train = train_loader.next_batch(batch_size)
    assert len(inputs_train) <= micro_bs or len(inputs_train) % micro_bs == 0
    for micro_inputs_train, micro_targets_train in zip(inputs_train.split(micro_bs), targets_train.split(micro_bs)):
        ddp_model(micro_inputs_train, micro_targets_train, sliding_window_num_blocks).backward()
    # momentum warmup for Muon
    frac = min(step/300, 1)
    for group in optimizer2.param_groups:
        group['momentum'] = (1 - frac) * 0.85 + frac * 0.95
    # step the optimizers and schedulers
    for opt, sched in zip(optimizers, schedulers):
        opt.step()
        if step != train_steps-1:
            sched.step()
    # null the gradients
    model.zero_grad(set_to_none=True)
    # logging
    approx_time = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f'step:{step+1}/{train_steps} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms', console=True)

print0(f'peak memory consumption: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB')
dist.destroy_process_group()

====================================================================================================
Running Python 3.10.12 (main, Sep 11 2024, 15:47:36) [GCC 11.4.0]
Running PyTorch 2.6.0.dev20241231+cu126 compiled for CUDA 12.6
Sat Jan 11 23:08:58 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.183.06             Driver Version: 535.183.06   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:19:00.0 Off |                    0 |
| N/A   38C    P0             126W / 700W |   7713MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:3B:00.0 Off |                    0 |
| N/A   32C    P0             114W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:4C:00.0 Off |                    0 |
| N/A   29C    P0             117W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   37C    P0             129W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:9B:00.0 Off |                    0 |
| N/A   38C    P0             123W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:BB:00.0 Off |                    0 |
| N/A   30C    P0             115W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:CB:00.0 Off |                    0 |
| N/A   36C    P0             116W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:DB:00.0 Off |                    0 |
| N/A   30C    P0             117W / 700W |   3211MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
Training dataloader files: ['data/fineweb10B/fineweb_train_000001.bin', 'data/fineweb10B/fineweb_train_000002.bin', 'data/fineweb10B/fineweb_train_000003.bin', 'data/fineweb10B/fineweb_train_000004.bin', 'data/fineweb10B/fineweb_train_000005.bin', 'data/fineweb10B/fineweb_train_000006.bin', 'data/fineweb10B/fineweb_train_000007.bin', 'data/fineweb10B/fineweb_train_000008.bin', 'data/fineweb10B/fineweb_train_000009.bin']
Validation dataloader files: ['data/fineweb10B/fineweb_val_000000.bin']
====================================================================================================
step:0/1370 val_loss:10.8258 train_time:0ms step_avg:nanms
step:1/1370 train_time:29565ms step_avg:nanms
step:2/1370 train_time:29649ms step_avg:nanms
step:3/1370 train_time:29831ms step_avg:nanms
step:4/1370 train_time:29966ms step_avg:nanms
step:5/1370 train_time:30100ms step_avg:nanms
step:6/1370 train_time:30236ms step_avg:nanms
step:7/1370 train_time:30370ms step_avg:nanms
step:8/1370 train_time:30504ms step_avg:nanms
step:9/1370 train_time:30639ms step_avg:nanms
step:10/1370 train_time:30777ms step_avg:nanms
step:11/1370 train_time:139ms step_avg:nanms
step:12/1370 train_time:273ms step_avg:nanms
step:13/1370 train_time:410ms step_avg:136.59ms
step:14/1370 train_time:544ms step_avg:136.05ms
step:15/1370 train_time:679ms step_avg:135.79ms
step:16/1370 train_time:814ms step_avg:135.67ms
step:17/1370 train_time:951ms step_avg:135.89ms
step:18/1370 train_time:1088ms step_avg:136.05ms
step:19/1370 train_time:1225ms step_avg:136.16ms
step:20/1370 train_time:1361ms step_avg:136.13ms
step:21/1370 train_time:1497ms step_avg:136.11ms
step:22/1370 train_time:1632ms step_avg:136.01ms
step:23/1370 train_time:1768ms step_avg:136.02ms
step:24/1370 train_time:1906ms step_avg:136.11ms
step:25/1370 train_time:2043ms step_avg:136.17ms
step:26/1370 train_time:2177ms step_avg:136.07ms
step:27/1370 train_time:2314ms step_avg:136.11ms
step:28/1370 train_time:2451ms step_avg:136.18ms
step:29/1370 train_time:2588ms step_avg:136.19ms
step:30/1370 train_time:2724ms step_avg:136.20ms
step:31/1370 train_time:2860ms step_avg:136.19ms
step:32/1370 train_time:2995ms step_avg:136.16ms
step:33/1370 train_time:3132ms step_avg:136.19ms
step:34/1370 train_time:3269ms step_avg:136.21ms
step:35/1370 train_time:3406ms step_avg:136.24ms
step:36/1370 train_time:3543ms step_avg:136.27ms
step:37/1370 train_time:3677ms step_avg:136.20ms
step:38/1370 train_time:3814ms step_avg:136.20ms
step:39/1370 train_time:3951ms step_avg:136.25ms
step:40/1370 train_time:4088ms step_avg:136.26ms
step:41/1370 train_time:4224ms step_avg:136.25ms
step:42/1370 train_time:4360ms step_avg:136.24ms
step:43/1370 train_time:4496ms step_avg:136.23ms
step:44/1370 train_time:4631ms step_avg:136.20ms
step:45/1370 train_time:4768ms step_avg:136.22ms
step:46/1370 train_time:4903ms step_avg:136.19ms
step:47/1370 train_time:5041ms step_avg:136.24ms
step:48/1370 train_time:5175ms step_avg:136.20ms
step:49/1370 train_time:5312ms step_avg:136.21ms
step:50/1370 train_time:5448ms step_avg:136.20ms
step:51/1370 train_time:5584ms step_avg:136.21ms
step:52/1370 train_time:5721ms step_avg:136.21ms
step:53/1370 train_time:5858ms step_avg:136.24ms
step:54/1370 train_time:5995ms step_avg:136.25ms
step:55/1370 train_time:6132ms step_avg:136.27ms
step:56/1370 train_time:6270ms step_avg:136.31ms
step:57/1370 train_time:6409ms step_avg:136.35ms
step:58/1370 train_time:6547ms step_avg:136.39ms
step:59/1370 train_time:6684ms step_avg:136.40ms
step:60/1370 train_time:6819ms step_avg:136.39ms
step:61/1370 train_time:6956ms step_avg:136.40ms
step:62/1370 train_time:7092ms step_avg:136.38ms
step:63/1370 train_time:7229ms step_avg:136.39ms
step:64/1370 train_time:7366ms step_avg:136.42ms
step:65/1370 train_time:7505ms step_avg:136.45ms
step:66/1370 train_time:7643ms step_avg:136.48ms
step:67/1370 train_time:7777ms step_avg:136.44ms
step:68/1370 train_time:7914ms step_avg:136.45ms
step:69/1370 train_time:8052ms step_avg:136.47ms
step:70/1370 train_time:8188ms step_avg:136.46ms
step:71/1370 train_time:8326ms step_avg:136.49ms
step:72/1370 train_time:8462ms step_avg:136.49ms
step:73/1370 train_time:8598ms step_avg:136.47ms
step:74/1370 train_time:8733ms step_avg:136.46ms
step:75/1370 train_time:8869ms step_avg:136.45ms
step:76/1370 train_time:9006ms step_avg:136.45ms
step:77/1370 train_time:9143ms step_avg:136.47ms
step:78/1370 train_time:9277ms step_avg:136.43ms
step:79/1370 train_time:9413ms step_avg:136.43ms
step:80/1370 train_time:9550ms step_avg:136.43ms
step:81/1370 train_time:9687ms step_avg:136.44ms
step:82/1370 train_time:9823ms step_avg:136.44ms
step:83/1370 train_time:9960ms step_avg:136.43ms
step:84/1370 train_time:10095ms step_avg:136.43ms
step:85/1370 train_time:10231ms step_avg:136.41ms
step:86/1370 train_time:10368ms step_avg:136.42ms
step:87/1370 train_time:10505ms step_avg:136.43ms
step:88/1370 train_time:10642ms step_avg:136.44ms
step:89/1370 train_time:10777ms step_avg:136.41ms
step:90/1370 train_time:10913ms step_avg:136.42ms
step:91/1370 train_time:11051ms step_avg:136.43ms
step:92/1370 train_time:11188ms step_avg:136.44ms
step:93/1370 train_time:11325ms step_avg:136.45ms
step:94/1370 train_time:11462ms step_avg:136.45ms
step:95/1370 train_time:11598ms step_avg:136.45ms
step:96/1370 train_time:11734ms step_avg:136.44ms
step:97/1370 train_time:11870ms step_avg:136.43ms
step:98/1370 train_time:12007ms step_avg:136.45ms
step:99/1370 train_time:12145ms step_avg:136.47ms
step:100/1370 train_time:12280ms step_avg:136.45ms
step:101/1370 train_time:12418ms step_avg:136.46ms
step:102/1370 train_time:12555ms step_avg:136.46ms
step:103/1370 train_time:12692ms step_avg:136.47ms
step:104/1370 train_time:12834ms step_avg:136.54ms
step:105/1370 train_time:12974ms step_avg:136.56ms
step:106/1370 train_time:13113ms step_avg:136.59ms
step:107/1370 train_time:13252ms step_avg:136.62ms
step:108/1370 train_time:13391ms step_avg:136.64ms
step:109/1370 train_time:13531ms step_avg:136.68ms
step:110/1370 train_time:13672ms step_avg:136.72ms
step:111/1370 train_time:13812ms step_avg:136.76ms
step:112/1370 train_time:13952ms step_avg:136.78ms
step:113/1370 train_time:14091ms step_avg:136.80ms
step:114/1370 train_time:14232ms step_avg:136.85ms
step:115/1370 train_time:14371ms step_avg:136.87ms
step:116/1370 train_time:14511ms step_avg:136.90ms
step:117/1370 train_time:14651ms step_avg:136.93ms
step:118/1370 train_time:14791ms step_avg:136.96ms
step:119/1370 train_time:14933ms step_avg:137.00ms
step:120/1370 train_time:15073ms step_avg:137.03ms
step:121/1370 train_time:15214ms step_avg:137.06ms
step:122/1370 train_time:15355ms step_avg:137.09ms
step:123/1370 train_time:15493ms step_avg:137.11ms
step:124/1370 train_time:15633ms step_avg:137.13ms
step:125/1370 train_time:15773ms step_avg:137.15ms
step:125/1370 val_loss:4.3823 train_time:15837ms step_avg:137.71ms
step:126/1370 train_time:15915ms step_avg:137.20ms
step:127/1370 train_time:16060ms step_avg:137.26ms
step:128/1370 train_time:16200ms step_avg:137.29ms
step:129/1370 train_time:16338ms step_avg:137.29ms
step:130/1370 train_time:16477ms step_avg:137.31ms
step:131/1370 train_time:16615ms step_avg:137.32ms
step:132/1370 train_time:16754ms step_avg:137.33ms
step:133/1370 train_time:16897ms step_avg:137.37ms
step:134/1370 train_time:17039ms step_avg:137.41ms
step:135/1370 train_time:17180ms step_avg:137.44ms
step:136/1370 train_time:17320ms step_avg:137.46ms
step:137/1370 train_time:17459ms step_avg:137.48ms
step:138/1370 train_time:17598ms step_avg:137.48ms
step:139/1370 train_time:17738ms step_avg:137.50ms
step:140/1370 train_time:17880ms step_avg:137.53ms
step:141/1370 train_time:18021ms step_avg:137.56ms
step:142/1370 train_time:18161ms step_avg:137.58ms
step:143/1370 train_time:18302ms step_avg:137.61ms
step:144/1370 train_time:18441ms step_avg:137.62ms
step:145/1370 train_time:18581ms step_avg:137.64ms
step:146/1370 train_time:18720ms step_avg:137.65ms
step:147/1370 train_time:18861ms step_avg:137.67ms
step:148/1370 train_time:19001ms step_avg:137.69ms
step:149/1370 train_time:19142ms step_avg:137.72ms
step:150/1370 train_time:19283ms step_avg:137.73ms
step:151/1370 train_time:19423ms step_avg:137.75ms
step:152/1370 train_time:19563ms step_avg:137.77ms
step:153/1370 train_time:19702ms step_avg:137.78ms
step:154/1370 train_time:19843ms step_avg:137.80ms
step:155/1370 train_time:19984ms step_avg:137.82ms
step:156/1370 train_time:20123ms step_avg:137.83ms
step:157/1370 train_time:20263ms step_avg:137.84ms
step:158/1370 train_time:20402ms step_avg:137.85ms
step:159/1370 train_time:20542ms step_avg:137.87ms
step:160/1370 train_time:20682ms step_avg:137.88ms
step:161/1370 train_time:20823ms step_avg:137.90ms
step:162/1370 train_time:20964ms step_avg:137.92ms
step:163/1370 train_time:21103ms step_avg:137.93ms
step:164/1370 train_time:21244ms step_avg:137.95ms
step:165/1370 train_time:21385ms step_avg:137.97ms
step:166/1370 train_time:21525ms step_avg:137.98ms
step:167/1370 train_time:21666ms step_avg:138.00ms
step:168/1370 train_time:21807ms step_avg:138.02ms
step:169/1370 train_time:21947ms step_avg:138.03ms
step:170/1370 train_time:22088ms step_avg:138.05ms
step:171/1370 train_time:22229ms step_avg:138.07ms
step:172/1370 train_time:22371ms step_avg:138.09ms
step:173/1370 train_time:22510ms step_avg:138.10ms
step:174/1370 train_time:22651ms step_avg:138.12ms
step:175/1370 train_time:22791ms step_avg:138.13ms
step:176/1370 train_time:22931ms step_avg:138.14ms
step:177/1370 train_time:23071ms step_avg:138.15ms
step:178/1370 train_time:23211ms step_avg:138.16ms
step:179/1370 train_time:23353ms step_avg:138.18ms
step:180/1370 train_time:23494ms step_avg:138.20ms
step:181/1370 train_time:23633ms step_avg:138.21ms
step:182/1370 train_time:23775ms step_avg:138.23ms
step:183/1370 train_time:23915ms step_avg:138.24ms
step:184/1370 train_time:24056ms step_avg:138.25ms
step:185/1370 train_time:24196ms step_avg:138.27ms
step:186/1370 train_time:24339ms step_avg:138.29ms
step:187/1370 train_time:24480ms step_avg:138.30ms
step:188/1370 train_time:24619ms step_avg:138.31ms
step:189/1370 train_time:24761ms step_avg:138.33ms
step:190/1370 train_time:24901ms step_avg:138.34ms
step:191/1370 train_time:25078ms step_avg:138.55ms
step:192/1370 train_time:25216ms step_avg:138.55ms
step:193/1370 train_time:25356ms step_avg:138.56ms
step:194/1370 train_time:25495ms step_avg:138.56ms
step:195/1370 train_time:25634ms step_avg:138.56ms
step:196/1370 train_time:25774ms step_avg:138.57ms
step:197/1370 train_time:25916ms step_avg:138.59ms
step:198/1370 train_time:26060ms step_avg:138.62ms
step:199/1370 train_time:26200ms step_avg:138.63ms
step:200/1370 train_time:26340ms step_avg:138.63ms
step:201/1370 train_time:26481ms step_avg:138.64ms
step:202/1370 train_time:26620ms step_avg:138.64ms
step:203/1370 train_time:26759ms step_avg:138.65ms
step:204/1370 train_time:26900ms step_avg:138.66ms
step:205/1370 train_time:27046ms step_avg:138.70ms
step:206/1370 train_time:27190ms step_avg:138.72ms
step:207/1370 train_time:27332ms step_avg:138.74ms
step:208/1370 train_time:27474ms step_avg:138.76ms
step:209/1370 train_time:27616ms step_avg:138.77ms
step:210/1370 train_time:27759ms step_avg:138.79ms
step:211/1370 train_time:27902ms step_avg:138.82ms
step:212/1370 train_time:28047ms step_avg:138.85ms
step:213/1370 train_time:28191ms step_avg:138.87ms
step:214/1370 train_time:28332ms step_avg:138.88ms
step:215/1370 train_time:28475ms step_avg:138.90ms
step:216/1370 train_time:28617ms step_avg:138.92ms
step:217/1370 train_time:28760ms step_avg:138.94ms
step:218/1370 train_time:28903ms step_avg:138.96ms
step:219/1370 train_time:29046ms step_avg:138.98ms
step:220/1370 train_time:29190ms step_avg:139.00ms
step:221/1370 train_time:29333ms step_avg:139.02ms
step:222/1370 train_time:29476ms step_avg:139.04ms
step:223/1370 train_time:29619ms step_avg:139.06ms
step:224/1370 train_time:29763ms step_avg:139.08ms
step:225/1370 train_time:29905ms step_avg:139.09ms
step:226/1370 train_time:30048ms step_avg:139.11ms
step:227/1370 train_time:30192ms step_avg:139.13ms
step:228/1370 train_time:30335ms step_avg:139.15ms
step:229/1370 train_time:30480ms step_avg:139.18ms
step:230/1370 train_time:30622ms step_avg:139.19ms
step:231/1370 train_time:30766ms step_avg:139.21ms
step:232/1370 train_time:30908ms step_avg:139.23ms
step:233/1370 train_time:31051ms step_avg:139.24ms
step:234/1370 train_time:31194ms step_avg:139.26ms
step:235/1370 train_time:31337ms step_avg:139.27ms
step:236/1370 train_time:31481ms step_avg:139.30ms
step:237/1370 train_time:31624ms step_avg:139.31ms
step:238/1370 train_time:31768ms step_avg:139.33ms
step:239/1370 train_time:31909ms step_avg:139.34ms
step:240/1370 train_time:32052ms step_avg:139.36ms
step:241/1370 train_time:32195ms step_avg:139.37ms
step:242/1370 train_time:32341ms step_avg:139.40ms
step:243/1370 train_time:32484ms step_avg:139.42ms
step:244/1370 train_time:32627ms step_avg:139.43ms
step:245/1370 train_time:32770ms step_avg:139.45ms
step:246/1370 train_time:32912ms step_avg:139.46ms
step:247/1370 train_time:33055ms step_avg:139.47ms
step:248/1370 train_time:33199ms step_avg:139.49ms
step:249/1370 train_time:33344ms step_avg:139.51ms
step:250/1370 train_time:33487ms step_avg:139.53ms
step:250/1370 val_loss:3.9629 train_time:33551ms step_avg:139.80ms
step:251/1370 train_time:33630ms step_avg:139.54ms
step:252/1370 train_time:33774ms step_avg:139.56ms
step:253/1370 train_time:33917ms step_avg:139.57ms
step:254/1370 train_time:34058ms step_avg:139.58ms
step:255/1370 train_time:34200ms step_avg:139.59ms
step:256/1370 train_time:34343ms step_avg:139.60ms
step:257/1370 train_time:34486ms step_avg:139.62ms
step:258/1370 train_time:34631ms step_avg:139.64ms
step:259/1370 train_time:34777ms step_avg:139.67ms
step:260/1370 train_time:34920ms step_avg:139.68ms
step:261/1370 train_time:35063ms step_avg:139.69ms
step:262/1370 train_time:35204ms step_avg:139.70ms
step:263/1370 train_time:35346ms step_avg:139.71ms
step:264/1370 train_time:35490ms step_avg:139.73ms
step:265/1370 train_time:35633ms step_avg:139.74ms
step:266/1370 train_time:35778ms step_avg:139.76ms
step:267/1370 train_time:35921ms step_avg:139.77ms
step:268/1370 train_time:36065ms step_avg:139.79ms
step:269/1370 train_time:36208ms step_avg:139.80ms
step:270/1370 train_time:36350ms step_avg:139.81ms
step:271/1370 train_time:36493ms step_avg:139.82ms
step:272/1370 train_time:36637ms step_avg:139.83ms
step:273/1370 train_time:36781ms step_avg:139.85ms
step:274/1370 train_time:36923ms step_avg:139.86ms
step:275/1370 train_time:37066ms step_avg:139.87ms
step:276/1370 train_time:37207ms step_avg:139.87ms
step:277/1370 train_time:37350ms step_avg:139.89ms
step:278/1370 train_time:37493ms step_avg:139.90ms
step:279/1370 train_time:37638ms step_avg:139.92ms
step:280/1370 train_time:37782ms step_avg:139.93ms
step:281/1370 train_time:37926ms step_avg:139.95ms
step:282/1370 train_time:38071ms step_avg:139.97ms
step:283/1370 train_time:38212ms step_avg:139.97ms
step:284/1370 train_time:38354ms step_avg:139.98ms
step:285/1370 train_time:38497ms step_avg:139.99ms
step:286/1370 train_time:38642ms step_avg:140.01ms
step:287/1370 train_time:38786ms step_avg:140.02ms
step:288/1370 train_time:38929ms step_avg:140.03ms
step:289/1370 train_time:39073ms step_avg:140.05ms
step:290/1370 train_time:39214ms step_avg:140.05ms
step:291/1370 train_time:39358ms step_avg:140.06ms
step:292/1370 train_time:39500ms step_avg:140.07ms
step:293/1370 train_time:39644ms step_avg:140.08ms
step:294/1370 train_time:39788ms step_avg:140.10ms
step:295/1370 train_time:39931ms step_avg:140.11ms
step:296/1370 train_time:40074ms step_avg:140.12ms
step:297/1370 train_time:40217ms step_avg:140.13ms
step:298/1370 train_time:40360ms step_avg:140.14ms
step:299/1370 train_time:40502ms step_avg:140.15ms
step:300/1370 train_time:40646ms step_avg:140.16ms
step:301/1370 train_time:40790ms step_avg:140.17ms
step:302/1370 train_time:40932ms step_avg:140.18ms
step:303/1370 train_time:41075ms step_avg:140.19ms
step:304/1370 train_time:41218ms step_avg:140.20ms
step:305/1370 train_time:41362ms step_avg:140.21ms
step:306/1370 train_time:41505ms step_avg:140.22ms
step:307/1370 train_time:41649ms step_avg:140.23ms
step:308/1370 train_time:41794ms step_avg:140.25ms
step:309/1370 train_time:41942ms step_avg:140.28ms
step:310/1370 train_time:42088ms step_avg:140.29ms
step:311/1370 train_time:42232ms step_avg:140.31ms
step:312/1370 train_time:42378ms step_avg:140.32ms
step:313/1370 train_time:42522ms step_avg:140.34ms
step:314/1370 train_time:42667ms step_avg:140.35ms
step:315/1370 train_time:42811ms step_avg:140.36ms
step:316/1370 train_time:42958ms step_avg:140.39ms
step:317/1370 train_time:43104ms step_avg:140.40ms
step:318/1370 train_time:43249ms step_avg:140.42ms
step:319/1370 train_time:43393ms step_avg:140.43ms
step:320/1370 train_time:43540ms step_avg:140.45ms
step:321/1370 train_time:43685ms step_avg:140.47ms
step:322/1370 train_time:43828ms step_avg:140.47ms
step:323/1370 train_time:43974ms step_avg:140.49ms
step:324/1370 train_time:44119ms step_avg:140.51ms
step:325/1370 train_time:44266ms step_avg:140.53ms
step:326/1370 train_time:44409ms step_avg:140.54ms
step:327/1370 train_time:44554ms step_avg:140.55ms
step:328/1370 train_time:44700ms step_avg:140.57ms
step:329/1370 train_time:44847ms step_avg:140.59ms
step:330/1370 train_time:44991ms step_avg:140.60ms
step:331/1370 train_time:45138ms step_avg:140.62ms
step:332/1370 train_time:45283ms step_avg:140.63ms
step:333/1370 train_time:45428ms step_avg:140.64ms
step:334/1370 train_time:45574ms step_avg:140.66ms
step:335/1370 train_time:45719ms step_avg:140.67ms
step:336/1370 train_time:45866ms step_avg:140.69ms
step:337/1370 train_time:46010ms step_avg:140.70ms
step:338/1370 train_time:46157ms step_avg:140.72ms
step:339/1370 train_time:46302ms step_avg:140.74ms
step:340/1370 train_time:46447ms step_avg:140.75ms
step:341/1370 train_time:46592ms step_avg:140.76ms
step:342/1370 train_time:46736ms step_avg:140.77ms
step:343/1370 train_time:46882ms step_avg:140.79ms
step:344/1370 train_time:47027ms step_avg:140.80ms
step:345/1370 train_time:47172ms step_avg:140.81ms
step:346/1370 train_time:47316ms step_avg:140.82ms
step:347/1370 train_time:47463ms step_avg:140.84ms
step:348/1370 train_time:47607ms step_avg:140.85ms
step:349/1370 train_time:47752ms step_avg:140.86ms
step:350/1370 train_time:47896ms step_avg:140.87ms
step:351/1370 train_time:48043ms step_avg:140.89ms
step:352/1370 train_time:48188ms step_avg:140.90ms
step:353/1370 train_time:48332ms step_avg:140.91ms
step:354/1370 train_time:48478ms step_avg:140.92ms
step:355/1370 train_time:48623ms step_avg:140.94ms
step:356/1370 train_time:48768ms step_avg:140.95ms
step:357/1370 train_time:48912ms step_avg:140.96ms
step:358/1370 train_time:49060ms step_avg:140.98ms
step:359/1370 train_time:49205ms step_avg:140.99ms
step:360/1370 train_time:49351ms step_avg:141.00ms
step:361/1370 train_time:49495ms step_avg:141.01ms
step:362/1370 train_time:49642ms step_avg:141.03ms
step:363/1370 train_time:49787ms step_avg:141.04ms
step:364/1370 train_time:49931ms step_avg:141.05ms
step:365/1370 train_time:50077ms step_avg:141.06ms
step:366/1370 train_time:50223ms step_avg:141.08ms
step:367/1370 train_time:50369ms step_avg:141.09ms
step:368/1370 train_time:50512ms step_avg:141.09ms
step:369/1370 train_time:50658ms step_avg:141.11ms
step:370/1370 train_time:50802ms step_avg:141.12ms
step:371/1370 train_time:50947ms step_avg:141.13ms
step:372/1370 train_time:51091ms step_avg:141.14ms
step:373/1370 train_time:51237ms step_avg:141.15ms
step:374/1370 train_time:51384ms step_avg:141.16ms
step:375/1370 train_time:51528ms step_avg:141.17ms
step:375/1370 val_loss:3.7787 train_time:51595ms step_avg:141.36ms
step:376/1370 train_time:51676ms step_avg:141.19ms
step:377/1370 train_time:51823ms step_avg:141.21ms
step:378/1370 train_time:51968ms step_avg:141.22ms
step:379/1370 train_time:52111ms step_avg:141.22ms
step:380/1370 train_time:52256ms step_avg:141.23ms
step:381/1370 train_time:52433ms step_avg:141.33ms
step:382/1370 train_time:52589ms step_avg:141.37ms
step:383/1370 train_time:52732ms step_avg:141.37ms
step:384/1370 train_time:52875ms step_avg:141.38ms
step:385/1370 train_time:53020ms step_avg:141.39ms
step:386/1370 train_time:53164ms step_avg:141.39ms
step:387/1370 train_time:53308ms step_avg:141.40ms
step:388/1370 train_time:53456ms step_avg:141.42ms
step:389/1370 train_time:53602ms step_avg:141.43ms
step:390/1370 train_time:53747ms step_avg:141.44ms
step:391/1370 train_time:53891ms step_avg:141.45ms
step:392/1370 train_time:54037ms step_avg:141.46ms
step:393/1370 train_time:54181ms step_avg:141.46ms
step:394/1370 train_time:54328ms step_avg:141.48ms
step:395/1370 train_time:54472ms step_avg:141.49ms
step:396/1370 train_time:54619ms step_avg:141.50ms
step:397/1370 train_time:54764ms step_avg:141.51ms
step:398/1370 train_time:54908ms step_avg:141.52ms
step:399/1370 train_time:55054ms step_avg:141.53ms
step:400/1370 train_time:55199ms step_avg:141.54ms
step:401/1370 train_time:55345ms step_avg:141.55ms
step:402/1370 train_time:55491ms step_avg:141.56ms
step:403/1370 train_time:55637ms step_avg:141.57ms
step:404/1370 train_time:55782ms step_avg:141.58ms
step:405/1370 train_time:55928ms step_avg:141.59ms
step:406/1370 train_time:56071ms step_avg:141.59ms
step:407/1370 train_time:56219ms step_avg:141.61ms
step:408/1370 train_time:56366ms step_avg:141.62ms
step:409/1370 train_time:56512ms step_avg:141.63ms
step:410/1370 train_time:56661ms step_avg:141.65ms
step:411/1370 train_time:56807ms step_avg:141.66ms
step:412/1370 train_time:56955ms step_avg:141.68ms
step:413/1370 train_time:57101ms step_avg:141.69ms
step:414/1370 train_time:57248ms step_avg:141.70ms
step:415/1370 train_time:57393ms step_avg:141.71ms
step:416/1370 train_time:57542ms step_avg:141.73ms
step:417/1370 train_time:57688ms step_avg:141.74ms
step:418/1370 train_time:57834ms step_avg:141.75ms
step:419/1370 train_time:57981ms step_avg:141.76ms
step:420/1370 train_time:58128ms step_avg:141.78ms
step:421/1370 train_time:58275ms step_avg:141.79ms
step:422/1370 train_time:58423ms step_avg:141.80ms
step:423/1370 train_time:58571ms step_avg:141.82ms
step:424/1370 train_time:58720ms step_avg:141.83ms
step:425/1370 train_time:58867ms step_avg:141.85ms
step:426/1370 train_time:59013ms step_avg:141.86ms
step:427/1370 train_time:59161ms step_avg:141.87ms
step:428/1370 train_time:59308ms step_avg:141.88ms
step:429/1370 train_time:59455ms step_avg:141.90ms
step:430/1370 train_time:59602ms step_avg:141.91ms
step:431/1370 train_time:59748ms step_avg:141.92ms
step:432/1370 train_time:59893ms step_avg:141.93ms
step:433/1370 train_time:60042ms step_avg:141.94ms
step:434/1370 train_time:60188ms step_avg:141.95ms
step:435/1370 train_time:60334ms step_avg:141.96ms
step:436/1370 train_time:60482ms step_avg:141.98ms
step:437/1370 train_time:60629ms step_avg:141.99ms
step:438/1370 train_time:60775ms step_avg:142.00ms
step:439/1370 train_time:60923ms step_avg:142.01ms
step:440/1370 train_time:61070ms step_avg:142.02ms
step:441/1370 train_time:61217ms step_avg:142.03ms
step:442/1370 train_time:61364ms step_avg:142.05ms
step:443/1370 train_time:61509ms step_avg:142.05ms
step:444/1370 train_time:61658ms step_avg:142.07ms
step:445/1370 train_time:61805ms step_avg:142.08ms
step:446/1370 train_time:61953ms step_avg:142.09ms
step:447/1370 train_time:62102ms step_avg:142.11ms
step:448/1370 train_time:62249ms step_avg:142.12ms
step:449/1370 train_time:62394ms step_avg:142.13ms
step:450/1370 train_time:62543ms step_avg:142.14ms
step:451/1370 train_time:62689ms step_avg:142.15ms
step:452/1370 train_time:62835ms step_avg:142.16ms
step:453/1370 train_time:62982ms step_avg:142.17ms
step:454/1370 train_time:63129ms step_avg:142.18ms
step:455/1370 train_time:63278ms step_avg:142.20ms
step:456/1370 train_time:63425ms step_avg:142.21ms
step:457/1370 train_time:63570ms step_avg:142.22ms
step:458/1370 train_time:63719ms step_avg:142.23ms
step:459/1370 train_time:63867ms step_avg:142.24ms
step:460/1370 train_time:64014ms step_avg:142.25ms
step:461/1370 train_time:64163ms step_avg:142.27ms
step:462/1370 train_time:64309ms step_avg:142.28ms
step:463/1370 train_time:64457ms step_avg:142.29ms
step:464/1370 train_time:64604ms step_avg:142.30ms
step:465/1370 train_time:64751ms step_avg:142.31ms
step:466/1370 train_time:64896ms step_avg:142.32ms
step:467/1370 train_time:65046ms step_avg:142.33ms
step:468/1370 train_time:65192ms step_avg:142.34ms
step:469/1370 train_time:65341ms step_avg:142.36ms
step:470/1370 train_time:65487ms step_avg:142.36ms
step:471/1370 train_time:65632ms step_avg:142.37ms
step:472/1370 train_time:65779ms step_avg:142.38ms
step:473/1370 train_time:65927ms step_avg:142.39ms
step:474/1370 train_time:66073ms step_avg:142.40ms
step:475/1370 train_time:66221ms step_avg:142.41ms
step:476/1370 train_time:66368ms step_avg:142.42ms
step:477/1370 train_time:66513ms step_avg:142.43ms
step:478/1370 train_time:66662ms step_avg:142.44ms
step:479/1370 train_time:66807ms step_avg:142.45ms
step:480/1370 train_time:66955ms step_avg:142.46ms
step:481/1370 train_time:67102ms step_avg:142.47ms
step:482/1370 train_time:67249ms step_avg:142.48ms
step:483/1370 train_time:67394ms step_avg:142.48ms
step:484/1370 train_time:67543ms step_avg:142.50ms
step:485/1370 train_time:67689ms step_avg:142.50ms
step:486/1370 train_time:67835ms step_avg:142.51ms
step:487/1370 train_time:67983ms step_avg:142.52ms
step:488/1370 train_time:68130ms step_avg:142.53ms
step:489/1370 train_time:68277ms step_avg:142.54ms
step:490/1370 train_time:68425ms step_avg:142.55ms
step:491/1370 train_time:68571ms step_avg:142.56ms
step:492/1370 train_time:68718ms step_avg:142.57ms
step:493/1370 train_time:68866ms step_avg:142.58ms
step:494/1370 train_time:69012ms step_avg:142.59ms
step:495/1370 train_time:69162ms step_avg:142.60ms
step:496/1370 train_time:69307ms step_avg:142.61ms
step:497/1370 train_time:69455ms step_avg:142.62ms
step:498/1370 train_time:69602ms step_avg:142.63ms
step:499/1370 train_time:69749ms step_avg:142.64ms
step:500/1370 train_time:69896ms step_avg:142.64ms
step:500/1370 val_loss:3.6605 train_time:69964ms step_avg:142.78ms
step:501/1370 train_time:70045ms step_avg:142.66ms
step:502/1370 train_time:70192ms step_avg:142.67ms
step:503/1370 train_time:70340ms step_avg:142.68ms
step:504/1370 train_time:70484ms step_avg:142.68ms
step:505/1370 train_time:70632ms step_avg:142.69ms
step:506/1370 train_time:70778ms step_avg:142.70ms
step:507/1370 train_time:70927ms step_avg:142.71ms
step:508/1370 train_time:71077ms step_avg:142.72ms
step:509/1370 train_time:71225ms step_avg:142.73ms
step:510/1370 train_time:71371ms step_avg:142.74ms
step:511/1370 train_time:71520ms step_avg:142.75ms
step:512/1370 train_time:71668ms step_avg:142.76ms
step:513/1370 train_time:71819ms step_avg:142.78ms
step:514/1370 train_time:71966ms step_avg:142.79ms
step:515/1370 train_time:72116ms step_avg:142.80ms
step:516/1370 train_time:72265ms step_avg:142.82ms
step:517/1370 train_time:72413ms step_avg:142.83ms
step:518/1370 train_time:72560ms step_avg:142.83ms
step:519/1370 train_time:72708ms step_avg:142.85ms
step:520/1370 train_time:72859ms step_avg:142.86ms
step:521/1370 train_time:73005ms step_avg:142.87ms
step:522/1370 train_time:73154ms step_avg:142.88ms
step:523/1370 train_time:73302ms step_avg:142.89ms
step:524/1370 train_time:73449ms step_avg:142.90ms
step:525/1370 train_time:73599ms step_avg:142.91ms
step:526/1370 train_time:73746ms step_avg:142.92ms
step:527/1370 train_time:73896ms step_avg:142.93ms
step:528/1370 train_time:74044ms step_avg:142.94ms
step:529/1370 train_time:74195ms step_avg:142.96ms
step:530/1370 train_time:74343ms step_avg:142.97ms
step:531/1370 train_time:74491ms step_avg:142.98ms
step:532/1370 train_time:74641ms step_avg:142.99ms
step:533/1370 train_time:74789ms step_avg:143.00ms
step:534/1370 train_time:74939ms step_avg:143.01ms
step:535/1370 train_time:75086ms step_avg:143.02ms
step:536/1370 train_time:75237ms step_avg:143.04ms
step:537/1370 train_time:75384ms step_avg:143.04ms
step:538/1370 train_time:75533ms step_avg:143.05ms
step:539/1370 train_time:75681ms step_avg:143.06ms
step:540/1370 train_time:75829ms step_avg:143.07ms
step:541/1370 train_time:75978ms step_avg:143.08ms
step:542/1370 train_time:76127ms step_avg:143.10ms
step:543/1370 train_time:76276ms step_avg:143.11ms
step:544/1370 train_time:76423ms step_avg:143.11ms
step:545/1370 train_time:76570ms step_avg:143.12ms
step:546/1370 train_time:76721ms step_avg:143.14ms
step:547/1370 train_time:76868ms step_avg:143.14ms
step:548/1370 train_time:77018ms step_avg:143.16ms
step:549/1370 train_time:77165ms step_avg:143.16ms
step:550/1370 train_time:77317ms step_avg:143.18ms
step:551/1370 train_time:77464ms step_avg:143.19ms
step:552/1370 train_time:77614ms step_avg:143.20ms
step:553/1370 train_time:77762ms step_avg:143.21ms
step:554/1370 train_time:77911ms step_avg:143.22ms
step:555/1370 train_time:78061ms step_avg:143.23ms
step:556/1370 train_time:78210ms step_avg:143.24ms
step:557/1370 train_time:78360ms step_avg:143.25ms
step:558/1370 train_time:78507ms step_avg:143.26ms
step:559/1370 train_time:78656ms step_avg:143.27ms
step:560/1370 train_time:78804ms step_avg:143.28ms
step:561/1370 train_time:78952ms step_avg:143.29ms
step:562/1370 train_time:79102ms step_avg:143.30ms
step:563/1370 train_time:79250ms step_avg:143.31ms
step:564/1370 train_time:79401ms step_avg:143.32ms
step:565/1370 train_time:79549ms step_avg:143.33ms
step:566/1370 train_time:79699ms step_avg:143.34ms
step:567/1370 train_time:79846ms step_avg:143.35ms
step:568/1370 train_time:79996ms step_avg:143.36ms
step:569/1370 train_time:80144ms step_avg:143.37ms
step:570/1370 train_time:80292ms step_avg:143.38ms
step:571/1370 train_time:80475ms step_avg:143.45ms
step:572/1370 train_time:80636ms step_avg:143.48ms
step:573/1370 train_time:80783ms step_avg:143.49ms
step:574/1370 train_time:80934ms step_avg:143.50ms
step:575/1370 train_time:81080ms step_avg:143.50ms
step:576/1370 train_time:81227ms step_avg:143.51ms
step:577/1370 train_time:81378ms step_avg:143.52ms
step:578/1370 train_time:81529ms step_avg:143.54ms
step:579/1370 train_time:81678ms step_avg:143.55ms
step:580/1370 train_time:81826ms step_avg:143.55ms
step:581/1370 train_time:81974ms step_avg:143.56ms
step:582/1370 train_time:82122ms step_avg:143.57ms
step:583/1370 train_time:82267ms step_avg:143.57ms
step:584/1370 train_time:82419ms step_avg:143.59ms
step:585/1370 train_time:82567ms step_avg:143.60ms
step:586/1370 train_time:82719ms step_avg:143.61ms
step:587/1370 train_time:82866ms step_avg:143.62ms
step:588/1370 train_time:83017ms step_avg:143.63ms
step:589/1370 train_time:83164ms step_avg:143.63ms
step:590/1370 train_time:83312ms step_avg:143.64ms
step:591/1370 train_time:83461ms step_avg:143.65ms
step:592/1370 train_time:83610ms step_avg:143.66ms
step:593/1370 train_time:83761ms step_avg:143.67ms
step:594/1370 train_time:83908ms step_avg:143.68ms
step:595/1370 train_time:84057ms step_avg:143.69ms
step:596/1370 train_time:84204ms step_avg:143.69ms
step:597/1370 train_time:84351ms step_avg:143.70ms
step:598/1370 train_time:84502ms step_avg:143.71ms
step:599/1370 train_time:84650ms step_avg:143.72ms
step:600/1370 train_time:84799ms step_avg:143.73ms
step:601/1370 train_time:84949ms step_avg:143.74ms
step:602/1370 train_time:85099ms step_avg:143.75ms
step:603/1370 train_time:85246ms step_avg:143.75ms
step:604/1370 train_time:85395ms step_avg:143.76ms
step:605/1370 train_time:85544ms step_avg:143.77ms
step:606/1370 train_time:85693ms step_avg:143.78ms
step:607/1370 train_time:85842ms step_avg:143.79ms
step:608/1370 train_time:85990ms step_avg:143.80ms
step:609/1370 train_time:86139ms step_avg:143.80ms
step:610/1370 train_time:86285ms step_avg:143.81ms
step:611/1370 train_time:86435ms step_avg:143.82ms
step:612/1370 train_time:86584ms step_avg:143.83ms
step:613/1370 train_time:86734ms step_avg:143.84ms
step:614/1370 train_time:86882ms step_avg:143.85ms
step:615/1370 train_time:87034ms step_avg:143.86ms
step:616/1370 train_time:87183ms step_avg:143.87ms
step:617/1370 train_time:87333ms step_avg:143.88ms
step:618/1370 train_time:87482ms step_avg:143.88ms
step:619/1370 train_time:87632ms step_avg:143.90ms
step:620/1370 train_time:87781ms step_avg:143.90ms
step:621/1370 train_time:87931ms step_avg:143.91ms
step:622/1370 train_time:88081ms step_avg:143.92ms
step:623/1370 train_time:88230ms step_avg:143.93ms
step:624/1370 train_time:88381ms step_avg:143.94ms
step:625/1370 train_time:88531ms step_avg:143.95ms
step:625/1370 val_loss:3.5770 train_time:88601ms step_avg:144.07ms
step:626/1370 train_time:88682ms step_avg:143.96ms
step:627/1370 train_time:88833ms step_avg:143.98ms
step:628/1370 train_time:88980ms step_avg:143.98ms
step:629/1370 train_time:89130ms step_avg:143.99ms
step:630/1370 train_time:89278ms step_avg:144.00ms
step:631/1370 train_time:89425ms step_avg:144.00ms
step:632/1370 train_time:89577ms step_avg:144.01ms
step:633/1370 train_time:89727ms step_avg:144.02ms
step:634/1370 train_time:89878ms step_avg:144.03ms
step:635/1370 train_time:90027ms step_avg:144.04ms
step:636/1370 train_time:90176ms step_avg:144.05ms
step:637/1370 train_time:90325ms step_avg:144.06ms
step:638/1370 train_time:90475ms step_avg:144.07ms
step:639/1370 train_time:90625ms step_avg:144.08ms
step:640/1370 train_time:90777ms step_avg:144.09ms
step:641/1370 train_time:90927ms step_avg:144.10ms
step:642/1370 train_time:91076ms step_avg:144.11ms
step:643/1370 train_time:91227ms step_avg:144.12ms
step:644/1370 train_time:91376ms step_avg:144.13ms
step:645/1370 train_time:91528ms step_avg:144.14ms
step:646/1370 train_time:91677ms step_avg:144.15ms
step:647/1370 train_time:91826ms step_avg:144.15ms
step:648/1370 train_time:91980ms step_avg:144.17ms
step:649/1370 train_time:92131ms step_avg:144.18ms
step:650/1370 train_time:92282ms step_avg:144.19ms
step:651/1370 train_time:92432ms step_avg:144.20ms
step:652/1370 train_time:92581ms step_avg:144.21ms
step:653/1370 train_time:92732ms step_avg:144.22ms
step:654/1370 train_time:92884ms step_avg:144.23ms
step:655/1370 train_time:93035ms step_avg:144.24ms
step:656/1370 train_time:93185ms step_avg:144.25ms
step:657/1370 train_time:93334ms step_avg:144.26ms
step:658/1370 train_time:93486ms step_avg:144.27ms
step:659/1370 train_time:93635ms step_avg:144.28ms
step:660/1370 train_time:93785ms step_avg:144.28ms
step:661/1370 train_time:93936ms step_avg:144.29ms
step:662/1370 train_time:94086ms step_avg:144.30ms
step:663/1370 train_time:94235ms step_avg:144.31ms
step:664/1370 train_time:94388ms step_avg:144.32ms
step:665/1370 train_time:94537ms step_avg:144.33ms
step:666/1370 train_time:94688ms step_avg:144.34ms
step:667/1370 train_time:94837ms step_avg:144.35ms
step:668/1370 train_time:94990ms step_avg:144.36ms
step:669/1370 train_time:95139ms step_avg:144.37ms
step:670/1370 train_time:95292ms step_avg:144.38ms
step:671/1370 train_time:95441ms step_avg:144.39ms
step:672/1370 train_time:95592ms step_avg:144.40ms
step:673/1370 train_time:95740ms step_avg:144.40ms
step:674/1370 train_time:95893ms step_avg:144.42ms
step:675/1370 train_time:96042ms step_avg:144.42ms
step:676/1370 train_time:96194ms step_avg:144.44ms
step:677/1370 train_time:96343ms step_avg:144.44ms
step:678/1370 train_time:96494ms step_avg:144.45ms
step:679/1370 train_time:96644ms step_avg:144.46ms
step:680/1370 train_time:96794ms step_avg:144.47ms
step:681/1370 train_time:96943ms step_avg:144.48ms
step:682/1370 train_time:97096ms step_avg:144.49ms
step:683/1370 train_time:97245ms step_avg:144.49ms
step:684/1370 train_time:97396ms step_avg:144.50ms
step:685/1370 train_time:97546ms step_avg:144.51ms
step:686/1370 train_time:97696ms step_avg:144.52ms
step:687/1370 train_time:97844ms step_avg:144.53ms
step:688/1370 train_time:97997ms step_avg:144.54ms
step:689/1370 train_time:98147ms step_avg:144.55ms
step:690/1370 train_time:98299ms step_avg:144.56ms
step:691/1370 train_time:98451ms step_avg:144.57ms
step:692/1370 train_time:98601ms step_avg:144.58ms
step:693/1370 train_time:98752ms step_avg:144.58ms
step:694/1370 train_time:98899ms step_avg:144.59ms
step:695/1370 train_time:99051ms step_avg:144.60ms
step:696/1370 train_time:99199ms step_avg:144.61ms
step:697/1370 train_time:99351ms step_avg:144.62ms
step:698/1370 train_time:99499ms step_avg:144.62ms
step:699/1370 train_time:99651ms step_avg:144.63ms
step:700/1370 train_time:99799ms step_avg:144.64ms
step:701/1370 train_time:99952ms step_avg:144.65ms
step:702/1370 train_time:100101ms step_avg:144.66ms
step:703/1370 train_time:100252ms step_avg:144.66ms
step:704/1370 train_time:100401ms step_avg:144.67ms
step:705/1370 train_time:100553ms step_avg:144.68ms
step:706/1370 train_time:100705ms step_avg:144.69ms
step:707/1370 train_time:100855ms step_avg:144.70ms
step:708/1370 train_time:101005ms step_avg:144.71ms
step:709/1370 train_time:101157ms step_avg:144.72ms
step:710/1370 train_time:101308ms step_avg:144.73ms
step:711/1370 train_time:101458ms step_avg:144.73ms
step:712/1370 train_time:101611ms step_avg:144.74ms
step:713/1370 train_time:101761ms step_avg:144.75ms
step:714/1370 train_time:101912ms step_avg:144.76ms
step:715/1370 train_time:102062ms step_avg:144.77ms
step:716/1370 train_time:102215ms step_avg:144.78ms
step:717/1370 train_time:102366ms step_avg:144.79ms
step:718/1370 train_time:102516ms step_avg:144.80ms
step:719/1370 train_time:102667ms step_avg:144.80ms
step:720/1370 train_time:102818ms step_avg:144.81ms
step:721/1370 train_time:102969ms step_avg:144.82ms
step:722/1370 train_time:103119ms step_avg:144.83ms
step:723/1370 train_time:103272ms step_avg:144.84ms
step:724/1370 train_time:103422ms step_avg:144.85ms
step:725/1370 train_time:103574ms step_avg:144.86ms
step:726/1370 train_time:103724ms step_avg:144.87ms
step:727/1370 train_time:103879ms step_avg:144.88ms
step:728/1370 train_time:104031ms step_avg:144.89ms
step:729/1370 train_time:104179ms step_avg:144.89ms
step:730/1370 train_time:104331ms step_avg:144.90ms
step:731/1370 train_time:104482ms step_avg:144.91ms
step:732/1370 train_time:104632ms step_avg:144.92ms
step:733/1370 train_time:104785ms step_avg:144.93ms
step:734/1370 train_time:104937ms step_avg:144.94ms
step:735/1370 train_time:105093ms step_avg:144.96ms
step:736/1370 train_time:105243ms step_avg:144.96ms
step:737/1370 train_time:105395ms step_avg:144.97ms
step:738/1370 train_time:105545ms step_avg:144.98ms
step:739/1370 train_time:105698ms step_avg:144.99ms
step:740/1370 train_time:105850ms step_avg:145.00ms
step:741/1370 train_time:106001ms step_avg:145.01ms
step:742/1370 train_time:106153ms step_avg:145.02ms
step:743/1370 train_time:106303ms step_avg:145.02ms
step:744/1370 train_time:106455ms step_avg:145.03ms
step:745/1370 train_time:106607ms step_avg:145.04ms
step:746/1370 train_time:106757ms step_avg:145.05ms
step:747/1370 train_time:106907ms step_avg:145.06ms
step:748/1370 train_time:107057ms step_avg:145.06ms
step:749/1370 train_time:107209ms step_avg:145.07ms
step:750/1370 train_time:107358ms step_avg:145.08ms
step:750/1370 val_loss:3.5234 train_time:107431ms step_avg:145.18ms
step:751/1370 train_time:107515ms step_avg:145.09ms
step:752/1370 train_time:107668ms step_avg:145.11ms
step:753/1370 train_time:107819ms step_avg:145.11ms
step:754/1370 train_time:107968ms step_avg:145.12ms
step:755/1370 train_time:108117ms step_avg:145.12ms
step:756/1370 train_time:108268ms step_avg:145.13ms
step:757/1370 train_time:108422ms step_avg:145.14ms
step:758/1370 train_time:108574ms step_avg:145.15ms
step:759/1370 train_time:108726ms step_avg:145.16ms
step:760/1370 train_time:108874ms step_avg:145.17ms
step:761/1370 train_time:109061ms step_avg:145.22ms
step:762/1370 train_time:109221ms step_avg:145.24ms
step:763/1370 train_time:109370ms step_avg:145.25ms
step:764/1370 train_time:109521ms step_avg:145.25ms
step:765/1370 train_time:109670ms step_avg:145.26ms
step:766/1370 train_time:109823ms step_avg:145.27ms
step:767/1370 train_time:109977ms step_avg:145.28ms
step:768/1370 train_time:110130ms step_avg:145.29ms
step:769/1370 train_time:110283ms step_avg:145.30ms
step:770/1370 train_time:110433ms step_avg:145.31ms
step:771/1370 train_time:110583ms step_avg:145.31ms
step:772/1370 train_time:110733ms step_avg:145.32ms
step:773/1370 train_time:110884ms step_avg:145.33ms
step:774/1370 train_time:111036ms step_avg:145.34ms
step:775/1370 train_time:111189ms step_avg:145.34ms
step:776/1370 train_time:111344ms step_avg:145.36ms
step:777/1370 train_time:111494ms step_avg:145.36ms
step:778/1370 train_time:111644ms step_avg:145.37ms
step:779/1370 train_time:111793ms step_avg:145.37ms
step:780/1370 train_time:111947ms step_avg:145.39ms
step:781/1370 train_time:112100ms step_avg:145.39ms
step:782/1370 train_time:112251ms step_avg:145.40ms
step:783/1370 train_time:112404ms step_avg:145.41ms
step:784/1370 train_time:112556ms step_avg:145.42ms
step:785/1370 train_time:112707ms step_avg:145.43ms
step:786/1370 train_time:112858ms step_avg:145.43ms
step:787/1370 train_time:113008ms step_avg:145.44ms
step:788/1370 train_time:113161ms step_avg:145.45ms
step:789/1370 train_time:113311ms step_avg:145.46ms
step:790/1370 train_time:113464ms step_avg:145.47ms
step:791/1370 train_time:113616ms step_avg:145.47ms
step:792/1370 train_time:113767ms step_avg:145.48ms
step:793/1370 train_time:113917ms step_avg:145.49ms
step:794/1370 train_time:114070ms step_avg:145.50ms
step:795/1370 train_time:114224ms step_avg:145.51ms
step:796/1370 train_time:114374ms step_avg:145.51ms
step:797/1370 train_time:114528ms step_avg:145.52ms
step:798/1370 train_time:114678ms step_avg:145.53ms
step:799/1370 train_time:114832ms step_avg:145.54ms
step:800/1370 train_time:114983ms step_avg:145.55ms
step:801/1370 train_time:115133ms step_avg:145.55ms
step:802/1370 train_time:115285ms step_avg:145.56ms
step:803/1370 train_time:115434ms step_avg:145.57ms
step:804/1370 train_time:115587ms step_avg:145.58ms
step:805/1370 train_time:115741ms step_avg:145.59ms
step:806/1370 train_time:115892ms step_avg:145.59ms
step:807/1370 train_time:116043ms step_avg:145.60ms
step:808/1370 train_time:116194ms step_avg:145.61ms
step:809/1370 train_time:116346ms step_avg:145.61ms
step:810/1370 train_time:116497ms step_avg:145.62ms
step:811/1370 train_time:116649ms step_avg:145.63ms
step:812/1370 train_time:116801ms step_avg:145.64ms
step:813/1370 train_time:116952ms step_avg:145.64ms
step:814/1370 train_time:117107ms step_avg:145.66ms
step:815/1370 train_time:117258ms step_avg:145.66ms
step:816/1370 train_time:117410ms step_avg:145.67ms
step:817/1370 train_time:117564ms step_avg:145.68ms
step:818/1370 train_time:117714ms step_avg:145.69ms
step:819/1370 train_time:117868ms step_avg:145.70ms
step:820/1370 train_time:118022ms step_avg:145.71ms
step:821/1370 train_time:118175ms step_avg:145.71ms
step:822/1370 train_time:118329ms step_avg:145.73ms
step:823/1370 train_time:118481ms step_avg:145.73ms
step:824/1370 train_time:118631ms step_avg:145.74ms
step:825/1370 train_time:118786ms step_avg:145.75ms
step:826/1370 train_time:118939ms step_avg:145.76ms
step:827/1370 train_time:119091ms step_avg:145.77ms
step:828/1370 train_time:119247ms step_avg:145.78ms
step:829/1370 train_time:119397ms step_avg:145.78ms
step:830/1370 train_time:119550ms step_avg:145.79ms
step:831/1370 train_time:119703ms step_avg:145.80ms
step:832/1370 train_time:119855ms step_avg:145.81ms
step:833/1370 train_time:120007ms step_avg:145.82ms
step:834/1370 train_time:120162ms step_avg:145.83ms
step:835/1370 train_time:120315ms step_avg:145.84ms
step:836/1370 train_time:120471ms step_avg:145.85ms
step:837/1370 train_time:120624ms step_avg:145.86ms
step:838/1370 train_time:120775ms step_avg:145.86ms
step:839/1370 train_time:120928ms step_avg:145.87ms
step:840/1370 train_time:121079ms step_avg:145.88ms
step:841/1370 train_time:121232ms step_avg:145.89ms
step:842/1370 train_time:121385ms step_avg:145.90ms
step:843/1370 train_time:121536ms step_avg:145.90ms
step:844/1370 train_time:121689ms step_avg:145.91ms
step:845/1370 train_time:121844ms step_avg:145.92ms
step:846/1370 train_time:121996ms step_avg:145.93ms
step:847/1370 train_time:122150ms step_avg:145.94ms
step:848/1370 train_time:122302ms step_avg:145.94ms
step:849/1370 train_time:122454ms step_avg:145.95ms
step:850/1370 train_time:122609ms step_avg:145.96ms
step:851/1370 train_time:122761ms step_avg:145.97ms
step:852/1370 train_time:122913ms step_avg:145.98ms
step:853/1370 train_time:123066ms step_avg:145.99ms
step:854/1370 train_time:123217ms step_avg:145.99ms
step:855/1370 train_time:123370ms step_avg:146.00ms
step:856/1370 train_time:123523ms step_avg:146.01ms
step:857/1370 train_time:123675ms step_avg:146.02ms
step:858/1370 train_time:123832ms step_avg:146.03ms
step:859/1370 train_time:123985ms step_avg:146.04ms
step:860/1370 train_time:124138ms step_avg:146.04ms
step:861/1370 train_time:124290ms step_avg:146.05ms
step:862/1370 train_time:124444ms step_avg:146.06ms
step:863/1370 train_time:124597ms step_avg:146.07ms
step:864/1370 train_time:124751ms step_avg:146.08ms
step:865/1370 train_time:124903ms step_avg:146.09ms
step:866/1370 train_time:125059ms step_avg:146.10ms
step:867/1370 train_time:125211ms step_avg:146.10ms
step:868/1370 train_time:125364ms step_avg:146.11ms
step:869/1370 train_time:125516ms step_avg:146.12ms
step:870/1370 train_time:125672ms step_avg:146.13ms
step:871/1370 train_time:125825ms step_avg:146.14ms
step:872/1370 train_time:125976ms step_avg:146.14ms
step:873/1370 train_time:126127ms step_avg:146.15ms
step:874/1370 train_time:126281ms step_avg:146.16ms
step:875/1370 train_time:126433ms step_avg:146.17ms
step:875/1370 val_loss:3.4695 train_time:126504ms step_avg:146.25ms
step:876/1370 train_time:126586ms step_avg:146.17ms
step:877/1370 train_time:126740ms step_avg:146.18ms
step:878/1370 train_time:126891ms step_avg:146.19ms
step:879/1370 train_time:127045ms step_avg:146.20ms
step:880/1370 train_time:127195ms step_avg:146.20ms
step:881/1370 train_time:127345ms step_avg:146.21ms
step:882/1370 train_time:127502ms step_avg:146.22ms
step:883/1370 train_time:127654ms step_avg:146.22ms
step:884/1370 train_time:127807ms step_avg:146.23ms
step:885/1370 train_time:127960ms step_avg:146.24ms
step:886/1370 train_time:128113ms step_avg:146.25ms
step:887/1370 train_time:128264ms step_avg:146.25ms
step:888/1370 train_time:128417ms step_avg:146.26ms
step:889/1370 train_time:128573ms step_avg:146.27ms
step:890/1370 train_time:128724ms step_avg:146.28ms
step:891/1370 train_time:128878ms step_avg:146.29ms
step:892/1370 train_time:129031ms step_avg:146.29ms
step:893/1370 train_time:129184ms step_avg:146.30ms
step:894/1370 train_time:129337ms step_avg:146.31ms
step:895/1370 train_time:129490ms step_avg:146.32ms
step:896/1370 train_time:129644ms step_avg:146.32ms
step:897/1370 train_time:129795ms step_avg:146.33ms
step:898/1370 train_time:129948ms step_avg:146.34ms
step:899/1370 train_time:130102ms step_avg:146.35ms
step:900/1370 train_time:130254ms step_avg:146.35ms
step:901/1370 train_time:130408ms step_avg:146.36ms
step:902/1370 train_time:130560ms step_avg:146.37ms
step:903/1370 train_time:130713ms step_avg:146.37ms
step:904/1370 train_time:130867ms step_avg:146.38ms
step:905/1370 train_time:131019ms step_avg:146.39ms
step:906/1370 train_time:131172ms step_avg:146.40ms
step:907/1370 train_time:131327ms step_avg:146.41ms
step:908/1370 train_time:131480ms step_avg:146.41ms
step:909/1370 train_time:131632ms step_avg:146.42ms
step:910/1370 train_time:131790ms step_avg:146.43ms
step:911/1370 train_time:131943ms step_avg:146.44ms
step:912/1370 train_time:132096ms step_avg:146.45ms
step:913/1370 train_time:132249ms step_avg:146.46ms
step:914/1370 train_time:132402ms step_avg:146.46ms
step:915/1370 train_time:132557ms step_avg:146.47ms
step:916/1370 train_time:132709ms step_avg:146.48ms
step:917/1370 train_time:132863ms step_avg:146.49ms
step:918/1370 train_time:133017ms step_avg:146.49ms
step:919/1370 train_time:133176ms step_avg:146.51ms
step:920/1370 train_time:133329ms step_avg:146.52ms
step:921/1370 train_time:133485ms step_avg:146.53ms
step:922/1370 train_time:133643ms step_avg:146.54ms
step:923/1370 train_time:133794ms step_avg:146.54ms
step:924/1370 train_time:133948ms step_avg:146.55ms
step:925/1370 train_time:134106ms step_avg:146.56ms
step:926/1370 train_time:134258ms step_avg:146.57ms
step:927/1370 train_time:134412ms step_avg:146.58ms
step:928/1370 train_time:134566ms step_avg:146.59ms
step:929/1370 train_time:134720ms step_avg:146.59ms
step:930/1370 train_time:134876ms step_avg:146.60ms
step:931/1370 train_time:135027ms step_avg:146.61ms
step:932/1370 train_time:135183ms step_avg:146.62ms
step:933/1370 train_time:135337ms step_avg:146.63ms
step:934/1370 train_time:135490ms step_avg:146.63ms
step:935/1370 train_time:135645ms step_avg:146.64ms
step:936/1370 train_time:135799ms step_avg:146.65ms
step:937/1370 train_time:135959ms step_avg:146.67ms
step:938/1370 train_time:136112ms step_avg:146.67ms
step:939/1370 train_time:136267ms step_avg:146.68ms
step:940/1370 train_time:136421ms step_avg:146.69ms
step:941/1370 train_time:136574ms step_avg:146.70ms
step:942/1370 train_time:136726ms step_avg:146.70ms
step:943/1370 train_time:136882ms step_avg:146.71ms
step:944/1370 train_time:137040ms step_avg:146.72ms
step:945/1370 train_time:137195ms step_avg:146.73ms
step:946/1370 train_time:137349ms step_avg:146.74ms
step:947/1370 train_time:137502ms step_avg:146.75ms
step:948/1370 train_time:137656ms step_avg:146.75ms
step:949/1370 train_time:137810ms step_avg:146.76ms
step:950/1370 train_time:137966ms step_avg:146.77ms
step:951/1370 train_time:138158ms step_avg:146.82ms
step:952/1370 train_time:138309ms step_avg:146.82ms
step:953/1370 train_time:138465ms step_avg:146.83ms
step:954/1370 train_time:138616ms step_avg:146.84ms
step:955/1370 train_time:138768ms step_avg:146.84ms
step:956/1370 train_time:138921ms step_avg:146.85ms
step:957/1370 train_time:139079ms step_avg:146.86ms
step:958/1370 train_time:139237ms step_avg:146.87ms
step:959/1370 train_time:139392ms step_avg:146.88ms
step:960/1370 train_time:139547ms step_avg:146.89ms
step:961/1370 train_time:139701ms step_avg:146.90ms
step:962/1370 train_time:139853ms step_avg:146.90ms
step:963/1370 train_time:140010ms step_avg:146.91ms
step:964/1370 train_time:140164ms step_avg:146.92ms
step:965/1370 train_time:140316ms step_avg:146.93ms
step:966/1370 train_time:140470ms step_avg:146.94ms
step:967/1370 train_time:140623ms step_avg:146.94ms
step:968/1370 train_time:140773ms step_avg:146.94ms
step:969/1370 train_time:140928ms step_avg:146.95ms
step:970/1370 train_time:141084ms step_avg:146.96ms
step:971/1370 train_time:141239ms step_avg:146.97ms
step:972/1370 train_time:141391ms step_avg:146.98ms
step:973/1370 train_time:141544ms step_avg:146.98ms
step:974/1370 train_time:141697ms step_avg:146.99ms
step:975/1370 train_time:141851ms step_avg:147.00ms
step:976/1370 train_time:142006ms step_avg:147.00ms
step:977/1370 train_time:142160ms step_avg:147.01ms
step:978/1370 train_time:142312ms step_avg:147.02ms
step:979/1370 train_time:142465ms step_avg:147.02ms
step:980/1370 train_time:142617ms step_avg:147.03ms
step:981/1370 train_time:142767ms step_avg:147.03ms
step:982/1370 train_time:142920ms step_avg:147.04ms
step:983/1370 train_time:143073ms step_avg:147.04ms
step:984/1370 train_time:143226ms step_avg:147.05ms
step:985/1370 train_time:143381ms step_avg:147.06ms
step:986/1370 train_time:143541ms step_avg:147.07ms
step:987/1370 train_time:143692ms step_avg:147.08ms
step:988/1370 train_time:143845ms step_avg:147.08ms
step:989/1370 train_time:143996ms step_avg:147.08ms
step:990/1370 train_time:144149ms step_avg:147.09ms
step:991/1370 train_time:144304ms step_avg:147.10ms
step:992/1370 train_time:144460ms step_avg:147.11ms
step:993/1370 train_time:144622ms step_avg:147.12ms
step:994/1370 train_time:144774ms step_avg:147.13ms
step:995/1370 train_time:144926ms step_avg:147.13ms
step:996/1370 train_time:145078ms step_avg:147.14ms
step:997/1370 train_time:145230ms step_avg:147.14ms
step:998/1370 train_time:145385ms step_avg:147.15ms
step:999/1370 train_time:145539ms step_avg:147.16ms
step:1000/1370 train_time:145692ms step_avg:147.16ms
step:1000/1370 val_loss:3.4029 train_time:145764ms step_avg:147.24ms
step:1001/1370 train_time:145847ms step_avg:147.17ms
step:1002/1370 train_time:146004ms step_avg:147.18ms
step:1003/1370 train_time:146158ms step_avg:147.19ms
step:1004/1370 train_time:146314ms step_avg:147.20ms
step:1005/1370 train_time:146466ms step_avg:147.20ms
step:1006/1370 train_time:146618ms step_avg:147.21ms
step:1007/1370 train_time:146776ms step_avg:147.22ms
step:1008/1370 train_time:146931ms step_avg:147.22ms
step:1009/1370 train_time:147092ms step_avg:147.24ms
step:1010/1370 train_time:147245ms step_avg:147.24ms
step:1011/1370 train_time:147399ms step_avg:147.25ms
step:1012/1370 train_time:147549ms step_avg:147.25ms
step:1013/1370 train_time:147704ms step_avg:147.26ms
step:1014/1370 train_time:147856ms step_avg:147.27ms
step:1015/1370 train_time:148009ms step_avg:147.27ms
step:1016/1370 train_time:148163ms step_avg:147.28ms
step:1017/1370 train_time:148319ms step_avg:147.29ms
step:1018/1370 train_time:148471ms step_avg:147.29ms
step:1019/1370 train_time:148627ms step_avg:147.30ms
step:1020/1370 train_time:148784ms step_avg:147.31ms
step:1021/1370 train_time:148935ms step_avg:147.31ms
step:1022/1370 train_time:149089ms step_avg:147.32ms
step:1023/1370 train_time:149245ms step_avg:147.33ms
step:1024/1370 train_time:149401ms step_avg:147.34ms
step:1025/1370 train_time:149555ms step_avg:147.35ms
step:1026/1370 train_time:149709ms step_avg:147.35ms
step:1027/1370 train_time:149863ms step_avg:147.36ms
step:1028/1370 train_time:150020ms step_avg:147.37ms
step:1029/1370 train_time:150178ms step_avg:147.38ms
step:1030/1370 train_time:150331ms step_avg:147.38ms
step:1031/1370 train_time:150485ms step_avg:147.39ms
step:1032/1370 train_time:150640ms step_avg:147.40ms
step:1033/1370 train_time:150793ms step_avg:147.40ms
step:1034/1370 train_time:150948ms step_avg:147.41ms
step:1035/1370 train_time:151106ms step_avg:147.42ms
step:1036/1370 train_time:151260ms step_avg:147.43ms
step:1037/1370 train_time:151417ms step_avg:147.44ms
step:1038/1370 train_time:151572ms step_avg:147.44ms
step:1039/1370 train_time:151725ms step_avg:147.45ms
step:1040/1370 train_time:151878ms step_avg:147.45ms
step:1041/1370 train_time:152031ms step_avg:147.46ms
step:1042/1370 train_time:152185ms step_avg:147.47ms
step:1043/1370 train_time:152339ms step_avg:147.47ms
step:1044/1370 train_time:152497ms step_avg:147.48ms
step:1045/1370 train_time:152652ms step_avg:147.49ms
step:1046/1370 train_time:152805ms step_avg:147.50ms
step:1047/1370 train_time:152959ms step_avg:147.50ms
step:1048/1370 train_time:153114ms step_avg:147.51ms
step:1049/1370 train_time:153270ms step_avg:147.52ms
step:1050/1370 train_time:153429ms step_avg:147.53ms
step:1051/1370 train_time:153588ms step_avg:147.54ms
step:1052/1370 train_time:153743ms step_avg:147.55ms
step:1053/1370 train_time:153895ms step_avg:147.55ms
step:1054/1370 train_time:154050ms step_avg:147.56ms
step:1055/1370 train_time:154204ms step_avg:147.56ms
step:1056/1370 train_time:154359ms step_avg:147.57ms
step:1057/1370 train_time:154513ms step_avg:147.58ms
step:1058/1370 train_time:154672ms step_avg:147.59ms
step:1059/1370 train_time:154828ms step_avg:147.60ms
step:1060/1370 train_time:154984ms step_avg:147.60ms
step:1061/1370 train_time:155137ms step_avg:147.61ms
step:1062/1370 train_time:155294ms step_avg:147.62ms
step:1063/1370 train_time:155448ms step_avg:147.62ms
step:1064/1370 train_time:155603ms step_avg:147.63ms
step:1065/1370 train_time:155758ms step_avg:147.64ms
step:1066/1370 train_time:155917ms step_avg:147.65ms
step:1067/1370 train_time:156075ms step_avg:147.66ms
step:1068/1370 train_time:156228ms step_avg:147.66ms
step:1069/1370 train_time:156388ms step_avg:147.67ms
step:1070/1370 train_time:156542ms step_avg:147.68ms
step:1071/1370 train_time:156698ms step_avg:147.69ms
step:1072/1370 train_time:156850ms step_avg:147.69ms
step:1073/1370 train_time:157005ms step_avg:147.70ms
step:1074/1370 train_time:157158ms step_avg:147.71ms
step:1075/1370 train_time:157313ms step_avg:147.71ms
step:1076/1370 train_time:157468ms step_avg:147.72ms
step:1077/1370 train_time:157622ms step_avg:147.72ms
step:1078/1370 train_time:157781ms step_avg:147.73ms
step:1079/1370 train_time:157939ms step_avg:147.74ms
step:1080/1370 train_time:158095ms step_avg:147.75ms
step:1081/1370 train_time:158247ms step_avg:147.76ms
step:1082/1370 train_time:158402ms step_avg:147.76ms
step:1083/1370 train_time:158555ms step_avg:147.77ms
step:1084/1370 train_time:158713ms step_avg:147.78ms
step:1085/1370 train_time:158865ms step_avg:147.78ms
step:1086/1370 train_time:159023ms step_avg:147.79ms
step:1087/1370 train_time:159180ms step_avg:147.80ms
step:1088/1370 train_time:159333ms step_avg:147.80ms
step:1089/1370 train_time:159493ms step_avg:147.82ms
step:1090/1370 train_time:159654ms step_avg:147.83ms
step:1091/1370 train_time:159808ms step_avg:147.83ms
step:1092/1370 train_time:159962ms step_avg:147.84ms
step:1093/1370 train_time:160119ms step_avg:147.85ms
step:1094/1370 train_time:160272ms step_avg:147.85ms
step:1095/1370 train_time:160427ms step_avg:147.86ms
step:1096/1370 train_time:160586ms step_avg:147.87ms
step:1097/1370 train_time:160743ms step_avg:147.88ms
step:1098/1370 train_time:160897ms step_avg:147.88ms
step:1099/1370 train_time:161050ms step_avg:147.89ms
step:1100/1370 train_time:161205ms step_avg:147.89ms
step:1101/1370 train_time:161359ms step_avg:147.90ms
step:1102/1370 train_time:161519ms step_avg:147.91ms
step:1103/1370 train_time:161673ms step_avg:147.92ms
step:1104/1370 train_time:161828ms step_avg:147.92ms
step:1105/1370 train_time:161983ms step_avg:147.93ms
step:1106/1370 train_time:162136ms step_avg:147.93ms
step:1107/1370 train_time:162291ms step_avg:147.94ms
step:1108/1370 train_time:162448ms step_avg:147.95ms
step:1109/1370 train_time:162603ms step_avg:147.96ms
step:1110/1370 train_time:162757ms step_avg:147.96ms
step:1111/1370 train_time:162913ms step_avg:147.97ms
step:1112/1370 train_time:163067ms step_avg:147.97ms
step:1113/1370 train_time:163222ms step_avg:147.98ms
step:1114/1370 train_time:163377ms step_avg:147.99ms
step:1115/1370 train_time:163533ms step_avg:147.99ms
step:1116/1370 train_time:163686ms step_avg:148.00ms
step:1117/1370 train_time:163845ms step_avg:148.01ms
step:1118/1370 train_time:164005ms step_avg:148.02ms
step:1119/1370 train_time:164159ms step_avg:148.02ms
step:1120/1370 train_time:164316ms step_avg:148.03ms
step:1121/1370 train_time:164470ms step_avg:148.04ms
step:1122/1370 train_time:164626ms step_avg:148.05ms
step:1123/1370 train_time:164782ms step_avg:148.05ms
step:1124/1370 train_time:164942ms step_avg:148.06ms
step:1125/1370 train_time:165097ms step_avg:148.07ms
step:1125/1370 val_loss:3.3495 train_time:165169ms step_avg:148.13ms
step:1126/1370 train_time:165253ms step_avg:148.08ms
step:1127/1370 train_time:165411ms step_avg:148.08ms
step:1128/1370 train_time:165566ms step_avg:148.09ms
step:1129/1370 train_time:165727ms step_avg:148.10ms
step:1130/1370 train_time:165881ms step_avg:148.11ms
step:1131/1370 train_time:166039ms step_avg:148.12ms
step:1132/1370 train_time:166193ms step_avg:148.12ms
step:1133/1370 train_time:166352ms step_avg:148.13ms
step:1134/1370 train_time:166508ms step_avg:148.14ms
step:1135/1370 train_time:166661ms step_avg:148.14ms
step:1136/1370 train_time:166821ms step_avg:148.15ms
step:1137/1370 train_time:166975ms step_avg:148.16ms
step:1138/1370 train_time:167132ms step_avg:148.17ms
step:1139/1370 train_time:167287ms step_avg:148.17ms
step:1140/1370 train_time:167444ms step_avg:148.18ms
step:1141/1370 train_time:167635ms step_avg:148.22ms
step:1142/1370 train_time:167800ms step_avg:148.23ms
step:1143/1370 train_time:167960ms step_avg:148.24ms
step:1144/1370 train_time:168117ms step_avg:148.25ms
step:1145/1370 train_time:168270ms step_avg:148.26ms
step:1146/1370 train_time:168427ms step_avg:148.26ms
step:1147/1370 train_time:168584ms step_avg:148.27ms
step:1148/1370 train_time:168742ms step_avg:148.28ms
step:1149/1370 train_time:168898ms step_avg:148.29ms
step:1150/1370 train_time:169053ms step_avg:148.29ms
step:1151/1370 train_time:169210ms step_avg:148.30ms
step:1152/1370 train_time:169364ms step_avg:148.31ms
step:1153/1370 train_time:169522ms step_avg:148.31ms
step:1154/1370 train_time:169676ms step_avg:148.32ms
step:1155/1370 train_time:169834ms step_avg:148.33ms
step:1156/1370 train_time:169994ms step_avg:148.34ms
step:1157/1370 train_time:170153ms step_avg:148.35ms
step:1158/1370 train_time:170309ms step_avg:148.35ms
step:1159/1370 train_time:170462ms step_avg:148.36ms
step:1160/1370 train_time:170616ms step_avg:148.36ms
step:1161/1370 train_time:170773ms step_avg:148.37ms
step:1162/1370 train_time:170930ms step_avg:148.38ms
step:1163/1370 train_time:171086ms step_avg:148.38ms
step:1164/1370 train_time:171243ms step_avg:148.39ms
step:1165/1370 train_time:171396ms step_avg:148.39ms
step:1166/1370 train_time:171554ms step_avg:148.40ms
step:1167/1370 train_time:171708ms step_avg:148.41ms
step:1168/1370 train_time:171861ms step_avg:148.41ms
step:1169/1370 train_time:172018ms step_avg:148.42ms
step:1170/1370 train_time:172175ms step_avg:148.43ms
step:1171/1370 train_time:172333ms step_avg:148.44ms
step:1172/1370 train_time:172489ms step_avg:148.44ms
step:1173/1370 train_time:172644ms step_avg:148.45ms
step:1174/1370 train_time:172807ms step_avg:148.46ms
step:1175/1370 train_time:172963ms step_avg:148.47ms
step:1176/1370 train_time:173123ms step_avg:148.48ms
step:1177/1370 train_time:173289ms step_avg:148.49ms
step:1178/1370 train_time:173447ms step_avg:148.50ms
step:1179/1370 train_time:173602ms step_avg:148.50ms
step:1180/1370 train_time:173767ms step_avg:148.52ms
step:1181/1370 train_time:173922ms step_avg:148.52ms
step:1182/1370 train_time:174077ms step_avg:148.53ms
step:1183/1370 train_time:174233ms step_avg:148.54ms
step:1184/1370 train_time:174387ms step_avg:148.54ms
step:1185/1370 train_time:174545ms step_avg:148.55ms
step:1186/1370 train_time:174702ms step_avg:148.56ms
step:1187/1370 train_time:174863ms step_avg:148.57ms
step:1188/1370 train_time:175018ms step_avg:148.57ms
step:1189/1370 train_time:175175ms step_avg:148.58ms
step:1190/1370 train_time:175334ms step_avg:148.59ms
step:1191/1370 train_time:175489ms step_avg:148.59ms
step:1192/1370 train_time:175644ms step_avg:148.60ms
step:1193/1370 train_time:175799ms step_avg:148.60ms
step:1194/1370 train_time:175957ms step_avg:148.61ms
step:1195/1370 train_time:176112ms step_avg:148.62ms
step:1196/1370 train_time:176269ms step_avg:148.62ms
step:1197/1370 train_time:176426ms step_avg:148.63ms
step:1198/1370 train_time:176587ms step_avg:148.64ms
step:1199/1370 train_time:176741ms step_avg:148.65ms
step:1200/1370 train_time:176897ms step_avg:148.65ms
step:1201/1370 train_time:177056ms step_avg:148.66ms
step:1202/1370 train_time:177222ms step_avg:148.68ms
step:1203/1370 train_time:177381ms step_avg:148.68ms
step:1204/1370 train_time:177538ms step_avg:148.69ms
step:1205/1370 train_time:177693ms step_avg:148.70ms
step:1206/1370 train_time:177852ms step_avg:148.71ms
step:1207/1370 train_time:178007ms step_avg:148.71ms
step:1208/1370 train_time:178163ms step_avg:148.72ms
step:1209/1370 train_time:178319ms step_avg:148.72ms
step:1210/1370 train_time:178481ms step_avg:148.73ms
step:1211/1370 train_time:178636ms step_avg:148.74ms
step:1212/1370 train_time:178791ms step_avg:148.74ms
step:1213/1370 train_time:178948ms step_avg:148.75ms
step:1214/1370 train_time:179106ms step_avg:148.76ms
step:1215/1370 train_time:179261ms step_avg:148.76ms
step:1216/1370 train_time:179417ms step_avg:148.77ms
step:1217/1370 train_time:179574ms step_avg:148.78ms
step:1218/1370 train_time:179726ms step_avg:148.78ms
step:1219/1370 train_time:179879ms step_avg:148.78ms
step:1220/1370 train_time:180035ms step_avg:148.79ms
step:1221/1370 train_time:180192ms step_avg:148.80ms
step:1222/1370 train_time:180348ms step_avg:148.80ms
step:1223/1370 train_time:180504ms step_avg:148.81ms
step:1224/1370 train_time:180663ms step_avg:148.82ms
step:1225/1370 train_time:180821ms step_avg:148.82ms
step:1226/1370 train_time:180976ms step_avg:148.83ms
step:1227/1370 train_time:181135ms step_avg:148.84ms
step:1228/1370 train_time:181290ms step_avg:148.84ms
step:1229/1370 train_time:181446ms step_avg:148.85ms
step:1230/1370 train_time:181606ms step_avg:148.86ms
step:1231/1370 train_time:181765ms step_avg:148.87ms
step:1232/1370 train_time:181928ms step_avg:148.88ms
step:1233/1370 train_time:182083ms step_avg:148.88ms
step:1234/1370 train_time:182238ms step_avg:148.89ms
step:1235/1370 train_time:182394ms step_avg:148.89ms
step:1236/1370 train_time:182552ms step_avg:148.90ms
step:1237/1370 train_time:182706ms step_avg:148.90ms
step:1238/1370 train_time:182869ms step_avg:148.92ms
step:1239/1370 train_time:183027ms step_avg:148.92ms
step:1240/1370 train_time:183183ms step_avg:148.93ms
step:1241/1370 train_time:183347ms step_avg:148.94ms
step:1242/1370 train_time:183503ms step_avg:148.95ms
step:1243/1370 train_time:183661ms step_avg:148.95ms
step:1244/1370 train_time:183817ms step_avg:148.96ms
step:1245/1370 train_time:183973ms step_avg:148.97ms
step:1246/1370 train_time:184127ms step_avg:148.97ms
step:1247/1370 train_time:184286ms step_avg:148.98ms
step:1248/1370 train_time:184443ms step_avg:148.98ms
step:1249/1370 train_time:184598ms step_avg:148.99ms
step:1250/1370 train_time:184756ms step_avg:149.00ms
step:1250/1370 val_loss:3.3035 train_time:184830ms step_avg:149.06ms
step:1251/1370 train_time:184918ms step_avg:149.01ms
step:1252/1370 train_time:185075ms step_avg:149.01ms
step:1253/1370 train_time:185229ms step_avg:149.02ms
step:1254/1370 train_time:185385ms step_avg:149.02ms
step:1255/1370 train_time:185550ms step_avg:149.04ms
step:1256/1370 train_time:185706ms step_avg:149.04ms
step:1257/1370 train_time:185861ms step_avg:149.05ms
step:1258/1370 train_time:186020ms step_avg:149.05ms
step:1259/1370 train_time:186180ms step_avg:149.06ms
step:1260/1370 train_time:186335ms step_avg:149.07ms
step:1261/1370 train_time:186493ms step_avg:149.08ms
step:1262/1370 train_time:186653ms step_avg:149.08ms
step:1263/1370 train_time:186810ms step_avg:149.09ms
step:1264/1370 train_time:186965ms step_avg:149.09ms
step:1265/1370 train_time:187120ms step_avg:149.10ms
step:1266/1370 train_time:187277ms step_avg:149.11ms
step:1267/1370 train_time:187436ms step_avg:149.11ms
step:1268/1370 train_time:187594ms step_avg:149.12ms
step:1269/1370 train_time:187754ms step_avg:149.13ms
step:1270/1370 train_time:187911ms step_avg:149.14ms
step:1271/1370 train_time:188070ms step_avg:149.14ms
step:1272/1370 train_time:188225ms step_avg:149.15ms
step:1273/1370 train_time:188380ms step_avg:149.15ms
step:1274/1370 train_time:188535ms step_avg:149.16ms
step:1275/1370 train_time:188689ms step_avg:149.16ms
step:1276/1370 train_time:188844ms step_avg:149.17ms
step:1277/1370 train_time:189001ms step_avg:149.17ms
step:1278/1370 train_time:189156ms step_avg:149.18ms
step:1279/1370 train_time:189314ms step_avg:149.18ms
step:1280/1370 train_time:189478ms step_avg:149.19ms
step:1281/1370 train_time:189633ms step_avg:149.20ms
step:1282/1370 train_time:189788ms step_avg:149.20ms
step:1283/1370 train_time:189949ms step_avg:149.21ms
step:1284/1370 train_time:190106ms step_avg:149.22ms
step:1285/1370 train_time:190261ms step_avg:149.22ms
step:1286/1370 train_time:190416ms step_avg:149.23ms
step:1287/1370 train_time:190572ms step_avg:149.23ms
step:1288/1370 train_time:190728ms step_avg:149.24ms
step:1289/1370 train_time:190889ms step_avg:149.25ms
step:1290/1370 train_time:191050ms step_avg:149.26ms
step:1291/1370 train_time:191211ms step_avg:149.27ms
step:1292/1370 train_time:191368ms step_avg:149.27ms
step:1293/1370 train_time:191525ms step_avg:149.28ms
step:1294/1370 train_time:191682ms step_avg:149.28ms
step:1295/1370 train_time:191838ms step_avg:149.29ms
step:1296/1370 train_time:191994ms step_avg:149.30ms
step:1297/1370 train_time:192155ms step_avg:149.30ms
step:1298/1370 train_time:192312ms step_avg:149.31ms
step:1299/1370 train_time:192467ms step_avg:149.31ms
step:1300/1370 train_time:192621ms step_avg:149.32ms
step:1301/1370 train_time:192776ms step_avg:149.32ms
step:1302/1370 train_time:192932ms step_avg:149.33ms
step:1303/1370 train_time:193090ms step_avg:149.33ms
step:1304/1370 train_time:193251ms step_avg:149.34ms
step:1305/1370 train_time:193407ms step_avg:149.35ms
step:1306/1370 train_time:193567ms step_avg:149.36ms
step:1307/1370 train_time:193720ms step_avg:149.36ms
step:1308/1370 train_time:193879ms step_avg:149.37ms
step:1309/1370 train_time:194035ms step_avg:149.37ms
step:1310/1370 train_time:194189ms step_avg:149.38ms
step:1311/1370 train_time:194346ms step_avg:149.38ms
step:1312/1370 train_time:194501ms step_avg:149.39ms
step:1313/1370 train_time:194654ms step_avg:149.39ms
step:1314/1370 train_time:194810ms step_avg:149.39ms
step:1315/1370 train_time:194966ms step_avg:149.40ms
step:1316/1370 train_time:195121ms step_avg:149.40ms
step:1317/1370 train_time:195276ms step_avg:149.41ms
step:1318/1370 train_time:195440ms step_avg:149.42ms
step:1319/1370 train_time:195598ms step_avg:149.43ms
step:1320/1370 train_time:195756ms step_avg:149.43ms
step:1321/1370 train_time:195916ms step_avg:149.44ms
step:1322/1370 train_time:196077ms step_avg:149.45ms
step:1323/1370 train_time:196231ms step_avg:149.45ms
step:1324/1370 train_time:196388ms step_avg:149.46ms
step:1325/1370 train_time:196546ms step_avg:149.46ms
step:1326/1370 train_time:196707ms step_avg:149.47ms
step:1327/1370 train_time:196865ms step_avg:149.48ms
step:1328/1370 train_time:197019ms step_avg:149.48ms
step:1329/1370 train_time:197191ms step_avg:149.50ms
step:1330/1370 train_time:197352ms step_avg:149.51ms
step:1331/1370 train_time:197546ms step_avg:149.54ms
step:1332/1370 train_time:197714ms step_avg:149.56ms
step:1333/1370 train_time:197871ms step_avg:149.56ms
step:1334/1370 train_time:198028ms step_avg:149.57ms
step:1335/1370 train_time:198184ms step_avg:149.57ms
step:1336/1370 train_time:198347ms step_avg:149.58ms
step:1337/1370 train_time:198506ms step_avg:149.59ms
step:1338/1370 train_time:198663ms step_avg:149.60ms
step:1339/1370 train_time:198820ms step_avg:149.60ms
step:1340/1370 train_time:198980ms step_avg:149.61ms
step:1341/1370 train_time:199135ms step_avg:149.61ms
step:1342/1370 train_time:199293ms step_avg:149.62ms
step:1343/1370 train_time:199450ms step_avg:149.63ms
step:1344/1370 train_time:199608ms step_avg:149.63ms
step:1345/1370 train_time:199767ms step_avg:149.64ms
step:1346/1370 train_time:199924ms step_avg:149.64ms
step:1347/1370 train_time:200084ms step_avg:149.65ms
step:1348/1370 train_time:200242ms step_avg:149.66ms
step:1349/1370 train_time:200397ms step_avg:149.66ms
step:1350/1370 train_time:200551ms step_avg:149.66ms
step:1351/1370 train_time:200707ms step_avg:149.67ms
step:1352/1370 train_time:200870ms step_avg:149.68ms
step:1353/1370 train_time:201030ms step_avg:149.69ms
step:1354/1370 train_time:201191ms step_avg:149.70ms
step:1355/1370 train_time:201350ms step_avg:149.70ms
step:1356/1370 train_time:201506ms step_avg:149.71ms
step:1357/1370 train_time:201666ms step_avg:149.72ms
step:1358/1370 train_time:201824ms step_avg:149.72ms
step:1359/1370 train_time:201983ms step_avg:149.73ms
step:1360/1370 train_time:202144ms step_avg:149.74ms
step:1361/1370 train_time:202304ms step_avg:149.74ms
step:1362/1370 train_time:202463ms step_avg:149.75ms
step:1363/1370 train_time:202625ms step_avg:149.76ms
step:1364/1370 train_time:202781ms step_avg:149.76ms
step:1365/1370 train_time:202936ms step_avg:149.77ms
step:1366/1370 train_time:203095ms step_avg:149.78ms
step:1367/1370 train_time:203254ms step_avg:149.78ms
step:1368/1370 train_time:203415ms step_avg:149.79ms
step:1369/1370 train_time:203580ms step_avg:149.80ms
step:1370/1370 train_time:203741ms step_avg:149.81ms
step:1370/1370 val_loss:3.2790 train_time:203812ms step_avg:149.86ms
peak memory consumption: 32620 MiB

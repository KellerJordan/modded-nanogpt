import os
import sys
with open(sys.argv[0]) as f:
    code = f.read() # read the code of this file ASAP, for logging
import uuid
import time
import glob
import subprocess
import contextlib
from dataclasses import dataclass

import torch
torch.empty(1, device='cuda', requires_grad=True).backward()
from torch import nn
import torch.nn.functional as F
import torch.distributed as dist
from torch.nn.parallel import DistributedDataParallel as DDP
# use of FlexAttention contributed by @KoszarskyB
from torch.nn.attention.flex_attention import BlockMask, flex_attention

# -----------------------------------------------------------------------------
# Muon optimizer

@torch.compile
def zeropower_via_newtonschulz5(G, steps):
    """
    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a
    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose
    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at
    zero even beyond the point where the iteration no longer converges all the way to one everywhere
    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T
    where S' is diagonal with S_{ii}' ~ Uniform(0.5, 1.5), which turns out not to hurt model
    performance at all relative to UV^T, where USV^T = G is the SVD.
    """
    assert len(G.shape) == 2
    a, b, c = (3.4445, -4.7750,  2.0315)
    X = G.bfloat16()
    if G.size(0) > G.size(1):
        X = X.T

    # Ensure spectral norm is at most 1
    X = X / (X.norm() + 1e-7)
    # Perform the NS iterations
    for _ in range(steps):
        A = X @ X.T
        B = b * A + c * A @ A # adapted from suggestion by @jxbz, @leloykun, and @YouJiacheng
        X = a * X + B @ X
    
    if G.size(0) > G.size(1):
        X = X.T
    return X

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Some warnings:
    - This optimizer assumes that all parameters passed in are 2D.
    - It should not be used for the embedding layer, the final fully connected layer, or any {0,1}-D
    parameters; those should all be optimized by a standard method (e.g., AdamW).
    - To use it with 4D convolutional filters, it works well to just flatten their last 3 dimensions.
    - We believe it is unlikely to work well for training with small batch size.
    - We believe it may not work well for finetuning pretrained models, but we haven't tested this.
    - We have not yet tried this optimizer for training scenarios larger than NanoGPT (124M).

    Arguments:
        lr: The learning rate used by the internal SGD.
        momentum: The momentum used by the internal SGD.
        nesterov: Whether to use Nesterov-style momentum in the internal SGD. (recommended)
        ns_steps: The number of Newton-Schulz iteration steps to use.
    """
    def __init__(self, params, lr=0.02, momentum=0.95, nesterov=True, ns_steps=5):
        self.world_size = int(os.environ['WORLD_SIZE'])
        self.rank = int(os.environ['RANK'])
        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, ns_steps=ns_steps)
        assert all(isinstance(p, torch.Tensor) for p in params)
        sizes = {p.numel() for p in params}
        param_groups = [dict(params=[p for p in params if p.numel() == size],
                             update_buffer=[torch.empty(size, device='cuda', dtype=torch.bfloat16) for _ in range(self.world_size)])
                        for size in sizes]
        super().__init__(param_groups, defaults)

    def step(self):

        for group in self.param_groups:

            lr = group['lr']
            momentum = group['momentum']
            nesterov = group['nesterov']
            ns_steps = group['ns_steps']
            update_buffers = group['update_buffer']
            # generate weight updates in distributed fashion
            params = group['params']
            handle = None
            params_world = None
            def update_prev():
                if params_world is None:
                    return
                assert handle is not None
                handle.wait()
                for p_world, g_world in zip(params_world, update_buffers):
                    p_world.data.add_(
                        g_world.view_as(p_world),
                        alpha=-lr * max(1, p_world.size(0) / p_world.size(1)) ** 0.5,
                    )
            for base_i in range(len(params))[::self.world_size]:
                if base_i + rank < len(params):
                    p = params[base_i + self.rank]
                    g = p.grad
                    assert g is not None
                    state = self.state[p]
                    if 'momentum_buffer' not in state:
                        state['momentum_buffer'] = torch.zeros_like(g)
                    buf = state['momentum_buffer']
                    buf.lerp_(g, 1 - momentum)
                    g = g.lerp_(buf, momentum) if nesterov else buf
                    g = zeropower_via_newtonschulz5(g, steps=ns_steps).flatten()
                else:
                    g = update_buffers[rank]
                update_prev() # async all_gather instead of sync all_reduce by @YouJiacheng
                handle = dist.all_gather(update_buffers, g, async_op=True)
                params_world = params[base_i : base_i + self.world_size]
            update_prev()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the GPT-2 model

def norm(x):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):

    def __init__(self, in_features, out_features):
        super().__init__(in_features, out_features, bias=False)

    def forward(self, x):
        return F.linear(x, self.weight.type_as(x))

class Rotary(nn.Module):

    def __init__(self, dim, max_seq_len=65536):
        super().__init__()
        # half-truncate RoPE by @YouJiacheng
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=dim//4, dtype=torch.float32)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(dim//4)])
        t = torch.arange(max_seq_len, dtype=torch.float32)
        theta = torch.einsum('i,j -> ij', t, angular_freq)
        self.cos = nn.Buffer(theta.cos(), persistent=False)
        self.sin = nn.Buffer(theta.sin(), persistent=False)

    def forward(self, x):
        cos, sin = self.cos[None, :x.size(-3), None, :], self.sin[None, :x.size(-3), None, :]
        x1, x2 = x.float().chunk(2, dim=-1)
        y1 = x1 * cos + x2 * sin
        y2 = x1 * (-sin) + x2 * cos
        return torch.cat((y1, y2), 3).type_as(x)

class CausalSelfAttention(nn.Module):

    def __init__(self, dim, num_heads):
        super().__init__()
        assert dim % num_heads == 0
        self.num_heads = num_heads
        self.c_q = CastedLinear(dim, dim)
        self.c_k = CastedLinear(dim, dim)
        self.c_v = CastedLinear(dim, dim)
        self.lambdas = nn.Parameter(torch.tensor([0.5, 0.5]))
        self.rotary = Rotary(dim // num_heads) # dim // num_heads = head_dim
        self.c_proj = CastedLinear(dim, dim)
        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977
        self.attn_scale = nn.Parameter(torch.tensor(1.0 / (dim // num_heads) ** 0.5))

    def forward(self, x, ve, block_mask):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, 'Must use batch size = 1 for FlexAttention'
        q = self.c_q(x).view(B, T, self.num_heads, -1)
        k = self.c_k(x).view(B, T, self.num_heads, -1)
        v = self.c_v(x).view(B, T, self.num_heads, -1)
        if ve is not None:
            v = self.lambdas[0] * v + self.lambdas[1] * ve.view_as(v) # @KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = self.lambdas[0] * v
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = self.rotary(q), self.rotary(k)
        y = flex_attention(q.transpose(1, 2) * self.attn_scale, k.transpose(1, 2), v.transpose(1, 2), block_mask=block_mask, scale=1.)
        y = y.transpose(1, 2).contiguous().view_as(x) # re-assemble all head outputs side by side
        y = self.c_proj(y)
        return y

class MLP(nn.Module):

    def __init__(self, dim):
        super().__init__()
        self.c_fc = CastedLinear(dim, 4 * dim)
        self.c_proj = CastedLinear(4 * dim, dim)
        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977

    def forward(self, x):
        x = self.c_fc(x)
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = self.c_proj(x)
        return x

class Block(nn.Module):

    def __init__(self, model_dim, num_heads, use_attn=True):
        super().__init__()
        self.attn = CausalSelfAttention(model_dim, num_heads) if use_attn else None
        self.mlp = MLP(model_dim)
        self.lambdas = nn.Parameter(torch.tensor([1., 0.]))

    def forward(self, x, ve, x0, block_mask):
        x = self.lambdas[0] * x + self.lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), ve, block_mask)
        x = x + self.mlp(norm(x))
        return x

class ValueEmbedding(nn.Module):
    def __init__(self, vocab_size, model_dim):
        super().__init__()
        self.embed = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])

    def forward(self, inputs):
        ve = [emb(inputs).bfloat16() for emb in self.embed]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        ve = [ve[0], ve[1], ve[2], None, None, None, None, None, None, ve[0], ve[1], ve[2]]
        return ve

# -----------------------------------------------------------------------------
# The main GPT-2 model

class GPT(nn.Module):

    def __init__(self, vocab_size, num_layers, num_heads, model_dim):
        super().__init__()
        self.embed = nn.Embedding(vocab_size, model_dim)
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.blocks = nn.ModuleList([Block(model_dim, num_heads, use_attn=(i != 7))
                                     for i in range(num_layers)])
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual learning
        # U-net structure on token value embeddings by @leloykun
        self.value_embeds = ValueEmbedding(vocab_size, model_dim)
        self.lm_head = CastedLinear(model_dim, vocab_size)
        self.lm_head.weight.data.zero_() # @Grad62304977
        # U-net design by @brendanh0gan
        self.num_encoder_layers = num_layers // 2 # Half of the layers for encoder
        self.num_decoder_layers = num_layers - self.num_encoder_layers # Remaining for decoder
        # Add learnable skip connection weights for decoder layers
        self.skip_weights = nn.Parameter(torch.ones(self.num_decoder_layers))

    def forward(self, inputs, targets, sliding_window_num_blocks):
        BLOCK_SIZE = 128
        seq_len = len(inputs)
        assert seq_len % BLOCK_SIZE == 0
        total_num_blocks = seq_len // BLOCK_SIZE
        assert inputs.ndim == 1
        docs = (inputs == 50256).cumsum(0)
        docs_low = docs.view(-1, BLOCK_SIZE)[:, 0].contiguous()
        docs_high = docs.view(-1, BLOCK_SIZE)[:, -1].contiguous()

        def document_causal(b, h, q_idx, kv_idx):
            causal_mask = q_idx >= kv_idx
            document_mask = docs[q_idx] == docs[kv_idx]
            return causal_mask & document_mask

        def dense_to_ordered(dense_mask):
            num_blocks = dense_mask.sum(dim=-1, dtype=torch.int32)
            indices = dense_mask.argsort(dim=-1, descending=True, stable=True).to(torch.int32)
            return num_blocks[None, None].contiguous(), indices[None, None].contiguous()

        def create_doc_swc_block_mask(sliding_window_num_blocks):
            kv_idx = block_idx = torch.arange(total_num_blocks, dtype=torch.int32, device='cuda')
            q_idx = block_idx[:, None]
            causal_bm = q_idx >= kv_idx
            causal_full_bm = q_idx > kv_idx
            window_bm = q_idx - kv_idx < sliding_window_num_blocks
            window_full_bm = window_bm # block-wise sliding window by @YouJiacheng
            # document_bm = (docs_low[q_idx] <= docs_high[kv_idx]) & (docs_low[kv_idx] <= docs_high[q_idx])
            document_bm = (docs_low[:, None] <= docs_high) & (docs_low <= docs_high[:, None])
            document_full_bm = (docs_low[:, None] == docs_high) & (docs_low == docs_high[:, None])
            nonzero_bm = causal_bm & window_bm & document_bm
            full_bm  = causal_full_bm & window_full_bm & document_full_bm
            kv_num_blocks, kv_indices = dense_to_ordered(nonzero_bm & ~full_bm)
            full_kv_num_blocks, full_kv_indices = dense_to_ordered(full_bm)
            return BlockMask.from_kv_blocks(
                kv_num_blocks,
                kv_indices,
                full_kv_num_blocks,
                full_kv_indices,
                BLOCK_SIZE=BLOCK_SIZE,
                mask_mod=document_causal,
            )

        block_mask = create_doc_swc_block_mask(sliding_window_num_blocks)

        x0 = norm(self.embed(inputs[None]).bfloat16()) # use of norm here by @Grad62304977
        x = x0
        ve = self.value_embeds(inputs)
        assert len(ve) == len(self.blocks)
        ve_enc, ve_dec = ve[:self.num_encoder_layers], ve[self.num_encoder_layers:]

        # Store outputs for U-Net skip connections
        skip_connections = []
        # Encoder pass - process only the first half of the blocks
        for i in range(self.num_encoder_layers):
            x = self.blocks[i](x, ve_enc[i], x0, block_mask)
            skip_connections.append(x)
        # Decoder pass - process the remaining blocks with weighted skip connections
        for i in range(self.num_decoder_layers):
            x = x + self.skip_weights[i] * skip_connections.pop()
            # U-net structure on token value embeddings by @leloykun
            x = self.blocks[self.num_encoder_layers + i](x, ve_dec[i], x0, block_mask)

        x = norm(x)
        logits = self.lm_head(x)
        logits = 15 * torch.tanh(logits / 15) # @Grad62304977 added tanh softcapping, @KoszarskyB reduced it from 30 to 15
        logits = logits.float()
        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets)
        return loss

# -----------------------------------------------------------------------------
# Our own simple Distributed Data Loader

def _load_data_shard(path):
    # only reads the header, returns header data
    # header is 256 int32
    header = torch.from_file(path, False, 256, dtype=torch.int32)
    assert header[0] == 20240520, 'magic number mismatch in the data .bin file'
    assert header[1] == 1, 'unsupported version'
    num_tokens = int(header[2]) # number of tokens (claimed)
    with open(path, 'rb', buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, 'number of tokens read does not match header'
    return tokens

class DistributedDataLoader:

    def __init__(self, filename_pattern):
        self.rank = int(os.environ['RANK'])
        self.world_size = int(os.environ['WORLD_SIZE'])
        self.files = sorted(glob.glob(filename_pattern))
        self.reset()

    def reset(self):
        self.current_shard = -1
        self.advance()

    def advance(self):
        self.current_shard = (self.current_shard + 1) % len(self.files)
        self.current_position = 0
        self.tokens = _load_data_shard(self.files[self.current_shard])

    def next_batch(self, batch_size):
        assert batch_size % self.world_size == 0
        device_batch_size = batch_size // self.world_size
        # load next shard if necessary
        if self.current_position + batch_size + 1 >= len(self.tokens):
            self.advance()
        pos = self.current_position + self.rank * device_batch_size
        device_batch_tokens = self.tokens[pos:pos+device_batch_size+1]
        # advance current position
        self.current_position += batch_size
        inputs = device_batch_tokens[:-1].to(device='cuda', dtype=torch.int32, non_blocking=True)
        targets = device_batch_tokens[1:].to(device='cuda', dtype=torch.int64, non_blocking=True)
        return inputs, targets

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files = 'data/fineweb10B/fineweb_train_*.bin' # input .bin to train on
    val_files = 'data/fineweb10B/fineweb_val_*.bin' # input .bin to eval validation loss on
    val_tokens = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # optimization
    batch_size = 8*64*1024 # batch size in tokens
    num_iterations = 1370 # number of iterations to run
    cooldown_frac = 0.4 # fraction of training spent cooling down the learning rate
    bf16_embeds = True
    # evaluation and logging
    val_loss_every = 125 # every how many steps to evaluate val loss? 0 for only at the end
    # implementation
    max_device_batch_size = 64*1024 # batch size per device in tokens
    save_checkpoint = False
args = Hyperparameters()

micro_bs = args.max_device_batch_size

# set up DDP (distributed data parallel). torchrun sets this env variable
rank = int(os.environ['RANK'])
local_rank = int(os.environ['LOCAL_RANK'])
world_size = int(os.environ['WORLD_SIZE'])
assert torch.cuda.is_available()
torch.cuda.set_device(local_rank)
dist.init_process_group(backend='nccl', device_id=torch.device(local_rank))
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = uuid.uuid4()
    os.makedirs('logs', exist_ok=True)
    logfile = f'logs/{run_id}.txt'
    print(logfile)

def print0(s, console=False):
    if master_process:
        with open(logfile, 'a') as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0('='*100)
# log information about the hardware/software environment this is running on
print0(f'Running Python {sys.version}')
print0(f'Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}')
print0(subprocess.run(['nvidia-smi'], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout)
print0('='*100)

# load data
train_loader = DistributedDataLoader(args.train_files)
val_loader = DistributedDataLoader(args.val_files)
print0(f'Training dataloader files: {train_loader.files}')
print0(f'Validation dataloader files: {val_loader.files}')
print0('='*100)

# there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency. suggested to me by @Grad62304977.
# this originates from Karpathy's experiments.
model = GPT(vocab_size=50304, num_layers=12, num_heads=6, model_dim=768)
model = model.cuda()
if args.bf16_embeds:
    for m in model.modules():
        if isinstance(m, nn.Embedding):
            m.bfloat16()
model: GPT = torch.compile(model)
ddp_model = DDP(model, device_ids=[local_rank], broadcast_buffers=False, gradient_as_bucket_view=True)

# collect the parameters to optimize
hidden_matrix_params = [p for p in model.blocks.parameters() if p.ndim == 2]
embed_params = [model.embed.weight, *model.value_embeds.parameters()]
scalar_params = [p for n, p in model.named_parameters() if p.ndim < 2 and "attn_scale" not in n]
attn_scale_params = [p for n, p in model.named_parameters() if p.ndim < 2 and "attn_scale" in n]
head_params = [model.lm_head.weight]

# init the optimizer(s)
optimizer1 = torch.optim.Adam([dict(params=embed_params, lr=0.6),
                               dict(params=head_params, lr=0.008),
                               dict(params=scalar_params, lr=0.04),
                               dict(params=attn_scale_params, lr=0.01)],
                              betas=(0.8, 0.95), fused=True)
optimizer2 = Muon(hidden_matrix_params, lr=0.05, momentum=0.95)
optimizers = [optimizer1, optimizer2]

# learning rate schedule: stable then decay
def get_lr(it):
    t = 1 - it / args.num_iterations # time remaining in training
    assert 1 >= t > 0
    # 1) constant lr for first part of training
    if t >= args.cooldown_frac:
        return 1.0
    # 2) then linear cooldown
    else:
        return t / args.cooldown_frac
schedulers = [torch.optim.lr_scheduler.LambdaLR(opt, get_lr) for opt in optimizers]

# sliding window size schedule: linear increase over training in chunks of 128 from 128 -> 1792. By @fernbear.bsky.social
def get_sliding_window_blocks(it):
    x = it / args.num_iterations # training progress
    assert 0 <= x <= 1
    return int(((1 - x) * 128 + x * 1856) // 128)
sliding_window_num_blocks = torch.tensor(1, dtype=torch.int32, device='cuda')

# Start training loop
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    # This effectively ignores timing first 10 steps, which are slower for weird reasons.
    # Alternately, and slightly more correctly in terms of benchmarking, we could do 10
    # steps with dummy data first, and then re-initialize the model and reset the loader.
    if step == 10:
        training_time_ms = 0
        t0 = time.perf_counter()
    timed_steps = float('nan') if step <= 11 else (step - 10) + 1 # <= 11 to avoid bug in val

    sliding_window_num_blocks.copy_(get_sliding_window_blocks(step))

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        # run validation batches
        model.eval()
        val_loader.reset()
        val_loss = 0.0
        # calculate the number of steps to take in the val loop.
        val_batch_size = world_size * micro_bs
        assert args.val_tokens % val_batch_size == 0
        val_steps = args.val_tokens // val_batch_size
        for _ in range(val_steps):
            with torch.no_grad():
                inputs_val, targets_val = val_loader.next_batch(val_batch_size)
                val_loss += ddp_model(inputs_val, targets_val, sliding_window_num_blocks)
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        val_loss /= val_steps
        # logging
        print0(f'step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/(timed_steps-1):.2f}ms', console=True)
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f'logs/{run_id}', exist_ok=True)
            torch.save(log, f'logs/{run_id}/state_step{step:06d}.pt')
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    model.train()
    batch_size = args.batch_size
    assert batch_size % world_size == 0
    inputs_train, targets_train = train_loader.next_batch(batch_size)
    assert len(inputs_train) <= micro_bs or len(inputs_train) % micro_bs == 0
    for micro_inputs_train, micro_targets_train in zip(inputs_train.split(micro_bs), targets_train.split(micro_bs)):
        ddp_model(micro_inputs_train, micro_targets_train, sliding_window_num_blocks).backward()
    # momentum warmup for Muon
    frac = min(step/300, 1)
    for group in optimizer2.param_groups:
        group['momentum'] = (1 - frac) * 0.85 + frac * 0.95
    # step the optimizers and schedulers
    for opt, sched in zip(optimizers, schedulers):
        opt.step()
        if step != train_steps-1:
            sched.step()
    # null the gradients
    model.zero_grad(set_to_none=True)
    # logging
    approx_time = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f'step:{step+1}/{train_steps} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms', console=True)

print0(f'peak memory consumption: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB')
dist.destroy_process_group()

====================================================================================================
Running Python 3.10.12 (main, Sep 11 2024, 15:47:36) [GCC 11.4.0]
Running PyTorch 2.6.0.dev20241231+cu126 compiled for CUDA 12.6
Sat Jan 11 23:04:03 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.183.06             Driver Version: 535.183.06   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:19:00.0 Off |                    0 |
| N/A   39C    P0             127W / 700W |   7713MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:3B:00.0 Off |                    0 |
| N/A   33C    P0             114W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:4C:00.0 Off |                    0 |
| N/A   30C    P0             118W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   38C    P0             130W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:9B:00.0 Off |                    0 |
| N/A   39C    P0             124W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:BB:00.0 Off |                    0 |
| N/A   31C    P0             115W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:CB:00.0 Off |                    0 |
| N/A   37C    P0             116W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:DB:00.0 Off |                    0 |
| N/A   30C    P0             117W / 700W |   3211MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
Training dataloader files: ['data/fineweb10B/fineweb_train_000001.bin', 'data/fineweb10B/fineweb_train_000002.bin', 'data/fineweb10B/fineweb_train_000003.bin', 'data/fineweb10B/fineweb_train_000004.bin', 'data/fineweb10B/fineweb_train_000005.bin', 'data/fineweb10B/fineweb_train_000006.bin', 'data/fineweb10B/fineweb_train_000007.bin', 'data/fineweb10B/fineweb_train_000008.bin', 'data/fineweb10B/fineweb_train_000009.bin']
Validation dataloader files: ['data/fineweb10B/fineweb_val_000000.bin']
====================================================================================================
step:0/1370 val_loss:10.8258 train_time:0ms step_avg:nanms
step:1/1370 train_time:29462ms step_avg:nanms
step:2/1370 train_time:29544ms step_avg:nanms
step:3/1370 train_time:29725ms step_avg:nanms
step:4/1370 train_time:29858ms step_avg:nanms
step:5/1370 train_time:29992ms step_avg:nanms
step:6/1370 train_time:30125ms step_avg:nanms
step:7/1370 train_time:30259ms step_avg:nanms
step:8/1370 train_time:30393ms step_avg:nanms
step:9/1370 train_time:30527ms step_avg:nanms
step:10/1370 train_time:30669ms step_avg:nanms
step:11/1370 train_time:139ms step_avg:nanms
step:12/1370 train_time:276ms step_avg:nanms
step:13/1370 train_time:411ms step_avg:136.88ms
step:14/1370 train_time:545ms step_avg:136.15ms
step:15/1370 train_time:680ms step_avg:135.94ms
step:16/1370 train_time:816ms step_avg:136.01ms
step:17/1370 train_time:951ms step_avg:135.88ms
step:18/1370 train_time:1090ms step_avg:136.31ms
step:19/1370 train_time:1228ms step_avg:136.49ms
step:20/1370 train_time:1364ms step_avg:136.40ms
step:21/1370 train_time:1499ms step_avg:136.29ms
step:22/1370 train_time:1635ms step_avg:136.29ms
step:23/1370 train_time:1770ms step_avg:136.12ms
step:24/1370 train_time:1905ms step_avg:136.06ms
step:25/1370 train_time:2042ms step_avg:136.12ms
step:26/1370 train_time:2180ms step_avg:136.28ms
step:27/1370 train_time:2318ms step_avg:136.36ms
step:28/1370 train_time:2455ms step_avg:136.37ms
step:29/1370 train_time:2590ms step_avg:136.31ms
step:30/1370 train_time:2725ms step_avg:136.27ms
step:31/1370 train_time:2861ms step_avg:136.24ms
step:32/1370 train_time:2998ms step_avg:136.30ms
step:33/1370 train_time:3134ms step_avg:136.28ms
step:34/1370 train_time:3272ms step_avg:136.32ms
step:35/1370 train_time:3409ms step_avg:136.35ms
step:36/1370 train_time:3543ms step_avg:136.27ms
step:37/1370 train_time:3680ms step_avg:136.28ms
step:38/1370 train_time:3819ms step_avg:136.39ms
step:39/1370 train_time:3954ms step_avg:136.34ms
step:40/1370 train_time:4091ms step_avg:136.36ms
step:41/1370 train_time:4228ms step_avg:136.37ms
step:42/1370 train_time:4364ms step_avg:136.39ms
step:43/1370 train_time:4500ms step_avg:136.37ms
step:44/1370 train_time:4638ms step_avg:136.41ms
step:45/1370 train_time:4773ms step_avg:136.38ms
step:46/1370 train_time:4910ms step_avg:136.39ms
step:47/1370 train_time:5045ms step_avg:136.36ms
step:48/1370 train_time:5182ms step_avg:136.38ms
step:49/1370 train_time:5320ms step_avg:136.41ms
step:50/1370 train_time:5457ms step_avg:136.43ms
step:51/1370 train_time:5593ms step_avg:136.41ms
step:52/1370 train_time:5731ms step_avg:136.44ms
step:53/1370 train_time:5866ms step_avg:136.42ms
step:54/1370 train_time:6003ms step_avg:136.42ms
step:55/1370 train_time:6138ms step_avg:136.40ms
step:56/1370 train_time:6276ms step_avg:136.43ms
step:57/1370 train_time:6416ms step_avg:136.51ms
step:58/1370 train_time:6551ms step_avg:136.48ms
step:59/1370 train_time:6690ms step_avg:136.52ms
step:60/1370 train_time:6827ms step_avg:136.53ms
step:61/1370 train_time:6962ms step_avg:136.52ms
step:62/1370 train_time:7101ms step_avg:136.56ms
step:63/1370 train_time:7238ms step_avg:136.56ms
step:64/1370 train_time:7373ms step_avg:136.55ms
step:65/1370 train_time:7511ms step_avg:136.56ms
step:66/1370 train_time:7646ms step_avg:136.54ms
step:67/1370 train_time:7782ms step_avg:136.53ms
step:68/1370 train_time:7921ms step_avg:136.56ms
step:69/1370 train_time:8058ms step_avg:136.57ms
step:70/1370 train_time:8194ms step_avg:136.57ms
step:71/1370 train_time:8330ms step_avg:136.56ms
step:72/1370 train_time:8467ms step_avg:136.56ms
step:73/1370 train_time:8603ms step_avg:136.55ms
step:74/1370 train_time:8741ms step_avg:136.57ms
step:75/1370 train_time:8878ms step_avg:136.58ms
step:76/1370 train_time:9017ms step_avg:136.62ms
step:77/1370 train_time:9152ms step_avg:136.59ms
step:78/1370 train_time:9288ms step_avg:136.59ms
step:79/1370 train_time:9425ms step_avg:136.59ms
step:80/1370 train_time:9561ms step_avg:136.58ms
step:81/1370 train_time:9700ms step_avg:136.61ms
step:82/1370 train_time:9836ms step_avg:136.62ms
step:83/1370 train_time:9972ms step_avg:136.60ms
step:84/1370 train_time:10109ms step_avg:136.61ms
step:85/1370 train_time:10244ms step_avg:136.58ms
step:86/1370 train_time:10381ms step_avg:136.60ms
step:87/1370 train_time:10519ms step_avg:136.61ms
step:88/1370 train_time:10657ms step_avg:136.62ms
step:89/1370 train_time:10794ms step_avg:136.63ms
step:90/1370 train_time:10931ms step_avg:136.64ms
step:91/1370 train_time:11067ms step_avg:136.63ms
step:92/1370 train_time:11203ms step_avg:136.62ms
step:93/1370 train_time:11340ms step_avg:136.63ms
step:94/1370 train_time:11477ms step_avg:136.63ms
step:95/1370 train_time:11614ms step_avg:136.63ms
step:96/1370 train_time:11749ms step_avg:136.61ms
step:97/1370 train_time:11886ms step_avg:136.62ms
step:98/1370 train_time:12025ms step_avg:136.65ms
step:99/1370 train_time:12160ms step_avg:136.63ms
step:100/1370 train_time:12296ms step_avg:136.62ms
step:101/1370 train_time:12432ms step_avg:136.62ms
step:102/1370 train_time:12570ms step_avg:136.63ms
step:103/1370 train_time:12708ms step_avg:136.64ms
step:104/1370 train_time:12848ms step_avg:136.68ms
step:105/1370 train_time:12989ms step_avg:136.72ms
step:106/1370 train_time:13129ms step_avg:136.76ms
step:107/1370 train_time:13268ms step_avg:136.78ms
step:108/1370 train_time:13407ms step_avg:136.80ms
step:109/1370 train_time:13546ms step_avg:136.83ms
step:110/1370 train_time:13685ms step_avg:136.85ms
step:111/1370 train_time:13825ms step_avg:136.89ms
step:112/1370 train_time:13965ms step_avg:136.91ms
step:113/1370 train_time:14105ms step_avg:136.94ms
step:114/1370 train_time:14244ms step_avg:136.96ms
step:115/1370 train_time:14383ms step_avg:136.98ms
step:116/1370 train_time:14524ms step_avg:137.02ms
step:117/1370 train_time:14663ms step_avg:137.03ms
step:118/1370 train_time:14803ms step_avg:137.07ms
step:119/1370 train_time:14943ms step_avg:137.09ms
step:120/1370 train_time:15082ms step_avg:137.11ms
step:121/1370 train_time:15222ms step_avg:137.14ms
step:122/1370 train_time:15362ms step_avg:137.16ms
step:123/1370 train_time:15501ms step_avg:137.18ms
step:124/1370 train_time:15641ms step_avg:137.20ms
step:125/1370 train_time:15781ms step_avg:137.23ms
step:125/1370 val_loss:4.3695 train_time:15847ms step_avg:137.80ms
step:126/1370 train_time:15928ms step_avg:137.31ms
step:127/1370 train_time:16069ms step_avg:137.34ms
step:128/1370 train_time:16208ms step_avg:137.36ms
step:129/1370 train_time:16347ms step_avg:137.37ms
step:130/1370 train_time:16486ms step_avg:137.38ms
step:131/1370 train_time:16624ms step_avg:137.39ms
step:132/1370 train_time:16766ms step_avg:137.43ms
step:133/1370 train_time:16907ms step_avg:137.45ms
step:134/1370 train_time:17048ms step_avg:137.49ms
step:135/1370 train_time:17188ms step_avg:137.51ms
step:136/1370 train_time:17329ms step_avg:137.53ms
step:137/1370 train_time:17470ms step_avg:137.56ms
step:138/1370 train_time:17609ms step_avg:137.57ms
step:139/1370 train_time:17748ms step_avg:137.58ms
step:140/1370 train_time:17888ms step_avg:137.60ms
step:141/1370 train_time:18029ms step_avg:137.63ms
step:142/1370 train_time:18171ms step_avg:137.66ms
step:143/1370 train_time:18311ms step_avg:137.68ms
step:144/1370 train_time:18451ms step_avg:137.69ms
step:145/1370 train_time:18589ms step_avg:137.70ms
step:146/1370 train_time:18728ms step_avg:137.71ms
step:147/1370 train_time:18868ms step_avg:137.72ms
step:148/1370 train_time:19008ms step_avg:137.74ms
step:149/1370 train_time:19149ms step_avg:137.76ms
step:150/1370 train_time:19289ms step_avg:137.78ms
step:151/1370 train_time:19429ms step_avg:137.79ms
step:152/1370 train_time:19569ms step_avg:137.81ms
step:153/1370 train_time:19708ms step_avg:137.82ms
step:154/1370 train_time:19849ms step_avg:137.84ms
step:155/1370 train_time:19988ms step_avg:137.85ms
step:156/1370 train_time:20130ms step_avg:137.87ms
step:157/1370 train_time:20270ms step_avg:137.89ms
step:158/1370 train_time:20411ms step_avg:137.92ms
step:159/1370 train_time:20551ms step_avg:137.93ms
step:160/1370 train_time:20691ms step_avg:137.94ms
step:161/1370 train_time:20831ms step_avg:137.96ms
step:162/1370 train_time:20970ms step_avg:137.96ms
step:163/1370 train_time:21110ms step_avg:137.97ms
step:164/1370 train_time:21251ms step_avg:137.99ms
step:165/1370 train_time:21391ms step_avg:138.00ms
step:166/1370 train_time:21530ms step_avg:138.02ms
step:167/1370 train_time:21670ms step_avg:138.03ms
step:168/1370 train_time:21811ms step_avg:138.05ms
step:169/1370 train_time:21952ms step_avg:138.07ms
step:170/1370 train_time:22093ms step_avg:138.08ms
step:171/1370 train_time:22233ms step_avg:138.10ms
step:172/1370 train_time:22372ms step_avg:138.10ms
step:173/1370 train_time:22512ms step_avg:138.11ms
step:174/1370 train_time:22652ms step_avg:138.12ms
step:175/1370 train_time:22792ms step_avg:138.13ms
step:176/1370 train_time:22933ms step_avg:138.15ms
step:177/1370 train_time:23073ms step_avg:138.16ms
step:178/1370 train_time:23212ms step_avg:138.17ms
step:179/1370 train_time:23354ms step_avg:138.19ms
step:180/1370 train_time:23494ms step_avg:138.20ms
step:181/1370 train_time:23634ms step_avg:138.21ms
step:182/1370 train_time:23775ms step_avg:138.23ms
step:183/1370 train_time:23916ms step_avg:138.24ms
step:184/1370 train_time:24059ms step_avg:138.27ms
step:185/1370 train_time:24199ms step_avg:138.28ms
step:186/1370 train_time:24339ms step_avg:138.29ms
step:187/1370 train_time:24478ms step_avg:138.29ms
step:188/1370 train_time:24618ms step_avg:138.30ms
step:189/1370 train_time:24758ms step_avg:138.31ms
step:190/1370 train_time:24898ms step_avg:138.32ms
step:191/1370 train_time:25075ms step_avg:138.54ms
step:192/1370 train_time:25213ms step_avg:138.53ms
step:193/1370 train_time:25351ms step_avg:138.53ms
step:194/1370 train_time:25489ms step_avg:138.53ms
step:195/1370 train_time:25628ms step_avg:138.53ms
step:196/1370 train_time:25768ms step_avg:138.54ms
step:197/1370 train_time:25909ms step_avg:138.55ms
step:198/1370 train_time:26053ms step_avg:138.58ms
step:199/1370 train_time:26193ms step_avg:138.59ms
step:200/1370 train_time:26334ms step_avg:138.60ms
step:201/1370 train_time:26473ms step_avg:138.60ms
step:202/1370 train_time:26612ms step_avg:138.60ms
step:203/1370 train_time:26751ms step_avg:138.60ms
step:204/1370 train_time:26891ms step_avg:138.61ms
step:205/1370 train_time:27036ms step_avg:138.65ms
step:206/1370 train_time:27178ms step_avg:138.67ms
step:207/1370 train_time:27323ms step_avg:138.69ms
step:208/1370 train_time:27465ms step_avg:138.71ms
step:209/1370 train_time:27607ms step_avg:138.73ms
step:210/1370 train_time:27749ms step_avg:138.75ms
step:211/1370 train_time:27891ms step_avg:138.76ms
step:212/1370 train_time:28035ms step_avg:138.79ms
step:213/1370 train_time:28178ms step_avg:138.81ms
step:214/1370 train_time:28322ms step_avg:138.83ms
step:215/1370 train_time:28465ms step_avg:138.85ms
step:216/1370 train_time:28607ms step_avg:138.87ms
step:217/1370 train_time:28749ms step_avg:138.89ms
step:218/1370 train_time:28892ms step_avg:138.91ms
step:219/1370 train_time:29036ms step_avg:138.93ms
step:220/1370 train_time:29177ms step_avg:138.94ms
step:221/1370 train_time:29320ms step_avg:138.96ms
step:222/1370 train_time:29463ms step_avg:138.98ms
step:223/1370 train_time:29606ms step_avg:138.99ms
step:224/1370 train_time:29748ms step_avg:139.01ms
step:225/1370 train_time:29889ms step_avg:139.02ms
step:226/1370 train_time:30032ms step_avg:139.04ms
step:227/1370 train_time:30175ms step_avg:139.06ms
step:228/1370 train_time:30318ms step_avg:139.07ms
step:229/1370 train_time:30462ms step_avg:139.10ms
step:230/1370 train_time:30604ms step_avg:139.11ms
step:231/1370 train_time:30749ms step_avg:139.13ms
step:232/1370 train_time:30890ms step_avg:139.15ms
step:233/1370 train_time:31033ms step_avg:139.16ms
step:234/1370 train_time:31175ms step_avg:139.18ms
step:235/1370 train_time:31320ms step_avg:139.20ms
step:236/1370 train_time:31465ms step_avg:139.22ms
step:237/1370 train_time:31607ms step_avg:139.24ms
step:238/1370 train_time:31751ms step_avg:139.26ms
step:239/1370 train_time:31894ms step_avg:139.28ms
step:240/1370 train_time:32037ms step_avg:139.29ms
step:241/1370 train_time:32179ms step_avg:139.30ms
step:242/1370 train_time:32320ms step_avg:139.31ms
step:243/1370 train_time:32465ms step_avg:139.33ms
step:244/1370 train_time:32607ms step_avg:139.35ms
step:245/1370 train_time:32751ms step_avg:139.37ms
step:246/1370 train_time:32893ms step_avg:139.38ms
step:247/1370 train_time:33037ms step_avg:139.40ms
step:248/1370 train_time:33178ms step_avg:139.40ms
step:249/1370 train_time:33321ms step_avg:139.42ms
step:250/1370 train_time:33465ms step_avg:139.44ms
step:250/1370 val_loss:3.9589 train_time:33529ms step_avg:139.71ms
step:251/1370 train_time:33609ms step_avg:139.45ms
step:252/1370 train_time:33754ms step_avg:139.48ms
step:253/1370 train_time:33896ms step_avg:139.49ms
step:254/1370 train_time:34038ms step_avg:139.50ms
step:255/1370 train_time:34179ms step_avg:139.51ms
step:256/1370 train_time:34320ms step_avg:139.51ms
step:257/1370 train_time:34463ms step_avg:139.52ms
step:258/1370 train_time:34607ms step_avg:139.54ms
step:259/1370 train_time:34752ms step_avg:139.57ms
step:260/1370 train_time:34895ms step_avg:139.58ms
step:261/1370 train_time:35038ms step_avg:139.59ms
step:262/1370 train_time:35179ms step_avg:139.60ms
step:263/1370 train_time:35322ms step_avg:139.61ms
step:264/1370 train_time:35465ms step_avg:139.62ms
step:265/1370 train_time:35610ms step_avg:139.65ms
step:266/1370 train_time:35755ms step_avg:139.67ms
step:267/1370 train_time:35898ms step_avg:139.68ms
step:268/1370 train_time:36040ms step_avg:139.69ms
step:269/1370 train_time:36181ms step_avg:139.70ms
step:270/1370 train_time:36324ms step_avg:139.71ms
step:271/1370 train_time:36467ms step_avg:139.72ms
step:272/1370 train_time:36612ms step_avg:139.74ms
step:273/1370 train_time:36756ms step_avg:139.76ms
step:274/1370 train_time:36898ms step_avg:139.77ms
step:275/1370 train_time:37042ms step_avg:139.78ms
step:276/1370 train_time:37183ms step_avg:139.79ms
step:277/1370 train_time:37326ms step_avg:139.80ms
step:278/1370 train_time:37468ms step_avg:139.80ms
step:279/1370 train_time:37612ms step_avg:139.82ms
step:280/1370 train_time:37755ms step_avg:139.83ms
step:281/1370 train_time:37897ms step_avg:139.84ms
step:282/1370 train_time:38040ms step_avg:139.85ms
step:283/1370 train_time:38182ms step_avg:139.86ms
step:284/1370 train_time:38326ms step_avg:139.88ms
step:285/1370 train_time:38470ms step_avg:139.89ms
step:286/1370 train_time:38613ms step_avg:139.90ms
step:287/1370 train_time:38758ms step_avg:139.92ms
step:288/1370 train_time:38899ms step_avg:139.92ms
step:289/1370 train_time:39042ms step_avg:139.94ms
step:290/1370 train_time:39184ms step_avg:139.94ms
step:291/1370 train_time:39327ms step_avg:139.95ms
step:292/1370 train_time:39470ms step_avg:139.96ms
step:293/1370 train_time:39614ms step_avg:139.98ms
step:294/1370 train_time:39757ms step_avg:139.99ms
step:295/1370 train_time:39899ms step_avg:140.00ms
step:296/1370 train_time:40042ms step_avg:140.01ms
step:297/1370 train_time:40184ms step_avg:140.01ms
step:298/1370 train_time:40328ms step_avg:140.03ms
step:299/1370 train_time:40472ms step_avg:140.04ms
step:300/1370 train_time:40615ms step_avg:140.05ms
step:301/1370 train_time:40759ms step_avg:140.06ms
step:302/1370 train_time:40902ms step_avg:140.08ms
step:303/1370 train_time:41046ms step_avg:140.09ms
step:304/1370 train_time:41188ms step_avg:140.09ms
step:305/1370 train_time:41331ms step_avg:140.11ms
step:306/1370 train_time:41474ms step_avg:140.11ms
step:307/1370 train_time:41620ms step_avg:140.14ms
step:308/1370 train_time:41764ms step_avg:140.15ms
step:309/1370 train_time:41910ms step_avg:140.17ms
step:310/1370 train_time:42057ms step_avg:140.19ms
step:311/1370 train_time:42201ms step_avg:140.20ms
step:312/1370 train_time:42346ms step_avg:140.22ms
step:313/1370 train_time:42490ms step_avg:140.23ms
step:314/1370 train_time:42636ms step_avg:140.25ms
step:315/1370 train_time:42780ms step_avg:140.26ms
step:316/1370 train_time:42925ms step_avg:140.28ms
step:317/1370 train_time:43070ms step_avg:140.29ms
step:318/1370 train_time:43215ms step_avg:140.31ms
step:319/1370 train_time:43360ms step_avg:140.32ms
step:320/1370 train_time:43505ms step_avg:140.34ms
step:321/1370 train_time:43652ms step_avg:140.36ms
step:322/1370 train_time:43797ms step_avg:140.38ms
step:323/1370 train_time:43942ms step_avg:140.39ms
step:324/1370 train_time:44085ms step_avg:140.40ms
step:325/1370 train_time:44232ms step_avg:140.42ms
step:326/1370 train_time:44377ms step_avg:140.43ms
step:327/1370 train_time:44522ms step_avg:140.45ms
step:328/1370 train_time:44667ms step_avg:140.46ms
step:329/1370 train_time:44813ms step_avg:140.48ms
step:330/1370 train_time:44957ms step_avg:140.49ms
step:331/1370 train_time:45100ms step_avg:140.50ms
step:332/1370 train_time:45246ms step_avg:140.52ms
step:333/1370 train_time:45392ms step_avg:140.53ms
step:334/1370 train_time:45536ms step_avg:140.54ms
step:335/1370 train_time:45680ms step_avg:140.56ms
step:336/1370 train_time:45827ms step_avg:140.57ms
step:337/1370 train_time:45972ms step_avg:140.59ms
step:338/1370 train_time:46118ms step_avg:140.60ms
step:339/1370 train_time:46264ms step_avg:140.62ms
step:340/1370 train_time:46409ms step_avg:140.63ms
step:341/1370 train_time:46555ms step_avg:140.65ms
step:342/1370 train_time:46699ms step_avg:140.66ms
step:343/1370 train_time:46843ms step_avg:140.67ms
step:344/1370 train_time:46987ms step_avg:140.68ms
step:345/1370 train_time:47134ms step_avg:140.70ms
step:346/1370 train_time:47278ms step_avg:140.71ms
step:347/1370 train_time:47422ms step_avg:140.72ms
step:348/1370 train_time:47569ms step_avg:140.74ms
step:349/1370 train_time:47714ms step_avg:140.75ms
step:350/1370 train_time:47860ms step_avg:140.76ms
step:351/1370 train_time:48004ms step_avg:140.77ms
step:352/1370 train_time:48149ms step_avg:140.79ms
step:353/1370 train_time:48294ms step_avg:140.80ms
step:354/1370 train_time:48439ms step_avg:140.81ms
step:355/1370 train_time:48583ms step_avg:140.82ms
step:356/1370 train_time:48730ms step_avg:140.84ms
step:357/1370 train_time:48875ms step_avg:140.85ms
step:358/1370 train_time:49020ms step_avg:140.86ms
step:359/1370 train_time:49164ms step_avg:140.87ms
step:360/1370 train_time:49311ms step_avg:140.89ms
step:361/1370 train_time:49456ms step_avg:140.90ms
step:362/1370 train_time:49601ms step_avg:140.91ms
step:363/1370 train_time:49745ms step_avg:140.92ms
step:364/1370 train_time:49889ms step_avg:140.93ms
step:365/1370 train_time:50035ms step_avg:140.94ms
step:366/1370 train_time:50180ms step_avg:140.95ms
step:367/1370 train_time:50324ms step_avg:140.96ms
step:368/1370 train_time:50468ms step_avg:140.97ms
step:369/1370 train_time:50614ms step_avg:140.99ms
step:370/1370 train_time:50760ms step_avg:141.00ms
step:371/1370 train_time:50903ms step_avg:141.00ms
step:372/1370 train_time:51050ms step_avg:141.02ms
step:373/1370 train_time:51195ms step_avg:141.03ms
step:374/1370 train_time:51340ms step_avg:141.04ms
step:375/1370 train_time:51483ms step_avg:141.05ms
step:375/1370 val_loss:3.7740 train_time:51549ms step_avg:141.23ms
step:376/1370 train_time:51631ms step_avg:141.07ms
step:377/1370 train_time:51775ms step_avg:141.08ms
step:378/1370 train_time:51921ms step_avg:141.09ms
step:379/1370 train_time:52065ms step_avg:141.10ms
step:380/1370 train_time:52209ms step_avg:141.10ms
step:381/1370 train_time:52386ms step_avg:141.20ms
step:382/1370 train_time:52531ms step_avg:141.21ms
step:383/1370 train_time:52674ms step_avg:141.22ms
step:384/1370 train_time:52817ms step_avg:141.22ms
step:385/1370 train_time:52961ms step_avg:141.23ms
step:386/1370 train_time:53105ms step_avg:141.24ms
step:387/1370 train_time:53251ms step_avg:141.25ms
step:388/1370 train_time:53398ms step_avg:141.26ms
step:389/1370 train_time:53545ms step_avg:141.28ms
step:390/1370 train_time:53690ms step_avg:141.29ms
step:391/1370 train_time:53834ms step_avg:141.30ms
step:392/1370 train_time:53978ms step_avg:141.30ms
step:393/1370 train_time:54123ms step_avg:141.31ms
step:394/1370 train_time:54270ms step_avg:141.33ms
step:395/1370 train_time:54414ms step_avg:141.34ms
step:396/1370 train_time:54560ms step_avg:141.35ms
step:397/1370 train_time:54705ms step_avg:141.36ms
step:398/1370 train_time:54851ms step_avg:141.37ms
step:399/1370 train_time:54995ms step_avg:141.37ms
step:400/1370 train_time:55141ms step_avg:141.39ms
step:401/1370 train_time:55287ms step_avg:141.40ms
step:402/1370 train_time:55433ms step_avg:141.41ms
step:403/1370 train_time:55577ms step_avg:141.42ms
step:404/1370 train_time:55724ms step_avg:141.43ms
step:405/1370 train_time:55866ms step_avg:141.43ms
step:406/1370 train_time:56012ms step_avg:141.45ms
step:407/1370 train_time:56157ms step_avg:141.45ms
step:408/1370 train_time:56305ms step_avg:141.47ms
step:409/1370 train_time:56451ms step_avg:141.48ms
step:410/1370 train_time:56597ms step_avg:141.49ms
step:411/1370 train_time:56747ms step_avg:141.51ms
step:412/1370 train_time:56892ms step_avg:141.52ms
step:413/1370 train_time:57038ms step_avg:141.53ms
step:414/1370 train_time:57184ms step_avg:141.55ms
step:415/1370 train_time:57330ms step_avg:141.56ms
step:416/1370 train_time:57475ms step_avg:141.56ms
step:417/1370 train_time:57624ms step_avg:141.58ms
step:418/1370 train_time:57772ms step_avg:141.60ms
step:419/1370 train_time:57916ms step_avg:141.60ms
step:420/1370 train_time:58064ms step_avg:141.62ms
step:421/1370 train_time:58210ms step_avg:141.63ms
step:422/1370 train_time:58355ms step_avg:141.64ms
step:423/1370 train_time:58502ms step_avg:141.65ms
step:424/1370 train_time:58649ms step_avg:141.66ms
step:425/1370 train_time:58795ms step_avg:141.68ms
step:426/1370 train_time:58944ms step_avg:141.69ms
step:427/1370 train_time:59089ms step_avg:141.70ms
step:428/1370 train_time:59236ms step_avg:141.71ms
step:429/1370 train_time:59383ms step_avg:141.73ms
step:430/1370 train_time:59529ms step_avg:141.74ms
step:431/1370 train_time:59674ms step_avg:141.74ms
step:432/1370 train_time:59821ms step_avg:141.76ms
step:433/1370 train_time:59967ms step_avg:141.77ms
step:434/1370 train_time:60113ms step_avg:141.78ms
step:435/1370 train_time:60260ms step_avg:141.79ms
step:436/1370 train_time:60407ms step_avg:141.80ms
step:437/1370 train_time:60553ms step_avg:141.81ms
step:438/1370 train_time:60700ms step_avg:141.82ms
step:439/1370 train_time:60848ms step_avg:141.84ms
step:440/1370 train_time:60994ms step_avg:141.85ms
step:441/1370 train_time:61143ms step_avg:141.86ms
step:442/1370 train_time:61290ms step_avg:141.87ms
step:443/1370 train_time:61437ms step_avg:141.89ms
step:444/1370 train_time:61584ms step_avg:141.90ms
step:445/1370 train_time:61731ms step_avg:141.91ms
step:446/1370 train_time:61876ms step_avg:141.92ms
step:447/1370 train_time:62024ms step_avg:141.93ms
step:448/1370 train_time:62171ms step_avg:141.94ms
step:449/1370 train_time:62317ms step_avg:141.95ms
step:450/1370 train_time:62465ms step_avg:141.96ms
step:451/1370 train_time:62612ms step_avg:141.98ms
step:452/1370 train_time:62758ms step_avg:141.99ms
step:453/1370 train_time:62906ms step_avg:142.00ms
step:454/1370 train_time:63054ms step_avg:142.01ms
step:455/1370 train_time:63202ms step_avg:142.03ms
step:456/1370 train_time:63348ms step_avg:142.04ms
step:457/1370 train_time:63493ms step_avg:142.04ms
step:458/1370 train_time:63641ms step_avg:142.06ms
step:459/1370 train_time:63789ms step_avg:142.07ms
step:460/1370 train_time:63935ms step_avg:142.08ms
step:461/1370 train_time:64083ms step_avg:142.09ms
step:462/1370 train_time:64231ms step_avg:142.10ms
step:463/1370 train_time:64376ms step_avg:142.11ms
step:464/1370 train_time:64525ms step_avg:142.13ms
step:465/1370 train_time:64671ms step_avg:142.13ms
step:466/1370 train_time:64819ms step_avg:142.15ms
step:467/1370 train_time:64969ms step_avg:142.16ms
step:468/1370 train_time:65114ms step_avg:142.17ms
step:469/1370 train_time:65261ms step_avg:142.18ms
step:470/1370 train_time:65408ms step_avg:142.19ms
step:471/1370 train_time:65555ms step_avg:142.20ms
step:472/1370 train_time:65703ms step_avg:142.21ms
step:473/1370 train_time:65849ms step_avg:142.22ms
step:474/1370 train_time:65995ms step_avg:142.23ms
step:475/1370 train_time:66143ms step_avg:142.24ms
step:476/1370 train_time:66289ms step_avg:142.25ms
step:477/1370 train_time:66436ms step_avg:142.26ms
step:478/1370 train_time:66583ms step_avg:142.27ms
step:479/1370 train_time:66732ms step_avg:142.28ms
step:480/1370 train_time:66876ms step_avg:142.29ms
step:481/1370 train_time:67024ms step_avg:142.30ms
step:482/1370 train_time:67171ms step_avg:142.31ms
step:483/1370 train_time:67317ms step_avg:142.32ms
step:484/1370 train_time:67464ms step_avg:142.33ms
step:485/1370 train_time:67611ms step_avg:142.34ms
step:486/1370 train_time:67757ms step_avg:142.35ms
step:487/1370 train_time:67905ms step_avg:142.36ms
step:488/1370 train_time:68053ms step_avg:142.37ms
step:489/1370 train_time:68198ms step_avg:142.38ms
step:490/1370 train_time:68346ms step_avg:142.39ms
step:491/1370 train_time:68492ms step_avg:142.39ms
step:492/1370 train_time:68639ms step_avg:142.40ms
step:493/1370 train_time:68786ms step_avg:142.41ms
step:494/1370 train_time:68934ms step_avg:142.42ms
step:495/1370 train_time:69079ms step_avg:142.43ms
step:496/1370 train_time:69228ms step_avg:142.44ms
step:497/1370 train_time:69374ms step_avg:142.45ms
step:498/1370 train_time:69522ms step_avg:142.46ms
step:499/1370 train_time:69668ms step_avg:142.47ms
step:500/1370 train_time:69815ms step_avg:142.48ms
step:500/1370 val_loss:3.6577 train_time:69882ms step_avg:142.62ms
step:501/1370 train_time:69963ms step_avg:142.49ms
step:502/1370 train_time:70112ms step_avg:142.50ms
step:503/1370 train_time:70258ms step_avg:142.51ms
step:504/1370 train_time:70405ms step_avg:142.52ms
step:505/1370 train_time:70550ms step_avg:142.53ms
step:506/1370 train_time:70696ms step_avg:142.53ms
step:507/1370 train_time:70843ms step_avg:142.54ms
step:508/1370 train_time:70992ms step_avg:142.55ms
step:509/1370 train_time:71140ms step_avg:142.56ms
step:510/1370 train_time:71288ms step_avg:142.58ms
step:511/1370 train_time:71437ms step_avg:142.59ms
step:512/1370 train_time:71584ms step_avg:142.60ms
step:513/1370 train_time:71734ms step_avg:142.61ms
step:514/1370 train_time:71883ms step_avg:142.63ms
step:515/1370 train_time:72033ms step_avg:142.64ms
step:516/1370 train_time:72183ms step_avg:142.65ms
step:517/1370 train_time:72332ms step_avg:142.67ms
step:518/1370 train_time:72478ms step_avg:142.67ms
step:519/1370 train_time:72626ms step_avg:142.68ms
step:520/1370 train_time:72773ms step_avg:142.69ms
step:521/1370 train_time:72920ms step_avg:142.70ms
step:522/1370 train_time:73070ms step_avg:142.71ms
step:523/1370 train_time:73219ms step_avg:142.73ms
step:524/1370 train_time:73369ms step_avg:142.74ms
step:525/1370 train_time:73517ms step_avg:142.75ms
step:526/1370 train_time:73665ms step_avg:142.76ms
step:527/1370 train_time:73813ms step_avg:142.77ms
step:528/1370 train_time:73958ms step_avg:142.78ms
step:529/1370 train_time:74109ms step_avg:142.79ms
step:530/1370 train_time:74258ms step_avg:142.80ms
step:531/1370 train_time:74408ms step_avg:142.82ms
step:532/1370 train_time:74555ms step_avg:142.83ms
step:533/1370 train_time:74704ms step_avg:142.84ms
step:534/1370 train_time:74851ms step_avg:142.85ms
step:535/1370 train_time:74998ms step_avg:142.85ms
step:536/1370 train_time:75147ms step_avg:142.87ms
step:537/1370 train_time:75298ms step_avg:142.88ms
step:538/1370 train_time:75446ms step_avg:142.89ms
step:539/1370 train_time:75594ms step_avg:142.90ms
step:540/1370 train_time:75741ms step_avg:142.91ms
step:541/1370 train_time:75890ms step_avg:142.92ms
step:542/1370 train_time:76037ms step_avg:142.93ms
step:543/1370 train_time:76186ms step_avg:142.94ms
step:544/1370 train_time:76334ms step_avg:142.95ms
step:545/1370 train_time:76481ms step_avg:142.95ms
step:546/1370 train_time:76631ms step_avg:142.97ms
step:547/1370 train_time:76780ms step_avg:142.98ms
step:548/1370 train_time:76930ms step_avg:142.99ms
step:549/1370 train_time:77078ms step_avg:143.00ms
step:550/1370 train_time:77229ms step_avg:143.02ms
step:551/1370 train_time:77376ms step_avg:143.02ms
step:552/1370 train_time:77525ms step_avg:143.04ms
step:553/1370 train_time:77672ms step_avg:143.04ms
step:554/1370 train_time:77820ms step_avg:143.05ms
step:555/1370 train_time:77969ms step_avg:143.06ms
step:556/1370 train_time:78116ms step_avg:143.07ms
step:557/1370 train_time:78265ms step_avg:143.08ms
step:558/1370 train_time:78413ms step_avg:143.09ms
step:559/1370 train_time:78560ms step_avg:143.10ms
step:560/1370 train_time:78710ms step_avg:143.11ms
step:561/1370 train_time:78857ms step_avg:143.12ms
step:562/1370 train_time:79006ms step_avg:143.13ms
step:563/1370 train_time:79154ms step_avg:143.14ms
step:564/1370 train_time:79302ms step_avg:143.14ms
step:565/1370 train_time:79450ms step_avg:143.15ms
step:566/1370 train_time:79598ms step_avg:143.16ms
step:567/1370 train_time:79747ms step_avg:143.17ms
step:568/1370 train_time:79896ms step_avg:143.18ms
step:569/1370 train_time:80044ms step_avg:143.19ms
step:570/1370 train_time:80192ms step_avg:143.20ms
step:571/1370 train_time:80373ms step_avg:143.27ms
step:572/1370 train_time:80519ms step_avg:143.27ms
step:573/1370 train_time:80667ms step_avg:143.28ms
step:574/1370 train_time:80816ms step_avg:143.29ms
step:575/1370 train_time:80962ms step_avg:143.30ms
step:576/1370 train_time:81110ms step_avg:143.30ms
step:577/1370 train_time:81259ms step_avg:143.31ms
step:578/1370 train_time:81412ms step_avg:143.33ms
step:579/1370 train_time:81559ms step_avg:143.34ms
step:580/1370 train_time:81710ms step_avg:143.35ms
step:581/1370 train_time:81856ms step_avg:143.36ms
step:582/1370 train_time:82003ms step_avg:143.36ms
step:583/1370 train_time:82150ms step_avg:143.37ms
step:584/1370 train_time:82300ms step_avg:143.38ms
step:585/1370 train_time:82452ms step_avg:143.39ms
step:586/1370 train_time:82600ms step_avg:143.40ms
step:587/1370 train_time:82749ms step_avg:143.41ms
step:588/1370 train_time:82897ms step_avg:143.42ms
step:589/1370 train_time:83045ms step_avg:143.43ms
step:590/1370 train_time:83194ms step_avg:143.44ms
step:591/1370 train_time:83341ms step_avg:143.44ms
step:592/1370 train_time:83493ms step_avg:143.46ms
step:593/1370 train_time:83642ms step_avg:143.47ms
step:594/1370 train_time:83791ms step_avg:143.48ms
step:595/1370 train_time:83939ms step_avg:143.49ms
step:596/1370 train_time:84088ms step_avg:143.49ms
step:597/1370 train_time:84236ms step_avg:143.50ms
step:598/1370 train_time:84386ms step_avg:143.51ms
step:599/1370 train_time:84535ms step_avg:143.52ms
step:600/1370 train_time:84682ms step_avg:143.53ms
step:601/1370 train_time:84833ms step_avg:143.54ms
step:602/1370 train_time:84980ms step_avg:143.55ms
step:603/1370 train_time:85131ms step_avg:143.56ms
step:604/1370 train_time:85277ms step_avg:143.56ms
step:605/1370 train_time:85428ms step_avg:143.58ms
step:606/1370 train_time:85575ms step_avg:143.58ms
step:607/1370 train_time:85723ms step_avg:143.59ms
step:608/1370 train_time:85872ms step_avg:143.60ms
step:609/1370 train_time:86020ms step_avg:143.61ms
step:610/1370 train_time:86169ms step_avg:143.61ms
step:611/1370 train_time:86317ms step_avg:143.62ms
step:612/1370 train_time:86468ms step_avg:143.63ms
step:613/1370 train_time:86617ms step_avg:143.64ms
step:614/1370 train_time:86766ms step_avg:143.65ms
step:615/1370 train_time:86917ms step_avg:143.66ms
step:616/1370 train_time:87066ms step_avg:143.67ms
step:617/1370 train_time:87216ms step_avg:143.68ms
step:618/1370 train_time:87366ms step_avg:143.69ms
step:619/1370 train_time:87517ms step_avg:143.71ms
step:620/1370 train_time:87665ms step_avg:143.71ms
step:621/1370 train_time:87815ms step_avg:143.72ms
step:622/1370 train_time:87963ms step_avg:143.73ms
step:623/1370 train_time:88114ms step_avg:143.74ms
step:624/1370 train_time:88263ms step_avg:143.75ms
step:625/1370 train_time:88414ms step_avg:143.76ms
step:625/1370 val_loss:3.5761 train_time:88482ms step_avg:143.87ms
step:626/1370 train_time:88564ms step_avg:143.77ms
step:627/1370 train_time:88717ms step_avg:143.79ms
step:628/1370 train_time:88865ms step_avg:143.79ms
step:629/1370 train_time:89013ms step_avg:143.80ms
step:630/1370 train_time:89161ms step_avg:143.81ms
step:631/1370 train_time:89309ms step_avg:143.81ms
step:632/1370 train_time:89461ms step_avg:143.83ms
step:633/1370 train_time:89612ms step_avg:143.84ms
step:634/1370 train_time:89761ms step_avg:143.85ms
step:635/1370 train_time:89911ms step_avg:143.86ms
step:636/1370 train_time:90060ms step_avg:143.87ms
step:637/1370 train_time:90210ms step_avg:143.88ms
step:638/1370 train_time:90359ms step_avg:143.88ms
step:639/1370 train_time:90508ms step_avg:143.89ms
step:640/1370 train_time:90660ms step_avg:143.90ms
step:641/1370 train_time:90810ms step_avg:143.91ms
step:642/1370 train_time:90959ms step_avg:143.92ms
step:643/1370 train_time:91109ms step_avg:143.93ms
step:644/1370 train_time:91259ms step_avg:143.94ms
step:645/1370 train_time:91410ms step_avg:143.95ms
step:646/1370 train_time:91559ms step_avg:143.96ms
step:647/1370 train_time:91708ms step_avg:143.97ms
step:648/1370 train_time:91861ms step_avg:143.98ms
step:649/1370 train_time:92012ms step_avg:143.99ms
step:650/1370 train_time:92162ms step_avg:144.00ms
step:651/1370 train_time:92313ms step_avg:144.01ms
step:652/1370 train_time:92462ms step_avg:144.02ms
step:653/1370 train_time:92613ms step_avg:144.03ms
step:654/1370 train_time:92764ms step_avg:144.04ms
step:655/1370 train_time:92915ms step_avg:144.05ms
step:656/1370 train_time:93064ms step_avg:144.06ms
step:657/1370 train_time:93214ms step_avg:144.07ms
step:658/1370 train_time:93363ms step_avg:144.08ms
step:659/1370 train_time:93513ms step_avg:144.09ms
step:660/1370 train_time:93662ms step_avg:144.10ms
step:661/1370 train_time:93813ms step_avg:144.11ms
step:662/1370 train_time:93962ms step_avg:144.11ms
step:663/1370 train_time:94113ms step_avg:144.12ms
step:664/1370 train_time:94262ms step_avg:144.13ms
step:665/1370 train_time:94413ms step_avg:144.14ms
step:666/1370 train_time:94561ms step_avg:144.15ms
step:667/1370 train_time:94709ms step_avg:144.15ms
step:668/1370 train_time:94860ms step_avg:144.16ms
step:669/1370 train_time:95009ms step_avg:144.17ms
step:670/1370 train_time:95159ms step_avg:144.18ms
step:671/1370 train_time:95309ms step_avg:144.19ms
step:672/1370 train_time:95459ms step_avg:144.20ms
step:673/1370 train_time:95610ms step_avg:144.21ms
step:674/1370 train_time:95759ms step_avg:144.22ms
step:675/1370 train_time:95909ms step_avg:144.22ms
step:676/1370 train_time:96058ms step_avg:144.23ms
step:677/1370 train_time:96206ms step_avg:144.24ms
step:678/1370 train_time:96356ms step_avg:144.25ms
step:679/1370 train_time:96505ms step_avg:144.25ms
step:680/1370 train_time:96657ms step_avg:144.26ms
step:681/1370 train_time:96805ms step_avg:144.27ms
step:682/1370 train_time:96957ms step_avg:144.28ms
step:683/1370 train_time:97105ms step_avg:144.29ms
step:684/1370 train_time:97255ms step_avg:144.30ms
step:685/1370 train_time:97405ms step_avg:144.30ms
step:686/1370 train_time:97554ms step_avg:144.31ms
step:687/1370 train_time:97703ms step_avg:144.32ms
step:688/1370 train_time:97856ms step_avg:144.33ms
step:689/1370 train_time:98003ms step_avg:144.33ms
step:690/1370 train_time:98157ms step_avg:144.35ms
step:691/1370 train_time:98305ms step_avg:144.35ms
step:692/1370 train_time:98456ms step_avg:144.36ms
step:693/1370 train_time:98603ms step_avg:144.37ms
step:694/1370 train_time:98753ms step_avg:144.38ms
step:695/1370 train_time:98902ms step_avg:144.38ms
step:696/1370 train_time:99053ms step_avg:144.39ms
step:697/1370 train_time:99202ms step_avg:144.40ms
step:698/1370 train_time:99353ms step_avg:144.41ms
step:699/1370 train_time:99502ms step_avg:144.41ms
step:700/1370 train_time:99651ms step_avg:144.42ms
step:701/1370 train_time:99800ms step_avg:144.43ms
step:702/1370 train_time:99952ms step_avg:144.44ms
step:703/1370 train_time:100101ms step_avg:144.45ms
step:704/1370 train_time:100252ms step_avg:144.46ms
step:705/1370 train_time:100403ms step_avg:144.47ms
step:706/1370 train_time:100557ms step_avg:144.48ms
step:707/1370 train_time:100707ms step_avg:144.49ms
step:708/1370 train_time:100858ms step_avg:144.50ms
step:709/1370 train_time:101008ms step_avg:144.50ms
step:710/1370 train_time:101158ms step_avg:144.51ms
step:711/1370 train_time:101309ms step_avg:144.52ms
step:712/1370 train_time:101461ms step_avg:144.53ms
step:713/1370 train_time:101614ms step_avg:144.54ms
step:714/1370 train_time:101764ms step_avg:144.55ms
step:715/1370 train_time:101916ms step_avg:144.56ms
step:716/1370 train_time:102064ms step_avg:144.57ms
step:717/1370 train_time:102217ms step_avg:144.58ms
step:718/1370 train_time:102367ms step_avg:144.59ms
step:719/1370 train_time:102519ms step_avg:144.60ms
step:720/1370 train_time:102670ms step_avg:144.61ms
step:721/1370 train_time:102821ms step_avg:144.61ms
step:722/1370 train_time:102973ms step_avg:144.63ms
step:723/1370 train_time:103122ms step_avg:144.63ms
step:724/1370 train_time:103274ms step_avg:144.64ms
step:725/1370 train_time:103424ms step_avg:144.65ms
step:726/1370 train_time:103576ms step_avg:144.66ms
step:727/1370 train_time:103727ms step_avg:144.67ms
step:728/1370 train_time:103879ms step_avg:144.68ms
step:729/1370 train_time:104028ms step_avg:144.68ms
step:730/1370 train_time:104181ms step_avg:144.70ms
step:731/1370 train_time:104332ms step_avg:144.70ms
step:732/1370 train_time:104483ms step_avg:144.71ms
step:733/1370 train_time:104634ms step_avg:144.72ms
step:734/1370 train_time:104784ms step_avg:144.73ms
step:735/1370 train_time:104935ms step_avg:144.74ms
step:736/1370 train_time:105088ms step_avg:144.75ms
step:737/1370 train_time:105238ms step_avg:144.76ms
step:738/1370 train_time:105393ms step_avg:144.77ms
step:739/1370 train_time:105542ms step_avg:144.78ms
step:740/1370 train_time:105695ms step_avg:144.79ms
step:741/1370 train_time:105846ms step_avg:144.80ms
step:742/1370 train_time:105997ms step_avg:144.81ms
step:743/1370 train_time:106147ms step_avg:144.81ms
step:744/1370 train_time:106299ms step_avg:144.82ms
step:745/1370 train_time:106451ms step_avg:144.83ms
step:746/1370 train_time:106602ms step_avg:144.84ms
step:747/1370 train_time:106754ms step_avg:144.85ms
step:748/1370 train_time:106904ms step_avg:144.86ms
step:749/1370 train_time:107056ms step_avg:144.87ms
step:750/1370 train_time:107205ms step_avg:144.87ms
step:750/1370 val_loss:3.5228 train_time:107276ms step_avg:144.97ms
step:751/1370 train_time:107359ms step_avg:144.88ms
step:752/1370 train_time:107513ms step_avg:144.90ms
step:753/1370 train_time:107661ms step_avg:144.90ms
step:754/1370 train_time:107811ms step_avg:144.91ms
step:755/1370 train_time:107961ms step_avg:144.91ms
step:756/1370 train_time:108112ms step_avg:144.92ms
step:757/1370 train_time:108263ms step_avg:144.93ms
step:758/1370 train_time:108417ms step_avg:144.94ms
step:759/1370 train_time:108568ms step_avg:144.95ms
step:760/1370 train_time:108720ms step_avg:144.96ms
step:761/1370 train_time:108905ms step_avg:145.01ms
step:762/1370 train_time:109056ms step_avg:145.02ms
step:763/1370 train_time:109204ms step_avg:145.03ms
step:764/1370 train_time:109356ms step_avg:145.03ms
step:765/1370 train_time:109505ms step_avg:145.04ms
step:766/1370 train_time:109659ms step_avg:145.05ms
step:767/1370 train_time:109812ms step_avg:145.06ms
step:768/1370 train_time:109965ms step_avg:145.07ms
step:769/1370 train_time:110118ms step_avg:145.08ms
step:770/1370 train_time:110267ms step_avg:145.09ms
step:771/1370 train_time:110418ms step_avg:145.10ms
step:772/1370 train_time:110567ms step_avg:145.10ms
step:773/1370 train_time:110719ms step_avg:145.11ms
step:774/1370 train_time:110871ms step_avg:145.12ms
step:775/1370 train_time:111023ms step_avg:145.13ms
step:776/1370 train_time:111177ms step_avg:145.14ms
step:777/1370 train_time:111327ms step_avg:145.15ms
step:778/1370 train_time:111476ms step_avg:145.15ms
step:779/1370 train_time:111625ms step_avg:145.16ms
step:780/1370 train_time:111778ms step_avg:145.17ms
step:781/1370 train_time:111928ms step_avg:145.17ms
step:782/1370 train_time:112080ms step_avg:145.18ms
step:783/1370 train_time:112232ms step_avg:145.19ms
step:784/1370 train_time:112381ms step_avg:145.20ms
step:785/1370 train_time:112534ms step_avg:145.21ms
step:786/1370 train_time:112684ms step_avg:145.21ms
step:787/1370 train_time:112837ms step_avg:145.22ms
step:788/1370 train_time:112986ms step_avg:145.23ms
step:789/1370 train_time:113138ms step_avg:145.23ms
step:790/1370 train_time:113286ms step_avg:145.24ms
step:791/1370 train_time:113439ms step_avg:145.25ms
step:792/1370 train_time:113591ms step_avg:145.26ms
step:793/1370 train_time:113740ms step_avg:145.26ms
step:794/1370 train_time:113891ms step_avg:145.27ms
step:795/1370 train_time:114043ms step_avg:145.28ms
step:796/1370 train_time:114195ms step_avg:145.29ms
step:797/1370 train_time:114345ms step_avg:145.29ms
step:798/1370 train_time:114498ms step_avg:145.30ms
step:799/1370 train_time:114653ms step_avg:145.31ms
step:800/1370 train_time:114802ms step_avg:145.32ms
step:801/1370 train_time:114954ms step_avg:145.33ms
step:802/1370 train_time:115105ms step_avg:145.33ms
step:803/1370 train_time:115256ms step_avg:145.34ms
step:804/1370 train_time:115406ms step_avg:145.35ms
step:805/1370 train_time:115560ms step_avg:145.36ms
step:806/1370 train_time:115713ms step_avg:145.37ms
step:807/1370 train_time:115862ms step_avg:145.37ms
step:808/1370 train_time:116013ms step_avg:145.38ms
step:809/1370 train_time:116162ms step_avg:145.38ms
step:810/1370 train_time:116314ms step_avg:145.39ms
step:811/1370 train_time:116466ms step_avg:145.40ms
step:812/1370 train_time:116618ms step_avg:145.41ms
step:813/1370 train_time:116767ms step_avg:145.41ms
step:814/1370 train_time:116920ms step_avg:145.42ms
step:815/1370 train_time:117071ms step_avg:145.43ms
step:816/1370 train_time:117224ms step_avg:145.44ms
step:817/1370 train_time:117378ms step_avg:145.45ms
step:818/1370 train_time:117528ms step_avg:145.46ms
step:819/1370 train_time:117682ms step_avg:145.47ms
step:820/1370 train_time:117838ms step_avg:145.48ms
step:821/1370 train_time:117988ms step_avg:145.48ms
step:822/1370 train_time:118139ms step_avg:145.49ms
step:823/1370 train_time:118293ms step_avg:145.50ms
step:824/1370 train_time:118446ms step_avg:145.51ms
step:825/1370 train_time:118599ms step_avg:145.52ms
step:826/1370 train_time:118754ms step_avg:145.53ms
step:827/1370 train_time:118906ms step_avg:145.54ms
step:828/1370 train_time:119059ms step_avg:145.55ms
step:829/1370 train_time:119212ms step_avg:145.56ms
step:830/1370 train_time:119364ms step_avg:145.57ms
step:831/1370 train_time:119517ms step_avg:145.58ms
step:832/1370 train_time:119668ms step_avg:145.58ms
step:833/1370 train_time:119820ms step_avg:145.59ms
step:834/1370 train_time:119972ms step_avg:145.60ms
step:835/1370 train_time:120125ms step_avg:145.61ms
step:836/1370 train_time:120280ms step_avg:145.62ms
step:837/1370 train_time:120433ms step_avg:145.63ms
step:838/1370 train_time:120583ms step_avg:145.63ms
step:839/1370 train_time:120737ms step_avg:145.64ms
step:840/1370 train_time:120887ms step_avg:145.65ms
step:841/1370 train_time:121040ms step_avg:145.66ms
step:842/1370 train_time:121192ms step_avg:145.66ms
step:843/1370 train_time:121342ms step_avg:145.67ms
step:844/1370 train_time:121494ms step_avg:145.68ms
step:845/1370 train_time:121645ms step_avg:145.68ms
step:846/1370 train_time:121797ms step_avg:145.69ms
step:847/1370 train_time:121952ms step_avg:145.70ms
step:848/1370 train_time:122103ms step_avg:145.71ms
step:849/1370 train_time:122258ms step_avg:145.72ms
step:850/1370 train_time:122413ms step_avg:145.73ms
step:851/1370 train_time:122562ms step_avg:145.73ms
step:852/1370 train_time:122717ms step_avg:145.74ms
step:853/1370 train_time:122866ms step_avg:145.75ms
step:854/1370 train_time:123019ms step_avg:145.76ms
step:855/1370 train_time:123170ms step_avg:145.76ms
step:856/1370 train_time:123320ms step_avg:145.77ms
step:857/1370 train_time:123474ms step_avg:145.78ms
step:858/1370 train_time:123629ms step_avg:145.79ms
step:859/1370 train_time:123781ms step_avg:145.80ms
step:860/1370 train_time:123933ms step_avg:145.80ms
step:861/1370 train_time:124085ms step_avg:145.81ms
step:862/1370 train_time:124237ms step_avg:145.82ms
step:863/1370 train_time:124389ms step_avg:145.83ms
step:864/1370 train_time:124542ms step_avg:145.83ms
step:865/1370 train_time:124693ms step_avg:145.84ms
step:866/1370 train_time:124849ms step_avg:145.85ms
step:867/1370 train_time:124999ms step_avg:145.86ms
step:868/1370 train_time:125152ms step_avg:145.86ms
step:869/1370 train_time:125304ms step_avg:145.87ms
step:870/1370 train_time:125459ms step_avg:145.88ms
step:871/1370 train_time:125611ms step_avg:145.89ms
step:872/1370 train_time:125762ms step_avg:145.90ms
step:873/1370 train_time:125915ms step_avg:145.90ms
step:874/1370 train_time:126067ms step_avg:145.91ms
step:875/1370 train_time:126220ms step_avg:145.92ms
step:875/1370 val_loss:3.4698 train_time:126293ms step_avg:146.00ms
step:876/1370 train_time:126375ms step_avg:145.93ms
step:877/1370 train_time:126529ms step_avg:145.94ms
step:878/1370 train_time:126682ms step_avg:145.95ms
step:879/1370 train_time:126832ms step_avg:145.95ms
step:880/1370 train_time:126986ms step_avg:145.96ms
step:881/1370 train_time:127135ms step_avg:145.96ms
step:882/1370 train_time:127290ms step_avg:145.98ms
step:883/1370 train_time:127443ms step_avg:145.98ms
step:884/1370 train_time:127595ms step_avg:145.99ms
step:885/1370 train_time:127747ms step_avg:146.00ms
step:886/1370 train_time:127903ms step_avg:146.01ms
step:887/1370 train_time:128053ms step_avg:146.01ms
step:888/1370 train_time:128208ms step_avg:146.02ms
step:889/1370 train_time:128360ms step_avg:146.03ms
step:890/1370 train_time:128510ms step_avg:146.03ms
step:891/1370 train_time:128663ms step_avg:146.04ms
step:892/1370 train_time:128815ms step_avg:146.05ms
step:893/1370 train_time:128968ms step_avg:146.06ms
step:894/1370 train_time:129120ms step_avg:146.06ms
step:895/1370 train_time:129275ms step_avg:146.07ms
step:896/1370 train_time:129427ms step_avg:146.08ms
step:897/1370 train_time:129578ms step_avg:146.09ms
step:898/1370 train_time:129731ms step_avg:146.09ms
step:899/1370 train_time:129883ms step_avg:146.10ms
step:900/1370 train_time:130032ms step_avg:146.10ms
step:901/1370 train_time:130189ms step_avg:146.12ms
step:902/1370 train_time:130338ms step_avg:146.12ms
step:903/1370 train_time:130491ms step_avg:146.13ms
step:904/1370 train_time:130643ms step_avg:146.13ms
step:905/1370 train_time:130795ms step_avg:146.14ms
step:906/1370 train_time:130947ms step_avg:146.15ms
step:907/1370 train_time:131103ms step_avg:146.16ms
step:908/1370 train_time:131253ms step_avg:146.16ms
step:909/1370 train_time:131407ms step_avg:146.17ms
step:910/1370 train_time:131564ms step_avg:146.18ms
step:911/1370 train_time:131715ms step_avg:146.19ms
step:912/1370 train_time:131868ms step_avg:146.20ms
step:913/1370 train_time:132022ms step_avg:146.20ms
step:914/1370 train_time:132174ms step_avg:146.21ms
step:915/1370 train_time:132327ms step_avg:146.22ms
step:916/1370 train_time:132482ms step_avg:146.23ms
step:917/1370 train_time:132635ms step_avg:146.23ms
step:918/1370 train_time:132789ms step_avg:146.24ms
step:919/1370 train_time:132944ms step_avg:146.25ms
step:920/1370 train_time:133098ms step_avg:146.26ms
step:921/1370 train_time:133251ms step_avg:146.27ms
step:922/1370 train_time:133410ms step_avg:146.28ms
step:923/1370 train_time:133564ms step_avg:146.29ms
step:924/1370 train_time:133717ms step_avg:146.30ms
step:925/1370 train_time:133871ms step_avg:146.31ms
step:926/1370 train_time:134028ms step_avg:146.32ms
step:927/1370 train_time:134180ms step_avg:146.33ms
step:928/1370 train_time:134333ms step_avg:146.33ms
step:929/1370 train_time:134490ms step_avg:146.34ms
step:930/1370 train_time:134645ms step_avg:146.35ms
step:931/1370 train_time:134797ms step_avg:146.36ms
step:932/1370 train_time:134949ms step_avg:146.37ms
step:933/1370 train_time:135103ms step_avg:146.37ms
step:934/1370 train_time:135257ms step_avg:146.38ms
step:935/1370 train_time:135409ms step_avg:146.39ms
step:936/1370 train_time:135565ms step_avg:146.40ms
step:937/1370 train_time:135724ms step_avg:146.41ms
step:938/1370 train_time:135878ms step_avg:146.42ms
step:939/1370 train_time:136032ms step_avg:146.43ms
step:940/1370 train_time:136187ms step_avg:146.44ms
step:941/1370 train_time:136337ms step_avg:146.44ms
step:942/1370 train_time:136491ms step_avg:146.45ms
step:943/1370 train_time:136646ms step_avg:146.46ms
step:944/1370 train_time:136805ms step_avg:146.47ms
step:945/1370 train_time:136957ms step_avg:146.48ms
step:946/1370 train_time:137112ms step_avg:146.49ms
step:947/1370 train_time:137266ms step_avg:146.49ms
step:948/1370 train_time:137418ms step_avg:146.50ms
step:949/1370 train_time:137573ms step_avg:146.51ms
step:950/1370 train_time:137728ms step_avg:146.52ms
step:951/1370 train_time:137922ms step_avg:146.57ms
step:952/1370 train_time:138074ms step_avg:146.58ms
step:953/1370 train_time:138228ms step_avg:146.58ms
step:954/1370 train_time:138382ms step_avg:146.59ms
step:955/1370 train_time:138533ms step_avg:146.60ms
step:956/1370 train_time:138686ms step_avg:146.60ms
step:957/1370 train_time:138841ms step_avg:146.61ms
step:958/1370 train_time:138999ms step_avg:146.62ms
step:959/1370 train_time:139154ms step_avg:146.63ms
step:960/1370 train_time:139309ms step_avg:146.64ms
step:961/1370 train_time:139462ms step_avg:146.65ms
step:962/1370 train_time:139613ms step_avg:146.65ms
step:963/1370 train_time:139769ms step_avg:146.66ms
step:964/1370 train_time:139924ms step_avg:146.67ms
step:965/1370 train_time:140077ms step_avg:146.68ms
step:966/1370 train_time:140230ms step_avg:146.68ms
step:967/1370 train_time:140383ms step_avg:146.69ms
step:968/1370 train_time:140535ms step_avg:146.70ms
step:969/1370 train_time:140690ms step_avg:146.70ms
step:970/1370 train_time:140843ms step_avg:146.71ms
step:971/1370 train_time:140997ms step_avg:146.72ms
step:972/1370 train_time:141149ms step_avg:146.72ms
step:973/1370 train_time:141301ms step_avg:146.73ms
step:974/1370 train_time:141454ms step_avg:146.74ms
step:975/1370 train_time:141609ms step_avg:146.74ms
step:976/1370 train_time:141763ms step_avg:146.75ms
step:977/1370 train_time:141915ms step_avg:146.76ms
step:978/1370 train_time:142069ms step_avg:146.77ms
step:979/1370 train_time:142223ms step_avg:146.77ms
step:980/1370 train_time:142375ms step_avg:146.78ms
step:981/1370 train_time:142526ms step_avg:146.78ms
step:982/1370 train_time:142679ms step_avg:146.79ms
step:983/1370 train_time:142831ms step_avg:146.79ms
step:984/1370 train_time:142984ms step_avg:146.80ms
step:985/1370 train_time:143137ms step_avg:146.81ms
step:986/1370 train_time:143293ms step_avg:146.82ms
step:987/1370 train_time:143445ms step_avg:146.82ms
step:988/1370 train_time:143599ms step_avg:146.83ms
step:989/1370 train_time:143752ms step_avg:146.84ms
step:990/1370 train_time:143908ms step_avg:146.84ms
step:991/1370 train_time:144060ms step_avg:146.85ms
step:992/1370 train_time:144216ms step_avg:146.86ms
step:993/1370 train_time:144378ms step_avg:146.87ms
step:994/1370 train_time:144531ms step_avg:146.88ms
step:995/1370 train_time:144685ms step_avg:146.89ms
step:996/1370 train_time:144835ms step_avg:146.89ms
step:997/1370 train_time:144989ms step_avg:146.90ms
step:998/1370 train_time:145142ms step_avg:146.91ms
step:999/1370 train_time:145294ms step_avg:146.91ms
step:1000/1370 train_time:145448ms step_avg:146.92ms
step:1000/1370 val_loss:3.4040 train_time:145519ms step_avg:146.99ms
step:1001/1370 train_time:145604ms step_avg:146.93ms
step:1002/1370 train_time:145758ms step_avg:146.93ms
step:1003/1370 train_time:145912ms step_avg:146.94ms
step:1004/1370 train_time:146067ms step_avg:146.95ms
step:1005/1370 train_time:146219ms step_avg:146.95ms
step:1006/1370 train_time:146370ms step_avg:146.96ms
step:1007/1370 train_time:146526ms step_avg:146.97ms
step:1008/1370 train_time:146681ms step_avg:146.97ms
step:1009/1370 train_time:146841ms step_avg:146.99ms
step:1010/1370 train_time:146993ms step_avg:146.99ms
step:1011/1370 train_time:147147ms step_avg:147.00ms
step:1012/1370 train_time:147298ms step_avg:147.00ms
step:1013/1370 train_time:147452ms step_avg:147.01ms
step:1014/1370 train_time:147607ms step_avg:147.02ms
step:1015/1370 train_time:147759ms step_avg:147.02ms
step:1016/1370 train_time:147913ms step_avg:147.03ms
step:1017/1370 train_time:148069ms step_avg:147.04ms
step:1018/1370 train_time:148222ms step_avg:147.05ms
step:1019/1370 train_time:148377ms step_avg:147.05ms
step:1020/1370 train_time:148534ms step_avg:147.06ms
step:1021/1370 train_time:148688ms step_avg:147.07ms
step:1022/1370 train_time:148846ms step_avg:147.08ms
step:1023/1370 train_time:148999ms step_avg:147.09ms
step:1024/1370 train_time:149152ms step_avg:147.09ms
step:1025/1370 train_time:149308ms step_avg:147.10ms
step:1026/1370 train_time:149461ms step_avg:147.11ms
step:1027/1370 train_time:149614ms step_avg:147.11ms
step:1028/1370 train_time:149770ms step_avg:147.12ms
step:1029/1370 train_time:149927ms step_avg:147.13ms
step:1030/1370 train_time:150081ms step_avg:147.14ms
step:1031/1370 train_time:150233ms step_avg:147.14ms
step:1032/1370 train_time:150387ms step_avg:147.15ms
step:1033/1370 train_time:150541ms step_avg:147.16ms
step:1034/1370 train_time:150696ms step_avg:147.16ms
step:1035/1370 train_time:150853ms step_avg:147.17ms
step:1036/1370 train_time:151010ms step_avg:147.18ms
step:1037/1370 train_time:151167ms step_avg:147.19ms
step:1038/1370 train_time:151322ms step_avg:147.20ms
step:1039/1370 train_time:151475ms step_avg:147.21ms
step:1040/1370 train_time:151629ms step_avg:147.21ms
step:1041/1370 train_time:151784ms step_avg:147.22ms
step:1042/1370 train_time:151937ms step_avg:147.23ms
step:1043/1370 train_time:152091ms step_avg:147.23ms
step:1044/1370 train_time:152247ms step_avg:147.24ms
step:1045/1370 train_time:152403ms step_avg:147.25ms
step:1046/1370 train_time:152555ms step_avg:147.25ms
step:1047/1370 train_time:152709ms step_avg:147.26ms
step:1048/1370 train_time:152866ms step_avg:147.27ms
step:1049/1370 train_time:153022ms step_avg:147.28ms
step:1050/1370 train_time:153177ms step_avg:147.29ms
step:1051/1370 train_time:153337ms step_avg:147.30ms
step:1052/1370 train_time:153490ms step_avg:147.30ms
step:1053/1370 train_time:153645ms step_avg:147.31ms
step:1054/1370 train_time:153800ms step_avg:147.32ms
step:1055/1370 train_time:153953ms step_avg:147.32ms
step:1056/1370 train_time:154108ms step_avg:147.33ms
step:1057/1370 train_time:154261ms step_avg:147.34ms
step:1058/1370 train_time:154416ms step_avg:147.34ms
step:1059/1370 train_time:154574ms step_avg:147.35ms
step:1060/1370 train_time:154730ms step_avg:147.36ms
step:1061/1370 train_time:154884ms step_avg:147.37ms
step:1062/1370 train_time:155039ms step_avg:147.38ms
step:1063/1370 train_time:155193ms step_avg:147.38ms
step:1064/1370 train_time:155346ms step_avg:147.39ms
step:1065/1370 train_time:155500ms step_avg:147.39ms
step:1066/1370 train_time:155657ms step_avg:147.40ms
step:1067/1370 train_time:155813ms step_avg:147.41ms
step:1068/1370 train_time:155969ms step_avg:147.42ms
step:1069/1370 train_time:156128ms step_avg:147.43ms
step:1070/1370 train_time:156280ms step_avg:147.43ms
step:1071/1370 train_time:156434ms step_avg:147.44ms
step:1072/1370 train_time:156587ms step_avg:147.45ms
step:1073/1370 train_time:156739ms step_avg:147.45ms
step:1074/1370 train_time:156893ms step_avg:147.46ms
step:1075/1370 train_time:157048ms step_avg:147.46ms
step:1076/1370 train_time:157201ms step_avg:147.47ms
step:1077/1370 train_time:157353ms step_avg:147.47ms
step:1078/1370 train_time:157513ms step_avg:147.48ms
step:1079/1370 train_time:157670ms step_avg:147.49ms
step:1080/1370 train_time:157826ms step_avg:147.50ms
step:1081/1370 train_time:157979ms step_avg:147.51ms
step:1082/1370 train_time:158133ms step_avg:147.51ms
step:1083/1370 train_time:158287ms step_avg:147.52ms
step:1084/1370 train_time:158444ms step_avg:147.53ms
step:1085/1370 train_time:158596ms step_avg:147.53ms
step:1086/1370 train_time:158753ms step_avg:147.54ms
step:1087/1370 train_time:158911ms step_avg:147.55ms
step:1088/1370 train_time:159066ms step_avg:147.56ms
step:1089/1370 train_time:159222ms step_avg:147.56ms
step:1090/1370 train_time:159379ms step_avg:147.57ms
step:1091/1370 train_time:159535ms step_avg:147.58ms
step:1092/1370 train_time:159688ms step_avg:147.59ms
step:1093/1370 train_time:159845ms step_avg:147.60ms
step:1094/1370 train_time:159999ms step_avg:147.60ms
step:1095/1370 train_time:160154ms step_avg:147.61ms
step:1096/1370 train_time:160313ms step_avg:147.62ms
step:1097/1370 train_time:160469ms step_avg:147.63ms
step:1098/1370 train_time:160623ms step_avg:147.63ms
step:1099/1370 train_time:160776ms step_avg:147.64ms
step:1100/1370 train_time:160931ms step_avg:147.64ms
step:1101/1370 train_time:161085ms step_avg:147.65ms
step:1102/1370 train_time:161244ms step_avg:147.66ms
step:1103/1370 train_time:161399ms step_avg:147.67ms
step:1104/1370 train_time:161553ms step_avg:147.67ms
step:1105/1370 train_time:161708ms step_avg:147.68ms
step:1106/1370 train_time:161862ms step_avg:147.68ms
step:1107/1370 train_time:162016ms step_avg:147.69ms
step:1108/1370 train_time:162172ms step_avg:147.70ms
step:1109/1370 train_time:162327ms step_avg:147.70ms
step:1110/1370 train_time:162483ms step_avg:147.71ms
step:1111/1370 train_time:162637ms step_avg:147.72ms
step:1112/1370 train_time:162791ms step_avg:147.72ms
step:1113/1370 train_time:162946ms step_avg:147.73ms
step:1114/1370 train_time:163101ms step_avg:147.74ms
step:1115/1370 train_time:163256ms step_avg:147.74ms
step:1116/1370 train_time:163410ms step_avg:147.75ms
step:1117/1370 train_time:163569ms step_avg:147.76ms
step:1118/1370 train_time:163728ms step_avg:147.77ms
step:1119/1370 train_time:163882ms step_avg:147.77ms
step:1120/1370 train_time:164038ms step_avg:147.78ms
step:1121/1370 train_time:164193ms step_avg:147.79ms
step:1122/1370 train_time:164351ms step_avg:147.80ms
step:1123/1370 train_time:164506ms step_avg:147.80ms
step:1124/1370 train_time:164663ms step_avg:147.81ms
step:1125/1370 train_time:164819ms step_avg:147.82ms
step:1125/1370 val_loss:3.3506 train_time:164891ms step_avg:147.88ms
step:1126/1370 train_time:164975ms step_avg:147.83ms
step:1127/1370 train_time:165132ms step_avg:147.83ms
step:1128/1370 train_time:165286ms step_avg:147.84ms
step:1129/1370 train_time:165444ms step_avg:147.85ms
step:1130/1370 train_time:165599ms step_avg:147.86ms
step:1131/1370 train_time:165757ms step_avg:147.87ms
step:1132/1370 train_time:165911ms step_avg:147.87ms
step:1133/1370 train_time:166067ms step_avg:147.88ms
step:1134/1370 train_time:166223ms step_avg:147.89ms
step:1135/1370 train_time:166377ms step_avg:147.89ms
step:1136/1370 train_time:166537ms step_avg:147.90ms
step:1137/1370 train_time:166689ms step_avg:147.90ms
step:1138/1370 train_time:166845ms step_avg:147.91ms
step:1139/1370 train_time:167001ms step_avg:147.92ms
step:1140/1370 train_time:167155ms step_avg:147.92ms
step:1141/1370 train_time:167347ms step_avg:147.96ms
step:1142/1370 train_time:167503ms step_avg:147.97ms
step:1143/1370 train_time:167661ms step_avg:147.98ms
step:1144/1370 train_time:167814ms step_avg:147.98ms
step:1145/1370 train_time:167966ms step_avg:147.99ms
step:1146/1370 train_time:168123ms step_avg:148.00ms
step:1147/1370 train_time:168282ms step_avg:148.01ms
step:1148/1370 train_time:168439ms step_avg:148.01ms
step:1149/1370 train_time:168594ms step_avg:148.02ms
step:1150/1370 train_time:168748ms step_avg:148.02ms
step:1151/1370 train_time:168905ms step_avg:148.03ms
step:1152/1370 train_time:169062ms step_avg:148.04ms
step:1153/1370 train_time:169221ms step_avg:148.05ms
step:1154/1370 train_time:169378ms step_avg:148.06ms
step:1155/1370 train_time:169533ms step_avg:148.06ms
step:1156/1370 train_time:169692ms step_avg:148.07ms
step:1157/1370 train_time:169851ms step_avg:148.08ms
step:1158/1370 train_time:170006ms step_avg:148.09ms
step:1159/1370 train_time:170163ms step_avg:148.10ms
step:1160/1370 train_time:170316ms step_avg:148.10ms
step:1161/1370 train_time:170470ms step_avg:148.11ms
step:1162/1370 train_time:170626ms step_avg:148.11ms
step:1163/1370 train_time:170781ms step_avg:148.12ms
step:1164/1370 train_time:170937ms step_avg:148.13ms
step:1165/1370 train_time:171090ms step_avg:148.13ms
step:1166/1370 train_time:171247ms step_avg:148.14ms
step:1167/1370 train_time:171400ms step_avg:148.14ms
step:1168/1370 train_time:171555ms step_avg:148.15ms
step:1169/1370 train_time:171710ms step_avg:148.15ms
step:1170/1370 train_time:171867ms step_avg:148.16ms
step:1171/1370 train_time:172025ms step_avg:148.17ms
step:1172/1370 train_time:172182ms step_avg:148.18ms
step:1173/1370 train_time:172339ms step_avg:148.18ms
step:1174/1370 train_time:172500ms step_avg:148.20ms
step:1175/1370 train_time:172656ms step_avg:148.20ms
step:1176/1370 train_time:172814ms step_avg:148.21ms
step:1177/1370 train_time:172977ms step_avg:148.22ms
step:1178/1370 train_time:173132ms step_avg:148.23ms
step:1179/1370 train_time:173287ms step_avg:148.24ms
step:1180/1370 train_time:173453ms step_avg:148.25ms
step:1181/1370 train_time:173608ms step_avg:148.26ms
step:1182/1370 train_time:173762ms step_avg:148.26ms
step:1183/1370 train_time:173917ms step_avg:148.27ms
step:1184/1370 train_time:174074ms step_avg:148.27ms
step:1185/1370 train_time:174234ms step_avg:148.28ms
step:1186/1370 train_time:174390ms step_avg:148.29ms
step:1187/1370 train_time:174554ms step_avg:148.30ms
step:1188/1370 train_time:174708ms step_avg:148.31ms
step:1189/1370 train_time:174867ms step_avg:148.32ms
step:1190/1370 train_time:175024ms step_avg:148.33ms
step:1191/1370 train_time:175183ms step_avg:148.33ms
step:1192/1370 train_time:175337ms step_avg:148.34ms
step:1193/1370 train_time:175490ms step_avg:148.34ms
step:1194/1370 train_time:175646ms step_avg:148.35ms
step:1195/1370 train_time:175802ms step_avg:148.36ms
step:1196/1370 train_time:175959ms step_avg:148.36ms
step:1197/1370 train_time:176114ms step_avg:148.37ms
step:1198/1370 train_time:176275ms step_avg:148.38ms
step:1199/1370 train_time:176429ms step_avg:148.38ms
step:1200/1370 train_time:176585ms step_avg:148.39ms
step:1201/1370 train_time:176744ms step_avg:148.40ms
step:1202/1370 train_time:176912ms step_avg:148.42ms
step:1203/1370 train_time:177070ms step_avg:148.42ms
step:1204/1370 train_time:177226ms step_avg:148.43ms
step:1205/1370 train_time:177382ms step_avg:148.44ms
step:1206/1370 train_time:177540ms step_avg:148.45ms
step:1207/1370 train_time:177695ms step_avg:148.45ms
step:1208/1370 train_time:177850ms step_avg:148.46ms
step:1209/1370 train_time:178007ms step_avg:148.46ms
step:1210/1370 train_time:178168ms step_avg:148.47ms
step:1211/1370 train_time:178323ms step_avg:148.48ms
step:1212/1370 train_time:178479ms step_avg:148.49ms
step:1213/1370 train_time:178634ms step_avg:148.49ms
step:1214/1370 train_time:178791ms step_avg:148.50ms
step:1215/1370 train_time:178946ms step_avg:148.50ms
step:1216/1370 train_time:179101ms step_avg:148.51ms
step:1217/1370 train_time:179257ms step_avg:148.51ms
step:1218/1370 train_time:179408ms step_avg:148.52ms
step:1219/1370 train_time:179566ms step_avg:148.52ms
step:1220/1370 train_time:179720ms step_avg:148.53ms
step:1221/1370 train_time:179877ms step_avg:148.54ms
step:1222/1370 train_time:180032ms step_avg:148.54ms
step:1223/1370 train_time:180190ms step_avg:148.55ms
step:1224/1370 train_time:180348ms step_avg:148.56ms
step:1225/1370 train_time:180504ms step_avg:148.56ms
step:1226/1370 train_time:180660ms step_avg:148.57ms
step:1227/1370 train_time:180818ms step_avg:148.58ms
step:1228/1370 train_time:180972ms step_avg:148.58ms
step:1229/1370 train_time:181127ms step_avg:148.59ms
step:1230/1370 train_time:181289ms step_avg:148.60ms
step:1231/1370 train_time:181447ms step_avg:148.61ms
step:1232/1370 train_time:181609ms step_avg:148.62ms
step:1233/1370 train_time:181765ms step_avg:148.62ms
step:1234/1370 train_time:181919ms step_avg:148.63ms
step:1235/1370 train_time:182076ms step_avg:148.63ms
step:1236/1370 train_time:182231ms step_avg:148.64ms
step:1237/1370 train_time:182388ms step_avg:148.65ms
step:1238/1370 train_time:182553ms step_avg:148.66ms
step:1239/1370 train_time:182708ms step_avg:148.66ms
step:1240/1370 train_time:182868ms step_avg:148.67ms
step:1241/1370 train_time:183027ms step_avg:148.68ms
step:1242/1370 train_time:183183ms step_avg:148.69ms
step:1243/1370 train_time:183340ms step_avg:148.69ms
step:1244/1370 train_time:183495ms step_avg:148.70ms
step:1245/1370 train_time:183650ms step_avg:148.70ms
step:1246/1370 train_time:183806ms step_avg:148.71ms
step:1247/1370 train_time:183966ms step_avg:148.72ms
step:1248/1370 train_time:184120ms step_avg:148.72ms
step:1249/1370 train_time:184273ms step_avg:148.73ms
step:1250/1370 train_time:184429ms step_avg:148.73ms
step:1250/1370 val_loss:3.3049 train_time:184504ms step_avg:148.79ms
step:1251/1370 train_time:184592ms step_avg:148.74ms
step:1252/1370 train_time:184748ms step_avg:148.75ms
step:1253/1370 train_time:184904ms step_avg:148.76ms
step:1254/1370 train_time:185057ms step_avg:148.76ms
step:1255/1370 train_time:185224ms step_avg:148.77ms
step:1256/1370 train_time:185379ms step_avg:148.78ms
step:1257/1370 train_time:185537ms step_avg:148.79ms
step:1258/1370 train_time:185697ms step_avg:148.80ms
step:1259/1370 train_time:185853ms step_avg:148.80ms
step:1260/1370 train_time:186006ms step_avg:148.81ms
step:1261/1370 train_time:186163ms step_avg:148.81ms
step:1262/1370 train_time:186323ms step_avg:148.82ms
step:1263/1370 train_time:186480ms step_avg:148.83ms
step:1264/1370 train_time:186635ms step_avg:148.83ms
step:1265/1370 train_time:186793ms step_avg:148.84ms
step:1266/1370 train_time:186949ms step_avg:148.84ms
step:1267/1370 train_time:187108ms step_avg:148.85ms
step:1268/1370 train_time:187266ms step_avg:148.86ms
step:1269/1370 train_time:187426ms step_avg:148.87ms
step:1270/1370 train_time:187583ms step_avg:148.88ms
step:1271/1370 train_time:187745ms step_avg:148.89ms
step:1272/1370 train_time:187900ms step_avg:148.89ms
step:1273/1370 train_time:188056ms step_avg:148.90ms
step:1274/1370 train_time:188212ms step_avg:148.90ms
step:1275/1370 train_time:188368ms step_avg:148.91ms
step:1276/1370 train_time:188521ms step_avg:148.91ms
step:1277/1370 train_time:188679ms step_avg:148.92ms
step:1278/1370 train_time:188835ms step_avg:148.92ms
step:1279/1370 train_time:188994ms step_avg:148.93ms
step:1280/1370 train_time:189156ms step_avg:148.94ms
step:1281/1370 train_time:189314ms step_avg:148.95ms
step:1282/1370 train_time:189466ms step_avg:148.95ms
step:1283/1370 train_time:189622ms step_avg:148.96ms
step:1284/1370 train_time:189780ms step_avg:148.96ms
step:1285/1370 train_time:189936ms step_avg:148.97ms
step:1286/1370 train_time:190091ms step_avg:148.97ms
step:1287/1370 train_time:190248ms step_avg:148.98ms
step:1288/1370 train_time:190404ms step_avg:148.99ms
step:1289/1370 train_time:190566ms step_avg:149.00ms
step:1290/1370 train_time:190726ms step_avg:149.01ms
step:1291/1370 train_time:190887ms step_avg:149.01ms
step:1292/1370 train_time:191043ms step_avg:149.02ms
step:1293/1370 train_time:191198ms step_avg:149.02ms
step:1294/1370 train_time:191355ms step_avg:149.03ms
step:1295/1370 train_time:191512ms step_avg:149.04ms
step:1296/1370 train_time:191671ms step_avg:149.04ms
step:1297/1370 train_time:191829ms step_avg:149.05ms
step:1298/1370 train_time:191985ms step_avg:149.06ms
step:1299/1370 train_time:192140ms step_avg:149.06ms
step:1300/1370 train_time:192296ms step_avg:149.07ms
step:1301/1370 train_time:192451ms step_avg:149.07ms
step:1302/1370 train_time:192609ms step_avg:149.08ms
step:1303/1370 train_time:192769ms step_avg:149.09ms
step:1304/1370 train_time:192927ms step_avg:149.09ms
step:1305/1370 train_time:193083ms step_avg:149.10ms
step:1306/1370 train_time:193242ms step_avg:149.11ms
step:1307/1370 train_time:193396ms step_avg:149.11ms
step:1308/1370 train_time:193552ms step_avg:149.12ms
step:1309/1370 train_time:193711ms step_avg:149.12ms
step:1310/1370 train_time:193867ms step_avg:149.13ms
step:1311/1370 train_time:194021ms step_avg:149.13ms
step:1312/1370 train_time:194176ms step_avg:149.14ms
step:1313/1370 train_time:194331ms step_avg:149.14ms
step:1314/1370 train_time:194489ms step_avg:149.15ms
step:1315/1370 train_time:194647ms step_avg:149.15ms
step:1316/1370 train_time:194801ms step_avg:149.16ms
step:1317/1370 train_time:194956ms step_avg:149.16ms
step:1318/1370 train_time:195117ms step_avg:149.17ms
step:1319/1370 train_time:195275ms step_avg:149.18ms
step:1320/1370 train_time:195430ms step_avg:149.18ms
step:1321/1370 train_time:195588ms step_avg:149.19ms
step:1322/1370 train_time:195751ms step_avg:149.20ms
step:1323/1370 train_time:195907ms step_avg:149.21ms
step:1324/1370 train_time:196063ms step_avg:149.21ms
step:1325/1370 train_time:196219ms step_avg:149.22ms
step:1326/1370 train_time:196381ms step_avg:149.23ms
step:1327/1370 train_time:196536ms step_avg:149.23ms
step:1328/1370 train_time:196693ms step_avg:149.24ms
step:1329/1370 train_time:196866ms step_avg:149.25ms
step:1330/1370 train_time:197026ms step_avg:149.26ms
step:1331/1370 train_time:197219ms step_avg:149.29ms
step:1332/1370 train_time:197381ms step_avg:149.31ms
step:1333/1370 train_time:197538ms step_avg:149.31ms
step:1334/1370 train_time:197694ms step_avg:149.32ms
step:1335/1370 train_time:197848ms step_avg:149.32ms
step:1336/1370 train_time:198013ms step_avg:149.33ms
step:1337/1370 train_time:198172ms step_avg:149.34ms
step:1338/1370 train_time:198328ms step_avg:149.34ms
step:1339/1370 train_time:198488ms step_avg:149.35ms
step:1340/1370 train_time:198645ms step_avg:149.36ms
step:1341/1370 train_time:198799ms step_avg:149.36ms
step:1342/1370 train_time:198958ms step_avg:149.37ms
step:1343/1370 train_time:199115ms step_avg:149.37ms
step:1344/1370 train_time:199271ms step_avg:149.38ms
step:1345/1370 train_time:199427ms step_avg:149.38ms
step:1346/1370 train_time:199582ms step_avg:149.39ms
step:1347/1370 train_time:199738ms step_avg:149.39ms
step:1348/1370 train_time:199895ms step_avg:149.40ms
step:1349/1370 train_time:200050ms step_avg:149.40ms
step:1350/1370 train_time:200204ms step_avg:149.41ms
step:1351/1370 train_time:200360ms step_avg:149.41ms
step:1352/1370 train_time:200524ms step_avg:149.42ms
step:1353/1370 train_time:200688ms step_avg:149.43ms
step:1354/1370 train_time:200845ms step_avg:149.44ms
step:1355/1370 train_time:201002ms step_avg:149.44ms
step:1356/1370 train_time:201158ms step_avg:149.45ms
step:1357/1370 train_time:201317ms step_avg:149.46ms
step:1358/1370 train_time:201477ms step_avg:149.46ms
step:1359/1370 train_time:201636ms step_avg:149.47ms
step:1360/1370 train_time:201795ms step_avg:149.48ms
step:1361/1370 train_time:201955ms step_avg:149.49ms
step:1362/1370 train_time:202113ms step_avg:149.49ms
step:1363/1370 train_time:202276ms step_avg:149.50ms
step:1364/1370 train_time:202432ms step_avg:149.51ms
step:1365/1370 train_time:202586ms step_avg:149.51ms
step:1366/1370 train_time:202743ms step_avg:149.52ms
step:1367/1370 train_time:202902ms step_avg:149.52ms
step:1368/1370 train_time:203060ms step_avg:149.53ms
step:1369/1370 train_time:203226ms step_avg:149.54ms
step:1370/1370 train_time:203385ms step_avg:149.55ms
step:1370/1370 val_loss:3.2807 train_time:203457ms step_avg:149.60ms
peak memory consumption: 32619 MiB

import os
import sys
with open(sys.argv[0]) as f:
    code = f.read() # read the code of this file ASAP, for logging
import uuid
import time
import glob
import subprocess
import contextlib
from dataclasses import dataclass

import torch
torch.empty(1, device='cuda', requires_grad=True).backward()
from torch import nn
import torch.nn.functional as F
import torch.distributed as dist
from torch.nn.parallel import DistributedDataParallel as DDP
# use of FlexAttention contributed by @KoszarskyB
from torch.nn.attention.flex_attention import BlockMask, flex_attention

# -----------------------------------------------------------------------------
# Muon optimizer

@torch.compile
def zeropower_via_newtonschulz5(G, steps):
    """
    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a
    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose
    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at
    zero even beyond the point where the iteration no longer converges all the way to one everywhere
    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T
    where S' is diagonal with S_{ii}' ~ Uniform(0.5, 1.5), which turns out not to hurt model
    performance at all relative to UV^T, where USV^T = G is the SVD.
    """
    assert len(G.shape) == 2
    a, b, c = (3.4445, -4.7750,  2.0315)
    X = G.bfloat16()
    if G.size(0) > G.size(1):
        X = X.T

    # Ensure spectral norm is at most 1
    X = X / (X.norm() + 1e-7)
    # Perform the NS iterations
    for _ in range(steps):
        A = X @ X.T
        B = b * A + c * A @ A # adapted from suggestion by @jxbz, @leloykun, and @YouJiacheng
        X = a * X + B @ X
    
    if G.size(0) > G.size(1):
        X = X.T
    return X

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Some warnings:
    - This optimizer assumes that all parameters passed in are 2D.
    - It should not be used for the embedding layer, the final fully connected layer, or any {0,1}-D
    parameters; those should all be optimized by a standard method (e.g., AdamW).
    - To use it with 4D convolutional filters, it works well to just flatten their last 3 dimensions.
    - We believe it is unlikely to work well for training with small batch size.
    - We believe it may not work well for finetuning pretrained models, but we haven't tested this.
    - We have not yet tried this optimizer for training scenarios larger than NanoGPT (124M).

    Arguments:
        lr: The learning rate used by the internal SGD.
        momentum: The momentum used by the internal SGD.
        nesterov: Whether to use Nesterov-style momentum in the internal SGD. (recommended)
        ns_steps: The number of Newton-Schulz iteration steps to use.
    """
    def __init__(self, params, lr=0.02, momentum=0.95, nesterov=True, ns_steps=5):
        self.world_size = int(os.environ['WORLD_SIZE'])
        self.rank = int(os.environ['RANK'])
        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, ns_steps=ns_steps)
        assert all(isinstance(p, torch.Tensor) for p in params)
        sizes = {p.numel() for p in params}
        param_groups = [dict(params=[p for p in params if p.numel() == size],
                             update_buffer=[torch.empty(size, device='cuda', dtype=torch.bfloat16) for _ in range(self.world_size)])
                        for size in sizes]
        super().__init__(param_groups, defaults)

    def step(self):

        for group in self.param_groups:

            lr = group['lr']
            momentum = group['momentum']
            nesterov = group['nesterov']
            ns_steps = group['ns_steps']
            update_buffers = group['update_buffer']
            # generate weight updates in distributed fashion
            params = group['params']
            handle = None
            params_world = None
            def update_prev():
                if params_world is None:
                    return
                assert handle is not None
                handle.wait()
                for p_world, g_world in zip(params_world, update_buffers):
                    p_world.data.add_(
                        g_world.view_as(p_world),
                        alpha=-lr * max(1, p_world.size(0) / p_world.size(1)) ** 0.5,
                    )
            for base_i in range(len(params))[::self.world_size]:
                if base_i + rank < len(params):
                    p = params[base_i + self.rank]
                    g = p.grad
                    assert g is not None
                    state = self.state[p]
                    if 'momentum_buffer' not in state:
                        state['momentum_buffer'] = torch.zeros_like(g)
                    buf = state['momentum_buffer']
                    buf.lerp_(g, 1 - momentum)
                    g = g.lerp_(buf, momentum) if nesterov else buf
                    g = zeropower_via_newtonschulz5(g, steps=ns_steps).flatten()
                else:
                    g = update_buffers[rank]
                update_prev() # async all_gather instead of sync all_reduce by @YouJiacheng
                handle = dist.all_gather(update_buffers, g, async_op=True)
                params_world = params[base_i : base_i + self.world_size]
            update_prev()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the GPT-2 model

def norm(x):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):

    def __init__(self, in_features, out_features):
        super().__init__(in_features, out_features, bias=False)

    def forward(self, x):
        return F.linear(x, self.weight.type_as(x))

class Rotary(nn.Module):

    def __init__(self, dim, max_seq_len=65536):
        super().__init__()
        # half-truncate RoPE by @YouJiacheng
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=dim//4, dtype=torch.float32)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(dim//4)])
        t = torch.arange(max_seq_len, dtype=torch.float32)
        theta = torch.einsum('i,j -> ij', t, angular_freq)
        self.cos = nn.Buffer(theta.cos(), persistent=False)
        self.sin = nn.Buffer(theta.sin(), persistent=False)

    def forward(self, x):
        cos, sin = self.cos[None, :x.size(-3), None, :], self.sin[None, :x.size(-3), None, :]
        x1, x2 = x.float().chunk(2, dim=-1)
        y1 = x1 * cos + x2 * sin
        y2 = x1 * (-sin) + x2 * cos
        return torch.cat((y1, y2), 3).type_as(x)

class CausalSelfAttention(nn.Module):

    def __init__(self, dim, num_heads):
        super().__init__()
        assert dim % num_heads == 0
        self.num_heads = num_heads
        self.c_q = CastedLinear(dim, dim)
        self.c_k = CastedLinear(dim, dim)
        self.c_v = CastedLinear(dim, dim)
        self.lambdas = nn.Parameter(torch.tensor([0.5, 0.5]))
        self.rotary = Rotary(dim // num_heads) # dim // num_heads = head_dim
        self.c_proj = CastedLinear(dim, dim)
        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977
        self.attn_scale = nn.Parameter(torch.tensor(1.0 / (dim // num_heads) ** 0.5))

    def forward(self, x, ve, block_mask):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, 'Must use batch size = 1 for FlexAttention'
        q = self.c_q(x).view(B, T, self.num_heads, -1)
        k = self.c_k(x).view(B, T, self.num_heads, -1)
        v = self.c_v(x).view(B, T, self.num_heads, -1)
        if ve is not None:
            v = self.lambdas[0] * v + self.lambdas[1] * ve.view_as(v) # @KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = self.lambdas[0] * v
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = self.rotary(q), self.rotary(k)
        y = flex_attention(q.transpose(1, 2) * self.attn_scale, k.transpose(1, 2), v.transpose(1, 2), block_mask=block_mask, scale=1.)
        y = y.transpose(1, 2).contiguous().view_as(x) # re-assemble all head outputs side by side
        y = self.c_proj(y)
        return y

class MLP(nn.Module):

    def __init__(self, dim):
        super().__init__()
        self.c_fc = CastedLinear(dim, 4 * dim)
        self.c_proj = CastedLinear(4 * dim, dim)
        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977

    def forward(self, x):
        x = self.c_fc(x)
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = self.c_proj(x)
        return x

class Block(nn.Module):

    def __init__(self, model_dim, num_heads, use_attn=True):
        super().__init__()
        self.attn = CausalSelfAttention(model_dim, num_heads) if use_attn else None
        self.mlp = MLP(model_dim)
        self.lambdas = nn.Parameter(torch.tensor([1., 0.]))

    def forward(self, x, ve, x0, block_mask):
        x = self.lambdas[0] * x + self.lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), ve, block_mask)
        x = x + self.mlp(norm(x))
        return x

class ValueEmbedding(nn.Module):
    def __init__(self, vocab_size, model_dim):
        super().__init__()
        self.embed = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])

    def forward(self, inputs):
        ve = [emb(inputs).bfloat16() for emb in self.embed]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        ve = [ve[0], ve[1], ve[2], None, None, None, None, None, None, ve[0], ve[1], ve[2]]
        return ve

# -----------------------------------------------------------------------------
# The main GPT-2 model

class GPT(nn.Module):

    def __init__(self, vocab_size, num_layers, num_heads, model_dim):
        super().__init__()
        self.embed = nn.Embedding(vocab_size, model_dim)
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.blocks = nn.ModuleList([Block(model_dim, num_heads, use_attn=(i != 7))
                                     for i in range(num_layers)])
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual learning
        # U-net structure on token value embeddings by @leloykun
        self.value_embeds = ValueEmbedding(vocab_size, model_dim)
        self.lm_head = CastedLinear(model_dim, vocab_size)
        self.lm_head.weight.data.zero_() # @Grad62304977
        # U-net design by @brendanh0gan
        self.num_encoder_layers = num_layers // 2 # Half of the layers for encoder
        self.num_decoder_layers = num_layers - self.num_encoder_layers # Remaining for decoder
        # Add learnable skip connection weights for decoder layers
        self.skip_weights = nn.Parameter(torch.ones(self.num_decoder_layers))

    def forward(self, inputs, targets, sliding_window_num_blocks):
        BLOCK_SIZE = 128
        seq_len = len(inputs)
        assert seq_len % BLOCK_SIZE == 0
        total_num_blocks = seq_len // BLOCK_SIZE
        assert inputs.ndim == 1
        docs = (inputs == 50256).cumsum(0)
        docs_low = docs.view(-1, BLOCK_SIZE)[:, 0].contiguous()
        docs_high = docs.view(-1, BLOCK_SIZE)[:, -1].contiguous()

        def document_causal(b, h, q_idx, kv_idx):
            causal_mask = q_idx >= kv_idx
            document_mask = docs[q_idx] == docs[kv_idx]
            return causal_mask & document_mask

        def dense_to_ordered(dense_mask):
            num_blocks = dense_mask.sum(dim=-1, dtype=torch.int32)
            indices = dense_mask.argsort(dim=-1, descending=True, stable=True).to(torch.int32)
            return num_blocks[None, None].contiguous(), indices[None, None].contiguous()

        def create_doc_swc_block_mask(sliding_window_num_blocks):
            kv_idx = block_idx = torch.arange(total_num_blocks, dtype=torch.int32, device='cuda')
            q_idx = block_idx[:, None]
            causal_bm = q_idx >= kv_idx
            causal_full_bm = q_idx > kv_idx
            window_bm = q_idx - kv_idx < sliding_window_num_blocks
            window_full_bm = window_bm # block-wise sliding window by @YouJiacheng
            # document_bm = (docs_low[q_idx] <= docs_high[kv_idx]) & (docs_low[kv_idx] <= docs_high[q_idx])
            document_bm = (docs_low[:, None] <= docs_high) & (docs_low <= docs_high[:, None])
            document_full_bm = (docs_low[:, None] == docs_high) & (docs_low == docs_high[:, None])
            nonzero_bm = causal_bm & window_bm & document_bm
            full_bm  = causal_full_bm & window_full_bm & document_full_bm
            kv_num_blocks, kv_indices = dense_to_ordered(nonzero_bm & ~full_bm)
            full_kv_num_blocks, full_kv_indices = dense_to_ordered(full_bm)
            return BlockMask.from_kv_blocks(
                kv_num_blocks,
                kv_indices,
                full_kv_num_blocks,
                full_kv_indices,
                BLOCK_SIZE=BLOCK_SIZE,
                mask_mod=document_causal,
            )

        block_mask = create_doc_swc_block_mask(sliding_window_num_blocks)

        x0 = norm(self.embed(inputs[None]).bfloat16()) # use of norm here by @Grad62304977
        x = x0
        ve = self.value_embeds(inputs)
        assert len(ve) == len(self.blocks)
        ve_enc, ve_dec = ve[:self.num_encoder_layers], ve[self.num_encoder_layers:]

        # Store outputs for U-Net skip connections
        skip_connections = []
        # Encoder pass - process only the first half of the blocks
        for i in range(self.num_encoder_layers):
            x = self.blocks[i](x, ve_enc[i], x0, block_mask)
            skip_connections.append(x)
        # Decoder pass - process the remaining blocks with weighted skip connections
        for i in range(self.num_decoder_layers):
            x = x + self.skip_weights[i] * skip_connections.pop()
            # U-net structure on token value embeddings by @leloykun
            x = self.blocks[self.num_encoder_layers + i](x, ve_dec[i], x0, block_mask)

        x = norm(x)
        logits = self.lm_head(x)
        logits = 15 * torch.tanh(logits / 15) # @Grad62304977 added tanh softcapping, @KoszarskyB reduced it from 30 to 15
        logits = logits.float()
        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets)
        return loss

# -----------------------------------------------------------------------------
# Our own simple Distributed Data Loader

def _load_data_shard(path):
    # only reads the header, returns header data
    # header is 256 int32
    header = torch.from_file(path, False, 256, dtype=torch.int32)
    assert header[0] == 20240520, 'magic number mismatch in the data .bin file'
    assert header[1] == 1, 'unsupported version'
    num_tokens = int(header[2]) # number of tokens (claimed)
    with open(path, 'rb', buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, 'number of tokens read does not match header'
    return tokens

class DistributedDataLoader:

    def __init__(self, filename_pattern):
        self.rank = int(os.environ['RANK'])
        self.world_size = int(os.environ['WORLD_SIZE'])
        self.files = sorted(glob.glob(filename_pattern))
        self.reset()

    def reset(self):
        self.current_shard = -1
        self.advance()

    def advance(self):
        self.current_shard = (self.current_shard + 1) % len(self.files)
        self.current_position = 0
        self.tokens = _load_data_shard(self.files[self.current_shard])

    def next_batch(self, batch_size):
        assert batch_size % self.world_size == 0
        device_batch_size = batch_size // self.world_size
        # load next shard if necessary
        if self.current_position + batch_size + 1 >= len(self.tokens):
            self.advance()
        pos = self.current_position + self.rank * device_batch_size
        device_batch_tokens = self.tokens[pos:pos+device_batch_size+1]
        # advance current position
        self.current_position += batch_size
        inputs = device_batch_tokens[:-1].to(device='cuda', dtype=torch.int32, non_blocking=True)
        targets = device_batch_tokens[1:].to(device='cuda', dtype=torch.int64, non_blocking=True)
        return inputs, targets

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files = 'data/fineweb10B/fineweb_train_*.bin' # input .bin to train on
    val_files = 'data/fineweb10B/fineweb_val_*.bin' # input .bin to eval validation loss on
    val_tokens = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # optimization
    batch_size = 8*64*1024 # batch size in tokens
    num_iterations = 1370 # number of iterations to run
    cooldown_frac = 0.4 # fraction of training spent cooling down the learning rate
    bf16_embeds = True
    # evaluation and logging
    val_loss_every = 125 # every how many steps to evaluate val loss? 0 for only at the end
    # implementation
    max_device_batch_size = 64*1024 # batch size per device in tokens
    save_checkpoint = False
args = Hyperparameters()

micro_bs = args.max_device_batch_size

# set up DDP (distributed data parallel). torchrun sets this env variable
rank = int(os.environ['RANK'])
local_rank = int(os.environ['LOCAL_RANK'])
world_size = int(os.environ['WORLD_SIZE'])
assert torch.cuda.is_available()
torch.cuda.set_device(local_rank)
dist.init_process_group(backend='nccl', device_id=torch.device(local_rank))
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = uuid.uuid4()
    os.makedirs('logs', exist_ok=True)
    logfile = f'logs/{run_id}.txt'
    print(logfile)

def print0(s, console=False):
    if master_process:
        with open(logfile, 'a') as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0('='*100)
# log information about the hardware/software environment this is running on
print0(f'Running Python {sys.version}')
print0(f'Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}')
print0(subprocess.run(['nvidia-smi'], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout)
print0('='*100)

# load data
train_loader = DistributedDataLoader(args.train_files)
val_loader = DistributedDataLoader(args.val_files)
print0(f'Training dataloader files: {train_loader.files}')
print0(f'Validation dataloader files: {val_loader.files}')
print0('='*100)

# there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency. suggested to me by @Grad62304977.
# this originates from Karpathy's experiments.
model = GPT(vocab_size=50304, num_layers=12, num_heads=6, model_dim=768)
model = model.cuda()
if args.bf16_embeds:
    for m in model.modules():
        if isinstance(m, nn.Embedding):
            m.bfloat16()
model: GPT = torch.compile(model)
ddp_model = DDP(model, device_ids=[local_rank], broadcast_buffers=False, gradient_as_bucket_view=True)

# collect the parameters to optimize
hidden_matrix_params = [p for p in model.blocks.parameters() if p.ndim == 2]
embed_params = [model.embed.weight, *model.value_embeds.parameters()]
scalar_params = [p for n, p in model.named_parameters() if p.ndim < 2 and "attn_scale" not in n]
attn_scale_params = [p for n, p in model.named_parameters() if p.ndim < 2 and "attn_scale" in n]
head_params = [model.lm_head.weight]

# init the optimizer(s)
optimizer1 = torch.optim.Adam([dict(params=embed_params, lr=0.6),
                               dict(params=head_params, lr=0.008),
                               dict(params=scalar_params, lr=0.04),
                               dict(params=attn_scale_params, lr=0.01)],
                              betas=(0.8, 0.95), fused=True)
optimizer2 = Muon(hidden_matrix_params, lr=0.05, momentum=0.95)
optimizers = [optimizer1, optimizer2]

# learning rate schedule: stable then decay
def get_lr(it):
    t = 1 - it / args.num_iterations # time remaining in training
    assert 1 >= t > 0
    # 1) constant lr for first part of training
    if t >= args.cooldown_frac:
        return 1.0
    # 2) then linear cooldown
    else:
        return t / args.cooldown_frac
schedulers = [torch.optim.lr_scheduler.LambdaLR(opt, get_lr) for opt in optimizers]

# sliding window size schedule: linear increase over training in chunks of 128 from 128 -> 1792. By @fernbear.bsky.social
def get_sliding_window_blocks(it):
    x = it / args.num_iterations # training progress
    assert 0 <= x <= 1
    return int(((1 - x) * 128 + x * 1856) // 128)
sliding_window_num_blocks = torch.tensor(1, dtype=torch.int32, device='cuda')

# Start training loop
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    # This effectively ignores timing first 10 steps, which are slower for weird reasons.
    # Alternately, and slightly more correctly in terms of benchmarking, we could do 10
    # steps with dummy data first, and then re-initialize the model and reset the loader.
    if step == 10:
        training_time_ms = 0
        t0 = time.perf_counter()
    timed_steps = float('nan') if step <= 11 else (step - 10) + 1 # <= 11 to avoid bug in val

    sliding_window_num_blocks.copy_(get_sliding_window_blocks(step))

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        # run validation batches
        model.eval()
        val_loader.reset()
        val_loss = 0.0
        # calculate the number of steps to take in the val loop.
        val_batch_size = world_size * micro_bs
        assert args.val_tokens % val_batch_size == 0
        val_steps = args.val_tokens // val_batch_size
        for _ in range(val_steps):
            with torch.no_grad():
                inputs_val, targets_val = val_loader.next_batch(val_batch_size)
                val_loss += ddp_model(inputs_val, targets_val, sliding_window_num_blocks)
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        val_loss /= val_steps
        # logging
        print0(f'step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/(timed_steps-1):.2f}ms', console=True)
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f'logs/{run_id}', exist_ok=True)
            torch.save(log, f'logs/{run_id}/state_step{step:06d}.pt')
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    model.train()
    batch_size = args.batch_size
    assert batch_size % world_size == 0
    inputs_train, targets_train = train_loader.next_batch(batch_size)
    assert len(inputs_train) <= micro_bs or len(inputs_train) % micro_bs == 0
    for micro_inputs_train, micro_targets_train in zip(inputs_train.split(micro_bs), targets_train.split(micro_bs)):
        ddp_model(micro_inputs_train, micro_targets_train, sliding_window_num_blocks).backward()
    # momentum warmup for Muon
    frac = min(step/300, 1)
    for group in optimizer2.param_groups:
        group['momentum'] = (1 - frac) * 0.85 + frac * 0.95
    # step the optimizers and schedulers
    for opt, sched in zip(optimizers, schedulers):
        opt.step()
        if step != train_steps-1:
            sched.step()
    # null the gradients
    model.zero_grad(set_to_none=True)
    # logging
    approx_time = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f'step:{step+1}/{train_steps} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms', console=True)

print0(f'peak memory consumption: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB')
dist.destroy_process_group()

====================================================================================================
Running Python 3.10.12 (main, Sep 11 2024, 15:47:36) [GCC 11.4.0]
Running PyTorch 2.6.0.dev20241231+cu126 compiled for CUDA 12.6
Sat Jan 11 22:59:09 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.183.06             Driver Version: 535.183.06   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:19:00.0 Off |                    0 |
| N/A   38C    P0             126W / 700W |   7713MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:3B:00.0 Off |                    0 |
| N/A   32C    P0             114W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:4C:00.0 Off |                    0 |
| N/A   29C    P0             117W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   37C    P0             129W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:9B:00.0 Off |                    0 |
| N/A   38C    P0             123W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:BB:00.0 Off |                    0 |
| N/A   30C    P0             114W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:CB:00.0 Off |                    0 |
| N/A   37C    P0             116W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:DB:00.0 Off |                    0 |
| N/A   30C    P0             117W / 700W |   3211MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
Training dataloader files: ['data/fineweb10B/fineweb_train_000001.bin', 'data/fineweb10B/fineweb_train_000002.bin', 'data/fineweb10B/fineweb_train_000003.bin', 'data/fineweb10B/fineweb_train_000004.bin', 'data/fineweb10B/fineweb_train_000005.bin', 'data/fineweb10B/fineweb_train_000006.bin', 'data/fineweb10B/fineweb_train_000007.bin', 'data/fineweb10B/fineweb_train_000008.bin', 'data/fineweb10B/fineweb_train_000009.bin']
Validation dataloader files: ['data/fineweb10B/fineweb_val_000000.bin']
====================================================================================================
step:0/1370 val_loss:10.8258 train_time:0ms step_avg:nanms
step:1/1370 train_time:29673ms step_avg:nanms
step:2/1370 train_time:29772ms step_avg:nanms
step:3/1370 train_time:29956ms step_avg:nanms
step:4/1370 train_time:30091ms step_avg:nanms
step:5/1370 train_time:30226ms step_avg:nanms
step:6/1370 train_time:30360ms step_avg:nanms
step:7/1370 train_time:30493ms step_avg:nanms
step:8/1370 train_time:30628ms step_avg:nanms
step:9/1370 train_time:30763ms step_avg:nanms
step:10/1370 train_time:30903ms step_avg:nanms
step:11/1370 train_time:138ms step_avg:nanms
step:12/1370 train_time:274ms step_avg:nanms
step:13/1370 train_time:410ms step_avg:136.63ms
step:14/1370 train_time:546ms step_avg:136.49ms
step:15/1370 train_time:680ms step_avg:136.09ms
step:16/1370 train_time:815ms step_avg:135.78ms
step:17/1370 train_time:955ms step_avg:136.39ms
step:18/1370 train_time:1092ms step_avg:136.44ms
step:19/1370 train_time:1230ms step_avg:136.64ms
step:20/1370 train_time:1366ms step_avg:136.65ms
step:21/1370 train_time:1501ms step_avg:136.50ms
step:22/1370 train_time:1637ms step_avg:136.42ms
step:23/1370 train_time:1772ms step_avg:136.33ms
step:24/1370 train_time:1909ms step_avg:136.33ms
step:25/1370 train_time:2047ms step_avg:136.47ms
step:26/1370 train_time:2182ms step_avg:136.40ms
step:27/1370 train_time:2319ms step_avg:136.41ms
step:28/1370 train_time:2457ms step_avg:136.48ms
step:29/1370 train_time:2591ms step_avg:136.37ms
step:30/1370 train_time:2728ms step_avg:136.40ms
step:31/1370 train_time:2863ms step_avg:136.32ms
step:32/1370 train_time:2999ms step_avg:136.33ms
step:33/1370 train_time:3136ms step_avg:136.35ms
step:34/1370 train_time:3274ms step_avg:136.40ms
step:35/1370 train_time:3410ms step_avg:136.40ms
step:36/1370 train_time:3548ms step_avg:136.45ms
step:37/1370 train_time:3682ms step_avg:136.38ms
step:38/1370 train_time:3819ms step_avg:136.38ms
step:39/1370 train_time:3957ms step_avg:136.43ms
step:40/1370 train_time:4093ms step_avg:136.43ms
step:41/1370 train_time:4230ms step_avg:136.46ms
step:42/1370 train_time:4368ms step_avg:136.49ms
step:43/1370 train_time:4503ms step_avg:136.46ms
step:44/1370 train_time:4639ms step_avg:136.45ms
step:45/1370 train_time:4774ms step_avg:136.41ms
step:46/1370 train_time:4911ms step_avg:136.42ms
step:47/1370 train_time:5049ms step_avg:136.45ms
step:48/1370 train_time:5184ms step_avg:136.42ms
step:49/1370 train_time:5320ms step_avg:136.42ms
step:50/1370 train_time:5458ms step_avg:136.45ms
step:51/1370 train_time:5594ms step_avg:136.45ms
step:52/1370 train_time:5732ms step_avg:136.47ms
step:53/1370 train_time:5871ms step_avg:136.53ms
step:54/1370 train_time:6007ms step_avg:136.52ms
step:55/1370 train_time:6144ms step_avg:136.53ms
step:56/1370 train_time:6278ms step_avg:136.48ms
step:57/1370 train_time:6415ms step_avg:136.48ms
step:58/1370 train_time:6553ms step_avg:136.51ms
step:59/1370 train_time:6689ms step_avg:136.51ms
step:60/1370 train_time:6825ms step_avg:136.50ms
step:61/1370 train_time:6962ms step_avg:136.51ms
step:62/1370 train_time:7099ms step_avg:136.51ms
step:63/1370 train_time:7235ms step_avg:136.51ms
step:64/1370 train_time:7371ms step_avg:136.51ms
step:65/1370 train_time:7508ms step_avg:136.52ms
step:66/1370 train_time:7646ms step_avg:136.53ms
step:67/1370 train_time:7782ms step_avg:136.52ms
step:68/1370 train_time:7918ms step_avg:136.51ms
step:69/1370 train_time:8056ms step_avg:136.55ms
step:70/1370 train_time:8193ms step_avg:136.55ms
step:71/1370 train_time:8330ms step_avg:136.56ms
step:72/1370 train_time:8467ms step_avg:136.57ms
step:73/1370 train_time:8604ms step_avg:136.56ms
step:74/1370 train_time:8740ms step_avg:136.56ms
step:75/1370 train_time:8876ms step_avg:136.56ms
step:76/1370 train_time:9014ms step_avg:136.58ms
step:77/1370 train_time:9152ms step_avg:136.60ms
step:78/1370 train_time:9290ms step_avg:136.62ms
step:79/1370 train_time:9426ms step_avg:136.61ms
step:80/1370 train_time:9562ms step_avg:136.60ms
step:81/1370 train_time:9699ms step_avg:136.60ms
step:82/1370 train_time:9835ms step_avg:136.59ms
step:83/1370 train_time:9973ms step_avg:136.61ms
step:84/1370 train_time:10109ms step_avg:136.61ms
step:85/1370 train_time:10246ms step_avg:136.62ms
step:86/1370 train_time:10381ms step_avg:136.59ms
step:87/1370 train_time:10518ms step_avg:136.59ms
step:88/1370 train_time:10655ms step_avg:136.60ms
step:89/1370 train_time:10791ms step_avg:136.60ms
step:90/1370 train_time:10928ms step_avg:136.60ms
step:91/1370 train_time:11066ms step_avg:136.61ms
step:92/1370 train_time:11201ms step_avg:136.59ms
step:93/1370 train_time:11337ms step_avg:136.59ms
step:94/1370 train_time:11474ms step_avg:136.60ms
step:95/1370 train_time:11611ms step_avg:136.60ms
step:96/1370 train_time:11748ms step_avg:136.61ms
step:97/1370 train_time:11883ms step_avg:136.58ms
step:98/1370 train_time:12019ms step_avg:136.58ms
step:99/1370 train_time:12156ms step_avg:136.59ms
step:100/1370 train_time:12293ms step_avg:136.59ms
step:101/1370 train_time:12430ms step_avg:136.60ms
step:102/1370 train_time:12566ms step_avg:136.59ms
step:103/1370 train_time:12703ms step_avg:136.59ms
step:104/1370 train_time:12843ms step_avg:136.62ms
step:105/1370 train_time:12981ms step_avg:136.64ms
step:106/1370 train_time:13121ms step_avg:136.68ms
step:107/1370 train_time:13261ms step_avg:136.72ms
step:108/1370 train_time:13402ms step_avg:136.76ms
step:109/1370 train_time:13542ms step_avg:136.79ms
step:110/1370 train_time:13681ms step_avg:136.81ms
step:111/1370 train_time:13821ms step_avg:136.84ms
step:112/1370 train_time:13960ms step_avg:136.86ms
step:113/1370 train_time:14099ms step_avg:136.88ms
step:114/1370 train_time:14239ms step_avg:136.91ms
step:115/1370 train_time:14381ms step_avg:136.96ms
step:116/1370 train_time:14521ms step_avg:136.99ms
step:117/1370 train_time:14660ms step_avg:137.01ms
step:118/1370 train_time:14800ms step_avg:137.04ms
step:119/1370 train_time:14941ms step_avg:137.07ms
step:120/1370 train_time:15081ms step_avg:137.10ms
step:121/1370 train_time:15222ms step_avg:137.13ms
step:122/1370 train_time:15362ms step_avg:137.16ms
step:123/1370 train_time:15502ms step_avg:137.18ms
step:124/1370 train_time:15642ms step_avg:137.21ms
step:125/1370 train_time:15779ms step_avg:137.21ms
step:125/1370 val_loss:4.3897 train_time:15843ms step_avg:137.77ms
step:126/1370 train_time:15923ms step_avg:137.27ms
step:127/1370 train_time:16066ms step_avg:137.31ms
step:128/1370 train_time:16206ms step_avg:137.34ms
step:129/1370 train_time:16344ms step_avg:137.34ms
step:130/1370 train_time:16483ms step_avg:137.36ms
step:131/1370 train_time:16621ms step_avg:137.36ms
step:132/1370 train_time:16761ms step_avg:137.38ms
step:133/1370 train_time:16903ms step_avg:137.42ms
step:134/1370 train_time:17043ms step_avg:137.44ms
step:135/1370 train_time:17183ms step_avg:137.47ms
step:136/1370 train_time:17323ms step_avg:137.48ms
step:137/1370 train_time:17462ms step_avg:137.49ms
step:138/1370 train_time:17602ms step_avg:137.51ms
step:139/1370 train_time:17740ms step_avg:137.52ms
step:140/1370 train_time:17881ms step_avg:137.55ms
step:141/1370 train_time:18021ms step_avg:137.57ms
step:142/1370 train_time:18161ms step_avg:137.59ms
step:143/1370 train_time:18301ms step_avg:137.60ms
step:144/1370 train_time:18441ms step_avg:137.62ms
step:145/1370 train_time:18582ms step_avg:137.64ms
step:146/1370 train_time:18722ms step_avg:137.66ms
step:147/1370 train_time:18863ms step_avg:137.69ms
step:148/1370 train_time:19003ms step_avg:137.70ms
step:149/1370 train_time:19142ms step_avg:137.71ms
step:150/1370 train_time:19283ms step_avg:137.73ms
step:151/1370 train_time:19422ms step_avg:137.75ms
step:152/1370 train_time:19562ms step_avg:137.76ms
step:153/1370 train_time:19702ms step_avg:137.77ms
step:154/1370 train_time:19842ms step_avg:137.79ms
step:155/1370 train_time:19983ms step_avg:137.81ms
step:156/1370 train_time:20123ms step_avg:137.83ms
step:157/1370 train_time:20263ms step_avg:137.84ms
step:158/1370 train_time:20403ms step_avg:137.86ms
step:159/1370 train_time:20543ms step_avg:137.87ms
step:160/1370 train_time:20683ms step_avg:137.89ms
step:161/1370 train_time:20823ms step_avg:137.90ms
step:162/1370 train_time:20963ms step_avg:137.91ms
step:163/1370 train_time:21103ms step_avg:137.93ms
step:164/1370 train_time:21243ms step_avg:137.94ms
step:165/1370 train_time:21384ms step_avg:137.96ms
step:166/1370 train_time:21523ms step_avg:137.97ms
step:167/1370 train_time:21663ms step_avg:137.98ms
step:168/1370 train_time:21803ms step_avg:137.99ms
step:169/1370 train_time:21943ms step_avg:138.01ms
step:170/1370 train_time:22084ms step_avg:138.02ms
step:171/1370 train_time:22224ms step_avg:138.04ms
step:172/1370 train_time:22364ms step_avg:138.05ms
step:173/1370 train_time:22505ms step_avg:138.07ms
step:174/1370 train_time:22645ms step_avg:138.08ms
step:175/1370 train_time:22787ms step_avg:138.10ms
step:176/1370 train_time:22928ms step_avg:138.12ms
step:177/1370 train_time:23069ms step_avg:138.14ms
step:178/1370 train_time:23207ms step_avg:138.14ms
step:179/1370 train_time:23347ms step_avg:138.15ms
step:180/1370 train_time:23488ms step_avg:138.17ms
step:181/1370 train_time:23628ms step_avg:138.18ms
step:182/1370 train_time:23768ms step_avg:138.19ms
step:183/1370 train_time:23907ms step_avg:138.19ms
step:184/1370 train_time:24048ms step_avg:138.21ms
step:185/1370 train_time:24189ms step_avg:138.22ms
step:186/1370 train_time:24329ms step_avg:138.23ms
step:187/1370 train_time:24469ms step_avg:138.24ms
step:188/1370 train_time:24609ms step_avg:138.25ms
step:189/1370 train_time:24750ms step_avg:138.27ms
step:190/1370 train_time:24891ms step_avg:138.29ms
step:191/1370 train_time:25063ms step_avg:138.47ms
step:192/1370 train_time:25202ms step_avg:138.47ms
step:193/1370 train_time:25340ms step_avg:138.47ms
step:194/1370 train_time:25481ms step_avg:138.48ms
step:195/1370 train_time:25620ms step_avg:138.48ms
step:196/1370 train_time:25759ms step_avg:138.49ms
step:197/1370 train_time:25902ms step_avg:138.51ms
step:198/1370 train_time:26045ms step_avg:138.54ms
step:199/1370 train_time:26186ms step_avg:138.55ms
step:200/1370 train_time:26325ms step_avg:138.55ms
step:201/1370 train_time:26465ms step_avg:138.56ms
step:202/1370 train_time:26605ms step_avg:138.57ms
step:203/1370 train_time:26744ms step_avg:138.57ms
step:204/1370 train_time:26888ms step_avg:138.60ms
step:205/1370 train_time:27035ms step_avg:138.64ms
step:206/1370 train_time:27180ms step_avg:138.67ms
step:207/1370 train_time:27322ms step_avg:138.69ms
step:208/1370 train_time:27464ms step_avg:138.71ms
step:209/1370 train_time:27606ms step_avg:138.73ms
step:210/1370 train_time:27749ms step_avg:138.74ms
step:211/1370 train_time:27893ms step_avg:138.77ms
step:212/1370 train_time:28036ms step_avg:138.79ms
step:213/1370 train_time:28180ms step_avg:138.82ms
step:214/1370 train_time:28323ms step_avg:138.84ms
step:215/1370 train_time:28467ms step_avg:138.86ms
step:216/1370 train_time:28610ms step_avg:138.88ms
step:217/1370 train_time:28752ms step_avg:138.90ms
step:218/1370 train_time:28896ms step_avg:138.92ms
step:219/1370 train_time:29039ms step_avg:138.94ms
step:220/1370 train_time:29182ms step_avg:138.96ms
step:221/1370 train_time:29325ms step_avg:138.98ms
step:222/1370 train_time:29471ms step_avg:139.01ms
step:223/1370 train_time:29613ms step_avg:139.03ms
step:224/1370 train_time:29756ms step_avg:139.05ms
step:225/1370 train_time:29901ms step_avg:139.08ms
step:226/1370 train_time:30044ms step_avg:139.09ms
step:227/1370 train_time:30187ms step_avg:139.11ms
step:228/1370 train_time:30330ms step_avg:139.13ms
step:229/1370 train_time:30473ms step_avg:139.15ms
step:230/1370 train_time:30615ms step_avg:139.16ms
step:231/1370 train_time:30758ms step_avg:139.18ms
step:232/1370 train_time:30902ms step_avg:139.20ms
step:233/1370 train_time:31045ms step_avg:139.22ms
step:234/1370 train_time:31190ms step_avg:139.24ms
step:235/1370 train_time:31334ms step_avg:139.26ms
step:236/1370 train_time:31476ms step_avg:139.28ms
step:237/1370 train_time:31618ms step_avg:139.29ms
step:238/1370 train_time:31762ms step_avg:139.31ms
step:239/1370 train_time:31906ms step_avg:139.33ms
step:240/1370 train_time:32050ms step_avg:139.35ms
step:241/1370 train_time:32194ms step_avg:139.37ms
step:242/1370 train_time:32337ms step_avg:139.38ms
step:243/1370 train_time:32480ms step_avg:139.40ms
step:244/1370 train_time:32623ms step_avg:139.41ms
step:245/1370 train_time:32765ms step_avg:139.42ms
step:246/1370 train_time:32908ms step_avg:139.44ms
step:247/1370 train_time:33050ms step_avg:139.45ms
step:248/1370 train_time:33194ms step_avg:139.47ms
step:249/1370 train_time:33337ms step_avg:139.49ms
step:250/1370 train_time:33480ms step_avg:139.50ms
step:250/1370 val_loss:3.9588 train_time:33545ms step_avg:139.77ms
step:251/1370 train_time:33624ms step_avg:139.52ms
step:252/1370 train_time:33774ms step_avg:139.56ms
step:253/1370 train_time:33916ms step_avg:139.57ms
step:254/1370 train_time:34058ms step_avg:139.58ms
step:255/1370 train_time:34200ms step_avg:139.59ms
step:256/1370 train_time:34341ms step_avg:139.60ms
step:257/1370 train_time:34484ms step_avg:139.61ms
step:258/1370 train_time:34629ms step_avg:139.63ms
step:259/1370 train_time:34773ms step_avg:139.65ms
step:260/1370 train_time:34915ms step_avg:139.66ms
step:261/1370 train_time:35059ms step_avg:139.68ms
step:262/1370 train_time:35201ms step_avg:139.69ms
step:263/1370 train_time:35343ms step_avg:139.70ms
step:264/1370 train_time:35487ms step_avg:139.71ms
step:265/1370 train_time:35630ms step_avg:139.73ms
step:266/1370 train_time:35776ms step_avg:139.75ms
step:267/1370 train_time:35918ms step_avg:139.76ms
step:268/1370 train_time:36061ms step_avg:139.77ms
step:269/1370 train_time:36203ms step_avg:139.78ms
step:270/1370 train_time:36347ms step_avg:139.80ms
step:271/1370 train_time:36491ms step_avg:139.81ms
step:272/1370 train_time:36636ms step_avg:139.83ms
step:273/1370 train_time:36781ms step_avg:139.85ms
step:274/1370 train_time:36923ms step_avg:139.86ms
step:275/1370 train_time:37065ms step_avg:139.87ms
step:276/1370 train_time:37206ms step_avg:139.87ms
step:277/1370 train_time:37350ms step_avg:139.89ms
step:278/1370 train_time:37493ms step_avg:139.90ms
step:279/1370 train_time:37636ms step_avg:139.91ms
step:280/1370 train_time:37780ms step_avg:139.93ms
step:281/1370 train_time:37923ms step_avg:139.94ms
step:282/1370 train_time:38066ms step_avg:139.95ms
step:283/1370 train_time:38209ms step_avg:139.96ms
step:284/1370 train_time:38353ms step_avg:139.97ms
step:285/1370 train_time:38497ms step_avg:139.99ms
step:286/1370 train_time:38641ms step_avg:140.00ms
step:287/1370 train_time:38786ms step_avg:140.02ms
step:288/1370 train_time:38929ms step_avg:140.03ms
step:289/1370 train_time:39074ms step_avg:140.05ms
step:290/1370 train_time:39216ms step_avg:140.06ms
step:291/1370 train_time:39359ms step_avg:140.07ms
step:292/1370 train_time:39503ms step_avg:140.08ms
step:293/1370 train_time:39646ms step_avg:140.09ms
step:294/1370 train_time:39790ms step_avg:140.11ms
step:295/1370 train_time:39933ms step_avg:140.12ms
step:296/1370 train_time:40077ms step_avg:140.13ms
step:297/1370 train_time:40220ms step_avg:140.14ms
step:298/1370 train_time:40363ms step_avg:140.15ms
step:299/1370 train_time:40508ms step_avg:140.17ms
step:300/1370 train_time:40653ms step_avg:140.18ms
step:301/1370 train_time:40797ms step_avg:140.19ms
step:302/1370 train_time:40939ms step_avg:140.20ms
step:303/1370 train_time:41083ms step_avg:140.21ms
step:304/1370 train_time:41224ms step_avg:140.22ms
step:305/1370 train_time:41367ms step_avg:140.23ms
step:306/1370 train_time:41510ms step_avg:140.24ms
step:307/1370 train_time:41657ms step_avg:140.26ms
step:308/1370 train_time:41802ms step_avg:140.27ms
step:309/1370 train_time:41947ms step_avg:140.29ms
step:310/1370 train_time:42094ms step_avg:140.31ms
step:311/1370 train_time:42239ms step_avg:140.33ms
step:312/1370 train_time:42383ms step_avg:140.34ms
step:313/1370 train_time:42529ms step_avg:140.36ms
step:314/1370 train_time:42675ms step_avg:140.38ms
step:315/1370 train_time:42820ms step_avg:140.39ms
step:316/1370 train_time:42967ms step_avg:140.41ms
step:317/1370 train_time:43112ms step_avg:140.43ms
step:318/1370 train_time:43258ms step_avg:140.45ms
step:319/1370 train_time:43403ms step_avg:140.46ms
step:320/1370 train_time:43548ms step_avg:140.48ms
step:321/1370 train_time:43694ms step_avg:140.50ms
step:322/1370 train_time:43839ms step_avg:140.51ms
step:323/1370 train_time:43986ms step_avg:140.53ms
step:324/1370 train_time:44130ms step_avg:140.54ms
step:325/1370 train_time:44276ms step_avg:140.56ms
step:326/1370 train_time:44421ms step_avg:140.57ms
step:327/1370 train_time:44566ms step_avg:140.59ms
step:328/1370 train_time:44711ms step_avg:140.60ms
step:329/1370 train_time:44859ms step_avg:140.62ms
step:330/1370 train_time:45004ms step_avg:140.64ms
step:331/1370 train_time:45151ms step_avg:140.66ms
step:332/1370 train_time:45297ms step_avg:140.67ms
step:333/1370 train_time:45441ms step_avg:140.69ms
step:334/1370 train_time:45587ms step_avg:140.70ms
step:335/1370 train_time:45732ms step_avg:140.71ms
step:336/1370 train_time:45877ms step_avg:140.73ms
step:337/1370 train_time:46021ms step_avg:140.74ms
step:338/1370 train_time:46167ms step_avg:140.75ms
step:339/1370 train_time:46313ms step_avg:140.77ms
step:340/1370 train_time:46459ms step_avg:140.78ms
step:341/1370 train_time:46604ms step_avg:140.80ms
step:342/1370 train_time:46749ms step_avg:140.81ms
step:343/1370 train_time:46895ms step_avg:140.83ms
step:344/1370 train_time:47040ms step_avg:140.84ms
step:345/1370 train_time:47186ms step_avg:140.85ms
step:346/1370 train_time:47331ms step_avg:140.87ms
step:347/1370 train_time:47477ms step_avg:140.88ms
step:348/1370 train_time:47621ms step_avg:140.89ms
step:349/1370 train_time:47766ms step_avg:140.90ms
step:350/1370 train_time:47912ms step_avg:140.92ms
step:351/1370 train_time:48059ms step_avg:140.94ms
step:352/1370 train_time:48204ms step_avg:140.95ms
step:353/1370 train_time:48348ms step_avg:140.96ms
step:354/1370 train_time:48495ms step_avg:140.97ms
step:355/1370 train_time:48638ms step_avg:140.98ms
step:356/1370 train_time:48785ms step_avg:141.00ms
step:357/1370 train_time:48931ms step_avg:141.01ms
step:358/1370 train_time:49080ms step_avg:141.03ms
step:359/1370 train_time:49224ms step_avg:141.04ms
step:360/1370 train_time:49371ms step_avg:141.06ms
step:361/1370 train_time:49515ms step_avg:141.07ms
step:362/1370 train_time:49661ms step_avg:141.08ms
step:363/1370 train_time:49807ms step_avg:141.10ms
step:364/1370 train_time:49953ms step_avg:141.11ms
step:365/1370 train_time:50099ms step_avg:141.12ms
step:366/1370 train_time:50243ms step_avg:141.13ms
step:367/1370 train_time:50390ms step_avg:141.15ms
step:368/1370 train_time:50535ms step_avg:141.16ms
step:369/1370 train_time:50681ms step_avg:141.17ms
step:370/1370 train_time:50825ms step_avg:141.18ms
step:371/1370 train_time:50972ms step_avg:141.20ms
step:372/1370 train_time:51117ms step_avg:141.21ms
step:373/1370 train_time:51261ms step_avg:141.22ms
step:374/1370 train_time:51408ms step_avg:141.23ms
step:375/1370 train_time:51552ms step_avg:141.24ms
step:375/1370 val_loss:3.7724 train_time:51617ms step_avg:141.42ms
step:376/1370 train_time:51698ms step_avg:141.25ms
step:377/1370 train_time:51845ms step_avg:141.27ms
step:378/1370 train_time:51990ms step_avg:141.28ms
step:379/1370 train_time:52134ms step_avg:141.28ms
step:380/1370 train_time:52278ms step_avg:141.29ms
step:381/1370 train_time:52454ms step_avg:141.39ms
step:382/1370 train_time:52604ms step_avg:141.41ms
step:383/1370 train_time:52747ms step_avg:141.41ms
step:384/1370 train_time:52892ms step_avg:141.42ms
step:385/1370 train_time:53037ms step_avg:141.43ms
step:386/1370 train_time:53180ms step_avg:141.44ms
step:387/1370 train_time:53328ms step_avg:141.45ms
step:388/1370 train_time:53477ms step_avg:141.47ms
step:389/1370 train_time:53623ms step_avg:141.48ms
step:390/1370 train_time:53767ms step_avg:141.49ms
step:391/1370 train_time:53911ms step_avg:141.50ms
step:392/1370 train_time:54056ms step_avg:141.51ms
step:393/1370 train_time:54200ms step_avg:141.52ms
step:394/1370 train_time:54348ms step_avg:141.53ms
step:395/1370 train_time:54494ms step_avg:141.54ms
step:396/1370 train_time:54639ms step_avg:141.55ms
step:397/1370 train_time:54787ms step_avg:141.57ms
step:398/1370 train_time:54932ms step_avg:141.58ms
step:399/1370 train_time:55077ms step_avg:141.59ms
step:400/1370 train_time:55223ms step_avg:141.60ms
step:401/1370 train_time:55369ms step_avg:141.61ms
step:402/1370 train_time:55513ms step_avg:141.62ms
step:403/1370 train_time:55660ms step_avg:141.63ms
step:404/1370 train_time:55806ms step_avg:141.64ms
step:405/1370 train_time:55952ms step_avg:141.65ms
step:406/1370 train_time:56097ms step_avg:141.66ms
step:407/1370 train_time:56244ms step_avg:141.67ms
step:408/1370 train_time:56392ms step_avg:141.69ms
step:409/1370 train_time:56539ms step_avg:141.70ms
step:410/1370 train_time:56687ms step_avg:141.72ms
step:411/1370 train_time:56834ms step_avg:141.73ms
step:412/1370 train_time:56981ms step_avg:141.74ms
step:413/1370 train_time:57128ms step_avg:141.76ms
step:414/1370 train_time:57276ms step_avg:141.77ms
step:415/1370 train_time:57423ms step_avg:141.78ms
step:416/1370 train_time:57570ms step_avg:141.80ms
step:417/1370 train_time:57717ms step_avg:141.81ms
step:418/1370 train_time:57866ms step_avg:141.83ms
step:419/1370 train_time:58011ms step_avg:141.84ms
step:420/1370 train_time:58160ms step_avg:141.85ms
step:421/1370 train_time:58307ms step_avg:141.87ms
step:422/1370 train_time:58456ms step_avg:141.88ms
step:423/1370 train_time:58601ms step_avg:141.89ms
step:424/1370 train_time:58749ms step_avg:141.91ms
step:425/1370 train_time:58898ms step_avg:141.92ms
step:426/1370 train_time:59045ms step_avg:141.94ms
step:427/1370 train_time:59192ms step_avg:141.95ms
step:428/1370 train_time:59340ms step_avg:141.96ms
step:429/1370 train_time:59489ms step_avg:141.98ms
step:430/1370 train_time:59636ms step_avg:141.99ms
step:431/1370 train_time:59784ms step_avg:142.01ms
step:432/1370 train_time:59930ms step_avg:142.01ms
step:433/1370 train_time:60079ms step_avg:142.03ms
step:434/1370 train_time:60224ms step_avg:142.04ms
step:435/1370 train_time:60373ms step_avg:142.05ms
step:436/1370 train_time:60520ms step_avg:142.07ms
step:437/1370 train_time:60668ms step_avg:142.08ms
step:438/1370 train_time:60813ms step_avg:142.09ms
step:439/1370 train_time:60961ms step_avg:142.10ms
step:440/1370 train_time:61107ms step_avg:142.11ms
step:441/1370 train_time:61255ms step_avg:142.12ms
step:442/1370 train_time:61402ms step_avg:142.14ms
step:443/1370 train_time:61549ms step_avg:142.15ms
step:444/1370 train_time:61697ms step_avg:142.16ms
step:445/1370 train_time:61845ms step_avg:142.17ms
step:446/1370 train_time:61992ms step_avg:142.18ms
step:447/1370 train_time:62140ms step_avg:142.20ms
step:448/1370 train_time:62287ms step_avg:142.21ms
step:449/1370 train_time:62436ms step_avg:142.22ms
step:450/1370 train_time:62583ms step_avg:142.23ms
step:451/1370 train_time:62730ms step_avg:142.25ms
step:452/1370 train_time:62879ms step_avg:142.26ms
step:453/1370 train_time:63025ms step_avg:142.27ms
step:454/1370 train_time:63172ms step_avg:142.28ms
step:455/1370 train_time:63320ms step_avg:142.29ms
step:456/1370 train_time:63467ms step_avg:142.30ms
step:457/1370 train_time:63615ms step_avg:142.31ms
step:458/1370 train_time:63762ms step_avg:142.33ms
step:459/1370 train_time:63909ms step_avg:142.34ms
step:460/1370 train_time:64055ms step_avg:142.35ms
step:461/1370 train_time:64203ms step_avg:142.36ms
step:462/1370 train_time:64351ms step_avg:142.37ms
step:463/1370 train_time:64498ms step_avg:142.38ms
step:464/1370 train_time:64647ms step_avg:142.39ms
step:465/1370 train_time:64793ms step_avg:142.40ms
step:466/1370 train_time:64941ms step_avg:142.41ms
step:467/1370 train_time:65089ms step_avg:142.43ms
step:468/1370 train_time:65235ms step_avg:142.44ms
step:469/1370 train_time:65384ms step_avg:142.45ms
step:470/1370 train_time:65531ms step_avg:142.46ms
step:471/1370 train_time:65679ms step_avg:142.47ms
step:472/1370 train_time:65825ms step_avg:142.48ms
step:473/1370 train_time:65974ms step_avg:142.49ms
step:474/1370 train_time:66119ms step_avg:142.50ms
step:475/1370 train_time:66267ms step_avg:142.51ms
step:476/1370 train_time:66414ms step_avg:142.52ms
step:477/1370 train_time:66562ms step_avg:142.53ms
step:478/1370 train_time:66708ms step_avg:142.54ms
step:479/1370 train_time:66857ms step_avg:142.55ms
step:480/1370 train_time:67003ms step_avg:142.56ms
step:481/1370 train_time:67150ms step_avg:142.57ms
step:482/1370 train_time:67296ms step_avg:142.58ms
step:483/1370 train_time:67443ms step_avg:142.59ms
step:484/1370 train_time:67590ms step_avg:142.59ms
step:485/1370 train_time:67737ms step_avg:142.60ms
step:486/1370 train_time:67886ms step_avg:142.62ms
step:487/1370 train_time:68032ms step_avg:142.62ms
step:488/1370 train_time:68179ms step_avg:142.63ms
step:489/1370 train_time:68325ms step_avg:142.64ms
step:490/1370 train_time:68475ms step_avg:142.66ms
step:491/1370 train_time:68621ms step_avg:142.66ms
step:492/1370 train_time:68767ms step_avg:142.67ms
step:493/1370 train_time:68914ms step_avg:142.68ms
step:494/1370 train_time:69062ms step_avg:142.69ms
step:495/1370 train_time:69210ms step_avg:142.70ms
step:496/1370 train_time:69358ms step_avg:142.71ms
step:497/1370 train_time:69504ms step_avg:142.72ms
step:498/1370 train_time:69652ms step_avg:142.73ms
step:499/1370 train_time:69799ms step_avg:142.74ms
step:500/1370 train_time:69947ms step_avg:142.75ms
step:500/1370 val_loss:3.6582 train_time:70014ms step_avg:142.89ms
step:501/1370 train_time:70096ms step_avg:142.76ms
step:502/1370 train_time:70246ms step_avg:142.78ms
step:503/1370 train_time:70391ms step_avg:142.78ms
step:504/1370 train_time:70537ms step_avg:142.79ms
step:505/1370 train_time:70683ms step_avg:142.79ms
step:506/1370 train_time:70829ms step_avg:142.80ms
step:507/1370 train_time:70978ms step_avg:142.81ms
step:508/1370 train_time:71128ms step_avg:142.83ms
step:509/1370 train_time:71277ms step_avg:142.84ms
step:510/1370 train_time:71425ms step_avg:142.85ms
step:511/1370 train_time:71573ms step_avg:142.86ms
step:512/1370 train_time:71724ms step_avg:142.88ms
step:513/1370 train_time:71874ms step_avg:142.89ms
step:514/1370 train_time:72024ms step_avg:142.90ms
step:515/1370 train_time:72174ms step_avg:142.92ms
step:516/1370 train_time:72324ms step_avg:142.93ms
step:517/1370 train_time:72472ms step_avg:142.94ms
step:518/1370 train_time:72622ms step_avg:142.96ms
step:519/1370 train_time:72769ms step_avg:142.96ms
step:520/1370 train_time:72919ms step_avg:142.98ms
step:521/1370 train_time:73067ms step_avg:142.99ms
step:522/1370 train_time:73219ms step_avg:143.01ms
step:523/1370 train_time:73368ms step_avg:143.02ms
step:524/1370 train_time:73516ms step_avg:143.03ms
step:525/1370 train_time:73665ms step_avg:143.04ms
step:526/1370 train_time:73813ms step_avg:143.05ms
step:527/1370 train_time:73963ms step_avg:143.06ms
step:528/1370 train_time:74111ms step_avg:143.07ms
step:529/1370 train_time:74262ms step_avg:143.09ms
step:530/1370 train_time:74409ms step_avg:143.09ms
step:531/1370 train_time:74558ms step_avg:143.11ms
step:532/1370 train_time:74706ms step_avg:143.12ms
step:533/1370 train_time:74856ms step_avg:143.13ms
step:534/1370 train_time:75005ms step_avg:143.14ms
step:535/1370 train_time:75155ms step_avg:143.15ms
step:536/1370 train_time:75303ms step_avg:143.16ms
step:537/1370 train_time:75453ms step_avg:143.18ms
step:538/1370 train_time:75601ms step_avg:143.18ms
step:539/1370 train_time:75751ms step_avg:143.20ms
step:540/1370 train_time:75900ms step_avg:143.21ms
step:541/1370 train_time:76048ms step_avg:143.22ms
step:542/1370 train_time:76197ms step_avg:143.23ms
step:543/1370 train_time:76347ms step_avg:143.24ms
step:544/1370 train_time:76494ms step_avg:143.25ms
step:545/1370 train_time:76645ms step_avg:143.26ms
step:546/1370 train_time:76792ms step_avg:143.27ms
step:547/1370 train_time:76942ms step_avg:143.28ms
step:548/1370 train_time:77091ms step_avg:143.29ms
step:549/1370 train_time:77242ms step_avg:143.31ms
step:550/1370 train_time:77391ms step_avg:143.32ms
step:551/1370 train_time:77540ms step_avg:143.33ms
step:552/1370 train_time:77687ms step_avg:143.33ms
step:553/1370 train_time:77837ms step_avg:143.35ms
step:554/1370 train_time:77985ms step_avg:143.36ms
step:555/1370 train_time:78136ms step_avg:143.37ms
step:556/1370 train_time:78283ms step_avg:143.38ms
step:557/1370 train_time:78433ms step_avg:143.39ms
step:558/1370 train_time:78581ms step_avg:143.40ms
step:559/1370 train_time:78729ms step_avg:143.40ms
step:560/1370 train_time:78878ms step_avg:143.41ms
step:561/1370 train_time:79026ms step_avg:143.42ms
step:562/1370 train_time:79175ms step_avg:143.43ms
step:563/1370 train_time:79324ms step_avg:143.44ms
step:564/1370 train_time:79473ms step_avg:143.45ms
step:565/1370 train_time:79622ms step_avg:143.46ms
step:566/1370 train_time:79770ms step_avg:143.47ms
step:567/1370 train_time:79918ms step_avg:143.48ms
step:568/1370 train_time:80068ms step_avg:143.49ms
step:569/1370 train_time:80219ms step_avg:143.50ms
step:570/1370 train_time:80368ms step_avg:143.51ms
step:571/1370 train_time:80549ms step_avg:143.58ms
step:572/1370 train_time:80699ms step_avg:143.59ms
step:573/1370 train_time:80847ms step_avg:143.60ms
step:574/1370 train_time:80997ms step_avg:143.61ms
step:575/1370 train_time:81145ms step_avg:143.62ms
step:576/1370 train_time:81294ms step_avg:143.63ms
step:577/1370 train_time:81446ms step_avg:143.64ms
step:578/1370 train_time:81596ms step_avg:143.65ms
step:579/1370 train_time:81745ms step_avg:143.66ms
step:580/1370 train_time:81893ms step_avg:143.67ms
step:581/1370 train_time:82041ms step_avg:143.68ms
step:582/1370 train_time:82190ms step_avg:143.69ms
step:583/1370 train_time:82339ms step_avg:143.70ms
step:584/1370 train_time:82487ms step_avg:143.71ms
step:585/1370 train_time:82638ms step_avg:143.72ms
step:586/1370 train_time:82786ms step_avg:143.73ms
step:587/1370 train_time:82936ms step_avg:143.74ms
step:588/1370 train_time:83084ms step_avg:143.74ms
step:589/1370 train_time:83233ms step_avg:143.75ms
step:590/1370 train_time:83382ms step_avg:143.76ms
step:591/1370 train_time:83532ms step_avg:143.77ms
step:592/1370 train_time:83682ms step_avg:143.78ms
step:593/1370 train_time:83831ms step_avg:143.79ms
step:594/1370 train_time:83980ms step_avg:143.80ms
step:595/1370 train_time:84129ms step_avg:143.81ms
step:596/1370 train_time:84277ms step_avg:143.82ms
step:597/1370 train_time:84425ms step_avg:143.83ms
step:598/1370 train_time:84574ms step_avg:143.83ms
step:599/1370 train_time:84724ms step_avg:143.84ms
step:600/1370 train_time:84873ms step_avg:143.85ms
step:601/1370 train_time:85022ms step_avg:143.86ms
step:602/1370 train_time:85172ms step_avg:143.87ms
step:603/1370 train_time:85322ms step_avg:143.88ms
step:604/1370 train_time:85470ms step_avg:143.89ms
step:605/1370 train_time:85620ms step_avg:143.90ms
step:606/1370 train_time:85769ms step_avg:143.91ms
step:607/1370 train_time:85919ms step_avg:143.92ms
step:608/1370 train_time:86068ms step_avg:143.93ms
step:609/1370 train_time:86216ms step_avg:143.93ms
step:610/1370 train_time:86365ms step_avg:143.94ms
step:611/1370 train_time:86513ms step_avg:143.95ms
step:612/1370 train_time:86664ms step_avg:143.96ms
step:613/1370 train_time:86813ms step_avg:143.97ms
step:614/1370 train_time:86964ms step_avg:143.98ms
step:615/1370 train_time:87113ms step_avg:143.99ms
step:616/1370 train_time:87263ms step_avg:144.00ms
step:617/1370 train_time:87412ms step_avg:144.01ms
step:618/1370 train_time:87564ms step_avg:144.02ms
step:619/1370 train_time:87713ms step_avg:144.03ms
step:620/1370 train_time:87865ms step_avg:144.04ms
step:621/1370 train_time:88014ms step_avg:144.05ms
step:622/1370 train_time:88167ms step_avg:144.06ms
step:623/1370 train_time:88319ms step_avg:144.08ms
step:624/1370 train_time:88468ms step_avg:144.08ms
step:625/1370 train_time:88618ms step_avg:144.09ms
step:625/1370 val_loss:3.5738 train_time:88688ms step_avg:144.21ms
step:626/1370 train_time:88771ms step_avg:144.11ms
step:627/1370 train_time:88922ms step_avg:144.12ms
step:628/1370 train_time:89070ms step_avg:144.13ms
step:629/1370 train_time:89220ms step_avg:144.14ms
step:630/1370 train_time:89369ms step_avg:144.14ms
step:631/1370 train_time:89519ms step_avg:144.15ms
step:632/1370 train_time:89670ms step_avg:144.16ms
step:633/1370 train_time:89822ms step_avg:144.18ms
step:634/1370 train_time:89972ms step_avg:144.19ms
step:635/1370 train_time:90122ms step_avg:144.20ms
step:636/1370 train_time:90272ms step_avg:144.20ms
step:637/1370 train_time:90423ms step_avg:144.22ms
step:638/1370 train_time:90572ms step_avg:144.22ms
step:639/1370 train_time:90723ms step_avg:144.23ms
step:640/1370 train_time:90874ms step_avg:144.24ms
step:641/1370 train_time:91027ms step_avg:144.26ms
step:642/1370 train_time:91179ms step_avg:144.27ms
step:643/1370 train_time:91329ms step_avg:144.28ms
step:644/1370 train_time:91479ms step_avg:144.29ms
step:645/1370 train_time:91629ms step_avg:144.30ms
step:646/1370 train_time:91781ms step_avg:144.31ms
step:647/1370 train_time:91931ms step_avg:144.32ms
step:648/1370 train_time:92085ms step_avg:144.33ms
step:649/1370 train_time:92236ms step_avg:144.34ms
step:650/1370 train_time:92388ms step_avg:144.36ms
step:651/1370 train_time:92538ms step_avg:144.36ms
step:652/1370 train_time:92687ms step_avg:144.37ms
step:653/1370 train_time:92837ms step_avg:144.38ms
step:654/1370 train_time:92990ms step_avg:144.39ms
step:655/1370 train_time:93141ms step_avg:144.41ms
step:656/1370 train_time:93290ms step_avg:144.41ms
step:657/1370 train_time:93440ms step_avg:144.42ms
step:658/1370 train_time:93591ms step_avg:144.43ms
step:659/1370 train_time:93740ms step_avg:144.44ms
step:660/1370 train_time:93889ms step_avg:144.45ms
step:661/1370 train_time:94040ms step_avg:144.45ms
step:662/1370 train_time:94189ms step_avg:144.46ms
step:663/1370 train_time:94338ms step_avg:144.47ms
step:664/1370 train_time:94489ms step_avg:144.48ms
step:665/1370 train_time:94640ms step_avg:144.49ms
step:666/1370 train_time:94788ms step_avg:144.49ms
step:667/1370 train_time:94941ms step_avg:144.51ms
step:668/1370 train_time:95093ms step_avg:144.52ms
step:669/1370 train_time:95243ms step_avg:144.53ms
step:670/1370 train_time:95394ms step_avg:144.54ms
step:671/1370 train_time:95547ms step_avg:144.55ms
step:672/1370 train_time:95698ms step_avg:144.56ms
step:673/1370 train_time:95847ms step_avg:144.57ms
step:674/1370 train_time:95999ms step_avg:144.58ms
step:675/1370 train_time:96149ms step_avg:144.58ms
step:676/1370 train_time:96300ms step_avg:144.59ms
step:677/1370 train_time:96449ms step_avg:144.60ms
step:678/1370 train_time:96598ms step_avg:144.61ms
step:679/1370 train_time:96749ms step_avg:144.62ms
step:680/1370 train_time:96900ms step_avg:144.63ms
step:681/1370 train_time:97050ms step_avg:144.63ms
step:682/1370 train_time:97202ms step_avg:144.65ms
step:683/1370 train_time:97351ms step_avg:144.65ms
step:684/1370 train_time:97501ms step_avg:144.66ms
step:685/1370 train_time:97650ms step_avg:144.67ms
step:686/1370 train_time:97801ms step_avg:144.68ms
step:687/1370 train_time:97950ms step_avg:144.68ms
step:688/1370 train_time:98103ms step_avg:144.69ms
step:689/1370 train_time:98252ms step_avg:144.70ms
step:690/1370 train_time:98405ms step_avg:144.71ms
step:691/1370 train_time:98554ms step_avg:144.72ms
step:692/1370 train_time:98706ms step_avg:144.73ms
step:693/1370 train_time:98857ms step_avg:144.74ms
step:694/1370 train_time:99007ms step_avg:144.75ms
step:695/1370 train_time:99157ms step_avg:144.75ms
step:696/1370 train_time:99309ms step_avg:144.76ms
step:697/1370 train_time:99459ms step_avg:144.77ms
step:698/1370 train_time:99608ms step_avg:144.78ms
step:699/1370 train_time:99759ms step_avg:144.79ms
step:700/1370 train_time:99909ms step_avg:144.79ms
step:701/1370 train_time:100058ms step_avg:144.80ms
step:702/1370 train_time:100210ms step_avg:144.81ms
step:703/1370 train_time:100361ms step_avg:144.82ms
step:704/1370 train_time:100513ms step_avg:144.83ms
step:705/1370 train_time:100663ms step_avg:144.84ms
step:706/1370 train_time:100816ms step_avg:144.85ms
step:707/1370 train_time:100965ms step_avg:144.86ms
step:708/1370 train_time:101116ms step_avg:144.87ms
step:709/1370 train_time:101266ms step_avg:144.87ms
step:710/1370 train_time:101420ms step_avg:144.89ms
step:711/1370 train_time:101570ms step_avg:144.89ms
step:712/1370 train_time:101723ms step_avg:144.91ms
step:713/1370 train_time:101876ms step_avg:144.92ms
step:714/1370 train_time:102027ms step_avg:144.92ms
step:715/1370 train_time:102178ms step_avg:144.93ms
step:716/1370 train_time:102329ms step_avg:144.94ms
step:717/1370 train_time:102482ms step_avg:144.95ms
step:718/1370 train_time:102632ms step_avg:144.96ms
step:719/1370 train_time:102783ms step_avg:144.97ms
step:720/1370 train_time:102936ms step_avg:144.98ms
step:721/1370 train_time:103088ms step_avg:144.99ms
step:722/1370 train_time:103241ms step_avg:145.00ms
step:723/1370 train_time:103390ms step_avg:145.01ms
step:724/1370 train_time:103544ms step_avg:145.02ms
step:725/1370 train_time:103696ms step_avg:145.03ms
step:726/1370 train_time:103848ms step_avg:145.04ms
step:727/1370 train_time:104004ms step_avg:145.05ms
step:728/1370 train_time:104156ms step_avg:145.06ms
step:729/1370 train_time:104307ms step_avg:145.07ms
step:730/1370 train_time:104458ms step_avg:145.08ms
step:731/1370 train_time:104609ms step_avg:145.09ms
step:732/1370 train_time:104759ms step_avg:145.10ms
step:733/1370 train_time:104912ms step_avg:145.11ms
step:734/1370 train_time:105063ms step_avg:145.11ms
step:735/1370 train_time:105218ms step_avg:145.13ms
step:736/1370 train_time:105369ms step_avg:145.14ms
step:737/1370 train_time:105521ms step_avg:145.15ms
step:738/1370 train_time:105672ms step_avg:145.15ms
step:739/1370 train_time:105825ms step_avg:145.17ms
step:740/1370 train_time:105978ms step_avg:145.18ms
step:741/1370 train_time:106130ms step_avg:145.19ms
step:742/1370 train_time:106282ms step_avg:145.19ms
step:743/1370 train_time:106432ms step_avg:145.20ms
step:744/1370 train_time:106584ms step_avg:145.21ms
step:745/1370 train_time:106736ms step_avg:145.22ms
step:746/1370 train_time:106888ms step_avg:145.23ms
step:747/1370 train_time:107040ms step_avg:145.24ms
step:748/1370 train_time:107191ms step_avg:145.25ms
step:749/1370 train_time:107343ms step_avg:145.25ms
step:750/1370 train_time:107495ms step_avg:145.26ms
step:750/1370 val_loss:3.5197 train_time:107565ms step_avg:145.36ms
step:751/1370 train_time:107649ms step_avg:145.27ms
step:752/1370 train_time:107803ms step_avg:145.29ms
step:753/1370 train_time:107955ms step_avg:145.30ms
step:754/1370 train_time:108104ms step_avg:145.30ms
step:755/1370 train_time:108255ms step_avg:145.31ms
step:756/1370 train_time:108404ms step_avg:145.31ms
step:757/1370 train_time:108560ms step_avg:145.33ms
step:758/1370 train_time:108712ms step_avg:145.34ms
step:759/1370 train_time:108864ms step_avg:145.35ms
step:760/1370 train_time:109014ms step_avg:145.35ms
step:761/1370 train_time:109204ms step_avg:145.41ms
step:762/1370 train_time:109354ms step_avg:145.42ms
step:763/1370 train_time:109503ms step_avg:145.42ms
step:764/1370 train_time:109655ms step_avg:145.43ms
step:765/1370 train_time:109804ms step_avg:145.44ms
step:766/1370 train_time:109957ms step_avg:145.45ms
step:767/1370 train_time:110111ms step_avg:145.46ms
step:768/1370 train_time:110264ms step_avg:145.47ms
step:769/1370 train_time:110415ms step_avg:145.47ms
step:770/1370 train_time:110567ms step_avg:145.48ms
step:771/1370 train_time:110718ms step_avg:145.49ms
step:772/1370 train_time:110869ms step_avg:145.50ms
step:773/1370 train_time:111020ms step_avg:145.51ms
step:774/1370 train_time:111175ms step_avg:145.52ms
step:775/1370 train_time:111328ms step_avg:145.53ms
step:776/1370 train_time:111480ms step_avg:145.54ms
step:777/1370 train_time:111632ms step_avg:145.54ms
step:778/1370 train_time:111782ms step_avg:145.55ms
step:779/1370 train_time:111934ms step_avg:145.56ms
step:780/1370 train_time:112085ms step_avg:145.56ms
step:781/1370 train_time:112238ms step_avg:145.57ms
step:782/1370 train_time:112390ms step_avg:145.58ms
step:783/1370 train_time:112542ms step_avg:145.59ms
step:784/1370 train_time:112695ms step_avg:145.60ms
step:785/1370 train_time:112844ms step_avg:145.61ms
step:786/1370 train_time:112996ms step_avg:145.61ms
step:787/1370 train_time:113148ms step_avg:145.62ms
step:788/1370 train_time:113301ms step_avg:145.63ms
step:789/1370 train_time:113454ms step_avg:145.64ms
step:790/1370 train_time:113604ms step_avg:145.65ms
step:791/1370 train_time:113756ms step_avg:145.65ms
step:792/1370 train_time:113907ms step_avg:145.66ms
step:793/1370 train_time:114058ms step_avg:145.67ms
step:794/1370 train_time:114211ms step_avg:145.68ms
step:795/1370 train_time:114365ms step_avg:145.69ms
step:796/1370 train_time:114516ms step_avg:145.70ms
step:797/1370 train_time:114667ms step_avg:145.70ms
step:798/1370 train_time:114819ms step_avg:145.71ms
step:799/1370 train_time:114976ms step_avg:145.72ms
step:800/1370 train_time:115127ms step_avg:145.73ms
step:801/1370 train_time:115280ms step_avg:145.74ms
step:802/1370 train_time:115432ms step_avg:145.75ms
step:803/1370 train_time:115581ms step_avg:145.75ms
step:804/1370 train_time:115734ms step_avg:145.76ms
step:805/1370 train_time:115886ms step_avg:145.77ms
step:806/1370 train_time:116038ms step_avg:145.78ms
step:807/1370 train_time:116187ms step_avg:145.78ms
step:808/1370 train_time:116339ms step_avg:145.79ms
step:809/1370 train_time:116490ms step_avg:145.79ms
step:810/1370 train_time:116642ms step_avg:145.80ms
step:811/1370 train_time:116795ms step_avg:145.81ms
step:812/1370 train_time:116946ms step_avg:145.82ms
step:813/1370 train_time:117098ms step_avg:145.83ms
step:814/1370 train_time:117250ms step_avg:145.83ms
step:815/1370 train_time:117403ms step_avg:145.84ms
step:816/1370 train_time:117557ms step_avg:145.85ms
step:817/1370 train_time:117710ms step_avg:145.86ms
step:818/1370 train_time:117862ms step_avg:145.87ms
step:819/1370 train_time:118015ms step_avg:145.88ms
step:820/1370 train_time:118169ms step_avg:145.89ms
step:821/1370 train_time:118321ms step_avg:145.90ms
step:822/1370 train_time:118475ms step_avg:145.91ms
step:823/1370 train_time:118626ms step_avg:145.91ms
step:824/1370 train_time:118780ms step_avg:145.92ms
step:825/1370 train_time:118936ms step_avg:145.93ms
step:826/1370 train_time:119087ms step_avg:145.94ms
step:827/1370 train_time:119239ms step_avg:145.95ms
step:828/1370 train_time:119391ms step_avg:145.95ms
step:829/1370 train_time:119544ms step_avg:145.96ms
step:830/1370 train_time:119696ms step_avg:145.97ms
step:831/1370 train_time:119849ms step_avg:145.98ms
step:832/1370 train_time:120002ms step_avg:145.99ms
step:833/1370 train_time:120156ms step_avg:146.00ms
step:834/1370 train_time:120307ms step_avg:146.00ms
step:835/1370 train_time:120464ms step_avg:146.02ms
step:836/1370 train_time:120620ms step_avg:146.03ms
step:837/1370 train_time:120773ms step_avg:146.04ms
step:838/1370 train_time:120925ms step_avg:146.04ms
step:839/1370 train_time:121077ms step_avg:146.05ms
step:840/1370 train_time:121229ms step_avg:146.06ms
step:841/1370 train_time:121382ms step_avg:146.07ms
step:842/1370 train_time:121537ms step_avg:146.08ms
step:843/1370 train_time:121688ms step_avg:146.08ms
step:844/1370 train_time:121841ms step_avg:146.09ms
step:845/1370 train_time:121992ms step_avg:146.10ms
step:846/1370 train_time:122145ms step_avg:146.11ms
step:847/1370 train_time:122300ms step_avg:146.12ms
step:848/1370 train_time:122452ms step_avg:146.12ms
step:849/1370 train_time:122605ms step_avg:146.13ms
step:850/1370 train_time:122760ms step_avg:146.14ms
step:851/1370 train_time:122914ms step_avg:146.15ms
step:852/1370 train_time:123067ms step_avg:146.16ms
step:853/1370 train_time:123219ms step_avg:146.17ms
step:854/1370 train_time:123373ms step_avg:146.18ms
step:855/1370 train_time:123524ms step_avg:146.18ms
step:856/1370 train_time:123677ms step_avg:146.19ms
step:857/1370 train_time:123831ms step_avg:146.20ms
step:858/1370 train_time:123988ms step_avg:146.21ms
step:859/1370 train_time:124141ms step_avg:146.22ms
step:860/1370 train_time:124294ms step_avg:146.23ms
step:861/1370 train_time:124446ms step_avg:146.23ms
step:862/1370 train_time:124600ms step_avg:146.24ms
step:863/1370 train_time:124752ms step_avg:146.25ms
step:864/1370 train_time:124904ms step_avg:146.26ms
step:865/1370 train_time:125056ms step_avg:146.26ms
step:866/1370 train_time:125214ms step_avg:146.28ms
step:867/1370 train_time:125365ms step_avg:146.28ms
step:868/1370 train_time:125516ms step_avg:146.29ms
step:869/1370 train_time:125669ms step_avg:146.30ms
step:870/1370 train_time:125823ms step_avg:146.31ms
step:871/1370 train_time:125977ms step_avg:146.31ms
step:872/1370 train_time:126129ms step_avg:146.32ms
step:873/1370 train_time:126281ms step_avg:146.33ms
step:874/1370 train_time:126435ms step_avg:146.34ms
step:875/1370 train_time:126588ms step_avg:146.34ms
step:875/1370 val_loss:3.4679 train_time:126658ms step_avg:146.43ms
step:876/1370 train_time:126740ms step_avg:146.35ms
step:877/1370 train_time:126893ms step_avg:146.36ms
step:878/1370 train_time:127047ms step_avg:146.37ms
step:879/1370 train_time:127198ms step_avg:146.37ms
step:880/1370 train_time:127349ms step_avg:146.38ms
step:881/1370 train_time:127501ms step_avg:146.38ms
step:882/1370 train_time:127655ms step_avg:146.39ms
step:883/1370 train_time:127809ms step_avg:146.40ms
step:884/1370 train_time:127962ms step_avg:146.41ms
step:885/1370 train_time:128114ms step_avg:146.42ms
step:886/1370 train_time:128267ms step_avg:146.42ms
step:887/1370 train_time:128419ms step_avg:146.43ms
step:888/1370 train_time:128574ms step_avg:146.44ms
step:889/1370 train_time:128730ms step_avg:146.45ms
step:890/1370 train_time:128882ms step_avg:146.46ms
step:891/1370 train_time:129035ms step_avg:146.46ms
step:892/1370 train_time:129188ms step_avg:146.47ms
step:893/1370 train_time:129340ms step_avg:146.48ms
step:894/1370 train_time:129493ms step_avg:146.49ms
step:895/1370 train_time:129651ms step_avg:146.50ms
step:896/1370 train_time:129803ms step_avg:146.50ms
step:897/1370 train_time:129957ms step_avg:146.51ms
step:898/1370 train_time:130111ms step_avg:146.52ms
step:899/1370 train_time:130262ms step_avg:146.53ms
step:900/1370 train_time:130413ms step_avg:146.53ms
step:901/1370 train_time:130567ms step_avg:146.54ms
step:902/1370 train_time:130719ms step_avg:146.55ms
step:903/1370 train_time:130871ms step_avg:146.55ms
step:904/1370 train_time:131027ms step_avg:146.56ms
step:905/1370 train_time:131181ms step_avg:146.57ms
step:906/1370 train_time:131335ms step_avg:146.58ms
step:907/1370 train_time:131492ms step_avg:146.59ms
step:908/1370 train_time:131645ms step_avg:146.60ms
step:909/1370 train_time:131800ms step_avg:146.61ms
step:910/1370 train_time:131957ms step_avg:146.62ms
step:911/1370 train_time:132108ms step_avg:146.62ms
step:912/1370 train_time:132261ms step_avg:146.63ms
step:913/1370 train_time:132415ms step_avg:146.64ms
step:914/1370 train_time:132566ms step_avg:146.64ms
step:915/1370 train_time:132722ms step_avg:146.65ms
step:916/1370 train_time:132877ms step_avg:146.66ms
step:917/1370 train_time:133030ms step_avg:146.67ms
step:918/1370 train_time:133185ms step_avg:146.68ms
step:919/1370 train_time:133340ms step_avg:146.69ms
step:920/1370 train_time:133494ms step_avg:146.70ms
step:921/1370 train_time:133649ms step_avg:146.71ms
step:922/1370 train_time:133805ms step_avg:146.72ms
step:923/1370 train_time:133958ms step_avg:146.72ms
step:924/1370 train_time:134112ms step_avg:146.73ms
step:925/1370 train_time:134267ms step_avg:146.74ms
step:926/1370 train_time:134422ms step_avg:146.75ms
step:927/1370 train_time:134577ms step_avg:146.76ms
step:928/1370 train_time:134732ms step_avg:146.77ms
step:929/1370 train_time:134889ms step_avg:146.78ms
step:930/1370 train_time:135045ms step_avg:146.79ms
step:931/1370 train_time:135198ms step_avg:146.79ms
step:932/1370 train_time:135351ms step_avg:146.80ms
step:933/1370 train_time:135506ms step_avg:146.81ms
step:934/1370 train_time:135660ms step_avg:146.82ms
step:935/1370 train_time:135816ms step_avg:146.83ms
step:936/1370 train_time:135969ms step_avg:146.83ms
step:937/1370 train_time:136125ms step_avg:146.84ms
step:938/1370 train_time:136279ms step_avg:146.85ms
step:939/1370 train_time:136433ms step_avg:146.86ms
step:940/1370 train_time:136588ms step_avg:146.87ms
step:941/1370 train_time:136741ms step_avg:146.88ms
step:942/1370 train_time:136897ms step_avg:146.88ms
step:943/1370 train_time:137052ms step_avg:146.89ms
step:944/1370 train_time:137213ms step_avg:146.91ms
step:945/1370 train_time:137366ms step_avg:146.92ms
step:946/1370 train_time:137523ms step_avg:146.93ms
step:947/1370 train_time:137677ms step_avg:146.93ms
step:948/1370 train_time:137831ms step_avg:146.94ms
step:949/1370 train_time:137987ms step_avg:146.95ms
step:950/1370 train_time:138141ms step_avg:146.96ms
step:951/1370 train_time:138330ms step_avg:147.00ms
step:952/1370 train_time:138486ms step_avg:147.01ms
step:953/1370 train_time:138640ms step_avg:147.02ms
step:954/1370 train_time:138791ms step_avg:147.02ms
step:955/1370 train_time:138943ms step_avg:147.03ms
step:956/1370 train_time:139097ms step_avg:147.04ms
step:957/1370 train_time:139252ms step_avg:147.05ms
step:958/1370 train_time:139410ms step_avg:147.06ms
step:959/1370 train_time:139566ms step_avg:147.07ms
step:960/1370 train_time:139724ms step_avg:147.08ms
step:961/1370 train_time:139876ms step_avg:147.08ms
step:962/1370 train_time:140030ms step_avg:147.09ms
step:963/1370 train_time:140189ms step_avg:147.10ms
step:964/1370 train_time:140344ms step_avg:147.11ms
step:965/1370 train_time:140498ms step_avg:147.12ms
step:966/1370 train_time:140650ms step_avg:147.12ms
step:967/1370 train_time:140803ms step_avg:147.13ms
step:968/1370 train_time:140955ms step_avg:147.13ms
step:969/1370 train_time:141110ms step_avg:147.14ms
step:970/1370 train_time:141264ms step_avg:147.15ms
step:971/1370 train_time:141422ms step_avg:147.16ms
step:972/1370 train_time:141575ms step_avg:147.17ms
step:973/1370 train_time:141728ms step_avg:147.17ms
step:974/1370 train_time:141882ms step_avg:147.18ms
step:975/1370 train_time:142034ms step_avg:147.19ms
step:976/1370 train_time:142188ms step_avg:147.19ms
step:977/1370 train_time:142346ms step_avg:147.20ms
step:978/1370 train_time:142502ms step_avg:147.21ms
step:979/1370 train_time:142655ms step_avg:147.22ms
step:980/1370 train_time:142807ms step_avg:147.22ms
step:981/1370 train_time:142958ms step_avg:147.23ms
step:982/1370 train_time:143110ms step_avg:147.23ms
step:983/1370 train_time:143264ms step_avg:147.24ms
step:984/1370 train_time:143418ms step_avg:147.25ms
step:985/1370 train_time:143572ms step_avg:147.25ms
step:986/1370 train_time:143727ms step_avg:147.26ms
step:987/1370 train_time:143879ms step_avg:147.27ms
step:988/1370 train_time:144031ms step_avg:147.27ms
step:989/1370 train_time:144183ms step_avg:147.28ms
step:990/1370 train_time:144337ms step_avg:147.28ms
step:991/1370 train_time:144492ms step_avg:147.29ms
step:992/1370 train_time:144652ms step_avg:147.30ms
step:993/1370 train_time:144813ms step_avg:147.32ms
step:994/1370 train_time:144966ms step_avg:147.32ms
step:995/1370 train_time:145122ms step_avg:147.33ms
step:996/1370 train_time:145272ms step_avg:147.33ms
step:997/1370 train_time:145426ms step_avg:147.34ms
step:998/1370 train_time:145580ms step_avg:147.35ms
step:999/1370 train_time:145733ms step_avg:147.35ms
step:1000/1370 train_time:145887ms step_avg:147.36ms
step:1000/1370 val_loss:3.4012 train_time:145958ms step_avg:147.43ms
step:1001/1370 train_time:146043ms step_avg:147.37ms
step:1002/1370 train_time:146197ms step_avg:147.38ms
step:1003/1370 train_time:146352ms step_avg:147.38ms
step:1004/1370 train_time:146506ms step_avg:147.39ms
step:1005/1370 train_time:146659ms step_avg:147.40ms
step:1006/1370 train_time:146811ms step_avg:147.40ms
step:1007/1370 train_time:146969ms step_avg:147.41ms
step:1008/1370 train_time:147126ms step_avg:147.42ms
step:1009/1370 train_time:147287ms step_avg:147.43ms
step:1010/1370 train_time:147441ms step_avg:147.44ms
step:1011/1370 train_time:147593ms step_avg:147.45ms
step:1012/1370 train_time:147746ms step_avg:147.45ms
step:1013/1370 train_time:147901ms step_avg:147.46ms
step:1014/1370 train_time:148056ms step_avg:147.47ms
step:1015/1370 train_time:148210ms step_avg:147.47ms
step:1016/1370 train_time:148366ms step_avg:147.48ms
step:1017/1370 train_time:148523ms step_avg:147.49ms
step:1018/1370 train_time:148676ms step_avg:147.50ms
step:1019/1370 train_time:148832ms step_avg:147.50ms
step:1020/1370 train_time:148989ms step_avg:147.51ms
step:1021/1370 train_time:149145ms step_avg:147.52ms
step:1022/1370 train_time:149300ms step_avg:147.53ms
step:1023/1370 train_time:149456ms step_avg:147.54ms
step:1024/1370 train_time:149611ms step_avg:147.55ms
step:1025/1370 train_time:149767ms step_avg:147.55ms
step:1026/1370 train_time:149920ms step_avg:147.56ms
step:1027/1370 train_time:150074ms step_avg:147.57ms
step:1028/1370 train_time:150231ms step_avg:147.57ms
step:1029/1370 train_time:150388ms step_avg:147.58ms
step:1030/1370 train_time:150545ms step_avg:147.59ms
step:1031/1370 train_time:150696ms step_avg:147.60ms
step:1032/1370 train_time:150848ms step_avg:147.60ms
step:1033/1370 train_time:151004ms step_avg:147.61ms
step:1034/1370 train_time:151159ms step_avg:147.62ms
step:1035/1370 train_time:151317ms step_avg:147.63ms
step:1036/1370 train_time:151472ms step_avg:147.63ms
step:1037/1370 train_time:151629ms step_avg:147.64ms
step:1038/1370 train_time:151783ms step_avg:147.65ms
step:1039/1370 train_time:151936ms step_avg:147.65ms
step:1040/1370 train_time:152090ms step_avg:147.66ms
step:1041/1370 train_time:152246ms step_avg:147.67ms
step:1042/1370 train_time:152398ms step_avg:147.67ms
step:1043/1370 train_time:152553ms step_avg:147.68ms
step:1044/1370 train_time:152709ms step_avg:147.69ms
step:1045/1370 train_time:152866ms step_avg:147.70ms
step:1046/1370 train_time:153018ms step_avg:147.70ms
step:1047/1370 train_time:153174ms step_avg:147.71ms
step:1048/1370 train_time:153331ms step_avg:147.72ms
step:1049/1370 train_time:153490ms step_avg:147.73ms
step:1050/1370 train_time:153648ms step_avg:147.74ms
step:1051/1370 train_time:153804ms step_avg:147.75ms
step:1052/1370 train_time:153960ms step_avg:147.75ms
step:1053/1370 train_time:154114ms step_avg:147.76ms
step:1054/1370 train_time:154270ms step_avg:147.77ms
step:1055/1370 train_time:154425ms step_avg:147.78ms
step:1056/1370 train_time:154583ms step_avg:147.79ms
step:1057/1370 train_time:154740ms step_avg:147.79ms
step:1058/1370 train_time:154896ms step_avg:147.80ms
step:1059/1370 train_time:155052ms step_avg:147.81ms
step:1060/1370 train_time:155208ms step_avg:147.82ms
step:1061/1370 train_time:155363ms step_avg:147.82ms
step:1062/1370 train_time:155519ms step_avg:147.83ms
step:1063/1370 train_time:155673ms step_avg:147.84ms
step:1064/1370 train_time:155827ms step_avg:147.84ms
step:1065/1370 train_time:155984ms step_avg:147.85ms
step:1066/1370 train_time:156142ms step_avg:147.86ms
step:1067/1370 train_time:156296ms step_avg:147.87ms
step:1068/1370 train_time:156451ms step_avg:147.87ms
step:1069/1370 train_time:156613ms step_avg:147.89ms
step:1070/1370 train_time:156767ms step_avg:147.89ms
step:1071/1370 train_time:156923ms step_avg:147.90ms
step:1072/1370 train_time:157076ms step_avg:147.91ms
step:1073/1370 train_time:157229ms step_avg:147.91ms
step:1074/1370 train_time:157384ms step_avg:147.92ms
step:1075/1370 train_time:157541ms step_avg:147.93ms
step:1076/1370 train_time:157695ms step_avg:147.93ms
step:1077/1370 train_time:157848ms step_avg:147.94ms
step:1078/1370 train_time:158008ms step_avg:147.95ms
step:1079/1370 train_time:158165ms step_avg:147.96ms
step:1080/1370 train_time:158321ms step_avg:147.96ms
step:1081/1370 train_time:158473ms step_avg:147.97ms
step:1082/1370 train_time:158628ms step_avg:147.97ms
step:1083/1370 train_time:158785ms step_avg:147.98ms
step:1084/1370 train_time:158943ms step_avg:147.99ms
step:1085/1370 train_time:159096ms step_avg:148.00ms
step:1086/1370 train_time:159254ms step_avg:148.01ms
step:1087/1370 train_time:159411ms step_avg:148.01ms
step:1088/1370 train_time:159568ms step_avg:148.02ms
step:1089/1370 train_time:159728ms step_avg:148.03ms
step:1090/1370 train_time:159888ms step_avg:148.04ms
step:1091/1370 train_time:160046ms step_avg:148.05ms
step:1092/1370 train_time:160200ms step_avg:148.06ms
step:1093/1370 train_time:160355ms step_avg:148.07ms
step:1094/1370 train_time:160509ms step_avg:148.07ms
step:1095/1370 train_time:160666ms step_avg:148.08ms
step:1096/1370 train_time:160827ms step_avg:148.09ms
step:1097/1370 train_time:160982ms step_avg:148.10ms
step:1098/1370 train_time:161136ms step_avg:148.10ms
step:1099/1370 train_time:161290ms step_avg:148.11ms
step:1100/1370 train_time:161444ms step_avg:148.11ms
step:1101/1370 train_time:161597ms step_avg:148.12ms
step:1102/1370 train_time:161753ms step_avg:148.13ms
step:1103/1370 train_time:161909ms step_avg:148.13ms
step:1104/1370 train_time:162066ms step_avg:148.14ms
step:1105/1370 train_time:162223ms step_avg:148.15ms
step:1106/1370 train_time:162377ms step_avg:148.15ms
step:1107/1370 train_time:162531ms step_avg:148.16ms
step:1108/1370 train_time:162689ms step_avg:148.17ms
step:1109/1370 train_time:162844ms step_avg:148.17ms
step:1110/1370 train_time:162999ms step_avg:148.18ms
step:1111/1370 train_time:163158ms step_avg:148.19ms
step:1112/1370 train_time:163313ms step_avg:148.20ms
step:1113/1370 train_time:163468ms step_avg:148.20ms
step:1114/1370 train_time:163625ms step_avg:148.21ms
step:1115/1370 train_time:163780ms step_avg:148.22ms
step:1116/1370 train_time:163933ms step_avg:148.22ms
step:1117/1370 train_time:164092ms step_avg:148.23ms
step:1118/1370 train_time:164253ms step_avg:148.24ms
step:1119/1370 train_time:164408ms step_avg:148.25ms
step:1120/1370 train_time:164567ms step_avg:148.26ms
step:1121/1370 train_time:164723ms step_avg:148.27ms
step:1122/1370 train_time:164876ms step_avg:148.27ms
step:1123/1370 train_time:165032ms step_avg:148.28ms
step:1124/1370 train_time:165193ms step_avg:148.29ms
step:1125/1370 train_time:165349ms step_avg:148.30ms
step:1125/1370 val_loss:3.3482 train_time:165421ms step_avg:148.36ms
step:1126/1370 train_time:165505ms step_avg:148.30ms
step:1127/1370 train_time:165662ms step_avg:148.31ms
step:1128/1370 train_time:165817ms step_avg:148.32ms
step:1129/1370 train_time:165975ms step_avg:148.32ms
step:1130/1370 train_time:166129ms step_avg:148.33ms
step:1131/1370 train_time:166288ms step_avg:148.34ms
step:1132/1370 train_time:166445ms step_avg:148.35ms
step:1133/1370 train_time:166601ms step_avg:148.35ms
step:1134/1370 train_time:166760ms step_avg:148.36ms
step:1135/1370 train_time:166915ms step_avg:148.37ms
step:1136/1370 train_time:167074ms step_avg:148.38ms
step:1137/1370 train_time:167226ms step_avg:148.38ms
step:1138/1370 train_time:167385ms step_avg:148.39ms
step:1139/1370 train_time:167542ms step_avg:148.40ms
step:1140/1370 train_time:167697ms step_avg:148.40ms
step:1141/1370 train_time:167887ms step_avg:148.44ms
step:1142/1370 train_time:168041ms step_avg:148.45ms
step:1143/1370 train_time:168198ms step_avg:148.45ms
step:1144/1370 train_time:168354ms step_avg:148.46ms
step:1145/1370 train_time:168506ms step_avg:148.46ms
step:1146/1370 train_time:168664ms step_avg:148.47ms
step:1147/1370 train_time:168821ms step_avg:148.48ms
step:1148/1370 train_time:168978ms step_avg:148.49ms
step:1149/1370 train_time:169135ms step_avg:148.49ms
step:1150/1370 train_time:169290ms step_avg:148.50ms
step:1151/1370 train_time:169446ms step_avg:148.51ms
step:1152/1370 train_time:169602ms step_avg:148.51ms
step:1153/1370 train_time:169761ms step_avg:148.52ms
step:1154/1370 train_time:169918ms step_avg:148.53ms
step:1155/1370 train_time:170075ms step_avg:148.54ms
step:1156/1370 train_time:170235ms step_avg:148.55ms
step:1157/1370 train_time:170395ms step_avg:148.56ms
step:1158/1370 train_time:170552ms step_avg:148.56ms
step:1159/1370 train_time:170709ms step_avg:148.57ms
step:1160/1370 train_time:170863ms step_avg:148.58ms
step:1161/1370 train_time:171019ms step_avg:148.58ms
step:1162/1370 train_time:171175ms step_avg:148.59ms
step:1163/1370 train_time:171331ms step_avg:148.60ms
step:1164/1370 train_time:171488ms step_avg:148.60ms
step:1165/1370 train_time:171642ms step_avg:148.61ms
step:1166/1370 train_time:171796ms step_avg:148.61ms
step:1167/1370 train_time:171951ms step_avg:148.62ms
step:1168/1370 train_time:172107ms step_avg:148.62ms
step:1169/1370 train_time:172262ms step_avg:148.63ms
step:1170/1370 train_time:172417ms step_avg:148.63ms
step:1171/1370 train_time:172574ms step_avg:148.64ms
step:1172/1370 train_time:172730ms step_avg:148.65ms
step:1173/1370 train_time:172887ms step_avg:148.66ms
step:1174/1370 train_time:173051ms step_avg:148.67ms
step:1175/1370 train_time:173210ms step_avg:148.68ms
step:1176/1370 train_time:173372ms step_avg:148.69ms
step:1177/1370 train_time:173531ms step_avg:148.70ms
step:1178/1370 train_time:173688ms step_avg:148.71ms
step:1179/1370 train_time:173842ms step_avg:148.71ms
step:1180/1370 train_time:174007ms step_avg:148.72ms
step:1181/1370 train_time:174163ms step_avg:148.73ms
step:1182/1370 train_time:174318ms step_avg:148.74ms
step:1183/1370 train_time:174473ms step_avg:148.74ms
step:1184/1370 train_time:174629ms step_avg:148.75ms
step:1185/1370 train_time:174788ms step_avg:148.76ms
step:1186/1370 train_time:174943ms step_avg:148.76ms
step:1187/1370 train_time:175106ms step_avg:148.77ms
step:1188/1370 train_time:175261ms step_avg:148.78ms
step:1189/1370 train_time:175420ms step_avg:148.79ms
step:1190/1370 train_time:175578ms step_avg:148.80ms
step:1191/1370 train_time:175736ms step_avg:148.80ms
step:1192/1370 train_time:175891ms step_avg:148.81ms
step:1193/1370 train_time:176048ms step_avg:148.81ms
step:1194/1370 train_time:176205ms step_avg:148.82ms
step:1195/1370 train_time:176361ms step_avg:148.83ms
step:1196/1370 train_time:176517ms step_avg:148.83ms
step:1197/1370 train_time:176675ms step_avg:148.84ms
step:1198/1370 train_time:176835ms step_avg:148.85ms
step:1199/1370 train_time:176992ms step_avg:148.86ms
step:1200/1370 train_time:177148ms step_avg:148.86ms
step:1201/1370 train_time:177307ms step_avg:148.87ms
step:1202/1370 train_time:177475ms step_avg:148.89ms
step:1203/1370 train_time:177635ms step_avg:148.90ms
step:1204/1370 train_time:177792ms step_avg:148.90ms
step:1205/1370 train_time:177948ms step_avg:148.91ms
step:1206/1370 train_time:178104ms step_avg:148.92ms
step:1207/1370 train_time:178261ms step_avg:148.92ms
step:1208/1370 train_time:178417ms step_avg:148.93ms
step:1209/1370 train_time:178576ms step_avg:148.94ms
step:1210/1370 train_time:178736ms step_avg:148.95ms
step:1211/1370 train_time:178894ms step_avg:148.95ms
step:1212/1370 train_time:179051ms step_avg:148.96ms
step:1213/1370 train_time:179206ms step_avg:148.97ms
step:1214/1370 train_time:179363ms step_avg:148.97ms
step:1215/1370 train_time:179520ms step_avg:148.98ms
step:1216/1370 train_time:179674ms step_avg:148.98ms
step:1217/1370 train_time:179830ms step_avg:148.99ms
step:1218/1370 train_time:179986ms step_avg:149.00ms
step:1219/1370 train_time:180141ms step_avg:149.00ms
step:1220/1370 train_time:180296ms step_avg:149.00ms
step:1221/1370 train_time:180453ms step_avg:149.01ms
step:1222/1370 train_time:180610ms step_avg:149.02ms
step:1223/1370 train_time:180768ms step_avg:149.03ms
step:1224/1370 train_time:180927ms step_avg:149.03ms
step:1225/1370 train_time:181086ms step_avg:149.04ms
step:1226/1370 train_time:181243ms step_avg:149.05ms
step:1227/1370 train_time:181401ms step_avg:149.06ms
step:1228/1370 train_time:181556ms step_avg:149.06ms
step:1229/1370 train_time:181713ms step_avg:149.07ms
step:1230/1370 train_time:181876ms step_avg:149.08ms
step:1231/1370 train_time:182035ms step_avg:149.09ms
step:1232/1370 train_time:182195ms step_avg:149.10ms
step:1233/1370 train_time:182353ms step_avg:149.10ms
step:1234/1370 train_time:182509ms step_avg:149.11ms
step:1235/1370 train_time:182666ms step_avg:149.12ms
step:1236/1370 train_time:182821ms step_avg:149.12ms
step:1237/1370 train_time:182977ms step_avg:149.13ms
step:1238/1370 train_time:183141ms step_avg:149.14ms
step:1239/1370 train_time:183296ms step_avg:149.14ms
step:1240/1370 train_time:183456ms step_avg:149.15ms
step:1241/1370 train_time:183618ms step_avg:149.16ms
step:1242/1370 train_time:183776ms step_avg:149.17ms
step:1243/1370 train_time:183936ms step_avg:149.18ms
step:1244/1370 train_time:184093ms step_avg:149.18ms
step:1245/1370 train_time:184251ms step_avg:149.19ms
step:1246/1370 train_time:184407ms step_avg:149.20ms
step:1247/1370 train_time:184565ms step_avg:149.20ms
step:1248/1370 train_time:184720ms step_avg:149.21ms
step:1249/1370 train_time:184874ms step_avg:149.21ms
step:1250/1370 train_time:185031ms step_avg:149.22ms
step:1250/1370 val_loss:3.3029 train_time:185106ms step_avg:149.28ms
step:1251/1370 train_time:185196ms step_avg:149.23ms
step:1252/1370 train_time:185350ms step_avg:149.24ms
step:1253/1370 train_time:185506ms step_avg:149.24ms
step:1254/1370 train_time:185660ms step_avg:149.24ms
step:1255/1370 train_time:185826ms step_avg:149.26ms
step:1256/1370 train_time:185982ms step_avg:149.26ms
step:1257/1370 train_time:186139ms step_avg:149.27ms
step:1258/1370 train_time:186299ms step_avg:149.28ms
step:1259/1370 train_time:186455ms step_avg:149.28ms
step:1260/1370 train_time:186612ms step_avg:149.29ms
step:1261/1370 train_time:186769ms step_avg:149.30ms
step:1262/1370 train_time:186928ms step_avg:149.30ms
step:1263/1370 train_time:187087ms step_avg:149.31ms
step:1264/1370 train_time:187242ms step_avg:149.32ms
step:1265/1370 train_time:187399ms step_avg:149.32ms
step:1266/1370 train_time:187555ms step_avg:149.33ms
step:1267/1370 train_time:187713ms step_avg:149.33ms
step:1268/1370 train_time:187871ms step_avg:149.34ms
step:1269/1370 train_time:188032ms step_avg:149.35ms
step:1270/1370 train_time:188189ms step_avg:149.36ms
step:1271/1370 train_time:188346ms step_avg:149.36ms
step:1272/1370 train_time:188500ms step_avg:149.37ms
step:1273/1370 train_time:188655ms step_avg:149.37ms
step:1274/1370 train_time:188812ms step_avg:149.38ms
step:1275/1370 train_time:188968ms step_avg:149.38ms
step:1276/1370 train_time:189122ms step_avg:149.39ms
step:1277/1370 train_time:189279ms step_avg:149.39ms
step:1278/1370 train_time:189433ms step_avg:149.40ms
step:1279/1370 train_time:189592ms step_avg:149.40ms
step:1280/1370 train_time:189753ms step_avg:149.41ms
step:1281/1370 train_time:189909ms step_avg:149.42ms
step:1282/1370 train_time:190063ms step_avg:149.42ms
step:1283/1370 train_time:190220ms step_avg:149.43ms
step:1284/1370 train_time:190379ms step_avg:149.43ms
step:1285/1370 train_time:190536ms step_avg:149.44ms
step:1286/1370 train_time:190694ms step_avg:149.45ms
step:1287/1370 train_time:190850ms step_avg:149.45ms
step:1288/1370 train_time:191009ms step_avg:149.46ms
step:1289/1370 train_time:191172ms step_avg:149.47ms
step:1290/1370 train_time:191332ms step_avg:149.48ms
step:1291/1370 train_time:191493ms step_avg:149.49ms
step:1292/1370 train_time:191650ms step_avg:149.49ms
step:1293/1370 train_time:191809ms step_avg:149.50ms
step:1294/1370 train_time:191965ms step_avg:149.51ms
step:1295/1370 train_time:192122ms step_avg:149.51ms
step:1296/1370 train_time:192284ms step_avg:149.52ms
step:1297/1370 train_time:192443ms step_avg:149.53ms
step:1298/1370 train_time:192600ms step_avg:149.53ms
step:1299/1370 train_time:192754ms step_avg:149.54ms
step:1300/1370 train_time:192910ms step_avg:149.54ms
step:1301/1370 train_time:193065ms step_avg:149.55ms
step:1302/1370 train_time:193223ms step_avg:149.55ms
step:1303/1370 train_time:193382ms step_avg:149.56ms
step:1304/1370 train_time:193542ms step_avg:149.57ms
step:1305/1370 train_time:193697ms step_avg:149.57ms
step:1306/1370 train_time:193856ms step_avg:149.58ms
step:1307/1370 train_time:194010ms step_avg:149.58ms
step:1308/1370 train_time:194171ms step_avg:149.59ms
step:1309/1370 train_time:194327ms step_avg:149.60ms
step:1310/1370 train_time:194484ms step_avg:149.60ms
step:1311/1370 train_time:194639ms step_avg:149.61ms
step:1312/1370 train_time:194793ms step_avg:149.61ms
step:1313/1370 train_time:194948ms step_avg:149.61ms
step:1314/1370 train_time:195106ms step_avg:149.62ms
step:1315/1370 train_time:195265ms step_avg:149.63ms
step:1316/1370 train_time:195419ms step_avg:149.63ms
step:1317/1370 train_time:195575ms step_avg:149.64ms
step:1318/1370 train_time:195737ms step_avg:149.65ms
step:1319/1370 train_time:195894ms step_avg:149.65ms
step:1320/1370 train_time:196050ms step_avg:149.66ms
step:1321/1370 train_time:196211ms step_avg:149.67ms
step:1322/1370 train_time:196373ms step_avg:149.67ms
step:1323/1370 train_time:196527ms step_avg:149.68ms
step:1324/1370 train_time:196684ms step_avg:149.68ms
step:1325/1370 train_time:196844ms step_avg:149.69ms
step:1326/1370 train_time:197007ms step_avg:149.70ms
step:1327/1370 train_time:197162ms step_avg:149.71ms
step:1328/1370 train_time:197322ms step_avg:149.71ms
step:1329/1370 train_time:197497ms step_avg:149.73ms
step:1330/1370 train_time:197658ms step_avg:149.74ms
step:1331/1370 train_time:197849ms step_avg:149.77ms
step:1332/1370 train_time:198012ms step_avg:149.78ms
step:1333/1370 train_time:198170ms step_avg:149.79ms
step:1334/1370 train_time:198325ms step_avg:149.79ms
step:1335/1370 train_time:198480ms step_avg:149.80ms
step:1336/1370 train_time:198643ms step_avg:149.81ms
step:1337/1370 train_time:198803ms step_avg:149.81ms
step:1338/1370 train_time:198960ms step_avg:149.82ms
step:1339/1370 train_time:199122ms step_avg:149.83ms
step:1340/1370 train_time:199281ms step_avg:149.84ms
step:1341/1370 train_time:199437ms step_avg:149.84ms
step:1342/1370 train_time:199596ms step_avg:149.85ms
step:1343/1370 train_time:199752ms step_avg:149.85ms
step:1344/1370 train_time:199910ms step_avg:149.86ms
step:1345/1370 train_time:200067ms step_avg:149.86ms
step:1346/1370 train_time:200223ms step_avg:149.87ms
step:1347/1370 train_time:200382ms step_avg:149.87ms
step:1348/1370 train_time:200539ms step_avg:149.88ms
step:1349/1370 train_time:200699ms step_avg:149.89ms
step:1350/1370 train_time:200855ms step_avg:149.89ms
step:1351/1370 train_time:201013ms step_avg:149.90ms
step:1352/1370 train_time:201178ms step_avg:149.91ms
step:1353/1370 train_time:201339ms step_avg:149.92ms
step:1354/1370 train_time:201498ms step_avg:149.92ms
step:1355/1370 train_time:201654ms step_avg:149.93ms
step:1356/1370 train_time:201811ms step_avg:149.93ms
step:1357/1370 train_time:201970ms step_avg:149.94ms
step:1358/1370 train_time:202127ms step_avg:149.95ms
step:1359/1370 train_time:202286ms step_avg:149.95ms
step:1360/1370 train_time:202447ms step_avg:149.96ms
step:1361/1370 train_time:202606ms step_avg:149.97ms
step:1362/1370 train_time:202766ms step_avg:149.97ms
step:1363/1370 train_time:202929ms step_avg:149.98ms
step:1364/1370 train_time:203086ms step_avg:149.99ms
step:1365/1370 train_time:203242ms step_avg:149.99ms
step:1366/1370 train_time:203399ms step_avg:150.00ms
step:1367/1370 train_time:203556ms step_avg:150.00ms
step:1368/1370 train_time:203716ms step_avg:150.01ms
step:1369/1370 train_time:203881ms step_avg:150.02ms
step:1370/1370 train_time:204038ms step_avg:150.03ms
step:1370/1370 val_loss:3.2784 train_time:204111ms step_avg:150.08ms
peak memory consumption: 32619 MiB

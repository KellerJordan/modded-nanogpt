import os
import sys
with open(sys.argv[0]) as f:
    code = f.read() # read the code of this file ASAP, for logging
import uuid
import time
import glob
import subprocess
import contextlib
from dataclasses import dataclass

import torch
torch.empty(1, device='cuda', requires_grad=True).backward()
from torch import nn
import torch.nn.functional as F
import torch.distributed as dist
from torch.nn.parallel import DistributedDataParallel as DDP
# use of FlexAttention contributed by @KoszarskyB
from torch.nn.attention.flex_attention import BlockMask, flex_attention

# -----------------------------------------------------------------------------
# Muon optimizer

@torch.compile
def zeropower_via_newtonschulz5(G, steps):
    """
    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a
    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose
    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at
    zero even beyond the point where the iteration no longer converges all the way to one everywhere
    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T
    where S' is diagonal with S_{ii}' ~ Uniform(0.5, 1.5), which turns out not to hurt model
    performance at all relative to UV^T, where USV^T = G is the SVD.
    """
    assert len(G.shape) == 2
    a, b, c = (3.4445, -4.7750,  2.0315)
    X = G.bfloat16()
    if G.size(0) > G.size(1):
        X = X.T

    # Ensure spectral norm is at most 1
    X = X / (X.norm() + 1e-7)
    # Perform the NS iterations
    for _ in range(steps):
        A = X @ X.T
        B = b * A + c * A @ A # adapted from suggestion by @jxbz, @leloykun, and @YouJiacheng
        X = a * X + B @ X
    
    if G.size(0) > G.size(1):
        X = X.T
    return X

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Some warnings:
    - This optimizer assumes that all parameters passed in are 2D.
    - It should not be used for the embedding layer, the final fully connected layer, or any {0,1}-D
    parameters; those should all be optimized by a standard method (e.g., AdamW).
    - To use it with 4D convolutional filters, it works well to just flatten their last 3 dimensions.
    - We believe it is unlikely to work well for training with small batch size.
    - We believe it may not work well for finetuning pretrained models, but we haven't tested this.
    - We have not yet tried this optimizer for training scenarios larger than NanoGPT (124M).

    Arguments:
        lr: The learning rate used by the internal SGD.
        momentum: The momentum used by the internal SGD.
        nesterov: Whether to use Nesterov-style momentum in the internal SGD. (recommended)
        ns_steps: The number of Newton-Schulz iteration steps to use.
    """
    def __init__(self, params, lr=0.02, momentum=0.95, nesterov=True, ns_steps=5):
        self.world_size = int(os.environ['WORLD_SIZE'])
        self.rank = int(os.environ['RANK'])
        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, ns_steps=ns_steps)
        assert all(isinstance(p, torch.Tensor) for p in params)
        sizes = {p.numel() for p in params}
        param_groups = [dict(params=[p for p in params if p.numel() == size],
                             update_buffer=[torch.empty(size, device='cuda', dtype=torch.bfloat16) for _ in range(self.world_size)])
                        for size in sizes]
        super().__init__(param_groups, defaults)

    def step(self):

        for group in self.param_groups:

            lr = group['lr']
            momentum = group['momentum']
            nesterov = group['nesterov']
            ns_steps = group['ns_steps']
            update_buffers = group['update_buffer']
            # generate weight updates in distributed fashion
            params = group['params']
            handle = None
            params_world = None
            def update_prev():
                if params_world is None:
                    return
                assert handle is not None
                handle.wait()
                for p_world, g_world in zip(params_world, update_buffers):
                    p_world.data.add_(
                        g_world.view_as(p_world),
                        alpha=-lr * max(1, p_world.size(0) / p_world.size(1)) ** 0.5,
                    )
            for base_i in range(len(params))[::self.world_size]:
                if base_i + rank < len(params):
                    p = params[base_i + self.rank]
                    g = p.grad
                    assert g is not None
                    state = self.state[p]
                    if 'momentum_buffer' not in state:
                        state['momentum_buffer'] = torch.zeros_like(g)
                    buf = state['momentum_buffer']
                    buf.lerp_(g, 1 - momentum)
                    g = g.lerp_(buf, momentum) if nesterov else buf
                    g = zeropower_via_newtonschulz5(g, steps=ns_steps).flatten()
                else:
                    g = update_buffers[rank]
                update_prev() # async all_gather instead of sync all_reduce by @YouJiacheng
                handle = dist.all_gather(update_buffers, g, async_op=True)
                params_world = params[base_i : base_i + self.world_size]
            update_prev()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the GPT-2 model

def norm(x):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):

    def __init__(self, in_features, out_features):
        super().__init__(in_features, out_features, bias=False)

    def forward(self, x):
        return F.linear(x, self.weight.type_as(x))

class Rotary(nn.Module):

    def __init__(self, dim, max_seq_len=65536):
        super().__init__()
        # half-truncate RoPE by @YouJiacheng
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=dim//4, dtype=torch.float32)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(dim//4)])
        t = torch.arange(max_seq_len, dtype=torch.float32)
        theta = torch.einsum('i,j -> ij', t, angular_freq)
        self.cos = nn.Buffer(theta.cos(), persistent=False)
        self.sin = nn.Buffer(theta.sin(), persistent=False)

    def forward(self, x):
        cos, sin = self.cos[None, :x.size(-3), None, :], self.sin[None, :x.size(-3), None, :]
        x1, x2 = x.float().chunk(2, dim=-1)
        y1 = x1 * cos + x2 * sin
        y2 = x1 * (-sin) + x2 * cos
        return torch.cat((y1, y2), 3).type_as(x)

class CausalSelfAttention(nn.Module):

    def __init__(self, dim, num_heads):
        super().__init__()
        assert dim % num_heads == 0
        self.num_heads = num_heads
        self.c_q = CastedLinear(dim, dim)
        self.c_k = CastedLinear(dim, dim)
        self.c_v = CastedLinear(dim, dim)
        self.lambdas = nn.Parameter(torch.tensor([0.5, 0.5]))
        self.rotary = Rotary(dim // num_heads) # dim // num_heads = head_dim
        self.c_proj = CastedLinear(dim, dim)
        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977
        self.attn_scale = nn.Parameter(torch.tensor(1.0 / (dim // num_heads) ** 0.5))

    def forward(self, x, ve, block_mask):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, 'Must use batch size = 1 for FlexAttention'
        q = self.c_q(x).view(B, T, self.num_heads, -1)
        k = self.c_k(x).view(B, T, self.num_heads, -1)
        v = self.c_v(x).view(B, T, self.num_heads, -1)
        if ve is not None:
            v = self.lambdas[0] * v + self.lambdas[1] * ve.view_as(v) # @KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = self.lambdas[0] * v
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = self.rotary(q), self.rotary(k)
        y = flex_attention(q.transpose(1, 2) * self.attn_scale, k.transpose(1, 2), v.transpose(1, 2), block_mask=block_mask, scale=1.)
        y = y.transpose(1, 2).contiguous().view_as(x) # re-assemble all head outputs side by side
        y = self.c_proj(y)
        return y

class MLP(nn.Module):

    def __init__(self, dim):
        super().__init__()
        self.c_fc = CastedLinear(dim, 4 * dim)
        self.c_proj = CastedLinear(4 * dim, dim)
        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977

    def forward(self, x):
        x = self.c_fc(x)
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = self.c_proj(x)
        return x

class Block(nn.Module):

    def __init__(self, model_dim, num_heads, use_attn=True):
        super().__init__()
        self.attn = CausalSelfAttention(model_dim, num_heads) if use_attn else None
        self.mlp = MLP(model_dim)
        self.lambdas = nn.Parameter(torch.tensor([1., 0.]))

    def forward(self, x, ve, x0, block_mask):
        x = self.lambdas[0] * x + self.lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), ve, block_mask)
        x = x + self.mlp(norm(x))
        return x

class ValueEmbedding(nn.Module):
    def __init__(self, vocab_size, model_dim):
        super().__init__()
        self.embed = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])

    def forward(self, inputs):
        ve = [emb(inputs).bfloat16() for emb in self.embed]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        ve = [ve[0], ve[1], ve[2], None, None, None, None, None, None, ve[0], ve[1], ve[2]]
        return ve

# -----------------------------------------------------------------------------
# The main GPT-2 model

class GPT(nn.Module):

    def __init__(self, vocab_size, num_layers, num_heads, model_dim):
        super().__init__()
        self.embed = nn.Embedding(vocab_size, model_dim)
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.blocks = nn.ModuleList([Block(model_dim, num_heads, use_attn=(i != 7))
                                     for i in range(num_layers)])
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual learning
        # U-net structure on token value embeddings by @leloykun
        self.value_embeds = ValueEmbedding(vocab_size, model_dim)
        self.lm_head = CastedLinear(model_dim, vocab_size)
        self.lm_head.weight.data.zero_() # @Grad62304977
        # U-net design by @brendanh0gan
        self.num_encoder_layers = num_layers // 2 # Half of the layers for encoder
        self.num_decoder_layers = num_layers - self.num_encoder_layers # Remaining for decoder
        # Add learnable skip connection weights for decoder layers
        self.skip_weights = nn.Parameter(torch.ones(self.num_decoder_layers))

    def forward(self, inputs, targets, sliding_window_num_blocks):
        BLOCK_SIZE = 128
        seq_len = len(inputs)
        assert seq_len % BLOCK_SIZE == 0
        total_num_blocks = seq_len // BLOCK_SIZE
        assert inputs.ndim == 1
        docs = (inputs == 50256).cumsum(0)
        docs_low = docs.view(-1, BLOCK_SIZE)[:, 0].contiguous()
        docs_high = docs.view(-1, BLOCK_SIZE)[:, -1].contiguous()

        def document_causal(b, h, q_idx, kv_idx):
            causal_mask = q_idx >= kv_idx
            document_mask = docs[q_idx] == docs[kv_idx]
            return causal_mask & document_mask

        def dense_to_ordered(dense_mask):
            num_blocks = dense_mask.sum(dim=-1, dtype=torch.int32)
            indices = dense_mask.argsort(dim=-1, descending=True, stable=True).to(torch.int32)
            return num_blocks[None, None].contiguous(), indices[None, None].contiguous()

        def create_doc_swc_block_mask(sliding_window_num_blocks):
            kv_idx = block_idx = torch.arange(total_num_blocks, dtype=torch.int32, device='cuda')
            q_idx = block_idx[:, None]
            causal_bm = q_idx >= kv_idx
            causal_full_bm = q_idx > kv_idx
            window_bm = q_idx - kv_idx < sliding_window_num_blocks
            window_full_bm = window_bm # block-wise sliding window by @YouJiacheng
            # document_bm = (docs_low[q_idx] <= docs_high[kv_idx]) & (docs_low[kv_idx] <= docs_high[q_idx])
            document_bm = (docs_low[:, None] <= docs_high) & (docs_low <= docs_high[:, None])
            document_full_bm = (docs_low[:, None] == docs_high) & (docs_low == docs_high[:, None])
            nonzero_bm = causal_bm & window_bm & document_bm
            full_bm  = causal_full_bm & window_full_bm & document_full_bm
            kv_num_blocks, kv_indices = dense_to_ordered(nonzero_bm & ~full_bm)
            full_kv_num_blocks, full_kv_indices = dense_to_ordered(full_bm)
            return BlockMask.from_kv_blocks(
                kv_num_blocks,
                kv_indices,
                full_kv_num_blocks,
                full_kv_indices,
                BLOCK_SIZE=BLOCK_SIZE,
                mask_mod=document_causal,
            )

        block_mask = create_doc_swc_block_mask(sliding_window_num_blocks)

        x0 = norm(self.embed(inputs[None]).bfloat16()) # use of norm here by @Grad62304977
        x = x0
        ve = self.value_embeds(inputs)
        assert len(ve) == len(self.blocks)
        ve_enc, ve_dec = ve[:self.num_encoder_layers], ve[self.num_encoder_layers:]

        # Store outputs for U-Net skip connections
        skip_connections = []
        # Encoder pass - process only the first half of the blocks
        for i in range(self.num_encoder_layers):
            x = self.blocks[i](x, ve_enc[i], x0, block_mask)
            skip_connections.append(x)
        # Decoder pass - process the remaining blocks with weighted skip connections
        for i in range(self.num_decoder_layers):
            x = x + self.skip_weights[i] * skip_connections.pop()
            # U-net structure on token value embeddings by @leloykun
            x = self.blocks[self.num_encoder_layers + i](x, ve_dec[i], x0, block_mask)

        x = norm(x)
        logits = self.lm_head(x)
        logits = 15 * torch.tanh(logits / 15) # @Grad62304977 added tanh softcapping, @KoszarskyB reduced it from 30 to 15
        logits = logits.float()
        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets)
        return loss

# -----------------------------------------------------------------------------
# Our own simple Distributed Data Loader

def _load_data_shard(path):
    # only reads the header, returns header data
    # header is 256 int32
    header = torch.from_file(path, False, 256, dtype=torch.int32)
    assert header[0] == 20240520, 'magic number mismatch in the data .bin file'
    assert header[1] == 1, 'unsupported version'
    num_tokens = int(header[2]) # number of tokens (claimed)
    with open(path, 'rb', buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, 'number of tokens read does not match header'
    return tokens

class DistributedDataLoader:

    def __init__(self, filename_pattern):
        self.rank = int(os.environ['RANK'])
        self.world_size = int(os.environ['WORLD_SIZE'])
        self.files = sorted(glob.glob(filename_pattern))
        self.reset()

    def reset(self):
        self.current_shard = -1
        self.advance()

    def advance(self):
        self.current_shard = (self.current_shard + 1) % len(self.files)
        self.current_position = 0
        self.tokens = _load_data_shard(self.files[self.current_shard])

    def next_batch(self, batch_size):
        assert batch_size % self.world_size == 0
        device_batch_size = batch_size // self.world_size
        # load next shard if necessary
        if self.current_position + batch_size + 1 >= len(self.tokens):
            self.advance()
        pos = self.current_position + self.rank * device_batch_size
        device_batch_tokens = self.tokens[pos:pos+device_batch_size+1]
        # advance current position
        self.current_position += batch_size
        inputs = device_batch_tokens[:-1].to(device='cuda', dtype=torch.int32, non_blocking=True)
        targets = device_batch_tokens[1:].to(device='cuda', dtype=torch.int64, non_blocking=True)
        return inputs, targets

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files = 'data/fineweb10B/fineweb_train_*.bin' # input .bin to train on
    val_files = 'data/fineweb10B/fineweb_val_*.bin' # input .bin to eval validation loss on
    val_tokens = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # optimization
    batch_size = 8*64*1024 # batch size in tokens
    num_iterations = 1370 # number of iterations to run
    cooldown_frac = 0.4 # fraction of training spent cooling down the learning rate
    bf16_embeds = True
    # evaluation and logging
    val_loss_every = 125 # every how many steps to evaluate val loss? 0 for only at the end
    # implementation
    max_device_batch_size = 64*1024 # batch size per device in tokens
    save_checkpoint = False
args = Hyperparameters()

micro_bs = args.max_device_batch_size

# set up DDP (distributed data parallel). torchrun sets this env variable
rank = int(os.environ['RANK'])
local_rank = int(os.environ['LOCAL_RANK'])
world_size = int(os.environ['WORLD_SIZE'])
assert torch.cuda.is_available()
torch.cuda.set_device(local_rank)
dist.init_process_group(backend='nccl', device_id=torch.device(local_rank))
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = uuid.uuid4()
    os.makedirs('logs', exist_ok=True)
    logfile = f'logs/{run_id}.txt'
    print(logfile)

def print0(s, console=False):
    if master_process:
        with open(logfile, 'a') as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0('='*100)
# log information about the hardware/software environment this is running on
print0(f'Running Python {sys.version}')
print0(f'Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}')
print0(subprocess.run(['nvidia-smi'], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout)
print0('='*100)

# load data
train_loader = DistributedDataLoader(args.train_files)
val_loader = DistributedDataLoader(args.val_files)
print0(f'Training dataloader files: {train_loader.files}')
print0(f'Validation dataloader files: {val_loader.files}')
print0('='*100)

# there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency. suggested to me by @Grad62304977.
# this originates from Karpathy's experiments.
model = GPT(vocab_size=50304, num_layers=12, num_heads=6, model_dim=768)
model = model.cuda()
if args.bf16_embeds:
    for m in model.modules():
        if isinstance(m, nn.Embedding):
            m.bfloat16()
model: GPT = torch.compile(model)
ddp_model = DDP(model, device_ids=[local_rank], broadcast_buffers=False, gradient_as_bucket_view=True)

# collect the parameters to optimize
hidden_matrix_params = [p for p in model.blocks.parameters() if p.ndim == 2]
embed_params = [model.embed.weight, *model.value_embeds.parameters()]
scalar_params = [p for n, p in model.named_parameters() if p.ndim < 2 and "attn_scale" not in n]
attn_scale_params = [p for n, p in model.named_parameters() if p.ndim < 2 and "attn_scale" in n]
head_params = [model.lm_head.weight]

# init the optimizer(s)
optimizer1 = torch.optim.Adam([dict(params=embed_params, lr=0.6),
                               dict(params=head_params, lr=0.008),
                               dict(params=scalar_params, lr=0.04),
                               dict(params=attn_scale_params, lr=0.01)],
                              betas=(0.8, 0.95), fused=True)
optimizer2 = Muon(hidden_matrix_params, lr=0.05, momentum=0.95)
optimizers = [optimizer1, optimizer2]

# learning rate schedule: stable then decay
def get_lr(it):
    t = 1 - it / args.num_iterations # time remaining in training
    assert 1 >= t > 0
    # 1) constant lr for first part of training
    if t >= args.cooldown_frac:
        return 1.0
    # 2) then linear cooldown
    else:
        return t / args.cooldown_frac
schedulers = [torch.optim.lr_scheduler.LambdaLR(opt, get_lr) for opt in optimizers]

# sliding window size schedule: linear increase over training in chunks of 128 from 128 -> 1792. By @fernbear.bsky.social
def get_sliding_window_blocks(it):
    x = it / args.num_iterations # training progress
    assert 0 <= x <= 1
    return int(((1 - x) * 128 + x * 1856) // 128)
sliding_window_num_blocks = torch.tensor(1, dtype=torch.int32, device='cuda')

# Start training loop
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    # This effectively ignores timing first 10 steps, which are slower for weird reasons.
    # Alternately, and slightly more correctly in terms of benchmarking, we could do 10
    # steps with dummy data first, and then re-initialize the model and reset the loader.
    if step == 10:
        training_time_ms = 0
        t0 = time.perf_counter()
    timed_steps = float('nan') if step <= 11 else (step - 10) + 1 # <= 11 to avoid bug in val

    sliding_window_num_blocks.copy_(get_sliding_window_blocks(step))

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        # run validation batches
        model.eval()
        val_loader.reset()
        val_loss = 0.0
        # calculate the number of steps to take in the val loop.
        val_batch_size = world_size * micro_bs
        assert args.val_tokens % val_batch_size == 0
        val_steps = args.val_tokens // val_batch_size
        for _ in range(val_steps):
            with torch.no_grad():
                inputs_val, targets_val = val_loader.next_batch(val_batch_size)
                val_loss += ddp_model(inputs_val, targets_val, sliding_window_num_blocks)
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        val_loss /= val_steps
        # logging
        print0(f'step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/(timed_steps-1):.2f}ms', console=True)
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f'logs/{run_id}', exist_ok=True)
            torch.save(log, f'logs/{run_id}/state_step{step:06d}.pt')
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    model.train()
    batch_size = args.batch_size
    assert batch_size % world_size == 0
    inputs_train, targets_train = train_loader.next_batch(batch_size)
    assert len(inputs_train) <= micro_bs or len(inputs_train) % micro_bs == 0
    for micro_inputs_train, micro_targets_train in zip(inputs_train.split(micro_bs), targets_train.split(micro_bs)):
        ddp_model(micro_inputs_train, micro_targets_train, sliding_window_num_blocks).backward()
    # momentum warmup for Muon
    frac = min(step/300, 1)
    for group in optimizer2.param_groups:
        group['momentum'] = (1 - frac) * 0.85 + frac * 0.95
    # step the optimizers and schedulers
    for opt, sched in zip(optimizers, schedulers):
        opt.step()
        if step != train_steps-1:
            sched.step()
    # null the gradients
    model.zero_grad(set_to_none=True)
    # logging
    approx_time = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f'step:{step+1}/{train_steps} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms', console=True)

print0(f'peak memory consumption: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB')
dist.destroy_process_group()

====================================================================================================
Running Python 3.10.12 (main, Sep 11 2024, 15:47:36) [GCC 11.4.0]
Running PyTorch 2.6.0.dev20241231+cu126 compiled for CUDA 12.6
Sat Jan 11 23:34:42 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.183.06             Driver Version: 535.183.06   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:19:00.0 Off |                    0 |
| N/A   39C    P0             127W / 700W |   7713MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:3B:00.0 Off |                    0 |
| N/A   33C    P0             114W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:4C:00.0 Off |                    0 |
| N/A   30C    P0             117W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   38C    P0             130W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:9B:00.0 Off |                    0 |
| N/A   39C    P0             124W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:BB:00.0 Off |                    0 |
| N/A   31C    P0             115W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:CB:00.0 Off |                    0 |
| N/A   37C    P0             117W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:DB:00.0 Off |                    0 |
| N/A   30C    P0             118W / 700W |   3211MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
Training dataloader files: ['data/fineweb10B/fineweb_train_000001.bin', 'data/fineweb10B/fineweb_train_000002.bin', 'data/fineweb10B/fineweb_train_000003.bin', 'data/fineweb10B/fineweb_train_000004.bin', 'data/fineweb10B/fineweb_train_000005.bin', 'data/fineweb10B/fineweb_train_000006.bin', 'data/fineweb10B/fineweb_train_000007.bin', 'data/fineweb10B/fineweb_train_000008.bin', 'data/fineweb10B/fineweb_train_000009.bin']
Validation dataloader files: ['data/fineweb10B/fineweb_val_000000.bin']
====================================================================================================
step:0/1370 val_loss:10.8258 train_time:0ms step_avg:nanms
step:1/1370 train_time:29530ms step_avg:nanms
step:2/1370 train_time:29610ms step_avg:nanms
step:3/1370 train_time:29791ms step_avg:nanms
step:4/1370 train_time:29925ms step_avg:nanms
step:5/1370 train_time:30059ms step_avg:nanms
step:6/1370 train_time:30195ms step_avg:nanms
step:7/1370 train_time:30330ms step_avg:nanms
step:8/1370 train_time:30463ms step_avg:nanms
step:9/1370 train_time:30598ms step_avg:nanms
step:10/1370 train_time:30740ms step_avg:nanms
step:11/1370 train_time:137ms step_avg:nanms
step:12/1370 train_time:275ms step_avg:nanms
step:13/1370 train_time:409ms step_avg:136.30ms
step:14/1370 train_time:543ms step_avg:135.81ms
step:15/1370 train_time:679ms step_avg:135.72ms
step:16/1370 train_time:815ms step_avg:135.83ms
step:17/1370 train_time:952ms step_avg:136.06ms
step:18/1370 train_time:1089ms step_avg:136.15ms
step:19/1370 train_time:1224ms step_avg:136.04ms
step:20/1370 train_time:1360ms step_avg:136.04ms
step:21/1370 train_time:1497ms step_avg:136.12ms
step:22/1370 train_time:1633ms step_avg:136.08ms
step:23/1370 train_time:1769ms step_avg:136.07ms
step:24/1370 train_time:1906ms step_avg:136.14ms
step:25/1370 train_time:2042ms step_avg:136.13ms
step:26/1370 train_time:2179ms step_avg:136.21ms
step:27/1370 train_time:2317ms step_avg:136.32ms
step:28/1370 train_time:2456ms step_avg:136.43ms
step:29/1370 train_time:2591ms step_avg:136.34ms
step:30/1370 train_time:2725ms step_avg:136.25ms
step:31/1370 train_time:2860ms step_avg:136.21ms
step:32/1370 train_time:2998ms step_avg:136.29ms
step:33/1370 train_time:3133ms step_avg:136.21ms
step:34/1370 train_time:3272ms step_avg:136.33ms
step:35/1370 train_time:3408ms step_avg:136.31ms
step:36/1370 train_time:3544ms step_avg:136.31ms
step:37/1370 train_time:3680ms step_avg:136.30ms
step:38/1370 train_time:3818ms step_avg:136.34ms
step:39/1370 train_time:3955ms step_avg:136.38ms
step:40/1370 train_time:4092ms step_avg:136.41ms
step:41/1370 train_time:4227ms step_avg:136.37ms
step:42/1370 train_time:4363ms step_avg:136.35ms
step:43/1370 train_time:4501ms step_avg:136.40ms
step:44/1370 train_time:4638ms step_avg:136.42ms
step:45/1370 train_time:4774ms step_avg:136.39ms
step:46/1370 train_time:4909ms step_avg:136.37ms
step:47/1370 train_time:5046ms step_avg:136.39ms
step:48/1370 train_time:5182ms step_avg:136.38ms
step:49/1370 train_time:5319ms step_avg:136.38ms
step:50/1370 train_time:5456ms step_avg:136.40ms
step:51/1370 train_time:5593ms step_avg:136.42ms
step:52/1370 train_time:5728ms step_avg:136.37ms
step:53/1370 train_time:5863ms step_avg:136.34ms
step:54/1370 train_time:6000ms step_avg:136.37ms
step:55/1370 train_time:6135ms step_avg:136.34ms
step:56/1370 train_time:6275ms step_avg:136.41ms
step:57/1370 train_time:6412ms step_avg:136.42ms
step:58/1370 train_time:6548ms step_avg:136.42ms
step:59/1370 train_time:6685ms step_avg:136.42ms
step:60/1370 train_time:6820ms step_avg:136.40ms
step:61/1370 train_time:6957ms step_avg:136.40ms
step:62/1370 train_time:7093ms step_avg:136.40ms
step:63/1370 train_time:7230ms step_avg:136.41ms
step:64/1370 train_time:7366ms step_avg:136.41ms
step:65/1370 train_time:7502ms step_avg:136.40ms
step:66/1370 train_time:7639ms step_avg:136.42ms
step:67/1370 train_time:7776ms step_avg:136.43ms
step:68/1370 train_time:7916ms step_avg:136.48ms
step:69/1370 train_time:8053ms step_avg:136.49ms
step:70/1370 train_time:8189ms step_avg:136.48ms
step:71/1370 train_time:8324ms step_avg:136.45ms
step:72/1370 train_time:8460ms step_avg:136.44ms
step:73/1370 train_time:8597ms step_avg:136.46ms
step:74/1370 train_time:8733ms step_avg:136.45ms
step:75/1370 train_time:8872ms step_avg:136.49ms
step:76/1370 train_time:9008ms step_avg:136.49ms
step:77/1370 train_time:9146ms step_avg:136.51ms
step:78/1370 train_time:9282ms step_avg:136.50ms
step:79/1370 train_time:9419ms step_avg:136.51ms
step:80/1370 train_time:9557ms step_avg:136.53ms
step:81/1370 train_time:9694ms step_avg:136.54ms
step:82/1370 train_time:9829ms step_avg:136.52ms
step:83/1370 train_time:9965ms step_avg:136.51ms
step:84/1370 train_time:10102ms step_avg:136.52ms
step:85/1370 train_time:10239ms step_avg:136.52ms
step:86/1370 train_time:10378ms step_avg:136.55ms
step:87/1370 train_time:10514ms step_avg:136.55ms
step:88/1370 train_time:10653ms step_avg:136.58ms
step:89/1370 train_time:10789ms step_avg:136.57ms
step:90/1370 train_time:10924ms step_avg:136.56ms
step:91/1370 train_time:11060ms step_avg:136.54ms
step:92/1370 train_time:11198ms step_avg:136.56ms
step:93/1370 train_time:11335ms step_avg:136.57ms
step:94/1370 train_time:11473ms step_avg:136.58ms
step:95/1370 train_time:11611ms step_avg:136.60ms
step:96/1370 train_time:11747ms step_avg:136.59ms
step:97/1370 train_time:11883ms step_avg:136.59ms
step:98/1370 train_time:12020ms step_avg:136.60ms
step:99/1370 train_time:12159ms step_avg:136.62ms
step:100/1370 train_time:12297ms step_avg:136.64ms
step:101/1370 train_time:12434ms step_avg:136.64ms
step:102/1370 train_time:12572ms step_avg:136.65ms
step:103/1370 train_time:12710ms step_avg:136.67ms
step:104/1370 train_time:12851ms step_avg:136.71ms
step:105/1370 train_time:12991ms step_avg:136.74ms
step:106/1370 train_time:13129ms step_avg:136.76ms
step:107/1370 train_time:13270ms step_avg:136.81ms
step:108/1370 train_time:13411ms step_avg:136.85ms
step:109/1370 train_time:13552ms step_avg:136.89ms
step:110/1370 train_time:13692ms step_avg:136.92ms
step:111/1370 train_time:13832ms step_avg:136.95ms
step:112/1370 train_time:13972ms step_avg:136.98ms
step:113/1370 train_time:14110ms step_avg:136.99ms
step:114/1370 train_time:14250ms step_avg:137.02ms
step:115/1370 train_time:14391ms step_avg:137.05ms
step:116/1370 train_time:14529ms step_avg:137.07ms
step:117/1370 train_time:14669ms step_avg:137.10ms
step:118/1370 train_time:14809ms step_avg:137.12ms
step:119/1370 train_time:14950ms step_avg:137.16ms
step:120/1370 train_time:15091ms step_avg:137.19ms
step:121/1370 train_time:15230ms step_avg:137.20ms
step:122/1370 train_time:15370ms step_avg:137.24ms
step:123/1370 train_time:15511ms step_avg:137.26ms
step:124/1370 train_time:15652ms step_avg:137.30ms
step:125/1370 train_time:15793ms step_avg:137.33ms
step:125/1370 val_loss:4.3795 train_time:15856ms step_avg:137.88ms
step:126/1370 train_time:15936ms step_avg:137.38ms
step:127/1370 train_time:16077ms step_avg:137.41ms
step:128/1370 train_time:16217ms step_avg:137.43ms
step:129/1370 train_time:16354ms step_avg:137.43ms
step:130/1370 train_time:16494ms step_avg:137.45ms
step:131/1370 train_time:16632ms step_avg:137.45ms
step:132/1370 train_time:16774ms step_avg:137.49ms
step:133/1370 train_time:16916ms step_avg:137.53ms
step:134/1370 train_time:17056ms step_avg:137.55ms
step:135/1370 train_time:17197ms step_avg:137.57ms
step:136/1370 train_time:17337ms step_avg:137.59ms
step:137/1370 train_time:17477ms step_avg:137.61ms
step:138/1370 train_time:17617ms step_avg:137.63ms
step:139/1370 train_time:17755ms step_avg:137.64ms
step:140/1370 train_time:17896ms step_avg:137.66ms
step:141/1370 train_time:18038ms step_avg:137.69ms
step:142/1370 train_time:18178ms step_avg:137.71ms
step:143/1370 train_time:18319ms step_avg:137.74ms
step:144/1370 train_time:18458ms step_avg:137.74ms
step:145/1370 train_time:18599ms step_avg:137.77ms
step:146/1370 train_time:18740ms step_avg:137.79ms
step:147/1370 train_time:18880ms step_avg:137.81ms
step:148/1370 train_time:19021ms step_avg:137.83ms
step:149/1370 train_time:19162ms step_avg:137.86ms
step:150/1370 train_time:19303ms step_avg:137.88ms
step:151/1370 train_time:19443ms step_avg:137.90ms
step:152/1370 train_time:19583ms step_avg:137.91ms
step:153/1370 train_time:19723ms step_avg:137.92ms
step:154/1370 train_time:19865ms step_avg:137.95ms
step:155/1370 train_time:20006ms step_avg:137.97ms
step:156/1370 train_time:20147ms step_avg:137.99ms
step:157/1370 train_time:20289ms step_avg:138.02ms
step:158/1370 train_time:20430ms step_avg:138.04ms
step:159/1370 train_time:20572ms step_avg:138.06ms
step:160/1370 train_time:20713ms step_avg:138.08ms
step:161/1370 train_time:20852ms step_avg:138.09ms
step:162/1370 train_time:20993ms step_avg:138.11ms
step:163/1370 train_time:21134ms step_avg:138.13ms
step:164/1370 train_time:21275ms step_avg:138.15ms
step:165/1370 train_time:21416ms step_avg:138.17ms
step:166/1370 train_time:21554ms step_avg:138.17ms
step:167/1370 train_time:21696ms step_avg:138.19ms
step:168/1370 train_time:21836ms step_avg:138.20ms
step:169/1370 train_time:21978ms step_avg:138.23ms
step:170/1370 train_time:22120ms step_avg:138.25ms
step:171/1370 train_time:22260ms step_avg:138.26ms
step:172/1370 train_time:22402ms step_avg:138.28ms
step:173/1370 train_time:22541ms step_avg:138.29ms
step:174/1370 train_time:22683ms step_avg:138.31ms
step:175/1370 train_time:22824ms step_avg:138.33ms
step:176/1370 train_time:22964ms step_avg:138.34ms
step:177/1370 train_time:23104ms step_avg:138.35ms
step:178/1370 train_time:23244ms step_avg:138.36ms
step:179/1370 train_time:23384ms step_avg:138.37ms
step:180/1370 train_time:23524ms step_avg:138.38ms
step:181/1370 train_time:23665ms step_avg:138.39ms
step:182/1370 train_time:23806ms step_avg:138.41ms
step:183/1370 train_time:23948ms step_avg:138.43ms
step:184/1370 train_time:24089ms step_avg:138.44ms
step:185/1370 train_time:24231ms step_avg:138.46ms
step:186/1370 train_time:24372ms step_avg:138.48ms
step:187/1370 train_time:24512ms step_avg:138.49ms
step:188/1370 train_time:24652ms step_avg:138.49ms
step:189/1370 train_time:24794ms step_avg:138.51ms
step:190/1370 train_time:24934ms step_avg:138.52ms
step:191/1370 train_time:25113ms step_avg:138.75ms
step:192/1370 train_time:25251ms step_avg:138.74ms
step:193/1370 train_time:25390ms step_avg:138.75ms
step:194/1370 train_time:25530ms step_avg:138.75ms
step:195/1370 train_time:25669ms step_avg:138.75ms
step:196/1370 train_time:25807ms step_avg:138.75ms
step:197/1370 train_time:25948ms step_avg:138.76ms
step:198/1370 train_time:26095ms step_avg:138.80ms
step:199/1370 train_time:26236ms step_avg:138.82ms
step:200/1370 train_time:26376ms step_avg:138.82ms
step:201/1370 train_time:26514ms step_avg:138.82ms
step:202/1370 train_time:26654ms step_avg:138.82ms
step:203/1370 train_time:26793ms step_avg:138.82ms
step:204/1370 train_time:26936ms step_avg:138.85ms
step:205/1370 train_time:27079ms step_avg:138.87ms
step:206/1370 train_time:27224ms step_avg:138.90ms
step:207/1370 train_time:27368ms step_avg:138.92ms
step:208/1370 train_time:27511ms step_avg:138.94ms
step:209/1370 train_time:27652ms step_avg:138.95ms
step:210/1370 train_time:27794ms step_avg:138.97ms
step:211/1370 train_time:27938ms step_avg:138.99ms
step:212/1370 train_time:28081ms step_avg:139.01ms
step:213/1370 train_time:28224ms step_avg:139.04ms
step:214/1370 train_time:28367ms step_avg:139.05ms
step:215/1370 train_time:28511ms step_avg:139.08ms
step:216/1370 train_time:28651ms step_avg:139.08ms
step:217/1370 train_time:28794ms step_avg:139.10ms
step:218/1370 train_time:28938ms step_avg:139.12ms
step:219/1370 train_time:29085ms step_avg:139.16ms
step:220/1370 train_time:29229ms step_avg:139.18ms
step:221/1370 train_time:29373ms step_avg:139.21ms
step:222/1370 train_time:29516ms step_avg:139.23ms
step:223/1370 train_time:29658ms step_avg:139.24ms
step:224/1370 train_time:29801ms step_avg:139.26ms
step:225/1370 train_time:29945ms step_avg:139.28ms
step:226/1370 train_time:30088ms step_avg:139.30ms
step:227/1370 train_time:30230ms step_avg:139.31ms
step:228/1370 train_time:30374ms step_avg:139.33ms
step:229/1370 train_time:30517ms step_avg:139.35ms
step:230/1370 train_time:30658ms step_avg:139.35ms
step:231/1370 train_time:30801ms step_avg:139.37ms
step:232/1370 train_time:30945ms step_avg:139.39ms
step:233/1370 train_time:31089ms step_avg:139.41ms
step:234/1370 train_time:31232ms step_avg:139.43ms
step:235/1370 train_time:31375ms step_avg:139.45ms
step:236/1370 train_time:31517ms step_avg:139.46ms
step:237/1370 train_time:31659ms step_avg:139.47ms
step:238/1370 train_time:31803ms step_avg:139.49ms
step:239/1370 train_time:31948ms step_avg:139.51ms
step:240/1370 train_time:32091ms step_avg:139.53ms
step:241/1370 train_time:32234ms step_avg:139.54ms
step:242/1370 train_time:32377ms step_avg:139.56ms
step:243/1370 train_time:32520ms step_avg:139.57ms
step:244/1370 train_time:32662ms step_avg:139.58ms
step:245/1370 train_time:32805ms step_avg:139.60ms
step:246/1370 train_time:32948ms step_avg:139.61ms
step:247/1370 train_time:33092ms step_avg:139.63ms
step:248/1370 train_time:33237ms step_avg:139.65ms
step:249/1370 train_time:33380ms step_avg:139.66ms
step:250/1370 train_time:33522ms step_avg:139.67ms
step:250/1370 val_loss:3.9534 train_time:33587ms step_avg:139.95ms
step:251/1370 train_time:33667ms step_avg:139.70ms
step:252/1370 train_time:33811ms step_avg:139.72ms
step:253/1370 train_time:33953ms step_avg:139.72ms
step:254/1370 train_time:34094ms step_avg:139.73ms
step:255/1370 train_time:34235ms step_avg:139.74ms
step:256/1370 train_time:34375ms step_avg:139.74ms
step:257/1370 train_time:34519ms step_avg:139.75ms
step:258/1370 train_time:34661ms step_avg:139.76ms
step:259/1370 train_time:34807ms step_avg:139.79ms
step:260/1370 train_time:34950ms step_avg:139.80ms
step:261/1370 train_time:35093ms step_avg:139.81ms
step:262/1370 train_time:35236ms step_avg:139.83ms
step:263/1370 train_time:35377ms step_avg:139.83ms
step:264/1370 train_time:35520ms step_avg:139.84ms
step:265/1370 train_time:35663ms step_avg:139.86ms
step:266/1370 train_time:35809ms step_avg:139.88ms
step:267/1370 train_time:35952ms step_avg:139.89ms
step:268/1370 train_time:36096ms step_avg:139.91ms
step:269/1370 train_time:36239ms step_avg:139.92ms
step:270/1370 train_time:36382ms step_avg:139.93ms
step:271/1370 train_time:36525ms step_avg:139.94ms
step:272/1370 train_time:36670ms step_avg:139.96ms
step:273/1370 train_time:36813ms step_avg:139.97ms
step:274/1370 train_time:36955ms step_avg:139.98ms
step:275/1370 train_time:37099ms step_avg:139.99ms
step:276/1370 train_time:37241ms step_avg:140.00ms
step:277/1370 train_time:37385ms step_avg:140.02ms
step:278/1370 train_time:37529ms step_avg:140.03ms
step:279/1370 train_time:37671ms step_avg:140.04ms
step:280/1370 train_time:37814ms step_avg:140.05ms
step:281/1370 train_time:37957ms step_avg:140.06ms
step:282/1370 train_time:38101ms step_avg:140.08ms
step:283/1370 train_time:38245ms step_avg:140.09ms
step:284/1370 train_time:38389ms step_avg:140.10ms
step:285/1370 train_time:38530ms step_avg:140.11ms
step:286/1370 train_time:38673ms step_avg:140.12ms
step:287/1370 train_time:38816ms step_avg:140.13ms
step:288/1370 train_time:38959ms step_avg:140.14ms
step:289/1370 train_time:39102ms step_avg:140.15ms
step:290/1370 train_time:39248ms step_avg:140.17ms
step:291/1370 train_time:39392ms step_avg:140.18ms
step:292/1370 train_time:39534ms step_avg:140.19ms
step:293/1370 train_time:39677ms step_avg:140.20ms
step:294/1370 train_time:39819ms step_avg:140.21ms
step:295/1370 train_time:39960ms step_avg:140.21ms
step:296/1370 train_time:40107ms step_avg:140.23ms
step:297/1370 train_time:40249ms step_avg:140.24ms
step:298/1370 train_time:40393ms step_avg:140.25ms
step:299/1370 train_time:40535ms step_avg:140.26ms
step:300/1370 train_time:40678ms step_avg:140.27ms
step:301/1370 train_time:40821ms step_avg:140.28ms
step:302/1370 train_time:40964ms step_avg:140.29ms
step:303/1370 train_time:41108ms step_avg:140.30ms
step:304/1370 train_time:41250ms step_avg:140.31ms
step:305/1370 train_time:41393ms step_avg:140.31ms
step:306/1370 train_time:41538ms step_avg:140.33ms
step:307/1370 train_time:41682ms step_avg:140.34ms
step:308/1370 train_time:41827ms step_avg:140.36ms
step:309/1370 train_time:41973ms step_avg:140.38ms
step:310/1370 train_time:42118ms step_avg:140.39ms
step:311/1370 train_time:42262ms step_avg:140.40ms
step:312/1370 train_time:42409ms step_avg:140.43ms
step:313/1370 train_time:42553ms step_avg:140.44ms
step:314/1370 train_time:42699ms step_avg:140.46ms
step:315/1370 train_time:42844ms step_avg:140.47ms
step:316/1370 train_time:42990ms step_avg:140.49ms
step:317/1370 train_time:43136ms step_avg:140.51ms
step:318/1370 train_time:43280ms step_avg:140.52ms
step:319/1370 train_time:43426ms step_avg:140.54ms
step:320/1370 train_time:43571ms step_avg:140.55ms
step:321/1370 train_time:43717ms step_avg:140.57ms
step:322/1370 train_time:43861ms step_avg:140.58ms
step:323/1370 train_time:44008ms step_avg:140.60ms
step:324/1370 train_time:44152ms step_avg:140.61ms
step:325/1370 train_time:44298ms step_avg:140.63ms
step:326/1370 train_time:44444ms step_avg:140.65ms
step:327/1370 train_time:44590ms step_avg:140.66ms
step:328/1370 train_time:44736ms step_avg:140.68ms
step:329/1370 train_time:44880ms step_avg:140.69ms
step:330/1370 train_time:45025ms step_avg:140.70ms
step:331/1370 train_time:45171ms step_avg:140.72ms
step:332/1370 train_time:45317ms step_avg:140.74ms
step:333/1370 train_time:45459ms step_avg:140.74ms
step:334/1370 train_time:45605ms step_avg:140.76ms
step:335/1370 train_time:45750ms step_avg:140.77ms
step:336/1370 train_time:45896ms step_avg:140.78ms
step:337/1370 train_time:46039ms step_avg:140.79ms
step:338/1370 train_time:46185ms step_avg:140.81ms
step:339/1370 train_time:46332ms step_avg:140.83ms
step:340/1370 train_time:46478ms step_avg:140.84ms
step:341/1370 train_time:46623ms step_avg:140.85ms
step:342/1370 train_time:46768ms step_avg:140.87ms
step:343/1370 train_time:46913ms step_avg:140.88ms
step:344/1370 train_time:47058ms step_avg:140.89ms
step:345/1370 train_time:47205ms step_avg:140.91ms
step:346/1370 train_time:47350ms step_avg:140.92ms
step:347/1370 train_time:47495ms step_avg:140.94ms
step:348/1370 train_time:47640ms step_avg:140.95ms
step:349/1370 train_time:47786ms step_avg:140.96ms
step:350/1370 train_time:47931ms step_avg:140.97ms
step:351/1370 train_time:48075ms step_avg:140.98ms
step:352/1370 train_time:48220ms step_avg:140.99ms
step:353/1370 train_time:48365ms step_avg:141.01ms
step:354/1370 train_time:48511ms step_avg:141.02ms
step:355/1370 train_time:48655ms step_avg:141.03ms
step:356/1370 train_time:48800ms step_avg:141.04ms
step:357/1370 train_time:48942ms step_avg:141.04ms
step:358/1370 train_time:49090ms step_avg:141.06ms
step:359/1370 train_time:49235ms step_avg:141.07ms
step:360/1370 train_time:49380ms step_avg:141.09ms
step:361/1370 train_time:49526ms step_avg:141.10ms
step:362/1370 train_time:49672ms step_avg:141.11ms
step:363/1370 train_time:49817ms step_avg:141.13ms
step:364/1370 train_time:49960ms step_avg:141.13ms
step:365/1370 train_time:50107ms step_avg:141.15ms
step:366/1370 train_time:50253ms step_avg:141.16ms
step:367/1370 train_time:50398ms step_avg:141.17ms
step:368/1370 train_time:50543ms step_avg:141.18ms
step:369/1370 train_time:50689ms step_avg:141.20ms
step:370/1370 train_time:50836ms step_avg:141.21ms
step:371/1370 train_time:50980ms step_avg:141.22ms
step:372/1370 train_time:51124ms step_avg:141.23ms
step:373/1370 train_time:51270ms step_avg:141.24ms
step:374/1370 train_time:51415ms step_avg:141.25ms
step:375/1370 train_time:51559ms step_avg:141.26ms
step:375/1370 val_loss:3.7732 train_time:51626ms step_avg:141.44ms
step:376/1370 train_time:51707ms step_avg:141.28ms
step:377/1370 train_time:51853ms step_avg:141.29ms
step:378/1370 train_time:51999ms step_avg:141.30ms
step:379/1370 train_time:52141ms step_avg:141.30ms
step:380/1370 train_time:52286ms step_avg:141.31ms
step:381/1370 train_time:52469ms step_avg:141.43ms
step:382/1370 train_time:52613ms step_avg:141.43ms
step:383/1370 train_time:52758ms step_avg:141.44ms
step:384/1370 train_time:52902ms step_avg:141.45ms
step:385/1370 train_time:53045ms step_avg:141.45ms
step:386/1370 train_time:53189ms step_avg:141.46ms
step:387/1370 train_time:53338ms step_avg:141.48ms
step:388/1370 train_time:53486ms step_avg:141.50ms
step:389/1370 train_time:53631ms step_avg:141.51ms
step:390/1370 train_time:53777ms step_avg:141.52ms
step:391/1370 train_time:53923ms step_avg:141.53ms
step:392/1370 train_time:54066ms step_avg:141.53ms
step:393/1370 train_time:54211ms step_avg:141.54ms
step:394/1370 train_time:54357ms step_avg:141.55ms
step:395/1370 train_time:54503ms step_avg:141.57ms
step:396/1370 train_time:54647ms step_avg:141.57ms
step:397/1370 train_time:54793ms step_avg:141.58ms
step:398/1370 train_time:54938ms step_avg:141.59ms
step:399/1370 train_time:55083ms step_avg:141.60ms
step:400/1370 train_time:55228ms step_avg:141.61ms
step:401/1370 train_time:55375ms step_avg:141.62ms
step:402/1370 train_time:55521ms step_avg:141.64ms
step:403/1370 train_time:55666ms step_avg:141.64ms
step:404/1370 train_time:55813ms step_avg:141.66ms
step:405/1370 train_time:55958ms step_avg:141.67ms
step:406/1370 train_time:56104ms step_avg:141.68ms
step:407/1370 train_time:56250ms step_avg:141.69ms
step:408/1370 train_time:56398ms step_avg:141.70ms
step:409/1370 train_time:56544ms step_avg:141.71ms
step:410/1370 train_time:56692ms step_avg:141.73ms
step:411/1370 train_time:56839ms step_avg:141.74ms
step:412/1370 train_time:56985ms step_avg:141.75ms
step:413/1370 train_time:57131ms step_avg:141.76ms
step:414/1370 train_time:57278ms step_avg:141.78ms
step:415/1370 train_time:57425ms step_avg:141.79ms
step:416/1370 train_time:57574ms step_avg:141.81ms
step:417/1370 train_time:57722ms step_avg:141.82ms
step:418/1370 train_time:57867ms step_avg:141.83ms
step:419/1370 train_time:58015ms step_avg:141.85ms
step:420/1370 train_time:58163ms step_avg:141.86ms
step:421/1370 train_time:58309ms step_avg:141.87ms
step:422/1370 train_time:58457ms step_avg:141.89ms
step:423/1370 train_time:58604ms step_avg:141.90ms
step:424/1370 train_time:58751ms step_avg:141.91ms
step:425/1370 train_time:58900ms step_avg:141.93ms
step:426/1370 train_time:59046ms step_avg:141.94ms
step:427/1370 train_time:59194ms step_avg:141.95ms
step:428/1370 train_time:59339ms step_avg:141.96ms
step:429/1370 train_time:59486ms step_avg:141.97ms
step:430/1370 train_time:59635ms step_avg:141.99ms
step:431/1370 train_time:59784ms step_avg:142.00ms
step:432/1370 train_time:59931ms step_avg:142.02ms
step:433/1370 train_time:60078ms step_avg:142.03ms
step:434/1370 train_time:60226ms step_avg:142.04ms
step:435/1370 train_time:60374ms step_avg:142.06ms
step:436/1370 train_time:60521ms step_avg:142.07ms
step:437/1370 train_time:60667ms step_avg:142.08ms
step:438/1370 train_time:60815ms step_avg:142.09ms
step:439/1370 train_time:60962ms step_avg:142.10ms
step:440/1370 train_time:61108ms step_avg:142.11ms
step:441/1370 train_time:61256ms step_avg:142.12ms
step:442/1370 train_time:61404ms step_avg:142.14ms
step:443/1370 train_time:61551ms step_avg:142.15ms
step:444/1370 train_time:61699ms step_avg:142.16ms
step:445/1370 train_time:61845ms step_avg:142.17ms
step:446/1370 train_time:61993ms step_avg:142.19ms
step:447/1370 train_time:62139ms step_avg:142.19ms
step:448/1370 train_time:62287ms step_avg:142.21ms
step:449/1370 train_time:62436ms step_avg:142.22ms
step:450/1370 train_time:62583ms step_avg:142.24ms
step:451/1370 train_time:62731ms step_avg:142.25ms
step:452/1370 train_time:62878ms step_avg:142.26ms
step:453/1370 train_time:63025ms step_avg:142.27ms
step:454/1370 train_time:63173ms step_avg:142.28ms
step:455/1370 train_time:63321ms step_avg:142.30ms
step:456/1370 train_time:63466ms step_avg:142.30ms
step:457/1370 train_time:63614ms step_avg:142.31ms
step:458/1370 train_time:63762ms step_avg:142.33ms
step:459/1370 train_time:63907ms step_avg:142.33ms
step:460/1370 train_time:64055ms step_avg:142.34ms
step:461/1370 train_time:64203ms step_avg:142.36ms
step:462/1370 train_time:64348ms step_avg:142.36ms
step:463/1370 train_time:64497ms step_avg:142.38ms
step:464/1370 train_time:64642ms step_avg:142.38ms
step:465/1370 train_time:64791ms step_avg:142.40ms
step:466/1370 train_time:64939ms step_avg:142.41ms
step:467/1370 train_time:65086ms step_avg:142.42ms
step:468/1370 train_time:65234ms step_avg:142.43ms
step:469/1370 train_time:65380ms step_avg:142.44ms
step:470/1370 train_time:65525ms step_avg:142.45ms
step:471/1370 train_time:65673ms step_avg:142.46ms
step:472/1370 train_time:65820ms step_avg:142.47ms
step:473/1370 train_time:65967ms step_avg:142.48ms
step:474/1370 train_time:66116ms step_avg:142.49ms
step:475/1370 train_time:66263ms step_avg:142.50ms
step:476/1370 train_time:66408ms step_avg:142.51ms
step:477/1370 train_time:66555ms step_avg:142.52ms
step:478/1370 train_time:66703ms step_avg:142.53ms
step:479/1370 train_time:66849ms step_avg:142.53ms
step:480/1370 train_time:66998ms step_avg:142.55ms
step:481/1370 train_time:67144ms step_avg:142.56ms
step:482/1370 train_time:67292ms step_avg:142.57ms
step:483/1370 train_time:67438ms step_avg:142.57ms
step:484/1370 train_time:67586ms step_avg:142.59ms
step:485/1370 train_time:67733ms step_avg:142.59ms
step:486/1370 train_time:67880ms step_avg:142.61ms
step:487/1370 train_time:68027ms step_avg:142.62ms
step:488/1370 train_time:68176ms step_avg:142.63ms
step:489/1370 train_time:68323ms step_avg:142.64ms
step:490/1370 train_time:68469ms step_avg:142.64ms
step:491/1370 train_time:68617ms step_avg:142.66ms
step:492/1370 train_time:68764ms step_avg:142.66ms
step:493/1370 train_time:68912ms step_avg:142.67ms
step:494/1370 train_time:69059ms step_avg:142.68ms
step:495/1370 train_time:69207ms step_avg:142.69ms
step:496/1370 train_time:69354ms step_avg:142.70ms
step:497/1370 train_time:69501ms step_avg:142.71ms
step:498/1370 train_time:69645ms step_avg:142.72ms
step:499/1370 train_time:69795ms step_avg:142.73ms
step:500/1370 train_time:69941ms step_avg:142.74ms
step:500/1370 val_loss:3.6560 train_time:70009ms step_avg:142.88ms
step:501/1370 train_time:70090ms step_avg:142.75ms
step:502/1370 train_time:70237ms step_avg:142.76ms
step:503/1370 train_time:70383ms step_avg:142.77ms
step:504/1370 train_time:70529ms step_avg:142.77ms
step:505/1370 train_time:70675ms step_avg:142.78ms
step:506/1370 train_time:70819ms step_avg:142.78ms
step:507/1370 train_time:70970ms step_avg:142.80ms
step:508/1370 train_time:71119ms step_avg:142.81ms
step:509/1370 train_time:71269ms step_avg:142.82ms
step:510/1370 train_time:71416ms step_avg:142.83ms
step:511/1370 train_time:71566ms step_avg:142.85ms
step:512/1370 train_time:71714ms step_avg:142.86ms
step:513/1370 train_time:71864ms step_avg:142.87ms
step:514/1370 train_time:72014ms step_avg:142.88ms
step:515/1370 train_time:72162ms step_avg:142.90ms
step:516/1370 train_time:72312ms step_avg:142.91ms
step:517/1370 train_time:72460ms step_avg:142.92ms
step:518/1370 train_time:72610ms step_avg:142.93ms
step:519/1370 train_time:72757ms step_avg:142.94ms
step:520/1370 train_time:72906ms step_avg:142.95ms
step:521/1370 train_time:73054ms step_avg:142.96ms
step:522/1370 train_time:73202ms step_avg:142.97ms
step:523/1370 train_time:73352ms step_avg:142.99ms
step:524/1370 train_time:73501ms step_avg:143.00ms
step:525/1370 train_time:73649ms step_avg:143.01ms
step:526/1370 train_time:73797ms step_avg:143.02ms
step:527/1370 train_time:73948ms step_avg:143.03ms
step:528/1370 train_time:74096ms step_avg:143.04ms
step:529/1370 train_time:74244ms step_avg:143.05ms
step:530/1370 train_time:74394ms step_avg:143.07ms
step:531/1370 train_time:74544ms step_avg:143.08ms
step:532/1370 train_time:74693ms step_avg:143.09ms
step:533/1370 train_time:74841ms step_avg:143.10ms
step:534/1370 train_time:74989ms step_avg:143.11ms
step:535/1370 train_time:75137ms step_avg:143.12ms
step:536/1370 train_time:75286ms step_avg:143.13ms
step:537/1370 train_time:75435ms step_avg:143.14ms
step:538/1370 train_time:75583ms step_avg:143.15ms
step:539/1370 train_time:75733ms step_avg:143.16ms
step:540/1370 train_time:75880ms step_avg:143.17ms
step:541/1370 train_time:76030ms step_avg:143.18ms
step:542/1370 train_time:76178ms step_avg:143.19ms
step:543/1370 train_time:76326ms step_avg:143.20ms
step:544/1370 train_time:76474ms step_avg:143.21ms
step:545/1370 train_time:76622ms step_avg:143.22ms
step:546/1370 train_time:76771ms step_avg:143.23ms
step:547/1370 train_time:76919ms step_avg:143.24ms
step:548/1370 train_time:77071ms step_avg:143.25ms
step:549/1370 train_time:77218ms step_avg:143.26ms
step:550/1370 train_time:77369ms step_avg:143.28ms
step:551/1370 train_time:77516ms step_avg:143.28ms
step:552/1370 train_time:77666ms step_avg:143.30ms
step:553/1370 train_time:77814ms step_avg:143.30ms
step:554/1370 train_time:77963ms step_avg:143.31ms
step:555/1370 train_time:78112ms step_avg:143.33ms
step:556/1370 train_time:78260ms step_avg:143.33ms
step:557/1370 train_time:78410ms step_avg:143.35ms
step:558/1370 train_time:78559ms step_avg:143.36ms
step:559/1370 train_time:78708ms step_avg:143.37ms
step:560/1370 train_time:78857ms step_avg:143.38ms
step:561/1370 train_time:79006ms step_avg:143.39ms
step:562/1370 train_time:79156ms step_avg:143.40ms
step:563/1370 train_time:79304ms step_avg:143.41ms
step:564/1370 train_time:79453ms step_avg:143.42ms
step:565/1370 train_time:79601ms step_avg:143.43ms
step:566/1370 train_time:79752ms step_avg:143.44ms
step:567/1370 train_time:79899ms step_avg:143.45ms
step:568/1370 train_time:80049ms step_avg:143.46ms
step:569/1370 train_time:80198ms step_avg:143.47ms
step:570/1370 train_time:80346ms step_avg:143.48ms
step:571/1370 train_time:80535ms step_avg:143.56ms
step:572/1370 train_time:80683ms step_avg:143.56ms
step:573/1370 train_time:80832ms step_avg:143.57ms
step:574/1370 train_time:80980ms step_avg:143.58ms
step:575/1370 train_time:81128ms step_avg:143.59ms
step:576/1370 train_time:81275ms step_avg:143.60ms
step:577/1370 train_time:81426ms step_avg:143.61ms
step:578/1370 train_time:81576ms step_avg:143.62ms
step:579/1370 train_time:81725ms step_avg:143.63ms
step:580/1370 train_time:81875ms step_avg:143.64ms
step:581/1370 train_time:82023ms step_avg:143.65ms
step:582/1370 train_time:82172ms step_avg:143.66ms
step:583/1370 train_time:82317ms step_avg:143.66ms
step:584/1370 train_time:82469ms step_avg:143.67ms
step:585/1370 train_time:82616ms step_avg:143.68ms
step:586/1370 train_time:82767ms step_avg:143.69ms
step:587/1370 train_time:82916ms step_avg:143.70ms
step:588/1370 train_time:83065ms step_avg:143.71ms
step:589/1370 train_time:83212ms step_avg:143.72ms
step:590/1370 train_time:83362ms step_avg:143.73ms
step:591/1370 train_time:83512ms step_avg:143.74ms
step:592/1370 train_time:83663ms step_avg:143.75ms
step:593/1370 train_time:83811ms step_avg:143.76ms
step:594/1370 train_time:83959ms step_avg:143.76ms
step:595/1370 train_time:84108ms step_avg:143.77ms
step:596/1370 train_time:84256ms step_avg:143.78ms
step:597/1370 train_time:84404ms step_avg:143.79ms
step:598/1370 train_time:84554ms step_avg:143.80ms
step:599/1370 train_time:84702ms step_avg:143.81ms
step:600/1370 train_time:84854ms step_avg:143.82ms
step:601/1370 train_time:85004ms step_avg:143.83ms
step:602/1370 train_time:85152ms step_avg:143.84ms
step:603/1370 train_time:85302ms step_avg:143.85ms
step:604/1370 train_time:85452ms step_avg:143.86ms
step:605/1370 train_time:85599ms step_avg:143.86ms
step:606/1370 train_time:85749ms step_avg:143.87ms
step:607/1370 train_time:85897ms step_avg:143.88ms
step:608/1370 train_time:86047ms step_avg:143.89ms
step:609/1370 train_time:86196ms step_avg:143.90ms
step:610/1370 train_time:86348ms step_avg:143.91ms
step:611/1370 train_time:86497ms step_avg:143.92ms
step:612/1370 train_time:86647ms step_avg:143.93ms
step:613/1370 train_time:86797ms step_avg:143.94ms
step:614/1370 train_time:86947ms step_avg:143.95ms
step:615/1370 train_time:87098ms step_avg:143.96ms
step:616/1370 train_time:87248ms step_avg:143.97ms
step:617/1370 train_time:87398ms step_avg:143.98ms
step:618/1370 train_time:87549ms step_avg:144.00ms
step:619/1370 train_time:87699ms step_avg:144.00ms
step:620/1370 train_time:87851ms step_avg:144.02ms
step:621/1370 train_time:88001ms step_avg:144.03ms
step:622/1370 train_time:88153ms step_avg:144.04ms
step:623/1370 train_time:88303ms step_avg:144.05ms
step:624/1370 train_time:88454ms step_avg:144.06ms
step:625/1370 train_time:88602ms step_avg:144.07ms
step:625/1370 val_loss:3.5749 train_time:88672ms step_avg:144.18ms
step:626/1370 train_time:88755ms step_avg:144.08ms
step:627/1370 train_time:88907ms step_avg:144.10ms
step:628/1370 train_time:89056ms step_avg:144.10ms
step:629/1370 train_time:89206ms step_avg:144.11ms
step:630/1370 train_time:89355ms step_avg:144.12ms
step:631/1370 train_time:89505ms step_avg:144.13ms
step:632/1370 train_time:89656ms step_avg:144.14ms
step:633/1370 train_time:89810ms step_avg:144.16ms
step:634/1370 train_time:89959ms step_avg:144.17ms
step:635/1370 train_time:90108ms step_avg:144.17ms
step:636/1370 train_time:90258ms step_avg:144.18ms
step:637/1370 train_time:90409ms step_avg:144.19ms
step:638/1370 train_time:90558ms step_avg:144.20ms
step:639/1370 train_time:90708ms step_avg:144.21ms
step:640/1370 train_time:90858ms step_avg:144.22ms
step:641/1370 train_time:91008ms step_avg:144.23ms
step:642/1370 train_time:91156ms step_avg:144.23ms
step:643/1370 train_time:91306ms step_avg:144.24ms
step:644/1370 train_time:91456ms step_avg:144.25ms
step:645/1370 train_time:91607ms step_avg:144.26ms
step:646/1370 train_time:91757ms step_avg:144.27ms
step:647/1370 train_time:91909ms step_avg:144.28ms
step:648/1370 train_time:92059ms step_avg:144.29ms
step:649/1370 train_time:92209ms step_avg:144.30ms
step:650/1370 train_time:92359ms step_avg:144.31ms
step:651/1370 train_time:92508ms step_avg:144.32ms
step:652/1370 train_time:92657ms step_avg:144.33ms
step:653/1370 train_time:92809ms step_avg:144.34ms
step:654/1370 train_time:92961ms step_avg:144.35ms
step:655/1370 train_time:93112ms step_avg:144.36ms
step:656/1370 train_time:93262ms step_avg:144.37ms
step:657/1370 train_time:93412ms step_avg:144.38ms
step:658/1370 train_time:93563ms step_avg:144.39ms
step:659/1370 train_time:93712ms step_avg:144.39ms
step:660/1370 train_time:93863ms step_avg:144.40ms
step:661/1370 train_time:94014ms step_avg:144.41ms
step:662/1370 train_time:94163ms step_avg:144.42ms
step:663/1370 train_time:94312ms step_avg:144.43ms
step:664/1370 train_time:94465ms step_avg:144.44ms
step:665/1370 train_time:94615ms step_avg:144.45ms
step:666/1370 train_time:94766ms step_avg:144.46ms
step:667/1370 train_time:94915ms step_avg:144.47ms
step:668/1370 train_time:95069ms step_avg:144.48ms
step:669/1370 train_time:95222ms step_avg:144.49ms
step:670/1370 train_time:95371ms step_avg:144.50ms
step:671/1370 train_time:95522ms step_avg:144.51ms
step:672/1370 train_time:95671ms step_avg:144.52ms
step:673/1370 train_time:95821ms step_avg:144.53ms
step:674/1370 train_time:95971ms step_avg:144.53ms
step:675/1370 train_time:96122ms step_avg:144.55ms
step:676/1370 train_time:96272ms step_avg:144.55ms
step:677/1370 train_time:96424ms step_avg:144.56ms
step:678/1370 train_time:96572ms step_avg:144.57ms
step:679/1370 train_time:96725ms step_avg:144.58ms
step:680/1370 train_time:96873ms step_avg:144.59ms
step:681/1370 train_time:97025ms step_avg:144.60ms
step:682/1370 train_time:97175ms step_avg:144.61ms
step:683/1370 train_time:97326ms step_avg:144.62ms
step:684/1370 train_time:97476ms step_avg:144.62ms
step:685/1370 train_time:97628ms step_avg:144.63ms
step:686/1370 train_time:97775ms step_avg:144.64ms
step:687/1370 train_time:97926ms step_avg:144.65ms
step:688/1370 train_time:98076ms step_avg:144.66ms
step:689/1370 train_time:98229ms step_avg:144.67ms
step:690/1370 train_time:98380ms step_avg:144.68ms
step:691/1370 train_time:98530ms step_avg:144.68ms
step:692/1370 train_time:98680ms step_avg:144.69ms
step:693/1370 train_time:98831ms step_avg:144.70ms
step:694/1370 train_time:98979ms step_avg:144.71ms
step:695/1370 train_time:99129ms step_avg:144.71ms
step:696/1370 train_time:99277ms step_avg:144.72ms
step:697/1370 train_time:99429ms step_avg:144.73ms
step:698/1370 train_time:99579ms step_avg:144.74ms
step:699/1370 train_time:99730ms step_avg:144.75ms
step:700/1370 train_time:99878ms step_avg:144.75ms
step:701/1370 train_time:100029ms step_avg:144.76ms
step:702/1370 train_time:100178ms step_avg:144.77ms
step:703/1370 train_time:100330ms step_avg:144.78ms
step:704/1370 train_time:100480ms step_avg:144.78ms
step:705/1370 train_time:100631ms step_avg:144.79ms
step:706/1370 train_time:100786ms step_avg:144.81ms
step:707/1370 train_time:100935ms step_avg:144.81ms
step:708/1370 train_time:101086ms step_avg:144.82ms
step:709/1370 train_time:101236ms step_avg:144.83ms
step:710/1370 train_time:101386ms step_avg:144.84ms
step:711/1370 train_time:101537ms step_avg:144.85ms
step:712/1370 train_time:101689ms step_avg:144.86ms
step:713/1370 train_time:101840ms step_avg:144.87ms
step:714/1370 train_time:101993ms step_avg:144.88ms
step:715/1370 train_time:102144ms step_avg:144.88ms
step:716/1370 train_time:102297ms step_avg:144.90ms
step:717/1370 train_time:102450ms step_avg:144.91ms
step:718/1370 train_time:102599ms step_avg:144.91ms
step:719/1370 train_time:102751ms step_avg:144.92ms
step:720/1370 train_time:102904ms step_avg:144.94ms
step:721/1370 train_time:103054ms step_avg:144.94ms
step:722/1370 train_time:103208ms step_avg:144.95ms
step:723/1370 train_time:103358ms step_avg:144.96ms
step:724/1370 train_time:103509ms step_avg:144.97ms
step:725/1370 train_time:103660ms step_avg:144.98ms
step:726/1370 train_time:103811ms step_avg:144.99ms
step:727/1370 train_time:103965ms step_avg:145.00ms
step:728/1370 train_time:104118ms step_avg:145.01ms
step:729/1370 train_time:104270ms step_avg:145.02ms
step:730/1370 train_time:104425ms step_avg:145.03ms
step:731/1370 train_time:104576ms step_avg:145.04ms
step:732/1370 train_time:104728ms step_avg:145.05ms
step:733/1370 train_time:104878ms step_avg:145.06ms
step:734/1370 train_time:105029ms step_avg:145.07ms
step:735/1370 train_time:105181ms step_avg:145.08ms
step:736/1370 train_time:105333ms step_avg:145.09ms
step:737/1370 train_time:105485ms step_avg:145.10ms
step:738/1370 train_time:105637ms step_avg:145.11ms
step:739/1370 train_time:105789ms step_avg:145.12ms
step:740/1370 train_time:105940ms step_avg:145.12ms
step:741/1370 train_time:106093ms step_avg:145.13ms
step:742/1370 train_time:106243ms step_avg:145.14ms
step:743/1370 train_time:106393ms step_avg:145.15ms
step:744/1370 train_time:106546ms step_avg:145.16ms
step:745/1370 train_time:106702ms step_avg:145.17ms
step:746/1370 train_time:106852ms step_avg:145.18ms
step:747/1370 train_time:107004ms step_avg:145.19ms
step:748/1370 train_time:107156ms step_avg:145.20ms
step:749/1370 train_time:107308ms step_avg:145.21ms
step:750/1370 train_time:107459ms step_avg:145.22ms
step:750/1370 val_loss:3.5195 train_time:107531ms step_avg:145.31ms
step:751/1370 train_time:107615ms step_avg:145.23ms
step:752/1370 train_time:107767ms step_avg:145.24ms
step:753/1370 train_time:107916ms step_avg:145.24ms
step:754/1370 train_time:108066ms step_avg:145.25ms
step:755/1370 train_time:108216ms step_avg:145.26ms
step:756/1370 train_time:108369ms step_avg:145.27ms
step:757/1370 train_time:108523ms step_avg:145.28ms
step:758/1370 train_time:108677ms step_avg:145.29ms
step:759/1370 train_time:108827ms step_avg:145.30ms
step:760/1370 train_time:108978ms step_avg:145.30ms
step:761/1370 train_time:109170ms step_avg:145.37ms
step:762/1370 train_time:109319ms step_avg:145.37ms
step:763/1370 train_time:109471ms step_avg:145.38ms
step:764/1370 train_time:109620ms step_avg:145.38ms
step:765/1370 train_time:109772ms step_avg:145.39ms
step:766/1370 train_time:109923ms step_avg:145.40ms
step:767/1370 train_time:110079ms step_avg:145.41ms
step:768/1370 train_time:110232ms step_avg:145.42ms
step:769/1370 train_time:110383ms step_avg:145.43ms
step:770/1370 train_time:110535ms step_avg:145.44ms
step:771/1370 train_time:110685ms step_avg:145.45ms
step:772/1370 train_time:110835ms step_avg:145.45ms
step:773/1370 train_time:110986ms step_avg:145.46ms
step:774/1370 train_time:111138ms step_avg:145.47ms
step:775/1370 train_time:111291ms step_avg:145.48ms
step:776/1370 train_time:111444ms step_avg:145.49ms
step:777/1370 train_time:111597ms step_avg:145.50ms
step:778/1370 train_time:111747ms step_avg:145.50ms
step:779/1370 train_time:111897ms step_avg:145.51ms
step:780/1370 train_time:112050ms step_avg:145.52ms
step:781/1370 train_time:112202ms step_avg:145.53ms
step:782/1370 train_time:112354ms step_avg:145.54ms
step:783/1370 train_time:112504ms step_avg:145.54ms
step:784/1370 train_time:112657ms step_avg:145.55ms
step:785/1370 train_time:112806ms step_avg:145.56ms
step:786/1370 train_time:112957ms step_avg:145.56ms
step:787/1370 train_time:113110ms step_avg:145.57ms
step:788/1370 train_time:113260ms step_avg:145.58ms
step:789/1370 train_time:113413ms step_avg:145.59ms
step:790/1370 train_time:113563ms step_avg:145.59ms
step:791/1370 train_time:113713ms step_avg:145.60ms
step:792/1370 train_time:113865ms step_avg:145.61ms
step:793/1370 train_time:114015ms step_avg:145.61ms
step:794/1370 train_time:114169ms step_avg:145.62ms
step:795/1370 train_time:114322ms step_avg:145.63ms
step:796/1370 train_time:114475ms step_avg:145.64ms
step:797/1370 train_time:114625ms step_avg:145.65ms
step:798/1370 train_time:114778ms step_avg:145.66ms
step:799/1370 train_time:114932ms step_avg:145.67ms
step:800/1370 train_time:115082ms step_avg:145.67ms
step:801/1370 train_time:115235ms step_avg:145.68ms
step:802/1370 train_time:115386ms step_avg:145.69ms
step:803/1370 train_time:115538ms step_avg:145.70ms
step:804/1370 train_time:115690ms step_avg:145.71ms
step:805/1370 train_time:115845ms step_avg:145.72ms
step:806/1370 train_time:115996ms step_avg:145.72ms
step:807/1370 train_time:116145ms step_avg:145.73ms
step:808/1370 train_time:116296ms step_avg:145.73ms
step:809/1370 train_time:116448ms step_avg:145.74ms
step:810/1370 train_time:116599ms step_avg:145.75ms
step:811/1370 train_time:116752ms step_avg:145.76ms
step:812/1370 train_time:116904ms step_avg:145.77ms
step:813/1370 train_time:117055ms step_avg:145.77ms
step:814/1370 train_time:117208ms step_avg:145.78ms
step:815/1370 train_time:117360ms step_avg:145.79ms
step:816/1370 train_time:117515ms step_avg:145.80ms
step:817/1370 train_time:117668ms step_avg:145.81ms
step:818/1370 train_time:117819ms step_avg:145.82ms
step:819/1370 train_time:117975ms step_avg:145.83ms
step:820/1370 train_time:118128ms step_avg:145.84ms
step:821/1370 train_time:118281ms step_avg:145.85ms
step:822/1370 train_time:118435ms step_avg:145.86ms
step:823/1370 train_time:118585ms step_avg:145.86ms
step:824/1370 train_time:118737ms step_avg:145.87ms
step:825/1370 train_time:118894ms step_avg:145.88ms
step:826/1370 train_time:119048ms step_avg:145.89ms
step:827/1370 train_time:119200ms step_avg:145.90ms
step:828/1370 train_time:119354ms step_avg:145.91ms
step:829/1370 train_time:119506ms step_avg:145.92ms
step:830/1370 train_time:119658ms step_avg:145.92ms
step:831/1370 train_time:119810ms step_avg:145.93ms
step:832/1370 train_time:119961ms step_avg:145.94ms
step:833/1370 train_time:120114ms step_avg:145.95ms
step:834/1370 train_time:120268ms step_avg:145.96ms
step:835/1370 train_time:120422ms step_avg:145.97ms
step:836/1370 train_time:120577ms step_avg:145.98ms
step:837/1370 train_time:120730ms step_avg:145.99ms
step:838/1370 train_time:120882ms step_avg:145.99ms
step:839/1370 train_time:121033ms step_avg:146.00ms
step:840/1370 train_time:121184ms step_avg:146.00ms
step:841/1370 train_time:121336ms step_avg:146.01ms
step:842/1370 train_time:121488ms step_avg:146.02ms
step:843/1370 train_time:121639ms step_avg:146.03ms
step:844/1370 train_time:121791ms step_avg:146.03ms
step:845/1370 train_time:121943ms step_avg:146.04ms
step:846/1370 train_time:122097ms step_avg:146.05ms
step:847/1370 train_time:122252ms step_avg:146.06ms
step:848/1370 train_time:122403ms step_avg:146.07ms
step:849/1370 train_time:122558ms step_avg:146.08ms
step:850/1370 train_time:122713ms step_avg:146.09ms
step:851/1370 train_time:122867ms step_avg:146.10ms
step:852/1370 train_time:123019ms step_avg:146.10ms
step:853/1370 train_time:123172ms step_avg:146.11ms
step:854/1370 train_time:123322ms step_avg:146.12ms
step:855/1370 train_time:123475ms step_avg:146.12ms
step:856/1370 train_time:123624ms step_avg:146.13ms
step:857/1370 train_time:123778ms step_avg:146.14ms
step:858/1370 train_time:123934ms step_avg:146.15ms
step:859/1370 train_time:124085ms step_avg:146.15ms
step:860/1370 train_time:124236ms step_avg:146.16ms
step:861/1370 train_time:124389ms step_avg:146.17ms
step:862/1370 train_time:124541ms step_avg:146.18ms
step:863/1370 train_time:124695ms step_avg:146.18ms
step:864/1370 train_time:124849ms step_avg:146.19ms
step:865/1370 train_time:125000ms step_avg:146.20ms
step:866/1370 train_time:125157ms step_avg:146.21ms
step:867/1370 train_time:125308ms step_avg:146.22ms
step:868/1370 train_time:125459ms step_avg:146.22ms
step:869/1370 train_time:125612ms step_avg:146.23ms
step:870/1370 train_time:125769ms step_avg:146.24ms
step:871/1370 train_time:125921ms step_avg:146.25ms
step:872/1370 train_time:126074ms step_avg:146.26ms
step:873/1370 train_time:126225ms step_avg:146.26ms
step:874/1370 train_time:126377ms step_avg:146.27ms
step:875/1370 train_time:126530ms step_avg:146.28ms
step:875/1370 val_loss:3.4686 train_time:126601ms step_avg:146.36ms
step:876/1370 train_time:126684ms step_avg:146.29ms
step:877/1370 train_time:126840ms step_avg:146.30ms
step:878/1370 train_time:126993ms step_avg:146.30ms
step:879/1370 train_time:127145ms step_avg:146.31ms
step:880/1370 train_time:127298ms step_avg:146.32ms
step:881/1370 train_time:127449ms step_avg:146.32ms
step:882/1370 train_time:127603ms step_avg:146.33ms
step:883/1370 train_time:127758ms step_avg:146.34ms
step:884/1370 train_time:127913ms step_avg:146.35ms
step:885/1370 train_time:128064ms step_avg:146.36ms
step:886/1370 train_time:128220ms step_avg:146.37ms
step:887/1370 train_time:128372ms step_avg:146.38ms
step:888/1370 train_time:128526ms step_avg:146.39ms
step:889/1370 train_time:128682ms step_avg:146.40ms
step:890/1370 train_time:128833ms step_avg:146.40ms
step:891/1370 train_time:128988ms step_avg:146.41ms
step:892/1370 train_time:129141ms step_avg:146.42ms
step:893/1370 train_time:129294ms step_avg:146.43ms
step:894/1370 train_time:129447ms step_avg:146.43ms
step:895/1370 train_time:129601ms step_avg:146.44ms
step:896/1370 train_time:129755ms step_avg:146.45ms
step:897/1370 train_time:129908ms step_avg:146.46ms
step:898/1370 train_time:130061ms step_avg:146.46ms
step:899/1370 train_time:130213ms step_avg:146.47ms
step:900/1370 train_time:130364ms step_avg:146.48ms
step:901/1370 train_time:130519ms step_avg:146.49ms
step:902/1370 train_time:130669ms step_avg:146.49ms
step:903/1370 train_time:130822ms step_avg:146.50ms
step:904/1370 train_time:130976ms step_avg:146.51ms
step:905/1370 train_time:131128ms step_avg:146.51ms
step:906/1370 train_time:131282ms step_avg:146.52ms
step:907/1370 train_time:131440ms step_avg:146.53ms
step:908/1370 train_time:131594ms step_avg:146.54ms
step:909/1370 train_time:131747ms step_avg:146.55ms
step:910/1370 train_time:131905ms step_avg:146.56ms
step:911/1370 train_time:132060ms step_avg:146.57ms
step:912/1370 train_time:132214ms step_avg:146.58ms
step:913/1370 train_time:132367ms step_avg:146.59ms
step:914/1370 train_time:132520ms step_avg:146.59ms
step:915/1370 train_time:132674ms step_avg:146.60ms
step:916/1370 train_time:132828ms step_avg:146.61ms
step:917/1370 train_time:132981ms step_avg:146.62ms
step:918/1370 train_time:133138ms step_avg:146.63ms
step:919/1370 train_time:133294ms step_avg:146.64ms
step:920/1370 train_time:133445ms step_avg:146.64ms
step:921/1370 train_time:133601ms step_avg:146.65ms
step:922/1370 train_time:133758ms step_avg:146.66ms
step:923/1370 train_time:133913ms step_avg:146.67ms
step:924/1370 train_time:134066ms step_avg:146.68ms
step:925/1370 train_time:134222ms step_avg:146.69ms
step:926/1370 train_time:134375ms step_avg:146.70ms
step:927/1370 train_time:134529ms step_avg:146.71ms
step:928/1370 train_time:134684ms step_avg:146.71ms
step:929/1370 train_time:134840ms step_avg:146.73ms
step:930/1370 train_time:134997ms step_avg:146.74ms
step:931/1370 train_time:135148ms step_avg:146.74ms
step:932/1370 train_time:135301ms step_avg:146.75ms
step:933/1370 train_time:135455ms step_avg:146.76ms
step:934/1370 train_time:135610ms step_avg:146.76ms
step:935/1370 train_time:135763ms step_avg:146.77ms
step:936/1370 train_time:135920ms step_avg:146.78ms
step:937/1370 train_time:136077ms step_avg:146.79ms
step:938/1370 train_time:136230ms step_avg:146.80ms
step:939/1370 train_time:136386ms step_avg:146.81ms
step:940/1370 train_time:136540ms step_avg:146.82ms
step:941/1370 train_time:136694ms step_avg:146.82ms
step:942/1370 train_time:136847ms step_avg:146.83ms
step:943/1370 train_time:137002ms step_avg:146.84ms
step:944/1370 train_time:137161ms step_avg:146.85ms
step:945/1370 train_time:137317ms step_avg:146.86ms
step:946/1370 train_time:137472ms step_avg:146.87ms
step:947/1370 train_time:137626ms step_avg:146.88ms
step:948/1370 train_time:137781ms step_avg:146.89ms
step:949/1370 train_time:137937ms step_avg:146.90ms
step:950/1370 train_time:138091ms step_avg:146.91ms
step:951/1370 train_time:138286ms step_avg:146.96ms
step:952/1370 train_time:138439ms step_avg:146.96ms
step:953/1370 train_time:138593ms step_avg:146.97ms
step:954/1370 train_time:138746ms step_avg:146.98ms
step:955/1370 train_time:138899ms step_avg:146.98ms
step:956/1370 train_time:139052ms step_avg:146.99ms
step:957/1370 train_time:139207ms step_avg:147.00ms
step:958/1370 train_time:139365ms step_avg:147.01ms
step:959/1370 train_time:139523ms step_avg:147.02ms
step:960/1370 train_time:139680ms step_avg:147.03ms
step:961/1370 train_time:139833ms step_avg:147.04ms
step:962/1370 train_time:139987ms step_avg:147.04ms
step:963/1370 train_time:140146ms step_avg:147.06ms
step:964/1370 train_time:140300ms step_avg:147.06ms
step:965/1370 train_time:140453ms step_avg:147.07ms
step:966/1370 train_time:140607ms step_avg:147.08ms
step:967/1370 train_time:140759ms step_avg:147.08ms
step:968/1370 train_time:140912ms step_avg:147.09ms
step:969/1370 train_time:141066ms step_avg:147.10ms
step:970/1370 train_time:141220ms step_avg:147.10ms
step:971/1370 train_time:141374ms step_avg:147.11ms
step:972/1370 train_time:141527ms step_avg:147.12ms
step:973/1370 train_time:141681ms step_avg:147.12ms
step:974/1370 train_time:141835ms step_avg:147.13ms
step:975/1370 train_time:141991ms step_avg:147.14ms
step:976/1370 train_time:142143ms step_avg:147.15ms
step:977/1370 train_time:142299ms step_avg:147.16ms
step:978/1370 train_time:142453ms step_avg:147.16ms
step:979/1370 train_time:142608ms step_avg:147.17ms
step:980/1370 train_time:142760ms step_avg:147.17ms
step:981/1370 train_time:142912ms step_avg:147.18ms
step:982/1370 train_time:143065ms step_avg:147.19ms
step:983/1370 train_time:143218ms step_avg:147.19ms
step:984/1370 train_time:143371ms step_avg:147.20ms
step:985/1370 train_time:143524ms step_avg:147.20ms
step:986/1370 train_time:143680ms step_avg:147.21ms
step:987/1370 train_time:143834ms step_avg:147.22ms
step:988/1370 train_time:143985ms step_avg:147.22ms
step:989/1370 train_time:144137ms step_avg:147.23ms
step:990/1370 train_time:144291ms step_avg:147.24ms
step:991/1370 train_time:144443ms step_avg:147.24ms
step:992/1370 train_time:144601ms step_avg:147.25ms
step:993/1370 train_time:144763ms step_avg:147.27ms
step:994/1370 train_time:144918ms step_avg:147.27ms
step:995/1370 train_time:145071ms step_avg:147.28ms
step:996/1370 train_time:145222ms step_avg:147.28ms
step:997/1370 train_time:145377ms step_avg:147.29ms
step:998/1370 train_time:145529ms step_avg:147.30ms
step:999/1370 train_time:145684ms step_avg:147.30ms
step:1000/1370 train_time:145840ms step_avg:147.31ms
step:1000/1370 val_loss:3.4013 train_time:145911ms step_avg:147.38ms
step:1001/1370 train_time:145995ms step_avg:147.32ms
step:1002/1370 train_time:146151ms step_avg:147.33ms
step:1003/1370 train_time:146304ms step_avg:147.34ms
step:1004/1370 train_time:146460ms step_avg:147.34ms
step:1005/1370 train_time:146613ms step_avg:147.35ms
step:1006/1370 train_time:146765ms step_avg:147.35ms
step:1007/1370 train_time:146921ms step_avg:147.36ms
step:1008/1370 train_time:147077ms step_avg:147.37ms
step:1009/1370 train_time:147238ms step_avg:147.38ms
step:1010/1370 train_time:147389ms step_avg:147.39ms
step:1011/1370 train_time:147542ms step_avg:147.39ms
step:1012/1370 train_time:147695ms step_avg:147.40ms
step:1013/1370 train_time:147852ms step_avg:147.41ms
step:1014/1370 train_time:148004ms step_avg:147.41ms
step:1015/1370 train_time:148160ms step_avg:147.42ms
step:1016/1370 train_time:148314ms step_avg:147.43ms
step:1017/1370 train_time:148468ms step_avg:147.44ms
step:1018/1370 train_time:148621ms step_avg:147.44ms
step:1019/1370 train_time:148778ms step_avg:147.45ms
step:1020/1370 train_time:148934ms step_avg:147.46ms
step:1021/1370 train_time:149089ms step_avg:147.47ms
step:1022/1370 train_time:149243ms step_avg:147.47ms
step:1023/1370 train_time:149399ms step_avg:147.48ms
step:1024/1370 train_time:149554ms step_avg:147.49ms
step:1025/1370 train_time:149709ms step_avg:147.50ms
step:1026/1370 train_time:149862ms step_avg:147.50ms
step:1027/1370 train_time:150017ms step_avg:147.51ms
step:1028/1370 train_time:150175ms step_avg:147.52ms
step:1029/1370 train_time:150332ms step_avg:147.53ms
step:1030/1370 train_time:150487ms step_avg:147.54ms
step:1031/1370 train_time:150639ms step_avg:147.54ms
step:1032/1370 train_time:150794ms step_avg:147.55ms
step:1033/1370 train_time:150949ms step_avg:147.55ms
step:1034/1370 train_time:151102ms step_avg:147.56ms
step:1035/1370 train_time:151262ms step_avg:147.57ms
step:1036/1370 train_time:151418ms step_avg:147.58ms
step:1037/1370 train_time:151574ms step_avg:147.59ms
step:1038/1370 train_time:151732ms step_avg:147.60ms
step:1039/1370 train_time:151884ms step_avg:147.60ms
step:1040/1370 train_time:152037ms step_avg:147.61ms
step:1041/1370 train_time:152192ms step_avg:147.62ms
step:1042/1370 train_time:152346ms step_avg:147.62ms
step:1043/1370 train_time:152499ms step_avg:147.63ms
step:1044/1370 train_time:152657ms step_avg:147.64ms
step:1045/1370 train_time:152813ms step_avg:147.65ms
step:1046/1370 train_time:152966ms step_avg:147.65ms
step:1047/1370 train_time:153119ms step_avg:147.66ms
step:1048/1370 train_time:153278ms step_avg:147.67ms
step:1049/1370 train_time:153432ms step_avg:147.67ms
step:1050/1370 train_time:153590ms step_avg:147.68ms
step:1051/1370 train_time:153748ms step_avg:147.69ms
step:1052/1370 train_time:153901ms step_avg:147.70ms
step:1053/1370 train_time:154056ms step_avg:147.70ms
step:1054/1370 train_time:154213ms step_avg:147.71ms
step:1055/1370 train_time:154368ms step_avg:147.72ms
step:1056/1370 train_time:154525ms step_avg:147.73ms
step:1057/1370 train_time:154682ms step_avg:147.74ms
step:1058/1370 train_time:154840ms step_avg:147.75ms
step:1059/1370 train_time:154997ms step_avg:147.76ms
step:1060/1370 train_time:155151ms step_avg:147.76ms
step:1061/1370 train_time:155304ms step_avg:147.77ms
step:1062/1370 train_time:155460ms step_avg:147.78ms
step:1063/1370 train_time:155615ms step_avg:147.78ms
step:1064/1370 train_time:155770ms step_avg:147.79ms
step:1065/1370 train_time:155926ms step_avg:147.80ms
step:1066/1370 train_time:156085ms step_avg:147.81ms
step:1067/1370 train_time:156241ms step_avg:147.82ms
step:1068/1370 train_time:156394ms step_avg:147.82ms
step:1069/1370 train_time:156556ms step_avg:147.83ms
step:1070/1370 train_time:156707ms step_avg:147.84ms
step:1071/1370 train_time:156862ms step_avg:147.84ms
step:1072/1370 train_time:157017ms step_avg:147.85ms
step:1073/1370 train_time:157169ms step_avg:147.85ms
step:1074/1370 train_time:157323ms step_avg:147.86ms
step:1075/1370 train_time:157479ms step_avg:147.87ms
step:1076/1370 train_time:157635ms step_avg:147.88ms
step:1077/1370 train_time:157788ms step_avg:147.88ms
step:1078/1370 train_time:157947ms step_avg:147.89ms
step:1079/1370 train_time:158105ms step_avg:147.90ms
step:1080/1370 train_time:158261ms step_avg:147.91ms
step:1081/1370 train_time:158415ms step_avg:147.91ms
step:1082/1370 train_time:158568ms step_avg:147.92ms
step:1083/1370 train_time:158723ms step_avg:147.92ms
step:1084/1370 train_time:158880ms step_avg:147.93ms
step:1085/1370 train_time:159035ms step_avg:147.94ms
step:1086/1370 train_time:159190ms step_avg:147.95ms
step:1087/1370 train_time:159345ms step_avg:147.95ms
step:1088/1370 train_time:159501ms step_avg:147.96ms
step:1089/1370 train_time:159662ms step_avg:147.97ms
step:1090/1370 train_time:159821ms step_avg:147.98ms
step:1091/1370 train_time:159978ms step_avg:147.99ms
step:1092/1370 train_time:160131ms step_avg:148.00ms
step:1093/1370 train_time:160288ms step_avg:148.00ms
step:1094/1370 train_time:160441ms step_avg:148.01ms
step:1095/1370 train_time:160596ms step_avg:148.02ms
step:1096/1370 train_time:160755ms step_avg:148.02ms
step:1097/1370 train_time:160911ms step_avg:148.03ms
step:1098/1370 train_time:161064ms step_avg:148.04ms
step:1099/1370 train_time:161219ms step_avg:148.04ms
step:1100/1370 train_time:161372ms step_avg:148.05ms
step:1101/1370 train_time:161526ms step_avg:148.05ms
step:1102/1370 train_time:161683ms step_avg:148.06ms
step:1103/1370 train_time:161838ms step_avg:148.07ms
step:1104/1370 train_time:161993ms step_avg:148.07ms
step:1105/1370 train_time:162150ms step_avg:148.08ms
step:1106/1370 train_time:162304ms step_avg:148.09ms
step:1107/1370 train_time:162459ms step_avg:148.09ms
step:1108/1370 train_time:162616ms step_avg:148.10ms
step:1109/1370 train_time:162770ms step_avg:148.11ms
step:1110/1370 train_time:162927ms step_avg:148.12ms
step:1111/1370 train_time:163084ms step_avg:148.12ms
step:1112/1370 train_time:163240ms step_avg:148.13ms
step:1113/1370 train_time:163396ms step_avg:148.14ms
step:1114/1370 train_time:163553ms step_avg:148.15ms
step:1115/1370 train_time:163709ms step_avg:148.15ms
step:1116/1370 train_time:163860ms step_avg:148.16ms
step:1117/1370 train_time:164019ms step_avg:148.17ms
step:1118/1370 train_time:164180ms step_avg:148.18ms
step:1119/1370 train_time:164337ms step_avg:148.18ms
step:1120/1370 train_time:164493ms step_avg:148.19ms
step:1121/1370 train_time:164649ms step_avg:148.20ms
step:1122/1370 train_time:164804ms step_avg:148.20ms
step:1123/1370 train_time:164959ms step_avg:148.21ms
step:1124/1370 train_time:165117ms step_avg:148.22ms
step:1125/1370 train_time:165275ms step_avg:148.23ms
step:1125/1370 val_loss:3.3478 train_time:165347ms step_avg:148.29ms
step:1126/1370 train_time:165431ms step_avg:148.24ms
step:1127/1370 train_time:165589ms step_avg:148.24ms
step:1128/1370 train_time:165744ms step_avg:148.25ms
step:1129/1370 train_time:165902ms step_avg:148.26ms
step:1130/1370 train_time:166057ms step_avg:148.27ms
step:1131/1370 train_time:166214ms step_avg:148.27ms
step:1132/1370 train_time:166369ms step_avg:148.28ms
step:1133/1370 train_time:166525ms step_avg:148.29ms
step:1134/1370 train_time:166681ms step_avg:148.29ms
step:1135/1370 train_time:166836ms step_avg:148.30ms
step:1136/1370 train_time:166995ms step_avg:148.31ms
step:1137/1370 train_time:167148ms step_avg:148.31ms
step:1138/1370 train_time:167304ms step_avg:148.32ms
step:1139/1370 train_time:167461ms step_avg:148.33ms
step:1140/1370 train_time:167618ms step_avg:148.33ms
step:1141/1370 train_time:167815ms step_avg:148.38ms
step:1142/1370 train_time:167971ms step_avg:148.38ms
step:1143/1370 train_time:168131ms step_avg:148.39ms
step:1144/1370 train_time:168284ms step_avg:148.40ms
step:1145/1370 train_time:168439ms step_avg:148.40ms
step:1146/1370 train_time:168596ms step_avg:148.41ms
step:1147/1370 train_time:168755ms step_avg:148.42ms
step:1148/1370 train_time:168910ms step_avg:148.43ms
step:1149/1370 train_time:169064ms step_avg:148.43ms
step:1150/1370 train_time:169218ms step_avg:148.44ms
step:1151/1370 train_time:169376ms step_avg:148.44ms
step:1152/1370 train_time:169532ms step_avg:148.45ms
step:1153/1370 train_time:169690ms step_avg:148.46ms
step:1154/1370 train_time:169845ms step_avg:148.47ms
step:1155/1370 train_time:170002ms step_avg:148.47ms
step:1156/1370 train_time:170161ms step_avg:148.48ms
step:1157/1370 train_time:170319ms step_avg:148.49ms
step:1158/1370 train_time:170473ms step_avg:148.50ms
step:1159/1370 train_time:170629ms step_avg:148.50ms
step:1160/1370 train_time:170783ms step_avg:148.51ms
step:1161/1370 train_time:170941ms step_avg:148.52ms
step:1162/1370 train_time:171099ms step_avg:148.52ms
step:1163/1370 train_time:171255ms step_avg:148.53ms
step:1164/1370 train_time:171409ms step_avg:148.53ms
step:1165/1370 train_time:171563ms step_avg:148.54ms
step:1166/1370 train_time:171718ms step_avg:148.55ms
step:1167/1370 train_time:171873ms step_avg:148.55ms
step:1168/1370 train_time:172029ms step_avg:148.56ms
step:1169/1370 train_time:172185ms step_avg:148.56ms
step:1170/1370 train_time:172340ms step_avg:148.57ms
step:1171/1370 train_time:172498ms step_avg:148.58ms
step:1172/1370 train_time:172653ms step_avg:148.58ms
step:1173/1370 train_time:172812ms step_avg:148.59ms
step:1174/1370 train_time:172974ms step_avg:148.60ms
step:1175/1370 train_time:173131ms step_avg:148.61ms
step:1176/1370 train_time:173289ms step_avg:148.62ms
step:1177/1370 train_time:173454ms step_avg:148.63ms
step:1178/1370 train_time:173611ms step_avg:148.64ms
step:1179/1370 train_time:173767ms step_avg:148.65ms
step:1180/1370 train_time:173928ms step_avg:148.66ms
step:1181/1370 train_time:174084ms step_avg:148.66ms
step:1182/1370 train_time:174238ms step_avg:148.67ms
step:1183/1370 train_time:174394ms step_avg:148.67ms
step:1184/1370 train_time:174549ms step_avg:148.68ms
step:1185/1370 train_time:174707ms step_avg:148.69ms
step:1186/1370 train_time:174863ms step_avg:148.69ms
step:1187/1370 train_time:175025ms step_avg:148.70ms
step:1188/1370 train_time:175179ms step_avg:148.71ms
step:1189/1370 train_time:175337ms step_avg:148.72ms
step:1190/1370 train_time:175496ms step_avg:148.73ms
step:1191/1370 train_time:175653ms step_avg:148.73ms
step:1192/1370 train_time:175806ms step_avg:148.74ms
step:1193/1370 train_time:175962ms step_avg:148.74ms
step:1194/1370 train_time:176119ms step_avg:148.75ms
step:1195/1370 train_time:176275ms step_avg:148.76ms
step:1196/1370 train_time:176430ms step_avg:148.76ms
step:1197/1370 train_time:176590ms step_avg:148.77ms
step:1198/1370 train_time:176751ms step_avg:148.78ms
step:1199/1370 train_time:176907ms step_avg:148.79ms
step:1200/1370 train_time:177063ms step_avg:148.79ms
step:1201/1370 train_time:177220ms step_avg:148.80ms
step:1202/1370 train_time:177388ms step_avg:148.82ms
step:1203/1370 train_time:177547ms step_avg:148.82ms
step:1204/1370 train_time:177703ms step_avg:148.83ms
step:1205/1370 train_time:177858ms step_avg:148.84ms
step:1206/1370 train_time:178015ms step_avg:148.84ms
step:1207/1370 train_time:178173ms step_avg:148.85ms
step:1208/1370 train_time:178329ms step_avg:148.86ms
step:1209/1370 train_time:178485ms step_avg:148.86ms
step:1210/1370 train_time:178647ms step_avg:148.87ms
step:1211/1370 train_time:178802ms step_avg:148.88ms
step:1212/1370 train_time:178960ms step_avg:148.88ms
step:1213/1370 train_time:179114ms step_avg:148.89ms
step:1214/1370 train_time:179272ms step_avg:148.90ms
step:1215/1370 train_time:179428ms step_avg:148.90ms
step:1216/1370 train_time:179581ms step_avg:148.91ms
step:1217/1370 train_time:179738ms step_avg:148.91ms
step:1218/1370 train_time:179892ms step_avg:148.92ms
step:1219/1370 train_time:180047ms step_avg:148.92ms
step:1220/1370 train_time:180202ms step_avg:148.93ms
step:1221/1370 train_time:180357ms step_avg:148.93ms
step:1222/1370 train_time:180512ms step_avg:148.94ms
step:1223/1370 train_time:180671ms step_avg:148.95ms
step:1224/1370 train_time:180830ms step_avg:148.95ms
step:1225/1370 train_time:180986ms step_avg:148.96ms
step:1226/1370 train_time:181144ms step_avg:148.97ms
step:1227/1370 train_time:181304ms step_avg:148.98ms
step:1228/1370 train_time:181459ms step_avg:148.98ms
step:1229/1370 train_time:181614ms step_avg:148.99ms
step:1230/1370 train_time:181775ms step_avg:149.00ms
step:1231/1370 train_time:181934ms step_avg:149.00ms
step:1232/1370 train_time:182095ms step_avg:149.01ms
step:1233/1370 train_time:182253ms step_avg:149.02ms
step:1234/1370 train_time:182406ms step_avg:149.02ms
step:1235/1370 train_time:182563ms step_avg:149.03ms
step:1236/1370 train_time:182720ms step_avg:149.04ms
step:1237/1370 train_time:182876ms step_avg:149.04ms
step:1238/1370 train_time:183039ms step_avg:149.05ms
step:1239/1370 train_time:183198ms step_avg:149.06ms
step:1240/1370 train_time:183358ms step_avg:149.07ms
step:1241/1370 train_time:183519ms step_avg:149.08ms
step:1242/1370 train_time:183677ms step_avg:149.09ms
step:1243/1370 train_time:183837ms step_avg:149.10ms
step:1244/1370 train_time:183992ms step_avg:149.10ms
step:1245/1370 train_time:184149ms step_avg:149.11ms
step:1246/1370 train_time:184304ms step_avg:149.11ms
step:1247/1370 train_time:184466ms step_avg:149.12ms
step:1248/1370 train_time:184622ms step_avg:149.13ms
step:1249/1370 train_time:184779ms step_avg:149.14ms
step:1250/1370 train_time:184934ms step_avg:149.14ms
step:1250/1370 val_loss:3.3024 train_time:185008ms step_avg:149.20ms
step:1251/1370 train_time:185096ms step_avg:149.15ms
step:1252/1370 train_time:185252ms step_avg:149.16ms
step:1253/1370 train_time:185407ms step_avg:149.16ms
step:1254/1370 train_time:185561ms step_avg:149.16ms
step:1255/1370 train_time:185726ms step_avg:149.18ms
step:1256/1370 train_time:185882ms step_avg:149.18ms
step:1257/1370 train_time:186040ms step_avg:149.19ms
step:1258/1370 train_time:186200ms step_avg:149.20ms
step:1259/1370 train_time:186357ms step_avg:149.20ms
step:1260/1370 train_time:186510ms step_avg:149.21ms
step:1261/1370 train_time:186667ms step_avg:149.21ms
step:1262/1370 train_time:186829ms step_avg:149.22ms
step:1263/1370 train_time:186986ms step_avg:149.23ms
step:1264/1370 train_time:187141ms step_avg:149.23ms
step:1265/1370 train_time:187297ms step_avg:149.24ms
step:1266/1370 train_time:187454ms step_avg:149.25ms
step:1267/1370 train_time:187611ms step_avg:149.25ms
step:1268/1370 train_time:187769ms step_avg:149.26ms
step:1269/1370 train_time:187930ms step_avg:149.27ms
step:1270/1370 train_time:188086ms step_avg:149.27ms
step:1271/1370 train_time:188243ms step_avg:149.28ms
step:1272/1370 train_time:188399ms step_avg:149.29ms
step:1273/1370 train_time:188554ms step_avg:149.29ms
step:1274/1370 train_time:188710ms step_avg:149.30ms
step:1275/1370 train_time:188867ms step_avg:149.30ms
step:1276/1370 train_time:189021ms step_avg:149.31ms
step:1277/1370 train_time:189178ms step_avg:149.31ms
step:1278/1370 train_time:189332ms step_avg:149.32ms
step:1279/1370 train_time:189490ms step_avg:149.32ms
step:1280/1370 train_time:189653ms step_avg:149.33ms
step:1281/1370 train_time:189811ms step_avg:149.34ms
step:1282/1370 train_time:189967ms step_avg:149.34ms
step:1283/1370 train_time:190125ms step_avg:149.35ms
step:1284/1370 train_time:190283ms step_avg:149.36ms
step:1285/1370 train_time:190440ms step_avg:149.36ms
step:1286/1370 train_time:190598ms step_avg:149.37ms
step:1287/1370 train_time:190755ms step_avg:149.38ms
step:1288/1370 train_time:190913ms step_avg:149.38ms
step:1289/1370 train_time:191074ms step_avg:149.39ms
step:1290/1370 train_time:191234ms step_avg:149.40ms
step:1291/1370 train_time:191395ms step_avg:149.41ms
step:1292/1370 train_time:191554ms step_avg:149.42ms
step:1293/1370 train_time:191709ms step_avg:149.42ms
step:1294/1370 train_time:191869ms step_avg:149.43ms
step:1295/1370 train_time:192028ms step_avg:149.44ms
step:1296/1370 train_time:192185ms step_avg:149.44ms
step:1297/1370 train_time:192346ms step_avg:149.45ms
step:1298/1370 train_time:192502ms step_avg:149.46ms
step:1299/1370 train_time:192658ms step_avg:149.46ms
step:1300/1370 train_time:192812ms step_avg:149.47ms
step:1301/1370 train_time:192968ms step_avg:149.47ms
step:1302/1370 train_time:193128ms step_avg:149.48ms
step:1303/1370 train_time:193286ms step_avg:149.49ms
step:1304/1370 train_time:193448ms step_avg:149.50ms
step:1305/1370 train_time:193604ms step_avg:149.50ms
step:1306/1370 train_time:193763ms step_avg:149.51ms
step:1307/1370 train_time:193918ms step_avg:149.51ms
step:1308/1370 train_time:194076ms step_avg:149.52ms
step:1309/1370 train_time:194233ms step_avg:149.53ms
step:1310/1370 train_time:194391ms step_avg:149.53ms
step:1311/1370 train_time:194546ms step_avg:149.54ms
step:1312/1370 train_time:194699ms step_avg:149.54ms
step:1313/1370 train_time:194856ms step_avg:149.54ms
step:1314/1370 train_time:195014ms step_avg:149.55ms
step:1315/1370 train_time:195172ms step_avg:149.56ms
step:1316/1370 train_time:195325ms step_avg:149.56ms
step:1317/1370 train_time:195479ms step_avg:149.56ms
step:1318/1370 train_time:195640ms step_avg:149.57ms
step:1319/1370 train_time:195798ms step_avg:149.58ms
step:1320/1370 train_time:195954ms step_avg:149.58ms
step:1321/1370 train_time:196113ms step_avg:149.59ms
step:1322/1370 train_time:196275ms step_avg:149.60ms
step:1323/1370 train_time:196430ms step_avg:149.60ms
step:1324/1370 train_time:196586ms step_avg:149.61ms
step:1325/1370 train_time:196743ms step_avg:149.61ms
step:1326/1370 train_time:196904ms step_avg:149.62ms
step:1327/1370 train_time:197060ms step_avg:149.63ms
step:1328/1370 train_time:197219ms step_avg:149.63ms
step:1329/1370 train_time:197395ms step_avg:149.65ms
step:1330/1370 train_time:197555ms step_avg:149.66ms
step:1331/1370 train_time:197756ms step_avg:149.70ms
step:1332/1370 train_time:197919ms step_avg:149.71ms
step:1333/1370 train_time:198076ms step_avg:149.72ms
step:1334/1370 train_time:198231ms step_avg:149.72ms
step:1335/1370 train_time:198386ms step_avg:149.73ms
step:1336/1370 train_time:198551ms step_avg:149.74ms
step:1337/1370 train_time:198708ms step_avg:149.74ms
step:1338/1370 train_time:198868ms step_avg:149.75ms
step:1339/1370 train_time:199029ms step_avg:149.76ms
step:1340/1370 train_time:199188ms step_avg:149.77ms
step:1341/1370 train_time:199342ms step_avg:149.77ms
step:1342/1370 train_time:199501ms step_avg:149.78ms
step:1343/1370 train_time:199656ms step_avg:149.78ms
step:1344/1370 train_time:199811ms step_avg:149.78ms
step:1345/1370 train_time:199970ms step_avg:149.79ms
step:1346/1370 train_time:200130ms step_avg:149.80ms
step:1347/1370 train_time:200288ms step_avg:149.80ms
step:1348/1370 train_time:200445ms step_avg:149.81ms
step:1349/1370 train_time:200603ms step_avg:149.82ms
step:1350/1370 train_time:200759ms step_avg:149.82ms
step:1351/1370 train_time:200916ms step_avg:149.83ms
step:1352/1370 train_time:201079ms step_avg:149.84ms
step:1353/1370 train_time:201238ms step_avg:149.84ms
step:1354/1370 train_time:201396ms step_avg:149.85ms
step:1355/1370 train_time:201552ms step_avg:149.85ms
step:1356/1370 train_time:201709ms step_avg:149.86ms
step:1357/1370 train_time:201870ms step_avg:149.87ms
step:1358/1370 train_time:202030ms step_avg:149.87ms
step:1359/1370 train_time:202187ms step_avg:149.88ms
step:1360/1370 train_time:202344ms step_avg:149.88ms
step:1361/1370 train_time:202503ms step_avg:149.89ms
step:1362/1370 train_time:202661ms step_avg:149.90ms
step:1363/1370 train_time:202825ms step_avg:149.91ms
step:1364/1370 train_time:202979ms step_avg:149.91ms
step:1365/1370 train_time:203134ms step_avg:149.91ms
step:1366/1370 train_time:203294ms step_avg:149.92ms
step:1367/1370 train_time:203451ms step_avg:149.93ms
step:1368/1370 train_time:203610ms step_avg:149.93ms
step:1369/1370 train_time:203777ms step_avg:149.95ms
step:1370/1370 train_time:203934ms step_avg:149.95ms
step:1370/1370 val_loss:3.2783 train_time:204008ms step_avg:150.01ms
peak memory consumption: 32619 MiB

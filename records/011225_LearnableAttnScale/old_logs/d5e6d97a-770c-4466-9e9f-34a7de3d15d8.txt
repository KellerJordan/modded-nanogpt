import os
import sys
with open(sys.argv[0]) as f:
    code = f.read() # read the code of this file ASAP, for logging
import uuid
import time
import glob
import subprocess
import contextlib
from dataclasses import dataclass

import torch
torch.empty(1, device='cuda', requires_grad=True).backward()
from torch import nn
import torch.nn.functional as F
import torch.distributed as dist
from torch.nn.parallel import DistributedDataParallel as DDP
# use of FlexAttention contributed by @KoszarskyB
from torch.nn.attention.flex_attention import BlockMask, flex_attention

# -----------------------------------------------------------------------------
# Muon optimizer

@torch.compile
def zeropower_via_newtonschulz5(G, steps):
    """
    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a
    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose
    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at
    zero even beyond the point where the iteration no longer converges all the way to one everywhere
    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T
    where S' is diagonal with S_{ii}' ~ Uniform(0.5, 1.5), which turns out not to hurt model
    performance at all relative to UV^T, where USV^T = G is the SVD.
    """
    assert len(G.shape) == 2
    a, b, c = (3.4445, -4.7750,  2.0315)
    X = G.bfloat16()
    if G.size(0) > G.size(1):
        X = X.T

    # Ensure spectral norm is at most 1
    X = X / (X.norm() + 1e-7)
    # Perform the NS iterations
    for _ in range(steps):
        A = X @ X.T
        B = b * A + c * A @ A # adapted from suggestion by @jxbz, @leloykun, and @YouJiacheng
        X = a * X + B @ X
    
    if G.size(0) > G.size(1):
        X = X.T
    return X

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Some warnings:
    - This optimizer assumes that all parameters passed in are 2D.
    - It should not be used for the embedding layer, the final fully connected layer, or any {0,1}-D
    parameters; those should all be optimized by a standard method (e.g., AdamW).
    - To use it with 4D convolutional filters, it works well to just flatten their last 3 dimensions.
    - We believe it is unlikely to work well for training with small batch size.
    - We believe it may not work well for finetuning pretrained models, but we haven't tested this.
    - We have not yet tried this optimizer for training scenarios larger than NanoGPT (124M).

    Arguments:
        lr: The learning rate used by the internal SGD.
        momentum: The momentum used by the internal SGD.
        nesterov: Whether to use Nesterov-style momentum in the internal SGD. (recommended)
        ns_steps: The number of Newton-Schulz iteration steps to use.
    """
    def __init__(self, params, lr=0.02, momentum=0.95, nesterov=True, ns_steps=5):
        self.world_size = int(os.environ['WORLD_SIZE'])
        self.rank = int(os.environ['RANK'])
        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, ns_steps=ns_steps)
        assert all(isinstance(p, torch.Tensor) for p in params)
        sizes = {p.numel() for p in params}
        param_groups = [dict(params=[p for p in params if p.numel() == size],
                             update_buffer=[torch.empty(size, device='cuda', dtype=torch.bfloat16) for _ in range(self.world_size)])
                        for size in sizes]
        super().__init__(param_groups, defaults)

    def step(self):

        for group in self.param_groups:

            lr = group['lr']
            momentum = group['momentum']
            nesterov = group['nesterov']
            ns_steps = group['ns_steps']
            update_buffers = group['update_buffer']
            # generate weight updates in distributed fashion
            params = group['params']
            handle = None
            params_world = None
            def update_prev():
                if params_world is None:
                    return
                assert handle is not None
                handle.wait()
                for p_world, g_world in zip(params_world, update_buffers):
                    p_world.data.add_(
                        g_world.view_as(p_world),
                        alpha=-lr * max(1, p_world.size(0) / p_world.size(1)) ** 0.5,
                    )
            for base_i in range(len(params))[::self.world_size]:
                if base_i + rank < len(params):
                    p = params[base_i + self.rank]
                    g = p.grad
                    assert g is not None
                    state = self.state[p]
                    if 'momentum_buffer' not in state:
                        state['momentum_buffer'] = torch.zeros_like(g)
                    buf = state['momentum_buffer']
                    buf.lerp_(g, 1 - momentum)
                    g = g.lerp_(buf, momentum) if nesterov else buf
                    g = zeropower_via_newtonschulz5(g, steps=ns_steps).flatten()
                else:
                    g = update_buffers[rank]
                update_prev() # async all_gather instead of sync all_reduce by @YouJiacheng
                handle = dist.all_gather(update_buffers, g, async_op=True)
                params_world = params[base_i : base_i + self.world_size]
            update_prev()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the GPT-2 model

def norm(x):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):

    def __init__(self, in_features, out_features):
        super().__init__(in_features, out_features, bias=False)

    def forward(self, x):
        return F.linear(x, self.weight.type_as(x))

class Rotary(nn.Module):

    def __init__(self, dim, max_seq_len=65536):
        super().__init__()
        # half-truncate RoPE by @YouJiacheng
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=dim//4, dtype=torch.float32)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(dim//4)])
        t = torch.arange(max_seq_len, dtype=torch.float32)
        theta = torch.einsum('i,j -> ij', t, angular_freq)
        self.cos = nn.Buffer(theta.cos(), persistent=False)
        self.sin = nn.Buffer(theta.sin(), persistent=False)

    def forward(self, x):
        cos, sin = self.cos[None, :x.size(-3), None, :], self.sin[None, :x.size(-3), None, :]
        x1, x2 = x.float().chunk(2, dim=-1)
        y1 = x1 * cos + x2 * sin
        y2 = x1 * (-sin) + x2 * cos
        return torch.cat((y1, y2), 3).type_as(x)

class CausalSelfAttention(nn.Module):

    def __init__(self, dim, num_heads):
        super().__init__()
        assert dim % num_heads == 0
        self.num_heads = num_heads
        self.c_q = CastedLinear(dim, dim)
        self.c_k = CastedLinear(dim, dim)
        self.c_v = CastedLinear(dim, dim)
        self.lambdas = nn.Parameter(torch.tensor([0.5, 0.5]))
        self.rotary = Rotary(dim // num_heads) # dim // num_heads = head_dim
        self.c_proj = CastedLinear(dim, dim)
        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977
        self.attn_scale = nn.Parameter(torch.tensor(1.0 / (dim // num_heads) ** 0.5))

    def forward(self, x, ve, block_mask):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, 'Must use batch size = 1 for FlexAttention'
        q = self.c_q(x).view(B, T, self.num_heads, -1)
        k = self.c_k(x).view(B, T, self.num_heads, -1)
        v = self.c_v(x).view(B, T, self.num_heads, -1)
        if ve is not None:
            v = self.lambdas[0] * v + self.lambdas[1] * ve.view_as(v) # @KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = self.lambdas[0] * v
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = self.rotary(q), self.rotary(k)
        y = flex_attention(q.transpose(1, 2) * self.attn_scale, k.transpose(1, 2), v.transpose(1, 2), block_mask=block_mask, scale=1.)
        y = y.transpose(1, 2).contiguous().view_as(x) # re-assemble all head outputs side by side
        y = self.c_proj(y)
        return y

class MLP(nn.Module):

    def __init__(self, dim):
        super().__init__()
        self.c_fc = CastedLinear(dim, 4 * dim)
        self.c_proj = CastedLinear(4 * dim, dim)
        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977

    def forward(self, x):
        x = self.c_fc(x)
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = self.c_proj(x)
        return x

class Block(nn.Module):

    def __init__(self, model_dim, num_heads, use_attn=True):
        super().__init__()
        self.attn = CausalSelfAttention(model_dim, num_heads) if use_attn else None
        self.mlp = MLP(model_dim)
        self.lambdas = nn.Parameter(torch.tensor([1., 0.]))

    def forward(self, x, ve, x0, block_mask):
        x = self.lambdas[0] * x + self.lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), ve, block_mask)
        x = x + self.mlp(norm(x))
        return x

class ValueEmbedding(nn.Module):
    def __init__(self, vocab_size, model_dim):
        super().__init__()
        self.embed = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])

    def forward(self, inputs):
        ve = [emb(inputs).bfloat16() for emb in self.embed]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        ve = [ve[0], ve[1], ve[2], None, None, None, None, None, None, ve[0], ve[1], ve[2]]
        return ve

# -----------------------------------------------------------------------------
# The main GPT-2 model

class GPT(nn.Module):

    def __init__(self, vocab_size, num_layers, num_heads, model_dim):
        super().__init__()
        self.embed = nn.Embedding(vocab_size, model_dim)
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.blocks = nn.ModuleList([Block(model_dim, num_heads, use_attn=(i != 7))
                                     for i in range(num_layers)])
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual learning
        # U-net structure on token value embeddings by @leloykun
        self.value_embeds = ValueEmbedding(vocab_size, model_dim)
        self.lm_head = CastedLinear(model_dim, vocab_size)
        self.lm_head.weight.data.zero_() # @Grad62304977
        # U-net design by @brendanh0gan
        self.num_encoder_layers = num_layers // 2 # Half of the layers for encoder
        self.num_decoder_layers = num_layers - self.num_encoder_layers # Remaining for decoder
        # Add learnable skip connection weights for decoder layers
        self.skip_weights = nn.Parameter(torch.ones(self.num_decoder_layers))

    def forward(self, inputs, targets, sliding_window_num_blocks):
        BLOCK_SIZE = 128
        seq_len = len(inputs)
        assert seq_len % BLOCK_SIZE == 0
        total_num_blocks = seq_len // BLOCK_SIZE
        assert inputs.ndim == 1
        docs = (inputs == 50256).cumsum(0)
        docs_low = docs.view(-1, BLOCK_SIZE)[:, 0].contiguous()
        docs_high = docs.view(-1, BLOCK_SIZE)[:, -1].contiguous()

        def document_causal(b, h, q_idx, kv_idx):
            causal_mask = q_idx >= kv_idx
            document_mask = docs[q_idx] == docs[kv_idx]
            return causal_mask & document_mask

        def dense_to_ordered(dense_mask):
            num_blocks = dense_mask.sum(dim=-1, dtype=torch.int32)
            indices = dense_mask.argsort(dim=-1, descending=True, stable=True).to(torch.int32)
            return num_blocks[None, None].contiguous(), indices[None, None].contiguous()

        def create_doc_swc_block_mask(sliding_window_num_blocks):
            kv_idx = block_idx = torch.arange(total_num_blocks, dtype=torch.int32, device='cuda')
            q_idx = block_idx[:, None]
            causal_bm = q_idx >= kv_idx
            causal_full_bm = q_idx > kv_idx
            window_bm = q_idx - kv_idx < sliding_window_num_blocks
            window_full_bm = window_bm # block-wise sliding window by @YouJiacheng
            # document_bm = (docs_low[q_idx] <= docs_high[kv_idx]) & (docs_low[kv_idx] <= docs_high[q_idx])
            document_bm = (docs_low[:, None] <= docs_high) & (docs_low <= docs_high[:, None])
            document_full_bm = (docs_low[:, None] == docs_high) & (docs_low == docs_high[:, None])
            nonzero_bm = causal_bm & window_bm & document_bm
            full_bm  = causal_full_bm & window_full_bm & document_full_bm
            kv_num_blocks, kv_indices = dense_to_ordered(nonzero_bm & ~full_bm)
            full_kv_num_blocks, full_kv_indices = dense_to_ordered(full_bm)
            return BlockMask.from_kv_blocks(
                kv_num_blocks,
                kv_indices,
                full_kv_num_blocks,
                full_kv_indices,
                BLOCK_SIZE=BLOCK_SIZE,
                mask_mod=document_causal,
            )

        block_mask = create_doc_swc_block_mask(sliding_window_num_blocks)

        x0 = norm(self.embed(inputs[None]).bfloat16()) # use of norm here by @Grad62304977
        x = x0
        ve = self.value_embeds(inputs)
        assert len(ve) == len(self.blocks)
        ve_enc, ve_dec = ve[:self.num_encoder_layers], ve[self.num_encoder_layers:]

        # Store outputs for U-Net skip connections
        skip_connections = []
        # Encoder pass - process only the first half of the blocks
        for i in range(self.num_encoder_layers):
            x = self.blocks[i](x, ve_enc[i], x0, block_mask)
            skip_connections.append(x)
        # Decoder pass - process the remaining blocks with weighted skip connections
        for i in range(self.num_decoder_layers):
            x = x + self.skip_weights[i] * skip_connections.pop()
            # U-net structure on token value embeddings by @leloykun
            x = self.blocks[self.num_encoder_layers + i](x, ve_dec[i], x0, block_mask)

        x = norm(x)
        logits = self.lm_head(x)
        logits = 15 * torch.tanh(logits / 15) # @Grad62304977 added tanh softcapping, @KoszarskyB reduced it from 30 to 15
        logits = logits.float()
        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets)
        return loss

# -----------------------------------------------------------------------------
# Our own simple Distributed Data Loader

def _load_data_shard(path):
    # only reads the header, returns header data
    # header is 256 int32
    header = torch.from_file(path, False, 256, dtype=torch.int32)
    assert header[0] == 20240520, 'magic number mismatch in the data .bin file'
    assert header[1] == 1, 'unsupported version'
    num_tokens = int(header[2]) # number of tokens (claimed)
    with open(path, 'rb', buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, 'number of tokens read does not match header'
    return tokens

class DistributedDataLoader:

    def __init__(self, filename_pattern):
        self.rank = int(os.environ['RANK'])
        self.world_size = int(os.environ['WORLD_SIZE'])
        self.files = sorted(glob.glob(filename_pattern))
        self.reset()

    def reset(self):
        self.current_shard = -1
        self.advance()

    def advance(self):
        self.current_shard = (self.current_shard + 1) % len(self.files)
        self.current_position = 0
        self.tokens = _load_data_shard(self.files[self.current_shard])

    def next_batch(self, batch_size):
        assert batch_size % self.world_size == 0
        device_batch_size = batch_size // self.world_size
        # load next shard if necessary
        if self.current_position + batch_size + 1 >= len(self.tokens):
            self.advance()
        pos = self.current_position + self.rank * device_batch_size
        device_batch_tokens = self.tokens[pos:pos+device_batch_size+1]
        # advance current position
        self.current_position += batch_size
        inputs = device_batch_tokens[:-1].to(device='cuda', dtype=torch.int32, non_blocking=True)
        targets = device_batch_tokens[1:].to(device='cuda', dtype=torch.int64, non_blocking=True)
        return inputs, targets

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files = 'data/fineweb10B/fineweb_train_*.bin' # input .bin to train on
    val_files = 'data/fineweb10B/fineweb_val_*.bin' # input .bin to eval validation loss on
    val_tokens = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # optimization
    batch_size = 8*64*1024 # batch size in tokens
    num_iterations = 1370 # number of iterations to run
    cooldown_frac = 0.4 # fraction of training spent cooling down the learning rate
    bf16_embeds = True
    # evaluation and logging
    val_loss_every = 125 # every how many steps to evaluate val loss? 0 for only at the end
    # implementation
    max_device_batch_size = 64*1024 # batch size per device in tokens
    save_checkpoint = False
args = Hyperparameters()

micro_bs = args.max_device_batch_size

# set up DDP (distributed data parallel). torchrun sets this env variable
rank = int(os.environ['RANK'])
local_rank = int(os.environ['LOCAL_RANK'])
world_size = int(os.environ['WORLD_SIZE'])
assert torch.cuda.is_available()
torch.cuda.set_device(local_rank)
dist.init_process_group(backend='nccl', device_id=torch.device(local_rank))
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = uuid.uuid4()
    os.makedirs('logs', exist_ok=True)
    logfile = f'logs/{run_id}.txt'
    print(logfile)

def print0(s, console=False):
    if master_process:
        with open(logfile, 'a') as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0('='*100)
# log information about the hardware/software environment this is running on
print0(f'Running Python {sys.version}')
print0(f'Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}')
print0(subprocess.run(['nvidia-smi'], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout)
print0('='*100)

# load data
train_loader = DistributedDataLoader(args.train_files)
val_loader = DistributedDataLoader(args.val_files)
print0(f'Training dataloader files: {train_loader.files}')
print0(f'Validation dataloader files: {val_loader.files}')
print0('='*100)

# there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency. suggested to me by @Grad62304977.
# this originates from Karpathy's experiments.
model = GPT(vocab_size=50304, num_layers=12, num_heads=6, model_dim=768)
model = model.cuda()
if args.bf16_embeds:
    for m in model.modules():
        if isinstance(m, nn.Embedding):
            m.bfloat16()
model: GPT = torch.compile(model)
ddp_model = DDP(model, device_ids=[local_rank], broadcast_buffers=False, gradient_as_bucket_view=True)

# collect the parameters to optimize
hidden_matrix_params = [p for p in model.blocks.parameters() if p.ndim == 2]
embed_params = [model.embed.weight, *model.value_embeds.parameters()]
scalar_params = [p for n, p in model.named_parameters() if p.ndim < 2 and "attn_scale" not in n]
attn_scale_params = [p for n, p in model.named_parameters() if p.ndim < 2 and "attn_scale" in n]
head_params = [model.lm_head.weight]

# init the optimizer(s)
optimizer1 = torch.optim.Adam([dict(params=embed_params, lr=0.6),
                               dict(params=head_params, lr=0.008),
                               dict(params=scalar_params, lr=0.04),
                               dict(params=attn_scale_params, lr=0.01)],
                              betas=(0.8, 0.95), fused=True)
optimizer2 = Muon(hidden_matrix_params, lr=0.05, momentum=0.95)
optimizers = [optimizer1, optimizer2]

# learning rate schedule: stable then decay
def get_lr(it):
    t = 1 - it / args.num_iterations # time remaining in training
    assert 1 >= t > 0
    # 1) constant lr for first part of training
    if t >= args.cooldown_frac:
        return 1.0
    # 2) then linear cooldown
    else:
        return t / args.cooldown_frac
schedulers = [torch.optim.lr_scheduler.LambdaLR(opt, get_lr) for opt in optimizers]

# sliding window size schedule: linear increase over training in chunks of 128 from 128 -> 1792. By @fernbear.bsky.social
def get_sliding_window_blocks(it):
    x = it / args.num_iterations # training progress
    assert 0 <= x <= 1
    return int(((1 - x) * 128 + x * 1856) // 128)
sliding_window_num_blocks = torch.tensor(1, dtype=torch.int32, device='cuda')

# Start training loop
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    # This effectively ignores timing first 10 steps, which are slower for weird reasons.
    # Alternately, and slightly more correctly in terms of benchmarking, we could do 10
    # steps with dummy data first, and then re-initialize the model and reset the loader.
    if step == 10:
        training_time_ms = 0
        t0 = time.perf_counter()
    timed_steps = float('nan') if step <= 11 else (step - 10) + 1 # <= 11 to avoid bug in val

    sliding_window_num_blocks.copy_(get_sliding_window_blocks(step))

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        # run validation batches
        model.eval()
        val_loader.reset()
        val_loss = 0.0
        # calculate the number of steps to take in the val loop.
        val_batch_size = world_size * micro_bs
        assert args.val_tokens % val_batch_size == 0
        val_steps = args.val_tokens // val_batch_size
        for _ in range(val_steps):
            with torch.no_grad():
                inputs_val, targets_val = val_loader.next_batch(val_batch_size)
                val_loss += ddp_model(inputs_val, targets_val, sliding_window_num_blocks)
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        val_loss /= val_steps
        # logging
        print0(f'step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/(timed_steps-1):.2f}ms', console=True)
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f'logs/{run_id}', exist_ok=True)
            torch.save(log, f'logs/{run_id}/state_step{step:06d}.pt')
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    model.train()
    batch_size = args.batch_size
    assert batch_size % world_size == 0
    inputs_train, targets_train = train_loader.next_batch(batch_size)
    assert len(inputs_train) <= micro_bs or len(inputs_train) % micro_bs == 0
    for micro_inputs_train, micro_targets_train in zip(inputs_train.split(micro_bs), targets_train.split(micro_bs)):
        ddp_model(micro_inputs_train, micro_targets_train, sliding_window_num_blocks).backward()
    # momentum warmup for Muon
    frac = min(step/300, 1)
    for group in optimizer2.param_groups:
        group['momentum'] = (1 - frac) * 0.85 + frac * 0.95
    # step the optimizers and schedulers
    for opt, sched in zip(optimizers, schedulers):
        opt.step()
        if step != train_steps-1:
            sched.step()
    # null the gradients
    model.zero_grad(set_to_none=True)
    # logging
    approx_time = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f'step:{step+1}/{train_steps} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms', console=True)

print0(f'peak memory consumption: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB')
dist.destroy_process_group()

====================================================================================================
Running Python 3.10.12 (main, Sep 11 2024, 15:47:36) [GCC 11.4.0]
Running PyTorch 2.6.0.dev20241231+cu126 compiled for CUDA 12.6
Sat Jan 11 22:54:15 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.183.06             Driver Version: 535.183.06   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:19:00.0 Off |                    0 |
| N/A   33C    P0             122W / 700W |   7713MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:3B:00.0 Off |                    0 |
| N/A   30C    P0             113W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:4C:00.0 Off |                    0 |
| N/A   27C    P0             117W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   32C    P0             124W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:9B:00.0 Off |                    0 |
| N/A   33C    P0             120W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:BB:00.0 Off |                    0 |
| N/A   28C    P0             113W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:CB:00.0 Off |                    0 |
| N/A   32C    P0             112W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:DB:00.0 Off |                    0 |
| N/A   28C    P0             115W / 700W |   3211MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
Training dataloader files: ['data/fineweb10B/fineweb_train_000001.bin', 'data/fineweb10B/fineweb_train_000002.bin', 'data/fineweb10B/fineweb_train_000003.bin', 'data/fineweb10B/fineweb_train_000004.bin', 'data/fineweb10B/fineweb_train_000005.bin', 'data/fineweb10B/fineweb_train_000006.bin', 'data/fineweb10B/fineweb_train_000007.bin', 'data/fineweb10B/fineweb_train_000008.bin', 'data/fineweb10B/fineweb_train_000009.bin']
Validation dataloader files: ['data/fineweb10B/fineweb_val_000000.bin']
====================================================================================================
step:0/1370 val_loss:10.8258 train_time:0ms step_avg:nanms
step:1/1370 train_time:29644ms step_avg:nanms
step:2/1370 train_time:29716ms step_avg:nanms
step:3/1370 train_time:29904ms step_avg:nanms
step:4/1370 train_time:30037ms step_avg:nanms
step:5/1370 train_time:30171ms step_avg:nanms
step:6/1370 train_time:30305ms step_avg:nanms
step:7/1370 train_time:30438ms step_avg:nanms
step:8/1370 train_time:30571ms step_avg:nanms
step:9/1370 train_time:30706ms step_avg:nanms
step:10/1370 train_time:30850ms step_avg:nanms
step:11/1370 train_time:138ms step_avg:nanms
step:12/1370 train_time:275ms step_avg:nanms
step:13/1370 train_time:410ms step_avg:136.52ms
step:14/1370 train_time:545ms step_avg:136.17ms
step:15/1370 train_time:679ms step_avg:135.73ms
step:16/1370 train_time:814ms step_avg:135.73ms
step:17/1370 train_time:951ms step_avg:135.86ms
step:18/1370 train_time:1089ms step_avg:136.06ms
step:19/1370 train_time:1227ms step_avg:136.28ms
step:20/1370 train_time:1362ms step_avg:136.16ms
step:21/1370 train_time:1498ms step_avg:136.16ms
step:22/1370 train_time:1634ms step_avg:136.19ms
step:23/1370 train_time:1768ms step_avg:135.97ms
step:24/1370 train_time:1904ms step_avg:136.02ms
step:25/1370 train_time:2041ms step_avg:136.08ms
step:26/1370 train_time:2180ms step_avg:136.27ms
step:27/1370 train_time:2317ms step_avg:136.29ms
step:28/1370 train_time:2453ms step_avg:136.28ms
step:29/1370 train_time:2588ms step_avg:136.20ms
step:30/1370 train_time:2725ms step_avg:136.23ms
step:31/1370 train_time:2860ms step_avg:136.18ms
step:32/1370 train_time:2995ms step_avg:136.15ms
step:33/1370 train_time:3132ms step_avg:136.18ms
step:34/1370 train_time:3268ms step_avg:136.16ms
step:35/1370 train_time:3405ms step_avg:136.19ms
step:36/1370 train_time:3541ms step_avg:136.18ms
step:37/1370 train_time:3677ms step_avg:136.19ms
step:38/1370 train_time:3812ms step_avg:136.15ms
step:39/1370 train_time:3949ms step_avg:136.17ms
step:40/1370 train_time:4086ms step_avg:136.19ms
step:41/1370 train_time:4221ms step_avg:136.17ms
step:42/1370 train_time:4357ms step_avg:136.17ms
step:43/1370 train_time:4493ms step_avg:136.15ms
step:44/1370 train_time:4631ms step_avg:136.19ms
step:45/1370 train_time:4764ms step_avg:136.12ms
step:46/1370 train_time:4901ms step_avg:136.13ms
step:47/1370 train_time:5038ms step_avg:136.16ms
step:48/1370 train_time:5175ms step_avg:136.18ms
step:49/1370 train_time:5311ms step_avg:136.18ms
step:50/1370 train_time:5447ms step_avg:136.16ms
step:51/1370 train_time:5583ms step_avg:136.17ms
step:52/1370 train_time:5718ms step_avg:136.15ms
step:53/1370 train_time:5854ms step_avg:136.15ms
step:54/1370 train_time:5990ms step_avg:136.13ms
step:55/1370 train_time:6127ms step_avg:136.16ms
step:56/1370 train_time:6262ms step_avg:136.13ms
step:57/1370 train_time:6398ms step_avg:136.13ms
step:58/1370 train_time:6534ms step_avg:136.13ms
step:59/1370 train_time:6669ms step_avg:136.10ms
step:60/1370 train_time:6806ms step_avg:136.12ms
step:61/1370 train_time:6943ms step_avg:136.13ms
step:62/1370 train_time:7080ms step_avg:136.15ms
step:63/1370 train_time:7217ms step_avg:136.18ms
step:64/1370 train_time:7353ms step_avg:136.17ms
step:65/1370 train_time:7489ms step_avg:136.17ms
step:66/1370 train_time:7626ms step_avg:136.18ms
step:67/1370 train_time:7761ms step_avg:136.16ms
step:68/1370 train_time:7897ms step_avg:136.16ms
step:69/1370 train_time:8035ms step_avg:136.19ms
step:70/1370 train_time:8172ms step_avg:136.21ms
step:71/1370 train_time:8309ms step_avg:136.22ms
step:72/1370 train_time:8444ms step_avg:136.20ms
step:73/1370 train_time:8582ms step_avg:136.22ms
step:74/1370 train_time:8719ms step_avg:136.23ms
step:75/1370 train_time:8855ms step_avg:136.24ms
step:76/1370 train_time:8991ms step_avg:136.23ms
step:77/1370 train_time:9129ms step_avg:136.26ms
step:78/1370 train_time:9265ms step_avg:136.25ms
step:79/1370 train_time:9402ms step_avg:136.26ms
step:80/1370 train_time:9538ms step_avg:136.26ms
step:81/1370 train_time:9675ms step_avg:136.27ms
step:82/1370 train_time:9811ms step_avg:136.26ms
step:83/1370 train_time:9950ms step_avg:136.29ms
step:84/1370 train_time:10085ms step_avg:136.29ms
step:85/1370 train_time:10221ms step_avg:136.28ms
step:86/1370 train_time:10357ms step_avg:136.28ms
step:87/1370 train_time:10492ms step_avg:136.26ms
step:88/1370 train_time:10630ms step_avg:136.28ms
step:89/1370 train_time:10765ms step_avg:136.26ms
step:90/1370 train_time:10902ms step_avg:136.27ms
step:91/1370 train_time:11039ms step_avg:136.28ms
step:92/1370 train_time:11177ms step_avg:136.30ms
step:93/1370 train_time:11314ms step_avg:136.31ms
step:94/1370 train_time:11451ms step_avg:136.32ms
step:95/1370 train_time:11588ms step_avg:136.33ms
step:96/1370 train_time:11725ms step_avg:136.33ms
step:97/1370 train_time:11859ms step_avg:136.32ms
step:98/1370 train_time:11995ms step_avg:136.31ms
step:99/1370 train_time:12133ms step_avg:136.32ms
step:100/1370 train_time:12268ms step_avg:136.31ms
step:101/1370 train_time:12404ms step_avg:136.31ms
step:102/1370 train_time:12542ms step_avg:136.32ms
step:103/1370 train_time:12678ms step_avg:136.33ms
step:104/1370 train_time:12818ms step_avg:136.36ms
step:105/1370 train_time:12956ms step_avg:136.38ms
step:106/1370 train_time:13096ms step_avg:136.41ms
step:107/1370 train_time:13237ms step_avg:136.46ms
step:108/1370 train_time:13377ms step_avg:136.50ms
step:109/1370 train_time:13518ms step_avg:136.55ms
step:110/1370 train_time:13658ms step_avg:136.58ms
step:111/1370 train_time:13796ms step_avg:136.60ms
step:112/1370 train_time:13936ms step_avg:136.63ms
step:113/1370 train_time:14075ms step_avg:136.65ms
step:114/1370 train_time:14215ms step_avg:136.68ms
step:115/1370 train_time:14354ms step_avg:136.71ms
step:116/1370 train_time:14493ms step_avg:136.72ms
step:117/1370 train_time:14633ms step_avg:136.76ms
step:118/1370 train_time:14772ms step_avg:136.78ms
step:119/1370 train_time:14913ms step_avg:136.82ms
step:120/1370 train_time:15054ms step_avg:136.86ms
step:121/1370 train_time:15194ms step_avg:136.88ms
step:122/1370 train_time:15335ms step_avg:136.92ms
step:123/1370 train_time:15474ms step_avg:136.94ms
step:124/1370 train_time:15613ms step_avg:136.96ms
step:125/1370 train_time:15754ms step_avg:136.99ms
step:125/1370 val_loss:4.3875 train_time:15816ms step_avg:137.53ms
step:126/1370 train_time:15895ms step_avg:137.02ms
step:127/1370 train_time:16038ms step_avg:137.07ms
step:128/1370 train_time:16178ms step_avg:137.10ms
step:129/1370 train_time:16315ms step_avg:137.10ms
step:130/1370 train_time:16455ms step_avg:137.12ms
step:131/1370 train_time:16592ms step_avg:137.12ms
step:132/1370 train_time:16730ms step_avg:137.13ms
step:133/1370 train_time:16871ms step_avg:137.17ms
step:134/1370 train_time:17012ms step_avg:137.19ms
step:135/1370 train_time:17152ms step_avg:137.22ms
step:136/1370 train_time:17291ms step_avg:137.23ms
step:137/1370 train_time:17430ms step_avg:137.24ms
step:138/1370 train_time:17569ms step_avg:137.26ms
step:139/1370 train_time:17709ms step_avg:137.28ms
step:140/1370 train_time:17850ms step_avg:137.31ms
step:141/1370 train_time:17990ms step_avg:137.33ms
step:142/1370 train_time:18133ms step_avg:137.37ms
step:143/1370 train_time:18273ms step_avg:137.39ms
step:144/1370 train_time:18412ms step_avg:137.41ms
step:145/1370 train_time:18552ms step_avg:137.42ms
step:146/1370 train_time:18692ms step_avg:137.44ms
step:147/1370 train_time:18830ms step_avg:137.45ms
step:148/1370 train_time:18971ms step_avg:137.47ms
step:149/1370 train_time:19112ms step_avg:137.50ms
step:150/1370 train_time:19253ms step_avg:137.52ms
step:151/1370 train_time:19391ms step_avg:137.52ms
step:152/1370 train_time:19532ms step_avg:137.55ms
step:153/1370 train_time:19672ms step_avg:137.57ms
step:154/1370 train_time:19811ms step_avg:137.58ms
step:155/1370 train_time:19951ms step_avg:137.59ms
step:156/1370 train_time:20090ms step_avg:137.60ms
step:157/1370 train_time:20231ms step_avg:137.62ms
step:158/1370 train_time:20371ms step_avg:137.64ms
step:159/1370 train_time:20511ms step_avg:137.66ms
step:160/1370 train_time:20652ms step_avg:137.68ms
step:161/1370 train_time:20790ms step_avg:137.68ms
step:162/1370 train_time:20930ms step_avg:137.70ms
step:163/1370 train_time:21071ms step_avg:137.72ms
step:164/1370 train_time:21211ms step_avg:137.73ms
step:165/1370 train_time:21352ms step_avg:137.75ms
step:166/1370 train_time:21490ms step_avg:137.76ms
step:167/1370 train_time:21630ms step_avg:137.77ms
step:168/1370 train_time:21771ms step_avg:137.79ms
step:169/1370 train_time:21910ms step_avg:137.80ms
step:170/1370 train_time:22050ms step_avg:137.81ms
step:171/1370 train_time:22190ms step_avg:137.83ms
step:172/1370 train_time:22331ms step_avg:137.84ms
step:173/1370 train_time:22472ms step_avg:137.86ms
step:174/1370 train_time:22612ms step_avg:137.88ms
step:175/1370 train_time:22752ms step_avg:137.89ms
step:176/1370 train_time:22891ms step_avg:137.90ms
step:177/1370 train_time:23031ms step_avg:137.91ms
step:178/1370 train_time:23172ms step_avg:137.93ms
step:179/1370 train_time:23312ms step_avg:137.94ms
step:180/1370 train_time:23452ms step_avg:137.95ms
step:181/1370 train_time:23591ms step_avg:137.96ms
step:182/1370 train_time:23732ms step_avg:137.98ms
step:183/1370 train_time:23872ms step_avg:137.99ms
step:184/1370 train_time:24012ms step_avg:138.00ms
step:185/1370 train_time:24153ms step_avg:138.01ms
step:186/1370 train_time:24292ms step_avg:138.02ms
step:187/1370 train_time:24432ms step_avg:138.03ms
step:188/1370 train_time:24572ms step_avg:138.04ms
step:189/1370 train_time:24712ms step_avg:138.05ms
step:190/1370 train_time:24852ms step_avg:138.07ms
step:191/1370 train_time:25030ms step_avg:138.29ms
step:192/1370 train_time:25170ms step_avg:138.29ms
step:193/1370 train_time:25309ms step_avg:138.30ms
step:194/1370 train_time:25448ms step_avg:138.30ms
step:195/1370 train_time:25586ms step_avg:138.30ms
step:196/1370 train_time:25725ms step_avg:138.30ms
step:197/1370 train_time:25868ms step_avg:138.33ms
step:198/1370 train_time:26010ms step_avg:138.35ms
step:199/1370 train_time:26150ms step_avg:138.36ms
step:200/1370 train_time:26290ms step_avg:138.37ms
step:201/1370 train_time:26429ms step_avg:138.37ms
step:202/1370 train_time:26568ms step_avg:138.38ms
step:203/1370 train_time:26708ms step_avg:138.39ms
step:204/1370 train_time:26851ms step_avg:138.41ms
step:205/1370 train_time:26994ms step_avg:138.43ms
step:206/1370 train_time:27139ms step_avg:138.46ms
step:207/1370 train_time:27283ms step_avg:138.49ms
step:208/1370 train_time:27423ms step_avg:138.50ms
step:209/1370 train_time:27567ms step_avg:138.53ms
step:210/1370 train_time:27708ms step_avg:138.54ms
step:211/1370 train_time:27851ms step_avg:138.56ms
step:212/1370 train_time:27993ms step_avg:138.58ms
step:213/1370 train_time:28137ms step_avg:138.61ms
step:214/1370 train_time:28281ms step_avg:138.63ms
step:215/1370 train_time:28423ms step_avg:138.65ms
step:216/1370 train_time:28567ms step_avg:138.67ms
step:217/1370 train_time:28708ms step_avg:138.68ms
step:218/1370 train_time:28850ms step_avg:138.70ms
step:219/1370 train_time:28993ms step_avg:138.72ms
step:220/1370 train_time:29136ms step_avg:138.74ms
step:221/1370 train_time:29278ms step_avg:138.76ms
step:222/1370 train_time:29421ms step_avg:138.78ms
step:223/1370 train_time:29566ms step_avg:138.81ms
step:224/1370 train_time:29707ms step_avg:138.82ms
step:225/1370 train_time:29850ms step_avg:138.84ms
step:226/1370 train_time:29992ms step_avg:138.85ms
step:227/1370 train_time:30135ms step_avg:138.87ms
step:228/1370 train_time:30279ms step_avg:138.90ms
step:229/1370 train_time:30422ms step_avg:138.91ms
step:230/1370 train_time:30566ms step_avg:138.94ms
step:231/1370 train_time:30708ms step_avg:138.95ms
step:232/1370 train_time:30851ms step_avg:138.97ms
step:233/1370 train_time:30992ms step_avg:138.98ms
step:234/1370 train_time:31137ms step_avg:139.00ms
step:235/1370 train_time:31282ms step_avg:139.03ms
step:236/1370 train_time:31425ms step_avg:139.05ms
step:237/1370 train_time:31567ms step_avg:139.06ms
step:238/1370 train_time:31708ms step_avg:139.07ms
step:239/1370 train_time:31850ms step_avg:139.08ms
step:240/1370 train_time:31993ms step_avg:139.10ms
step:241/1370 train_time:32136ms step_avg:139.12ms
step:242/1370 train_time:32281ms step_avg:139.14ms
step:243/1370 train_time:32423ms step_avg:139.16ms
step:244/1370 train_time:32567ms step_avg:139.17ms
step:245/1370 train_time:32708ms step_avg:139.18ms
step:246/1370 train_time:32850ms step_avg:139.20ms
step:247/1370 train_time:32992ms step_avg:139.21ms
step:248/1370 train_time:33135ms step_avg:139.22ms
step:249/1370 train_time:33279ms step_avg:139.24ms
step:250/1370 train_time:33424ms step_avg:139.27ms
step:250/1370 val_loss:3.9589 train_time:33489ms step_avg:139.54ms
step:251/1370 train_time:33570ms step_avg:139.30ms
step:252/1370 train_time:33715ms step_avg:139.32ms
step:253/1370 train_time:33859ms step_avg:139.34ms
step:254/1370 train_time:33999ms step_avg:139.34ms
step:255/1370 train_time:34141ms step_avg:139.35ms
step:256/1370 train_time:34283ms step_avg:139.36ms
step:257/1370 train_time:34424ms step_avg:139.37ms
step:258/1370 train_time:34570ms step_avg:139.39ms
step:259/1370 train_time:34714ms step_avg:139.41ms
step:260/1370 train_time:34857ms step_avg:139.43ms
step:261/1370 train_time:34998ms step_avg:139.43ms
step:262/1370 train_time:35139ms step_avg:139.44ms
step:263/1370 train_time:35282ms step_avg:139.45ms
step:264/1370 train_time:35424ms step_avg:139.47ms
step:265/1370 train_time:35569ms step_avg:139.49ms
step:266/1370 train_time:35713ms step_avg:139.50ms
step:267/1370 train_time:35857ms step_avg:139.52ms
step:268/1370 train_time:35999ms step_avg:139.53ms
step:269/1370 train_time:36142ms step_avg:139.54ms
step:270/1370 train_time:36284ms step_avg:139.55ms
step:271/1370 train_time:36426ms step_avg:139.56ms
step:272/1370 train_time:36570ms step_avg:139.58ms
step:273/1370 train_time:36712ms step_avg:139.59ms
step:274/1370 train_time:36854ms step_avg:139.60ms
step:275/1370 train_time:36998ms step_avg:139.61ms
step:276/1370 train_time:37142ms step_avg:139.63ms
step:277/1370 train_time:37286ms step_avg:139.65ms
step:278/1370 train_time:37428ms step_avg:139.66ms
step:279/1370 train_time:37572ms step_avg:139.67ms
step:280/1370 train_time:37714ms step_avg:139.68ms
step:281/1370 train_time:37857ms step_avg:139.69ms
step:282/1370 train_time:37999ms step_avg:139.70ms
step:283/1370 train_time:38143ms step_avg:139.72ms
step:284/1370 train_time:38287ms step_avg:139.73ms
step:285/1370 train_time:38430ms step_avg:139.74ms
step:286/1370 train_time:38572ms step_avg:139.76ms
step:287/1370 train_time:38714ms step_avg:139.76ms
step:288/1370 train_time:38857ms step_avg:139.77ms
step:289/1370 train_time:38998ms step_avg:139.78ms
step:290/1370 train_time:39142ms step_avg:139.79ms
step:291/1370 train_time:39285ms step_avg:139.80ms
step:292/1370 train_time:39429ms step_avg:139.82ms
step:293/1370 train_time:39574ms step_avg:139.84ms
step:294/1370 train_time:39717ms step_avg:139.85ms
step:295/1370 train_time:39861ms step_avg:139.86ms
step:296/1370 train_time:40004ms step_avg:139.87ms
step:297/1370 train_time:40148ms step_avg:139.89ms
step:298/1370 train_time:40290ms step_avg:139.90ms
step:299/1370 train_time:40432ms step_avg:139.90ms
step:300/1370 train_time:40576ms step_avg:139.92ms
step:301/1370 train_time:40718ms step_avg:139.92ms
step:302/1370 train_time:40861ms step_avg:139.94ms
step:303/1370 train_time:41003ms step_avg:139.94ms
step:304/1370 train_time:41146ms step_avg:139.95ms
step:305/1370 train_time:41289ms step_avg:139.96ms
step:306/1370 train_time:41432ms step_avg:139.97ms
step:307/1370 train_time:41579ms step_avg:140.00ms
step:308/1370 train_time:41724ms step_avg:140.01ms
step:309/1370 train_time:41870ms step_avg:140.03ms
step:310/1370 train_time:42014ms step_avg:140.05ms
step:311/1370 train_time:42160ms step_avg:140.06ms
step:312/1370 train_time:42304ms step_avg:140.08ms
step:313/1370 train_time:42451ms step_avg:140.10ms
step:314/1370 train_time:42596ms step_avg:140.12ms
step:315/1370 train_time:42741ms step_avg:140.13ms
step:316/1370 train_time:42885ms step_avg:140.15ms
step:317/1370 train_time:43030ms step_avg:140.16ms
step:318/1370 train_time:43175ms step_avg:140.18ms
step:319/1370 train_time:43321ms step_avg:140.20ms
step:320/1370 train_time:43466ms step_avg:140.21ms
step:321/1370 train_time:43610ms step_avg:140.23ms
step:322/1370 train_time:43755ms step_avg:140.24ms
step:323/1370 train_time:43900ms step_avg:140.25ms
step:324/1370 train_time:44045ms step_avg:140.27ms
step:325/1370 train_time:44191ms step_avg:140.29ms
step:326/1370 train_time:44337ms step_avg:140.31ms
step:327/1370 train_time:44483ms step_avg:140.32ms
step:328/1370 train_time:44627ms step_avg:140.34ms
step:329/1370 train_time:44773ms step_avg:140.35ms
step:330/1370 train_time:44918ms step_avg:140.37ms
step:331/1370 train_time:45063ms step_avg:140.38ms
step:332/1370 train_time:45208ms step_avg:140.40ms
step:333/1370 train_time:45353ms step_avg:140.41ms
step:334/1370 train_time:45499ms step_avg:140.43ms
step:335/1370 train_time:45645ms step_avg:140.45ms
step:336/1370 train_time:45792ms step_avg:140.47ms
step:337/1370 train_time:45937ms step_avg:140.48ms
step:338/1370 train_time:46082ms step_avg:140.49ms
step:339/1370 train_time:46226ms step_avg:140.51ms
step:340/1370 train_time:46372ms step_avg:140.52ms
step:341/1370 train_time:46515ms step_avg:140.53ms
step:342/1370 train_time:46661ms step_avg:140.55ms
step:343/1370 train_time:46806ms step_avg:140.56ms
step:344/1370 train_time:46953ms step_avg:140.58ms
step:345/1370 train_time:47098ms step_avg:140.59ms
step:346/1370 train_time:47243ms step_avg:140.60ms
step:347/1370 train_time:47388ms step_avg:140.62ms
step:348/1370 train_time:47532ms step_avg:140.63ms
step:349/1370 train_time:47678ms step_avg:140.64ms
step:350/1370 train_time:47822ms step_avg:140.65ms
step:351/1370 train_time:47967ms step_avg:140.67ms
step:352/1370 train_time:48111ms step_avg:140.68ms
step:353/1370 train_time:48257ms step_avg:140.69ms
step:354/1370 train_time:48401ms step_avg:140.70ms
step:355/1370 train_time:48547ms step_avg:140.72ms
step:356/1370 train_time:48693ms step_avg:140.73ms
step:357/1370 train_time:48838ms step_avg:140.74ms
step:358/1370 train_time:48983ms step_avg:140.76ms
step:359/1370 train_time:49129ms step_avg:140.77ms
step:360/1370 train_time:49274ms step_avg:140.78ms
step:361/1370 train_time:49419ms step_avg:140.79ms
step:362/1370 train_time:49565ms step_avg:140.81ms
step:363/1370 train_time:49709ms step_avg:140.82ms
step:364/1370 train_time:49854ms step_avg:140.83ms
step:365/1370 train_time:49999ms step_avg:140.84ms
step:366/1370 train_time:50145ms step_avg:140.86ms
step:367/1370 train_time:50289ms step_avg:140.87ms
step:368/1370 train_time:50432ms step_avg:140.87ms
step:369/1370 train_time:50578ms step_avg:140.89ms
step:370/1370 train_time:50724ms step_avg:140.90ms
step:371/1370 train_time:50869ms step_avg:140.91ms
step:372/1370 train_time:51014ms step_avg:140.92ms
step:373/1370 train_time:51160ms step_avg:140.94ms
step:374/1370 train_time:51305ms step_avg:140.95ms
step:375/1370 train_time:51451ms step_avg:140.96ms
step:375/1370 val_loss:3.7767 train_time:51517ms step_avg:141.14ms
step:376/1370 train_time:51598ms step_avg:140.98ms
step:377/1370 train_time:51742ms step_avg:140.99ms
step:378/1370 train_time:51886ms step_avg:140.99ms
step:379/1370 train_time:52030ms step_avg:141.00ms
step:380/1370 train_time:52173ms step_avg:141.01ms
step:381/1370 train_time:52360ms step_avg:141.13ms
step:382/1370 train_time:52503ms step_avg:141.14ms
step:383/1370 train_time:52649ms step_avg:141.15ms
step:384/1370 train_time:52791ms step_avg:141.15ms
step:385/1370 train_time:52935ms step_avg:141.16ms
step:386/1370 train_time:53078ms step_avg:141.17ms
step:387/1370 train_time:53224ms step_avg:141.18ms
step:388/1370 train_time:53373ms step_avg:141.20ms
step:389/1370 train_time:53519ms step_avg:141.21ms
step:390/1370 train_time:53664ms step_avg:141.22ms
step:391/1370 train_time:53806ms step_avg:141.22ms
step:392/1370 train_time:53950ms step_avg:141.23ms
step:393/1370 train_time:54094ms step_avg:141.24ms
step:394/1370 train_time:54240ms step_avg:141.25ms
step:395/1370 train_time:54385ms step_avg:141.26ms
step:396/1370 train_time:54531ms step_avg:141.27ms
step:397/1370 train_time:54677ms step_avg:141.28ms
step:398/1370 train_time:54822ms step_avg:141.29ms
step:399/1370 train_time:54967ms step_avg:141.30ms
step:400/1370 train_time:55111ms step_avg:141.31ms
step:401/1370 train_time:55257ms step_avg:141.32ms
step:402/1370 train_time:55401ms step_avg:141.33ms
step:403/1370 train_time:55547ms step_avg:141.34ms
step:404/1370 train_time:55692ms step_avg:141.35ms
step:405/1370 train_time:55837ms step_avg:141.36ms
step:406/1370 train_time:55982ms step_avg:141.37ms
step:407/1370 train_time:56127ms step_avg:141.38ms
step:408/1370 train_time:56274ms step_avg:141.39ms
step:409/1370 train_time:56420ms step_avg:141.40ms
step:410/1370 train_time:56568ms step_avg:141.42ms
step:411/1370 train_time:56713ms step_avg:141.43ms
step:412/1370 train_time:56861ms step_avg:141.44ms
step:413/1370 train_time:57007ms step_avg:141.46ms
step:414/1370 train_time:57155ms step_avg:141.47ms
step:415/1370 train_time:57301ms step_avg:141.48ms
step:416/1370 train_time:57448ms step_avg:141.50ms
step:417/1370 train_time:57595ms step_avg:141.51ms
step:418/1370 train_time:57741ms step_avg:141.52ms
step:419/1370 train_time:57887ms step_avg:141.53ms
step:420/1370 train_time:58035ms step_avg:141.55ms
step:421/1370 train_time:58181ms step_avg:141.56ms
step:422/1370 train_time:58328ms step_avg:141.57ms
step:423/1370 train_time:58477ms step_avg:141.59ms
step:424/1370 train_time:58622ms step_avg:141.60ms
step:425/1370 train_time:58770ms step_avg:141.61ms
step:426/1370 train_time:58916ms step_avg:141.62ms
step:427/1370 train_time:59062ms step_avg:141.64ms
step:428/1370 train_time:59210ms step_avg:141.65ms
step:429/1370 train_time:59356ms step_avg:141.66ms
step:430/1370 train_time:59501ms step_avg:141.67ms
step:431/1370 train_time:59649ms step_avg:141.68ms
step:432/1370 train_time:59796ms step_avg:141.70ms
step:433/1370 train_time:59942ms step_avg:141.71ms
step:434/1370 train_time:60089ms step_avg:141.72ms
step:435/1370 train_time:60236ms step_avg:141.73ms
step:436/1370 train_time:60382ms step_avg:141.74ms
step:437/1370 train_time:60529ms step_avg:141.75ms
step:438/1370 train_time:60676ms step_avg:141.77ms
step:439/1370 train_time:60821ms step_avg:141.77ms
step:440/1370 train_time:60968ms step_avg:141.79ms
step:441/1370 train_time:61115ms step_avg:141.80ms
step:442/1370 train_time:61263ms step_avg:141.81ms
step:443/1370 train_time:61411ms step_avg:141.83ms
step:444/1370 train_time:61556ms step_avg:141.83ms
step:445/1370 train_time:61701ms step_avg:141.84ms
step:446/1370 train_time:61849ms step_avg:141.86ms
step:447/1370 train_time:61995ms step_avg:141.87ms
step:448/1370 train_time:62143ms step_avg:141.88ms
step:449/1370 train_time:62291ms step_avg:141.89ms
step:450/1370 train_time:62439ms step_avg:141.91ms
step:451/1370 train_time:62585ms step_avg:141.92ms
step:452/1370 train_time:62731ms step_avg:141.93ms
step:453/1370 train_time:62879ms step_avg:141.94ms
step:454/1370 train_time:63026ms step_avg:141.95ms
step:455/1370 train_time:63173ms step_avg:141.96ms
step:456/1370 train_time:63319ms step_avg:141.97ms
step:457/1370 train_time:63467ms step_avg:141.98ms
step:458/1370 train_time:63612ms step_avg:141.99ms
step:459/1370 train_time:63760ms step_avg:142.00ms
step:460/1370 train_time:63905ms step_avg:142.01ms
step:461/1370 train_time:64052ms step_avg:142.02ms
step:462/1370 train_time:64199ms step_avg:142.03ms
step:463/1370 train_time:64346ms step_avg:142.04ms
step:464/1370 train_time:64492ms step_avg:142.05ms
step:465/1370 train_time:64638ms step_avg:142.06ms
step:466/1370 train_time:64785ms step_avg:142.07ms
step:467/1370 train_time:64933ms step_avg:142.09ms
step:468/1370 train_time:65080ms step_avg:142.10ms
step:469/1370 train_time:65227ms step_avg:142.11ms
step:470/1370 train_time:65373ms step_avg:142.12ms
step:471/1370 train_time:65519ms step_avg:142.12ms
step:472/1370 train_time:65667ms step_avg:142.14ms
step:473/1370 train_time:65813ms step_avg:142.14ms
step:474/1370 train_time:65961ms step_avg:142.16ms
step:475/1370 train_time:66107ms step_avg:142.17ms
step:476/1370 train_time:66254ms step_avg:142.18ms
step:477/1370 train_time:66400ms step_avg:142.18ms
step:478/1370 train_time:66547ms step_avg:142.19ms
step:479/1370 train_time:66693ms step_avg:142.20ms
step:480/1370 train_time:66839ms step_avg:142.21ms
step:481/1370 train_time:66986ms step_avg:142.22ms
step:482/1370 train_time:67132ms step_avg:142.23ms
step:483/1370 train_time:67278ms step_avg:142.24ms
step:484/1370 train_time:67424ms step_avg:142.25ms
step:485/1370 train_time:67572ms step_avg:142.26ms
step:486/1370 train_time:67719ms step_avg:142.27ms
step:487/1370 train_time:67866ms step_avg:142.28ms
step:488/1370 train_time:68012ms step_avg:142.29ms
step:489/1370 train_time:68159ms step_avg:142.29ms
step:490/1370 train_time:68305ms step_avg:142.30ms
step:491/1370 train_time:68453ms step_avg:142.31ms
step:492/1370 train_time:68599ms step_avg:142.32ms
step:493/1370 train_time:68745ms step_avg:142.33ms
step:494/1370 train_time:68892ms step_avg:142.34ms
step:495/1370 train_time:69040ms step_avg:142.35ms
step:496/1370 train_time:69187ms step_avg:142.36ms
step:497/1370 train_time:69333ms step_avg:142.37ms
step:498/1370 train_time:69479ms step_avg:142.38ms
step:499/1370 train_time:69625ms step_avg:142.38ms
step:500/1370 train_time:69771ms step_avg:142.39ms
step:500/1370 val_loss:3.6589 train_time:69840ms step_avg:142.53ms
step:501/1370 train_time:69921ms step_avg:142.40ms
step:502/1370 train_time:70069ms step_avg:142.42ms
step:503/1370 train_time:70215ms step_avg:142.42ms
step:504/1370 train_time:70363ms step_avg:142.43ms
step:505/1370 train_time:70508ms step_avg:142.44ms
step:506/1370 train_time:70654ms step_avg:142.45ms
step:507/1370 train_time:70800ms step_avg:142.45ms
step:508/1370 train_time:70949ms step_avg:142.47ms
step:509/1370 train_time:71097ms step_avg:142.48ms
step:510/1370 train_time:71245ms step_avg:142.49ms
step:511/1370 train_time:71394ms step_avg:142.50ms
step:512/1370 train_time:71544ms step_avg:142.52ms
step:513/1370 train_time:71692ms step_avg:142.53ms
step:514/1370 train_time:71840ms step_avg:142.54ms
step:515/1370 train_time:71988ms step_avg:142.55ms
step:516/1370 train_time:72137ms step_avg:142.56ms
step:517/1370 train_time:72285ms step_avg:142.57ms
step:518/1370 train_time:72434ms step_avg:142.59ms
step:519/1370 train_time:72582ms step_avg:142.60ms
step:520/1370 train_time:72731ms step_avg:142.61ms
step:521/1370 train_time:72879ms step_avg:142.62ms
step:522/1370 train_time:73027ms step_avg:142.63ms
step:523/1370 train_time:73174ms step_avg:142.64ms
step:524/1370 train_time:73324ms step_avg:142.65ms
step:525/1370 train_time:73471ms step_avg:142.66ms
step:526/1370 train_time:73620ms step_avg:142.67ms
step:527/1370 train_time:73767ms step_avg:142.68ms
step:528/1370 train_time:73915ms step_avg:142.69ms
step:529/1370 train_time:74064ms step_avg:142.71ms
step:530/1370 train_time:74213ms step_avg:142.72ms
step:531/1370 train_time:74361ms step_avg:142.73ms
step:532/1370 train_time:74509ms step_avg:142.74ms
step:533/1370 train_time:74657ms step_avg:142.75ms
step:534/1370 train_time:74803ms step_avg:142.75ms
step:535/1370 train_time:74955ms step_avg:142.77ms
step:536/1370 train_time:75104ms step_avg:142.78ms
step:537/1370 train_time:75253ms step_avg:142.79ms
step:538/1370 train_time:75401ms step_avg:142.80ms
step:539/1370 train_time:75550ms step_avg:142.82ms
step:540/1370 train_time:75699ms step_avg:142.83ms
step:541/1370 train_time:75847ms step_avg:142.84ms
step:542/1370 train_time:75997ms step_avg:142.85ms
step:543/1370 train_time:76146ms step_avg:142.86ms
step:544/1370 train_time:76296ms step_avg:142.88ms
step:545/1370 train_time:76444ms step_avg:142.89ms
step:546/1370 train_time:76593ms step_avg:142.90ms
step:547/1370 train_time:76742ms step_avg:142.91ms
step:548/1370 train_time:76889ms step_avg:142.92ms
step:549/1370 train_time:77038ms step_avg:142.93ms
step:550/1370 train_time:77186ms step_avg:142.94ms
step:551/1370 train_time:77335ms step_avg:142.95ms
step:552/1370 train_time:77483ms step_avg:142.96ms
step:553/1370 train_time:77631ms step_avg:142.97ms
step:554/1370 train_time:77778ms step_avg:142.97ms
step:555/1370 train_time:77927ms step_avg:142.99ms
step:556/1370 train_time:78074ms step_avg:142.99ms
step:557/1370 train_time:78224ms step_avg:143.01ms
step:558/1370 train_time:78372ms step_avg:143.01ms
step:559/1370 train_time:78520ms step_avg:143.02ms
step:560/1370 train_time:78668ms step_avg:143.03ms
step:561/1370 train_time:78817ms step_avg:143.04ms
step:562/1370 train_time:78965ms step_avg:143.05ms
step:563/1370 train_time:79114ms step_avg:143.06ms
step:564/1370 train_time:79262ms step_avg:143.07ms
step:565/1370 train_time:79409ms step_avg:143.08ms
step:566/1370 train_time:79559ms step_avg:143.09ms
step:567/1370 train_time:79706ms step_avg:143.10ms
step:568/1370 train_time:79855ms step_avg:143.11ms
step:569/1370 train_time:80004ms step_avg:143.12ms
step:570/1370 train_time:80152ms step_avg:143.13ms
step:571/1370 train_time:80333ms step_avg:143.20ms
step:572/1370 train_time:80484ms step_avg:143.21ms
step:573/1370 train_time:80632ms step_avg:143.22ms
step:574/1370 train_time:80781ms step_avg:143.23ms
step:575/1370 train_time:80928ms step_avg:143.24ms
step:576/1370 train_time:81076ms step_avg:143.24ms
step:577/1370 train_time:81225ms step_avg:143.25ms
step:578/1370 train_time:81378ms step_avg:143.27ms
step:579/1370 train_time:81528ms step_avg:143.28ms
step:580/1370 train_time:81677ms step_avg:143.29ms
step:581/1370 train_time:81823ms step_avg:143.30ms
step:582/1370 train_time:81971ms step_avg:143.31ms
step:583/1370 train_time:82118ms step_avg:143.31ms
step:584/1370 train_time:82268ms step_avg:143.32ms
step:585/1370 train_time:82417ms step_avg:143.33ms
step:586/1370 train_time:82565ms step_avg:143.34ms
step:587/1370 train_time:82714ms step_avg:143.35ms
step:588/1370 train_time:82861ms step_avg:143.36ms
step:589/1370 train_time:83009ms step_avg:143.37ms
step:590/1370 train_time:83158ms step_avg:143.38ms
step:591/1370 train_time:83305ms step_avg:143.38ms
step:592/1370 train_time:83456ms step_avg:143.40ms
step:593/1370 train_time:83604ms step_avg:143.40ms
step:594/1370 train_time:83753ms step_avg:143.41ms
step:595/1370 train_time:83901ms step_avg:143.42ms
step:596/1370 train_time:84049ms step_avg:143.43ms
step:597/1370 train_time:84198ms step_avg:143.44ms
step:598/1370 train_time:84347ms step_avg:143.45ms
step:599/1370 train_time:84497ms step_avg:143.46ms
step:600/1370 train_time:84647ms step_avg:143.47ms
step:601/1370 train_time:84796ms step_avg:143.48ms
step:602/1370 train_time:84944ms step_avg:143.49ms
step:603/1370 train_time:85093ms step_avg:143.50ms
step:604/1370 train_time:85241ms step_avg:143.50ms
step:605/1370 train_time:85388ms step_avg:143.51ms
step:606/1370 train_time:85539ms step_avg:143.52ms
step:607/1370 train_time:85687ms step_avg:143.53ms
step:608/1370 train_time:85836ms step_avg:143.54ms
step:609/1370 train_time:85983ms step_avg:143.54ms
step:610/1370 train_time:86133ms step_avg:143.56ms
step:611/1370 train_time:86281ms step_avg:143.56ms
step:612/1370 train_time:86430ms step_avg:143.57ms
step:613/1370 train_time:86581ms step_avg:143.58ms
step:614/1370 train_time:86731ms step_avg:143.59ms
step:615/1370 train_time:86880ms step_avg:143.60ms
step:616/1370 train_time:87029ms step_avg:143.61ms
step:617/1370 train_time:87179ms step_avg:143.62ms
step:618/1370 train_time:87328ms step_avg:143.63ms
step:619/1370 train_time:87478ms step_avg:143.64ms
step:620/1370 train_time:87629ms step_avg:143.65ms
step:621/1370 train_time:87779ms step_avg:143.67ms
step:622/1370 train_time:87928ms step_avg:143.67ms
step:623/1370 train_time:88077ms step_avg:143.68ms
step:624/1370 train_time:88226ms step_avg:143.69ms
step:625/1370 train_time:88376ms step_avg:143.70ms
step:625/1370 val_loss:3.5756 train_time:88446ms step_avg:143.82ms
step:626/1370 train_time:88527ms step_avg:143.71ms
step:627/1370 train_time:88679ms step_avg:143.73ms
step:628/1370 train_time:88826ms step_avg:143.73ms
step:629/1370 train_time:88975ms step_avg:143.74ms
step:630/1370 train_time:89123ms step_avg:143.75ms
step:631/1370 train_time:89271ms step_avg:143.75ms
step:632/1370 train_time:89422ms step_avg:143.77ms
step:633/1370 train_time:89574ms step_avg:143.78ms
step:634/1370 train_time:89725ms step_avg:143.79ms
step:635/1370 train_time:89875ms step_avg:143.80ms
step:636/1370 train_time:90024ms step_avg:143.81ms
step:637/1370 train_time:90174ms step_avg:143.82ms
step:638/1370 train_time:90323ms step_avg:143.83ms
step:639/1370 train_time:90474ms step_avg:143.84ms
step:640/1370 train_time:90625ms step_avg:143.85ms
step:641/1370 train_time:90775ms step_avg:143.86ms
step:642/1370 train_time:90926ms step_avg:143.87ms
step:643/1370 train_time:91077ms step_avg:143.88ms
step:644/1370 train_time:91225ms step_avg:143.89ms
step:645/1370 train_time:91375ms step_avg:143.90ms
step:646/1370 train_time:91524ms step_avg:143.91ms
step:647/1370 train_time:91675ms step_avg:143.92ms
step:648/1370 train_time:91828ms step_avg:143.93ms
step:649/1370 train_time:91981ms step_avg:143.95ms
step:650/1370 train_time:92132ms step_avg:143.96ms
step:651/1370 train_time:92282ms step_avg:143.97ms
step:652/1370 train_time:92433ms step_avg:143.98ms
step:653/1370 train_time:92583ms step_avg:143.99ms
step:654/1370 train_time:92733ms step_avg:143.99ms
step:655/1370 train_time:92882ms step_avg:144.00ms
step:656/1370 train_time:93032ms step_avg:144.01ms
step:657/1370 train_time:93182ms step_avg:144.02ms
step:658/1370 train_time:93332ms step_avg:144.03ms
step:659/1370 train_time:93483ms step_avg:144.04ms
step:660/1370 train_time:93633ms step_avg:144.05ms
step:661/1370 train_time:93785ms step_avg:144.06ms
step:662/1370 train_time:93933ms step_avg:144.07ms
step:663/1370 train_time:94083ms step_avg:144.08ms
step:664/1370 train_time:94232ms step_avg:144.09ms
step:665/1370 train_time:94382ms step_avg:144.09ms
step:666/1370 train_time:94530ms step_avg:144.10ms
step:667/1370 train_time:94682ms step_avg:144.11ms
step:668/1370 train_time:94832ms step_avg:144.12ms
step:669/1370 train_time:94983ms step_avg:144.13ms
step:670/1370 train_time:95132ms step_avg:144.14ms
step:671/1370 train_time:95282ms step_avg:144.15ms
step:672/1370 train_time:95431ms step_avg:144.15ms
step:673/1370 train_time:95580ms step_avg:144.16ms
step:674/1370 train_time:95730ms step_avg:144.17ms
step:675/1370 train_time:95880ms step_avg:144.18ms
step:676/1370 train_time:96030ms step_avg:144.19ms
step:677/1370 train_time:96182ms step_avg:144.20ms
step:678/1370 train_time:96329ms step_avg:144.21ms
step:679/1370 train_time:96480ms step_avg:144.21ms
step:680/1370 train_time:96630ms step_avg:144.22ms
step:681/1370 train_time:96780ms step_avg:144.23ms
step:682/1370 train_time:96929ms step_avg:144.24ms
step:683/1370 train_time:97079ms step_avg:144.25ms
step:684/1370 train_time:97229ms step_avg:144.26ms
step:685/1370 train_time:97379ms step_avg:144.27ms
step:686/1370 train_time:97526ms step_avg:144.27ms
step:687/1370 train_time:97676ms step_avg:144.28ms
step:688/1370 train_time:97828ms step_avg:144.29ms
step:689/1370 train_time:97977ms step_avg:144.30ms
step:690/1370 train_time:98129ms step_avg:144.31ms
step:691/1370 train_time:98278ms step_avg:144.31ms
step:692/1370 train_time:98428ms step_avg:144.32ms
step:693/1370 train_time:98577ms step_avg:144.33ms
step:694/1370 train_time:98727ms step_avg:144.34ms
step:695/1370 train_time:98876ms step_avg:144.34ms
step:696/1370 train_time:99026ms step_avg:144.35ms
step:697/1370 train_time:99176ms step_avg:144.36ms
step:698/1370 train_time:99325ms step_avg:144.37ms
step:699/1370 train_time:99475ms step_avg:144.38ms
step:700/1370 train_time:99624ms step_avg:144.38ms
step:701/1370 train_time:99774ms step_avg:144.39ms
step:702/1370 train_time:99923ms step_avg:144.40ms
step:703/1370 train_time:100072ms step_avg:144.40ms
step:704/1370 train_time:100224ms step_avg:144.42ms
step:705/1370 train_time:100375ms step_avg:144.42ms
step:706/1370 train_time:100527ms step_avg:144.44ms
step:707/1370 train_time:100677ms step_avg:144.44ms
step:708/1370 train_time:100825ms step_avg:144.45ms
step:709/1370 train_time:100976ms step_avg:144.46ms
step:710/1370 train_time:101127ms step_avg:144.47ms
step:711/1370 train_time:101279ms step_avg:144.48ms
step:712/1370 train_time:101430ms step_avg:144.49ms
step:713/1370 train_time:101582ms step_avg:144.50ms
step:714/1370 train_time:101731ms step_avg:144.50ms
step:715/1370 train_time:101883ms step_avg:144.51ms
step:716/1370 train_time:102034ms step_avg:144.52ms
step:717/1370 train_time:102186ms step_avg:144.53ms
step:718/1370 train_time:102336ms step_avg:144.54ms
step:719/1370 train_time:102488ms step_avg:144.55ms
step:720/1370 train_time:102640ms step_avg:144.56ms
step:721/1370 train_time:102790ms step_avg:144.57ms
step:722/1370 train_time:102942ms step_avg:144.58ms
step:723/1370 train_time:103095ms step_avg:144.59ms
step:724/1370 train_time:103246ms step_avg:144.60ms
step:725/1370 train_time:103397ms step_avg:144.61ms
step:726/1370 train_time:103547ms step_avg:144.62ms
step:727/1370 train_time:103702ms step_avg:144.63ms
step:728/1370 train_time:103851ms step_avg:144.64ms
step:729/1370 train_time:104002ms step_avg:144.65ms
step:730/1370 train_time:104154ms step_avg:144.66ms
step:731/1370 train_time:104304ms step_avg:144.67ms
step:732/1370 train_time:104455ms step_avg:144.67ms
step:733/1370 train_time:104605ms step_avg:144.68ms
step:734/1370 train_time:104756ms step_avg:144.69ms
step:735/1370 train_time:104908ms step_avg:144.70ms
step:736/1370 train_time:105059ms step_avg:144.71ms
step:737/1370 train_time:105211ms step_avg:144.72ms
step:738/1370 train_time:105361ms step_avg:144.73ms
step:739/1370 train_time:105514ms step_avg:144.74ms
step:740/1370 train_time:105665ms step_avg:144.75ms
step:741/1370 train_time:105816ms step_avg:144.75ms
step:742/1370 train_time:105967ms step_avg:144.76ms
step:743/1370 train_time:106118ms step_avg:144.77ms
step:744/1370 train_time:106267ms step_avg:144.78ms
step:745/1370 train_time:106422ms step_avg:144.79ms
step:746/1370 train_time:106571ms step_avg:144.80ms
step:747/1370 train_time:106722ms step_avg:144.81ms
step:748/1370 train_time:106872ms step_avg:144.81ms
step:749/1370 train_time:107023ms step_avg:144.82ms
step:750/1370 train_time:107174ms step_avg:144.83ms
step:750/1370 val_loss:3.5222 train_time:107247ms step_avg:144.93ms
step:751/1370 train_time:107331ms step_avg:144.85ms
step:752/1370 train_time:107482ms step_avg:144.85ms
step:753/1370 train_time:107632ms step_avg:144.86ms
step:754/1370 train_time:107781ms step_avg:144.87ms
step:755/1370 train_time:107931ms step_avg:144.87ms
step:756/1370 train_time:108080ms step_avg:144.88ms
step:757/1370 train_time:108233ms step_avg:144.89ms
step:758/1370 train_time:108385ms step_avg:144.90ms
step:759/1370 train_time:108537ms step_avg:144.91ms
step:760/1370 train_time:108685ms step_avg:144.91ms
step:761/1370 train_time:108880ms step_avg:144.98ms
step:762/1370 train_time:109030ms step_avg:144.99ms
step:763/1370 train_time:109180ms step_avg:144.99ms
step:764/1370 train_time:109330ms step_avg:145.00ms
step:765/1370 train_time:109479ms step_avg:145.01ms
step:766/1370 train_time:109632ms step_avg:145.02ms
step:767/1370 train_time:109785ms step_avg:145.03ms
step:768/1370 train_time:109938ms step_avg:145.04ms
step:769/1370 train_time:110090ms step_avg:145.05ms
step:770/1370 train_time:110241ms step_avg:145.05ms
step:771/1370 train_time:110393ms step_avg:145.06ms
step:772/1370 train_time:110542ms step_avg:145.07ms
step:773/1370 train_time:110693ms step_avg:145.08ms
step:774/1370 train_time:110845ms step_avg:145.09ms
step:775/1370 train_time:110998ms step_avg:145.10ms
step:776/1370 train_time:111151ms step_avg:145.11ms
step:777/1370 train_time:111303ms step_avg:145.12ms
step:778/1370 train_time:111453ms step_avg:145.12ms
step:779/1370 train_time:111601ms step_avg:145.13ms
step:780/1370 train_time:111754ms step_avg:145.13ms
step:781/1370 train_time:111905ms step_avg:145.14ms
step:782/1370 train_time:112058ms step_avg:145.15ms
step:783/1370 train_time:112208ms step_avg:145.16ms
step:784/1370 train_time:112361ms step_avg:145.17ms
step:785/1370 train_time:112511ms step_avg:145.18ms
step:786/1370 train_time:112662ms step_avg:145.18ms
step:787/1370 train_time:112813ms step_avg:145.19ms
step:788/1370 train_time:112965ms step_avg:145.20ms
step:789/1370 train_time:113114ms step_avg:145.20ms
step:790/1370 train_time:113266ms step_avg:145.21ms
step:791/1370 train_time:113416ms step_avg:145.22ms
step:792/1370 train_time:113568ms step_avg:145.23ms
step:793/1370 train_time:113717ms step_avg:145.23ms
step:794/1370 train_time:113870ms step_avg:145.24ms
step:795/1370 train_time:114024ms step_avg:145.25ms
step:796/1370 train_time:114176ms step_avg:145.26ms
step:797/1370 train_time:114329ms step_avg:145.27ms
step:798/1370 train_time:114479ms step_avg:145.28ms
step:799/1370 train_time:114633ms step_avg:145.29ms
step:800/1370 train_time:114784ms step_avg:145.30ms
step:801/1370 train_time:114936ms step_avg:145.30ms
step:802/1370 train_time:115088ms step_avg:145.31ms
step:803/1370 train_time:115239ms step_avg:145.32ms
step:804/1370 train_time:115390ms step_avg:145.33ms
step:805/1370 train_time:115545ms step_avg:145.34ms
step:806/1370 train_time:115696ms step_avg:145.35ms
step:807/1370 train_time:115845ms step_avg:145.35ms
step:808/1370 train_time:115995ms step_avg:145.36ms
step:809/1370 train_time:116147ms step_avg:145.37ms
step:810/1370 train_time:116297ms step_avg:145.37ms
step:811/1370 train_time:116447ms step_avg:145.38ms
step:812/1370 train_time:116599ms step_avg:145.39ms
step:813/1370 train_time:116749ms step_avg:145.39ms
step:814/1370 train_time:116900ms step_avg:145.40ms
step:815/1370 train_time:117053ms step_avg:145.41ms
step:816/1370 train_time:117205ms step_avg:145.42ms
step:817/1370 train_time:117358ms step_avg:145.42ms
step:818/1370 train_time:117509ms step_avg:145.43ms
step:819/1370 train_time:117664ms step_avg:145.44ms
step:820/1370 train_time:117818ms step_avg:145.45ms
step:821/1370 train_time:117971ms step_avg:145.46ms
step:822/1370 train_time:118122ms step_avg:145.47ms
step:823/1370 train_time:118276ms step_avg:145.48ms
step:824/1370 train_time:118428ms step_avg:145.49ms
step:825/1370 train_time:118582ms step_avg:145.50ms
step:826/1370 train_time:118736ms step_avg:145.51ms
step:827/1370 train_time:118888ms step_avg:145.52ms
step:828/1370 train_time:119040ms step_avg:145.53ms
step:829/1370 train_time:119192ms step_avg:145.53ms
step:830/1370 train_time:119344ms step_avg:145.54ms
step:831/1370 train_time:119497ms step_avg:145.55ms
step:832/1370 train_time:119651ms step_avg:145.56ms
step:833/1370 train_time:119801ms step_avg:145.57ms
step:834/1370 train_time:119954ms step_avg:145.58ms
step:835/1370 train_time:120107ms step_avg:145.58ms
step:836/1370 train_time:120259ms step_avg:145.59ms
step:837/1370 train_time:120410ms step_avg:145.60ms
step:838/1370 train_time:120562ms step_avg:145.61ms
step:839/1370 train_time:120714ms step_avg:145.61ms
step:840/1370 train_time:120866ms step_avg:145.62ms
step:841/1370 train_time:121018ms step_avg:145.63ms
step:842/1370 train_time:121171ms step_avg:145.64ms
step:843/1370 train_time:121322ms step_avg:145.64ms
step:844/1370 train_time:121475ms step_avg:145.65ms
step:845/1370 train_time:121625ms step_avg:145.66ms
step:846/1370 train_time:121779ms step_avg:145.67ms
step:847/1370 train_time:121932ms step_avg:145.68ms
step:848/1370 train_time:122083ms step_avg:145.68ms
step:849/1370 train_time:122238ms step_avg:145.69ms
step:850/1370 train_time:122390ms step_avg:145.70ms
step:851/1370 train_time:122544ms step_avg:145.71ms
step:852/1370 train_time:122697ms step_avg:145.72ms
step:853/1370 train_time:122849ms step_avg:145.73ms
step:854/1370 train_time:123000ms step_avg:145.73ms
step:855/1370 train_time:123152ms step_avg:145.74ms
step:856/1370 train_time:123303ms step_avg:145.75ms
step:857/1370 train_time:123458ms step_avg:145.76ms
step:858/1370 train_time:123613ms step_avg:145.77ms
step:859/1370 train_time:123765ms step_avg:145.78ms
step:860/1370 train_time:123916ms step_avg:145.78ms
step:861/1370 train_time:124069ms step_avg:145.79ms
step:862/1370 train_time:124223ms step_avg:145.80ms
step:863/1370 train_time:124376ms step_avg:145.81ms
step:864/1370 train_time:124530ms step_avg:145.82ms
step:865/1370 train_time:124679ms step_avg:145.82ms
step:866/1370 train_time:124836ms step_avg:145.84ms
step:867/1370 train_time:124988ms step_avg:145.84ms
step:868/1370 train_time:125140ms step_avg:145.85ms
step:869/1370 train_time:125291ms step_avg:145.86ms
step:870/1370 train_time:125446ms step_avg:145.87ms
step:871/1370 train_time:125598ms step_avg:145.87ms
step:872/1370 train_time:125751ms step_avg:145.88ms
step:873/1370 train_time:125901ms step_avg:145.89ms
step:874/1370 train_time:126056ms step_avg:145.90ms
step:875/1370 train_time:126207ms step_avg:145.90ms
step:875/1370 val_loss:3.4685 train_time:126278ms step_avg:145.99ms
step:876/1370 train_time:126361ms step_avg:145.91ms
step:877/1370 train_time:126516ms step_avg:145.92ms
step:878/1370 train_time:126667ms step_avg:145.93ms
step:879/1370 train_time:126819ms step_avg:145.94ms
step:880/1370 train_time:126970ms step_avg:145.94ms
step:881/1370 train_time:127120ms step_avg:145.95ms
step:882/1370 train_time:127274ms step_avg:145.96ms
step:883/1370 train_time:127428ms step_avg:145.97ms
step:884/1370 train_time:127582ms step_avg:145.97ms
step:885/1370 train_time:127733ms step_avg:145.98ms
step:886/1370 train_time:127889ms step_avg:145.99ms
step:887/1370 train_time:128039ms step_avg:146.00ms
step:888/1370 train_time:128194ms step_avg:146.01ms
step:889/1370 train_time:128349ms step_avg:146.02ms
step:890/1370 train_time:128500ms step_avg:146.02ms
step:891/1370 train_time:128651ms step_avg:146.03ms
step:892/1370 train_time:128805ms step_avg:146.04ms
step:893/1370 train_time:128957ms step_avg:146.04ms
step:894/1370 train_time:129111ms step_avg:146.05ms
step:895/1370 train_time:129266ms step_avg:146.06ms
step:896/1370 train_time:129419ms step_avg:146.07ms
step:897/1370 train_time:129570ms step_avg:146.08ms
step:898/1370 train_time:129723ms step_avg:146.08ms
step:899/1370 train_time:129874ms step_avg:146.09ms
step:900/1370 train_time:130026ms step_avg:146.10ms
step:901/1370 train_time:130181ms step_avg:146.11ms
step:902/1370 train_time:130331ms step_avg:146.11ms
step:903/1370 train_time:130483ms step_avg:146.12ms
step:904/1370 train_time:130636ms step_avg:146.12ms
step:905/1370 train_time:130789ms step_avg:146.13ms
step:906/1370 train_time:130940ms step_avg:146.14ms
step:907/1370 train_time:131097ms step_avg:146.15ms
step:908/1370 train_time:131248ms step_avg:146.16ms
step:909/1370 train_time:131402ms step_avg:146.16ms
step:910/1370 train_time:131559ms step_avg:146.18ms
step:911/1370 train_time:131711ms step_avg:146.18ms
step:912/1370 train_time:131861ms step_avg:146.19ms
step:913/1370 train_time:132016ms step_avg:146.20ms
step:914/1370 train_time:132169ms step_avg:146.20ms
step:915/1370 train_time:132322ms step_avg:146.21ms
step:916/1370 train_time:132476ms step_avg:146.22ms
step:917/1370 train_time:132629ms step_avg:146.23ms
step:918/1370 train_time:132784ms step_avg:146.24ms
step:919/1370 train_time:132937ms step_avg:146.25ms
step:920/1370 train_time:133091ms step_avg:146.25ms
step:921/1370 train_time:133246ms step_avg:146.26ms
step:922/1370 train_time:133404ms step_avg:146.28ms
step:923/1370 train_time:133556ms step_avg:146.28ms
step:924/1370 train_time:133710ms step_avg:146.29ms
step:925/1370 train_time:133864ms step_avg:146.30ms
step:926/1370 train_time:134016ms step_avg:146.31ms
step:927/1370 train_time:134169ms step_avg:146.31ms
step:928/1370 train_time:134325ms step_avg:146.32ms
step:929/1370 train_time:134480ms step_avg:146.33ms
step:930/1370 train_time:134634ms step_avg:146.34ms
step:931/1370 train_time:134787ms step_avg:146.35ms
step:932/1370 train_time:134938ms step_avg:146.35ms
step:933/1370 train_time:135092ms step_avg:146.36ms
step:934/1370 train_time:135247ms step_avg:146.37ms
step:935/1370 train_time:135401ms step_avg:146.38ms
step:936/1370 train_time:135555ms step_avg:146.39ms
step:937/1370 train_time:135711ms step_avg:146.40ms
step:938/1370 train_time:135865ms step_avg:146.41ms
step:939/1370 train_time:136018ms step_avg:146.41ms
step:940/1370 train_time:136171ms step_avg:146.42ms
step:941/1370 train_time:136326ms step_avg:146.43ms
step:942/1370 train_time:136478ms step_avg:146.44ms
step:943/1370 train_time:136634ms step_avg:146.45ms
step:944/1370 train_time:136793ms step_avg:146.46ms
step:945/1370 train_time:136948ms step_avg:146.47ms
step:946/1370 train_time:137103ms step_avg:146.48ms
step:947/1370 train_time:137257ms step_avg:146.49ms
step:948/1370 train_time:137411ms step_avg:146.49ms
step:949/1370 train_time:137568ms step_avg:146.50ms
step:950/1370 train_time:137724ms step_avg:146.51ms
step:951/1370 train_time:137920ms step_avg:146.57ms
step:952/1370 train_time:138071ms step_avg:146.57ms
step:953/1370 train_time:138226ms step_avg:146.58ms
step:954/1370 train_time:138379ms step_avg:146.59ms
step:955/1370 train_time:138530ms step_avg:146.59ms
step:956/1370 train_time:138684ms step_avg:146.60ms
step:957/1370 train_time:138838ms step_avg:146.61ms
step:958/1370 train_time:138997ms step_avg:146.62ms
step:959/1370 train_time:139152ms step_avg:146.63ms
step:960/1370 train_time:139307ms step_avg:146.64ms
step:961/1370 train_time:139458ms step_avg:146.64ms
step:962/1370 train_time:139611ms step_avg:146.65ms
step:963/1370 train_time:139767ms step_avg:146.66ms
step:964/1370 train_time:139921ms step_avg:146.67ms
step:965/1370 train_time:140074ms step_avg:146.67ms
step:966/1370 train_time:140228ms step_avg:146.68ms
step:967/1370 train_time:140379ms step_avg:146.69ms
step:968/1370 train_time:140531ms step_avg:146.69ms
step:969/1370 train_time:140685ms step_avg:146.70ms
step:970/1370 train_time:140836ms step_avg:146.70ms
step:971/1370 train_time:140993ms step_avg:146.71ms
step:972/1370 train_time:141146ms step_avg:146.72ms
step:973/1370 train_time:141299ms step_avg:146.73ms
step:974/1370 train_time:141452ms step_avg:146.73ms
step:975/1370 train_time:141607ms step_avg:146.74ms
step:976/1370 train_time:141760ms step_avg:146.75ms
step:977/1370 train_time:141913ms step_avg:146.76ms
step:978/1370 train_time:142068ms step_avg:146.76ms
step:979/1370 train_time:142222ms step_avg:146.77ms
step:980/1370 train_time:142376ms step_avg:146.78ms
step:981/1370 train_time:142528ms step_avg:146.79ms
step:982/1370 train_time:142682ms step_avg:146.79ms
step:983/1370 train_time:142835ms step_avg:146.80ms
step:984/1370 train_time:142989ms step_avg:146.81ms
step:985/1370 train_time:143143ms step_avg:146.81ms
step:986/1370 train_time:143299ms step_avg:146.82ms
step:987/1370 train_time:143452ms step_avg:146.83ms
step:988/1370 train_time:143604ms step_avg:146.83ms
step:989/1370 train_time:143756ms step_avg:146.84ms
step:990/1370 train_time:143910ms step_avg:146.85ms
step:991/1370 train_time:144063ms step_avg:146.85ms
step:992/1370 train_time:144221ms step_avg:146.86ms
step:993/1370 train_time:144386ms step_avg:146.88ms
step:994/1370 train_time:144538ms step_avg:146.89ms
step:995/1370 train_time:144689ms step_avg:146.89ms
step:996/1370 train_time:144840ms step_avg:146.90ms
step:997/1370 train_time:144991ms step_avg:146.90ms
step:998/1370 train_time:145144ms step_avg:146.91ms
step:999/1370 train_time:145299ms step_avg:146.91ms
step:1000/1370 train_time:145452ms step_avg:146.92ms
step:1000/1370 val_loss:3.4031 train_time:145523ms step_avg:146.99ms
step:1001/1370 train_time:145607ms step_avg:146.93ms
step:1002/1370 train_time:145763ms step_avg:146.94ms
step:1003/1370 train_time:145916ms step_avg:146.94ms
step:1004/1370 train_time:146072ms step_avg:146.95ms
step:1005/1370 train_time:146225ms step_avg:146.96ms
step:1006/1370 train_time:146375ms step_avg:146.96ms
step:1007/1370 train_time:146530ms step_avg:146.97ms
step:1008/1370 train_time:146684ms step_avg:146.98ms
step:1009/1370 train_time:146845ms step_avg:146.99ms
step:1010/1370 train_time:146998ms step_avg:147.00ms
step:1011/1370 train_time:147152ms step_avg:147.00ms
step:1012/1370 train_time:147304ms step_avg:147.01ms
step:1013/1370 train_time:147458ms step_avg:147.02ms
step:1014/1370 train_time:147611ms step_avg:147.02ms
step:1015/1370 train_time:147767ms step_avg:147.03ms
step:1016/1370 train_time:147921ms step_avg:147.04ms
step:1017/1370 train_time:148075ms step_avg:147.05ms
step:1018/1370 train_time:148230ms step_avg:147.05ms
step:1019/1370 train_time:148387ms step_avg:147.06ms
step:1020/1370 train_time:148545ms step_avg:147.07ms
step:1021/1370 train_time:148698ms step_avg:147.08ms
step:1022/1370 train_time:148852ms step_avg:147.09ms
step:1023/1370 train_time:149007ms step_avg:147.09ms
step:1024/1370 train_time:149163ms step_avg:147.10ms
step:1025/1370 train_time:149317ms step_avg:147.11ms
step:1026/1370 train_time:149471ms step_avg:147.12ms
step:1027/1370 train_time:149624ms step_avg:147.12ms
step:1028/1370 train_time:149779ms step_avg:147.13ms
step:1029/1370 train_time:149935ms step_avg:147.14ms
step:1030/1370 train_time:150090ms step_avg:147.15ms
step:1031/1370 train_time:150242ms step_avg:147.15ms
step:1032/1370 train_time:150395ms step_avg:147.16ms
step:1033/1370 train_time:150550ms step_avg:147.17ms
step:1034/1370 train_time:150705ms step_avg:147.17ms
step:1035/1370 train_time:150864ms step_avg:147.18ms
step:1036/1370 train_time:151019ms step_avg:147.19ms
step:1037/1370 train_time:151173ms step_avg:147.20ms
step:1038/1370 train_time:151327ms step_avg:147.21ms
step:1039/1370 train_time:151479ms step_avg:147.21ms
step:1040/1370 train_time:151632ms step_avg:147.22ms
step:1041/1370 train_time:151788ms step_avg:147.22ms
step:1042/1370 train_time:151940ms step_avg:147.23ms
step:1043/1370 train_time:152094ms step_avg:147.23ms
step:1044/1370 train_time:152251ms step_avg:147.24ms
step:1045/1370 train_time:152406ms step_avg:147.25ms
step:1046/1370 train_time:152562ms step_avg:147.26ms
step:1047/1370 train_time:152715ms step_avg:147.27ms
step:1048/1370 train_time:152871ms step_avg:147.28ms
step:1049/1370 train_time:153026ms step_avg:147.28ms
step:1050/1370 train_time:153183ms step_avg:147.29ms
step:1051/1370 train_time:153338ms step_avg:147.30ms
step:1052/1370 train_time:153492ms step_avg:147.31ms
step:1053/1370 train_time:153646ms step_avg:147.31ms
step:1054/1370 train_time:153803ms step_avg:147.32ms
step:1055/1370 train_time:153957ms step_avg:147.33ms
step:1056/1370 train_time:154112ms step_avg:147.33ms
step:1057/1370 train_time:154267ms step_avg:147.34ms
step:1058/1370 train_time:154426ms step_avg:147.35ms
step:1059/1370 train_time:154580ms step_avg:147.36ms
step:1060/1370 train_time:154735ms step_avg:147.37ms
step:1061/1370 train_time:154889ms step_avg:147.37ms
step:1062/1370 train_time:155045ms step_avg:147.38ms
step:1063/1370 train_time:155198ms step_avg:147.39ms
step:1064/1370 train_time:155352ms step_avg:147.39ms
step:1065/1370 train_time:155506ms step_avg:147.40ms
step:1066/1370 train_time:155667ms step_avg:147.41ms
step:1067/1370 train_time:155824ms step_avg:147.42ms
step:1068/1370 train_time:155976ms step_avg:147.43ms
step:1069/1370 train_time:156134ms step_avg:147.44ms
step:1070/1370 train_time:156288ms step_avg:147.44ms
step:1071/1370 train_time:156443ms step_avg:147.45ms
step:1072/1370 train_time:156595ms step_avg:147.45ms
step:1073/1370 train_time:156750ms step_avg:147.46ms
step:1074/1370 train_time:156903ms step_avg:147.47ms
step:1075/1370 train_time:157058ms step_avg:147.47ms
step:1076/1370 train_time:157210ms step_avg:147.48ms
step:1077/1370 train_time:157364ms step_avg:147.48ms
step:1078/1370 train_time:157523ms step_avg:147.49ms
step:1079/1370 train_time:157682ms step_avg:147.50ms
step:1080/1370 train_time:157836ms step_avg:147.51ms
step:1081/1370 train_time:157990ms step_avg:147.52ms
step:1082/1370 train_time:158143ms step_avg:147.52ms
step:1083/1370 train_time:158295ms step_avg:147.53ms
step:1084/1370 train_time:158453ms step_avg:147.54ms
step:1085/1370 train_time:158607ms step_avg:147.54ms
step:1086/1370 train_time:158764ms step_avg:147.55ms
step:1087/1370 train_time:158920ms step_avg:147.56ms
step:1088/1370 train_time:159073ms step_avg:147.56ms
step:1089/1370 train_time:159231ms step_avg:147.57ms
step:1090/1370 train_time:159392ms step_avg:147.58ms
step:1091/1370 train_time:159548ms step_avg:147.59ms
step:1092/1370 train_time:159702ms step_avg:147.60ms
step:1093/1370 train_time:159858ms step_avg:147.61ms
step:1094/1370 train_time:160013ms step_avg:147.61ms
step:1095/1370 train_time:160167ms step_avg:147.62ms
step:1096/1370 train_time:160325ms step_avg:147.63ms
step:1097/1370 train_time:160480ms step_avg:147.64ms
step:1098/1370 train_time:160635ms step_avg:147.64ms
step:1099/1370 train_time:160789ms step_avg:147.65ms
step:1100/1370 train_time:160944ms step_avg:147.66ms
step:1101/1370 train_time:161098ms step_avg:147.66ms
step:1102/1370 train_time:161253ms step_avg:147.67ms
step:1103/1370 train_time:161408ms step_avg:147.67ms
step:1104/1370 train_time:161562ms step_avg:147.68ms
step:1105/1370 train_time:161718ms step_avg:147.69ms
step:1106/1370 train_time:161872ms step_avg:147.69ms
step:1107/1370 train_time:162026ms step_avg:147.70ms
step:1108/1370 train_time:162181ms step_avg:147.71ms
step:1109/1370 train_time:162336ms step_avg:147.71ms
step:1110/1370 train_time:162490ms step_avg:147.72ms
step:1111/1370 train_time:162646ms step_avg:147.73ms
step:1112/1370 train_time:162802ms step_avg:147.73ms
step:1113/1370 train_time:162958ms step_avg:147.74ms
step:1114/1370 train_time:163112ms step_avg:147.75ms
step:1115/1370 train_time:163267ms step_avg:147.75ms
step:1116/1370 train_time:163420ms step_avg:147.76ms
step:1117/1370 train_time:163576ms step_avg:147.76ms
step:1118/1370 train_time:163734ms step_avg:147.77ms
step:1119/1370 train_time:163890ms step_avg:147.78ms
step:1120/1370 train_time:164048ms step_avg:147.79ms
step:1121/1370 train_time:164202ms step_avg:147.80ms
step:1122/1370 train_time:164357ms step_avg:147.80ms
step:1123/1370 train_time:164512ms step_avg:147.81ms
step:1124/1370 train_time:164671ms step_avg:147.82ms
step:1125/1370 train_time:164827ms step_avg:147.83ms
step:1125/1370 val_loss:3.3493 train_time:164899ms step_avg:147.89ms
step:1126/1370 train_time:164981ms step_avg:147.83ms
step:1127/1370 train_time:165138ms step_avg:147.84ms
step:1128/1370 train_time:165294ms step_avg:147.85ms
step:1129/1370 train_time:165452ms step_avg:147.86ms
step:1130/1370 train_time:165605ms step_avg:147.86ms
step:1131/1370 train_time:165764ms step_avg:147.87ms
step:1132/1370 train_time:165918ms step_avg:147.88ms
step:1133/1370 train_time:166075ms step_avg:147.88ms
step:1134/1370 train_time:166231ms step_avg:147.89ms
step:1135/1370 train_time:166385ms step_avg:147.90ms
step:1136/1370 train_time:166545ms step_avg:147.91ms
step:1137/1370 train_time:166699ms step_avg:147.91ms
step:1138/1370 train_time:166855ms step_avg:147.92ms
step:1139/1370 train_time:167010ms step_avg:147.93ms
step:1140/1370 train_time:167166ms step_avg:147.93ms
step:1141/1370 train_time:167366ms step_avg:147.98ms
step:1142/1370 train_time:167521ms step_avg:147.99ms
step:1143/1370 train_time:167680ms step_avg:148.00ms
step:1144/1370 train_time:167834ms step_avg:148.00ms
step:1145/1370 train_time:167987ms step_avg:148.01ms
step:1146/1370 train_time:168143ms step_avg:148.01ms
step:1147/1370 train_time:168302ms step_avg:148.02ms
step:1148/1370 train_time:168460ms step_avg:148.03ms
step:1149/1370 train_time:168616ms step_avg:148.04ms
step:1150/1370 train_time:168771ms step_avg:148.04ms
step:1151/1370 train_time:168926ms step_avg:148.05ms
step:1152/1370 train_time:169083ms step_avg:148.06ms
step:1153/1370 train_time:169240ms step_avg:148.07ms
step:1154/1370 train_time:169396ms step_avg:148.07ms
step:1155/1370 train_time:169552ms step_avg:148.08ms
step:1156/1370 train_time:169713ms step_avg:148.09ms
step:1157/1370 train_time:169870ms step_avg:148.10ms
step:1158/1370 train_time:170023ms step_avg:148.10ms
step:1159/1370 train_time:170180ms step_avg:148.11ms
step:1160/1370 train_time:170334ms step_avg:148.12ms
step:1161/1370 train_time:170492ms step_avg:148.13ms
step:1162/1370 train_time:170649ms step_avg:148.13ms
step:1163/1370 train_time:170803ms step_avg:148.14ms
step:1164/1370 train_time:170959ms step_avg:148.14ms
step:1165/1370 train_time:171112ms step_avg:148.15ms
step:1166/1370 train_time:171269ms step_avg:148.16ms
step:1167/1370 train_time:171422ms step_avg:148.16ms
step:1168/1370 train_time:171578ms step_avg:148.17ms
step:1169/1370 train_time:171735ms step_avg:148.18ms
step:1170/1370 train_time:171891ms step_avg:148.18ms
step:1171/1370 train_time:172049ms step_avg:148.19ms
step:1172/1370 train_time:172203ms step_avg:148.20ms
step:1173/1370 train_time:172361ms step_avg:148.20ms
step:1174/1370 train_time:172522ms step_avg:148.21ms
step:1175/1370 train_time:172681ms step_avg:148.22ms
step:1176/1370 train_time:172840ms step_avg:148.23ms
step:1177/1370 train_time:173004ms step_avg:148.25ms
step:1178/1370 train_time:173161ms step_avg:148.25ms
step:1179/1370 train_time:173317ms step_avg:148.26ms
step:1180/1370 train_time:173480ms step_avg:148.27ms
step:1181/1370 train_time:173635ms step_avg:148.28ms
step:1182/1370 train_time:173788ms step_avg:148.28ms
step:1183/1370 train_time:173943ms step_avg:148.29ms
step:1184/1370 train_time:174099ms step_avg:148.30ms
step:1185/1370 train_time:174258ms step_avg:148.30ms
step:1186/1370 train_time:174413ms step_avg:148.31ms
step:1187/1370 train_time:174577ms step_avg:148.32ms
step:1188/1370 train_time:174730ms step_avg:148.33ms
step:1189/1370 train_time:174890ms step_avg:148.34ms
step:1190/1370 train_time:175045ms step_avg:148.34ms
step:1191/1370 train_time:175202ms step_avg:148.35ms
step:1192/1370 train_time:175356ms step_avg:148.36ms
step:1193/1370 train_time:175512ms step_avg:148.36ms
step:1194/1370 train_time:175668ms step_avg:148.37ms
step:1195/1370 train_time:175822ms step_avg:148.37ms
step:1196/1370 train_time:175980ms step_avg:148.38ms
step:1197/1370 train_time:176136ms step_avg:148.39ms
step:1198/1370 train_time:176295ms step_avg:148.40ms
step:1199/1370 train_time:176451ms step_avg:148.40ms
step:1200/1370 train_time:176606ms step_avg:148.41ms
step:1201/1370 train_time:176762ms step_avg:148.41ms
step:1202/1370 train_time:176928ms step_avg:148.43ms
step:1203/1370 train_time:177087ms step_avg:148.44ms
step:1204/1370 train_time:177243ms step_avg:148.44ms
step:1205/1370 train_time:177398ms step_avg:148.45ms
step:1206/1370 train_time:177554ms step_avg:148.46ms
step:1207/1370 train_time:177708ms step_avg:148.46ms
step:1208/1370 train_time:177864ms step_avg:148.47ms
step:1209/1370 train_time:178021ms step_avg:148.47ms
step:1210/1370 train_time:178181ms step_avg:148.48ms
step:1211/1370 train_time:178335ms step_avg:148.49ms
step:1212/1370 train_time:178492ms step_avg:148.50ms
step:1213/1370 train_time:178648ms step_avg:148.50ms
step:1214/1370 train_time:178803ms step_avg:148.51ms
step:1215/1370 train_time:178959ms step_avg:148.51ms
step:1216/1370 train_time:179113ms step_avg:148.52ms
step:1217/1370 train_time:179268ms step_avg:148.52ms
step:1218/1370 train_time:179421ms step_avg:148.53ms
step:1219/1370 train_time:179576ms step_avg:148.53ms
step:1220/1370 train_time:179729ms step_avg:148.54ms
step:1221/1370 train_time:179884ms step_avg:148.54ms
step:1222/1370 train_time:180042ms step_avg:148.55ms
step:1223/1370 train_time:180200ms step_avg:148.56ms
step:1224/1370 train_time:180360ms step_avg:148.57ms
step:1225/1370 train_time:180517ms step_avg:148.57ms
step:1226/1370 train_time:180674ms step_avg:148.58ms
step:1227/1370 train_time:180831ms step_avg:148.59ms
step:1228/1370 train_time:180988ms step_avg:148.59ms
step:1229/1370 train_time:181143ms step_avg:148.60ms
step:1230/1370 train_time:181303ms step_avg:148.61ms
step:1231/1370 train_time:181460ms step_avg:148.62ms
step:1232/1370 train_time:181621ms step_avg:148.63ms
step:1233/1370 train_time:181778ms step_avg:148.63ms
step:1234/1370 train_time:181934ms step_avg:148.64ms
step:1235/1370 train_time:182091ms step_avg:148.65ms
step:1236/1370 train_time:182247ms step_avg:148.65ms
step:1237/1370 train_time:182403ms step_avg:148.66ms
step:1238/1370 train_time:182565ms step_avg:148.67ms
step:1239/1370 train_time:182721ms step_avg:148.67ms
step:1240/1370 train_time:182882ms step_avg:148.68ms
step:1241/1370 train_time:183045ms step_avg:148.70ms
step:1242/1370 train_time:183202ms step_avg:148.70ms
step:1243/1370 train_time:183360ms step_avg:148.71ms
step:1244/1370 train_time:183514ms step_avg:148.71ms
step:1245/1370 train_time:183671ms step_avg:148.72ms
step:1246/1370 train_time:183825ms step_avg:148.73ms
step:1247/1370 train_time:183985ms step_avg:148.73ms
step:1248/1370 train_time:184141ms step_avg:148.74ms
step:1249/1370 train_time:184298ms step_avg:148.75ms
step:1250/1370 train_time:184455ms step_avg:148.75ms
step:1250/1370 val_loss:3.3040 train_time:184530ms step_avg:148.81ms
step:1251/1370 train_time:184616ms step_avg:148.76ms
step:1252/1370 train_time:184773ms step_avg:148.77ms
step:1253/1370 train_time:184928ms step_avg:148.78ms
step:1254/1370 train_time:185081ms step_avg:148.78ms
step:1255/1370 train_time:185247ms step_avg:148.79ms
step:1256/1370 train_time:185402ms step_avg:148.80ms
step:1257/1370 train_time:185560ms step_avg:148.81ms
step:1258/1370 train_time:185722ms step_avg:148.82ms
step:1259/1370 train_time:185880ms step_avg:148.82ms
step:1260/1370 train_time:186034ms step_avg:148.83ms
step:1261/1370 train_time:186190ms step_avg:148.83ms
step:1262/1370 train_time:186351ms step_avg:148.84ms
step:1263/1370 train_time:186508ms step_avg:148.85ms
step:1264/1370 train_time:186662ms step_avg:148.85ms
step:1265/1370 train_time:186820ms step_avg:148.86ms
step:1266/1370 train_time:186977ms step_avg:148.87ms
step:1267/1370 train_time:187136ms step_avg:148.87ms
step:1268/1370 train_time:187293ms step_avg:148.88ms
step:1269/1370 train_time:187452ms step_avg:148.89ms
step:1270/1370 train_time:187609ms step_avg:148.90ms
step:1271/1370 train_time:187767ms step_avg:148.90ms
step:1272/1370 train_time:187924ms step_avg:148.91ms
step:1273/1370 train_time:188081ms step_avg:148.92ms
step:1274/1370 train_time:188235ms step_avg:148.92ms
step:1275/1370 train_time:188391ms step_avg:148.93ms
step:1276/1370 train_time:188545ms step_avg:148.93ms
step:1277/1370 train_time:188702ms step_avg:148.94ms
step:1278/1370 train_time:188856ms step_avg:148.94ms
step:1279/1370 train_time:189013ms step_avg:148.95ms
step:1280/1370 train_time:189177ms step_avg:148.96ms
step:1281/1370 train_time:189332ms step_avg:148.96ms
step:1282/1370 train_time:189485ms step_avg:148.97ms
step:1283/1370 train_time:189644ms step_avg:148.97ms
step:1284/1370 train_time:189802ms step_avg:148.98ms
step:1285/1370 train_time:189958ms step_avg:148.99ms
step:1286/1370 train_time:190115ms step_avg:148.99ms
step:1287/1370 train_time:190269ms step_avg:149.00ms
step:1288/1370 train_time:190427ms step_avg:149.00ms
step:1289/1370 train_time:190589ms step_avg:149.01ms
step:1290/1370 train_time:190747ms step_avg:149.02ms
step:1291/1370 train_time:190907ms step_avg:149.03ms
step:1292/1370 train_time:191065ms step_avg:149.04ms
step:1293/1370 train_time:191224ms step_avg:149.04ms
step:1294/1370 train_time:191381ms step_avg:149.05ms
step:1295/1370 train_time:191538ms step_avg:149.06ms
step:1296/1370 train_time:191698ms step_avg:149.07ms
step:1297/1370 train_time:191855ms step_avg:149.07ms
step:1298/1370 train_time:192011ms step_avg:149.08ms
step:1299/1370 train_time:192165ms step_avg:149.08ms
step:1300/1370 train_time:192322ms step_avg:149.09ms
step:1301/1370 train_time:192479ms step_avg:149.09ms
step:1302/1370 train_time:192638ms step_avg:149.10ms
step:1303/1370 train_time:192799ms step_avg:149.11ms
step:1304/1370 train_time:192958ms step_avg:149.12ms
step:1305/1370 train_time:193114ms step_avg:149.12ms
step:1306/1370 train_time:193273ms step_avg:149.13ms
step:1307/1370 train_time:193427ms step_avg:149.13ms
step:1308/1370 train_time:193584ms step_avg:149.14ms
step:1309/1370 train_time:193739ms step_avg:149.14ms
step:1310/1370 train_time:193894ms step_avg:149.15ms
step:1311/1370 train_time:194048ms step_avg:149.15ms
step:1312/1370 train_time:194202ms step_avg:149.16ms
step:1313/1370 train_time:194357ms step_avg:149.16ms
step:1314/1370 train_time:194516ms step_avg:149.17ms
step:1315/1370 train_time:194671ms step_avg:149.17ms
step:1316/1370 train_time:194825ms step_avg:149.18ms
step:1317/1370 train_time:194981ms step_avg:149.18ms
step:1318/1370 train_time:195141ms step_avg:149.19ms
step:1319/1370 train_time:195299ms step_avg:149.20ms
step:1320/1370 train_time:195455ms step_avg:149.20ms
step:1321/1370 train_time:195612ms step_avg:149.21ms
step:1322/1370 train_time:195775ms step_avg:149.22ms
step:1323/1370 train_time:195929ms step_avg:149.22ms
step:1324/1370 train_time:196085ms step_avg:149.23ms
step:1325/1370 train_time:196244ms step_avg:149.23ms
step:1326/1370 train_time:196405ms step_avg:149.24ms
step:1327/1370 train_time:196561ms step_avg:149.25ms
step:1328/1370 train_time:196718ms step_avg:149.25ms
step:1329/1370 train_time:196893ms step_avg:149.27ms
step:1330/1370 train_time:197053ms step_avg:149.28ms
step:1331/1370 train_time:197246ms step_avg:149.32ms
step:1332/1370 train_time:197410ms step_avg:149.33ms
step:1333/1370 train_time:197568ms step_avg:149.33ms
step:1334/1370 train_time:197725ms step_avg:149.34ms
step:1335/1370 train_time:197879ms step_avg:149.34ms
step:1336/1370 train_time:198044ms step_avg:149.35ms
step:1337/1370 train_time:198204ms step_avg:149.36ms
step:1338/1370 train_time:198360ms step_avg:149.37ms
step:1339/1370 train_time:198518ms step_avg:149.37ms
step:1340/1370 train_time:198678ms step_avg:149.38ms
step:1341/1370 train_time:198832ms step_avg:149.39ms
step:1342/1370 train_time:198990ms step_avg:149.39ms
step:1343/1370 train_time:199148ms step_avg:149.40ms
step:1344/1370 train_time:199303ms step_avg:149.40ms
step:1345/1370 train_time:199460ms step_avg:149.41ms
step:1346/1370 train_time:199619ms step_avg:149.42ms
step:1347/1370 train_time:199779ms step_avg:149.42ms
step:1348/1370 train_time:199935ms step_avg:149.43ms
step:1349/1370 train_time:200092ms step_avg:149.43ms
step:1350/1370 train_time:200246ms step_avg:149.44ms
step:1351/1370 train_time:200405ms step_avg:149.44ms
step:1352/1370 train_time:200567ms step_avg:149.45ms
step:1353/1370 train_time:200729ms step_avg:149.46ms
step:1354/1370 train_time:200889ms step_avg:149.47ms
step:1355/1370 train_time:201047ms step_avg:149.48ms
step:1356/1370 train_time:201203ms step_avg:149.48ms
step:1357/1370 train_time:201362ms step_avg:149.49ms
step:1358/1370 train_time:201521ms step_avg:149.50ms
step:1359/1370 train_time:201680ms step_avg:149.50ms
step:1360/1370 train_time:201839ms step_avg:149.51ms
step:1361/1370 train_time:201999ms step_avg:149.52ms
step:1362/1370 train_time:202159ms step_avg:149.53ms
step:1363/1370 train_time:202322ms step_avg:149.54ms
step:1364/1370 train_time:202477ms step_avg:149.54ms
step:1365/1370 train_time:202631ms step_avg:149.54ms
step:1366/1370 train_time:202789ms step_avg:149.55ms
step:1367/1370 train_time:202945ms step_avg:149.55ms
step:1368/1370 train_time:203106ms step_avg:149.56ms
step:1369/1370 train_time:203267ms step_avg:149.57ms
step:1370/1370 train_time:203426ms step_avg:149.58ms
step:1370/1370 val_loss:3.2801 train_time:203498ms step_avg:149.63ms
peak memory consumption: 32619 MiB

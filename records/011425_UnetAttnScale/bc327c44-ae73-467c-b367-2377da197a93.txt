import os
import sys
with open(sys.argv[0]) as f:
    code = f.read() # read the code of this file ASAP, for logging
import uuid
import time
import glob
import subprocess
import contextlib
from dataclasses import dataclass

import torch
torch.empty(1, device='cuda', requires_grad=True).backward()
from torch import nn
import torch.nn.functional as F
import torch.distributed as dist
from torch.nn.parallel import DistributedDataParallel as DDP
# use of FlexAttention contributed by @KoszarskyB
from torch.nn.attention.flex_attention import BlockMask, flex_attention

# -----------------------------------------------------------------------------
# Muon optimizer

@torch.compile
def zeropower_via_newtonschulz5(G, steps):
    """
    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a
    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose
    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at
    zero even beyond the point where the iteration no longer converges all the way to one everywhere
    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T
    where S' is diagonal with S_{ii}' ~ Uniform(0.5, 1.5), which turns out not to hurt model
    performance at all relative to UV^T, where USV^T = G is the SVD.
    """
    assert len(G.shape) == 2
    a, b, c = (3.4445, -4.7750,  2.0315)
    X = G.bfloat16()
    if G.size(0) > G.size(1):
        X = X.T

    # Ensure spectral norm is at most 1
    X = X / (X.norm() + 1e-7)
    # Perform the NS iterations
    for _ in range(steps):
        A = X @ X.T
        B = b * A + c * A @ A # adapted from suggestion by @jxbz, @leloykun, and @YouJiacheng
        X = a * X + B @ X
    
    if G.size(0) > G.size(1):
        X = X.T
    return X

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Some warnings:
    - This optimizer assumes that all parameters passed in are 2D.
    - It should not be used for the embedding layer, the final fully connected layer, or any {0,1}-D
    parameters; those should all be optimized by a standard method (e.g., AdamW).
    - To use it with 4D convolutional filters, it works well to just flatten their last 3 dimensions.
    - We believe it is unlikely to work well for training with small batch size.
    - We believe it may not work well for finetuning pretrained models, but we haven't tested this.
    - We have not yet tried this optimizer for training scenarios larger than NanoGPT (124M).

    Arguments:
        lr: The learning rate used by the internal SGD.
        momentum: The momentum used by the internal SGD.
        nesterov: Whether to use Nesterov-style momentum in the internal SGD. (recommended)
        ns_steps: The number of Newton-Schulz iteration steps to use.
    """
    def __init__(self, params, lr=0.02, momentum=0.95, nesterov=True, ns_steps=5):
        self.world_size = int(os.environ['WORLD_SIZE'])
        self.rank = int(os.environ['RANK'])
        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, ns_steps=ns_steps)
        assert all(isinstance(p, torch.Tensor) for p in params)
        sizes = {p.numel() for p in params}
        param_groups = [dict(params=[p for p in params if p.numel() == size],
                             update_buffer=[torch.empty(size, device='cuda', dtype=torch.bfloat16) for _ in range(self.world_size)])
                        for size in sizes]
        super().__init__(param_groups, defaults)

    def step(self):

        for group in self.param_groups:

            lr = group['lr']
            momentum = group['momentum']
            nesterov = group['nesterov']
            ns_steps = group['ns_steps']
            update_buffers = group['update_buffer']
            # generate weight updates in distributed fashion
            params = group['params']
            handle = None
            params_world = None
            def update_prev():
                if params_world is None:
                    return
                assert handle is not None
                handle.wait()
                for p_world, g_world in zip(params_world, update_buffers):
                    p_world.data.add_(
                        g_world.view_as(p_world),
                        alpha=-lr * max(1, p_world.size(0) / p_world.size(1)) ** 0.5,
                    )
            for base_i in range(len(params))[::self.world_size]:
                if base_i + rank < len(params):
                    p = params[base_i + self.rank]
                    g = p.grad
                    assert g is not None
                    state = self.state[p]
                    if 'momentum_buffer' not in state:
                        state['momentum_buffer'] = torch.zeros_like(g)
                    buf = state['momentum_buffer']
                    buf.lerp_(g, 1 - momentum)
                    g = g.lerp_(buf, momentum) if nesterov else buf
                    g = zeropower_via_newtonschulz5(g, steps=ns_steps).flatten()
                else:
                    g = update_buffers[rank]
                update_prev() # async all_gather instead of sync all_reduce by @YouJiacheng
                handle = dist.all_gather(update_buffers, g, async_op=True)
                params_world = params[base_i : base_i + self.world_size]
            update_prev()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the GPT-2 model

def norm(x):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):

    def __init__(self, in_features, out_features):
        super().__init__(in_features, out_features, bias=False)

    def forward(self, x):
        return F.linear(x, self.weight.type_as(x))

class Rotary(nn.Module):

    def __init__(self, dim, max_seq_len=65536):
        super().__init__()
        # half-truncate RoPE by @YouJiacheng
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=dim//4, dtype=torch.float32)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(dim//4)])
        t = torch.arange(max_seq_len, dtype=torch.float32)
        theta = torch.einsum('i,j -> ij', t, angular_freq)
        self.cos = nn.Buffer(theta.cos(), persistent=False)
        self.sin = nn.Buffer(theta.sin(), persistent=False)

    def forward(self, x):
        cos, sin = self.cos[None, :x.size(-3), None, :], self.sin[None, :x.size(-3), None, :]
        x1, x2 = x.float().chunk(2, dim=-1)
        y1 = x1 * cos + x2 * sin
        y2 = x1 * (-sin) + x2 * cos
        return torch.cat((y1, y2), 3).type_as(x)

class CausalSelfAttention(nn.Module):

    def __init__(self, dim, num_heads, layer_id):
        super().__init__()
        assert dim % num_heads == 0
        self.num_heads = num_heads
        self.layer_id = layer_id
        self.c_q = CastedLinear(dim, dim)
        self.c_k = CastedLinear(dim, dim)
        self.c_v = CastedLinear(dim, dim)
        self.lambdas = nn.Parameter(torch.tensor([0.5, 0.5]))
        self.rotary = Rotary(dim // num_heads) # dim // num_heads = head_dim
        self.c_proj = CastedLinear(dim, dim)
        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977
        # self.attn_scale = nn.Parameter(torch.tensor(1.0 / (dim // num_heads) ** 0.5))
        # self.attn_scale = nn.Parameter(torch.tensor(0.20))
        self.attn_scale = 0.13 + 0.01 * min(layer_id, 11 - layer_id)

    def forward(self, x, ve, block_mask):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, 'Must use batch size = 1 for FlexAttention'
        q = self.c_q(x).view(B, T, self.num_heads, -1)
        k = self.c_k(x).view(B, T, self.num_heads, -1)
        v = self.c_v(x).view(B, T, self.num_heads, -1)
        if ve is not None:
            v = self.lambdas[0] * v + self.lambdas[1] * ve.view_as(v) # @KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = self.lambdas[0] * v
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = self.rotary(q), self.rotary(k)
        # y = flex_attention(q.transpose(1, 2) * self.attn_scale, k.transpose(1, 2), v.transpose(1, 2), block_mask=block_mask, scale=1.)
        y = flex_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), block_mask=block_mask, scale=self.attn_scale)
        y = y.transpose(1, 2).contiguous().view_as(x) # re-assemble all head outputs side by side
        y = self.c_proj(y)
        return y

class MLP(nn.Module):

    def __init__(self, dim):
        super().__init__()
        self.c_fc = CastedLinear(dim, 4 * dim)
        self.c_proj = CastedLinear(4 * dim, dim)
        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977

    def forward(self, x):
        x = self.c_fc(x)
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = self.c_proj(x)
        return x

class Block(nn.Module):

    def __init__(self, model_dim, num_heads, layer_id, use_attn=True):
        super().__init__()
        self.attn = CausalSelfAttention(model_dim, num_heads, layer_id) if use_attn else None
        self.mlp = MLP(model_dim)
        self.lambdas = nn.Parameter(torch.tensor([1., 0.]))

    def forward(self, x, ve, x0, block_mask):
        x = self.lambdas[0] * x + self.lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), ve, block_mask)
        x = x + self.mlp(norm(x))
        return x

class ValueEmbedding(nn.Module):
    def __init__(self, vocab_size, model_dim):
        super().__init__()
        self.embed = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])

    def forward(self, inputs):
        ve = [emb(inputs).bfloat16() for emb in self.embed]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        ve = [ve[0], ve[1], ve[2], None, None, None, None, None, None, ve[0], ve[1], ve[2]]
        return ve

# -----------------------------------------------------------------------------
# The main GPT-2 model

class GPT(nn.Module):

    def __init__(self, vocab_size, num_layers, num_heads, model_dim):
        super().__init__()
        self.embed = nn.Embedding(vocab_size, model_dim)
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.blocks = nn.ModuleList([Block(model_dim, num_heads, layer_id=layer_id, use_attn=(layer_id != 7))
                                     for layer_id in range(num_layers)])
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual learning
        # U-net structure on token value embeddings by @leloykun
        self.value_embeds = ValueEmbedding(vocab_size, model_dim)
        self.lm_head = CastedLinear(model_dim, vocab_size)
        self.lm_head.weight.data.zero_() # @Grad62304977
        # U-net design by @brendanh0gan
        self.num_encoder_layers = num_layers // 2 # Half of the layers for encoder
        self.num_decoder_layers = num_layers - self.num_encoder_layers # Remaining for decoder
        # Add learnable skip connection weights for decoder layers
        self.skip_weights = nn.Parameter(torch.ones(self.num_decoder_layers))

    def forward(self, inputs, targets, sliding_window_num_blocks):
        BLOCK_SIZE = 128
        seq_len = len(inputs)
        assert seq_len % BLOCK_SIZE == 0
        total_num_blocks = seq_len // BLOCK_SIZE
        assert inputs.ndim == 1
        docs = (inputs == 50256).cumsum(0)
        docs_low = docs.view(-1, BLOCK_SIZE)[:, 0].contiguous()
        docs_high = docs.view(-1, BLOCK_SIZE)[:, -1].contiguous()

        def document_causal(b, h, q_idx, kv_idx):
            causal_mask = q_idx >= kv_idx
            document_mask = docs[q_idx] == docs[kv_idx]
            return causal_mask & document_mask

        def dense_to_ordered(dense_mask):
            num_blocks = dense_mask.sum(dim=-1, dtype=torch.int32)
            indices = dense_mask.argsort(dim=-1, descending=True, stable=True).to(torch.int32)
            return num_blocks[None, None].contiguous(), indices[None, None].contiguous()

        def create_doc_swc_block_mask(sliding_window_num_blocks):
            kv_idx = block_idx = torch.arange(total_num_blocks, dtype=torch.int32, device='cuda')
            q_idx = block_idx[:, None]
            causal_bm = q_idx >= kv_idx
            causal_full_bm = q_idx > kv_idx
            window_bm = q_idx - kv_idx < sliding_window_num_blocks
            window_full_bm = window_bm # block-wise sliding window by @YouJiacheng
            # document_bm = (docs_low[q_idx] <= docs_high[kv_idx]) & (docs_low[kv_idx] <= docs_high[q_idx])
            document_bm = (docs_low[:, None] <= docs_high) & (docs_low <= docs_high[:, None])
            document_full_bm = (docs_low[:, None] == docs_high) & (docs_low == docs_high[:, None])
            nonzero_bm = causal_bm & window_bm & document_bm
            full_bm  = causal_full_bm & window_full_bm & document_full_bm
            kv_num_blocks, kv_indices = dense_to_ordered(nonzero_bm & ~full_bm)
            full_kv_num_blocks, full_kv_indices = dense_to_ordered(full_bm)
            return BlockMask.from_kv_blocks(
                kv_num_blocks,
                kv_indices,
                full_kv_num_blocks,
                full_kv_indices,
                BLOCK_SIZE=BLOCK_SIZE,
                mask_mod=document_causal,
            )

        block_mask = create_doc_swc_block_mask(sliding_window_num_blocks)

        x0 = norm(self.embed(inputs[None]).bfloat16()) # use of norm here by @Grad62304977
        x = x0
        ve = self.value_embeds(inputs)
        assert len(ve) == len(self.blocks)
        ve_enc, ve_dec = ve[:self.num_encoder_layers], ve[self.num_encoder_layers:]

        # Store outputs for U-Net skip connections
        skip_connections = []
        # Encoder pass - process only the first half of the blocks
        for i in range(self.num_encoder_layers):
            x = self.blocks[i](x, ve_enc[i], x0, block_mask)
            skip_connections.append(x)
        # Decoder pass - process the remaining blocks with weighted skip connections
        for i in range(self.num_decoder_layers):
            x = x + self.skip_weights[i] * skip_connections.pop()
            # U-net structure on token value embeddings by @leloykun
            x = self.blocks[self.num_encoder_layers + i](x, ve_dec[i], x0, block_mask)

        x = norm(x)
        logits = self.lm_head(x)
        logits = 15 * torch.tanh(logits / 15) # @Grad62304977 added tanh softcapping, @KoszarskyB reduced it from 30 to 15
        logits = logits.float()
        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets)
        return loss

# -----------------------------------------------------------------------------
# Our own simple Distributed Data Loader

def _load_data_shard(path):
    # only reads the header, returns header data
    # header is 256 int32
    header = torch.from_file(path, False, 256, dtype=torch.int32)
    assert header[0] == 20240520, 'magic number mismatch in the data .bin file'
    assert header[1] == 1, 'unsupported version'
    num_tokens = int(header[2]) # number of tokens (claimed)
    with open(path, 'rb', buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, 'number of tokens read does not match header'
    return tokens

class DistributedDataLoader:

    def __init__(self, filename_pattern):
        self.rank = int(os.environ['RANK'])
        self.world_size = int(os.environ['WORLD_SIZE'])
        self.files = sorted(glob.glob(filename_pattern))
        self.reset()

    def reset(self):
        self.current_shard = -1
        self.advance()

    def advance(self):
        self.current_shard = (self.current_shard + 1) % len(self.files)
        self.current_position = 0
        self.tokens = _load_data_shard(self.files[self.current_shard])

    def next_batch(self, batch_size):
        assert batch_size % self.world_size == 0
        device_batch_size = batch_size // self.world_size
        # load next shard if necessary
        if self.current_position + batch_size + 1 >= len(self.tokens):
            self.advance()
        pos = self.current_position + self.rank * device_batch_size
        device_batch_tokens = self.tokens[pos:pos+device_batch_size+1]
        # advance current position
        self.current_position += batch_size
        inputs = device_batch_tokens[:-1].to(device='cuda', dtype=torch.int32, non_blocking=True)
        targets = device_batch_tokens[1:].to(device='cuda', dtype=torch.int64, non_blocking=True)
        return inputs, targets

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files = 'data/fineweb10B/fineweb_train_*.bin' # input .bin to train on
    val_files = 'data/fineweb10B/fineweb_val_*.bin' # input .bin to eval validation loss on
    val_tokens = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # optimization
    batch_size = 8*64*1024 # batch size in tokens
    num_iterations = 1370 # number of iterations to run
    cooldown_frac = 0.4 # fraction of training spent cooling down the learning rate
    bf16_embeds = True
    # evaluation and logging
    val_loss_every = 125 # every how many steps to evaluate val loss? 0 for only at the end
    # implementation
    max_device_batch_size = 64*1024 # batch size per device in tokens
    save_checkpoint = False
args = Hyperparameters()

micro_bs = args.max_device_batch_size

# set up DDP (distributed data parallel). torchrun sets this env variable
rank = int(os.environ['RANK'])
local_rank = int(os.environ['LOCAL_RANK'])
world_size = int(os.environ['WORLD_SIZE'])
assert torch.cuda.is_available()
torch.cuda.set_device(local_rank)
dist.init_process_group(backend='nccl', device_id=torch.device(local_rank))
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = uuid.uuid4()
    os.makedirs('logs', exist_ok=True)
    logfile = f'logs/{run_id}.txt'
    print(logfile)

def print0(s, console=False):
    if master_process:
        with open(logfile, 'a') as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0('='*100)
# log information about the hardware/software environment this is running on
print0(f'Running Python {sys.version}')
print0(f'Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}')
print0(subprocess.run(['nvidia-smi'], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout)
print0('='*100)

# load data
train_loader = DistributedDataLoader(args.train_files)
val_loader = DistributedDataLoader(args.val_files)
print0(f'Training dataloader files: {train_loader.files}')
print0(f'Validation dataloader files: {val_loader.files}')
print0('='*100)

# there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency. suggested to me by @Grad62304977.
# this originates from Karpathy's experiments.
model = GPT(vocab_size=50304, num_layers=12, num_heads=6, model_dim=768)
model = model.cuda()
if args.bf16_embeds:
    for m in model.modules():
        if isinstance(m, nn.Embedding):
            m.bfloat16()
model: GPT = torch.compile(model)
ddp_model = DDP(model, device_ids=[local_rank], broadcast_buffers=False, gradient_as_bucket_view=True)

# collect the parameters to optimize
hidden_matrix_params = [p for p in model.blocks.parameters() if p.ndim == 2]
embed_params = [model.embed.weight, *model.value_embeds.parameters()]
scalar_params = [p for n, p in model.named_parameters() if p.ndim < 2 and "attn_scale" not in n]
attn_scale_params = [p for n, p in model.named_parameters() if p.ndim < 2 and "attn_scale" in n]
head_params = [model.lm_head.weight]

# init the optimizer(s)
optimizer1 = torch.optim.Adam([dict(params=embed_params, lr=0.6),
                               dict(params=head_params, lr=0.008),
                               dict(params=scalar_params, lr=0.04),
                               dict(params=attn_scale_params, lr=0.01)],
                              betas=(0.8, 0.95), fused=True)
optimizer2 = Muon(hidden_matrix_params, lr=0.05, momentum=0.95)
optimizers = [optimizer1, optimizer2]

# learning rate schedule: stable then decay
def get_lr(it):
    t = 1 - it / args.num_iterations # time remaining in training
    assert 1 >= t > 0
    # 1) constant lr for first part of training
    if t >= args.cooldown_frac:
        return 1.0
    # 2) then linear cooldown
    else:
        return t / args.cooldown_frac
schedulers = [torch.optim.lr_scheduler.LambdaLR(opt, get_lr) for opt in optimizers]

# sliding window size schedule: linear increase over training in chunks of 128 from 128 -> 1792. By @fernbear.bsky.social
def get_sliding_window_blocks(it):
    x = it / args.num_iterations # training progress
    assert 0 <= x <= 1
    return int(((1 - x) * 128 + x * 1856) // 128)
sliding_window_num_blocks = torch.tensor(1, dtype=torch.int32, device='cuda')

# Start training loop
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    # This effectively ignores timing first 10 steps, which are slower for weird reasons.
    # Alternately, and slightly more correctly in terms of benchmarking, we could do 10
    # steps with dummy data first, and then re-initialize the model and reset the loader.
    if step == 10:
        training_time_ms = 0
        t0 = time.perf_counter()
    timed_steps = float('nan') if step <= 11 else (step - 10) + 1 # <= 11 to avoid bug in val

    sliding_window_num_blocks.copy_(get_sliding_window_blocks(step))

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        # run validation batches
        model.eval()
        val_loader.reset()
        val_loss = 0.0
        # calculate the number of steps to take in the val loop.
        val_batch_size = world_size * micro_bs
        assert args.val_tokens % val_batch_size == 0
        val_steps = args.val_tokens // val_batch_size
        for _ in range(val_steps):
            with torch.no_grad():
                inputs_val, targets_val = val_loader.next_batch(val_batch_size)
                val_loss += ddp_model(inputs_val, targets_val, sliding_window_num_blocks)
        # Print attention scales
        for n, p in model.named_parameters():
            if p.ndim < 2 and "attn_scale" in n:
                print0(f'{n}: {p.item()}', console=True)
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        val_loss /= val_steps
        # logging
        print0(f'step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/(timed_steps-1):.2f}ms', console=True)
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f'logs/{run_id}', exist_ok=True)
            torch.save(log, f'logs/{run_id}/state_step{step:06d}.pt')
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    model.train()
    batch_size = args.batch_size
    assert batch_size % world_size == 0
    inputs_train, targets_train = train_loader.next_batch(batch_size)
    assert len(inputs_train) <= micro_bs or len(inputs_train) % micro_bs == 0
    for micro_inputs_train, micro_targets_train in zip(inputs_train.split(micro_bs), targets_train.split(micro_bs)):
        ddp_model(micro_inputs_train, micro_targets_train, sliding_window_num_blocks).backward()
    # momentum warmup for Muon
    frac = min(step/300, 1)
    for group in optimizer2.param_groups:
        group['momentum'] = (1 - frac) * 0.85 + frac * 0.95
    # step the optimizers and schedulers
    for opt, sched in zip(optimizers, schedulers):
        opt.step()
        if step != train_steps-1:
            sched.step()
    # null the gradients
    model.zero_grad(set_to_none=True)
    # logging
    approx_time = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f'step:{step+1}/{train_steps} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms', console=True)

print0(f'peak memory consumption: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB')
dist.destroy_process_group()

====================================================================================================
Running Python 3.10.12 (main, Sep 11 2024, 15:47:36) [GCC 11.4.0]
Running PyTorch 2.6.0.dev20241231+cu126 compiled for CUDA 12.6
Tue Jan 14 14:23:32 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.183.06             Driver Version: 535.183.06   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:19:00.0 Off |                    0 |
| N/A   37C    P0             125W / 700W |   7713MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:3B:00.0 Off |                    0 |
| N/A   31C    P0             113W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:4C:00.0 Off |                    0 |
| N/A   28C    P0             117W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   36C    P0             128W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:9B:00.0 Off |                    0 |
| N/A   37C    P0             122W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:BB:00.0 Off |                    0 |
| N/A   29C    P0             114W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:CB:00.0 Off |                    0 |
| N/A   35C    P0             115W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:DB:00.0 Off |                    0 |
| N/A   29C    P0             116W / 700W |   3211MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
Training dataloader files: ['data/fineweb10B/fineweb_train_000001.bin', 'data/fineweb10B/fineweb_train_000002.bin', 'data/fineweb10B/fineweb_train_000003.bin', 'data/fineweb10B/fineweb_train_000004.bin', 'data/fineweb10B/fineweb_train_000005.bin', 'data/fineweb10B/fineweb_train_000006.bin', 'data/fineweb10B/fineweb_train_000007.bin', 'data/fineweb10B/fineweb_train_000008.bin', 'data/fineweb10B/fineweb_train_000009.bin']
Validation dataloader files: ['data/fineweb10B/fineweb_val_000000.bin']
====================================================================================================
step:0/1370 val_loss:10.8258 train_time:0ms step_avg:nanms
step:1/1370 train_time:27567ms step_avg:nanms
step:2/1370 train_time:27651ms step_avg:nanms
step:3/1370 train_time:27833ms step_avg:nanms
step:4/1370 train_time:27966ms step_avg:nanms
step:5/1370 train_time:28101ms step_avg:nanms
step:6/1370 train_time:28233ms step_avg:nanms
step:7/1370 train_time:28365ms step_avg:nanms
step:8/1370 train_time:28499ms step_avg:nanms
step:9/1370 train_time:28632ms step_avg:nanms
step:10/1370 train_time:28773ms step_avg:nanms
step:11/1370 train_time:136ms step_avg:nanms
step:12/1370 train_time:272ms step_avg:nanms
step:13/1370 train_time:407ms step_avg:135.73ms
step:14/1370 train_time:542ms step_avg:135.62ms
step:15/1370 train_time:675ms step_avg:135.10ms
step:16/1370 train_time:810ms step_avg:135.02ms
step:17/1370 train_time:945ms step_avg:134.94ms
step:18/1370 train_time:1082ms step_avg:135.24ms
step:19/1370 train_time:1220ms step_avg:135.51ms
step:20/1370 train_time:1355ms step_avg:135.48ms
step:21/1370 train_time:1490ms step_avg:135.42ms
step:22/1370 train_time:1625ms step_avg:135.42ms
step:23/1370 train_time:1759ms step_avg:135.33ms
step:24/1370 train_time:1894ms step_avg:135.26ms
step:25/1370 train_time:2029ms step_avg:135.28ms
step:26/1370 train_time:2164ms step_avg:135.26ms
step:27/1370 train_time:2301ms step_avg:135.33ms
step:28/1370 train_time:2437ms step_avg:135.38ms
step:29/1370 train_time:2571ms step_avg:135.29ms
step:30/1370 train_time:2705ms step_avg:135.27ms
step:31/1370 train_time:2839ms step_avg:135.21ms
step:32/1370 train_time:2973ms step_avg:135.13ms
step:33/1370 train_time:3109ms step_avg:135.15ms
step:34/1370 train_time:3245ms step_avg:135.22ms
step:35/1370 train_time:3378ms step_avg:135.13ms
step:36/1370 train_time:3514ms step_avg:135.17ms
step:37/1370 train_time:3650ms step_avg:135.18ms
step:38/1370 train_time:3785ms step_avg:135.18ms
step:39/1370 train_time:3919ms step_avg:135.14ms
step:40/1370 train_time:4053ms step_avg:135.12ms
step:41/1370 train_time:4189ms step_avg:135.13ms
step:42/1370 train_time:4324ms step_avg:135.13ms
step:43/1370 train_time:4460ms step_avg:135.16ms
step:44/1370 train_time:4594ms step_avg:135.13ms
step:45/1370 train_time:4731ms step_avg:135.18ms
step:46/1370 train_time:4866ms step_avg:135.17ms
step:47/1370 train_time:5001ms step_avg:135.16ms
step:48/1370 train_time:5136ms step_avg:135.15ms
step:49/1370 train_time:5270ms step_avg:135.13ms
step:50/1370 train_time:5406ms step_avg:135.14ms
step:51/1370 train_time:5541ms step_avg:135.14ms
step:52/1370 train_time:5676ms step_avg:135.13ms
step:53/1370 train_time:5811ms step_avg:135.13ms
step:54/1370 train_time:5946ms step_avg:135.14ms
step:55/1370 train_time:6081ms step_avg:135.14ms
step:56/1370 train_time:6216ms step_avg:135.14ms
step:57/1370 train_time:6352ms step_avg:135.14ms
step:58/1370 train_time:6486ms step_avg:135.12ms
step:59/1370 train_time:6622ms step_avg:135.14ms
step:60/1370 train_time:6757ms step_avg:135.13ms
step:61/1370 train_time:6892ms step_avg:135.14ms
step:62/1370 train_time:7029ms step_avg:135.17ms
step:63/1370 train_time:7163ms step_avg:135.15ms
step:64/1370 train_time:7300ms step_avg:135.18ms
step:65/1370 train_time:7434ms step_avg:135.16ms
step:66/1370 train_time:7569ms step_avg:135.17ms
step:67/1370 train_time:7704ms step_avg:135.15ms
step:68/1370 train_time:7840ms step_avg:135.18ms
step:69/1370 train_time:7975ms step_avg:135.17ms
step:70/1370 train_time:8111ms step_avg:135.18ms
step:71/1370 train_time:8245ms step_avg:135.17ms
step:72/1370 train_time:8380ms step_avg:135.15ms
step:73/1370 train_time:8515ms step_avg:135.15ms
step:74/1370 train_time:8650ms step_avg:135.15ms
step:75/1370 train_time:8785ms step_avg:135.15ms
step:76/1370 train_time:8920ms step_avg:135.14ms
step:77/1370 train_time:9054ms step_avg:135.14ms
step:78/1370 train_time:9191ms step_avg:135.16ms
step:79/1370 train_time:9328ms step_avg:135.18ms
step:80/1370 train_time:9462ms step_avg:135.17ms
step:81/1370 train_time:9598ms step_avg:135.18ms
step:82/1370 train_time:9732ms step_avg:135.17ms
step:83/1370 train_time:9868ms step_avg:135.18ms
step:84/1370 train_time:10003ms step_avg:135.18ms
step:85/1370 train_time:10140ms step_avg:135.20ms
step:86/1370 train_time:10275ms step_avg:135.20ms
step:87/1370 train_time:10410ms step_avg:135.20ms
step:88/1370 train_time:10546ms step_avg:135.20ms
step:89/1370 train_time:10680ms step_avg:135.19ms
step:90/1370 train_time:10816ms step_avg:135.19ms
step:91/1370 train_time:10951ms step_avg:135.20ms
step:92/1370 train_time:11085ms step_avg:135.18ms
step:93/1370 train_time:11221ms step_avg:135.19ms
step:94/1370 train_time:11356ms step_avg:135.19ms
step:95/1370 train_time:11491ms step_avg:135.19ms
step:96/1370 train_time:11627ms step_avg:135.20ms
step:97/1370 train_time:11761ms step_avg:135.19ms
step:98/1370 train_time:11895ms step_avg:135.17ms
step:99/1370 train_time:12032ms step_avg:135.19ms
step:100/1370 train_time:12167ms step_avg:135.18ms
step:101/1370 train_time:12303ms step_avg:135.20ms
step:102/1370 train_time:12439ms step_avg:135.21ms
step:103/1370 train_time:12575ms step_avg:135.21ms
step:104/1370 train_time:12714ms step_avg:135.26ms
step:105/1370 train_time:12852ms step_avg:135.29ms
step:106/1370 train_time:12991ms step_avg:135.33ms
step:107/1370 train_time:13129ms step_avg:135.36ms
step:108/1370 train_time:13266ms step_avg:135.37ms
step:109/1370 train_time:13406ms step_avg:135.41ms
step:110/1370 train_time:13545ms step_avg:135.45ms
step:111/1370 train_time:13682ms step_avg:135.47ms
step:112/1370 train_time:13822ms step_avg:135.51ms
step:113/1370 train_time:13961ms step_avg:135.54ms
step:114/1370 train_time:14099ms step_avg:135.56ms
step:115/1370 train_time:14237ms step_avg:135.59ms
step:116/1370 train_time:14374ms step_avg:135.61ms
step:117/1370 train_time:14515ms step_avg:135.66ms
step:118/1370 train_time:14654ms step_avg:135.68ms
step:119/1370 train_time:14794ms step_avg:135.72ms
step:120/1370 train_time:14933ms step_avg:135.75ms
step:121/1370 train_time:15071ms step_avg:135.78ms
step:122/1370 train_time:15210ms step_avg:135.81ms
step:123/1370 train_time:15350ms step_avg:135.84ms
step:124/1370 train_time:15488ms step_avg:135.86ms
step:125/1370 train_time:15628ms step_avg:135.90ms
step:125/1370 val_loss:4.3715 train_time:15695ms step_avg:136.48ms
step:126/1370 train_time:15768ms step_avg:135.93ms
step:127/1370 train_time:15910ms step_avg:135.98ms
step:128/1370 train_time:16048ms step_avg:136.00ms
step:129/1370 train_time:16186ms step_avg:136.01ms
step:130/1370 train_time:16322ms step_avg:136.02ms
step:131/1370 train_time:16460ms step_avg:136.03ms
step:132/1370 train_time:16597ms step_avg:136.04ms
step:133/1370 train_time:16739ms step_avg:136.09ms
step:134/1370 train_time:16880ms step_avg:136.13ms
step:135/1370 train_time:17018ms step_avg:136.15ms
step:136/1370 train_time:17157ms step_avg:136.17ms
step:137/1370 train_time:17296ms step_avg:136.19ms
step:138/1370 train_time:17433ms step_avg:136.19ms
step:139/1370 train_time:17572ms step_avg:136.22ms
step:140/1370 train_time:17710ms step_avg:136.23ms
step:141/1370 train_time:17850ms step_avg:136.26ms
step:142/1370 train_time:17989ms step_avg:136.28ms
step:143/1370 train_time:18127ms step_avg:136.30ms
step:144/1370 train_time:18267ms step_avg:136.32ms
step:145/1370 train_time:18405ms step_avg:136.33ms
step:146/1370 train_time:18544ms step_avg:136.35ms
step:147/1370 train_time:18683ms step_avg:136.37ms
step:148/1370 train_time:18823ms step_avg:136.40ms
step:149/1370 train_time:18963ms step_avg:136.43ms
step:150/1370 train_time:19102ms step_avg:136.44ms
step:151/1370 train_time:19239ms step_avg:136.45ms
step:152/1370 train_time:19378ms step_avg:136.47ms
step:153/1370 train_time:19516ms step_avg:136.48ms
step:154/1370 train_time:19656ms step_avg:136.50ms
step:155/1370 train_time:19796ms step_avg:136.52ms
step:156/1370 train_time:19936ms step_avg:136.55ms
step:157/1370 train_time:20077ms step_avg:136.57ms
step:158/1370 train_time:20215ms step_avg:136.59ms
step:159/1370 train_time:20355ms step_avg:136.61ms
step:160/1370 train_time:20495ms step_avg:136.63ms
step:161/1370 train_time:20634ms step_avg:136.65ms
step:162/1370 train_time:20772ms step_avg:136.66ms
step:163/1370 train_time:20912ms step_avg:136.68ms
step:164/1370 train_time:21051ms step_avg:136.70ms
step:165/1370 train_time:21191ms step_avg:136.71ms
step:166/1370 train_time:21329ms step_avg:136.72ms
step:167/1370 train_time:21469ms step_avg:136.75ms
step:168/1370 train_time:21608ms step_avg:136.76ms
step:169/1370 train_time:21747ms step_avg:136.78ms
step:170/1370 train_time:21887ms step_avg:136.80ms
step:171/1370 train_time:22026ms step_avg:136.81ms
step:172/1370 train_time:22167ms step_avg:136.84ms
step:173/1370 train_time:22307ms step_avg:136.85ms
step:174/1370 train_time:22445ms step_avg:136.86ms
step:175/1370 train_time:22585ms step_avg:136.88ms
step:176/1370 train_time:22723ms step_avg:136.89ms
step:177/1370 train_time:22864ms step_avg:136.91ms
step:178/1370 train_time:23003ms step_avg:136.92ms
step:179/1370 train_time:23143ms step_avg:136.94ms
step:180/1370 train_time:23282ms step_avg:136.95ms
step:181/1370 train_time:23420ms step_avg:136.96ms
step:182/1370 train_time:23561ms step_avg:136.98ms
step:183/1370 train_time:23701ms step_avg:137.00ms
step:184/1370 train_time:23840ms step_avg:137.01ms
step:185/1370 train_time:23980ms step_avg:137.03ms
step:186/1370 train_time:24119ms step_avg:137.04ms
step:187/1370 train_time:24259ms step_avg:137.06ms
step:188/1370 train_time:24399ms step_avg:137.07ms
step:189/1370 train_time:24539ms step_avg:137.09ms
step:190/1370 train_time:24679ms step_avg:137.10ms
step:191/1370 train_time:24857ms step_avg:137.33ms
step:192/1370 train_time:24996ms step_avg:137.34ms
step:193/1370 train_time:25134ms step_avg:137.34ms
step:194/1370 train_time:25273ms step_avg:137.35ms
step:195/1370 train_time:25410ms step_avg:137.35ms
step:196/1370 train_time:25548ms step_avg:137.35ms
step:197/1370 train_time:25687ms step_avg:137.36ms
step:198/1370 train_time:25829ms step_avg:137.39ms
step:199/1370 train_time:25969ms step_avg:137.40ms
step:200/1370 train_time:26108ms step_avg:137.41ms
step:201/1370 train_time:26246ms step_avg:137.41ms
step:202/1370 train_time:26384ms step_avg:137.42ms
step:203/1370 train_time:26522ms step_avg:137.42ms
step:204/1370 train_time:26661ms step_avg:137.43ms
step:205/1370 train_time:26807ms step_avg:137.47ms
step:206/1370 train_time:26947ms step_avg:137.49ms
step:207/1370 train_time:27089ms step_avg:137.51ms
step:208/1370 train_time:27229ms step_avg:137.52ms
step:209/1370 train_time:27370ms step_avg:137.54ms
step:210/1370 train_time:27510ms step_avg:137.55ms
step:211/1370 train_time:27651ms step_avg:137.57ms
step:212/1370 train_time:27793ms step_avg:137.59ms
step:213/1370 train_time:27936ms step_avg:137.61ms
step:214/1370 train_time:28077ms step_avg:137.63ms
step:215/1370 train_time:28219ms step_avg:137.65ms
step:216/1370 train_time:28361ms step_avg:137.67ms
step:217/1370 train_time:28502ms step_avg:137.69ms
step:218/1370 train_time:28643ms step_avg:137.70ms
step:219/1370 train_time:28786ms step_avg:137.73ms
step:220/1370 train_time:28928ms step_avg:137.75ms
step:221/1370 train_time:29070ms step_avg:137.77ms
step:222/1370 train_time:29209ms step_avg:137.78ms
step:223/1370 train_time:29351ms step_avg:137.80ms
step:224/1370 train_time:29493ms step_avg:137.82ms
step:225/1370 train_time:29635ms step_avg:137.84ms
step:226/1370 train_time:29779ms step_avg:137.87ms
step:227/1370 train_time:29919ms step_avg:137.88ms
step:228/1370 train_time:30062ms step_avg:137.90ms
step:229/1370 train_time:30202ms step_avg:137.91ms
step:230/1370 train_time:30344ms step_avg:137.93ms
step:231/1370 train_time:30487ms step_avg:137.95ms
step:232/1370 train_time:30630ms step_avg:137.97ms
step:233/1370 train_time:30772ms step_avg:137.99ms
step:234/1370 train_time:30914ms step_avg:138.01ms
step:235/1370 train_time:31058ms step_avg:138.04ms
step:236/1370 train_time:31199ms step_avg:138.05ms
step:237/1370 train_time:31341ms step_avg:138.07ms
step:238/1370 train_time:31481ms step_avg:138.08ms
step:239/1370 train_time:31623ms step_avg:138.09ms
step:240/1370 train_time:31766ms step_avg:138.11ms
step:241/1370 train_time:31907ms step_avg:138.13ms
step:242/1370 train_time:32048ms step_avg:138.14ms
step:243/1370 train_time:32190ms step_avg:138.15ms
step:244/1370 train_time:32330ms step_avg:138.16ms
step:245/1370 train_time:32471ms step_avg:138.17ms
step:246/1370 train_time:32611ms step_avg:138.18ms
step:247/1370 train_time:32753ms step_avg:138.20ms
step:248/1370 train_time:32895ms step_avg:138.21ms
step:249/1370 train_time:33037ms step_avg:138.23ms
step:250/1370 train_time:33181ms step_avg:138.25ms
step:250/1370 val_loss:3.9571 train_time:33249ms step_avg:138.54ms
step:251/1370 train_time:33324ms step_avg:138.27ms
step:252/1370 train_time:33468ms step_avg:138.30ms
step:253/1370 train_time:33609ms step_avg:138.31ms
step:254/1370 train_time:33749ms step_avg:138.31ms
step:255/1370 train_time:33890ms step_avg:138.33ms
step:256/1370 train_time:34030ms step_avg:138.33ms
step:257/1370 train_time:34171ms step_avg:138.34ms
step:258/1370 train_time:34314ms step_avg:138.36ms
step:259/1370 train_time:34459ms step_avg:138.39ms
step:260/1370 train_time:34603ms step_avg:138.41ms
step:261/1370 train_time:34742ms step_avg:138.42ms
step:262/1370 train_time:34885ms step_avg:138.43ms
step:263/1370 train_time:35024ms step_avg:138.44ms
step:264/1370 train_time:35165ms step_avg:138.44ms
step:265/1370 train_time:35308ms step_avg:138.46ms
step:266/1370 train_time:35450ms step_avg:138.48ms
step:267/1370 train_time:35593ms step_avg:138.49ms
step:268/1370 train_time:35735ms step_avg:138.51ms
step:269/1370 train_time:35878ms step_avg:138.52ms
step:270/1370 train_time:36018ms step_avg:138.53ms
step:271/1370 train_time:36159ms step_avg:138.54ms
step:272/1370 train_time:36303ms step_avg:138.56ms
step:273/1370 train_time:36443ms step_avg:138.57ms
step:274/1370 train_time:36587ms step_avg:138.59ms
step:275/1370 train_time:36728ms step_avg:138.60ms
step:276/1370 train_time:36869ms step_avg:138.61ms
step:277/1370 train_time:37012ms step_avg:138.62ms
step:278/1370 train_time:37154ms step_avg:138.63ms
step:279/1370 train_time:37296ms step_avg:138.65ms
step:280/1370 train_time:37439ms step_avg:138.66ms
step:281/1370 train_time:37583ms step_avg:138.68ms
step:282/1370 train_time:37724ms step_avg:138.69ms
step:283/1370 train_time:37867ms step_avg:138.71ms
step:284/1370 train_time:38010ms step_avg:138.72ms
step:285/1370 train_time:38151ms step_avg:138.73ms
step:286/1370 train_time:38293ms step_avg:138.74ms
step:287/1370 train_time:38434ms step_avg:138.75ms
step:288/1370 train_time:38577ms step_avg:138.77ms
step:289/1370 train_time:38718ms step_avg:138.77ms
step:290/1370 train_time:38860ms step_avg:138.78ms
step:291/1370 train_time:39001ms step_avg:138.79ms
step:292/1370 train_time:39143ms step_avg:138.80ms
step:293/1370 train_time:39286ms step_avg:138.82ms
step:294/1370 train_time:39426ms step_avg:138.82ms
step:295/1370 train_time:39567ms step_avg:138.83ms
step:296/1370 train_time:39709ms step_avg:138.84ms
step:297/1370 train_time:39851ms step_avg:138.85ms
step:298/1370 train_time:39992ms step_avg:138.86ms
step:299/1370 train_time:40133ms step_avg:138.87ms
step:300/1370 train_time:40276ms step_avg:138.88ms
step:301/1370 train_time:40418ms step_avg:138.89ms
step:302/1370 train_time:40560ms step_avg:138.90ms
step:303/1370 train_time:40702ms step_avg:138.91ms
step:304/1370 train_time:40843ms step_avg:138.92ms
step:305/1370 train_time:40987ms step_avg:138.94ms
step:306/1370 train_time:41128ms step_avg:138.95ms
step:307/1370 train_time:41272ms step_avg:138.96ms
step:308/1370 train_time:41418ms step_avg:138.99ms
step:309/1370 train_time:41561ms step_avg:139.00ms
step:310/1370 train_time:41705ms step_avg:139.02ms
step:311/1370 train_time:41849ms step_avg:139.03ms
step:312/1370 train_time:41995ms step_avg:139.05ms
step:313/1370 train_time:42138ms step_avg:139.07ms
step:314/1370 train_time:42284ms step_avg:139.09ms
step:315/1370 train_time:42426ms step_avg:139.10ms
step:316/1370 train_time:42570ms step_avg:139.12ms
step:317/1370 train_time:42715ms step_avg:139.14ms
step:318/1370 train_time:42860ms step_avg:139.16ms
step:319/1370 train_time:43004ms step_avg:139.17ms
step:320/1370 train_time:43146ms step_avg:139.18ms
step:321/1370 train_time:43292ms step_avg:139.20ms
step:322/1370 train_time:43435ms step_avg:139.21ms
step:323/1370 train_time:43581ms step_avg:139.24ms
step:324/1370 train_time:43724ms step_avg:139.25ms
step:325/1370 train_time:43866ms step_avg:139.26ms
step:326/1370 train_time:44010ms step_avg:139.27ms
step:327/1370 train_time:44153ms step_avg:139.28ms
step:328/1370 train_time:44297ms step_avg:139.30ms
step:329/1370 train_time:44442ms step_avg:139.32ms
step:330/1370 train_time:44587ms step_avg:139.33ms
step:331/1370 train_time:44730ms step_avg:139.35ms
step:332/1370 train_time:44875ms step_avg:139.36ms
step:333/1370 train_time:45020ms step_avg:139.38ms
step:334/1370 train_time:45164ms step_avg:139.39ms
step:335/1370 train_time:45306ms step_avg:139.40ms
step:336/1370 train_time:45450ms step_avg:139.42ms
step:337/1370 train_time:45593ms step_avg:139.43ms
step:338/1370 train_time:45737ms step_avg:139.44ms
step:339/1370 train_time:45883ms step_avg:139.46ms
step:340/1370 train_time:46025ms step_avg:139.47ms
step:341/1370 train_time:46169ms step_avg:139.48ms
step:342/1370 train_time:46312ms step_avg:139.49ms
step:343/1370 train_time:46457ms step_avg:139.51ms
step:344/1370 train_time:46602ms step_avg:139.53ms
step:345/1370 train_time:46745ms step_avg:139.54ms
step:346/1370 train_time:46888ms step_avg:139.55ms
step:347/1370 train_time:47031ms step_avg:139.56ms
step:348/1370 train_time:47174ms step_avg:139.57ms
step:349/1370 train_time:47318ms step_avg:139.58ms
step:350/1370 train_time:47463ms step_avg:139.60ms
step:351/1370 train_time:47607ms step_avg:139.61ms
step:352/1370 train_time:47750ms step_avg:139.62ms
step:353/1370 train_time:47895ms step_avg:139.64ms
step:354/1370 train_time:48039ms step_avg:139.65ms
step:355/1370 train_time:48183ms step_avg:139.66ms
step:356/1370 train_time:48326ms step_avg:139.67ms
step:357/1370 train_time:48469ms step_avg:139.68ms
step:358/1370 train_time:48612ms step_avg:139.69ms
step:359/1370 train_time:48756ms step_avg:139.70ms
step:360/1370 train_time:48902ms step_avg:139.72ms
step:361/1370 train_time:49044ms step_avg:139.73ms
step:362/1370 train_time:49187ms step_avg:139.74ms
step:363/1370 train_time:49331ms step_avg:139.75ms
step:364/1370 train_time:49475ms step_avg:139.76ms
step:365/1370 train_time:49622ms step_avg:139.78ms
step:366/1370 train_time:49766ms step_avg:139.79ms
step:367/1370 train_time:49910ms step_avg:139.80ms
step:368/1370 train_time:50054ms step_avg:139.82ms
step:369/1370 train_time:50199ms step_avg:139.83ms
step:370/1370 train_time:50342ms step_avg:139.84ms
step:371/1370 train_time:50486ms step_avg:139.85ms
step:372/1370 train_time:50630ms step_avg:139.86ms
step:373/1370 train_time:50773ms step_avg:139.87ms
step:374/1370 train_time:50918ms step_avg:139.89ms
step:375/1370 train_time:51062ms step_avg:139.90ms
step:375/1370 val_loss:3.7755 train_time:51131ms step_avg:140.09ms
step:376/1370 train_time:51207ms step_avg:139.91ms
step:377/1370 train_time:51352ms step_avg:139.92ms
step:378/1370 train_time:51494ms step_avg:139.93ms
step:379/1370 train_time:51637ms step_avg:139.94ms
step:380/1370 train_time:51782ms step_avg:139.95ms
step:381/1370 train_time:51966ms step_avg:140.07ms
step:382/1370 train_time:52107ms step_avg:140.07ms
step:383/1370 train_time:52249ms step_avg:140.08ms
step:384/1370 train_time:52392ms step_avg:140.09ms
step:385/1370 train_time:52535ms step_avg:140.09ms
step:386/1370 train_time:52677ms step_avg:140.10ms
step:387/1370 train_time:52822ms step_avg:140.11ms
step:388/1370 train_time:52971ms step_avg:140.13ms
step:389/1370 train_time:53116ms step_avg:140.15ms
step:390/1370 train_time:53260ms step_avg:140.16ms
step:391/1370 train_time:53404ms step_avg:140.17ms
step:392/1370 train_time:53546ms step_avg:140.17ms
step:393/1370 train_time:53690ms step_avg:140.18ms
step:394/1370 train_time:53833ms step_avg:140.19ms
step:395/1370 train_time:53979ms step_avg:140.20ms
step:396/1370 train_time:54125ms step_avg:140.22ms
step:397/1370 train_time:54269ms step_avg:140.23ms
step:398/1370 train_time:54411ms step_avg:140.24ms
step:399/1370 train_time:54555ms step_avg:140.25ms
step:400/1370 train_time:54699ms step_avg:140.25ms
step:401/1370 train_time:54844ms step_avg:140.27ms
step:402/1370 train_time:54991ms step_avg:140.28ms
step:403/1370 train_time:55133ms step_avg:140.29ms
step:404/1370 train_time:55276ms step_avg:140.29ms
step:405/1370 train_time:55418ms step_avg:140.30ms
step:406/1370 train_time:55563ms step_avg:140.31ms
step:407/1370 train_time:55708ms step_avg:140.32ms
step:408/1370 train_time:55851ms step_avg:140.33ms
step:409/1370 train_time:55998ms step_avg:140.35ms
step:410/1370 train_time:56145ms step_avg:140.36ms
step:411/1370 train_time:56291ms step_avg:140.38ms
step:412/1370 train_time:56435ms step_avg:140.39ms
step:413/1370 train_time:56580ms step_avg:140.40ms
step:414/1370 train_time:56727ms step_avg:140.41ms
step:415/1370 train_time:56871ms step_avg:140.42ms
step:416/1370 train_time:57017ms step_avg:140.44ms
step:417/1370 train_time:57165ms step_avg:140.45ms
step:418/1370 train_time:57310ms step_avg:140.47ms
step:419/1370 train_time:57454ms step_avg:140.47ms
step:420/1370 train_time:57603ms step_avg:140.49ms
step:421/1370 train_time:57748ms step_avg:140.51ms
step:422/1370 train_time:57894ms step_avg:140.52ms
step:423/1370 train_time:58038ms step_avg:140.53ms
step:424/1370 train_time:58185ms step_avg:140.54ms
step:425/1370 train_time:58331ms step_avg:140.56ms
step:426/1370 train_time:58476ms step_avg:140.57ms
step:427/1370 train_time:58622ms step_avg:140.58ms
step:428/1370 train_time:58769ms step_avg:140.60ms
step:429/1370 train_time:58913ms step_avg:140.60ms
step:430/1370 train_time:59059ms step_avg:140.62ms
step:431/1370 train_time:59208ms step_avg:140.64ms
step:432/1370 train_time:59352ms step_avg:140.64ms
step:433/1370 train_time:59498ms step_avg:140.66ms
step:434/1370 train_time:59643ms step_avg:140.67ms
step:435/1370 train_time:59791ms step_avg:140.68ms
step:436/1370 train_time:59935ms step_avg:140.69ms
step:437/1370 train_time:60081ms step_avg:140.71ms
step:438/1370 train_time:60229ms step_avg:140.72ms
step:439/1370 train_time:60373ms step_avg:140.73ms
step:440/1370 train_time:60520ms step_avg:140.74ms
step:441/1370 train_time:60667ms step_avg:140.76ms
step:442/1370 train_time:60812ms step_avg:140.77ms
step:443/1370 train_time:60958ms step_avg:140.78ms
step:444/1370 train_time:61105ms step_avg:140.80ms
step:445/1370 train_time:61250ms step_avg:140.80ms
step:446/1370 train_time:61396ms step_avg:140.82ms
step:447/1370 train_time:61542ms step_avg:140.83ms
step:448/1370 train_time:61689ms step_avg:140.84ms
step:449/1370 train_time:61833ms step_avg:140.85ms
step:450/1370 train_time:61979ms step_avg:140.86ms
step:451/1370 train_time:62128ms step_avg:140.88ms
step:452/1370 train_time:62272ms step_avg:140.89ms
step:453/1370 train_time:62419ms step_avg:140.90ms
step:454/1370 train_time:62566ms step_avg:140.91ms
step:455/1370 train_time:62711ms step_avg:140.92ms
step:456/1370 train_time:62856ms step_avg:140.93ms
step:457/1370 train_time:63004ms step_avg:140.95ms
step:458/1370 train_time:63149ms step_avg:140.96ms
step:459/1370 train_time:63295ms step_avg:140.97ms
step:460/1370 train_time:63439ms step_avg:140.98ms
step:461/1370 train_time:63587ms step_avg:140.99ms
step:462/1370 train_time:63732ms step_avg:141.00ms
step:463/1370 train_time:63877ms step_avg:141.01ms
step:464/1370 train_time:64025ms step_avg:141.02ms
step:465/1370 train_time:64170ms step_avg:141.03ms
step:466/1370 train_time:64317ms step_avg:141.05ms
step:467/1370 train_time:64464ms step_avg:141.06ms
step:468/1370 train_time:64611ms step_avg:141.07ms
step:469/1370 train_time:64755ms step_avg:141.08ms
step:470/1370 train_time:64902ms step_avg:141.09ms
step:471/1370 train_time:65048ms step_avg:141.10ms
step:472/1370 train_time:65193ms step_avg:141.11ms
step:473/1370 train_time:65338ms step_avg:141.12ms
step:474/1370 train_time:65485ms step_avg:141.13ms
step:475/1370 train_time:65630ms step_avg:141.14ms
step:476/1370 train_time:65776ms step_avg:141.15ms
step:477/1370 train_time:65925ms step_avg:141.17ms
step:478/1370 train_time:66071ms step_avg:141.18ms
step:479/1370 train_time:66217ms step_avg:141.19ms
step:480/1370 train_time:66363ms step_avg:141.20ms
step:481/1370 train_time:66509ms step_avg:141.21ms
step:482/1370 train_time:66652ms step_avg:141.21ms
step:483/1370 train_time:66799ms step_avg:141.22ms
step:484/1370 train_time:66945ms step_avg:141.23ms
step:485/1370 train_time:67091ms step_avg:141.24ms
step:486/1370 train_time:67236ms step_avg:141.25ms
step:487/1370 train_time:67385ms step_avg:141.27ms
step:488/1370 train_time:67531ms step_avg:141.28ms
step:489/1370 train_time:67674ms step_avg:141.28ms
step:490/1370 train_time:67820ms step_avg:141.29ms
step:491/1370 train_time:67968ms step_avg:141.31ms
step:492/1370 train_time:68112ms step_avg:141.31ms
step:493/1370 train_time:68257ms step_avg:141.32ms
step:494/1370 train_time:68405ms step_avg:141.33ms
step:495/1370 train_time:68551ms step_avg:141.34ms
step:496/1370 train_time:68697ms step_avg:141.35ms
step:497/1370 train_time:68843ms step_avg:141.36ms
step:498/1370 train_time:68989ms step_avg:141.37ms
step:499/1370 train_time:69133ms step_avg:141.38ms
step:500/1370 train_time:69279ms step_avg:141.39ms
step:500/1370 val_loss:3.6590 train_time:69351ms step_avg:141.53ms
step:501/1370 train_time:69427ms step_avg:141.40ms
step:502/1370 train_time:69577ms step_avg:141.42ms
step:503/1370 train_time:69722ms step_avg:141.42ms
step:504/1370 train_time:69865ms step_avg:141.43ms
step:505/1370 train_time:70009ms step_avg:141.43ms
step:506/1370 train_time:70155ms step_avg:141.44ms
step:507/1370 train_time:70301ms step_avg:141.45ms
step:508/1370 train_time:70448ms step_avg:141.46ms
step:509/1370 train_time:70597ms step_avg:141.48ms
step:510/1370 train_time:70742ms step_avg:141.48ms
step:511/1370 train_time:70889ms step_avg:141.49ms
step:512/1370 train_time:71036ms step_avg:141.51ms
step:513/1370 train_time:71183ms step_avg:141.52ms
step:514/1370 train_time:71332ms step_avg:141.53ms
step:515/1370 train_time:71482ms step_avg:141.55ms
step:516/1370 train_time:71631ms step_avg:141.56ms
step:517/1370 train_time:71780ms step_avg:141.58ms
step:518/1370 train_time:71926ms step_avg:141.59ms
step:519/1370 train_time:72073ms step_avg:141.60ms
step:520/1370 train_time:72221ms step_avg:141.61ms
step:521/1370 train_time:72367ms step_avg:141.62ms
step:522/1370 train_time:72516ms step_avg:141.63ms
step:523/1370 train_time:72662ms step_avg:141.64ms
step:524/1370 train_time:72809ms step_avg:141.65ms
step:525/1370 train_time:72958ms step_avg:141.67ms
step:526/1370 train_time:73104ms step_avg:141.67ms
step:527/1370 train_time:73253ms step_avg:141.69ms
step:528/1370 train_time:73401ms step_avg:141.70ms
step:529/1370 train_time:73548ms step_avg:141.71ms
step:530/1370 train_time:73697ms step_avg:141.72ms
step:531/1370 train_time:73842ms step_avg:141.73ms
step:532/1370 train_time:73989ms step_avg:141.74ms
step:533/1370 train_time:74137ms step_avg:141.75ms
step:534/1370 train_time:74283ms step_avg:141.76ms
step:535/1370 train_time:74430ms step_avg:141.77ms
step:536/1370 train_time:74580ms step_avg:141.79ms
step:537/1370 train_time:74727ms step_avg:141.80ms
step:538/1370 train_time:74877ms step_avg:141.81ms
step:539/1370 train_time:75023ms step_avg:141.82ms
step:540/1370 train_time:75171ms step_avg:141.83ms
step:541/1370 train_time:75319ms step_avg:141.84ms
step:542/1370 train_time:75465ms step_avg:141.85ms
step:543/1370 train_time:75613ms step_avg:141.86ms
step:544/1370 train_time:75760ms step_avg:141.87ms
step:545/1370 train_time:75908ms step_avg:141.88ms
step:546/1370 train_time:76057ms step_avg:141.90ms
step:547/1370 train_time:76203ms step_avg:141.90ms
step:548/1370 train_time:76351ms step_avg:141.92ms
step:549/1370 train_time:76500ms step_avg:141.93ms
step:550/1370 train_time:76647ms step_avg:141.94ms
step:551/1370 train_time:76797ms step_avg:141.95ms
step:552/1370 train_time:76942ms step_avg:141.96ms
step:553/1370 train_time:77090ms step_avg:141.97ms
step:554/1370 train_time:77238ms step_avg:141.98ms
step:555/1370 train_time:77385ms step_avg:141.99ms
step:556/1370 train_time:77531ms step_avg:142.00ms
step:557/1370 train_time:77680ms step_avg:142.01ms
step:558/1370 train_time:77826ms step_avg:142.02ms
step:559/1370 train_time:77973ms step_avg:142.03ms
step:560/1370 train_time:78121ms step_avg:142.04ms
step:561/1370 train_time:78267ms step_avg:142.05ms
step:562/1370 train_time:78415ms step_avg:142.06ms
step:563/1370 train_time:78562ms step_avg:142.06ms
step:564/1370 train_time:78710ms step_avg:142.08ms
step:565/1370 train_time:78859ms step_avg:142.09ms
step:566/1370 train_time:79004ms step_avg:142.09ms
step:567/1370 train_time:79150ms step_avg:142.10ms
step:568/1370 train_time:79300ms step_avg:142.11ms
step:569/1370 train_time:79447ms step_avg:142.12ms
step:570/1370 train_time:79596ms step_avg:142.14ms
step:571/1370 train_time:79782ms step_avg:142.21ms
step:572/1370 train_time:79928ms step_avg:142.22ms
step:573/1370 train_time:80077ms step_avg:142.23ms
step:574/1370 train_time:80224ms step_avg:142.24ms
step:575/1370 train_time:80370ms step_avg:142.25ms
step:576/1370 train_time:80517ms step_avg:142.26ms
step:577/1370 train_time:80664ms step_avg:142.26ms
step:578/1370 train_time:80813ms step_avg:142.28ms
step:579/1370 train_time:80962ms step_avg:142.29ms
step:580/1370 train_time:81108ms step_avg:142.30ms
step:581/1370 train_time:81256ms step_avg:142.31ms
step:582/1370 train_time:81402ms step_avg:142.31ms
step:583/1370 train_time:81549ms step_avg:142.32ms
step:584/1370 train_time:81700ms step_avg:142.33ms
step:585/1370 train_time:81847ms step_avg:142.34ms
step:586/1370 train_time:81996ms step_avg:142.35ms
step:587/1370 train_time:82142ms step_avg:142.36ms
step:588/1370 train_time:82290ms step_avg:142.37ms
step:589/1370 train_time:82437ms step_avg:142.38ms
step:590/1370 train_time:82584ms step_avg:142.39ms
step:591/1370 train_time:82732ms step_avg:142.40ms
step:592/1370 train_time:82881ms step_avg:142.41ms
step:593/1370 train_time:83028ms step_avg:142.42ms
step:594/1370 train_time:83176ms step_avg:142.42ms
step:595/1370 train_time:83322ms step_avg:142.43ms
step:596/1370 train_time:83469ms step_avg:142.44ms
step:597/1370 train_time:83617ms step_avg:142.45ms
step:598/1370 train_time:83763ms step_avg:142.45ms
step:599/1370 train_time:83911ms step_avg:142.46ms
step:600/1370 train_time:84060ms step_avg:142.47ms
step:601/1370 train_time:84207ms step_avg:142.48ms
step:602/1370 train_time:84354ms step_avg:142.49ms
step:603/1370 train_time:84502ms step_avg:142.50ms
step:604/1370 train_time:84647ms step_avg:142.50ms
step:605/1370 train_time:84796ms step_avg:142.51ms
step:606/1370 train_time:84943ms step_avg:142.52ms
step:607/1370 train_time:85090ms step_avg:142.53ms
step:608/1370 train_time:85238ms step_avg:142.54ms
step:609/1370 train_time:85384ms step_avg:142.54ms
step:610/1370 train_time:85531ms step_avg:142.55ms
step:611/1370 train_time:85680ms step_avg:142.56ms
step:612/1370 train_time:85827ms step_avg:142.57ms
step:613/1370 train_time:85980ms step_avg:142.59ms
step:614/1370 train_time:86127ms step_avg:142.59ms
step:615/1370 train_time:86276ms step_avg:142.61ms
step:616/1370 train_time:86424ms step_avg:142.61ms
step:617/1370 train_time:86574ms step_avg:142.63ms
step:618/1370 train_time:86723ms step_avg:142.64ms
step:619/1370 train_time:86873ms step_avg:142.65ms
step:620/1370 train_time:87022ms step_avg:142.66ms
step:621/1370 train_time:87171ms step_avg:142.67ms
step:622/1370 train_time:87320ms step_avg:142.68ms
step:623/1370 train_time:87468ms step_avg:142.69ms
step:624/1370 train_time:87619ms step_avg:142.70ms
step:625/1370 train_time:87764ms step_avg:142.71ms
step:625/1370 val_loss:3.5768 train_time:87841ms step_avg:142.83ms
step:626/1370 train_time:87917ms step_avg:142.72ms
step:627/1370 train_time:88069ms step_avg:142.74ms
step:628/1370 train_time:88215ms step_avg:142.74ms
step:629/1370 train_time:88364ms step_avg:142.75ms
step:630/1370 train_time:88511ms step_avg:142.76ms
step:631/1370 train_time:88659ms step_avg:142.77ms
step:632/1370 train_time:88806ms step_avg:142.78ms
step:633/1370 train_time:88957ms step_avg:142.79ms
step:634/1370 train_time:89107ms step_avg:142.80ms
step:635/1370 train_time:89255ms step_avg:142.81ms
step:636/1370 train_time:89403ms step_avg:142.82ms
step:637/1370 train_time:89552ms step_avg:142.83ms
step:638/1370 train_time:89701ms step_avg:142.84ms
step:639/1370 train_time:89848ms step_avg:142.84ms
step:640/1370 train_time:89997ms step_avg:142.85ms
step:641/1370 train_time:90149ms step_avg:142.87ms
step:642/1370 train_time:90298ms step_avg:142.88ms
step:643/1370 train_time:90448ms step_avg:142.89ms
step:644/1370 train_time:90597ms step_avg:142.90ms
step:645/1370 train_time:90748ms step_avg:142.91ms
step:646/1370 train_time:90896ms step_avg:142.92ms
step:647/1370 train_time:91046ms step_avg:142.93ms
step:648/1370 train_time:91198ms step_avg:142.94ms
step:649/1370 train_time:91349ms step_avg:142.96ms
step:650/1370 train_time:91498ms step_avg:142.96ms
step:651/1370 train_time:91648ms step_avg:142.98ms
step:652/1370 train_time:91796ms step_avg:142.98ms
step:653/1370 train_time:91946ms step_avg:142.99ms
step:654/1370 train_time:92095ms step_avg:143.00ms
step:655/1370 train_time:92245ms step_avg:143.01ms
step:656/1370 train_time:92392ms step_avg:143.02ms
step:657/1370 train_time:92543ms step_avg:143.03ms
step:658/1370 train_time:92691ms step_avg:143.04ms
step:659/1370 train_time:92843ms step_avg:143.05ms
step:660/1370 train_time:92989ms step_avg:143.06ms
step:661/1370 train_time:93140ms step_avg:143.07ms
step:662/1370 train_time:93289ms step_avg:143.08ms
step:663/1370 train_time:93436ms step_avg:143.09ms
step:664/1370 train_time:93588ms step_avg:143.10ms
step:665/1370 train_time:93737ms step_avg:143.11ms
step:666/1370 train_time:93886ms step_avg:143.12ms
step:667/1370 train_time:94033ms step_avg:143.12ms
step:668/1370 train_time:94184ms step_avg:143.14ms
step:669/1370 train_time:94332ms step_avg:143.14ms
step:670/1370 train_time:94484ms step_avg:143.16ms
step:671/1370 train_time:94632ms step_avg:143.17ms
step:672/1370 train_time:94784ms step_avg:143.18ms
step:673/1370 train_time:94930ms step_avg:143.18ms
step:674/1370 train_time:95079ms step_avg:143.19ms
step:675/1370 train_time:95228ms step_avg:143.20ms
step:676/1370 train_time:95379ms step_avg:143.21ms
step:677/1370 train_time:95528ms step_avg:143.22ms
step:678/1370 train_time:95678ms step_avg:143.23ms
step:679/1370 train_time:95828ms step_avg:143.24ms
step:680/1370 train_time:95978ms step_avg:143.25ms
step:681/1370 train_time:96127ms step_avg:143.26ms
step:682/1370 train_time:96275ms step_avg:143.27ms
step:683/1370 train_time:96424ms step_avg:143.28ms
step:684/1370 train_time:96571ms step_avg:143.28ms
step:685/1370 train_time:96723ms step_avg:143.29ms
step:686/1370 train_time:96869ms step_avg:143.30ms
step:687/1370 train_time:97018ms step_avg:143.31ms
step:688/1370 train_time:97169ms step_avg:143.32ms
step:689/1370 train_time:97318ms step_avg:143.32ms
step:690/1370 train_time:97468ms step_avg:143.34ms
step:691/1370 train_time:97617ms step_avg:143.34ms
step:692/1370 train_time:97767ms step_avg:143.35ms
step:693/1370 train_time:97915ms step_avg:143.36ms
step:694/1370 train_time:98065ms step_avg:143.37ms
step:695/1370 train_time:98212ms step_avg:143.38ms
step:696/1370 train_time:98364ms step_avg:143.39ms
step:697/1370 train_time:98511ms step_avg:143.39ms
step:698/1370 train_time:98661ms step_avg:143.40ms
step:699/1370 train_time:98808ms step_avg:143.41ms
step:700/1370 train_time:98957ms step_avg:143.42ms
step:701/1370 train_time:99106ms step_avg:143.42ms
step:702/1370 train_time:99257ms step_avg:143.43ms
step:703/1370 train_time:99406ms step_avg:143.44ms
step:704/1370 train_time:99554ms step_avg:143.45ms
step:705/1370 train_time:99703ms step_avg:143.46ms
step:706/1370 train_time:99855ms step_avg:143.47ms
step:707/1370 train_time:100004ms step_avg:143.48ms
step:708/1370 train_time:100153ms step_avg:143.49ms
step:709/1370 train_time:100302ms step_avg:143.49ms
step:710/1370 train_time:100451ms step_avg:143.50ms
step:711/1370 train_time:100601ms step_avg:143.51ms
step:712/1370 train_time:100750ms step_avg:143.52ms
step:713/1370 train_time:100903ms step_avg:143.53ms
step:714/1370 train_time:101051ms step_avg:143.54ms
step:715/1370 train_time:101202ms step_avg:143.55ms
step:716/1370 train_time:101351ms step_avg:143.56ms
step:717/1370 train_time:101502ms step_avg:143.57ms
step:718/1370 train_time:101650ms step_avg:143.57ms
step:719/1370 train_time:101800ms step_avg:143.58ms
step:720/1370 train_time:101953ms step_avg:143.60ms
step:721/1370 train_time:102105ms step_avg:143.61ms
step:722/1370 train_time:102256ms step_avg:143.62ms
step:723/1370 train_time:102405ms step_avg:143.63ms
step:724/1370 train_time:102555ms step_avg:143.63ms
step:725/1370 train_time:102705ms step_avg:143.64ms
step:726/1370 train_time:102856ms step_avg:143.65ms
step:727/1370 train_time:103008ms step_avg:143.67ms
step:728/1370 train_time:103158ms step_avg:143.67ms
step:729/1370 train_time:103306ms step_avg:143.68ms
step:730/1370 train_time:103458ms step_avg:143.69ms
step:731/1370 train_time:103607ms step_avg:143.70ms
step:732/1370 train_time:103755ms step_avg:143.71ms
step:733/1370 train_time:103907ms step_avg:143.72ms
step:734/1370 train_time:104057ms step_avg:143.72ms
step:735/1370 train_time:104208ms step_avg:143.74ms
step:736/1370 train_time:104362ms step_avg:143.75ms
step:737/1370 train_time:104512ms step_avg:143.76ms
step:738/1370 train_time:104664ms step_avg:143.77ms
step:739/1370 train_time:104813ms step_avg:143.78ms
step:740/1370 train_time:104965ms step_avg:143.79ms
step:741/1370 train_time:105115ms step_avg:143.80ms
step:742/1370 train_time:105267ms step_avg:143.81ms
step:743/1370 train_time:105417ms step_avg:143.82ms
step:744/1370 train_time:105568ms step_avg:143.83ms
step:745/1370 train_time:105721ms step_avg:143.84ms
step:746/1370 train_time:105870ms step_avg:143.84ms
step:747/1370 train_time:106019ms step_avg:143.85ms
step:748/1370 train_time:106170ms step_avg:143.86ms
step:749/1370 train_time:106321ms step_avg:143.87ms
step:750/1370 train_time:106470ms step_avg:143.88ms
step:750/1370 val_loss:3.5240 train_time:106547ms step_avg:143.98ms
step:751/1370 train_time:106624ms step_avg:143.89ms
step:752/1370 train_time:106777ms step_avg:143.90ms
step:753/1370 train_time:106927ms step_avg:143.91ms
step:754/1370 train_time:107075ms step_avg:143.92ms
step:755/1370 train_time:107225ms step_avg:143.93ms
step:756/1370 train_time:107373ms step_avg:143.93ms
step:757/1370 train_time:107526ms step_avg:143.94ms
step:758/1370 train_time:107676ms step_avg:143.95ms
step:759/1370 train_time:107825ms step_avg:143.96ms
step:760/1370 train_time:107973ms step_avg:143.96ms
step:761/1370 train_time:108164ms step_avg:144.03ms
step:762/1370 train_time:108314ms step_avg:144.03ms
step:763/1370 train_time:108464ms step_avg:144.04ms
step:764/1370 train_time:108614ms step_avg:144.05ms
step:765/1370 train_time:108762ms step_avg:144.06ms
step:766/1370 train_time:108914ms step_avg:144.07ms
step:767/1370 train_time:109065ms step_avg:144.08ms
step:768/1370 train_time:109217ms step_avg:144.09ms
step:769/1370 train_time:109372ms step_avg:144.10ms
step:770/1370 train_time:109521ms step_avg:144.11ms
step:771/1370 train_time:109671ms step_avg:144.11ms
step:772/1370 train_time:109820ms step_avg:144.12ms
step:773/1370 train_time:109970ms step_avg:144.13ms
step:774/1370 train_time:110123ms step_avg:144.14ms
step:775/1370 train_time:110273ms step_avg:144.15ms
step:776/1370 train_time:110427ms step_avg:144.16ms
step:777/1370 train_time:110578ms step_avg:144.17ms
step:778/1370 train_time:110730ms step_avg:144.18ms
step:779/1370 train_time:110878ms step_avg:144.18ms
step:780/1370 train_time:111032ms step_avg:144.20ms
step:781/1370 train_time:111181ms step_avg:144.20ms
step:782/1370 train_time:111332ms step_avg:144.21ms
step:783/1370 train_time:111482ms step_avg:144.22ms
step:784/1370 train_time:111633ms step_avg:144.23ms
step:785/1370 train_time:111782ms step_avg:144.23ms
step:786/1370 train_time:111933ms step_avg:144.24ms
step:787/1370 train_time:112083ms step_avg:144.25ms
step:788/1370 train_time:112234ms step_avg:144.26ms
step:789/1370 train_time:112383ms step_avg:144.27ms
step:790/1370 train_time:112533ms step_avg:144.27ms
step:791/1370 train_time:112683ms step_avg:144.28ms
step:792/1370 train_time:112834ms step_avg:144.29ms
step:793/1370 train_time:112983ms step_avg:144.30ms
step:794/1370 train_time:113134ms step_avg:144.30ms
step:795/1370 train_time:113286ms step_avg:144.31ms
step:796/1370 train_time:113435ms step_avg:144.32ms
step:797/1370 train_time:113586ms step_avg:144.33ms
step:798/1370 train_time:113736ms step_avg:144.34ms
step:799/1370 train_time:113891ms step_avg:144.35ms
step:800/1370 train_time:114040ms step_avg:144.35ms
step:801/1370 train_time:114192ms step_avg:144.36ms
step:802/1370 train_time:114342ms step_avg:144.37ms
step:803/1370 train_time:114492ms step_avg:144.38ms
step:804/1370 train_time:114640ms step_avg:144.38ms
step:805/1370 train_time:114795ms step_avg:144.40ms
step:806/1370 train_time:114944ms step_avg:144.40ms
step:807/1370 train_time:115095ms step_avg:144.41ms
step:808/1370 train_time:115246ms step_avg:144.42ms
step:809/1370 train_time:115394ms step_avg:144.42ms
step:810/1370 train_time:115543ms step_avg:144.43ms
step:811/1370 train_time:115694ms step_avg:144.44ms
step:812/1370 train_time:115846ms step_avg:144.45ms
step:813/1370 train_time:115994ms step_avg:144.45ms
step:814/1370 train_time:116146ms step_avg:144.46ms
step:815/1370 train_time:116295ms step_avg:144.47ms
step:816/1370 train_time:116449ms step_avg:144.48ms
step:817/1370 train_time:116599ms step_avg:144.48ms
step:818/1370 train_time:116750ms step_avg:144.49ms
step:819/1370 train_time:116901ms step_avg:144.50ms
step:820/1370 train_time:117058ms step_avg:144.52ms
step:821/1370 train_time:117210ms step_avg:144.53ms
step:822/1370 train_time:117360ms step_avg:144.53ms
step:823/1370 train_time:117512ms step_avg:144.54ms
step:824/1370 train_time:117661ms step_avg:144.55ms
step:825/1370 train_time:117814ms step_avg:144.56ms
step:826/1370 train_time:117967ms step_avg:144.57ms
step:827/1370 train_time:118116ms step_avg:144.57ms
step:828/1370 train_time:118270ms step_avg:144.58ms
step:829/1370 train_time:118421ms step_avg:144.59ms
step:830/1370 train_time:118573ms step_avg:144.60ms
step:831/1370 train_time:118724ms step_avg:144.61ms
step:832/1370 train_time:118875ms step_avg:144.62ms
step:833/1370 train_time:119025ms step_avg:144.62ms
step:834/1370 train_time:119175ms step_avg:144.63ms
step:835/1370 train_time:119330ms step_avg:144.64ms
step:836/1370 train_time:119481ms step_avg:144.65ms
step:837/1370 train_time:119633ms step_avg:144.66ms
step:838/1370 train_time:119784ms step_avg:144.67ms
step:839/1370 train_time:119933ms step_avg:144.67ms
step:840/1370 train_time:120083ms step_avg:144.68ms
step:841/1370 train_time:120235ms step_avg:144.69ms
step:842/1370 train_time:120389ms step_avg:144.70ms
step:843/1370 train_time:120538ms step_avg:144.70ms
step:844/1370 train_time:120690ms step_avg:144.71ms
step:845/1370 train_time:120840ms step_avg:144.72ms
step:846/1370 train_time:120993ms step_avg:144.73ms
step:847/1370 train_time:121145ms step_avg:144.74ms
step:848/1370 train_time:121294ms step_avg:144.74ms
step:849/1370 train_time:121446ms step_avg:144.75ms
step:850/1370 train_time:121598ms step_avg:144.76ms
step:851/1370 train_time:121752ms step_avg:144.77ms
step:852/1370 train_time:121903ms step_avg:144.78ms
step:853/1370 train_time:122054ms step_avg:144.79ms
step:854/1370 train_time:122204ms step_avg:144.79ms
step:855/1370 train_time:122357ms step_avg:144.80ms
step:856/1370 train_time:122508ms step_avg:144.81ms
step:857/1370 train_time:122659ms step_avg:144.82ms
step:858/1370 train_time:122815ms step_avg:144.83ms
step:859/1370 train_time:122966ms step_avg:144.84ms
step:860/1370 train_time:123116ms step_avg:144.84ms
step:861/1370 train_time:123267ms step_avg:144.85ms
step:862/1370 train_time:123418ms step_avg:144.86ms
step:863/1370 train_time:123570ms step_avg:144.87ms
step:864/1370 train_time:123724ms step_avg:144.88ms
step:865/1370 train_time:123874ms step_avg:144.88ms
step:866/1370 train_time:124033ms step_avg:144.90ms
step:867/1370 train_time:124183ms step_avg:144.90ms
step:868/1370 train_time:124333ms step_avg:144.91ms
step:869/1370 train_time:124484ms step_avg:144.92ms
step:870/1370 train_time:124638ms step_avg:144.93ms
step:871/1370 train_time:124790ms step_avg:144.94ms
step:872/1370 train_time:124940ms step_avg:144.94ms
step:873/1370 train_time:125093ms step_avg:144.95ms
step:874/1370 train_time:125244ms step_avg:144.96ms
step:875/1370 train_time:125395ms step_avg:144.97ms
step:875/1370 val_loss:3.4707 train_time:125471ms step_avg:145.05ms
step:876/1370 train_time:125547ms step_avg:144.97ms
step:877/1370 train_time:125703ms step_avg:144.99ms
step:878/1370 train_time:125855ms step_avg:144.99ms
step:879/1370 train_time:126005ms step_avg:145.00ms
step:880/1370 train_time:126156ms step_avg:145.01ms
step:881/1370 train_time:126305ms step_avg:145.01ms
step:882/1370 train_time:126459ms step_avg:145.02ms
step:883/1370 train_time:126611ms step_avg:145.03ms
step:884/1370 train_time:126764ms step_avg:145.04ms
step:885/1370 train_time:126914ms step_avg:145.04ms
step:886/1370 train_time:127068ms step_avg:145.05ms
step:887/1370 train_time:127218ms step_avg:145.06ms
step:888/1370 train_time:127371ms step_avg:145.07ms
step:889/1370 train_time:127526ms step_avg:145.08ms
step:890/1370 train_time:127677ms step_avg:145.09ms
step:891/1370 train_time:127826ms step_avg:145.09ms
step:892/1370 train_time:127980ms step_avg:145.10ms
step:893/1370 train_time:128130ms step_avg:145.11ms
step:894/1370 train_time:128283ms step_avg:145.12ms
step:895/1370 train_time:128437ms step_avg:145.13ms
step:896/1370 train_time:128587ms step_avg:145.13ms
step:897/1370 train_time:128741ms step_avg:145.14ms
step:898/1370 train_time:128895ms step_avg:145.15ms
step:899/1370 train_time:129045ms step_avg:145.16ms
step:900/1370 train_time:129196ms step_avg:145.16ms
step:901/1370 train_time:129348ms step_avg:145.17ms
step:902/1370 train_time:129499ms step_avg:145.18ms
step:903/1370 train_time:129651ms step_avg:145.19ms
step:904/1370 train_time:129804ms step_avg:145.19ms
step:905/1370 train_time:129957ms step_avg:145.20ms
step:906/1370 train_time:130107ms step_avg:145.21ms
step:907/1370 train_time:130261ms step_avg:145.22ms
step:908/1370 train_time:130412ms step_avg:145.23ms
step:909/1370 train_time:130566ms step_avg:145.23ms
step:910/1370 train_time:130723ms step_avg:145.25ms
step:911/1370 train_time:130876ms step_avg:145.26ms
step:912/1370 train_time:131025ms step_avg:145.26ms
step:913/1370 train_time:131179ms step_avg:145.27ms
step:914/1370 train_time:131328ms step_avg:145.27ms
step:915/1370 train_time:131482ms step_avg:145.28ms
step:916/1370 train_time:131634ms step_avg:145.29ms
step:917/1370 train_time:131786ms step_avg:145.30ms
step:918/1370 train_time:131940ms step_avg:145.31ms
step:919/1370 train_time:132096ms step_avg:145.32ms
step:920/1370 train_time:132246ms step_avg:145.32ms
step:921/1370 train_time:132401ms step_avg:145.34ms
step:922/1370 train_time:132557ms step_avg:145.35ms
step:923/1370 train_time:132708ms step_avg:145.35ms
step:924/1370 train_time:132862ms step_avg:145.36ms
step:925/1370 train_time:133015ms step_avg:145.37ms
step:926/1370 train_time:133168ms step_avg:145.38ms
step:927/1370 train_time:133320ms step_avg:145.39ms
step:928/1370 train_time:133476ms step_avg:145.40ms
step:929/1370 train_time:133630ms step_avg:145.41ms
step:930/1370 train_time:133785ms step_avg:145.42ms
step:931/1370 train_time:133938ms step_avg:145.43ms
step:932/1370 train_time:134088ms step_avg:145.43ms
step:933/1370 train_time:134241ms step_avg:145.44ms
step:934/1370 train_time:134393ms step_avg:145.45ms
step:935/1370 train_time:134547ms step_avg:145.46ms
step:936/1370 train_time:134703ms step_avg:145.47ms
step:937/1370 train_time:134860ms step_avg:145.48ms
step:938/1370 train_time:135011ms step_avg:145.49ms
step:939/1370 train_time:135165ms step_avg:145.50ms
step:940/1370 train_time:135319ms step_avg:145.50ms
step:941/1370 train_time:135470ms step_avg:145.51ms
step:942/1370 train_time:135621ms step_avg:145.52ms
step:943/1370 train_time:135778ms step_avg:145.53ms
step:944/1370 train_time:135936ms step_avg:145.54ms
step:945/1370 train_time:136089ms step_avg:145.55ms
step:946/1370 train_time:136245ms step_avg:145.56ms
step:947/1370 train_time:136398ms step_avg:145.57ms
step:948/1370 train_time:136549ms step_avg:145.57ms
step:949/1370 train_time:136703ms step_avg:145.58ms
step:950/1370 train_time:136856ms step_avg:145.59ms
step:951/1370 train_time:137053ms step_avg:145.65ms
step:952/1370 train_time:137204ms step_avg:145.65ms
step:953/1370 train_time:137359ms step_avg:145.66ms
step:954/1370 train_time:137510ms step_avg:145.67ms
step:955/1370 train_time:137662ms step_avg:145.67ms
step:956/1370 train_time:137814ms step_avg:145.68ms
step:957/1370 train_time:137969ms step_avg:145.69ms
step:958/1370 train_time:138126ms step_avg:145.70ms
step:959/1370 train_time:138282ms step_avg:145.71ms
step:960/1370 train_time:138437ms step_avg:145.72ms
step:961/1370 train_time:138590ms step_avg:145.73ms
step:962/1370 train_time:138744ms step_avg:145.74ms
step:963/1370 train_time:138902ms step_avg:145.75ms
step:964/1370 train_time:139054ms step_avg:145.76ms
step:965/1370 train_time:139204ms step_avg:145.76ms
step:966/1370 train_time:139358ms step_avg:145.77ms
step:967/1370 train_time:139508ms step_avg:145.78ms
step:968/1370 train_time:139661ms step_avg:145.78ms
step:969/1370 train_time:139816ms step_avg:145.79ms
step:970/1370 train_time:139967ms step_avg:145.80ms
step:971/1370 train_time:140120ms step_avg:145.81ms
step:972/1370 train_time:140272ms step_avg:145.81ms
step:973/1370 train_time:140424ms step_avg:145.82ms
step:974/1370 train_time:140578ms step_avg:145.83ms
step:975/1370 train_time:140732ms step_avg:145.84ms
step:976/1370 train_time:140886ms step_avg:145.84ms
step:977/1370 train_time:141040ms step_avg:145.85ms
step:978/1370 train_time:141192ms step_avg:145.86ms
step:979/1370 train_time:141344ms step_avg:145.87ms
step:980/1370 train_time:141496ms step_avg:145.87ms
step:981/1370 train_time:141645ms step_avg:145.88ms
step:982/1370 train_time:141797ms step_avg:145.88ms
step:983/1370 train_time:141948ms step_avg:145.89ms
step:984/1370 train_time:142100ms step_avg:145.89ms
step:985/1370 train_time:142252ms step_avg:145.90ms
step:986/1370 train_time:142407ms step_avg:145.91ms
step:987/1370 train_time:142559ms step_avg:145.92ms
step:988/1370 train_time:142710ms step_avg:145.92ms
step:989/1370 train_time:142862ms step_avg:145.93ms
step:990/1370 train_time:143015ms step_avg:145.93ms
step:991/1370 train_time:143165ms step_avg:145.94ms
step:992/1370 train_time:143324ms step_avg:145.95ms
step:993/1370 train_time:143487ms step_avg:145.97ms
step:994/1370 train_time:143640ms step_avg:145.98ms
step:995/1370 train_time:143790ms step_avg:145.98ms
step:996/1370 train_time:143941ms step_avg:145.98ms
step:997/1370 train_time:144092ms step_avg:145.99ms
step:998/1370 train_time:144246ms step_avg:146.00ms
step:999/1370 train_time:144399ms step_avg:146.01ms
step:1000/1370 train_time:144551ms step_avg:146.01ms
step:1000/1370 val_loss:3.4042 train_time:144628ms step_avg:146.09ms
step:1001/1370 train_time:144705ms step_avg:146.02ms
step:1002/1370 train_time:144860ms step_avg:146.03ms
step:1003/1370 train_time:145013ms step_avg:146.04ms
step:1004/1370 train_time:145168ms step_avg:146.04ms
step:1005/1370 train_time:145320ms step_avg:146.05ms
step:1006/1370 train_time:145470ms step_avg:146.05ms
step:1007/1370 train_time:145626ms step_avg:146.06ms
step:1008/1370 train_time:145781ms step_avg:146.07ms
step:1009/1370 train_time:145939ms step_avg:146.08ms
step:1010/1370 train_time:146090ms step_avg:146.09ms
step:1011/1370 train_time:146245ms step_avg:146.10ms
step:1012/1370 train_time:146396ms step_avg:146.10ms
step:1013/1370 train_time:146549ms step_avg:146.11ms
step:1014/1370 train_time:146703ms step_avg:146.12ms
step:1015/1370 train_time:146856ms step_avg:146.13ms
step:1016/1370 train_time:147010ms step_avg:146.13ms
step:1017/1370 train_time:147165ms step_avg:146.14ms
step:1018/1370 train_time:147318ms step_avg:146.15ms
step:1019/1370 train_time:147474ms step_avg:146.16ms
step:1020/1370 train_time:147630ms step_avg:146.17ms
step:1021/1370 train_time:147784ms step_avg:146.18ms
step:1022/1370 train_time:147938ms step_avg:146.18ms
step:1023/1370 train_time:148091ms step_avg:146.19ms
step:1024/1370 train_time:148246ms step_avg:146.20ms
step:1025/1370 train_time:148400ms step_avg:146.21ms
step:1026/1370 train_time:148550ms step_avg:146.21ms
step:1027/1370 train_time:148703ms step_avg:146.22ms
step:1028/1370 train_time:148858ms step_avg:146.23ms
step:1029/1370 train_time:149017ms step_avg:146.24ms
step:1030/1370 train_time:149172ms step_avg:146.25ms
step:1031/1370 train_time:149325ms step_avg:146.25ms
step:1032/1370 train_time:149477ms step_avg:146.26ms
step:1033/1370 train_time:149631ms step_avg:146.27ms
step:1034/1370 train_time:149785ms step_avg:146.27ms
step:1035/1370 train_time:149942ms step_avg:146.28ms
step:1036/1370 train_time:150097ms step_avg:146.29ms
step:1037/1370 train_time:150253ms step_avg:146.30ms
step:1038/1370 train_time:150408ms step_avg:146.31ms
step:1039/1370 train_time:150560ms step_avg:146.32ms
step:1040/1370 train_time:150712ms step_avg:146.32ms
step:1041/1370 train_time:150866ms step_avg:146.33ms
step:1042/1370 train_time:151017ms step_avg:146.33ms
step:1043/1370 train_time:151167ms step_avg:146.34ms
step:1044/1370 train_time:151326ms step_avg:146.35ms
step:1045/1370 train_time:151481ms step_avg:146.36ms
step:1046/1370 train_time:151633ms step_avg:146.36ms
step:1047/1370 train_time:151787ms step_avg:146.37ms
step:1048/1370 train_time:151942ms step_avg:146.38ms
step:1049/1370 train_time:152097ms step_avg:146.39ms
step:1050/1370 train_time:152255ms step_avg:146.40ms
step:1051/1370 train_time:152413ms step_avg:146.41ms
step:1052/1370 train_time:152565ms step_avg:146.42ms
step:1053/1370 train_time:152718ms step_avg:146.42ms
step:1054/1370 train_time:152874ms step_avg:146.43ms
step:1055/1370 train_time:153028ms step_avg:146.44ms
step:1056/1370 train_time:153184ms step_avg:146.45ms
step:1057/1370 train_time:153338ms step_avg:146.45ms
step:1058/1370 train_time:153493ms step_avg:146.46ms
step:1059/1370 train_time:153648ms step_avg:146.47ms
step:1060/1370 train_time:153803ms step_avg:146.48ms
step:1061/1370 train_time:153954ms step_avg:146.48ms
step:1062/1370 train_time:154110ms step_avg:146.49ms
step:1063/1370 train_time:154264ms step_avg:146.50ms
step:1064/1370 train_time:154418ms step_avg:146.51ms
step:1065/1370 train_time:154571ms step_avg:146.51ms
step:1066/1370 train_time:154730ms step_avg:146.52ms
step:1067/1370 train_time:154885ms step_avg:146.53ms
step:1068/1370 train_time:155037ms step_avg:146.54ms
step:1069/1370 train_time:155198ms step_avg:146.55ms
step:1070/1370 train_time:155351ms step_avg:146.56ms
step:1071/1370 train_time:155506ms step_avg:146.57ms
step:1072/1370 train_time:155659ms step_avg:146.57ms
step:1073/1370 train_time:155811ms step_avg:146.58ms
step:1074/1370 train_time:155963ms step_avg:146.58ms
step:1075/1370 train_time:156118ms step_avg:146.59ms
step:1076/1370 train_time:156272ms step_avg:146.60ms
step:1077/1370 train_time:156428ms step_avg:146.61ms
step:1078/1370 train_time:156587ms step_avg:146.62ms
step:1079/1370 train_time:156744ms step_avg:146.63ms
step:1080/1370 train_time:156899ms step_avg:146.63ms
step:1081/1370 train_time:157051ms step_avg:146.64ms
step:1082/1370 train_time:157205ms step_avg:146.65ms
step:1083/1370 train_time:157358ms step_avg:146.65ms
step:1084/1370 train_time:157516ms step_avg:146.66ms
step:1085/1370 train_time:157669ms step_avg:146.67ms
step:1086/1370 train_time:157827ms step_avg:146.68ms
step:1087/1370 train_time:157983ms step_avg:146.69ms
step:1088/1370 train_time:158135ms step_avg:146.69ms
step:1089/1370 train_time:158294ms step_avg:146.70ms
step:1090/1370 train_time:158452ms step_avg:146.72ms
step:1091/1370 train_time:158607ms step_avg:146.72ms
step:1092/1370 train_time:158759ms step_avg:146.73ms
step:1093/1370 train_time:158915ms step_avg:146.74ms
step:1094/1370 train_time:159068ms step_avg:146.74ms
step:1095/1370 train_time:159222ms step_avg:146.75ms
step:1096/1370 train_time:159378ms step_avg:146.76ms
step:1097/1370 train_time:159532ms step_avg:146.76ms
step:1098/1370 train_time:159686ms step_avg:146.77ms
step:1099/1370 train_time:159839ms step_avg:146.78ms
step:1100/1370 train_time:159990ms step_avg:146.78ms
step:1101/1370 train_time:160142ms step_avg:146.78ms
step:1102/1370 train_time:160298ms step_avg:146.79ms
step:1103/1370 train_time:160451ms step_avg:146.80ms
step:1104/1370 train_time:160604ms step_avg:146.80ms
step:1105/1370 train_time:160759ms step_avg:146.81ms
step:1106/1370 train_time:160913ms step_avg:146.82ms
step:1107/1370 train_time:161067ms step_avg:146.82ms
step:1108/1370 train_time:161225ms step_avg:146.84ms
step:1109/1370 train_time:161378ms step_avg:146.84ms
step:1110/1370 train_time:161532ms step_avg:146.85ms
step:1111/1370 train_time:161688ms step_avg:146.86ms
step:1112/1370 train_time:161843ms step_avg:146.86ms
step:1113/1370 train_time:161995ms step_avg:146.87ms
step:1114/1370 train_time:162152ms step_avg:146.88ms
step:1115/1370 train_time:162307ms step_avg:146.88ms
step:1116/1370 train_time:162458ms step_avg:146.89ms
step:1117/1370 train_time:162615ms step_avg:146.90ms
step:1118/1370 train_time:162773ms step_avg:146.91ms
step:1119/1370 train_time:162930ms step_avg:146.92ms
step:1120/1370 train_time:163086ms step_avg:146.92ms
step:1121/1370 train_time:163241ms step_avg:146.93ms
step:1122/1370 train_time:163392ms step_avg:146.94ms
step:1123/1370 train_time:163547ms step_avg:146.94ms
step:1124/1370 train_time:163705ms step_avg:146.95ms
step:1125/1370 train_time:163859ms step_avg:146.96ms
step:1125/1370 val_loss:3.3503 train_time:163936ms step_avg:147.03ms
step:1126/1370 train_time:164013ms step_avg:146.96ms
step:1127/1370 train_time:164170ms step_avg:146.97ms
step:1128/1370 train_time:164325ms step_avg:146.98ms
step:1129/1370 train_time:164482ms step_avg:146.99ms
step:1130/1370 train_time:164636ms step_avg:147.00ms
step:1131/1370 train_time:164793ms step_avg:147.01ms
step:1132/1370 train_time:164947ms step_avg:147.01ms
step:1133/1370 train_time:165102ms step_avg:147.02ms
step:1134/1370 train_time:165258ms step_avg:147.03ms
step:1135/1370 train_time:165412ms step_avg:147.03ms
step:1136/1370 train_time:165571ms step_avg:147.04ms
step:1137/1370 train_time:165727ms step_avg:147.05ms
step:1138/1370 train_time:165882ms step_avg:147.06ms
step:1139/1370 train_time:166036ms step_avg:147.06ms
step:1140/1370 train_time:166191ms step_avg:147.07ms
step:1141/1370 train_time:166388ms step_avg:147.12ms
step:1142/1370 train_time:166540ms step_avg:147.12ms
step:1143/1370 train_time:166698ms step_avg:147.13ms
step:1144/1370 train_time:166854ms step_avg:147.14ms
step:1145/1370 train_time:167006ms step_avg:147.14ms
step:1146/1370 train_time:167164ms step_avg:147.15ms
step:1147/1370 train_time:167321ms step_avg:147.16ms
step:1148/1370 train_time:167476ms step_avg:147.17ms
step:1149/1370 train_time:167631ms step_avg:147.17ms
step:1150/1370 train_time:167784ms step_avg:147.18ms
step:1151/1370 train_time:167939ms step_avg:147.19ms
step:1152/1370 train_time:168095ms step_avg:147.19ms
step:1153/1370 train_time:168253ms step_avg:147.20ms
step:1154/1370 train_time:168408ms step_avg:147.21ms
step:1155/1370 train_time:168562ms step_avg:147.22ms
step:1156/1370 train_time:168721ms step_avg:147.23ms
step:1157/1370 train_time:168878ms step_avg:147.23ms
step:1158/1370 train_time:169032ms step_avg:147.24ms
step:1159/1370 train_time:169186ms step_avg:147.25ms
step:1160/1370 train_time:169339ms step_avg:147.25ms
step:1161/1370 train_time:169495ms step_avg:147.26ms
step:1162/1370 train_time:169652ms step_avg:147.27ms
step:1163/1370 train_time:169806ms step_avg:147.27ms
step:1164/1370 train_time:169961ms step_avg:147.28ms
step:1165/1370 train_time:170114ms step_avg:147.28ms
step:1166/1370 train_time:170269ms step_avg:147.29ms
step:1167/1370 train_time:170421ms step_avg:147.30ms
step:1168/1370 train_time:170576ms step_avg:147.30ms
step:1169/1370 train_time:170732ms step_avg:147.31ms
step:1170/1370 train_time:170885ms step_avg:147.31ms
step:1171/1370 train_time:171043ms step_avg:147.32ms
step:1172/1370 train_time:171199ms step_avg:147.33ms
step:1173/1370 train_time:171354ms step_avg:147.34ms
step:1174/1370 train_time:171518ms step_avg:147.35ms
step:1175/1370 train_time:171673ms step_avg:147.36ms
step:1176/1370 train_time:171829ms step_avg:147.37ms
step:1177/1370 train_time:171990ms step_avg:147.38ms
step:1178/1370 train_time:172145ms step_avg:147.38ms
step:1179/1370 train_time:172298ms step_avg:147.39ms
step:1180/1370 train_time:172461ms step_avg:147.40ms
step:1181/1370 train_time:172618ms step_avg:147.41ms
step:1182/1370 train_time:172772ms step_avg:147.42ms
step:1183/1370 train_time:172925ms step_avg:147.42ms
step:1184/1370 train_time:173079ms step_avg:147.43ms
step:1185/1370 train_time:173238ms step_avg:147.44ms
step:1186/1370 train_time:173391ms step_avg:147.44ms
step:1187/1370 train_time:173555ms step_avg:147.46ms
step:1188/1370 train_time:173709ms step_avg:147.46ms
step:1189/1370 train_time:173865ms step_avg:147.47ms
step:1190/1370 train_time:174021ms step_avg:147.48ms
step:1191/1370 train_time:174179ms step_avg:147.48ms
step:1192/1370 train_time:174332ms step_avg:147.49ms
step:1193/1370 train_time:174487ms step_avg:147.50ms
step:1194/1370 train_time:174645ms step_avg:147.50ms
step:1195/1370 train_time:174799ms step_avg:147.51ms
step:1196/1370 train_time:174956ms step_avg:147.52ms
step:1197/1370 train_time:175113ms step_avg:147.53ms
step:1198/1370 train_time:175274ms step_avg:147.54ms
step:1199/1370 train_time:175427ms step_avg:147.54ms
step:1200/1370 train_time:175580ms step_avg:147.55ms
step:1201/1370 train_time:175734ms step_avg:147.55ms
step:1202/1370 train_time:175901ms step_avg:147.57ms
step:1203/1370 train_time:176059ms step_avg:147.58ms
step:1204/1370 train_time:176216ms step_avg:147.58ms
step:1205/1370 train_time:176371ms step_avg:147.59ms
step:1206/1370 train_time:176525ms step_avg:147.60ms
step:1207/1370 train_time:176681ms step_avg:147.60ms
step:1208/1370 train_time:176838ms step_avg:147.61ms
step:1209/1370 train_time:176992ms step_avg:147.62ms
step:1210/1370 train_time:177153ms step_avg:147.63ms
step:1211/1370 train_time:177309ms step_avg:147.63ms
step:1212/1370 train_time:177464ms step_avg:147.64ms
step:1213/1370 train_time:177618ms step_avg:147.65ms
step:1214/1370 train_time:177774ms step_avg:147.65ms
step:1215/1370 train_time:177930ms step_avg:147.66ms
step:1216/1370 train_time:178084ms step_avg:147.67ms
step:1217/1370 train_time:178239ms step_avg:147.67ms
step:1218/1370 train_time:178392ms step_avg:147.68ms
step:1219/1370 train_time:178547ms step_avg:147.68ms
step:1220/1370 train_time:178702ms step_avg:147.69ms
step:1221/1370 train_time:178857ms step_avg:147.69ms
step:1222/1370 train_time:179013ms step_avg:147.70ms
step:1223/1370 train_time:179172ms step_avg:147.71ms
step:1224/1370 train_time:179330ms step_avg:147.72ms
step:1225/1370 train_time:179485ms step_avg:147.72ms
step:1226/1370 train_time:179640ms step_avg:147.73ms
step:1227/1370 train_time:179797ms step_avg:147.74ms
step:1228/1370 train_time:179954ms step_avg:147.75ms
step:1229/1370 train_time:180108ms step_avg:147.75ms
step:1230/1370 train_time:180269ms step_avg:147.76ms
step:1231/1370 train_time:180428ms step_avg:147.77ms
step:1232/1370 train_time:180587ms step_avg:147.78ms
step:1233/1370 train_time:180744ms step_avg:147.79ms
step:1234/1370 train_time:180897ms step_avg:147.79ms
step:1235/1370 train_time:181054ms step_avg:147.80ms
step:1236/1370 train_time:181212ms step_avg:147.81ms
step:1237/1370 train_time:181366ms step_avg:147.81ms
step:1238/1370 train_time:181530ms step_avg:147.83ms
step:1239/1370 train_time:181686ms step_avg:147.83ms
step:1240/1370 train_time:181848ms step_avg:147.84ms
step:1241/1370 train_time:182009ms step_avg:147.85ms
step:1242/1370 train_time:182168ms step_avg:147.86ms
step:1243/1370 train_time:182327ms step_avg:147.87ms
step:1244/1370 train_time:182480ms step_avg:147.88ms
step:1245/1370 train_time:182636ms step_avg:147.88ms
step:1246/1370 train_time:182790ms step_avg:147.89ms
step:1247/1370 train_time:182949ms step_avg:147.90ms
step:1248/1370 train_time:183103ms step_avg:147.90ms
step:1249/1370 train_time:183258ms step_avg:147.91ms
step:1250/1370 train_time:183413ms step_avg:147.91ms
step:1250/1370 val_loss:3.3050 train_time:183494ms step_avg:147.98ms
step:1251/1370 train_time:183575ms step_avg:147.93ms
step:1252/1370 train_time:183730ms step_avg:147.93ms
step:1253/1370 train_time:183885ms step_avg:147.94ms
step:1254/1370 train_time:184039ms step_avg:147.94ms
step:1255/1370 train_time:184205ms step_avg:147.96ms
step:1256/1370 train_time:184359ms step_avg:147.96ms
step:1257/1370 train_time:184513ms step_avg:147.97ms
step:1258/1370 train_time:184673ms step_avg:147.97ms
step:1259/1370 train_time:184829ms step_avg:147.98ms
step:1260/1370 train_time:184984ms step_avg:147.99ms
step:1261/1370 train_time:185141ms step_avg:147.99ms
step:1262/1370 train_time:185299ms step_avg:148.00ms
step:1263/1370 train_time:185454ms step_avg:148.01ms
step:1264/1370 train_time:185606ms step_avg:148.01ms
step:1265/1370 train_time:185762ms step_avg:148.02ms
step:1266/1370 train_time:185917ms step_avg:148.02ms
step:1267/1370 train_time:186074ms step_avg:148.03ms
step:1268/1370 train_time:186229ms step_avg:148.04ms
step:1269/1370 train_time:186389ms step_avg:148.05ms
step:1270/1370 train_time:186543ms step_avg:148.05ms
step:1271/1370 train_time:186700ms step_avg:148.06ms
step:1272/1370 train_time:186856ms step_avg:148.06ms
step:1273/1370 train_time:187008ms step_avg:148.07ms
step:1274/1370 train_time:187162ms step_avg:148.07ms
step:1275/1370 train_time:187318ms step_avg:148.08ms
step:1276/1370 train_time:187473ms step_avg:148.08ms
step:1277/1370 train_time:187628ms step_avg:148.09ms
step:1278/1370 train_time:187782ms step_avg:148.09ms
step:1279/1370 train_time:187939ms step_avg:148.10ms
step:1280/1370 train_time:188100ms step_avg:148.11ms
step:1281/1370 train_time:188255ms step_avg:148.12ms
step:1282/1370 train_time:188407ms step_avg:148.12ms
step:1283/1370 train_time:188562ms step_avg:148.12ms
step:1284/1370 train_time:188721ms step_avg:148.13ms
step:1285/1370 train_time:188876ms step_avg:148.14ms
step:1286/1370 train_time:189030ms step_avg:148.14ms
step:1287/1370 train_time:189185ms step_avg:148.15ms
step:1288/1370 train_time:189342ms step_avg:148.15ms
step:1289/1370 train_time:189504ms step_avg:148.17ms
step:1290/1370 train_time:189665ms step_avg:148.18ms
step:1291/1370 train_time:189825ms step_avg:148.18ms
step:1292/1370 train_time:189981ms step_avg:148.19ms
step:1293/1370 train_time:190142ms step_avg:148.20ms
step:1294/1370 train_time:190297ms step_avg:148.21ms
step:1295/1370 train_time:190453ms step_avg:148.21ms
step:1296/1370 train_time:190610ms step_avg:148.22ms
step:1297/1370 train_time:190769ms step_avg:148.23ms
step:1298/1370 train_time:190925ms step_avg:148.23ms
step:1299/1370 train_time:191081ms step_avg:148.24ms
step:1300/1370 train_time:191235ms step_avg:148.24ms
step:1301/1370 train_time:191388ms step_avg:148.25ms
step:1302/1370 train_time:191546ms step_avg:148.26ms
step:1303/1370 train_time:191705ms step_avg:148.26ms
step:1304/1370 train_time:191866ms step_avg:148.27ms
step:1305/1370 train_time:192022ms step_avg:148.28ms
step:1306/1370 train_time:192180ms step_avg:148.29ms
step:1307/1370 train_time:192332ms step_avg:148.29ms
step:1308/1370 train_time:192488ms step_avg:148.30ms
step:1309/1370 train_time:192644ms step_avg:148.30ms
step:1310/1370 train_time:192800ms step_avg:148.31ms
step:1311/1370 train_time:192953ms step_avg:148.31ms
step:1312/1370 train_time:193106ms step_avg:148.32ms
step:1313/1370 train_time:193261ms step_avg:148.32ms
step:1314/1370 train_time:193416ms step_avg:148.32ms
step:1315/1370 train_time:193570ms step_avg:148.33ms
step:1316/1370 train_time:193724ms step_avg:148.33ms
step:1317/1370 train_time:193878ms step_avg:148.34ms
step:1318/1370 train_time:194040ms step_avg:148.35ms
step:1319/1370 train_time:194197ms step_avg:148.36ms
step:1320/1370 train_time:194354ms step_avg:148.36ms
step:1321/1370 train_time:194512ms step_avg:148.37ms
step:1322/1370 train_time:194674ms step_avg:148.38ms
step:1323/1370 train_time:194830ms step_avg:148.39ms
step:1324/1370 train_time:194985ms step_avg:148.39ms
step:1325/1370 train_time:195142ms step_avg:148.40ms
step:1326/1370 train_time:195304ms step_avg:148.41ms
step:1327/1370 train_time:195459ms step_avg:148.41ms
step:1328/1370 train_time:195612ms step_avg:148.42ms
step:1329/1370 train_time:195786ms step_avg:148.44ms
step:1330/1370 train_time:195946ms step_avg:148.44ms
step:1331/1370 train_time:196143ms step_avg:148.48ms
step:1332/1370 train_time:196306ms step_avg:148.49ms
step:1333/1370 train_time:196462ms step_avg:148.50ms
step:1334/1370 train_time:196616ms step_avg:148.50ms
step:1335/1370 train_time:196769ms step_avg:148.51ms
step:1336/1370 train_time:196934ms step_avg:148.52ms
step:1337/1370 train_time:197092ms step_avg:148.52ms
step:1338/1370 train_time:197251ms step_avg:148.53ms
step:1339/1370 train_time:197411ms step_avg:148.54ms
step:1340/1370 train_time:197572ms step_avg:148.55ms
step:1341/1370 train_time:197725ms step_avg:148.55ms
step:1342/1370 train_time:197884ms step_avg:148.56ms
step:1343/1370 train_time:198041ms step_avg:148.57ms
step:1344/1370 train_time:198196ms step_avg:148.57ms
step:1345/1370 train_time:198352ms step_avg:148.58ms
step:1346/1370 train_time:198508ms step_avg:148.58ms
step:1347/1370 train_time:198667ms step_avg:148.59ms
step:1348/1370 train_time:198823ms step_avg:148.60ms
step:1349/1370 train_time:198982ms step_avg:148.60ms
step:1350/1370 train_time:199136ms step_avg:148.61ms
step:1351/1370 train_time:199291ms step_avg:148.61ms
step:1352/1370 train_time:199453ms step_avg:148.62ms
step:1353/1370 train_time:199612ms step_avg:148.63ms
step:1354/1370 train_time:199773ms step_avg:148.64ms
step:1355/1370 train_time:199933ms step_avg:148.65ms
step:1356/1370 train_time:200090ms step_avg:148.66ms
step:1357/1370 train_time:200248ms step_avg:148.66ms
step:1358/1370 train_time:200409ms step_avg:148.67ms
step:1359/1370 train_time:200564ms step_avg:148.68ms
step:1360/1370 train_time:200723ms step_avg:148.68ms
step:1361/1370 train_time:200881ms step_avg:148.69ms
step:1362/1370 train_time:201039ms step_avg:148.70ms
step:1363/1370 train_time:201202ms step_avg:148.71ms
step:1364/1370 train_time:201356ms step_avg:148.71ms
step:1365/1370 train_time:201507ms step_avg:148.71ms
step:1366/1370 train_time:201666ms step_avg:148.72ms
step:1367/1370 train_time:201824ms step_avg:148.73ms
step:1368/1370 train_time:201983ms step_avg:148.74ms
step:1369/1370 train_time:202148ms step_avg:148.75ms
step:1370/1370 train_time:202306ms step_avg:148.75ms
step:1370/1370 val_loss:3.2808 train_time:202386ms step_avg:148.81ms
peak memory consumption: 31565 MiB

import os
import sys
with open(sys.argv[0]) as f:
    code = f.read() # read the code of this file ASAP, for logging
import uuid
import time
import glob
import subprocess
import contextlib
from dataclasses import dataclass

import torch
torch.empty(1, device='cuda', requires_grad=True).backward()
from torch import nn
import torch.nn.functional as F
import torch.distributed as dist
from torch.nn.parallel import DistributedDataParallel as DDP
# use of FlexAttention contributed by @KoszarskyB
from torch.nn.attention.flex_attention import BlockMask, flex_attention

# -----------------------------------------------------------------------------
# Muon optimizer

@torch.compile
def zeropower_via_newtonschulz5(G, steps):
    """
    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a
    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose
    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at
    zero even beyond the point where the iteration no longer converges all the way to one everywhere
    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T
    where S' is diagonal with S_{ii}' ~ Uniform(0.5, 1.5), which turns out not to hurt model
    performance at all relative to UV^T, where USV^T = G is the SVD.
    """
    assert len(G.shape) == 2
    a, b, c = (3.4445, -4.7750,  2.0315)
    X = G.bfloat16()
    if G.size(0) > G.size(1):
        X = X.T

    # Ensure spectral norm is at most 1
    X = X / (X.norm() + 1e-7)
    # Perform the NS iterations
    for _ in range(steps):
        A = X @ X.T
        B = b * A + c * A @ A # adapted from suggestion by @jxbz, @leloykun, and @YouJiacheng
        X = a * X + B @ X
    
    if G.size(0) > G.size(1):
        X = X.T
    return X

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Some warnings:
    - This optimizer assumes that all parameters passed in are 2D.
    - It should not be used for the embedding layer, the final fully connected layer, or any {0,1}-D
    parameters; those should all be optimized by a standard method (e.g., AdamW).
    - To use it with 4D convolutional filters, it works well to just flatten their last 3 dimensions.
    - We believe it is unlikely to work well for training with small batch size.
    - We believe it may not work well for finetuning pretrained models, but we haven't tested this.
    - We have not yet tried this optimizer for training scenarios larger than NanoGPT (124M).

    Arguments:
        lr: The learning rate used by the internal SGD.
        momentum: The momentum used by the internal SGD.
        nesterov: Whether to use Nesterov-style momentum in the internal SGD. (recommended)
        ns_steps: The number of Newton-Schulz iteration steps to use.
    """
    def __init__(self, params, lr=0.02, momentum=0.95, nesterov=True, ns_steps=5):
        self.world_size = int(os.environ['WORLD_SIZE'])
        self.rank = int(os.environ['RANK'])
        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, ns_steps=ns_steps)
        assert all(isinstance(p, torch.Tensor) for p in params)
        sizes = {p.numel() for p in params}
        param_groups = [dict(params=[p for p in params if p.numel() == size],
                             update_buffer=[torch.empty(size, device='cuda', dtype=torch.bfloat16) for _ in range(self.world_size)])
                        for size in sizes]
        super().__init__(param_groups, defaults)

    def step(self):

        for group in self.param_groups:

            lr = group['lr']
            momentum = group['momentum']
            nesterov = group['nesterov']
            ns_steps = group['ns_steps']
            update_buffers = group['update_buffer']
            # generate weight updates in distributed fashion
            params = group['params']
            handle = None
            params_world = None
            def update_prev():
                if params_world is None:
                    return
                assert handle is not None
                handle.wait()
                for p_world, g_world in zip(params_world, update_buffers):
                    p_world.data.add_(
                        g_world.view_as(p_world),
                        alpha=-lr * max(1, p_world.size(0) / p_world.size(1)) ** 0.5,
                    )
            for base_i in range(len(params))[::self.world_size]:
                if base_i + rank < len(params):
                    p = params[base_i + self.rank]
                    g = p.grad
                    assert g is not None
                    state = self.state[p]
                    if 'momentum_buffer' not in state:
                        state['momentum_buffer'] = torch.zeros_like(g)
                    buf = state['momentum_buffer']
                    buf.lerp_(g, 1 - momentum)
                    g = g.lerp_(buf, momentum) if nesterov else buf
                    g = zeropower_via_newtonschulz5(g, steps=ns_steps).flatten()
                else:
                    g = update_buffers[rank]
                update_prev() # async all_gather instead of sync all_reduce by @YouJiacheng
                handle = dist.all_gather(update_buffers, g, async_op=True)
                params_world = params[base_i : base_i + self.world_size]
            update_prev()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the GPT-2 model

def norm(x):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):

    def __init__(self, in_features, out_features):
        super().__init__(in_features, out_features, bias=False)

    def forward(self, x):
        return F.linear(x, self.weight.type_as(x))

class Rotary(nn.Module):

    def __init__(self, dim, max_seq_len=65536):
        super().__init__()
        # half-truncate RoPE by @YouJiacheng
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=dim//4, dtype=torch.float32)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(dim//4)])
        t = torch.arange(max_seq_len, dtype=torch.float32)
        theta = torch.einsum('i,j -> ij', t, angular_freq)
        self.cos = nn.Buffer(theta.cos(), persistent=False)
        self.sin = nn.Buffer(theta.sin(), persistent=False)

    def forward(self, x):
        cos, sin = self.cos[None, :x.size(-3), None, :], self.sin[None, :x.size(-3), None, :]
        x1, x2 = x.float().chunk(2, dim=-1)
        y1 = x1 * cos + x2 * sin
        y2 = x1 * (-sin) + x2 * cos
        return torch.cat((y1, y2), 3).type_as(x)

class CausalSelfAttention(nn.Module):

    def __init__(self, dim, num_heads, layer_id):
        super().__init__()
        assert dim % num_heads == 0
        self.num_heads = num_heads
        self.layer_id = layer_id
        self.c_q = CastedLinear(dim, dim)
        self.c_k = CastedLinear(dim, dim)
        self.c_v = CastedLinear(dim, dim)
        self.lambdas = nn.Parameter(torch.tensor([0.5, 0.5]))
        self.rotary = Rotary(dim // num_heads) # dim // num_heads = head_dim
        self.c_proj = CastedLinear(dim, dim)
        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977
        # self.attn_scale = nn.Parameter(torch.tensor(1.0 / (dim // num_heads) ** 0.5))
        # self.attn_scale = nn.Parameter(torch.tensor(0.20))
        self.attn_scale = 0.13 + 0.01 * min(layer_id, 11 - layer_id)

    def forward(self, x, ve, block_mask):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, 'Must use batch size = 1 for FlexAttention'
        q = self.c_q(x).view(B, T, self.num_heads, -1)
        k = self.c_k(x).view(B, T, self.num_heads, -1)
        v = self.c_v(x).view(B, T, self.num_heads, -1)
        if ve is not None:
            v = self.lambdas[0] * v + self.lambdas[1] * ve.view_as(v) # @KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = self.lambdas[0] * v
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = self.rotary(q), self.rotary(k)
        # y = flex_attention(q.transpose(1, 2) * self.attn_scale, k.transpose(1, 2), v.transpose(1, 2), block_mask=block_mask, scale=1.)
        y = flex_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), block_mask=block_mask, scale=self.attn_scale)
        y = y.transpose(1, 2).contiguous().view_as(x) # re-assemble all head outputs side by side
        y = self.c_proj(y)
        return y

class MLP(nn.Module):

    def __init__(self, dim):
        super().__init__()
        self.c_fc = CastedLinear(dim, 4 * dim)
        self.c_proj = CastedLinear(4 * dim, dim)
        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977

    def forward(self, x):
        x = self.c_fc(x)
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = self.c_proj(x)
        return x

class Block(nn.Module):

    def __init__(self, model_dim, num_heads, layer_id, use_attn=True):
        super().__init__()
        self.attn = CausalSelfAttention(model_dim, num_heads, layer_id) if use_attn else None
        self.mlp = MLP(model_dim)
        self.lambdas = nn.Parameter(torch.tensor([1., 0.]))

    def forward(self, x, ve, x0, block_mask):
        x = self.lambdas[0] * x + self.lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), ve, block_mask)
        x = x + self.mlp(norm(x))
        return x

class ValueEmbedding(nn.Module):
    def __init__(self, vocab_size, model_dim):
        super().__init__()
        self.embed = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])

    def forward(self, inputs):
        ve = [emb(inputs).bfloat16() for emb in self.embed]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        ve = [ve[0], ve[1], ve[2], None, None, None, None, None, None, ve[0], ve[1], ve[2]]
        return ve

# -----------------------------------------------------------------------------
# The main GPT-2 model

class GPT(nn.Module):

    def __init__(self, vocab_size, num_layers, num_heads, model_dim):
        super().__init__()
        self.embed = nn.Embedding(vocab_size, model_dim)
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.blocks = nn.ModuleList([Block(model_dim, num_heads, layer_id=layer_id, use_attn=(layer_id != 7))
                                     for layer_id in range(num_layers)])
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual learning
        # U-net structure on token value embeddings by @leloykun
        self.value_embeds = ValueEmbedding(vocab_size, model_dim)
        self.lm_head = CastedLinear(model_dim, vocab_size)
        self.lm_head.weight.data.zero_() # @Grad62304977
        # U-net design by @brendanh0gan
        self.num_encoder_layers = num_layers // 2 # Half of the layers for encoder
        self.num_decoder_layers = num_layers - self.num_encoder_layers # Remaining for decoder
        # Add learnable skip connection weights for decoder layers
        self.skip_weights = nn.Parameter(torch.ones(self.num_decoder_layers))

    def forward(self, inputs, targets, sliding_window_num_blocks):
        BLOCK_SIZE = 128
        seq_len = len(inputs)
        assert seq_len % BLOCK_SIZE == 0
        total_num_blocks = seq_len // BLOCK_SIZE
        assert inputs.ndim == 1
        docs = (inputs == 50256).cumsum(0)
        docs_low = docs.view(-1, BLOCK_SIZE)[:, 0].contiguous()
        docs_high = docs.view(-1, BLOCK_SIZE)[:, -1].contiguous()

        def document_causal(b, h, q_idx, kv_idx):
            causal_mask = q_idx >= kv_idx
            document_mask = docs[q_idx] == docs[kv_idx]
            return causal_mask & document_mask

        def dense_to_ordered(dense_mask):
            num_blocks = dense_mask.sum(dim=-1, dtype=torch.int32)
            indices = dense_mask.argsort(dim=-1, descending=True, stable=True).to(torch.int32)
            return num_blocks[None, None].contiguous(), indices[None, None].contiguous()

        def create_doc_swc_block_mask(sliding_window_num_blocks):
            kv_idx = block_idx = torch.arange(total_num_blocks, dtype=torch.int32, device='cuda')
            q_idx = block_idx[:, None]
            causal_bm = q_idx >= kv_idx
            causal_full_bm = q_idx > kv_idx
            window_bm = q_idx - kv_idx < sliding_window_num_blocks
            window_full_bm = window_bm # block-wise sliding window by @YouJiacheng
            # document_bm = (docs_low[q_idx] <= docs_high[kv_idx]) & (docs_low[kv_idx] <= docs_high[q_idx])
            document_bm = (docs_low[:, None] <= docs_high) & (docs_low <= docs_high[:, None])
            document_full_bm = (docs_low[:, None] == docs_high) & (docs_low == docs_high[:, None])
            nonzero_bm = causal_bm & window_bm & document_bm
            full_bm  = causal_full_bm & window_full_bm & document_full_bm
            kv_num_blocks, kv_indices = dense_to_ordered(nonzero_bm & ~full_bm)
            full_kv_num_blocks, full_kv_indices = dense_to_ordered(full_bm)
            return BlockMask.from_kv_blocks(
                kv_num_blocks,
                kv_indices,
                full_kv_num_blocks,
                full_kv_indices,
                BLOCK_SIZE=BLOCK_SIZE,
                mask_mod=document_causal,
            )

        block_mask = create_doc_swc_block_mask(sliding_window_num_blocks)

        x0 = norm(self.embed(inputs[None]).bfloat16()) # use of norm here by @Grad62304977
        x = x0
        ve = self.value_embeds(inputs)
        assert len(ve) == len(self.blocks)
        ve_enc, ve_dec = ve[:self.num_encoder_layers], ve[self.num_encoder_layers:]

        # Store outputs for U-Net skip connections
        skip_connections = []
        # Encoder pass - process only the first half of the blocks
        for i in range(self.num_encoder_layers):
            x = self.blocks[i](x, ve_enc[i], x0, block_mask)
            skip_connections.append(x)
        # Decoder pass - process the remaining blocks with weighted skip connections
        for i in range(self.num_decoder_layers):
            x = x + self.skip_weights[i] * skip_connections.pop()
            # U-net structure on token value embeddings by @leloykun
            x = self.blocks[self.num_encoder_layers + i](x, ve_dec[i], x0, block_mask)

        x = norm(x)
        logits = self.lm_head(x)
        logits = 15 * torch.tanh(logits / 15) # @Grad62304977 added tanh softcapping, @KoszarskyB reduced it from 30 to 15
        logits = logits.float()
        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets)
        return loss

# -----------------------------------------------------------------------------
# Our own simple Distributed Data Loader

def _load_data_shard(path):
    # only reads the header, returns header data
    # header is 256 int32
    header = torch.from_file(path, False, 256, dtype=torch.int32)
    assert header[0] == 20240520, 'magic number mismatch in the data .bin file'
    assert header[1] == 1, 'unsupported version'
    num_tokens = int(header[2]) # number of tokens (claimed)
    with open(path, 'rb', buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, 'number of tokens read does not match header'
    return tokens

class DistributedDataLoader:

    def __init__(self, filename_pattern):
        self.rank = int(os.environ['RANK'])
        self.world_size = int(os.environ['WORLD_SIZE'])
        self.files = sorted(glob.glob(filename_pattern))
        self.reset()

    def reset(self):
        self.current_shard = -1
        self.advance()

    def advance(self):
        self.current_shard = (self.current_shard + 1) % len(self.files)
        self.current_position = 0
        self.tokens = _load_data_shard(self.files[self.current_shard])

    def next_batch(self, batch_size):
        assert batch_size % self.world_size == 0
        device_batch_size = batch_size // self.world_size
        # load next shard if necessary
        if self.current_position + batch_size + 1 >= len(self.tokens):
            self.advance()
        pos = self.current_position + self.rank * device_batch_size
        device_batch_tokens = self.tokens[pos:pos+device_batch_size+1]
        # advance current position
        self.current_position += batch_size
        inputs = device_batch_tokens[:-1].to(device='cuda', dtype=torch.int32, non_blocking=True)
        targets = device_batch_tokens[1:].to(device='cuda', dtype=torch.int64, non_blocking=True)
        return inputs, targets

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files = 'data/fineweb10B/fineweb_train_*.bin' # input .bin to train on
    val_files = 'data/fineweb10B/fineweb_val_*.bin' # input .bin to eval validation loss on
    val_tokens = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # optimization
    batch_size = 8*64*1024 # batch size in tokens
    num_iterations = 1375 # number of iterations to run
    cooldown_frac = 0.4 # fraction of training spent cooling down the learning rate
    bf16_embeds = True
    # evaluation and logging
    val_loss_every = 125 # every how many steps to evaluate val loss? 0 for only at the end
    # implementation
    max_device_batch_size = 64*1024 # batch size per device in tokens
    save_checkpoint = False
args = Hyperparameters()

micro_bs = args.max_device_batch_size

# set up DDP (distributed data parallel). torchrun sets this env variable
rank = int(os.environ['RANK'])
local_rank = int(os.environ['LOCAL_RANK'])
world_size = int(os.environ['WORLD_SIZE'])
assert torch.cuda.is_available()
torch.cuda.set_device(local_rank)
dist.init_process_group(backend='nccl', device_id=torch.device(local_rank))
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = uuid.uuid4()
    os.makedirs('logs', exist_ok=True)
    logfile = f'logs/{run_id}.txt'
    print(logfile)

def print0(s, console=False):
    if master_process:
        with open(logfile, 'a') as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0('='*100)
# log information about the hardware/software environment this is running on
print0(f'Running Python {sys.version}')
print0(f'Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}')
print0(subprocess.run(['nvidia-smi'], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout)
print0('='*100)

# load data
train_loader = DistributedDataLoader(args.train_files)
val_loader = DistributedDataLoader(args.val_files)
print0(f'Training dataloader files: {train_loader.files}')
print0(f'Validation dataloader files: {val_loader.files}')
print0('='*100)

# there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency. suggested to me by @Grad62304977.
# this originates from Karpathy's experiments.
model = GPT(vocab_size=50304, num_layers=12, num_heads=6, model_dim=768)
model = model.cuda()
if args.bf16_embeds:
    for m in model.modules():
        if isinstance(m, nn.Embedding):
            m.bfloat16()
model: GPT = torch.compile(model)
ddp_model = DDP(model, device_ids=[local_rank], broadcast_buffers=False, gradient_as_bucket_view=True)

# collect the parameters to optimize
hidden_matrix_params = [p for p in model.blocks.parameters() if p.ndim == 2]
embed_params = [model.embed.weight, *model.value_embeds.parameters()]
scalar_params = [p for n, p in model.named_parameters() if p.ndim < 2 and "attn_scale" not in n]
attn_scale_params = [p for n, p in model.named_parameters() if p.ndim < 2 and "attn_scale" in n]
head_params = [model.lm_head.weight]

# init the optimizer(s)
optimizer1 = torch.optim.Adam([dict(params=embed_params, lr=0.6),
                               dict(params=head_params, lr=0.008),
                               dict(params=scalar_params, lr=0.04),
                               dict(params=attn_scale_params, lr=0.01)],
                              betas=(0.8, 0.95), fused=True)
optimizer2 = Muon(hidden_matrix_params, lr=0.05, momentum=0.95)
optimizers = [optimizer1, optimizer2]

# learning rate schedule: stable then decay
def get_lr(it):
    t = 1 - it / args.num_iterations # time remaining in training
    assert 1 >= t > 0
    # 1) constant lr for first part of training
    if t >= args.cooldown_frac:
        return 1.0
    # 2) then linear cooldown
    else:
        return t / args.cooldown_frac
schedulers = [torch.optim.lr_scheduler.LambdaLR(opt, get_lr) for opt in optimizers]

# sliding window size schedule: linear increase over training in chunks of 128 from 128 -> 1792. By @fernbear.bsky.social
def get_sliding_window_blocks(it):
    x = it / args.num_iterations # training progress
    assert 0 <= x <= 1
    return int(((1 - x) * 128 + x * 1856) // 128)
sliding_window_num_blocks = torch.tensor(1, dtype=torch.int32, device='cuda')

# Start training loop
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    # This effectively ignores timing first 10 steps, which are slower for weird reasons.
    # Alternately, and slightly more correctly in terms of benchmarking, we could do 10
    # steps with dummy data first, and then re-initialize the model and reset the loader.
    if step == 10:
        training_time_ms = 0
        t0 = time.perf_counter()
    timed_steps = float('nan') if step <= 11 else (step - 10) + 1 # <= 11 to avoid bug in val

    sliding_window_num_blocks.copy_(get_sliding_window_blocks(step))

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        # run validation batches
        model.eval()
        val_loader.reset()
        val_loss = 0.0
        # calculate the number of steps to take in the val loop.
        val_batch_size = world_size * micro_bs
        assert args.val_tokens % val_batch_size == 0
        val_steps = args.val_tokens // val_batch_size
        for _ in range(val_steps):
            with torch.no_grad():
                inputs_val, targets_val = val_loader.next_batch(val_batch_size)
                val_loss += ddp_model(inputs_val, targets_val, sliding_window_num_blocks)
        # Print attention scales
        for n, p in model.named_parameters():
            if p.ndim < 2 and "attn_scale" in n:
                print0(f'{n}: {p.item()}', console=True)
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        val_loss /= val_steps
        # logging
        print0(f'step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/(timed_steps-1):.2f}ms', console=True)
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f'logs/{run_id}', exist_ok=True)
            torch.save(log, f'logs/{run_id}/state_step{step:06d}.pt')
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    model.train()
    batch_size = args.batch_size
    assert batch_size % world_size == 0
    inputs_train, targets_train = train_loader.next_batch(batch_size)
    assert len(inputs_train) <= micro_bs or len(inputs_train) % micro_bs == 0
    for micro_inputs_train, micro_targets_train in zip(inputs_train.split(micro_bs), targets_train.split(micro_bs)):
        ddp_model(micro_inputs_train, micro_targets_train, sliding_window_num_blocks).backward()
    # momentum warmup for Muon
    frac = min(step/300, 1)
    for group in optimizer2.param_groups:
        group['momentum'] = (1 - frac) * 0.85 + frac * 0.95
    # step the optimizers and schedulers
    for opt, sched in zip(optimizers, schedulers):
        opt.step()
        if step != train_steps-1:
            sched.step()
    # null the gradients
    model.zero_grad(set_to_none=True)
    # logging
    approx_time = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f'step:{step+1}/{train_steps} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms', console=True)

print0(f'peak memory consumption: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB')
dist.destroy_process_group()

====================================================================================================
Running Python 3.10.12 (main, Sep 11 2024, 15:47:36) [GCC 11.4.0]
Running PyTorch 2.6.0.dev20241231+cu126 compiled for CUDA 12.6
Tue Jan 14 15:44:22 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.183.06             Driver Version: 535.183.06   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:19:00.0 Off |                    0 |
| N/A   38C    P0             126W / 700W |   7713MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:3B:00.0 Off |                    0 |
| N/A   32C    P0             114W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:4C:00.0 Off |                    0 |
| N/A   29C    P0             117W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   37C    P0             129W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:9B:00.0 Off |                    0 |
| N/A   38C    P0             123W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:BB:00.0 Off |                    0 |
| N/A   30C    P0             114W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:CB:00.0 Off |                    0 |
| N/A   37C    P0             116W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:DB:00.0 Off |                    0 |
| N/A   29C    P0             117W / 700W |   3211MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
Training dataloader files: ['data/fineweb10B/fineweb_train_000001.bin', 'data/fineweb10B/fineweb_train_000002.bin', 'data/fineweb10B/fineweb_train_000003.bin', 'data/fineweb10B/fineweb_train_000004.bin', 'data/fineweb10B/fineweb_train_000005.bin', 'data/fineweb10B/fineweb_train_000006.bin', 'data/fineweb10B/fineweb_train_000007.bin', 'data/fineweb10B/fineweb_train_000008.bin', 'data/fineweb10B/fineweb_train_000009.bin']
Validation dataloader files: ['data/fineweb10B/fineweb_val_000000.bin']
====================================================================================================
step:0/1375 val_loss:10.8258 train_time:0ms step_avg:nanms
step:1/1375 train_time:27541ms step_avg:nanms
step:2/1375 train_time:27608ms step_avg:nanms
step:3/1375 train_time:27795ms step_avg:nanms
step:4/1375 train_time:27929ms step_avg:nanms
step:5/1375 train_time:28062ms step_avg:nanms
step:6/1375 train_time:28196ms step_avg:nanms
step:7/1375 train_time:28328ms step_avg:nanms
step:8/1375 train_time:28462ms step_avg:nanms
step:9/1375 train_time:28595ms step_avg:nanms
step:10/1375 train_time:28737ms step_avg:nanms
step:11/1375 train_time:137ms step_avg:nanms
step:12/1375 train_time:270ms step_avg:nanms
step:13/1375 train_time:405ms step_avg:135.02ms
step:14/1375 train_time:539ms step_avg:134.85ms
step:15/1375 train_time:672ms step_avg:134.41ms
step:16/1375 train_time:805ms step_avg:134.23ms
step:17/1375 train_time:941ms step_avg:134.41ms
step:18/1375 train_time:1078ms step_avg:134.71ms
step:19/1375 train_time:1214ms step_avg:134.91ms
step:20/1375 train_time:1349ms step_avg:134.87ms
step:21/1375 train_time:1485ms step_avg:135.00ms
step:22/1375 train_time:1620ms step_avg:135.02ms
step:23/1375 train_time:1753ms step_avg:134.86ms
step:24/1375 train_time:1888ms step_avg:134.87ms
step:25/1375 train_time:2024ms step_avg:134.91ms
step:26/1375 train_time:2161ms step_avg:135.08ms
step:27/1375 train_time:2296ms step_avg:135.05ms
step:28/1375 train_time:2431ms step_avg:135.05ms
step:29/1375 train_time:2566ms step_avg:135.06ms
step:30/1375 train_time:2701ms step_avg:135.04ms
step:31/1375 train_time:2836ms step_avg:135.06ms
step:32/1375 train_time:2970ms step_avg:134.98ms
step:33/1375 train_time:3106ms step_avg:135.05ms
step:34/1375 train_time:3242ms step_avg:135.08ms
step:35/1375 train_time:3377ms step_avg:135.09ms
step:36/1375 train_time:3513ms step_avg:135.12ms
step:37/1375 train_time:3648ms step_avg:135.12ms
step:38/1375 train_time:3784ms step_avg:135.14ms
step:39/1375 train_time:3920ms step_avg:135.16ms
step:40/1375 train_time:4055ms step_avg:135.16ms
step:41/1375 train_time:4191ms step_avg:135.18ms
step:42/1375 train_time:4326ms step_avg:135.18ms
step:43/1375 train_time:4462ms step_avg:135.21ms
step:44/1375 train_time:4597ms step_avg:135.20ms
step:45/1375 train_time:4731ms step_avg:135.18ms
step:46/1375 train_time:4867ms step_avg:135.19ms
step:47/1375 train_time:5003ms step_avg:135.21ms
step:48/1375 train_time:5139ms step_avg:135.23ms
step:49/1375 train_time:5277ms step_avg:135.30ms
step:50/1375 train_time:5411ms step_avg:135.27ms
step:51/1375 train_time:5546ms step_avg:135.27ms
step:52/1375 train_time:5681ms step_avg:135.26ms
step:53/1375 train_time:5815ms step_avg:135.24ms
step:54/1375 train_time:5949ms step_avg:135.20ms
step:55/1375 train_time:6086ms step_avg:135.24ms
step:56/1375 train_time:6220ms step_avg:135.23ms
step:57/1375 train_time:6358ms step_avg:135.27ms
step:58/1375 train_time:6491ms step_avg:135.22ms
step:59/1375 train_time:6626ms step_avg:135.23ms
step:60/1375 train_time:6762ms step_avg:135.24ms
step:61/1375 train_time:6897ms step_avg:135.23ms
step:62/1375 train_time:7032ms step_avg:135.22ms
step:63/1375 train_time:7167ms step_avg:135.22ms
step:64/1375 train_time:7303ms step_avg:135.24ms
step:65/1375 train_time:7438ms step_avg:135.24ms
step:66/1375 train_time:7575ms step_avg:135.26ms
step:67/1375 train_time:7709ms step_avg:135.25ms
step:68/1375 train_time:7845ms step_avg:135.26ms
step:69/1375 train_time:7981ms step_avg:135.27ms
step:70/1375 train_time:8116ms step_avg:135.26ms
step:71/1375 train_time:8251ms step_avg:135.26ms
step:72/1375 train_time:8387ms step_avg:135.27ms
step:73/1375 train_time:8523ms step_avg:135.28ms
step:74/1375 train_time:8659ms step_avg:135.30ms
step:75/1375 train_time:8794ms step_avg:135.29ms
step:76/1375 train_time:8929ms step_avg:135.29ms
step:77/1375 train_time:9064ms step_avg:135.28ms
step:78/1375 train_time:9200ms step_avg:135.29ms
step:79/1375 train_time:9336ms step_avg:135.30ms
step:80/1375 train_time:9471ms step_avg:135.29ms
step:81/1375 train_time:9607ms step_avg:135.31ms
step:82/1375 train_time:9743ms step_avg:135.32ms
step:83/1375 train_time:9880ms step_avg:135.34ms
step:84/1375 train_time:10015ms step_avg:135.34ms
step:85/1375 train_time:10150ms step_avg:135.33ms
step:86/1375 train_time:10285ms step_avg:135.33ms
step:87/1375 train_time:10419ms step_avg:135.31ms
step:88/1375 train_time:10554ms step_avg:135.31ms
step:89/1375 train_time:10689ms step_avg:135.30ms
step:90/1375 train_time:10824ms step_avg:135.30ms
step:91/1375 train_time:10960ms step_avg:135.31ms
step:92/1375 train_time:11095ms step_avg:135.30ms
step:93/1375 train_time:11230ms step_avg:135.30ms
step:94/1375 train_time:11366ms step_avg:135.31ms
step:95/1375 train_time:11502ms step_avg:135.31ms
step:96/1375 train_time:11638ms step_avg:135.32ms
step:97/1375 train_time:11773ms step_avg:135.32ms
step:98/1375 train_time:11910ms step_avg:135.34ms
step:99/1375 train_time:12046ms step_avg:135.34ms
step:100/1375 train_time:12181ms step_avg:135.34ms
step:101/1375 train_time:12317ms step_avg:135.36ms
step:102/1375 train_time:12452ms step_avg:135.35ms
step:103/1375 train_time:12589ms step_avg:135.36ms
step:104/1375 train_time:12728ms step_avg:135.41ms
step:105/1375 train_time:12869ms step_avg:135.46ms
step:106/1375 train_time:13008ms step_avg:135.50ms
step:107/1375 train_time:13145ms step_avg:135.52ms
step:108/1375 train_time:13283ms step_avg:135.54ms
step:109/1375 train_time:13423ms step_avg:135.59ms
step:110/1375 train_time:13562ms step_avg:135.62ms
step:111/1375 train_time:13701ms step_avg:135.65ms
step:112/1375 train_time:13840ms step_avg:135.68ms
step:113/1375 train_time:13978ms step_avg:135.71ms
step:114/1375 train_time:14117ms step_avg:135.74ms
step:115/1375 train_time:14254ms step_avg:135.76ms
step:116/1375 train_time:14393ms step_avg:135.78ms
step:117/1375 train_time:14532ms step_avg:135.81ms
step:118/1375 train_time:14670ms step_avg:135.83ms
step:119/1375 train_time:14810ms step_avg:135.87ms
step:120/1375 train_time:14948ms step_avg:135.89ms
step:121/1375 train_time:15088ms step_avg:135.93ms
step:122/1375 train_time:15228ms step_avg:135.96ms
step:123/1375 train_time:15367ms step_avg:135.99ms
step:124/1375 train_time:15506ms step_avg:136.02ms
step:125/1375 train_time:15645ms step_avg:136.04ms
step:125/1375 val_loss:4.3783 train_time:15713ms step_avg:136.63ms
step:126/1375 train_time:15791ms step_avg:136.13ms
step:127/1375 train_time:15931ms step_avg:136.16ms
step:128/1375 train_time:16071ms step_avg:136.19ms
step:129/1375 train_time:16208ms step_avg:136.20ms
step:130/1375 train_time:16345ms step_avg:136.21ms
step:131/1375 train_time:16483ms step_avg:136.23ms
step:132/1375 train_time:16620ms step_avg:136.23ms
step:133/1375 train_time:16762ms step_avg:136.27ms
step:134/1375 train_time:16902ms step_avg:136.30ms
step:135/1375 train_time:17043ms step_avg:136.34ms
step:136/1375 train_time:17183ms step_avg:136.37ms
step:137/1375 train_time:17320ms step_avg:136.38ms
step:138/1375 train_time:17457ms step_avg:136.39ms
step:139/1375 train_time:17595ms step_avg:136.40ms
step:140/1375 train_time:17734ms step_avg:136.42ms
step:141/1375 train_time:17876ms step_avg:136.45ms
step:142/1375 train_time:18015ms step_avg:136.48ms
step:143/1375 train_time:18155ms step_avg:136.51ms
step:144/1375 train_time:18293ms step_avg:136.51ms
step:145/1375 train_time:18433ms step_avg:136.54ms
step:146/1375 train_time:18570ms step_avg:136.55ms
step:147/1375 train_time:18710ms step_avg:136.57ms
step:148/1375 train_time:18848ms step_avg:136.58ms
step:149/1375 train_time:18988ms step_avg:136.60ms
step:150/1375 train_time:19126ms step_avg:136.61ms
step:151/1375 train_time:19264ms step_avg:136.63ms
step:152/1375 train_time:19403ms step_avg:136.64ms
step:153/1375 train_time:19541ms step_avg:136.65ms
step:154/1375 train_time:19681ms step_avg:136.67ms
step:155/1375 train_time:19820ms step_avg:136.69ms
step:156/1375 train_time:19958ms step_avg:136.70ms
step:157/1375 train_time:20096ms step_avg:136.71ms
step:158/1375 train_time:20235ms step_avg:136.72ms
step:159/1375 train_time:20376ms step_avg:136.75ms
step:160/1375 train_time:20515ms step_avg:136.77ms
step:161/1375 train_time:20656ms step_avg:136.79ms
step:162/1375 train_time:20793ms step_avg:136.80ms
step:163/1375 train_time:20932ms step_avg:136.81ms
step:164/1375 train_time:21072ms step_avg:136.83ms
step:165/1375 train_time:21211ms step_avg:136.84ms
step:166/1375 train_time:21350ms step_avg:136.86ms
step:167/1375 train_time:21489ms step_avg:136.87ms
step:168/1375 train_time:21628ms step_avg:136.88ms
step:169/1375 train_time:21768ms step_avg:136.90ms
step:170/1375 train_time:21907ms step_avg:136.92ms
step:171/1375 train_time:22045ms step_avg:136.92ms
step:172/1375 train_time:22185ms step_avg:136.95ms
step:173/1375 train_time:22325ms step_avg:136.96ms
step:174/1375 train_time:22463ms step_avg:136.97ms
step:175/1375 train_time:22603ms step_avg:136.99ms
step:176/1375 train_time:22743ms step_avg:137.01ms
step:177/1375 train_time:22883ms step_avg:137.03ms
step:178/1375 train_time:23022ms step_avg:137.04ms
step:179/1375 train_time:23162ms step_avg:137.05ms
step:180/1375 train_time:23301ms step_avg:137.06ms
step:181/1375 train_time:23440ms step_avg:137.07ms
step:182/1375 train_time:23579ms step_avg:137.09ms
step:183/1375 train_time:23717ms step_avg:137.09ms
step:184/1375 train_time:23856ms step_avg:137.10ms
step:185/1375 train_time:23994ms step_avg:137.11ms
step:186/1375 train_time:24135ms step_avg:137.13ms
step:187/1375 train_time:24275ms step_avg:137.15ms
step:188/1375 train_time:24414ms step_avg:137.16ms
step:189/1375 train_time:24555ms step_avg:137.18ms
step:190/1375 train_time:24693ms step_avg:137.18ms
step:191/1375 train_time:24870ms step_avg:137.40ms
step:192/1375 train_time:25008ms step_avg:137.41ms
step:193/1375 train_time:25144ms step_avg:137.40ms
step:194/1375 train_time:25283ms step_avg:137.41ms
step:195/1375 train_time:25419ms step_avg:137.40ms
step:196/1375 train_time:25558ms step_avg:137.41ms
step:197/1375 train_time:25698ms step_avg:137.42ms
step:198/1375 train_time:25843ms step_avg:137.46ms
step:199/1375 train_time:25984ms step_avg:137.48ms
step:200/1375 train_time:26122ms step_avg:137.48ms
step:201/1375 train_time:26260ms step_avg:137.49ms
step:202/1375 train_time:26399ms step_avg:137.49ms
step:203/1375 train_time:26537ms step_avg:137.50ms
step:204/1375 train_time:26676ms step_avg:137.51ms
step:205/1375 train_time:26817ms step_avg:137.52ms
step:206/1375 train_time:26960ms step_avg:137.55ms
step:207/1375 train_time:27104ms step_avg:137.59ms
step:208/1375 train_time:27246ms step_avg:137.60ms
step:209/1375 train_time:27387ms step_avg:137.62ms
step:210/1375 train_time:27526ms step_avg:137.63ms
step:211/1375 train_time:27670ms step_avg:137.66ms
step:212/1375 train_time:27812ms step_avg:137.68ms
step:213/1375 train_time:27954ms step_avg:137.70ms
step:214/1375 train_time:28096ms step_avg:137.72ms
step:215/1375 train_time:28238ms step_avg:137.74ms
step:216/1375 train_time:28379ms step_avg:137.76ms
step:217/1375 train_time:28520ms step_avg:137.78ms
step:218/1375 train_time:28662ms step_avg:137.80ms
step:219/1375 train_time:28804ms step_avg:137.82ms
step:220/1375 train_time:28946ms step_avg:137.84ms
step:221/1375 train_time:29088ms step_avg:137.86ms
step:222/1375 train_time:29230ms step_avg:137.88ms
step:223/1375 train_time:29372ms step_avg:137.90ms
step:224/1375 train_time:29514ms step_avg:137.92ms
step:225/1375 train_time:29656ms step_avg:137.94ms
step:226/1375 train_time:29797ms step_avg:137.95ms
step:227/1375 train_time:29940ms step_avg:137.97ms
step:228/1375 train_time:30082ms step_avg:137.99ms
step:229/1375 train_time:30224ms step_avg:138.01ms
step:230/1375 train_time:30367ms step_avg:138.03ms
step:231/1375 train_time:30508ms step_avg:138.05ms
step:232/1375 train_time:30649ms step_avg:138.06ms
step:233/1375 train_time:30792ms step_avg:138.08ms
step:234/1375 train_time:30934ms step_avg:138.10ms
step:235/1375 train_time:31077ms step_avg:138.12ms
step:236/1375 train_time:31218ms step_avg:138.13ms
step:237/1375 train_time:31361ms step_avg:138.15ms
step:238/1375 train_time:31503ms step_avg:138.17ms
step:239/1375 train_time:31645ms step_avg:138.19ms
step:240/1375 train_time:31788ms step_avg:138.21ms
step:241/1375 train_time:31931ms step_avg:138.23ms
step:242/1375 train_time:32072ms step_avg:138.24ms
step:243/1375 train_time:32214ms step_avg:138.26ms
step:244/1375 train_time:32356ms step_avg:138.27ms
step:245/1375 train_time:32498ms step_avg:138.29ms
step:246/1375 train_time:32640ms step_avg:138.30ms
step:247/1375 train_time:32782ms step_avg:138.32ms
step:248/1375 train_time:32923ms step_avg:138.33ms
step:249/1375 train_time:33068ms step_avg:138.36ms
step:250/1375 train_time:33210ms step_avg:138.37ms
step:250/1375 val_loss:3.9582 train_time:33280ms step_avg:138.67ms
step:251/1375 train_time:33355ms step_avg:138.40ms
step:252/1375 train_time:33500ms step_avg:138.43ms
step:253/1375 train_time:33642ms step_avg:138.45ms
step:254/1375 train_time:33784ms step_avg:138.46ms
step:255/1375 train_time:33924ms step_avg:138.47ms
step:256/1375 train_time:34065ms step_avg:138.47ms
step:257/1375 train_time:34205ms step_avg:138.48ms
step:258/1375 train_time:34350ms step_avg:138.51ms
step:259/1375 train_time:34494ms step_avg:138.53ms
step:260/1375 train_time:34635ms step_avg:138.54ms
step:261/1375 train_time:34777ms step_avg:138.55ms
step:262/1375 train_time:34918ms step_avg:138.56ms
step:263/1375 train_time:35060ms step_avg:138.58ms
step:264/1375 train_time:35200ms step_avg:138.58ms
step:265/1375 train_time:35343ms step_avg:138.60ms
step:266/1375 train_time:35488ms step_avg:138.62ms
step:267/1375 train_time:35631ms step_avg:138.64ms
step:268/1375 train_time:35772ms step_avg:138.65ms
step:269/1375 train_time:35913ms step_avg:138.66ms
step:270/1375 train_time:36054ms step_avg:138.67ms
step:271/1375 train_time:36196ms step_avg:138.68ms
step:272/1375 train_time:36339ms step_avg:138.70ms
step:273/1375 train_time:36482ms step_avg:138.72ms
step:274/1375 train_time:36627ms step_avg:138.74ms
step:275/1375 train_time:36768ms step_avg:138.75ms
step:276/1375 train_time:36910ms step_avg:138.76ms
step:277/1375 train_time:37052ms step_avg:138.77ms
step:278/1375 train_time:37193ms step_avg:138.78ms
step:279/1375 train_time:37336ms step_avg:138.79ms
step:280/1375 train_time:37477ms step_avg:138.81ms
step:281/1375 train_time:37621ms step_avg:138.82ms
step:282/1375 train_time:37763ms step_avg:138.83ms
step:283/1375 train_time:37905ms step_avg:138.85ms
step:284/1375 train_time:38047ms step_avg:138.86ms
step:285/1375 train_time:38189ms step_avg:138.87ms
step:286/1375 train_time:38332ms step_avg:138.89ms
step:287/1375 train_time:38475ms step_avg:138.90ms
step:288/1375 train_time:38617ms step_avg:138.91ms
step:289/1375 train_time:38760ms step_avg:138.93ms
step:290/1375 train_time:38902ms step_avg:138.94ms
step:291/1375 train_time:39044ms step_avg:138.95ms
step:292/1375 train_time:39187ms step_avg:138.96ms
step:293/1375 train_time:39329ms step_avg:138.97ms
step:294/1375 train_time:39473ms step_avg:138.99ms
step:295/1375 train_time:39615ms step_avg:139.00ms
step:296/1375 train_time:39758ms step_avg:139.02ms
step:297/1375 train_time:39899ms step_avg:139.02ms
step:298/1375 train_time:40041ms step_avg:139.03ms
step:299/1375 train_time:40183ms step_avg:139.04ms
step:300/1375 train_time:40325ms step_avg:139.05ms
step:301/1375 train_time:40467ms step_avg:139.06ms
step:302/1375 train_time:40611ms step_avg:139.08ms
step:303/1375 train_time:40752ms step_avg:139.09ms
step:304/1375 train_time:40893ms step_avg:139.09ms
step:305/1375 train_time:41035ms step_avg:139.10ms
step:306/1375 train_time:41177ms step_avg:139.11ms
step:307/1375 train_time:41321ms step_avg:139.13ms
step:308/1375 train_time:41465ms step_avg:139.14ms
step:309/1375 train_time:41609ms step_avg:139.16ms
step:310/1375 train_time:41753ms step_avg:139.18ms
step:311/1375 train_time:41897ms step_avg:139.19ms
step:312/1375 train_time:42041ms step_avg:139.21ms
step:313/1375 train_time:42187ms step_avg:139.23ms
step:314/1375 train_time:42331ms step_avg:139.25ms
step:315/1375 train_time:42475ms step_avg:139.26ms
step:316/1375 train_time:42619ms step_avg:139.28ms
step:317/1375 train_time:42765ms step_avg:139.30ms
step:318/1375 train_time:42908ms step_avg:139.31ms
step:319/1375 train_time:43051ms step_avg:139.32ms
step:320/1375 train_time:43195ms step_avg:139.34ms
step:321/1375 train_time:43341ms step_avg:139.36ms
step:322/1375 train_time:43486ms step_avg:139.38ms
step:323/1375 train_time:43630ms step_avg:139.39ms
step:324/1375 train_time:43775ms step_avg:139.41ms
step:325/1375 train_time:43919ms step_avg:139.43ms
step:326/1375 train_time:44062ms step_avg:139.44ms
step:327/1375 train_time:44208ms step_avg:139.46ms
step:328/1375 train_time:44353ms step_avg:139.48ms
step:329/1375 train_time:44498ms step_avg:139.49ms
step:330/1375 train_time:44642ms step_avg:139.51ms
step:331/1375 train_time:44788ms step_avg:139.53ms
step:332/1375 train_time:44932ms step_avg:139.54ms
step:333/1375 train_time:45075ms step_avg:139.55ms
step:334/1375 train_time:45217ms step_avg:139.56ms
step:335/1375 train_time:45362ms step_avg:139.58ms
step:336/1375 train_time:45507ms step_avg:139.59ms
step:337/1375 train_time:45651ms step_avg:139.61ms
step:338/1375 train_time:45796ms step_avg:139.62ms
step:339/1375 train_time:45941ms step_avg:139.64ms
step:340/1375 train_time:46087ms step_avg:139.66ms
step:341/1375 train_time:46231ms step_avg:139.67ms
step:342/1375 train_time:46375ms step_avg:139.68ms
step:343/1375 train_time:46518ms step_avg:139.69ms
step:344/1375 train_time:46663ms step_avg:139.71ms
step:345/1375 train_time:46808ms step_avg:139.73ms
step:346/1375 train_time:46953ms step_avg:139.74ms
step:347/1375 train_time:47096ms step_avg:139.75ms
step:348/1375 train_time:47241ms step_avg:139.77ms
step:349/1375 train_time:47387ms step_avg:139.78ms
step:350/1375 train_time:47532ms step_avg:139.80ms
step:351/1375 train_time:47675ms step_avg:139.81ms
step:352/1375 train_time:47819ms step_avg:139.82ms
step:353/1375 train_time:47965ms step_avg:139.84ms
step:354/1375 train_time:48109ms step_avg:139.85ms
step:355/1375 train_time:48252ms step_avg:139.86ms
step:356/1375 train_time:48396ms step_avg:139.87ms
step:357/1375 train_time:48542ms step_avg:139.89ms
step:358/1375 train_time:48686ms step_avg:139.90ms
step:359/1375 train_time:48830ms step_avg:139.91ms
step:360/1375 train_time:48975ms step_avg:139.93ms
step:361/1375 train_time:49118ms step_avg:139.94ms
step:362/1375 train_time:49262ms step_avg:139.95ms
step:363/1375 train_time:49407ms step_avg:139.96ms
step:364/1375 train_time:49552ms step_avg:139.98ms
step:365/1375 train_time:49695ms step_avg:139.99ms
step:366/1375 train_time:49839ms step_avg:140.00ms
step:367/1375 train_time:49986ms step_avg:140.02ms
step:368/1375 train_time:50132ms step_avg:140.03ms
step:369/1375 train_time:50276ms step_avg:140.04ms
step:370/1375 train_time:50419ms step_avg:140.05ms
step:371/1375 train_time:50564ms step_avg:140.07ms
step:372/1375 train_time:50708ms step_avg:140.08ms
step:373/1375 train_time:50852ms step_avg:140.09ms
step:374/1375 train_time:50995ms step_avg:140.10ms
step:375/1375 train_time:51139ms step_avg:140.11ms
step:375/1375 val_loss:3.7769 train_time:51210ms step_avg:140.30ms
step:376/1375 train_time:51286ms step_avg:140.13ms
step:377/1375 train_time:51431ms step_avg:140.14ms
step:378/1375 train_time:51575ms step_avg:140.15ms
step:379/1375 train_time:51719ms step_avg:140.16ms
step:380/1375 train_time:51862ms step_avg:140.17ms
step:381/1375 train_time:52049ms step_avg:140.29ms
step:382/1375 train_time:52192ms step_avg:140.30ms
step:383/1375 train_time:52335ms step_avg:140.31ms
step:384/1375 train_time:52479ms step_avg:140.32ms
step:385/1375 train_time:52622ms step_avg:140.32ms
step:386/1375 train_time:52765ms step_avg:140.33ms
step:387/1375 train_time:52908ms step_avg:140.34ms
step:388/1375 train_time:53057ms step_avg:140.36ms
step:389/1375 train_time:53203ms step_avg:140.38ms
step:390/1375 train_time:53346ms step_avg:140.38ms
step:391/1375 train_time:53490ms step_avg:140.39ms
step:392/1375 train_time:53633ms step_avg:140.40ms
step:393/1375 train_time:53777ms step_avg:140.41ms
step:394/1375 train_time:53924ms step_avg:140.43ms
step:395/1375 train_time:54069ms step_avg:140.44ms
step:396/1375 train_time:54214ms step_avg:140.45ms
step:397/1375 train_time:54360ms step_avg:140.46ms
step:398/1375 train_time:54504ms step_avg:140.47ms
step:399/1375 train_time:54647ms step_avg:140.48ms
step:400/1375 train_time:54790ms step_avg:140.49ms
step:401/1375 train_time:54934ms step_avg:140.50ms
step:402/1375 train_time:55080ms step_avg:140.51ms
step:403/1375 train_time:55223ms step_avg:140.52ms
step:404/1375 train_time:55367ms step_avg:140.53ms
step:405/1375 train_time:55512ms step_avg:140.54ms
step:406/1375 train_time:55657ms step_avg:140.55ms
step:407/1375 train_time:55801ms step_avg:140.56ms
step:408/1375 train_time:55944ms step_avg:140.56ms
step:409/1375 train_time:56089ms step_avg:140.57ms
step:410/1375 train_time:56237ms step_avg:140.59ms
step:411/1375 train_time:56384ms step_avg:140.61ms
step:412/1375 train_time:56528ms step_avg:140.62ms
step:413/1375 train_time:56675ms step_avg:140.63ms
step:414/1375 train_time:56822ms step_avg:140.65ms
step:415/1375 train_time:56968ms step_avg:140.66ms
step:416/1375 train_time:57114ms step_avg:140.67ms
step:417/1375 train_time:57263ms step_avg:140.69ms
step:418/1375 train_time:57409ms step_avg:140.71ms
step:419/1375 train_time:57555ms step_avg:140.72ms
step:420/1375 train_time:57702ms step_avg:140.74ms
step:421/1375 train_time:57847ms step_avg:140.75ms
step:422/1375 train_time:57994ms step_avg:140.76ms
step:423/1375 train_time:58141ms step_avg:140.78ms
step:424/1375 train_time:58287ms step_avg:140.79ms
step:425/1375 train_time:58434ms step_avg:140.81ms
step:426/1375 train_time:58581ms step_avg:140.82ms
step:427/1375 train_time:58727ms step_avg:140.83ms
step:428/1375 train_time:58871ms step_avg:140.84ms
step:429/1375 train_time:59018ms step_avg:140.85ms
step:430/1375 train_time:59165ms step_avg:140.87ms
step:431/1375 train_time:59310ms step_avg:140.88ms
step:432/1375 train_time:59458ms step_avg:140.89ms
step:433/1375 train_time:59603ms step_avg:140.91ms
step:434/1375 train_time:59748ms step_avg:140.91ms
step:435/1375 train_time:59895ms step_avg:140.93ms
step:436/1375 train_time:60042ms step_avg:140.94ms
step:437/1375 train_time:60188ms step_avg:140.96ms
step:438/1375 train_time:60333ms step_avg:140.97ms
step:439/1375 train_time:60480ms step_avg:140.98ms
step:440/1375 train_time:60625ms step_avg:140.99ms
step:441/1375 train_time:60771ms step_avg:141.00ms
step:442/1375 train_time:60919ms step_avg:141.02ms
step:443/1375 train_time:61065ms step_avg:141.03ms
step:444/1375 train_time:61211ms step_avg:141.04ms
step:445/1375 train_time:61359ms step_avg:141.06ms
step:446/1375 train_time:61506ms step_avg:141.07ms
step:447/1375 train_time:61649ms step_avg:141.07ms
step:448/1375 train_time:61795ms step_avg:141.08ms
step:449/1375 train_time:61942ms step_avg:141.10ms
step:450/1375 train_time:62089ms step_avg:141.11ms
step:451/1375 train_time:62238ms step_avg:141.13ms
step:452/1375 train_time:62385ms step_avg:141.14ms
step:453/1375 train_time:62529ms step_avg:141.15ms
step:454/1375 train_time:62677ms step_avg:141.16ms
step:455/1375 train_time:62823ms step_avg:141.18ms
step:456/1375 train_time:62967ms step_avg:141.18ms
step:457/1375 train_time:63112ms step_avg:141.19ms
step:458/1375 train_time:63260ms step_avg:141.21ms
step:459/1375 train_time:63407ms step_avg:141.22ms
step:460/1375 train_time:63550ms step_avg:141.22ms
step:461/1375 train_time:63697ms step_avg:141.23ms
step:462/1375 train_time:63844ms step_avg:141.25ms
step:463/1375 train_time:63989ms step_avg:141.26ms
step:464/1375 train_time:64134ms step_avg:141.26ms
step:465/1375 train_time:64281ms step_avg:141.28ms
step:466/1375 train_time:64427ms step_avg:141.29ms
step:467/1375 train_time:64572ms step_avg:141.30ms
step:468/1375 train_time:64719ms step_avg:141.31ms
step:469/1375 train_time:64865ms step_avg:141.32ms
step:470/1375 train_time:65010ms step_avg:141.33ms
step:471/1375 train_time:65157ms step_avg:141.34ms
step:472/1375 train_time:65303ms step_avg:141.35ms
step:473/1375 train_time:65448ms step_avg:141.36ms
step:474/1375 train_time:65594ms step_avg:141.37ms
step:475/1375 train_time:65741ms step_avg:141.38ms
step:476/1375 train_time:65888ms step_avg:141.39ms
step:477/1375 train_time:66035ms step_avg:141.40ms
step:478/1375 train_time:66182ms step_avg:141.41ms
step:479/1375 train_time:66327ms step_avg:141.42ms
step:480/1375 train_time:66472ms step_avg:141.43ms
step:481/1375 train_time:66618ms step_avg:141.44ms
step:482/1375 train_time:66765ms step_avg:141.45ms
step:483/1375 train_time:66911ms step_avg:141.46ms
step:484/1375 train_time:67057ms step_avg:141.47ms
step:485/1375 train_time:67204ms step_avg:141.48ms
step:486/1375 train_time:67349ms step_avg:141.49ms
step:487/1375 train_time:67493ms step_avg:141.50ms
step:488/1375 train_time:67641ms step_avg:141.51ms
step:489/1375 train_time:67787ms step_avg:141.52ms
step:490/1375 train_time:67933ms step_avg:141.53ms
step:491/1375 train_time:68081ms step_avg:141.54ms
step:492/1375 train_time:68226ms step_avg:141.55ms
step:493/1375 train_time:68371ms step_avg:141.55ms
step:494/1375 train_time:68517ms step_avg:141.56ms
step:495/1375 train_time:68664ms step_avg:141.58ms
step:496/1375 train_time:68809ms step_avg:141.58ms
step:497/1375 train_time:68955ms step_avg:141.59ms
step:498/1375 train_time:69100ms step_avg:141.60ms
step:499/1375 train_time:69246ms step_avg:141.61ms
step:500/1375 train_time:69390ms step_avg:141.61ms
step:500/1375 val_loss:3.6578 train_time:69462ms step_avg:141.76ms
step:501/1375 train_time:69537ms step_avg:141.62ms
step:502/1375 train_time:69683ms step_avg:141.63ms
step:503/1375 train_time:69829ms step_avg:141.64ms
step:504/1375 train_time:69973ms step_avg:141.65ms
step:505/1375 train_time:70120ms step_avg:141.66ms
step:506/1375 train_time:70265ms step_avg:141.66ms
step:507/1375 train_time:70411ms step_avg:141.67ms
step:508/1375 train_time:70561ms step_avg:141.69ms
step:509/1375 train_time:70706ms step_avg:141.70ms
step:510/1375 train_time:70852ms step_avg:141.70ms
step:511/1375 train_time:70999ms step_avg:141.71ms
step:512/1375 train_time:71146ms step_avg:141.73ms
step:513/1375 train_time:71293ms step_avg:141.73ms
step:514/1375 train_time:71443ms step_avg:141.75ms
step:515/1375 train_time:71589ms step_avg:141.76ms
step:516/1375 train_time:71740ms step_avg:141.78ms
step:517/1375 train_time:71887ms step_avg:141.79ms
step:518/1375 train_time:72034ms step_avg:141.80ms
step:519/1375 train_time:72182ms step_avg:141.81ms
step:520/1375 train_time:72328ms step_avg:141.82ms
step:521/1375 train_time:72474ms step_avg:141.83ms
step:522/1375 train_time:72623ms step_avg:141.84ms
step:523/1375 train_time:72770ms step_avg:141.85ms
step:524/1375 train_time:72920ms step_avg:141.87ms
step:525/1375 train_time:73067ms step_avg:141.88ms
step:526/1375 train_time:73215ms step_avg:141.89ms
step:527/1375 train_time:73362ms step_avg:141.90ms
step:528/1375 train_time:73508ms step_avg:141.91ms
step:529/1375 train_time:73658ms step_avg:141.92ms
step:530/1375 train_time:73805ms step_avg:141.93ms
step:531/1375 train_time:73952ms step_avg:141.94ms
step:532/1375 train_time:74100ms step_avg:141.95ms
step:533/1375 train_time:74247ms step_avg:141.96ms
step:534/1375 train_time:74393ms step_avg:141.97ms
step:535/1375 train_time:74543ms step_avg:141.99ms
step:536/1375 train_time:74692ms step_avg:142.00ms
step:537/1375 train_time:74842ms step_avg:142.01ms
step:538/1375 train_time:74988ms step_avg:142.02ms
step:539/1375 train_time:75138ms step_avg:142.04ms
step:540/1375 train_time:75286ms step_avg:142.05ms
step:541/1375 train_time:75434ms step_avg:142.06ms
step:542/1375 train_time:75582ms step_avg:142.07ms
step:543/1375 train_time:75729ms step_avg:142.08ms
step:544/1375 train_time:75876ms step_avg:142.09ms
step:545/1375 train_time:76025ms step_avg:142.10ms
step:546/1375 train_time:76172ms step_avg:142.11ms
step:547/1375 train_time:76321ms step_avg:142.13ms
step:548/1375 train_time:76469ms step_avg:142.14ms
step:549/1375 train_time:76618ms step_avg:142.15ms
step:550/1375 train_time:76767ms step_avg:142.16ms
step:551/1375 train_time:76913ms step_avg:142.17ms
step:552/1375 train_time:77062ms step_avg:142.18ms
step:553/1375 train_time:77208ms step_avg:142.19ms
step:554/1375 train_time:77357ms step_avg:142.20ms
step:555/1375 train_time:77505ms step_avg:142.21ms
step:556/1375 train_time:77652ms step_avg:142.22ms
step:557/1375 train_time:77801ms step_avg:142.23ms
step:558/1375 train_time:77949ms step_avg:142.24ms
step:559/1375 train_time:78096ms step_avg:142.25ms
step:560/1375 train_time:78245ms step_avg:142.26ms
step:561/1375 train_time:78392ms step_avg:142.27ms
step:562/1375 train_time:78540ms step_avg:142.28ms
step:563/1375 train_time:78687ms step_avg:142.29ms
step:564/1375 train_time:78837ms step_avg:142.30ms
step:565/1375 train_time:78984ms step_avg:142.31ms
step:566/1375 train_time:79130ms step_avg:142.32ms
step:567/1375 train_time:79276ms step_avg:142.33ms
step:568/1375 train_time:79424ms step_avg:142.34ms
step:569/1375 train_time:79572ms step_avg:142.35ms
step:570/1375 train_time:79720ms step_avg:142.36ms
step:571/1375 train_time:79912ms step_avg:142.45ms
step:572/1375 train_time:80059ms step_avg:142.45ms
step:573/1375 train_time:80205ms step_avg:142.46ms
step:574/1375 train_time:80353ms step_avg:142.47ms
step:575/1375 train_time:80501ms step_avg:142.48ms
step:576/1375 train_time:80647ms step_avg:142.49ms
step:577/1375 train_time:80795ms step_avg:142.49ms
step:578/1375 train_time:80945ms step_avg:142.51ms
step:579/1375 train_time:81092ms step_avg:142.52ms
step:580/1375 train_time:81242ms step_avg:142.53ms
step:581/1375 train_time:81388ms step_avg:142.54ms
step:582/1375 train_time:81536ms step_avg:142.55ms
step:583/1375 train_time:81683ms step_avg:142.55ms
step:584/1375 train_time:81829ms step_avg:142.56ms
step:585/1375 train_time:81977ms step_avg:142.57ms
step:586/1375 train_time:82126ms step_avg:142.58ms
step:587/1375 train_time:82273ms step_avg:142.59ms
step:588/1375 train_time:82422ms step_avg:142.60ms
step:589/1375 train_time:82569ms step_avg:142.61ms
step:590/1375 train_time:82717ms step_avg:142.62ms
step:591/1375 train_time:82866ms step_avg:142.63ms
step:592/1375 train_time:83014ms step_avg:142.64ms
step:593/1375 train_time:83163ms step_avg:142.65ms
step:594/1375 train_time:83309ms step_avg:142.65ms
step:595/1375 train_time:83456ms step_avg:142.66ms
step:596/1375 train_time:83604ms step_avg:142.67ms
step:597/1375 train_time:83753ms step_avg:142.68ms
step:598/1375 train_time:83901ms step_avg:142.69ms
step:599/1375 train_time:84047ms step_avg:142.70ms
step:600/1375 train_time:84194ms step_avg:142.70ms
step:601/1375 train_time:84343ms step_avg:142.71ms
step:602/1375 train_time:84489ms step_avg:142.72ms
step:603/1375 train_time:84639ms step_avg:142.73ms
step:604/1375 train_time:84787ms step_avg:142.74ms
step:605/1375 train_time:84933ms step_avg:142.75ms
step:606/1375 train_time:85081ms step_avg:142.75ms
step:607/1375 train_time:85227ms step_avg:142.76ms
step:608/1375 train_time:85375ms step_avg:142.77ms
step:609/1375 train_time:85523ms step_avg:142.78ms
step:610/1375 train_time:85669ms step_avg:142.78ms
step:611/1375 train_time:85817ms step_avg:142.79ms
step:612/1375 train_time:85965ms step_avg:142.80ms
step:613/1375 train_time:86112ms step_avg:142.81ms
step:614/1375 train_time:86262ms step_avg:142.82ms
step:615/1375 train_time:86408ms step_avg:142.82ms
step:616/1375 train_time:86558ms step_avg:142.84ms
step:617/1375 train_time:86707ms step_avg:142.85ms
step:618/1375 train_time:86857ms step_avg:142.86ms
step:619/1375 train_time:87007ms step_avg:142.87ms
step:620/1375 train_time:87155ms step_avg:142.88ms
step:621/1375 train_time:87304ms step_avg:142.89ms
step:622/1375 train_time:87454ms step_avg:142.90ms
step:623/1375 train_time:87603ms step_avg:142.91ms
step:624/1375 train_time:87751ms step_avg:142.92ms
step:625/1375 train_time:87900ms step_avg:142.93ms
step:625/1375 val_loss:3.5735 train_time:87975ms step_avg:143.05ms
step:626/1375 train_time:88052ms step_avg:142.94ms
step:627/1375 train_time:88202ms step_avg:142.95ms
step:628/1375 train_time:88350ms step_avg:142.96ms
step:629/1375 train_time:88497ms step_avg:142.97ms
step:630/1375 train_time:88646ms step_avg:142.98ms
step:631/1375 train_time:88791ms step_avg:142.98ms
step:632/1375 train_time:88941ms step_avg:142.99ms
step:633/1375 train_time:89092ms step_avg:143.00ms
step:634/1375 train_time:89242ms step_avg:143.02ms
step:635/1375 train_time:89390ms step_avg:143.02ms
step:636/1375 train_time:89539ms step_avg:143.03ms
step:637/1375 train_time:89689ms step_avg:143.04ms
step:638/1375 train_time:89836ms step_avg:143.05ms
step:639/1375 train_time:89985ms step_avg:143.06ms
step:640/1375 train_time:90134ms step_avg:143.07ms
step:641/1375 train_time:90284ms step_avg:143.08ms
step:642/1375 train_time:90433ms step_avg:143.09ms
step:643/1375 train_time:90583ms step_avg:143.10ms
step:644/1375 train_time:90732ms step_avg:143.11ms
step:645/1375 train_time:90880ms step_avg:143.12ms
step:646/1375 train_time:91030ms step_avg:143.13ms
step:647/1375 train_time:91176ms step_avg:143.13ms
step:648/1375 train_time:91331ms step_avg:143.15ms
step:649/1375 train_time:91480ms step_avg:143.16ms
step:650/1375 train_time:91631ms step_avg:143.17ms
step:651/1375 train_time:91779ms step_avg:143.18ms
step:652/1375 train_time:91930ms step_avg:143.19ms
step:653/1375 train_time:92078ms step_avg:143.20ms
step:654/1375 train_time:92231ms step_avg:143.22ms
step:655/1375 train_time:92378ms step_avg:143.22ms
step:656/1375 train_time:92530ms step_avg:143.23ms
step:657/1375 train_time:92676ms step_avg:143.24ms
step:658/1375 train_time:92828ms step_avg:143.25ms
step:659/1375 train_time:92976ms step_avg:143.26ms
step:660/1375 train_time:93125ms step_avg:143.27ms
step:661/1375 train_time:93274ms step_avg:143.28ms
step:662/1375 train_time:93424ms step_avg:143.29ms
step:663/1375 train_time:93571ms step_avg:143.29ms
step:664/1375 train_time:93721ms step_avg:143.30ms
step:665/1375 train_time:93871ms step_avg:143.31ms
step:666/1375 train_time:94019ms step_avg:143.32ms
step:667/1375 train_time:94169ms step_avg:143.33ms
step:668/1375 train_time:94318ms step_avg:143.34ms
step:669/1375 train_time:94469ms step_avg:143.35ms
step:670/1375 train_time:94617ms step_avg:143.36ms
step:671/1375 train_time:94768ms step_avg:143.37ms
step:672/1375 train_time:94915ms step_avg:143.38ms
step:673/1375 train_time:95065ms step_avg:143.39ms
step:674/1375 train_time:95215ms step_avg:143.40ms
step:675/1375 train_time:95365ms step_avg:143.41ms
step:676/1375 train_time:95514ms step_avg:143.41ms
step:677/1375 train_time:95664ms step_avg:143.42ms
step:678/1375 train_time:95812ms step_avg:143.43ms
step:679/1375 train_time:95964ms step_avg:143.44ms
step:680/1375 train_time:96112ms step_avg:143.45ms
step:681/1375 train_time:96261ms step_avg:143.46ms
step:682/1375 train_time:96411ms step_avg:143.47ms
step:683/1375 train_time:96560ms step_avg:143.48ms
step:684/1375 train_time:96709ms step_avg:143.49ms
step:685/1375 train_time:96858ms step_avg:143.49ms
step:686/1375 train_time:97006ms step_avg:143.50ms
step:687/1375 train_time:97155ms step_avg:143.51ms
step:688/1375 train_time:97308ms step_avg:143.52ms
step:689/1375 train_time:97456ms step_avg:143.53ms
step:690/1375 train_time:97608ms step_avg:143.54ms
step:691/1375 train_time:97756ms step_avg:143.55ms
step:692/1375 train_time:97908ms step_avg:143.56ms
step:693/1375 train_time:98055ms step_avg:143.56ms
step:694/1375 train_time:98206ms step_avg:143.58ms
step:695/1375 train_time:98352ms step_avg:143.58ms
step:696/1375 train_time:98502ms step_avg:143.59ms
step:697/1375 train_time:98652ms step_avg:143.60ms
step:698/1375 train_time:98800ms step_avg:143.60ms
step:699/1375 train_time:98950ms step_avg:143.61ms
step:700/1375 train_time:99097ms step_avg:143.62ms
step:701/1375 train_time:99247ms step_avg:143.63ms
step:702/1375 train_time:99397ms step_avg:143.64ms
step:703/1375 train_time:99547ms step_avg:143.65ms
step:704/1375 train_time:99695ms step_avg:143.65ms
step:705/1375 train_time:99845ms step_avg:143.66ms
step:706/1375 train_time:99996ms step_avg:143.67ms
step:707/1375 train_time:100145ms step_avg:143.68ms
step:708/1375 train_time:100292ms step_avg:143.69ms
step:709/1375 train_time:100443ms step_avg:143.70ms
step:710/1375 train_time:100592ms step_avg:143.70ms
step:711/1375 train_time:100744ms step_avg:143.71ms
step:712/1375 train_time:100892ms step_avg:143.72ms
step:713/1375 train_time:101043ms step_avg:143.73ms
step:714/1375 train_time:101191ms step_avg:143.74ms
step:715/1375 train_time:101340ms step_avg:143.75ms
step:716/1375 train_time:101492ms step_avg:143.76ms
step:717/1375 train_time:101643ms step_avg:143.77ms
step:718/1375 train_time:101793ms step_avg:143.78ms
step:719/1375 train_time:101943ms step_avg:143.78ms
step:720/1375 train_time:102094ms step_avg:143.79ms
step:721/1375 train_time:102244ms step_avg:143.80ms
step:722/1375 train_time:102395ms step_avg:143.81ms
step:723/1375 train_time:102544ms step_avg:143.82ms
step:724/1375 train_time:102696ms step_avg:143.83ms
step:725/1375 train_time:102846ms step_avg:143.84ms
step:726/1375 train_time:102995ms step_avg:143.85ms
step:727/1375 train_time:103149ms step_avg:143.86ms
step:728/1375 train_time:103298ms step_avg:143.87ms
step:729/1375 train_time:103448ms step_avg:143.88ms
step:730/1375 train_time:103598ms step_avg:143.89ms
step:731/1375 train_time:103750ms step_avg:143.90ms
step:732/1375 train_time:103898ms step_avg:143.90ms
step:733/1375 train_time:104050ms step_avg:143.91ms
step:734/1375 train_time:104198ms step_avg:143.92ms
step:735/1375 train_time:104351ms step_avg:143.93ms
step:736/1375 train_time:104501ms step_avg:143.94ms
step:737/1375 train_time:104651ms step_avg:143.95ms
step:738/1375 train_time:104803ms step_avg:143.96ms
step:739/1375 train_time:104953ms step_avg:143.97ms
step:740/1375 train_time:105105ms step_avg:143.98ms
step:741/1375 train_time:105256ms step_avg:143.99ms
step:742/1375 train_time:105408ms step_avg:144.00ms
step:743/1375 train_time:105557ms step_avg:144.01ms
step:744/1375 train_time:105708ms step_avg:144.02ms
step:745/1375 train_time:105860ms step_avg:144.03ms
step:746/1375 train_time:106010ms step_avg:144.04ms
step:747/1375 train_time:106159ms step_avg:144.04ms
step:748/1375 train_time:106311ms step_avg:144.05ms
step:749/1375 train_time:106460ms step_avg:144.06ms
step:750/1375 train_time:106611ms step_avg:144.07ms
step:750/1375 val_loss:3.5205 train_time:106688ms step_avg:144.17ms
step:751/1375 train_time:106765ms step_avg:144.08ms
step:752/1375 train_time:106916ms step_avg:144.09ms
step:753/1375 train_time:107066ms step_avg:144.10ms
step:754/1375 train_time:107216ms step_avg:144.11ms
step:755/1375 train_time:107366ms step_avg:144.12ms
step:756/1375 train_time:107515ms step_avg:144.12ms
step:757/1375 train_time:107667ms step_avg:144.13ms
step:758/1375 train_time:107818ms step_avg:144.14ms
step:759/1375 train_time:107969ms step_avg:144.15ms
step:760/1375 train_time:108119ms step_avg:144.16ms
step:761/1375 train_time:108308ms step_avg:144.22ms
step:762/1375 train_time:108458ms step_avg:144.23ms
step:763/1375 train_time:108606ms step_avg:144.23ms
step:764/1375 train_time:108758ms step_avg:144.24ms
step:765/1375 train_time:108908ms step_avg:144.25ms
step:766/1375 train_time:109060ms step_avg:144.26ms
step:767/1375 train_time:109211ms step_avg:144.27ms
step:768/1375 train_time:109364ms step_avg:144.28ms
step:769/1375 train_time:109515ms step_avg:144.29ms
step:770/1375 train_time:109666ms step_avg:144.30ms
step:771/1375 train_time:109817ms step_avg:144.31ms
step:772/1375 train_time:109966ms step_avg:144.31ms
step:773/1375 train_time:110116ms step_avg:144.32ms
step:774/1375 train_time:110268ms step_avg:144.33ms
step:775/1375 train_time:110422ms step_avg:144.34ms
step:776/1375 train_time:110572ms step_avg:144.35ms
step:777/1375 train_time:110725ms step_avg:144.36ms
step:778/1375 train_time:110873ms step_avg:144.37ms
step:779/1375 train_time:111023ms step_avg:144.37ms
step:780/1375 train_time:111175ms step_avg:144.38ms
step:781/1375 train_time:111326ms step_avg:144.39ms
step:782/1375 train_time:111477ms step_avg:144.40ms
step:783/1375 train_time:111627ms step_avg:144.41ms
step:784/1375 train_time:111779ms step_avg:144.42ms
step:785/1375 train_time:111928ms step_avg:144.42ms
step:786/1375 train_time:112079ms step_avg:144.43ms
step:787/1375 train_time:112229ms step_avg:144.44ms
step:788/1375 train_time:112380ms step_avg:144.45ms
step:789/1375 train_time:112528ms step_avg:144.45ms
step:790/1375 train_time:112680ms step_avg:144.46ms
step:791/1375 train_time:112829ms step_avg:144.47ms
step:792/1375 train_time:112981ms step_avg:144.48ms
step:793/1375 train_time:113130ms step_avg:144.48ms
step:794/1375 train_time:113282ms step_avg:144.49ms
step:795/1375 train_time:113435ms step_avg:144.50ms
step:796/1375 train_time:113587ms step_avg:144.51ms
step:797/1375 train_time:113738ms step_avg:144.52ms
step:798/1375 train_time:113890ms step_avg:144.53ms
step:799/1375 train_time:114046ms step_avg:144.54ms
step:800/1375 train_time:114194ms step_avg:144.55ms
step:801/1375 train_time:114344ms step_avg:144.56ms
step:802/1375 train_time:114496ms step_avg:144.57ms
step:803/1375 train_time:114646ms step_avg:144.57ms
step:804/1375 train_time:114796ms step_avg:144.58ms
step:805/1375 train_time:114949ms step_avg:144.59ms
step:806/1375 train_time:115101ms step_avg:144.60ms
step:807/1375 train_time:115249ms step_avg:144.60ms
step:808/1375 train_time:115400ms step_avg:144.61ms
step:809/1375 train_time:115548ms step_avg:144.62ms
step:810/1375 train_time:115700ms step_avg:144.62ms
step:811/1375 train_time:115850ms step_avg:144.63ms
step:812/1375 train_time:116001ms step_avg:144.64ms
step:813/1375 train_time:116148ms step_avg:144.64ms
step:814/1375 train_time:116300ms step_avg:144.65ms
step:815/1375 train_time:116449ms step_avg:144.66ms
step:816/1375 train_time:116602ms step_avg:144.67ms
step:817/1375 train_time:116753ms step_avg:144.68ms
step:818/1375 train_time:116905ms step_avg:144.68ms
step:819/1375 train_time:117057ms step_avg:144.69ms
step:820/1375 train_time:117212ms step_avg:144.71ms
step:821/1375 train_time:117364ms step_avg:144.72ms
step:822/1375 train_time:117514ms step_avg:144.72ms
step:823/1375 train_time:117667ms step_avg:144.73ms
step:824/1375 train_time:117819ms step_avg:144.74ms
step:825/1375 train_time:117971ms step_avg:144.75ms
step:826/1375 train_time:118124ms step_avg:144.76ms
step:827/1375 train_time:118274ms step_avg:144.77ms
step:828/1375 train_time:118426ms step_avg:144.78ms
step:829/1375 train_time:118579ms step_avg:144.79ms
step:830/1375 train_time:118731ms step_avg:144.79ms
step:831/1375 train_time:118884ms step_avg:144.80ms
step:832/1375 train_time:119036ms step_avg:144.81ms
step:833/1375 train_time:119187ms step_avg:144.82ms
step:834/1375 train_time:119340ms step_avg:144.83ms
step:835/1375 train_time:119492ms step_avg:144.84ms
step:836/1375 train_time:119646ms step_avg:144.85ms
step:837/1375 train_time:119798ms step_avg:144.86ms
step:838/1375 train_time:119949ms step_avg:144.87ms
step:839/1375 train_time:120100ms step_avg:144.87ms
step:840/1375 train_time:120251ms step_avg:144.88ms
step:841/1375 train_time:120404ms step_avg:144.89ms
step:842/1375 train_time:120557ms step_avg:144.90ms
step:843/1375 train_time:120707ms step_avg:144.91ms
step:844/1375 train_time:120860ms step_avg:144.92ms
step:845/1375 train_time:121008ms step_avg:144.92ms
step:846/1375 train_time:121162ms step_avg:144.93ms
step:847/1375 train_time:121313ms step_avg:144.94ms
step:848/1375 train_time:121465ms step_avg:144.95ms
step:849/1375 train_time:121617ms step_avg:144.96ms
step:850/1375 train_time:121771ms step_avg:144.96ms
step:851/1375 train_time:121924ms step_avg:144.98ms
step:852/1375 train_time:122077ms step_avg:144.98ms
step:853/1375 train_time:122226ms step_avg:144.99ms
step:854/1375 train_time:122378ms step_avg:145.00ms
step:855/1375 train_time:122527ms step_avg:145.00ms
step:856/1375 train_time:122677ms step_avg:145.01ms
step:857/1375 train_time:122829ms step_avg:145.02ms
step:858/1375 train_time:122986ms step_avg:145.03ms
step:859/1375 train_time:123138ms step_avg:145.04ms
step:860/1375 train_time:123289ms step_avg:145.05ms
step:861/1375 train_time:123442ms step_avg:145.06ms
step:862/1375 train_time:123594ms step_avg:145.06ms
step:863/1375 train_time:123746ms step_avg:145.07ms
step:864/1375 train_time:123899ms step_avg:145.08ms
step:865/1375 train_time:124049ms step_avg:145.09ms
step:866/1375 train_time:124205ms step_avg:145.10ms
step:867/1375 train_time:124355ms step_avg:145.11ms
step:868/1375 train_time:124505ms step_avg:145.11ms
step:869/1375 train_time:124657ms step_avg:145.12ms
step:870/1375 train_time:124812ms step_avg:145.13ms
step:871/1375 train_time:124965ms step_avg:145.14ms
step:872/1375 train_time:125117ms step_avg:145.15ms
step:873/1375 train_time:125267ms step_avg:145.15ms
step:874/1375 train_time:125420ms step_avg:145.16ms
step:875/1375 train_time:125573ms step_avg:145.17ms
step:875/1375 val_loss:3.4679 train_time:125651ms step_avg:145.26ms
step:876/1375 train_time:125728ms step_avg:145.18ms
step:877/1375 train_time:125879ms step_avg:145.19ms
step:878/1375 train_time:126032ms step_avg:145.20ms
step:879/1375 train_time:126182ms step_avg:145.20ms
step:880/1375 train_time:126334ms step_avg:145.21ms
step:881/1375 train_time:126483ms step_avg:145.22ms
step:882/1375 train_time:126637ms step_avg:145.23ms
step:883/1375 train_time:126790ms step_avg:145.23ms
step:884/1375 train_time:126943ms step_avg:145.24ms
step:885/1375 train_time:127094ms step_avg:145.25ms
step:886/1375 train_time:127249ms step_avg:145.26ms
step:887/1375 train_time:127399ms step_avg:145.27ms
step:888/1375 train_time:127553ms step_avg:145.28ms
step:889/1375 train_time:127705ms step_avg:145.28ms
step:890/1375 train_time:127855ms step_avg:145.29ms
step:891/1375 train_time:128008ms step_avg:145.30ms
step:892/1375 train_time:128161ms step_avg:145.31ms
step:893/1375 train_time:128314ms step_avg:145.32ms
step:894/1375 train_time:128466ms step_avg:145.32ms
step:895/1375 train_time:128620ms step_avg:145.33ms
step:896/1375 train_time:128773ms step_avg:145.34ms
step:897/1375 train_time:128926ms step_avg:145.35ms
step:898/1375 train_time:129077ms step_avg:145.36ms
step:899/1375 train_time:129229ms step_avg:145.36ms
step:900/1375 train_time:129378ms step_avg:145.37ms
step:901/1375 train_time:129534ms step_avg:145.38ms
step:902/1375 train_time:129683ms step_avg:145.38ms
step:903/1375 train_time:129836ms step_avg:145.39ms
step:904/1375 train_time:129987ms step_avg:145.40ms
step:905/1375 train_time:130139ms step_avg:145.41ms
step:906/1375 train_time:130292ms step_avg:145.41ms
step:907/1375 train_time:130445ms step_avg:145.42ms
step:908/1375 train_time:130596ms step_avg:145.43ms
step:909/1375 train_time:130749ms step_avg:145.44ms
step:910/1375 train_time:130904ms step_avg:145.45ms
step:911/1375 train_time:131055ms step_avg:145.46ms
step:912/1375 train_time:131208ms step_avg:145.46ms
step:913/1375 train_time:131361ms step_avg:145.47ms
step:914/1375 train_time:131513ms step_avg:145.48ms
step:915/1375 train_time:131666ms step_avg:145.49ms
step:916/1375 train_time:131818ms step_avg:145.49ms
step:917/1375 train_time:131969ms step_avg:145.50ms
step:918/1375 train_time:132122ms step_avg:145.51ms
step:919/1375 train_time:132278ms step_avg:145.52ms
step:920/1375 train_time:132431ms step_avg:145.53ms
step:921/1375 train_time:132584ms step_avg:145.54ms
step:922/1375 train_time:132743ms step_avg:145.55ms
step:923/1375 train_time:132895ms step_avg:145.56ms
step:924/1375 train_time:133051ms step_avg:145.57ms
step:925/1375 train_time:133205ms step_avg:145.58ms
step:926/1375 train_time:133359ms step_avg:145.59ms
step:927/1375 train_time:133512ms step_avg:145.60ms
step:928/1375 train_time:133665ms step_avg:145.60ms
step:929/1375 train_time:133820ms step_avg:145.61ms
step:930/1375 train_time:133974ms step_avg:145.62ms
step:931/1375 train_time:134126ms step_avg:145.63ms
step:932/1375 train_time:134278ms step_avg:145.64ms
step:933/1375 train_time:134433ms step_avg:145.65ms
step:934/1375 train_time:134587ms step_avg:145.66ms
step:935/1375 train_time:134741ms step_avg:145.67ms
step:936/1375 train_time:134894ms step_avg:145.67ms
step:937/1375 train_time:135052ms step_avg:145.69ms
step:938/1375 train_time:135204ms step_avg:145.69ms
step:939/1375 train_time:135358ms step_avg:145.70ms
step:940/1375 train_time:135512ms step_avg:145.71ms
step:941/1375 train_time:135663ms step_avg:145.72ms
step:942/1375 train_time:135815ms step_avg:145.72ms
step:943/1375 train_time:135970ms step_avg:145.73ms
step:944/1375 train_time:136128ms step_avg:145.75ms
step:945/1375 train_time:136280ms step_avg:145.75ms
step:946/1375 train_time:136437ms step_avg:145.77ms
step:947/1375 train_time:136591ms step_avg:145.78ms
step:948/1375 train_time:136745ms step_avg:145.78ms
step:949/1375 train_time:136899ms step_avg:145.79ms
step:950/1375 train_time:137053ms step_avg:145.80ms
step:951/1375 train_time:137250ms step_avg:145.86ms
step:952/1375 train_time:137399ms step_avg:145.86ms
step:953/1375 train_time:137553ms step_avg:145.87ms
step:954/1375 train_time:137704ms step_avg:145.87ms
step:955/1375 train_time:137855ms step_avg:145.88ms
step:956/1375 train_time:138008ms step_avg:145.89ms
step:957/1375 train_time:138163ms step_avg:145.90ms
step:958/1375 train_time:138321ms step_avg:145.91ms
step:959/1375 train_time:138475ms step_avg:145.92ms
step:960/1375 train_time:138629ms step_avg:145.93ms
step:961/1375 train_time:138780ms step_avg:145.93ms
step:962/1375 train_time:138933ms step_avg:145.94ms
step:963/1375 train_time:139091ms step_avg:145.95ms
step:964/1375 train_time:139243ms step_avg:145.96ms
step:965/1375 train_time:139396ms step_avg:145.96ms
step:966/1375 train_time:139550ms step_avg:145.97ms
step:967/1375 train_time:139702ms step_avg:145.98ms
step:968/1375 train_time:139854ms step_avg:145.99ms
step:969/1375 train_time:140008ms step_avg:145.99ms
step:970/1375 train_time:140161ms step_avg:146.00ms
step:971/1375 train_time:140317ms step_avg:146.01ms
step:972/1375 train_time:140469ms step_avg:146.02ms
step:973/1375 train_time:140620ms step_avg:146.02ms
step:974/1375 train_time:140774ms step_avg:146.03ms
step:975/1375 train_time:140926ms step_avg:146.04ms
step:976/1375 train_time:141076ms step_avg:146.04ms
step:977/1375 train_time:141231ms step_avg:146.05ms
step:978/1375 train_time:141382ms step_avg:146.06ms
step:979/1375 train_time:141534ms step_avg:146.06ms
step:980/1375 train_time:141688ms step_avg:146.07ms
step:981/1375 train_time:141838ms step_avg:146.07ms
step:982/1375 train_time:141990ms step_avg:146.08ms
step:983/1375 train_time:142141ms step_avg:146.09ms
step:984/1375 train_time:142295ms step_avg:146.09ms
step:985/1375 train_time:142447ms step_avg:146.10ms
step:986/1375 train_time:142600ms step_avg:146.11ms
step:987/1375 train_time:142752ms step_avg:146.11ms
step:988/1375 train_time:142906ms step_avg:146.12ms
step:989/1375 train_time:143058ms step_avg:146.13ms
step:990/1375 train_time:143212ms step_avg:146.13ms
step:991/1375 train_time:143362ms step_avg:146.14ms
step:992/1375 train_time:143522ms step_avg:146.15ms
step:993/1375 train_time:143682ms step_avg:146.17ms
step:994/1375 train_time:143836ms step_avg:146.17ms
step:995/1375 train_time:143987ms step_avg:146.18ms
step:996/1375 train_time:144138ms step_avg:146.18ms
step:997/1375 train_time:144290ms step_avg:146.19ms
step:998/1375 train_time:144440ms step_avg:146.19ms
step:999/1375 train_time:144593ms step_avg:146.20ms
step:1000/1375 train_time:144745ms step_avg:146.21ms
step:1000/1375 val_loss:3.4017 train_time:144822ms step_avg:146.28ms
step:1001/1375 train_time:144899ms step_avg:146.21ms
step:1002/1375 train_time:145056ms step_avg:146.23ms
step:1003/1375 train_time:145210ms step_avg:146.23ms
step:1004/1375 train_time:145363ms step_avg:146.24ms
step:1005/1375 train_time:145516ms step_avg:146.25ms
step:1006/1375 train_time:145667ms step_avg:146.25ms
step:1007/1375 train_time:145818ms step_avg:146.26ms
step:1008/1375 train_time:145974ms step_avg:146.27ms
step:1009/1375 train_time:146131ms step_avg:146.28ms
step:1010/1375 train_time:146282ms step_avg:146.28ms
step:1011/1375 train_time:146434ms step_avg:146.29ms
step:1012/1375 train_time:146586ms step_avg:146.29ms
step:1013/1375 train_time:146741ms step_avg:146.30ms
step:1014/1375 train_time:146893ms step_avg:146.31ms
step:1015/1375 train_time:147047ms step_avg:146.32ms
step:1016/1375 train_time:147200ms step_avg:146.32ms
step:1017/1375 train_time:147354ms step_avg:146.33ms
step:1018/1375 train_time:147506ms step_avg:146.34ms
step:1019/1375 train_time:147660ms step_avg:146.34ms
step:1020/1375 train_time:147816ms step_avg:146.35ms
step:1021/1375 train_time:147968ms step_avg:146.36ms
step:1022/1375 train_time:148120ms step_avg:146.36ms
step:1023/1375 train_time:148276ms step_avg:146.37ms
step:1024/1375 train_time:148429ms step_avg:146.38ms
step:1025/1375 train_time:148585ms step_avg:146.39ms
step:1026/1375 train_time:148738ms step_avg:146.40ms
step:1027/1375 train_time:148891ms step_avg:146.40ms
step:1028/1375 train_time:149046ms step_avg:146.41ms
step:1029/1375 train_time:149201ms step_avg:146.42ms
step:1030/1375 train_time:149359ms step_avg:146.43ms
step:1031/1375 train_time:149511ms step_avg:146.44ms
step:1032/1375 train_time:149663ms step_avg:146.44ms
step:1033/1375 train_time:149818ms step_avg:146.45ms
step:1034/1375 train_time:149972ms step_avg:146.46ms
step:1035/1375 train_time:150128ms step_avg:146.47ms
step:1036/1375 train_time:150283ms step_avg:146.47ms
step:1037/1375 train_time:150442ms step_avg:146.49ms
step:1038/1375 train_time:150596ms step_avg:146.49ms
step:1039/1375 train_time:150749ms step_avg:146.50ms
step:1040/1375 train_time:150900ms step_avg:146.51ms
step:1041/1375 train_time:151056ms step_avg:146.51ms
step:1042/1375 train_time:151206ms step_avg:146.52ms
step:1043/1375 train_time:151360ms step_avg:146.52ms
step:1044/1375 train_time:151517ms step_avg:146.54ms
step:1045/1375 train_time:151673ms step_avg:146.54ms
step:1046/1375 train_time:151824ms step_avg:146.55ms
step:1047/1375 train_time:151978ms step_avg:146.56ms
step:1048/1375 train_time:152133ms step_avg:146.56ms
step:1049/1375 train_time:152287ms step_avg:146.57ms
step:1050/1375 train_time:152445ms step_avg:146.58ms
step:1051/1375 train_time:152604ms step_avg:146.59ms
step:1052/1375 train_time:152759ms step_avg:146.60ms
step:1053/1375 train_time:152912ms step_avg:146.61ms
step:1054/1375 train_time:153070ms step_avg:146.62ms
step:1055/1375 train_time:153223ms step_avg:146.62ms
step:1056/1375 train_time:153380ms step_avg:146.63ms
step:1057/1375 train_time:153536ms step_avg:146.64ms
step:1058/1375 train_time:153691ms step_avg:146.65ms
step:1059/1375 train_time:153847ms step_avg:146.66ms
step:1060/1375 train_time:154003ms step_avg:146.67ms
step:1061/1375 train_time:154156ms step_avg:146.68ms
step:1062/1375 train_time:154310ms step_avg:146.68ms
step:1063/1375 train_time:154465ms step_avg:146.69ms
step:1064/1375 train_time:154618ms step_avg:146.70ms
step:1065/1375 train_time:154772ms step_avg:146.70ms
step:1066/1375 train_time:154930ms step_avg:146.71ms
step:1067/1375 train_time:155085ms step_avg:146.72ms
step:1068/1375 train_time:155238ms step_avg:146.73ms
step:1069/1375 train_time:155400ms step_avg:146.74ms
step:1070/1375 train_time:155554ms step_avg:146.75ms
step:1071/1375 train_time:155710ms step_avg:146.76ms
step:1072/1375 train_time:155862ms step_avg:146.76ms
step:1073/1375 train_time:156014ms step_avg:146.77ms
step:1074/1375 train_time:156166ms step_avg:146.77ms
step:1075/1375 train_time:156320ms step_avg:146.78ms
step:1076/1375 train_time:156475ms step_avg:146.79ms
step:1077/1375 train_time:156629ms step_avg:146.79ms
step:1078/1375 train_time:156787ms step_avg:146.80ms
step:1079/1375 train_time:156947ms step_avg:146.82ms
step:1080/1375 train_time:157100ms step_avg:146.82ms
step:1081/1375 train_time:157253ms step_avg:146.83ms
step:1082/1375 train_time:157405ms step_avg:146.83ms
step:1083/1375 train_time:157559ms step_avg:146.84ms
step:1084/1375 train_time:157717ms step_avg:146.85ms
step:1085/1375 train_time:157870ms step_avg:146.86ms
step:1086/1375 train_time:158024ms step_avg:146.86ms
step:1087/1375 train_time:158181ms step_avg:146.87ms
step:1088/1375 train_time:158335ms step_avg:146.88ms
step:1089/1375 train_time:158494ms step_avg:146.89ms
step:1090/1375 train_time:158655ms step_avg:146.90ms
step:1091/1375 train_time:158807ms step_avg:146.91ms
step:1092/1375 train_time:158959ms step_avg:146.91ms
step:1093/1375 train_time:159115ms step_avg:146.92ms
step:1094/1375 train_time:159269ms step_avg:146.93ms
step:1095/1375 train_time:159423ms step_avg:146.93ms
step:1096/1375 train_time:159584ms step_avg:146.95ms
step:1097/1375 train_time:159737ms step_avg:146.95ms
step:1098/1375 train_time:159891ms step_avg:146.96ms
step:1099/1375 train_time:160043ms step_avg:146.96ms
step:1100/1375 train_time:160197ms step_avg:146.97ms
step:1101/1375 train_time:160349ms step_avg:146.97ms
step:1102/1375 train_time:160507ms step_avg:146.98ms
step:1103/1375 train_time:160661ms step_avg:146.99ms
step:1104/1375 train_time:160814ms step_avg:147.00ms
step:1105/1375 train_time:160969ms step_avg:147.00ms
step:1106/1375 train_time:161121ms step_avg:147.01ms
step:1107/1375 train_time:161275ms step_avg:147.01ms
step:1108/1375 train_time:161433ms step_avg:147.02ms
step:1109/1375 train_time:161584ms step_avg:147.03ms
step:1110/1375 train_time:161739ms step_avg:147.04ms
step:1111/1375 train_time:161896ms step_avg:147.04ms
step:1112/1375 train_time:162050ms step_avg:147.05ms
step:1113/1375 train_time:162205ms step_avg:147.06ms
step:1114/1375 train_time:162362ms step_avg:147.07ms
step:1115/1375 train_time:162517ms step_avg:147.07ms
step:1116/1375 train_time:162668ms step_avg:147.08ms
step:1117/1375 train_time:162825ms step_avg:147.09ms
step:1118/1375 train_time:162983ms step_avg:147.10ms
step:1119/1375 train_time:163139ms step_avg:147.10ms
step:1120/1375 train_time:163294ms step_avg:147.11ms
step:1121/1375 train_time:163447ms step_avg:147.12ms
step:1122/1375 train_time:163599ms step_avg:147.12ms
step:1123/1375 train_time:163755ms step_avg:147.13ms
step:1124/1375 train_time:163911ms step_avg:147.14ms
step:1125/1375 train_time:164065ms step_avg:147.14ms
step:1125/1375 val_loss:3.3478 train_time:164144ms step_avg:147.21ms
step:1126/1375 train_time:164221ms step_avg:147.15ms
step:1127/1375 train_time:164377ms step_avg:147.16ms
step:1128/1375 train_time:164534ms step_avg:147.17ms
step:1129/1375 train_time:164693ms step_avg:147.18ms
step:1130/1375 train_time:164847ms step_avg:147.18ms
step:1131/1375 train_time:165004ms step_avg:147.19ms
step:1132/1375 train_time:165157ms step_avg:147.20ms
step:1133/1375 train_time:165313ms step_avg:147.21ms
step:1134/1375 train_time:165471ms step_avg:147.22ms
step:1135/1375 train_time:165624ms step_avg:147.22ms
step:1136/1375 train_time:165785ms step_avg:147.23ms
step:1137/1375 train_time:165936ms step_avg:147.24ms
step:1138/1375 train_time:166092ms step_avg:147.24ms
step:1139/1375 train_time:166247ms step_avg:147.25ms
step:1140/1375 train_time:166402ms step_avg:147.26ms
step:1141/1375 train_time:166598ms step_avg:147.30ms
step:1142/1375 train_time:166752ms step_avg:147.31ms
step:1143/1375 train_time:166911ms step_avg:147.32ms
step:1144/1375 train_time:167067ms step_avg:147.33ms
step:1145/1375 train_time:167218ms step_avg:147.33ms
step:1146/1375 train_time:167376ms step_avg:147.34ms
step:1147/1375 train_time:167533ms step_avg:147.35ms
step:1148/1375 train_time:167689ms step_avg:147.35ms
step:1149/1375 train_time:167846ms step_avg:147.36ms
step:1150/1375 train_time:167997ms step_avg:147.37ms
step:1151/1375 train_time:168154ms step_avg:147.37ms
step:1152/1375 train_time:168312ms step_avg:147.38ms
step:1153/1375 train_time:168469ms step_avg:147.39ms
step:1154/1375 train_time:168623ms step_avg:147.40ms
step:1155/1375 train_time:168778ms step_avg:147.40ms
step:1156/1375 train_time:168938ms step_avg:147.42ms
step:1157/1375 train_time:169097ms step_avg:147.43ms
step:1158/1375 train_time:169253ms step_avg:147.43ms
step:1159/1375 train_time:169407ms step_avg:147.44ms
step:1160/1375 train_time:169557ms step_avg:147.44ms
step:1161/1375 train_time:169713ms step_avg:147.45ms
step:1162/1375 train_time:169869ms step_avg:147.46ms
step:1163/1375 train_time:170024ms step_avg:147.46ms
step:1164/1375 train_time:170179ms step_avg:147.47ms
step:1165/1375 train_time:170333ms step_avg:147.47ms
step:1166/1375 train_time:170489ms step_avg:147.48ms
step:1167/1375 train_time:170642ms step_avg:147.49ms
step:1168/1375 train_time:170798ms step_avg:147.49ms
step:1169/1375 train_time:170954ms step_avg:147.50ms
step:1170/1375 train_time:171109ms step_avg:147.51ms
step:1171/1375 train_time:171263ms step_avg:147.51ms
step:1172/1375 train_time:171419ms step_avg:147.52ms
step:1173/1375 train_time:171574ms step_avg:147.53ms
step:1174/1375 train_time:171737ms step_avg:147.54ms
step:1175/1375 train_time:171897ms step_avg:147.55ms
step:1176/1375 train_time:172057ms step_avg:147.56ms
step:1177/1375 train_time:172220ms step_avg:147.57ms
step:1178/1375 train_time:172374ms step_avg:147.58ms
step:1179/1375 train_time:172530ms step_avg:147.59ms
step:1180/1375 train_time:172692ms step_avg:147.60ms
step:1181/1375 train_time:172847ms step_avg:147.61ms
step:1182/1375 train_time:173001ms step_avg:147.61ms
step:1183/1375 train_time:173154ms step_avg:147.62ms
step:1184/1375 train_time:173311ms step_avg:147.62ms
step:1185/1375 train_time:173468ms step_avg:147.63ms
step:1186/1375 train_time:173622ms step_avg:147.64ms
step:1187/1375 train_time:173786ms step_avg:147.65ms
step:1188/1375 train_time:173938ms step_avg:147.66ms
step:1189/1375 train_time:174096ms step_avg:147.66ms
step:1190/1375 train_time:174254ms step_avg:147.67ms
step:1191/1375 train_time:174413ms step_avg:147.68ms
step:1192/1375 train_time:174565ms step_avg:147.69ms
step:1193/1375 train_time:174718ms step_avg:147.69ms
step:1194/1375 train_time:174873ms step_avg:147.70ms
step:1195/1375 train_time:175030ms step_avg:147.70ms
step:1196/1375 train_time:175186ms step_avg:147.71ms
step:1197/1375 train_time:175342ms step_avg:147.72ms
step:1198/1375 train_time:175502ms step_avg:147.73ms
step:1199/1375 train_time:175657ms step_avg:147.73ms
step:1200/1375 train_time:175812ms step_avg:147.74ms
step:1201/1375 train_time:175967ms step_avg:147.75ms
step:1202/1375 train_time:176133ms step_avg:147.76ms
step:1203/1375 train_time:176291ms step_avg:147.77ms
step:1204/1375 train_time:176447ms step_avg:147.78ms
step:1205/1375 train_time:176601ms step_avg:147.78ms
step:1206/1375 train_time:176757ms step_avg:147.79ms
step:1207/1375 train_time:176913ms step_avg:147.80ms
step:1208/1375 train_time:177069ms step_avg:147.80ms
step:1209/1375 train_time:177227ms step_avg:147.81ms
step:1210/1375 train_time:177386ms step_avg:147.82ms
step:1211/1375 train_time:177540ms step_avg:147.83ms
step:1212/1375 train_time:177695ms step_avg:147.83ms
step:1213/1375 train_time:177851ms step_avg:147.84ms
step:1214/1375 train_time:178011ms step_avg:147.85ms
step:1215/1375 train_time:178165ms step_avg:147.86ms
step:1216/1375 train_time:178318ms step_avg:147.86ms
step:1217/1375 train_time:178473ms step_avg:147.86ms
step:1218/1375 train_time:178626ms step_avg:147.87ms
step:1219/1375 train_time:178780ms step_avg:147.87ms
step:1220/1375 train_time:178934ms step_avg:147.88ms
step:1221/1375 train_time:179089ms step_avg:147.88ms
step:1222/1375 train_time:179242ms step_avg:147.89ms
step:1223/1375 train_time:179399ms step_avg:147.90ms
step:1224/1375 train_time:179557ms step_avg:147.91ms
step:1225/1375 train_time:179714ms step_avg:147.91ms
step:1226/1375 train_time:179869ms step_avg:147.92ms
step:1227/1375 train_time:180027ms step_avg:147.93ms
step:1228/1375 train_time:180183ms step_avg:147.93ms
step:1229/1375 train_time:180336ms step_avg:147.94ms
step:1230/1375 train_time:180498ms step_avg:147.95ms
step:1231/1375 train_time:180658ms step_avg:147.96ms
step:1232/1375 train_time:180818ms step_avg:147.97ms
step:1233/1375 train_time:180973ms step_avg:147.97ms
step:1234/1375 train_time:181130ms step_avg:147.98ms
step:1235/1375 train_time:181285ms step_avg:147.99ms
step:1236/1375 train_time:181440ms step_avg:147.99ms
step:1237/1375 train_time:181596ms step_avg:148.00ms
step:1238/1375 train_time:181759ms step_avg:148.01ms
step:1239/1375 train_time:181916ms step_avg:148.02ms
step:1240/1375 train_time:182077ms step_avg:148.03ms
step:1241/1375 train_time:182239ms step_avg:148.04ms
step:1242/1375 train_time:182396ms step_avg:148.05ms
step:1243/1375 train_time:182554ms step_avg:148.06ms
step:1244/1375 train_time:182708ms step_avg:148.06ms
step:1245/1375 train_time:182863ms step_avg:148.07ms
step:1246/1375 train_time:183017ms step_avg:148.07ms
step:1247/1375 train_time:183177ms step_avg:148.08ms
step:1248/1375 train_time:183332ms step_avg:148.09ms
step:1249/1375 train_time:183485ms step_avg:148.09ms
step:1250/1375 train_time:183640ms step_avg:148.10ms
step:1250/1375 val_loss:3.3024 train_time:183721ms step_avg:148.16ms
step:1251/1375 train_time:183801ms step_avg:148.11ms
step:1252/1375 train_time:183956ms step_avg:148.11ms
step:1253/1375 train_time:184111ms step_avg:148.12ms
step:1254/1375 train_time:184263ms step_avg:148.12ms
step:1255/1375 train_time:184431ms step_avg:148.14ms
step:1256/1375 train_time:184584ms step_avg:148.14ms
step:1257/1375 train_time:184742ms step_avg:148.15ms
step:1258/1375 train_time:184901ms step_avg:148.16ms
step:1259/1375 train_time:185060ms step_avg:148.17ms
step:1260/1375 train_time:185212ms step_avg:148.17ms
step:1261/1375 train_time:185368ms step_avg:148.18ms
step:1262/1375 train_time:185529ms step_avg:148.19ms
step:1263/1375 train_time:185685ms step_avg:148.19ms
step:1264/1375 train_time:185841ms step_avg:148.20ms
step:1265/1375 train_time:185994ms step_avg:148.20ms
step:1266/1375 train_time:186148ms step_avg:148.21ms
step:1267/1375 train_time:186305ms step_avg:148.21ms
step:1268/1375 train_time:186463ms step_avg:148.22ms
step:1269/1375 train_time:186623ms step_avg:148.23ms
step:1270/1375 train_time:186780ms step_avg:148.24ms
step:1271/1375 train_time:186938ms step_avg:148.25ms
step:1272/1375 train_time:187090ms step_avg:148.25ms
step:1273/1375 train_time:187243ms step_avg:148.25ms
step:1274/1375 train_time:187398ms step_avg:148.26ms
step:1275/1375 train_time:187555ms step_avg:148.27ms
step:1276/1375 train_time:187708ms step_avg:148.27ms
step:1277/1375 train_time:187867ms step_avg:148.28ms
step:1278/1375 train_time:188023ms step_avg:148.28ms
step:1279/1375 train_time:188181ms step_avg:148.29ms
step:1280/1375 train_time:188344ms step_avg:148.30ms
step:1281/1375 train_time:188499ms step_avg:148.31ms
step:1282/1375 train_time:188652ms step_avg:148.31ms
step:1283/1375 train_time:188809ms step_avg:148.32ms
step:1284/1375 train_time:188968ms step_avg:148.33ms
step:1285/1375 train_time:189124ms step_avg:148.33ms
step:1286/1375 train_time:189279ms step_avg:148.34ms
step:1287/1375 train_time:189433ms step_avg:148.34ms
step:1288/1375 train_time:189588ms step_avg:148.35ms
step:1289/1375 train_time:189750ms step_avg:148.36ms
step:1290/1375 train_time:189909ms step_avg:148.37ms
step:1291/1375 train_time:190068ms step_avg:148.37ms
step:1292/1375 train_time:190226ms step_avg:148.38ms
step:1293/1375 train_time:190383ms step_avg:148.39ms
step:1294/1375 train_time:190539ms step_avg:148.39ms
step:1295/1375 train_time:190696ms step_avg:148.40ms
step:1296/1375 train_time:190853ms step_avg:148.41ms
step:1297/1375 train_time:191011ms step_avg:148.42ms
step:1298/1375 train_time:191166ms step_avg:148.42ms
step:1299/1375 train_time:191323ms step_avg:148.43ms
step:1300/1375 train_time:191480ms step_avg:148.43ms
step:1301/1375 train_time:191633ms step_avg:148.44ms
step:1302/1375 train_time:191791ms step_avg:148.44ms
step:1303/1375 train_time:191948ms step_avg:148.45ms
step:1304/1375 train_time:192108ms step_avg:148.46ms
step:1305/1375 train_time:192264ms step_avg:148.47ms
step:1306/1375 train_time:192424ms step_avg:148.48ms
step:1307/1375 train_time:192578ms step_avg:148.48ms
step:1308/1375 train_time:192733ms step_avg:148.48ms
step:1309/1375 train_time:192888ms step_avg:148.49ms
step:1310/1375 train_time:193043ms step_avg:148.49ms
step:1311/1375 train_time:193195ms step_avg:148.50ms
step:1312/1375 train_time:193350ms step_avg:148.50ms
step:1313/1375 train_time:193506ms step_avg:148.51ms
step:1314/1375 train_time:193665ms step_avg:148.52ms
step:1315/1375 train_time:193819ms step_avg:148.52ms
step:1316/1375 train_time:193972ms step_avg:148.52ms
step:1317/1375 train_time:194127ms step_avg:148.53ms
step:1318/1375 train_time:194291ms step_avg:148.54ms
step:1319/1375 train_time:194448ms step_avg:148.55ms
step:1320/1375 train_time:194605ms step_avg:148.55ms
step:1321/1375 train_time:194762ms step_avg:148.56ms
step:1322/1375 train_time:194923ms step_avg:148.57ms
step:1323/1375 train_time:195077ms step_avg:148.57ms
step:1324/1375 train_time:195230ms step_avg:148.58ms
step:1325/1375 train_time:195387ms step_avg:148.58ms
step:1326/1375 train_time:195548ms step_avg:148.59ms
step:1327/1375 train_time:195702ms step_avg:148.60ms
step:1328/1375 train_time:195857ms step_avg:148.60ms
step:1329/1375 train_time:196032ms step_avg:148.62ms
step:1330/1375 train_time:196192ms step_avg:148.63ms
step:1331/1375 train_time:196387ms step_avg:148.67ms
step:1332/1375 train_time:196551ms step_avg:148.68ms
step:1333/1375 train_time:196708ms step_avg:148.68ms
step:1334/1375 train_time:196865ms step_avg:148.69ms
step:1335/1375 train_time:197019ms step_avg:148.69ms
step:1336/1375 train_time:197184ms step_avg:148.71ms
step:1337/1375 train_time:197344ms step_avg:148.71ms
step:1338/1375 train_time:197500ms step_avg:148.72ms
step:1339/1375 train_time:197659ms step_avg:148.73ms
step:1340/1375 train_time:197820ms step_avg:148.74ms
step:1341/1375 train_time:197974ms step_avg:148.74ms
step:1342/1375 train_time:198136ms step_avg:148.75ms
step:1343/1375 train_time:198291ms step_avg:148.76ms
step:1344/1375 train_time:198446ms step_avg:148.76ms
step:1345/1375 train_time:198603ms step_avg:148.77ms
step:1346/1375 train_time:198758ms step_avg:148.77ms
step:1347/1375 train_time:198915ms step_avg:148.78ms
step:1348/1375 train_time:199071ms step_avg:148.78ms
step:1349/1375 train_time:199228ms step_avg:148.79ms
step:1350/1375 train_time:199382ms step_avg:148.79ms
step:1351/1375 train_time:199538ms step_avg:148.80ms
step:1352/1375 train_time:199703ms step_avg:148.81ms
step:1353/1375 train_time:199862ms step_avg:148.82ms
step:1354/1375 train_time:200021ms step_avg:148.83ms
step:1355/1375 train_time:200178ms step_avg:148.83ms
step:1356/1375 train_time:200333ms step_avg:148.84ms
step:1357/1375 train_time:200492ms step_avg:148.84ms
step:1358/1375 train_time:200652ms step_avg:148.85ms
step:1359/1375 train_time:200808ms step_avg:148.86ms
step:1360/1375 train_time:200967ms step_avg:148.86ms
step:1361/1375 train_time:201126ms step_avg:148.87ms
step:1362/1375 train_time:201284ms step_avg:148.88ms
step:1363/1375 train_time:201449ms step_avg:148.89ms
step:1364/1375 train_time:201605ms step_avg:148.90ms
step:1365/1375 train_time:201758ms step_avg:148.90ms
step:1366/1375 train_time:201914ms step_avg:148.90ms
step:1367/1375 train_time:202071ms step_avg:148.91ms
step:1368/1375 train_time:202231ms step_avg:148.92ms
step:1369/1375 train_time:202396ms step_avg:148.93ms
step:1370/1375 train_time:202555ms step_avg:148.94ms
step:1371/1375 train_time:202710ms step_avg:148.94ms
step:1372/1375 train_time:202870ms step_avg:148.95ms
step:1373/1375 train_time:203025ms step_avg:148.95ms
step:1374/1375 train_time:203183ms step_avg:148.96ms
step:1375/1375 train_time:203340ms step_avg:148.97ms
step:1375/1375 val_loss:3.2768 train_time:203415ms step_avg:149.02ms
peak memory consumption: 31565 MiB

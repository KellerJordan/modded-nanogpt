import os
import sys
with open(sys.argv[0]) as f:
    code = f.read() # read the code of this file ASAP, for logging
import uuid
import time
import glob
import subprocess
import contextlib
from dataclasses import dataclass

import torch
torch.empty(1, device='cuda', requires_grad=True).backward()
from torch import nn
import torch.nn.functional as F
import torch.distributed as dist
from torch.nn.parallel import DistributedDataParallel as DDP
# use of FlexAttention contributed by @KoszarskyB
from torch.nn.attention.flex_attention import BlockMask, flex_attention

# -----------------------------------------------------------------------------
# Muon optimizer

@torch.compile
def zeropower_via_newtonschulz5(G, steps):
    """
    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a
    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose
    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at
    zero even beyond the point where the iteration no longer converges all the way to one everywhere
    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T
    where S' is diagonal with S_{ii}' ~ Uniform(0.5, 1.5), which turns out not to hurt model
    performance at all relative to UV^T, where USV^T = G is the SVD.
    """
    assert len(G.shape) == 2
    a, b, c = (3.4445, -4.7750,  2.0315)
    X = G.bfloat16()
    if G.size(0) > G.size(1):
        X = X.T

    # Ensure spectral norm is at most 1
    X = X / (X.norm() + 1e-7)
    # Perform the NS iterations
    for _ in range(steps):
        A = X @ X.T
        B = b * A + c * A @ A # adapted from suggestion by @jxbz, @leloykun, and @YouJiacheng
        X = a * X + B @ X
    
    if G.size(0) > G.size(1):
        X = X.T
    return X

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Some warnings:
    - This optimizer assumes that all parameters passed in are 2D.
    - It should not be used for the embedding layer, the final fully connected layer, or any {0,1}-D
    parameters; those should all be optimized by a standard method (e.g., AdamW).
    - To use it with 4D convolutional filters, it works well to just flatten their last 3 dimensions.
    - We believe it is unlikely to work well for training with small batch size.
    - We believe it may not work well for finetuning pretrained models, but we haven't tested this.
    - We have not yet tried this optimizer for training scenarios larger than NanoGPT (124M).

    Arguments:
        lr: The learning rate used by the internal SGD.
        momentum: The momentum used by the internal SGD.
        nesterov: Whether to use Nesterov-style momentum in the internal SGD. (recommended)
        ns_steps: The number of Newton-Schulz iteration steps to use.
    """
    def __init__(self, params, lr=0.02, momentum=0.95, nesterov=True, ns_steps=5):
        self.world_size = int(os.environ['WORLD_SIZE'])
        self.rank = int(os.environ['RANK'])
        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, ns_steps=ns_steps)
        assert all(isinstance(p, torch.Tensor) for p in params)
        sizes = {p.numel() for p in params}
        param_groups = [dict(params=[p for p in params if p.numel() == size],
                             update_buffer=[torch.empty(size, device='cuda', dtype=torch.bfloat16) for _ in range(self.world_size)])
                        for size in sizes]
        super().__init__(param_groups, defaults)

    def step(self):

        for group in self.param_groups:

            lr = group['lr']
            momentum = group['momentum']
            nesterov = group['nesterov']
            ns_steps = group['ns_steps']
            update_buffers = group['update_buffer']
            # generate weight updates in distributed fashion
            params = group['params']
            handle = None
            params_world = None
            def update_prev():
                if params_world is None:
                    return
                assert handle is not None
                handle.wait()
                for p_world, g_world in zip(params_world, update_buffers):
                    p_world.data.add_(
                        g_world.view_as(p_world),
                        alpha=-lr * max(1, p_world.size(0) / p_world.size(1)) ** 0.5,
                    )
            for base_i in range(len(params))[::self.world_size]:
                if base_i + rank < len(params):
                    p = params[base_i + self.rank]
                    g = p.grad
                    assert g is not None
                    state = self.state[p]
                    if 'momentum_buffer' not in state:
                        state['momentum_buffer'] = torch.zeros_like(g)
                    buf = state['momentum_buffer']
                    buf.lerp_(g, 1 - momentum)
                    g = g.lerp_(buf, momentum) if nesterov else buf
                    g = zeropower_via_newtonschulz5(g, steps=ns_steps).flatten()
                else:
                    g = update_buffers[rank]
                update_prev() # async all_gather instead of sync all_reduce by @YouJiacheng
                handle = dist.all_gather(update_buffers, g, async_op=True)
                params_world = params[base_i : base_i + self.world_size]
            update_prev()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the GPT-2 model

def norm(x):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):

    def __init__(self, in_features, out_features):
        super().__init__(in_features, out_features, bias=False)

    def forward(self, x):
        return F.linear(x, self.weight.type_as(x))

class Rotary(nn.Module):

    def __init__(self, dim, max_seq_len=65536):
        super().__init__()
        # half-truncate RoPE by @YouJiacheng
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=dim//4, dtype=torch.float32)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(dim//4)])
        t = torch.arange(max_seq_len, dtype=torch.float32)
        theta = torch.einsum('i,j -> ij', t, angular_freq)
        self.cos = nn.Buffer(theta.cos(), persistent=False)
        self.sin = nn.Buffer(theta.sin(), persistent=False)

    def forward(self, x):
        cos, sin = self.cos[None, :x.size(-3), None, :], self.sin[None, :x.size(-3), None, :]
        x1, x2 = x.float().chunk(2, dim=-1)
        y1 = x1 * cos + x2 * sin
        y2 = x1 * (-sin) + x2 * cos
        return torch.cat((y1, y2), 3).type_as(x)

class CausalSelfAttention(nn.Module):

    def __init__(self, dim, num_heads, layer_id):
        super().__init__()
        assert dim % num_heads == 0
        self.num_heads = num_heads
        self.layer_id = layer_id
        self.c_q = CastedLinear(dim, dim)
        self.c_k = CastedLinear(dim, dim)
        self.c_v = CastedLinear(dim, dim)
        self.lambdas = nn.Parameter(torch.tensor([0.5, 0.5]))
        self.rotary = Rotary(dim // num_heads) # dim // num_heads = head_dim
        self.c_proj = CastedLinear(dim, dim)
        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977
        # self.attn_scale = nn.Parameter(torch.tensor(1.0 / (dim // num_heads) ** 0.5))
        # self.attn_scale = nn.Parameter(torch.tensor(0.20))
        self.attn_scale = 0.13 + 0.01 * min(layer_id, 11 - layer_id)

    def forward(self, x, ve, block_mask):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, 'Must use batch size = 1 for FlexAttention'
        q = self.c_q(x).view(B, T, self.num_heads, -1)
        k = self.c_k(x).view(B, T, self.num_heads, -1)
        v = self.c_v(x).view(B, T, self.num_heads, -1)
        if ve is not None:
            v = self.lambdas[0] * v + self.lambdas[1] * ve.view_as(v) # @KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = self.lambdas[0] * v
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = self.rotary(q), self.rotary(k)
        # y = flex_attention(q.transpose(1, 2) * self.attn_scale, k.transpose(1, 2), v.transpose(1, 2), block_mask=block_mask, scale=1.)
        y = flex_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), block_mask=block_mask, scale=self.attn_scale)
        y = y.transpose(1, 2).contiguous().view_as(x) # re-assemble all head outputs side by side
        y = self.c_proj(y)
        return y

class MLP(nn.Module):

    def __init__(self, dim):
        super().__init__()
        self.c_fc = CastedLinear(dim, 4 * dim)
        self.c_proj = CastedLinear(4 * dim, dim)
        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977

    def forward(self, x):
        x = self.c_fc(x)
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = self.c_proj(x)
        return x

class Block(nn.Module):

    def __init__(self, model_dim, num_heads, layer_id, use_attn=True):
        super().__init__()
        self.attn = CausalSelfAttention(model_dim, num_heads, layer_id) if use_attn else None
        self.mlp = MLP(model_dim)
        self.lambdas = nn.Parameter(torch.tensor([1., 0.]))

    def forward(self, x, ve, x0, block_mask):
        x = self.lambdas[0] * x + self.lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), ve, block_mask)
        x = x + self.mlp(norm(x))
        return x

class ValueEmbedding(nn.Module):
    def __init__(self, vocab_size, model_dim):
        super().__init__()
        self.embed = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])

    def forward(self, inputs):
        ve = [emb(inputs).bfloat16() for emb in self.embed]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        ve = [ve[0], ve[1], ve[2], None, None, None, None, None, None, ve[0], ve[1], ve[2]]
        return ve

# -----------------------------------------------------------------------------
# The main GPT-2 model

class GPT(nn.Module):

    def __init__(self, vocab_size, num_layers, num_heads, model_dim):
        super().__init__()
        self.embed = nn.Embedding(vocab_size, model_dim)
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.blocks = nn.ModuleList([Block(model_dim, num_heads, layer_id=layer_id, use_attn=(layer_id != 7))
                                     for layer_id in range(num_layers)])
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual learning
        # U-net structure on token value embeddings by @leloykun
        self.value_embeds = ValueEmbedding(vocab_size, model_dim)
        self.lm_head = CastedLinear(model_dim, vocab_size)
        self.lm_head.weight.data.zero_() # @Grad62304977
        # U-net design by @brendanh0gan
        self.num_encoder_layers = num_layers // 2 # Half of the layers for encoder
        self.num_decoder_layers = num_layers - self.num_encoder_layers # Remaining for decoder
        # Add learnable skip connection weights for decoder layers
        self.skip_weights = nn.Parameter(torch.ones(self.num_decoder_layers))

    def forward(self, inputs, targets, sliding_window_num_blocks):
        BLOCK_SIZE = 128
        seq_len = len(inputs)
        assert seq_len % BLOCK_SIZE == 0
        total_num_blocks = seq_len // BLOCK_SIZE
        assert inputs.ndim == 1
        docs = (inputs == 50256).cumsum(0)
        docs_low = docs.view(-1, BLOCK_SIZE)[:, 0].contiguous()
        docs_high = docs.view(-1, BLOCK_SIZE)[:, -1].contiguous()

        def document_causal(b, h, q_idx, kv_idx):
            causal_mask = q_idx >= kv_idx
            document_mask = docs[q_idx] == docs[kv_idx]
            return causal_mask & document_mask

        def dense_to_ordered(dense_mask):
            num_blocks = dense_mask.sum(dim=-1, dtype=torch.int32)
            indices = dense_mask.argsort(dim=-1, descending=True, stable=True).to(torch.int32)
            return num_blocks[None, None].contiguous(), indices[None, None].contiguous()

        def create_doc_swc_block_mask(sliding_window_num_blocks):
            kv_idx = block_idx = torch.arange(total_num_blocks, dtype=torch.int32, device='cuda')
            q_idx = block_idx[:, None]
            causal_bm = q_idx >= kv_idx
            causal_full_bm = q_idx > kv_idx
            window_bm = q_idx - kv_idx < sliding_window_num_blocks
            window_full_bm = window_bm # block-wise sliding window by @YouJiacheng
            # document_bm = (docs_low[q_idx] <= docs_high[kv_idx]) & (docs_low[kv_idx] <= docs_high[q_idx])
            document_bm = (docs_low[:, None] <= docs_high) & (docs_low <= docs_high[:, None])
            document_full_bm = (docs_low[:, None] == docs_high) & (docs_low == docs_high[:, None])
            nonzero_bm = causal_bm & window_bm & document_bm
            full_bm  = causal_full_bm & window_full_bm & document_full_bm
            kv_num_blocks, kv_indices = dense_to_ordered(nonzero_bm & ~full_bm)
            full_kv_num_blocks, full_kv_indices = dense_to_ordered(full_bm)
            return BlockMask.from_kv_blocks(
                kv_num_blocks,
                kv_indices,
                full_kv_num_blocks,
                full_kv_indices,
                BLOCK_SIZE=BLOCK_SIZE,
                mask_mod=document_causal,
            )

        block_mask = create_doc_swc_block_mask(sliding_window_num_blocks)

        x0 = norm(self.embed(inputs[None]).bfloat16()) # use of norm here by @Grad62304977
        x = x0
        ve = self.value_embeds(inputs)
        assert len(ve) == len(self.blocks)
        ve_enc, ve_dec = ve[:self.num_encoder_layers], ve[self.num_encoder_layers:]

        # Store outputs for U-Net skip connections
        skip_connections = []
        # Encoder pass - process only the first half of the blocks
        for i in range(self.num_encoder_layers):
            x = self.blocks[i](x, ve_enc[i], x0, block_mask)
            skip_connections.append(x)
        # Decoder pass - process the remaining blocks with weighted skip connections
        for i in range(self.num_decoder_layers):
            x = x + self.skip_weights[i] * skip_connections.pop()
            # U-net structure on token value embeddings by @leloykun
            x = self.blocks[self.num_encoder_layers + i](x, ve_dec[i], x0, block_mask)

        x = norm(x)
        logits = self.lm_head(x)
        logits = 15 * torch.tanh(logits / 15) # @Grad62304977 added tanh softcapping, @KoszarskyB reduced it from 30 to 15
        logits = logits.float()
        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets)
        return loss

# -----------------------------------------------------------------------------
# Our own simple Distributed Data Loader

def _load_data_shard(path):
    # only reads the header, returns header data
    # header is 256 int32
    header = torch.from_file(path, False, 256, dtype=torch.int32)
    assert header[0] == 20240520, 'magic number mismatch in the data .bin file'
    assert header[1] == 1, 'unsupported version'
    num_tokens = int(header[2]) # number of tokens (claimed)
    with open(path, 'rb', buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, 'number of tokens read does not match header'
    return tokens

class DistributedDataLoader:

    def __init__(self, filename_pattern):
        self.rank = int(os.environ['RANK'])
        self.world_size = int(os.environ['WORLD_SIZE'])
        self.files = sorted(glob.glob(filename_pattern))
        self.reset()

    def reset(self):
        self.current_shard = -1
        self.advance()

    def advance(self):
        self.current_shard = (self.current_shard + 1) % len(self.files)
        self.current_position = 0
        self.tokens = _load_data_shard(self.files[self.current_shard])

    def next_batch(self, batch_size):
        assert batch_size % self.world_size == 0
        device_batch_size = batch_size // self.world_size
        # load next shard if necessary
        if self.current_position + batch_size + 1 >= len(self.tokens):
            self.advance()
        pos = self.current_position + self.rank * device_batch_size
        device_batch_tokens = self.tokens[pos:pos+device_batch_size+1]
        # advance current position
        self.current_position += batch_size
        inputs = device_batch_tokens[:-1].to(device='cuda', dtype=torch.int32, non_blocking=True)
        targets = device_batch_tokens[1:].to(device='cuda', dtype=torch.int64, non_blocking=True)
        return inputs, targets

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files = 'data/fineweb10B/fineweb_train_*.bin' # input .bin to train on
    val_files = 'data/fineweb10B/fineweb_val_*.bin' # input .bin to eval validation loss on
    val_tokens = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # optimization
    batch_size = 8*64*1024 # batch size in tokens
    num_iterations = 1375 # number of iterations to run
    cooldown_frac = 0.4 # fraction of training spent cooling down the learning rate
    bf16_embeds = True
    # evaluation and logging
    val_loss_every = 125 # every how many steps to evaluate val loss? 0 for only at the end
    # implementation
    max_device_batch_size = 64*1024 # batch size per device in tokens
    save_checkpoint = False
args = Hyperparameters()

micro_bs = args.max_device_batch_size

# set up DDP (distributed data parallel). torchrun sets this env variable
rank = int(os.environ['RANK'])
local_rank = int(os.environ['LOCAL_RANK'])
world_size = int(os.environ['WORLD_SIZE'])
assert torch.cuda.is_available()
torch.cuda.set_device(local_rank)
dist.init_process_group(backend='nccl', device_id=torch.device(local_rank))
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = uuid.uuid4()
    os.makedirs('logs', exist_ok=True)
    logfile = f'logs/{run_id}.txt'
    print(logfile)

def print0(s, console=False):
    if master_process:
        with open(logfile, 'a') as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0('='*100)
# log information about the hardware/software environment this is running on
print0(f'Running Python {sys.version}')
print0(f'Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}')
print0(subprocess.run(['nvidia-smi'], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout)
print0('='*100)

# load data
train_loader = DistributedDataLoader(args.train_files)
val_loader = DistributedDataLoader(args.val_files)
print0(f'Training dataloader files: {train_loader.files}')
print0(f'Validation dataloader files: {val_loader.files}')
print0('='*100)

# there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency. suggested to me by @Grad62304977.
# this originates from Karpathy's experiments.
model = GPT(vocab_size=50304, num_layers=12, num_heads=6, model_dim=768)
model = model.cuda()
if args.bf16_embeds:
    for m in model.modules():
        if isinstance(m, nn.Embedding):
            m.bfloat16()
model: GPT = torch.compile(model)
ddp_model = DDP(model, device_ids=[local_rank], broadcast_buffers=False, gradient_as_bucket_view=True)

# collect the parameters to optimize
hidden_matrix_params = [p for p in model.blocks.parameters() if p.ndim == 2]
embed_params = [model.embed.weight, *model.value_embeds.parameters()]
scalar_params = [p for n, p in model.named_parameters() if p.ndim < 2 and "attn_scale" not in n]
attn_scale_params = [p for n, p in model.named_parameters() if p.ndim < 2 and "attn_scale" in n]
head_params = [model.lm_head.weight]

# init the optimizer(s)
optimizer1 = torch.optim.Adam([dict(params=embed_params, lr=0.6),
                               dict(params=head_params, lr=0.008),
                               dict(params=scalar_params, lr=0.04),
                               dict(params=attn_scale_params, lr=0.01)],
                              betas=(0.8, 0.95), fused=True)
optimizer2 = Muon(hidden_matrix_params, lr=0.05, momentum=0.95)
optimizers = [optimizer1, optimizer2]

# learning rate schedule: stable then decay
def get_lr(it):
    t = 1 - it / args.num_iterations # time remaining in training
    assert 1 >= t > 0
    # 1) constant lr for first part of training
    if t >= args.cooldown_frac:
        return 1.0
    # 2) then linear cooldown
    else:
        return t / args.cooldown_frac
schedulers = [torch.optim.lr_scheduler.LambdaLR(opt, get_lr) for opt in optimizers]

# sliding window size schedule: linear increase over training in chunks of 128 from 128 -> 1792. By @fernbear.bsky.social
def get_sliding_window_blocks(it):
    x = it / args.num_iterations # training progress
    assert 0 <= x <= 1
    return int(((1 - x) * 128 + x * 1856) // 128)
sliding_window_num_blocks = torch.tensor(1, dtype=torch.int32, device='cuda')

# Start training loop
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    # This effectively ignores timing first 10 steps, which are slower for weird reasons.
    # Alternately, and slightly more correctly in terms of benchmarking, we could do 10
    # steps with dummy data first, and then re-initialize the model and reset the loader.
    if step == 10:
        training_time_ms = 0
        t0 = time.perf_counter()
    timed_steps = float('nan') if step <= 11 else (step - 10) + 1 # <= 11 to avoid bug in val

    sliding_window_num_blocks.copy_(get_sliding_window_blocks(step))

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        # run validation batches
        model.eval()
        val_loader.reset()
        val_loss = 0.0
        # calculate the number of steps to take in the val loop.
        val_batch_size = world_size * micro_bs
        assert args.val_tokens % val_batch_size == 0
        val_steps = args.val_tokens // val_batch_size
        for _ in range(val_steps):
            with torch.no_grad():
                inputs_val, targets_val = val_loader.next_batch(val_batch_size)
                val_loss += ddp_model(inputs_val, targets_val, sliding_window_num_blocks)
        # Print attention scales
        for n, p in model.named_parameters():
            if p.ndim < 2 and "attn_scale" in n:
                print0(f'{n}: {p.item()}', console=True)
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        val_loss /= val_steps
        # logging
        print0(f'step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/(timed_steps-1):.2f}ms', console=True)
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f'logs/{run_id}', exist_ok=True)
            torch.save(log, f'logs/{run_id}/state_step{step:06d}.pt')
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    model.train()
    batch_size = args.batch_size
    assert batch_size % world_size == 0
    inputs_train, targets_train = train_loader.next_batch(batch_size)
    assert len(inputs_train) <= micro_bs or len(inputs_train) % micro_bs == 0
    for micro_inputs_train, micro_targets_train in zip(inputs_train.split(micro_bs), targets_train.split(micro_bs)):
        ddp_model(micro_inputs_train, micro_targets_train, sliding_window_num_blocks).backward()
    # momentum warmup for Muon
    frac = min(step/300, 1)
    for group in optimizer2.param_groups:
        group['momentum'] = (1 - frac) * 0.85 + frac * 0.95
    # step the optimizers and schedulers
    for opt, sched in zip(optimizers, schedulers):
        opt.step()
        if step != train_steps-1:
            sched.step()
    # null the gradients
    model.zero_grad(set_to_none=True)
    # logging
    approx_time = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f'step:{step+1}/{train_steps} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms', console=True)

print0(f'peak memory consumption: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB')
dist.destroy_process_group()

====================================================================================================
Running Python 3.10.12 (main, Sep 11 2024, 15:47:36) [GCC 11.4.0]
Running PyTorch 2.6.0.dev20241231+cu126 compiled for CUDA 12.6
Tue Jan 14 15:05:32 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.183.06             Driver Version: 535.183.06   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:19:00.0 Off |                    0 |
| N/A   28C    P0             117W / 700W |   7713MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:3B:00.0 Off |                    0 |
| N/A   28C    P0             112W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:4C:00.0 Off |                    0 |
| N/A   24C    P0             115W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   27C    P0             120W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:9B:00.0 Off |                    0 |
| N/A   28C    P0             117W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:BB:00.0 Off |                    0 |
| N/A   26C    P0             112W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:CB:00.0 Off |                    0 |
| N/A   26C    P0             108W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:DB:00.0 Off |                    0 |
| N/A   25C    P0             114W / 700W |   3211MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
Training dataloader files: ['data/fineweb10B/fineweb_train_000001.bin', 'data/fineweb10B/fineweb_train_000002.bin', 'data/fineweb10B/fineweb_train_000003.bin', 'data/fineweb10B/fineweb_train_000004.bin', 'data/fineweb10B/fineweb_train_000005.bin', 'data/fineweb10B/fineweb_train_000006.bin', 'data/fineweb10B/fineweb_train_000007.bin', 'data/fineweb10B/fineweb_train_000008.bin', 'data/fineweb10B/fineweb_train_000009.bin']
Validation dataloader files: ['data/fineweb10B/fineweb_val_000000.bin']
====================================================================================================
step:0/1375 val_loss:10.8258 train_time:0ms step_avg:nanms
step:1/1375 train_time:27455ms step_avg:nanms
step:2/1375 train_time:27546ms step_avg:nanms
step:3/1375 train_time:27733ms step_avg:nanms
step:4/1375 train_time:27866ms step_avg:nanms
step:5/1375 train_time:27999ms step_avg:nanms
step:6/1375 train_time:28132ms step_avg:nanms
step:7/1375 train_time:28265ms step_avg:nanms
step:8/1375 train_time:28398ms step_avg:nanms
step:9/1375 train_time:28531ms step_avg:nanms
step:10/1375 train_time:28671ms step_avg:nanms
step:11/1375 train_time:135ms step_avg:nanms
step:12/1375 train_time:272ms step_avg:nanms
step:13/1375 train_time:407ms step_avg:135.76ms
step:14/1375 train_time:541ms step_avg:135.30ms
step:15/1375 train_time:674ms step_avg:134.81ms
step:16/1375 train_time:809ms step_avg:134.88ms
step:17/1375 train_time:943ms step_avg:134.77ms
step:18/1375 train_time:1079ms step_avg:134.88ms
step:19/1375 train_time:1216ms step_avg:135.06ms
step:20/1375 train_time:1355ms step_avg:135.49ms
step:21/1375 train_time:1490ms step_avg:135.42ms
step:22/1375 train_time:1625ms step_avg:135.41ms
step:23/1375 train_time:1760ms step_avg:135.35ms
step:24/1375 train_time:1895ms step_avg:135.35ms
step:25/1375 train_time:2029ms step_avg:135.29ms
step:26/1375 train_time:2164ms step_avg:135.23ms
step:27/1375 train_time:2299ms step_avg:135.22ms
step:28/1375 train_time:2436ms step_avg:135.33ms
step:29/1375 train_time:2573ms step_avg:135.40ms
step:30/1375 train_time:2709ms step_avg:135.43ms
step:31/1375 train_time:2842ms step_avg:135.34ms
step:32/1375 train_time:2976ms step_avg:135.29ms
step:33/1375 train_time:3113ms step_avg:135.33ms
step:34/1375 train_time:3248ms step_avg:135.31ms
step:35/1375 train_time:3384ms step_avg:135.36ms
step:36/1375 train_time:3518ms step_avg:135.31ms
step:37/1375 train_time:3655ms step_avg:135.38ms
step:38/1375 train_time:3788ms step_avg:135.28ms
step:39/1375 train_time:3923ms step_avg:135.28ms
step:40/1375 train_time:4058ms step_avg:135.27ms
step:41/1375 train_time:4194ms step_avg:135.29ms
step:42/1375 train_time:4330ms step_avg:135.31ms
step:43/1375 train_time:4465ms step_avg:135.30ms
step:44/1375 train_time:4599ms step_avg:135.27ms
step:45/1375 train_time:4734ms step_avg:135.27ms
step:46/1375 train_time:4869ms step_avg:135.25ms
step:47/1375 train_time:5004ms step_avg:135.24ms
step:48/1375 train_time:5138ms step_avg:135.21ms
step:49/1375 train_time:5274ms step_avg:135.23ms
step:50/1375 train_time:5410ms step_avg:135.24ms
step:51/1375 train_time:5544ms step_avg:135.23ms
step:52/1375 train_time:5679ms step_avg:135.22ms
step:53/1375 train_time:5815ms step_avg:135.24ms
step:54/1375 train_time:5951ms step_avg:135.25ms
step:55/1375 train_time:6085ms step_avg:135.23ms
step:56/1375 train_time:6219ms step_avg:135.20ms
step:57/1375 train_time:6356ms step_avg:135.22ms
step:58/1375 train_time:6490ms step_avg:135.20ms
step:59/1375 train_time:6624ms step_avg:135.18ms
step:60/1375 train_time:6759ms step_avg:135.18ms
step:61/1375 train_time:6895ms step_avg:135.19ms
step:62/1375 train_time:7031ms step_avg:135.20ms
step:63/1375 train_time:7164ms step_avg:135.18ms
step:64/1375 train_time:7300ms step_avg:135.18ms
step:65/1375 train_time:7435ms step_avg:135.19ms
step:66/1375 train_time:7570ms step_avg:135.19ms
step:67/1375 train_time:7705ms step_avg:135.18ms
step:68/1375 train_time:7841ms step_avg:135.19ms
step:69/1375 train_time:7975ms step_avg:135.18ms
step:70/1375 train_time:8110ms step_avg:135.17ms
step:71/1375 train_time:8247ms step_avg:135.19ms
step:72/1375 train_time:8380ms step_avg:135.17ms
step:73/1375 train_time:8516ms step_avg:135.18ms
step:74/1375 train_time:8653ms step_avg:135.20ms
step:75/1375 train_time:8786ms step_avg:135.17ms
step:76/1375 train_time:8920ms step_avg:135.15ms
step:77/1375 train_time:9055ms step_avg:135.15ms
step:78/1375 train_time:9191ms step_avg:135.16ms
step:79/1375 train_time:9326ms step_avg:135.17ms
step:80/1375 train_time:9461ms step_avg:135.16ms
step:81/1375 train_time:9597ms step_avg:135.17ms
step:82/1375 train_time:9733ms step_avg:135.18ms
step:83/1375 train_time:9867ms step_avg:135.16ms
step:84/1375 train_time:10001ms step_avg:135.15ms
step:85/1375 train_time:10136ms step_avg:135.15ms
step:86/1375 train_time:10272ms step_avg:135.16ms
step:87/1375 train_time:10408ms step_avg:135.17ms
step:88/1375 train_time:10541ms step_avg:135.15ms
step:89/1375 train_time:10677ms step_avg:135.15ms
step:90/1375 train_time:10812ms step_avg:135.16ms
step:91/1375 train_time:10947ms step_avg:135.15ms
step:92/1375 train_time:11081ms step_avg:135.14ms
step:93/1375 train_time:11217ms step_avg:135.14ms
step:94/1375 train_time:11352ms step_avg:135.14ms
step:95/1375 train_time:11485ms step_avg:135.12ms
step:96/1375 train_time:11619ms step_avg:135.11ms
step:97/1375 train_time:11755ms step_avg:135.12ms
step:98/1375 train_time:11891ms step_avg:135.13ms
step:99/1375 train_time:12025ms step_avg:135.11ms
step:100/1375 train_time:12160ms step_avg:135.11ms
step:101/1375 train_time:12296ms step_avg:135.12ms
step:102/1375 train_time:12431ms step_avg:135.12ms
step:103/1375 train_time:12567ms step_avg:135.13ms
step:104/1375 train_time:12705ms step_avg:135.16ms
step:105/1375 train_time:12843ms step_avg:135.19ms
step:106/1375 train_time:12980ms step_avg:135.21ms
step:107/1375 train_time:13119ms step_avg:135.24ms
step:108/1375 train_time:13260ms step_avg:135.30ms
step:109/1375 train_time:13401ms step_avg:135.37ms
step:110/1375 train_time:13541ms step_avg:135.41ms
step:111/1375 train_time:13679ms step_avg:135.43ms
step:112/1375 train_time:13817ms step_avg:135.46ms
step:113/1375 train_time:13956ms step_avg:135.50ms
step:114/1375 train_time:14094ms step_avg:135.52ms
step:115/1375 train_time:14234ms step_avg:135.56ms
step:116/1375 train_time:14371ms step_avg:135.58ms
step:117/1375 train_time:14509ms step_avg:135.60ms
step:118/1375 train_time:14647ms step_avg:135.62ms
step:119/1375 train_time:14786ms step_avg:135.65ms
step:120/1375 train_time:14924ms step_avg:135.67ms
step:121/1375 train_time:15062ms step_avg:135.69ms
step:122/1375 train_time:15200ms step_avg:135.72ms
step:123/1375 train_time:15339ms step_avg:135.75ms
step:124/1375 train_time:15478ms step_avg:135.78ms
step:125/1375 train_time:15617ms step_avg:135.80ms
step:125/1375 val_loss:4.3730 train_time:15683ms step_avg:136.38ms
step:126/1375 train_time:15757ms step_avg:135.84ms
step:127/1375 train_time:15897ms step_avg:135.87ms
step:128/1375 train_time:16035ms step_avg:135.89ms
step:129/1375 train_time:16171ms step_avg:135.89ms
step:130/1375 train_time:16308ms step_avg:135.90ms
step:131/1375 train_time:16446ms step_avg:135.92ms
step:132/1375 train_time:16585ms step_avg:135.94ms
step:133/1375 train_time:16726ms step_avg:135.99ms
step:134/1375 train_time:16868ms step_avg:136.03ms
step:135/1375 train_time:17007ms step_avg:136.05ms
step:136/1375 train_time:17143ms step_avg:136.06ms
step:137/1375 train_time:17281ms step_avg:136.07ms
step:138/1375 train_time:17419ms step_avg:136.08ms
step:139/1375 train_time:17554ms step_avg:136.08ms
step:140/1375 train_time:17692ms step_avg:136.09ms
step:141/1375 train_time:17832ms step_avg:136.12ms
step:142/1375 train_time:17971ms step_avg:136.14ms
step:143/1375 train_time:18110ms step_avg:136.17ms
step:144/1375 train_time:18249ms step_avg:136.19ms
step:145/1375 train_time:18388ms step_avg:136.21ms
step:146/1375 train_time:18527ms step_avg:136.23ms
step:147/1375 train_time:18666ms step_avg:136.25ms
step:148/1375 train_time:18804ms step_avg:136.26ms
step:149/1375 train_time:18942ms step_avg:136.28ms
step:150/1375 train_time:19080ms step_avg:136.29ms
step:151/1375 train_time:19218ms step_avg:136.30ms
step:152/1375 train_time:19357ms step_avg:136.31ms
step:153/1375 train_time:19494ms step_avg:136.32ms
step:154/1375 train_time:19633ms step_avg:136.34ms
step:155/1375 train_time:19771ms step_avg:136.36ms
step:156/1375 train_time:19912ms step_avg:136.38ms
step:157/1375 train_time:20051ms step_avg:136.40ms
step:158/1375 train_time:20192ms step_avg:136.43ms
step:159/1375 train_time:20331ms step_avg:136.45ms
step:160/1375 train_time:20470ms step_avg:136.46ms
step:161/1375 train_time:20609ms step_avg:136.48ms
step:162/1375 train_time:20748ms step_avg:136.50ms
step:163/1375 train_time:20887ms step_avg:136.51ms
step:164/1375 train_time:21026ms step_avg:136.53ms
step:165/1375 train_time:21165ms step_avg:136.55ms
step:166/1375 train_time:21304ms step_avg:136.57ms
step:167/1375 train_time:21443ms step_avg:136.58ms
step:168/1375 train_time:21582ms step_avg:136.60ms
step:169/1375 train_time:21720ms step_avg:136.60ms
step:170/1375 train_time:21859ms step_avg:136.62ms
step:171/1375 train_time:21998ms step_avg:136.63ms
step:172/1375 train_time:22137ms step_avg:136.65ms
step:173/1375 train_time:22276ms step_avg:136.66ms
step:174/1375 train_time:22416ms step_avg:136.68ms
step:175/1375 train_time:22554ms step_avg:136.69ms
step:176/1375 train_time:22693ms step_avg:136.70ms
step:177/1375 train_time:22831ms step_avg:136.71ms
step:178/1375 train_time:22972ms step_avg:136.74ms
step:179/1375 train_time:23112ms step_avg:136.75ms
step:180/1375 train_time:23250ms step_avg:136.76ms
step:181/1375 train_time:23388ms step_avg:136.77ms
step:182/1375 train_time:23526ms step_avg:136.78ms
step:183/1375 train_time:23665ms step_avg:136.79ms
step:184/1375 train_time:23804ms step_avg:136.80ms
step:185/1375 train_time:23943ms step_avg:136.81ms
step:186/1375 train_time:24081ms step_avg:136.82ms
step:187/1375 train_time:24219ms step_avg:136.83ms
step:188/1375 train_time:24356ms step_avg:136.83ms
step:189/1375 train_time:24496ms step_avg:136.85ms
step:190/1375 train_time:24633ms step_avg:136.85ms
step:191/1375 train_time:24816ms step_avg:137.11ms
step:192/1375 train_time:24953ms step_avg:137.10ms
step:193/1375 train_time:25090ms step_avg:137.10ms
step:194/1375 train_time:25227ms step_avg:137.10ms
step:195/1375 train_time:25365ms step_avg:137.11ms
step:196/1375 train_time:25504ms step_avg:137.12ms
step:197/1375 train_time:25643ms step_avg:137.13ms
step:198/1375 train_time:25785ms step_avg:137.15ms
step:199/1375 train_time:25926ms step_avg:137.17ms
step:200/1375 train_time:26066ms step_avg:137.19ms
step:201/1375 train_time:26204ms step_avg:137.19ms
step:202/1375 train_time:26342ms step_avg:137.20ms
step:203/1375 train_time:26479ms step_avg:137.20ms
step:204/1375 train_time:26617ms step_avg:137.20ms
step:205/1375 train_time:26758ms step_avg:137.22ms
step:206/1375 train_time:26900ms step_avg:137.25ms
step:207/1375 train_time:27043ms step_avg:137.28ms
step:208/1375 train_time:27185ms step_avg:137.30ms
step:209/1375 train_time:27326ms step_avg:137.31ms
step:210/1375 train_time:27467ms step_avg:137.33ms
step:211/1375 train_time:27607ms step_avg:137.35ms
step:212/1375 train_time:27748ms step_avg:137.37ms
step:213/1375 train_time:27891ms step_avg:137.39ms
step:214/1375 train_time:28032ms step_avg:137.41ms
step:215/1375 train_time:28175ms step_avg:137.44ms
step:216/1375 train_time:28317ms step_avg:137.46ms
step:217/1375 train_time:28458ms step_avg:137.48ms
step:218/1375 train_time:28599ms step_avg:137.50ms
step:219/1375 train_time:28740ms step_avg:137.51ms
step:220/1375 train_time:28881ms step_avg:137.53ms
step:221/1375 train_time:29023ms step_avg:137.55ms
step:222/1375 train_time:29166ms step_avg:137.58ms
step:223/1375 train_time:29309ms step_avg:137.60ms
step:224/1375 train_time:29450ms step_avg:137.62ms
step:225/1375 train_time:29592ms step_avg:137.64ms
step:226/1375 train_time:29733ms step_avg:137.66ms
step:227/1375 train_time:29874ms step_avg:137.67ms
step:228/1375 train_time:30018ms step_avg:137.69ms
step:229/1375 train_time:30160ms step_avg:137.72ms
step:230/1375 train_time:30300ms step_avg:137.73ms
step:231/1375 train_time:30443ms step_avg:137.75ms
step:232/1375 train_time:30584ms step_avg:137.77ms
step:233/1375 train_time:30725ms step_avg:137.78ms
step:234/1375 train_time:30867ms step_avg:137.80ms
step:235/1375 train_time:31009ms step_avg:137.82ms
step:236/1375 train_time:31150ms step_avg:137.83ms
step:237/1375 train_time:31292ms step_avg:137.85ms
step:238/1375 train_time:31434ms step_avg:137.87ms
step:239/1375 train_time:31575ms step_avg:137.88ms
step:240/1375 train_time:31715ms step_avg:137.89ms
step:241/1375 train_time:31860ms step_avg:137.92ms
step:242/1375 train_time:32001ms step_avg:137.93ms
step:243/1375 train_time:32143ms step_avg:137.95ms
step:244/1375 train_time:32285ms step_avg:137.97ms
step:245/1375 train_time:32427ms step_avg:137.99ms
step:246/1375 train_time:32569ms step_avg:138.01ms
step:247/1375 train_time:32711ms step_avg:138.02ms
step:248/1375 train_time:32853ms step_avg:138.04ms
step:249/1375 train_time:32996ms step_avg:138.06ms
step:250/1375 train_time:33138ms step_avg:138.07ms
step:250/1375 val_loss:3.9626 train_time:33207ms step_avg:138.36ms
step:251/1375 train_time:33284ms step_avg:138.11ms
step:252/1375 train_time:33428ms step_avg:138.13ms
step:253/1375 train_time:33570ms step_avg:138.15ms
step:254/1375 train_time:33710ms step_avg:138.16ms
step:255/1375 train_time:33851ms step_avg:138.17ms
step:256/1375 train_time:33991ms step_avg:138.18ms
step:257/1375 train_time:34132ms step_avg:138.19ms
step:258/1375 train_time:34274ms step_avg:138.20ms
step:259/1375 train_time:34418ms step_avg:138.22ms
step:260/1375 train_time:34561ms step_avg:138.24ms
step:261/1375 train_time:34702ms step_avg:138.25ms
step:262/1375 train_time:34845ms step_avg:138.27ms
step:263/1375 train_time:34987ms step_avg:138.29ms
step:264/1375 train_time:35128ms step_avg:138.30ms
step:265/1375 train_time:35269ms step_avg:138.31ms
step:266/1375 train_time:35410ms step_avg:138.32ms
step:267/1375 train_time:35552ms step_avg:138.33ms
step:268/1375 train_time:35694ms step_avg:138.35ms
step:269/1375 train_time:35838ms step_avg:138.37ms
step:270/1375 train_time:35982ms step_avg:138.39ms
step:271/1375 train_time:36122ms step_avg:138.40ms
step:272/1375 train_time:36266ms step_avg:138.42ms
step:273/1375 train_time:36407ms step_avg:138.43ms
step:274/1375 train_time:36548ms step_avg:138.44ms
step:275/1375 train_time:36690ms step_avg:138.45ms
step:276/1375 train_time:36831ms step_avg:138.46ms
step:277/1375 train_time:36974ms step_avg:138.48ms
step:278/1375 train_time:37116ms step_avg:138.49ms
step:279/1375 train_time:37258ms step_avg:138.51ms
step:280/1375 train_time:37400ms step_avg:138.52ms
step:281/1375 train_time:37543ms step_avg:138.54ms
step:282/1375 train_time:37684ms step_avg:138.55ms
step:283/1375 train_time:37825ms step_avg:138.55ms
step:284/1375 train_time:37966ms step_avg:138.56ms
step:285/1375 train_time:38107ms step_avg:138.57ms
step:286/1375 train_time:38248ms step_avg:138.58ms
step:287/1375 train_time:38390ms step_avg:138.59ms
step:288/1375 train_time:38531ms step_avg:138.60ms
step:289/1375 train_time:38673ms step_avg:138.61ms
step:290/1375 train_time:38815ms step_avg:138.62ms
step:291/1375 train_time:38956ms step_avg:138.63ms
step:292/1375 train_time:39098ms step_avg:138.64ms
step:293/1375 train_time:39240ms step_avg:138.66ms
step:294/1375 train_time:39383ms step_avg:138.67ms
step:295/1375 train_time:39524ms step_avg:138.68ms
step:296/1375 train_time:39666ms step_avg:138.69ms
step:297/1375 train_time:39808ms step_avg:138.71ms
step:298/1375 train_time:39948ms step_avg:138.71ms
step:299/1375 train_time:40091ms step_avg:138.72ms
step:300/1375 train_time:40232ms step_avg:138.73ms
step:301/1375 train_time:40374ms step_avg:138.74ms
step:302/1375 train_time:40516ms step_avg:138.75ms
step:303/1375 train_time:40657ms step_avg:138.76ms
step:304/1375 train_time:40798ms step_avg:138.77ms
step:305/1375 train_time:40939ms step_avg:138.78ms
step:306/1375 train_time:41081ms step_avg:138.79ms
step:307/1375 train_time:41224ms step_avg:138.80ms
step:308/1375 train_time:41369ms step_avg:138.82ms
step:309/1375 train_time:41512ms step_avg:138.84ms
step:310/1375 train_time:41655ms step_avg:138.85ms
step:311/1375 train_time:41799ms step_avg:138.87ms
step:312/1375 train_time:41943ms step_avg:138.89ms
step:313/1375 train_time:42088ms step_avg:138.90ms
step:314/1375 train_time:42230ms step_avg:138.92ms
step:315/1375 train_time:42374ms step_avg:138.93ms
step:316/1375 train_time:42517ms step_avg:138.94ms
step:317/1375 train_time:42661ms step_avg:138.96ms
step:318/1375 train_time:42805ms step_avg:138.98ms
step:319/1375 train_time:42950ms step_avg:139.00ms
step:320/1375 train_time:43094ms step_avg:139.01ms
step:321/1375 train_time:43237ms step_avg:139.03ms
step:322/1375 train_time:43381ms step_avg:139.04ms
step:323/1375 train_time:43525ms step_avg:139.06ms
step:324/1375 train_time:43668ms step_avg:139.07ms
step:325/1375 train_time:43812ms step_avg:139.09ms
step:326/1375 train_time:43956ms step_avg:139.10ms
step:327/1375 train_time:44100ms step_avg:139.12ms
step:328/1375 train_time:44243ms step_avg:139.13ms
step:329/1375 train_time:44387ms step_avg:139.14ms
step:330/1375 train_time:44530ms step_avg:139.16ms
step:331/1375 train_time:44673ms step_avg:139.17ms
step:332/1375 train_time:44818ms step_avg:139.19ms
step:333/1375 train_time:44961ms step_avg:139.20ms
step:334/1375 train_time:45105ms step_avg:139.21ms
step:335/1375 train_time:45249ms step_avg:139.23ms
step:336/1375 train_time:45392ms step_avg:139.24ms
step:337/1375 train_time:45535ms step_avg:139.25ms
step:338/1375 train_time:45679ms step_avg:139.26ms
step:339/1375 train_time:45822ms step_avg:139.28ms
step:340/1375 train_time:45967ms step_avg:139.29ms
step:341/1375 train_time:46109ms step_avg:139.30ms
step:342/1375 train_time:46254ms step_avg:139.32ms
step:343/1375 train_time:46399ms step_avg:139.34ms
step:344/1375 train_time:46542ms step_avg:139.35ms
step:345/1375 train_time:46686ms step_avg:139.36ms
step:346/1375 train_time:46829ms step_avg:139.37ms
step:347/1375 train_time:46973ms step_avg:139.39ms
step:348/1375 train_time:47116ms step_avg:139.40ms
step:349/1375 train_time:47262ms step_avg:139.42ms
step:350/1375 train_time:47405ms step_avg:139.43ms
step:351/1375 train_time:47548ms step_avg:139.44ms
step:352/1375 train_time:47690ms step_avg:139.45ms
step:353/1375 train_time:47834ms step_avg:139.46ms
step:354/1375 train_time:47978ms step_avg:139.47ms
step:355/1375 train_time:48121ms step_avg:139.48ms
step:356/1375 train_time:48266ms step_avg:139.50ms
step:357/1375 train_time:48410ms step_avg:139.51ms
step:358/1375 train_time:48553ms step_avg:139.52ms
step:359/1375 train_time:48697ms step_avg:139.53ms
step:360/1375 train_time:48844ms step_avg:139.56ms
step:361/1375 train_time:48988ms step_avg:139.57ms
step:362/1375 train_time:49132ms step_avg:139.58ms
step:363/1375 train_time:49277ms step_avg:139.59ms
step:364/1375 train_time:49422ms step_avg:139.61ms
step:365/1375 train_time:49565ms step_avg:139.62ms
step:366/1375 train_time:49710ms step_avg:139.64ms
step:367/1375 train_time:49854ms step_avg:139.65ms
step:368/1375 train_time:49997ms step_avg:139.66ms
step:369/1375 train_time:50141ms step_avg:139.67ms
step:370/1375 train_time:50285ms step_avg:139.68ms
step:371/1375 train_time:50428ms step_avg:139.69ms
step:372/1375 train_time:50571ms step_avg:139.70ms
step:373/1375 train_time:50716ms step_avg:139.71ms
step:374/1375 train_time:50861ms step_avg:139.73ms
step:375/1375 train_time:51004ms step_avg:139.74ms
step:375/1375 val_loss:3.7777 train_time:51073ms step_avg:139.93ms
step:376/1375 train_time:51149ms step_avg:139.75ms
step:377/1375 train_time:51295ms step_avg:139.77ms
step:378/1375 train_time:51439ms step_avg:139.78ms
step:379/1375 train_time:51581ms step_avg:139.79ms
step:380/1375 train_time:51724ms step_avg:139.79ms
step:381/1375 train_time:51912ms step_avg:139.92ms
step:382/1375 train_time:52054ms step_avg:139.93ms
step:383/1375 train_time:52195ms step_avg:139.93ms
step:384/1375 train_time:52337ms step_avg:139.94ms
step:385/1375 train_time:52480ms step_avg:139.95ms
step:386/1375 train_time:52623ms step_avg:139.95ms
step:387/1375 train_time:52767ms step_avg:139.97ms
step:388/1375 train_time:52914ms step_avg:139.98ms
step:389/1375 train_time:53059ms step_avg:140.00ms
step:390/1375 train_time:53203ms step_avg:140.01ms
step:391/1375 train_time:53346ms step_avg:140.02ms
step:392/1375 train_time:53490ms step_avg:140.03ms
step:393/1375 train_time:53633ms step_avg:140.03ms
step:394/1375 train_time:53777ms step_avg:140.05ms
step:395/1375 train_time:53922ms step_avg:140.06ms
step:396/1375 train_time:54066ms step_avg:140.07ms
step:397/1375 train_time:54210ms step_avg:140.08ms
step:398/1375 train_time:54355ms step_avg:140.09ms
step:399/1375 train_time:54500ms step_avg:140.10ms
step:400/1375 train_time:54643ms step_avg:140.11ms
step:401/1375 train_time:54787ms step_avg:140.12ms
step:402/1375 train_time:54930ms step_avg:140.13ms
step:403/1375 train_time:55075ms step_avg:140.14ms
step:404/1375 train_time:55219ms step_avg:140.15ms
step:405/1375 train_time:55363ms step_avg:140.16ms
step:406/1375 train_time:55507ms step_avg:140.17ms
step:407/1375 train_time:55652ms step_avg:140.18ms
step:408/1375 train_time:55795ms step_avg:140.19ms
step:409/1375 train_time:55939ms step_avg:140.20ms
step:410/1375 train_time:56084ms step_avg:140.21ms
step:411/1375 train_time:56230ms step_avg:140.22ms
step:412/1375 train_time:56374ms step_avg:140.23ms
step:413/1375 train_time:56520ms step_avg:140.25ms
step:414/1375 train_time:56666ms step_avg:140.26ms
step:415/1375 train_time:56812ms step_avg:140.28ms
step:416/1375 train_time:56957ms step_avg:140.29ms
step:417/1375 train_time:57103ms step_avg:140.30ms
step:418/1375 train_time:57250ms step_avg:140.32ms
step:419/1375 train_time:57397ms step_avg:140.33ms
step:420/1375 train_time:57542ms step_avg:140.35ms
step:421/1375 train_time:57687ms step_avg:140.36ms
step:422/1375 train_time:57834ms step_avg:140.37ms
step:423/1375 train_time:57979ms step_avg:140.38ms
step:424/1375 train_time:58124ms step_avg:140.40ms
step:425/1375 train_time:58270ms step_avg:140.41ms
step:426/1375 train_time:58416ms step_avg:140.42ms
step:427/1375 train_time:58561ms step_avg:140.43ms
step:428/1375 train_time:58707ms step_avg:140.45ms
step:429/1375 train_time:58854ms step_avg:140.46ms
step:430/1375 train_time:58999ms step_avg:140.47ms
step:431/1375 train_time:59146ms step_avg:140.49ms
step:432/1375 train_time:59292ms step_avg:140.50ms
step:433/1375 train_time:59438ms step_avg:140.51ms
step:434/1375 train_time:59582ms step_avg:140.52ms
step:435/1375 train_time:59728ms step_avg:140.54ms
step:436/1375 train_time:59875ms step_avg:140.55ms
step:437/1375 train_time:60021ms step_avg:140.56ms
step:438/1375 train_time:60166ms step_avg:140.57ms
step:439/1375 train_time:60311ms step_avg:140.58ms
step:440/1375 train_time:60458ms step_avg:140.60ms
step:441/1375 train_time:60603ms step_avg:140.61ms
step:442/1375 train_time:60749ms step_avg:140.62ms
step:443/1375 train_time:60895ms step_avg:140.64ms
step:444/1375 train_time:61041ms step_avg:140.65ms
step:445/1375 train_time:61186ms step_avg:140.66ms
step:446/1375 train_time:61331ms step_avg:140.67ms
step:447/1375 train_time:61476ms step_avg:140.68ms
step:448/1375 train_time:61622ms step_avg:140.69ms
step:449/1375 train_time:61770ms step_avg:140.71ms
step:450/1375 train_time:61915ms step_avg:140.72ms
step:451/1375 train_time:62061ms step_avg:140.73ms
step:452/1375 train_time:62207ms step_avg:140.74ms
step:453/1375 train_time:62352ms step_avg:140.75ms
step:454/1375 train_time:62499ms step_avg:140.76ms
step:455/1375 train_time:62644ms step_avg:140.77ms
step:456/1375 train_time:62791ms step_avg:140.79ms
step:457/1375 train_time:62938ms step_avg:140.80ms
step:458/1375 train_time:63083ms step_avg:140.81ms
step:459/1375 train_time:63229ms step_avg:140.82ms
step:460/1375 train_time:63374ms step_avg:140.83ms
step:461/1375 train_time:63520ms step_avg:140.84ms
step:462/1375 train_time:63667ms step_avg:140.86ms
step:463/1375 train_time:63814ms step_avg:140.87ms
step:464/1375 train_time:63959ms step_avg:140.88ms
step:465/1375 train_time:64105ms step_avg:140.89ms
step:466/1375 train_time:64251ms step_avg:140.90ms
step:467/1375 train_time:64398ms step_avg:140.92ms
step:468/1375 train_time:64542ms step_avg:140.92ms
step:469/1375 train_time:64689ms step_avg:140.93ms
step:470/1375 train_time:64833ms step_avg:140.94ms
step:471/1375 train_time:64980ms step_avg:140.95ms
step:472/1375 train_time:65126ms step_avg:140.96ms
step:473/1375 train_time:65271ms step_avg:140.97ms
step:474/1375 train_time:65418ms step_avg:140.99ms
step:475/1375 train_time:65563ms step_avg:141.00ms
step:476/1375 train_time:65710ms step_avg:141.01ms
step:477/1375 train_time:65855ms step_avg:141.02ms
step:478/1375 train_time:66001ms step_avg:141.03ms
step:479/1375 train_time:66146ms step_avg:141.04ms
step:480/1375 train_time:66293ms step_avg:141.05ms
step:481/1375 train_time:66439ms step_avg:141.06ms
step:482/1375 train_time:66585ms step_avg:141.07ms
step:483/1375 train_time:66730ms step_avg:141.08ms
step:484/1375 train_time:66876ms step_avg:141.09ms
step:485/1375 train_time:67020ms step_avg:141.10ms
step:486/1375 train_time:67167ms step_avg:141.11ms
step:487/1375 train_time:67312ms step_avg:141.12ms
step:488/1375 train_time:67457ms step_avg:141.12ms
step:489/1375 train_time:67603ms step_avg:141.13ms
step:490/1375 train_time:67750ms step_avg:141.15ms
step:491/1375 train_time:67897ms step_avg:141.16ms
step:492/1375 train_time:68043ms step_avg:141.17ms
step:493/1375 train_time:68188ms step_avg:141.18ms
step:494/1375 train_time:68334ms step_avg:141.19ms
step:495/1375 train_time:68480ms step_avg:141.20ms
step:496/1375 train_time:68626ms step_avg:141.20ms
step:497/1375 train_time:68771ms step_avg:141.21ms
step:498/1375 train_time:68918ms step_avg:141.23ms
step:499/1375 train_time:69063ms step_avg:141.23ms
step:500/1375 train_time:69209ms step_avg:141.24ms
step:500/1375 val_loss:3.6576 train_time:69280ms step_avg:141.39ms
step:501/1375 train_time:69356ms step_avg:141.25ms
step:502/1375 train_time:69503ms step_avg:141.27ms
step:503/1375 train_time:69650ms step_avg:141.28ms
step:504/1375 train_time:69795ms step_avg:141.29ms
step:505/1375 train_time:69940ms step_avg:141.29ms
step:506/1375 train_time:70083ms step_avg:141.30ms
step:507/1375 train_time:70228ms step_avg:141.30ms
step:508/1375 train_time:70378ms step_avg:141.32ms
step:509/1375 train_time:70524ms step_avg:141.33ms
step:510/1375 train_time:70670ms step_avg:141.34ms
step:511/1375 train_time:70817ms step_avg:141.35ms
step:512/1375 train_time:70965ms step_avg:141.36ms
step:513/1375 train_time:71112ms step_avg:141.38ms
step:514/1375 train_time:71259ms step_avg:141.39ms
step:515/1375 train_time:71408ms step_avg:141.40ms
step:516/1375 train_time:71556ms step_avg:141.42ms
step:517/1375 train_time:71703ms step_avg:141.43ms
step:518/1375 train_time:71850ms step_avg:141.44ms
step:519/1375 train_time:71997ms step_avg:141.45ms
step:520/1375 train_time:72144ms step_avg:141.46ms
step:521/1375 train_time:72293ms step_avg:141.47ms
step:522/1375 train_time:72439ms step_avg:141.48ms
step:523/1375 train_time:72586ms step_avg:141.49ms
step:524/1375 train_time:72733ms step_avg:141.50ms
step:525/1375 train_time:72879ms step_avg:141.51ms
step:526/1375 train_time:73027ms step_avg:141.53ms
step:527/1375 train_time:73177ms step_avg:141.54ms
step:528/1375 train_time:73323ms step_avg:141.55ms
step:529/1375 train_time:73470ms step_avg:141.56ms
step:530/1375 train_time:73617ms step_avg:141.57ms
step:531/1375 train_time:73764ms step_avg:141.58ms
step:532/1375 train_time:73911ms step_avg:141.59ms
step:533/1375 train_time:74059ms step_avg:141.60ms
step:534/1375 train_time:74206ms step_avg:141.61ms
step:535/1375 train_time:74354ms step_avg:141.63ms
step:536/1375 train_time:74503ms step_avg:141.64ms
step:537/1375 train_time:74650ms step_avg:141.65ms
step:538/1375 train_time:74797ms step_avg:141.66ms
step:539/1375 train_time:74943ms step_avg:141.67ms
step:540/1375 train_time:75090ms step_avg:141.68ms
step:541/1375 train_time:75237ms step_avg:141.69ms
step:542/1375 train_time:75385ms step_avg:141.70ms
step:543/1375 train_time:75533ms step_avg:141.71ms
step:544/1375 train_time:75680ms step_avg:141.72ms
step:545/1375 train_time:75827ms step_avg:141.73ms
step:546/1375 train_time:75974ms step_avg:141.74ms
step:547/1375 train_time:76121ms step_avg:141.75ms
step:548/1375 train_time:76269ms step_avg:141.76ms
step:549/1375 train_time:76418ms step_avg:141.78ms
step:550/1375 train_time:76565ms step_avg:141.79ms
step:551/1375 train_time:76713ms step_avg:141.80ms
step:552/1375 train_time:76860ms step_avg:141.81ms
step:553/1375 train_time:77008ms step_avg:141.82ms
step:554/1375 train_time:77156ms step_avg:141.83ms
step:555/1375 train_time:77303ms step_avg:141.84ms
step:556/1375 train_time:77451ms step_avg:141.85ms
step:557/1375 train_time:77598ms step_avg:141.86ms
step:558/1375 train_time:77745ms step_avg:141.87ms
step:559/1375 train_time:77893ms step_avg:141.88ms
step:560/1375 train_time:78040ms step_avg:141.89ms
step:561/1375 train_time:78186ms step_avg:141.90ms
step:562/1375 train_time:78332ms step_avg:141.91ms
step:563/1375 train_time:78479ms step_avg:141.92ms
step:564/1375 train_time:78626ms step_avg:141.92ms
step:565/1375 train_time:78773ms step_avg:141.93ms
step:566/1375 train_time:78921ms step_avg:141.94ms
step:567/1375 train_time:79070ms step_avg:141.96ms
step:568/1375 train_time:79217ms step_avg:141.97ms
step:569/1375 train_time:79364ms step_avg:141.97ms
step:570/1375 train_time:79510ms step_avg:141.98ms
step:571/1375 train_time:79701ms step_avg:142.07ms
step:572/1375 train_time:79847ms step_avg:142.08ms
step:573/1375 train_time:79994ms step_avg:142.09ms
step:574/1375 train_time:80143ms step_avg:142.10ms
step:575/1375 train_time:80290ms step_avg:142.11ms
step:576/1375 train_time:80436ms step_avg:142.11ms
step:577/1375 train_time:80583ms step_avg:142.12ms
step:578/1375 train_time:80734ms step_avg:142.14ms
step:579/1375 train_time:80881ms step_avg:142.15ms
step:580/1375 train_time:81029ms step_avg:142.16ms
step:581/1375 train_time:81176ms step_avg:142.17ms
step:582/1375 train_time:81323ms step_avg:142.17ms
step:583/1375 train_time:81469ms step_avg:142.18ms
step:584/1375 train_time:81618ms step_avg:142.19ms
step:585/1375 train_time:81765ms step_avg:142.20ms
step:586/1375 train_time:81914ms step_avg:142.21ms
step:587/1375 train_time:82061ms step_avg:142.22ms
step:588/1375 train_time:82209ms step_avg:142.23ms
step:589/1375 train_time:82355ms step_avg:142.24ms
step:590/1375 train_time:82503ms step_avg:142.25ms
step:591/1375 train_time:82651ms step_avg:142.26ms
step:592/1375 train_time:82800ms step_avg:142.27ms
step:593/1375 train_time:82948ms step_avg:142.28ms
step:594/1375 train_time:83097ms step_avg:142.29ms
step:595/1375 train_time:83243ms step_avg:142.30ms
step:596/1375 train_time:83391ms step_avg:142.31ms
step:597/1375 train_time:83538ms step_avg:142.31ms
step:598/1375 train_time:83685ms step_avg:142.32ms
step:599/1375 train_time:83833ms step_avg:142.33ms
step:600/1375 train_time:83981ms step_avg:142.34ms
step:601/1375 train_time:84129ms step_avg:142.35ms
step:602/1375 train_time:84276ms step_avg:142.36ms
step:603/1375 train_time:84425ms step_avg:142.37ms
step:604/1375 train_time:84572ms step_avg:142.38ms
step:605/1375 train_time:84719ms step_avg:142.39ms
step:606/1375 train_time:84867ms step_avg:142.39ms
step:607/1375 train_time:85014ms step_avg:142.40ms
step:608/1375 train_time:85160ms step_avg:142.41ms
step:609/1375 train_time:85309ms step_avg:142.42ms
step:610/1375 train_time:85456ms step_avg:142.43ms
step:611/1375 train_time:85602ms step_avg:142.43ms
step:612/1375 train_time:85751ms step_avg:142.44ms
step:613/1375 train_time:85900ms step_avg:142.45ms
step:614/1375 train_time:86049ms step_avg:142.46ms
step:615/1375 train_time:86197ms step_avg:142.47ms
step:616/1375 train_time:86345ms step_avg:142.48ms
step:617/1375 train_time:86494ms step_avg:142.49ms
step:618/1375 train_time:86641ms step_avg:142.50ms
step:619/1375 train_time:86792ms step_avg:142.51ms
step:620/1375 train_time:86940ms step_avg:142.52ms
step:621/1375 train_time:87088ms step_avg:142.53ms
step:622/1375 train_time:87239ms step_avg:142.55ms
step:623/1375 train_time:87389ms step_avg:142.56ms
step:624/1375 train_time:87538ms step_avg:142.57ms
step:625/1375 train_time:87686ms step_avg:142.58ms
step:625/1375 val_loss:3.5764 train_time:87763ms step_avg:142.70ms
step:626/1375 train_time:87838ms step_avg:142.59ms
step:627/1375 train_time:87988ms step_avg:142.61ms
step:628/1375 train_time:88135ms step_avg:142.61ms
step:629/1375 train_time:88283ms step_avg:142.62ms
step:630/1375 train_time:88431ms step_avg:142.63ms
step:631/1375 train_time:88578ms step_avg:142.64ms
step:632/1375 train_time:88729ms step_avg:142.65ms
step:633/1375 train_time:88878ms step_avg:142.66ms
step:634/1375 train_time:89029ms step_avg:142.67ms
step:635/1375 train_time:89176ms step_avg:142.68ms
step:636/1375 train_time:89324ms step_avg:142.69ms
step:637/1375 train_time:89473ms step_avg:142.70ms
step:638/1375 train_time:89620ms step_avg:142.71ms
step:639/1375 train_time:89769ms step_avg:142.72ms
step:640/1375 train_time:89919ms step_avg:142.73ms
step:641/1375 train_time:90069ms step_avg:142.74ms
step:642/1375 train_time:90217ms step_avg:142.75ms
step:643/1375 train_time:90366ms step_avg:142.76ms
step:644/1375 train_time:90515ms step_avg:142.77ms
step:645/1375 train_time:90663ms step_avg:142.78ms
step:646/1375 train_time:90814ms step_avg:142.79ms
step:647/1375 train_time:90962ms step_avg:142.80ms
step:648/1375 train_time:91116ms step_avg:142.81ms
step:649/1375 train_time:91264ms step_avg:142.82ms
step:650/1375 train_time:91416ms step_avg:142.84ms
step:651/1375 train_time:91566ms step_avg:142.85ms
step:652/1375 train_time:91715ms step_avg:142.86ms
step:653/1375 train_time:91863ms step_avg:142.87ms
step:654/1375 train_time:92014ms step_avg:142.88ms
step:655/1375 train_time:92162ms step_avg:142.89ms
step:656/1375 train_time:92312ms step_avg:142.90ms
step:657/1375 train_time:92458ms step_avg:142.90ms
step:658/1375 train_time:92608ms step_avg:142.91ms
step:659/1375 train_time:92756ms step_avg:142.92ms
step:660/1375 train_time:92906ms step_avg:142.93ms
step:661/1375 train_time:93056ms step_avg:142.94ms
step:662/1375 train_time:93205ms step_avg:142.95ms
step:663/1375 train_time:93353ms step_avg:142.96ms
step:664/1375 train_time:93501ms step_avg:142.97ms
step:665/1375 train_time:93652ms step_avg:142.98ms
step:666/1375 train_time:93799ms step_avg:142.99ms
step:667/1375 train_time:93950ms step_avg:143.00ms
step:668/1375 train_time:94101ms step_avg:143.01ms
step:669/1375 train_time:94251ms step_avg:143.02ms
step:670/1375 train_time:94400ms step_avg:143.03ms
step:671/1375 train_time:94549ms step_avg:143.04ms
step:672/1375 train_time:94697ms step_avg:143.05ms
step:673/1375 train_time:94845ms step_avg:143.05ms
step:674/1375 train_time:94994ms step_avg:143.06ms
step:675/1375 train_time:95144ms step_avg:143.07ms
step:676/1375 train_time:95294ms step_avg:143.08ms
step:677/1375 train_time:95443ms step_avg:143.09ms
step:678/1375 train_time:95591ms step_avg:143.10ms
step:679/1375 train_time:95741ms step_avg:143.11ms
step:680/1375 train_time:95892ms step_avg:143.12ms
step:681/1375 train_time:96038ms step_avg:143.13ms
step:682/1375 train_time:96188ms step_avg:143.14ms
step:683/1375 train_time:96336ms step_avg:143.14ms
step:684/1375 train_time:96486ms step_avg:143.15ms
step:685/1375 train_time:96635ms step_avg:143.16ms
step:686/1375 train_time:96783ms step_avg:143.17ms
step:687/1375 train_time:96930ms step_avg:143.18ms
step:688/1375 train_time:97079ms step_avg:143.18ms
step:689/1375 train_time:97229ms step_avg:143.19ms
step:690/1375 train_time:97379ms step_avg:143.20ms
step:691/1375 train_time:97528ms step_avg:143.21ms
step:692/1375 train_time:97677ms step_avg:143.22ms
step:693/1375 train_time:97825ms step_avg:143.23ms
step:694/1375 train_time:97974ms step_avg:143.24ms
step:695/1375 train_time:98122ms step_avg:143.24ms
step:696/1375 train_time:98272ms step_avg:143.25ms
step:697/1375 train_time:98419ms step_avg:143.26ms
step:698/1375 train_time:98569ms step_avg:143.27ms
step:699/1375 train_time:98718ms step_avg:143.28ms
step:700/1375 train_time:98867ms step_avg:143.29ms
step:701/1375 train_time:99016ms step_avg:143.29ms
step:702/1375 train_time:99166ms step_avg:143.30ms
step:703/1375 train_time:99315ms step_avg:143.31ms
step:704/1375 train_time:99463ms step_avg:143.32ms
step:705/1375 train_time:99615ms step_avg:143.33ms
step:706/1375 train_time:99767ms step_avg:143.34ms
step:707/1375 train_time:99917ms step_avg:143.35ms
step:708/1375 train_time:100064ms step_avg:143.36ms
step:709/1375 train_time:100214ms step_avg:143.37ms
step:710/1375 train_time:100362ms step_avg:143.37ms
step:711/1375 train_time:100513ms step_avg:143.39ms
step:712/1375 train_time:100662ms step_avg:143.39ms
step:713/1375 train_time:100813ms step_avg:143.40ms
step:714/1375 train_time:100961ms step_avg:143.41ms
step:715/1375 train_time:101111ms step_avg:143.42ms
step:716/1375 train_time:101260ms step_avg:143.43ms
step:717/1375 train_time:101412ms step_avg:143.44ms
step:718/1375 train_time:101560ms step_avg:143.45ms
step:719/1375 train_time:101710ms step_avg:143.45ms
step:720/1375 train_time:101860ms step_avg:143.46ms
step:721/1375 train_time:102011ms step_avg:143.48ms
step:722/1375 train_time:102160ms step_avg:143.48ms
step:723/1375 train_time:102311ms step_avg:143.49ms
step:724/1375 train_time:102460ms step_avg:143.50ms
step:725/1375 train_time:102611ms step_avg:143.51ms
step:726/1375 train_time:102761ms step_avg:143.52ms
step:727/1375 train_time:102913ms step_avg:143.53ms
step:728/1375 train_time:103062ms step_avg:143.54ms
step:729/1375 train_time:103212ms step_avg:143.55ms
step:730/1375 train_time:103363ms step_avg:143.56ms
step:731/1375 train_time:103514ms step_avg:143.57ms
step:732/1375 train_time:103662ms step_avg:143.58ms
step:733/1375 train_time:103814ms step_avg:143.59ms
step:734/1375 train_time:103964ms step_avg:143.60ms
step:735/1375 train_time:104116ms step_avg:143.61ms
step:736/1375 train_time:104268ms step_avg:143.62ms
step:737/1375 train_time:104418ms step_avg:143.63ms
step:738/1375 train_time:104569ms step_avg:143.64ms
step:739/1375 train_time:104719ms step_avg:143.65ms
step:740/1375 train_time:104871ms step_avg:143.66ms
step:741/1375 train_time:105020ms step_avg:143.67ms
step:742/1375 train_time:105171ms step_avg:143.68ms
step:743/1375 train_time:105319ms step_avg:143.68ms
step:744/1375 train_time:105469ms step_avg:143.69ms
step:745/1375 train_time:105621ms step_avg:143.70ms
step:746/1375 train_time:105771ms step_avg:143.71ms
step:747/1375 train_time:105921ms step_avg:143.72ms
step:748/1375 train_time:106074ms step_avg:143.73ms
step:749/1375 train_time:106224ms step_avg:143.74ms
step:750/1375 train_time:106374ms step_avg:143.75ms
step:750/1375 val_loss:3.5229 train_time:106450ms step_avg:143.85ms
step:751/1375 train_time:106528ms step_avg:143.76ms
step:752/1375 train_time:106683ms step_avg:143.78ms
step:753/1375 train_time:106831ms step_avg:143.78ms
step:754/1375 train_time:106980ms step_avg:143.79ms
step:755/1375 train_time:107130ms step_avg:143.80ms
step:756/1375 train_time:107279ms step_avg:143.81ms
step:757/1375 train_time:107430ms step_avg:143.82ms
step:758/1375 train_time:107583ms step_avg:143.83ms
step:759/1375 train_time:107731ms step_avg:143.83ms
step:760/1375 train_time:107881ms step_avg:143.84ms
step:761/1375 train_time:108073ms step_avg:143.91ms
step:762/1375 train_time:108220ms step_avg:143.91ms
step:763/1375 train_time:108369ms step_avg:143.92ms
step:764/1375 train_time:108518ms step_avg:143.92ms
step:765/1375 train_time:108667ms step_avg:143.93ms
step:766/1375 train_time:108819ms step_avg:143.94ms
step:767/1375 train_time:108971ms step_avg:143.95ms
step:768/1375 train_time:109124ms step_avg:143.96ms
step:769/1375 train_time:109273ms step_avg:143.97ms
step:770/1375 train_time:109425ms step_avg:143.98ms
step:771/1375 train_time:109573ms step_avg:143.99ms
step:772/1375 train_time:109722ms step_avg:143.99ms
step:773/1375 train_time:109871ms step_avg:144.00ms
step:774/1375 train_time:110024ms step_avg:144.01ms
step:775/1375 train_time:110176ms step_avg:144.02ms
step:776/1375 train_time:110328ms step_avg:144.03ms
step:777/1375 train_time:110480ms step_avg:144.04ms
step:778/1375 train_time:110628ms step_avg:144.05ms
step:779/1375 train_time:110777ms step_avg:144.05ms
step:780/1375 train_time:110928ms step_avg:144.06ms
step:781/1375 train_time:111080ms step_avg:144.07ms
step:782/1375 train_time:111230ms step_avg:144.08ms
step:783/1375 train_time:111381ms step_avg:144.09ms
step:784/1375 train_time:111531ms step_avg:144.10ms
step:785/1375 train_time:111680ms step_avg:144.10ms
step:786/1375 train_time:111830ms step_avg:144.11ms
step:787/1375 train_time:111980ms step_avg:144.12ms
step:788/1375 train_time:112130ms step_avg:144.13ms
step:789/1375 train_time:112280ms step_avg:144.13ms
step:790/1375 train_time:112429ms step_avg:144.14ms
step:791/1375 train_time:112580ms step_avg:144.15ms
step:792/1375 train_time:112730ms step_avg:144.16ms
step:793/1375 train_time:112880ms step_avg:144.16ms
step:794/1375 train_time:113031ms step_avg:144.17ms
step:795/1375 train_time:113184ms step_avg:144.18ms
step:796/1375 train_time:113333ms step_avg:144.19ms
step:797/1375 train_time:113486ms step_avg:144.20ms
step:798/1375 train_time:113634ms step_avg:144.21ms
step:799/1375 train_time:113787ms step_avg:144.22ms
step:800/1375 train_time:113937ms step_avg:144.22ms
step:801/1375 train_time:114087ms step_avg:144.23ms
step:802/1375 train_time:114238ms step_avg:144.24ms
step:803/1375 train_time:114387ms step_avg:144.25ms
step:804/1375 train_time:114538ms step_avg:144.25ms
step:805/1375 train_time:114690ms step_avg:144.26ms
step:806/1375 train_time:114840ms step_avg:144.27ms
step:807/1375 train_time:114990ms step_avg:144.28ms
step:808/1375 train_time:115141ms step_avg:144.29ms
step:809/1375 train_time:115289ms step_avg:144.29ms
step:810/1375 train_time:115440ms step_avg:144.30ms
step:811/1375 train_time:115590ms step_avg:144.31ms
step:812/1375 train_time:115740ms step_avg:144.31ms
step:813/1375 train_time:115889ms step_avg:144.32ms
step:814/1375 train_time:116040ms step_avg:144.33ms
step:815/1375 train_time:116187ms step_avg:144.33ms
step:816/1375 train_time:116341ms step_avg:144.34ms
step:817/1375 train_time:116492ms step_avg:144.35ms
step:818/1375 train_time:116644ms step_avg:144.36ms
step:819/1375 train_time:116794ms step_avg:144.37ms
step:820/1375 train_time:116948ms step_avg:144.38ms
step:821/1375 train_time:117100ms step_avg:144.39ms
step:822/1375 train_time:117251ms step_avg:144.40ms
step:823/1375 train_time:117402ms step_avg:144.41ms
step:824/1375 train_time:117551ms step_avg:144.41ms
step:825/1375 train_time:117707ms step_avg:144.43ms
step:826/1375 train_time:117859ms step_avg:144.44ms
step:827/1375 train_time:118010ms step_avg:144.44ms
step:828/1375 train_time:118162ms step_avg:144.45ms
step:829/1375 train_time:118312ms step_avg:144.46ms
step:830/1375 train_time:118464ms step_avg:144.47ms
step:831/1375 train_time:118614ms step_avg:144.48ms
step:832/1375 train_time:118766ms step_avg:144.48ms
step:833/1375 train_time:118916ms step_avg:144.49ms
step:834/1375 train_time:119067ms step_avg:144.50ms
step:835/1375 train_time:119222ms step_avg:144.51ms
step:836/1375 train_time:119376ms step_avg:144.52ms
step:837/1375 train_time:119527ms step_avg:144.53ms
step:838/1375 train_time:119679ms step_avg:144.54ms
step:839/1375 train_time:119829ms step_avg:144.55ms
step:840/1375 train_time:119979ms step_avg:144.55ms
step:841/1375 train_time:120130ms step_avg:144.56ms
step:842/1375 train_time:120285ms step_avg:144.57ms
step:843/1375 train_time:120434ms step_avg:144.58ms
step:844/1375 train_time:120585ms step_avg:144.59ms
step:845/1375 train_time:120734ms step_avg:144.59ms
step:846/1375 train_time:120886ms step_avg:144.60ms
step:847/1375 train_time:121037ms step_avg:144.61ms
step:848/1375 train_time:121189ms step_avg:144.62ms
step:849/1375 train_time:121343ms step_avg:144.63ms
step:850/1375 train_time:121496ms step_avg:144.64ms
step:851/1375 train_time:121649ms step_avg:144.65ms
step:852/1375 train_time:121802ms step_avg:144.66ms
step:853/1375 train_time:121950ms step_avg:144.66ms
step:854/1375 train_time:122101ms step_avg:144.67ms
step:855/1375 train_time:122251ms step_avg:144.68ms
step:856/1375 train_time:122402ms step_avg:144.68ms
step:857/1375 train_time:122552ms step_avg:144.69ms
step:858/1375 train_time:122708ms step_avg:144.70ms
step:859/1375 train_time:122860ms step_avg:144.71ms
step:860/1375 train_time:123009ms step_avg:144.72ms
step:861/1375 train_time:123164ms step_avg:144.73ms
step:862/1375 train_time:123315ms step_avg:144.74ms
step:863/1375 train_time:123466ms step_avg:144.74ms
step:864/1375 train_time:123619ms step_avg:144.75ms
step:865/1375 train_time:123770ms step_avg:144.76ms
step:866/1375 train_time:123927ms step_avg:144.77ms
step:867/1375 train_time:124077ms step_avg:144.78ms
step:868/1375 train_time:124227ms step_avg:144.79ms
step:869/1375 train_time:124378ms step_avg:144.79ms
step:870/1375 train_time:124531ms step_avg:144.80ms
step:871/1375 train_time:124681ms step_avg:144.81ms
step:872/1375 train_time:124830ms step_avg:144.81ms
step:873/1375 train_time:124982ms step_avg:144.82ms
step:874/1375 train_time:125132ms step_avg:144.83ms
step:875/1375 train_time:125286ms step_avg:144.84ms
step:875/1375 val_loss:3.4714 train_time:125362ms step_avg:144.93ms
step:876/1375 train_time:125437ms step_avg:144.85ms
step:877/1375 train_time:125590ms step_avg:144.86ms
step:878/1375 train_time:125743ms step_avg:144.86ms
step:879/1375 train_time:125893ms step_avg:144.87ms
step:880/1375 train_time:126045ms step_avg:144.88ms
step:881/1375 train_time:126194ms step_avg:144.88ms
step:882/1375 train_time:126349ms step_avg:144.90ms
step:883/1375 train_time:126503ms step_avg:144.91ms
step:884/1375 train_time:126656ms step_avg:144.92ms
step:885/1375 train_time:126808ms step_avg:144.92ms
step:886/1375 train_time:126962ms step_avg:144.93ms
step:887/1375 train_time:127111ms step_avg:144.94ms
step:888/1375 train_time:127264ms step_avg:144.95ms
step:889/1375 train_time:127418ms step_avg:144.96ms
step:890/1375 train_time:127568ms step_avg:144.96ms
step:891/1375 train_time:127723ms step_avg:144.97ms
step:892/1375 train_time:127874ms step_avg:144.98ms
step:893/1375 train_time:128024ms step_avg:144.99ms
step:894/1375 train_time:128176ms step_avg:145.00ms
step:895/1375 train_time:128331ms step_avg:145.01ms
step:896/1375 train_time:128483ms step_avg:145.01ms
step:897/1375 train_time:128632ms step_avg:145.02ms
step:898/1375 train_time:128785ms step_avg:145.03ms
step:899/1375 train_time:128935ms step_avg:145.03ms
step:900/1375 train_time:129086ms step_avg:145.04ms
step:901/1375 train_time:129240ms step_avg:145.05ms
step:902/1375 train_time:129389ms step_avg:145.05ms
step:903/1375 train_time:129541ms step_avg:145.06ms
step:904/1375 train_time:129693ms step_avg:145.07ms
step:905/1375 train_time:129845ms step_avg:145.08ms
step:906/1375 train_time:129994ms step_avg:145.08ms
step:907/1375 train_time:130151ms step_avg:145.10ms
step:908/1375 train_time:130303ms step_avg:145.10ms
step:909/1375 train_time:130456ms step_avg:145.11ms
step:910/1375 train_time:130612ms step_avg:145.12ms
step:911/1375 train_time:130763ms step_avg:145.13ms
step:912/1375 train_time:130913ms step_avg:145.14ms
step:913/1375 train_time:131067ms step_avg:145.15ms
step:914/1375 train_time:131217ms step_avg:145.15ms
step:915/1375 train_time:131369ms step_avg:145.16ms
step:916/1375 train_time:131522ms step_avg:145.17ms
step:917/1375 train_time:131673ms step_avg:145.17ms
step:918/1375 train_time:131826ms step_avg:145.18ms
step:919/1375 train_time:131981ms step_avg:145.19ms
step:920/1375 train_time:132133ms step_avg:145.20ms
step:921/1375 train_time:132287ms step_avg:145.21ms
step:922/1375 train_time:132443ms step_avg:145.22ms
step:923/1375 train_time:132594ms step_avg:145.23ms
step:924/1375 train_time:132747ms step_avg:145.24ms
step:925/1375 train_time:132902ms step_avg:145.25ms
step:926/1375 train_time:133057ms step_avg:145.26ms
step:927/1375 train_time:133209ms step_avg:145.27ms
step:928/1375 train_time:133362ms step_avg:145.27ms
step:929/1375 train_time:133516ms step_avg:145.28ms
step:930/1375 train_time:133670ms step_avg:145.29ms
step:931/1375 train_time:133822ms step_avg:145.30ms
step:932/1375 train_time:133973ms step_avg:145.31ms
step:933/1375 train_time:134126ms step_avg:145.32ms
step:934/1375 train_time:134280ms step_avg:145.32ms
step:935/1375 train_time:134434ms step_avg:145.33ms
step:936/1375 train_time:134588ms step_avg:145.34ms
step:937/1375 train_time:134743ms step_avg:145.35ms
step:938/1375 train_time:134895ms step_avg:145.36ms
step:939/1375 train_time:135049ms step_avg:145.37ms
step:940/1375 train_time:135202ms step_avg:145.38ms
step:941/1375 train_time:135352ms step_avg:145.38ms
step:942/1375 train_time:135506ms step_avg:145.39ms
step:943/1375 train_time:135662ms step_avg:145.40ms
step:944/1375 train_time:135823ms step_avg:145.42ms
step:945/1375 train_time:135973ms step_avg:145.43ms
step:946/1375 train_time:136127ms step_avg:145.44ms
step:947/1375 train_time:136280ms step_avg:145.44ms
step:948/1375 train_time:136432ms step_avg:145.45ms
step:949/1375 train_time:136588ms step_avg:145.46ms
step:950/1375 train_time:136740ms step_avg:145.47ms
step:951/1375 train_time:136939ms step_avg:145.52ms
step:952/1375 train_time:137088ms step_avg:145.53ms
step:953/1375 train_time:137240ms step_avg:145.54ms
step:954/1375 train_time:137392ms step_avg:145.54ms
step:955/1375 train_time:137546ms step_avg:145.55ms
step:956/1375 train_time:137697ms step_avg:145.56ms
step:957/1375 train_time:137852ms step_avg:145.57ms
step:958/1375 train_time:138011ms step_avg:145.58ms
step:959/1375 train_time:138167ms step_avg:145.59ms
step:960/1375 train_time:138322ms step_avg:145.60ms
step:961/1375 train_time:138473ms step_avg:145.61ms
step:962/1375 train_time:138626ms step_avg:145.62ms
step:963/1375 train_time:138782ms step_avg:145.63ms
step:964/1375 train_time:138933ms step_avg:145.63ms
step:965/1375 train_time:139087ms step_avg:145.64ms
step:966/1375 train_time:139240ms step_avg:145.65ms
step:967/1375 train_time:139391ms step_avg:145.65ms
step:968/1375 train_time:139541ms step_avg:145.66ms
step:969/1375 train_time:139694ms step_avg:145.67ms
step:970/1375 train_time:139847ms step_avg:145.67ms
step:971/1375 train_time:140002ms step_avg:145.68ms
step:972/1375 train_time:140152ms step_avg:145.69ms
step:973/1375 train_time:140305ms step_avg:145.70ms
step:974/1375 train_time:140456ms step_avg:145.70ms
step:975/1375 train_time:140609ms step_avg:145.71ms
step:976/1375 train_time:140761ms step_avg:145.72ms
step:977/1375 train_time:140913ms step_avg:145.72ms
step:978/1375 train_time:141067ms step_avg:145.73ms
step:979/1375 train_time:141220ms step_avg:145.74ms
step:980/1375 train_time:141372ms step_avg:145.74ms
step:981/1375 train_time:141524ms step_avg:145.75ms
step:982/1375 train_time:141674ms step_avg:145.76ms
step:983/1375 train_time:141826ms step_avg:145.76ms
step:984/1375 train_time:141976ms step_avg:145.77ms
step:985/1375 train_time:142129ms step_avg:145.77ms
step:986/1375 train_time:142283ms step_avg:145.78ms
step:987/1375 train_time:142434ms step_avg:145.79ms
step:988/1375 train_time:142587ms step_avg:145.79ms
step:989/1375 train_time:142738ms step_avg:145.80ms
step:990/1375 train_time:142891ms step_avg:145.81ms
step:991/1375 train_time:143043ms step_avg:145.81ms
step:992/1375 train_time:143201ms step_avg:145.83ms
step:993/1375 train_time:143360ms step_avg:145.84ms
step:994/1375 train_time:143512ms step_avg:145.85ms
step:995/1375 train_time:143664ms step_avg:145.85ms
step:996/1375 train_time:143813ms step_avg:145.86ms
step:997/1375 train_time:143967ms step_avg:145.86ms
step:998/1375 train_time:144120ms step_avg:145.87ms
step:999/1375 train_time:144273ms step_avg:145.88ms
step:1000/1375 train_time:144427ms step_avg:145.89ms
step:1000/1375 val_loss:3.4041 train_time:144502ms step_avg:145.96ms
step:1001/1375 train_time:144580ms step_avg:145.89ms
step:1002/1375 train_time:144733ms step_avg:145.90ms
step:1003/1375 train_time:144887ms step_avg:145.91ms
step:1004/1375 train_time:145042ms step_avg:145.92ms
step:1005/1375 train_time:145194ms step_avg:145.92ms
step:1006/1375 train_time:145345ms step_avg:145.93ms
step:1007/1375 train_time:145502ms step_avg:145.94ms
step:1008/1375 train_time:145655ms step_avg:145.95ms
step:1009/1375 train_time:145813ms step_avg:145.96ms
step:1010/1375 train_time:145966ms step_avg:145.97ms
step:1011/1375 train_time:146119ms step_avg:145.97ms
step:1012/1375 train_time:146270ms step_avg:145.98ms
step:1013/1375 train_time:146425ms step_avg:145.99ms
step:1014/1375 train_time:146578ms step_avg:145.99ms
step:1015/1375 train_time:146731ms step_avg:146.00ms
step:1016/1375 train_time:146884ms step_avg:146.01ms
step:1017/1375 train_time:147037ms step_avg:146.01ms
step:1018/1375 train_time:147188ms step_avg:146.02ms
step:1019/1375 train_time:147343ms step_avg:146.03ms
step:1020/1375 train_time:147499ms step_avg:146.04ms
step:1021/1375 train_time:147652ms step_avg:146.05ms
step:1022/1375 train_time:147807ms step_avg:146.05ms
step:1023/1375 train_time:147961ms step_avg:146.06ms
step:1024/1375 train_time:148113ms step_avg:146.07ms
step:1025/1375 train_time:148266ms step_avg:146.08ms
step:1026/1375 train_time:148419ms step_avg:146.08ms
step:1027/1375 train_time:148572ms step_avg:146.09ms
step:1028/1375 train_time:148727ms step_avg:146.10ms
step:1029/1375 train_time:148884ms step_avg:146.11ms
step:1030/1375 train_time:149041ms step_avg:146.12ms
step:1031/1375 train_time:149190ms step_avg:146.12ms
step:1032/1375 train_time:149344ms step_avg:146.13ms
step:1033/1375 train_time:149497ms step_avg:146.14ms
step:1034/1375 train_time:149651ms step_avg:146.14ms
step:1035/1375 train_time:149807ms step_avg:146.15ms
step:1036/1375 train_time:149962ms step_avg:146.16ms
step:1037/1375 train_time:150119ms step_avg:146.17ms
step:1038/1375 train_time:150273ms step_avg:146.18ms
step:1039/1375 train_time:150425ms step_avg:146.19ms
step:1040/1375 train_time:150577ms step_avg:146.19ms
step:1041/1375 train_time:150731ms step_avg:146.20ms
step:1042/1375 train_time:150884ms step_avg:146.21ms
step:1043/1375 train_time:151036ms step_avg:146.21ms
step:1044/1375 train_time:151192ms step_avg:146.22ms
step:1045/1375 train_time:151349ms step_avg:146.23ms
step:1046/1375 train_time:151502ms step_avg:146.24ms
step:1047/1375 train_time:151655ms step_avg:146.24ms
step:1048/1375 train_time:151809ms step_avg:146.25ms
step:1049/1375 train_time:151964ms step_avg:146.26ms
step:1050/1375 train_time:152120ms step_avg:146.27ms
step:1051/1375 train_time:152275ms step_avg:146.28ms
step:1052/1375 train_time:152428ms step_avg:146.28ms
step:1053/1375 train_time:152581ms step_avg:146.29ms
step:1054/1375 train_time:152739ms step_avg:146.30ms
step:1055/1375 train_time:152890ms step_avg:146.31ms
step:1056/1375 train_time:153045ms step_avg:146.31ms
step:1057/1375 train_time:153201ms step_avg:146.32ms
step:1058/1375 train_time:153356ms step_avg:146.33ms
step:1059/1375 train_time:153512ms step_avg:146.34ms
step:1060/1375 train_time:153667ms step_avg:146.35ms
step:1061/1375 train_time:153819ms step_avg:146.36ms
step:1062/1375 train_time:153973ms step_avg:146.36ms
step:1063/1375 train_time:154128ms step_avg:146.37ms
step:1064/1375 train_time:154280ms step_avg:146.38ms
step:1065/1375 train_time:154434ms step_avg:146.38ms
step:1066/1375 train_time:154591ms step_avg:146.39ms
step:1067/1375 train_time:154746ms step_avg:146.40ms
step:1068/1375 train_time:154900ms step_avg:146.41ms
step:1069/1375 train_time:155058ms step_avg:146.42ms
step:1070/1375 train_time:155210ms step_avg:146.42ms
step:1071/1375 train_time:155367ms step_avg:146.43ms
step:1072/1375 train_time:155519ms step_avg:146.44ms
step:1073/1375 train_time:155669ms step_avg:146.44ms
step:1074/1375 train_time:155823ms step_avg:146.45ms
step:1075/1375 train_time:155976ms step_avg:146.46ms
step:1076/1375 train_time:156130ms step_avg:146.46ms
step:1077/1375 train_time:156285ms step_avg:146.47ms
step:1078/1375 train_time:156444ms step_avg:146.48ms
step:1079/1375 train_time:156602ms step_avg:146.49ms
step:1080/1375 train_time:156755ms step_avg:146.50ms
step:1081/1375 train_time:156907ms step_avg:146.51ms
step:1082/1375 train_time:157063ms step_avg:146.51ms
step:1083/1375 train_time:157216ms step_avg:146.52ms
step:1084/1375 train_time:157372ms step_avg:146.53ms
step:1085/1375 train_time:157524ms step_avg:146.53ms
step:1086/1375 train_time:157681ms step_avg:146.54ms
step:1087/1375 train_time:157837ms step_avg:146.55ms
step:1088/1375 train_time:157990ms step_avg:146.56ms
step:1089/1375 train_time:158147ms step_avg:146.57ms
step:1090/1375 train_time:158306ms step_avg:146.58ms
step:1091/1375 train_time:158463ms step_avg:146.59ms
step:1092/1375 train_time:158616ms step_avg:146.59ms
step:1093/1375 train_time:158770ms step_avg:146.60ms
step:1094/1375 train_time:158924ms step_avg:146.61ms
step:1095/1375 train_time:159079ms step_avg:146.62ms
step:1096/1375 train_time:159237ms step_avg:146.63ms
step:1097/1375 train_time:159390ms step_avg:146.63ms
step:1098/1375 train_time:159544ms step_avg:146.64ms
step:1099/1375 train_time:159696ms step_avg:146.64ms
step:1100/1375 train_time:159847ms step_avg:146.65ms
step:1101/1375 train_time:160001ms step_avg:146.66ms
step:1102/1375 train_time:160154ms step_avg:146.66ms
step:1103/1375 train_time:160307ms step_avg:146.67ms
step:1104/1375 train_time:160460ms step_avg:146.67ms
step:1105/1375 train_time:160614ms step_avg:146.68ms
step:1106/1375 train_time:160767ms step_avg:146.69ms
step:1107/1375 train_time:160922ms step_avg:146.69ms
step:1108/1375 train_time:161077ms step_avg:146.70ms
step:1109/1375 train_time:161229ms step_avg:146.71ms
step:1110/1375 train_time:161384ms step_avg:146.71ms
step:1111/1375 train_time:161542ms step_avg:146.72ms
step:1112/1375 train_time:161698ms step_avg:146.73ms
step:1113/1375 train_time:161850ms step_avg:146.74ms
step:1114/1375 train_time:162008ms step_avg:146.75ms
step:1115/1375 train_time:162163ms step_avg:146.75ms
step:1116/1375 train_time:162313ms step_avg:146.76ms
step:1117/1375 train_time:162469ms step_avg:146.77ms
step:1118/1375 train_time:162628ms step_avg:146.78ms
step:1119/1375 train_time:162782ms step_avg:146.78ms
step:1120/1375 train_time:162936ms step_avg:146.79ms
step:1121/1375 train_time:163089ms step_avg:146.79ms
step:1122/1375 train_time:163242ms step_avg:146.80ms
step:1123/1375 train_time:163395ms step_avg:146.81ms
step:1124/1375 train_time:163551ms step_avg:146.81ms
step:1125/1375 train_time:163707ms step_avg:146.82ms
step:1125/1375 val_loss:3.3508 train_time:163787ms step_avg:146.89ms
step:1126/1375 train_time:163866ms step_avg:146.83ms
step:1127/1375 train_time:164024ms step_avg:146.84ms
step:1128/1375 train_time:164178ms step_avg:146.85ms
step:1129/1375 train_time:164339ms step_avg:146.86ms
step:1130/1375 train_time:164493ms step_avg:146.87ms
step:1131/1375 train_time:164650ms step_avg:146.88ms
step:1132/1375 train_time:164804ms step_avg:146.88ms
step:1133/1375 train_time:164961ms step_avg:146.89ms
step:1134/1375 train_time:165117ms step_avg:146.90ms
step:1135/1375 train_time:165270ms step_avg:146.91ms
step:1136/1375 train_time:165431ms step_avg:146.92ms
step:1137/1375 train_time:165585ms step_avg:146.93ms
step:1138/1375 train_time:165738ms step_avg:146.93ms
step:1139/1375 train_time:165893ms step_avg:146.94ms
step:1140/1375 train_time:166048ms step_avg:146.95ms
step:1141/1375 train_time:166248ms step_avg:146.99ms
step:1142/1375 train_time:166402ms step_avg:147.00ms
step:1143/1375 train_time:166561ms step_avg:147.01ms
step:1144/1375 train_time:166717ms step_avg:147.02ms
step:1145/1375 train_time:166868ms step_avg:147.02ms
step:1146/1375 train_time:167026ms step_avg:147.03ms
step:1147/1375 train_time:167181ms step_avg:147.04ms
step:1148/1375 train_time:167336ms step_avg:147.04ms
step:1149/1375 train_time:167491ms step_avg:147.05ms
step:1150/1375 train_time:167644ms step_avg:147.06ms
step:1151/1375 train_time:167800ms step_avg:147.06ms
step:1152/1375 train_time:167955ms step_avg:147.07ms
step:1153/1375 train_time:168112ms step_avg:147.08ms
step:1154/1375 train_time:168266ms step_avg:147.09ms
step:1155/1375 train_time:168422ms step_avg:147.09ms
step:1156/1375 train_time:168582ms step_avg:147.10ms
step:1157/1375 train_time:168739ms step_avg:147.11ms
step:1158/1375 train_time:168892ms step_avg:147.12ms
step:1159/1375 train_time:169047ms step_avg:147.13ms
step:1160/1375 train_time:169200ms step_avg:147.13ms
step:1161/1375 train_time:169354ms step_avg:147.14ms
step:1162/1375 train_time:169510ms step_avg:147.14ms
step:1163/1375 train_time:169665ms step_avg:147.15ms
step:1164/1375 train_time:169821ms step_avg:147.16ms
step:1165/1375 train_time:169972ms step_avg:147.16ms
step:1166/1375 train_time:170128ms step_avg:147.17ms
step:1167/1375 train_time:170281ms step_avg:147.17ms
step:1168/1375 train_time:170435ms step_avg:147.18ms
step:1169/1375 train_time:170591ms step_avg:147.19ms
step:1170/1375 train_time:170748ms step_avg:147.20ms
step:1171/1375 train_time:170904ms step_avg:147.20ms
step:1172/1375 train_time:171059ms step_avg:147.21ms
step:1173/1375 train_time:171212ms step_avg:147.22ms
step:1174/1375 train_time:171375ms step_avg:147.23ms
step:1175/1375 train_time:171530ms step_avg:147.24ms
step:1176/1375 train_time:171689ms step_avg:147.25ms
step:1177/1375 train_time:171853ms step_avg:147.26ms
step:1178/1375 train_time:172010ms step_avg:147.27ms
step:1179/1375 train_time:172166ms step_avg:147.28ms
step:1180/1375 train_time:172328ms step_avg:147.29ms
step:1181/1375 train_time:172481ms step_avg:147.29ms
step:1182/1375 train_time:172633ms step_avg:147.30ms
step:1183/1375 train_time:172787ms step_avg:147.30ms
step:1184/1375 train_time:172944ms step_avg:147.31ms
step:1185/1375 train_time:173102ms step_avg:147.32ms
step:1186/1375 train_time:173256ms step_avg:147.33ms
step:1187/1375 train_time:173420ms step_avg:147.34ms
step:1188/1375 train_time:173571ms step_avg:147.34ms
step:1189/1375 train_time:173730ms step_avg:147.35ms
step:1190/1375 train_time:173888ms step_avg:147.36ms
step:1191/1375 train_time:174046ms step_avg:147.37ms
step:1192/1375 train_time:174200ms step_avg:147.38ms
step:1193/1375 train_time:174353ms step_avg:147.38ms
step:1194/1375 train_time:174509ms step_avg:147.39ms
step:1195/1375 train_time:174664ms step_avg:147.40ms
step:1196/1375 train_time:174820ms step_avg:147.40ms
step:1197/1375 train_time:174976ms step_avg:147.41ms
step:1198/1375 train_time:175137ms step_avg:147.42ms
step:1199/1375 train_time:175291ms step_avg:147.43ms
step:1200/1375 train_time:175447ms step_avg:147.43ms
step:1201/1375 train_time:175602ms step_avg:147.44ms
step:1202/1375 train_time:175767ms step_avg:147.46ms
step:1203/1375 train_time:175926ms step_avg:147.47ms
step:1204/1375 train_time:176081ms step_avg:147.47ms
step:1205/1375 train_time:176235ms step_avg:147.48ms
step:1206/1375 train_time:176390ms step_avg:147.48ms
step:1207/1375 train_time:176545ms step_avg:147.49ms
step:1208/1375 train_time:176701ms step_avg:147.50ms
step:1209/1375 train_time:176856ms step_avg:147.50ms
step:1210/1375 train_time:177017ms step_avg:147.51ms
step:1211/1375 train_time:177171ms step_avg:147.52ms
step:1212/1375 train_time:177326ms step_avg:147.53ms
step:1213/1375 train_time:177480ms step_avg:147.53ms
step:1214/1375 train_time:177636ms step_avg:147.54ms
step:1215/1375 train_time:177791ms step_avg:147.54ms
step:1216/1375 train_time:177946ms step_avg:147.55ms
step:1217/1375 train_time:178100ms step_avg:147.56ms
step:1218/1375 train_time:178251ms step_avg:147.56ms
step:1219/1375 train_time:178404ms step_avg:147.56ms
step:1220/1375 train_time:178556ms step_avg:147.57ms
step:1221/1375 train_time:178709ms step_avg:147.57ms
step:1222/1375 train_time:178863ms step_avg:147.58ms
step:1223/1375 train_time:179018ms step_avg:147.58ms
step:1224/1375 train_time:179177ms step_avg:147.59ms
step:1225/1375 train_time:179335ms step_avg:147.60ms
step:1226/1375 train_time:179491ms step_avg:147.61ms
step:1227/1375 train_time:179649ms step_avg:147.62ms
step:1228/1375 train_time:179801ms step_avg:147.62ms
step:1229/1375 train_time:179956ms step_avg:147.63ms
step:1230/1375 train_time:180117ms step_avg:147.64ms
step:1231/1375 train_time:180275ms step_avg:147.65ms
step:1232/1375 train_time:180433ms step_avg:147.65ms
step:1233/1375 train_time:180589ms step_avg:147.66ms
step:1234/1375 train_time:180745ms step_avg:147.67ms
step:1235/1375 train_time:180900ms step_avg:147.67ms
step:1236/1375 train_time:181057ms step_avg:147.68ms
step:1237/1375 train_time:181212ms step_avg:147.69ms
step:1238/1375 train_time:181376ms step_avg:147.70ms
step:1239/1375 train_time:181531ms step_avg:147.71ms
step:1240/1375 train_time:181690ms step_avg:147.72ms
step:1241/1375 train_time:181854ms step_avg:147.73ms
step:1242/1375 train_time:182010ms step_avg:147.74ms
step:1243/1375 train_time:182169ms step_avg:147.74ms
step:1244/1375 train_time:182324ms step_avg:147.75ms
step:1245/1375 train_time:182478ms step_avg:147.76ms
step:1246/1375 train_time:182632ms step_avg:147.76ms
step:1247/1375 train_time:182790ms step_avg:147.77ms
step:1248/1375 train_time:182945ms step_avg:147.77ms
step:1249/1375 train_time:183099ms step_avg:147.78ms
step:1250/1375 train_time:183253ms step_avg:147.78ms
step:1250/1375 val_loss:3.3052 train_time:183335ms step_avg:147.85ms
step:1251/1375 train_time:183414ms step_avg:147.80ms
step:1252/1375 train_time:183572ms step_avg:147.80ms
step:1253/1375 train_time:183728ms step_avg:147.81ms
step:1254/1375 train_time:183881ms step_avg:147.81ms
step:1255/1375 train_time:184049ms step_avg:147.83ms
step:1256/1375 train_time:184204ms step_avg:147.84ms
step:1257/1375 train_time:184361ms step_avg:147.84ms
step:1258/1375 train_time:184520ms step_avg:147.85ms
step:1259/1375 train_time:184677ms step_avg:147.86ms
step:1260/1375 train_time:184829ms step_avg:147.86ms
step:1261/1375 train_time:184988ms step_avg:147.87ms
step:1262/1375 train_time:185147ms step_avg:147.88ms
step:1263/1375 train_time:185302ms step_avg:147.89ms
step:1264/1375 train_time:185458ms step_avg:147.89ms
step:1265/1375 train_time:185613ms step_avg:147.90ms
step:1266/1375 train_time:185769ms step_avg:147.91ms
step:1267/1375 train_time:185927ms step_avg:147.91ms
step:1268/1375 train_time:186084ms step_avg:147.92ms
step:1269/1375 train_time:186245ms step_avg:147.93ms
step:1270/1375 train_time:186400ms step_avg:147.94ms
step:1271/1375 train_time:186555ms step_avg:147.94ms
step:1272/1375 train_time:186711ms step_avg:147.95ms
step:1273/1375 train_time:186865ms step_avg:147.95ms
step:1274/1375 train_time:187019ms step_avg:147.96ms
step:1275/1375 train_time:187175ms step_avg:147.96ms
step:1276/1375 train_time:187328ms step_avg:147.97ms
step:1277/1375 train_time:187485ms step_avg:147.98ms
step:1278/1375 train_time:187640ms step_avg:147.98ms
step:1279/1375 train_time:187795ms step_avg:147.99ms
step:1280/1375 train_time:187959ms step_avg:148.00ms
step:1281/1375 train_time:188115ms step_avg:148.01ms
step:1282/1375 train_time:188267ms step_avg:148.01ms
step:1283/1375 train_time:188425ms step_avg:148.02ms
step:1284/1375 train_time:188585ms step_avg:148.03ms
step:1285/1375 train_time:188741ms step_avg:148.03ms
step:1286/1375 train_time:188894ms step_avg:148.04ms
step:1287/1375 train_time:189049ms step_avg:148.04ms
step:1288/1375 train_time:189205ms step_avg:148.05ms
step:1289/1375 train_time:189366ms step_avg:148.06ms
step:1290/1375 train_time:189526ms step_avg:148.07ms
step:1291/1375 train_time:189686ms step_avg:148.08ms
step:1292/1375 train_time:189843ms step_avg:148.08ms
step:1293/1375 train_time:190002ms step_avg:148.09ms
step:1294/1375 train_time:190157ms step_avg:148.10ms
step:1295/1375 train_time:190314ms step_avg:148.10ms
step:1296/1375 train_time:190473ms step_avg:148.11ms
step:1297/1375 train_time:190632ms step_avg:148.12ms
step:1298/1375 train_time:190788ms step_avg:148.13ms
step:1299/1375 train_time:190942ms step_avg:148.13ms
step:1300/1375 train_time:191098ms step_avg:148.14ms
step:1301/1375 train_time:191252ms step_avg:148.14ms
step:1302/1375 train_time:191409ms step_avg:148.15ms
step:1303/1375 train_time:191567ms step_avg:148.16ms
step:1304/1375 train_time:191727ms step_avg:148.17ms
step:1305/1375 train_time:191883ms step_avg:148.17ms
step:1306/1375 train_time:192041ms step_avg:148.18ms
step:1307/1375 train_time:192194ms step_avg:148.18ms
step:1308/1375 train_time:192350ms step_avg:148.19ms
step:1309/1375 train_time:192506ms step_avg:148.20ms
step:1310/1375 train_time:192661ms step_avg:148.20ms
step:1311/1375 train_time:192815ms step_avg:148.21ms
step:1312/1375 train_time:192968ms step_avg:148.21ms
step:1313/1375 train_time:193123ms step_avg:148.21ms
step:1314/1375 train_time:193282ms step_avg:148.22ms
step:1315/1375 train_time:193440ms step_avg:148.23ms
step:1316/1375 train_time:193592ms step_avg:148.23ms
step:1317/1375 train_time:193747ms step_avg:148.24ms
step:1318/1375 train_time:193908ms step_avg:148.25ms
step:1319/1375 train_time:194064ms step_avg:148.25ms
step:1320/1375 train_time:194221ms step_avg:148.26ms
step:1321/1375 train_time:194377ms step_avg:148.27ms
step:1322/1375 train_time:194537ms step_avg:148.28ms
step:1323/1375 train_time:194691ms step_avg:148.28ms
step:1324/1375 train_time:194846ms step_avg:148.28ms
step:1325/1375 train_time:195003ms step_avg:148.29ms
step:1326/1375 train_time:195162ms step_avg:148.30ms
step:1327/1375 train_time:195316ms step_avg:148.30ms
step:1328/1375 train_time:195470ms step_avg:148.31ms
step:1329/1375 train_time:195646ms step_avg:148.33ms
step:1330/1375 train_time:195806ms step_avg:148.34ms
step:1331/1375 train_time:196007ms step_avg:148.38ms
step:1332/1375 train_time:196167ms step_avg:148.39ms
step:1333/1375 train_time:196324ms step_avg:148.39ms
step:1334/1375 train_time:196480ms step_avg:148.40ms
step:1335/1375 train_time:196631ms step_avg:148.40ms
step:1336/1375 train_time:196797ms step_avg:148.41ms
step:1337/1375 train_time:196955ms step_avg:148.42ms
step:1338/1375 train_time:197111ms step_avg:148.43ms
step:1339/1375 train_time:197271ms step_avg:148.44ms
step:1340/1375 train_time:197430ms step_avg:148.44ms
step:1341/1375 train_time:197586ms step_avg:148.45ms
step:1342/1375 train_time:197745ms step_avg:148.46ms
step:1343/1375 train_time:197903ms step_avg:148.46ms
step:1344/1375 train_time:198060ms step_avg:148.47ms
step:1345/1375 train_time:198217ms step_avg:148.48ms
step:1346/1375 train_time:198374ms step_avg:148.48ms
step:1347/1375 train_time:198530ms step_avg:148.49ms
step:1348/1375 train_time:198687ms step_avg:148.50ms
step:1349/1375 train_time:198844ms step_avg:148.50ms
step:1350/1375 train_time:198996ms step_avg:148.50ms
step:1351/1375 train_time:199151ms step_avg:148.51ms
step:1352/1375 train_time:199314ms step_avg:148.52ms
step:1353/1375 train_time:199473ms step_avg:148.53ms
step:1354/1375 train_time:199632ms step_avg:148.54ms
step:1355/1375 train_time:199789ms step_avg:148.54ms
step:1356/1375 train_time:199945ms step_avg:148.55ms
step:1357/1375 train_time:200103ms step_avg:148.55ms
step:1358/1375 train_time:200261ms step_avg:148.56ms
step:1359/1375 train_time:200415ms step_avg:148.57ms
step:1360/1375 train_time:200575ms step_avg:148.57ms
step:1361/1375 train_time:200735ms step_avg:148.58ms
step:1362/1375 train_time:200894ms step_avg:148.59ms
step:1363/1375 train_time:201057ms step_avg:148.60ms
step:1364/1375 train_time:201211ms step_avg:148.61ms
step:1365/1375 train_time:201366ms step_avg:148.61ms
step:1366/1375 train_time:201523ms step_avg:148.62ms
step:1367/1375 train_time:201677ms step_avg:148.62ms
step:1368/1375 train_time:201834ms step_avg:148.63ms
step:1369/1375 train_time:201999ms step_avg:148.64ms
step:1370/1375 train_time:202159ms step_avg:148.65ms
step:1371/1375 train_time:202314ms step_avg:148.65ms
step:1372/1375 train_time:202475ms step_avg:148.66ms
step:1373/1375 train_time:202630ms step_avg:148.66ms
step:1374/1375 train_time:202788ms step_avg:148.67ms
step:1375/1375 train_time:202944ms step_avg:148.68ms
step:1375/1375 val_loss:3.2797 train_time:203019ms step_avg:148.73ms
peak memory consumption: 31565 MiB

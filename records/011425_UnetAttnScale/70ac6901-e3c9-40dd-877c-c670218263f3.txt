import os
import sys
with open(sys.argv[0]) as f:
    code = f.read() # read the code of this file ASAP, for logging
import uuid
import time
import glob
import subprocess
import contextlib
from dataclasses import dataclass

import torch
torch.empty(1, device='cuda', requires_grad=True).backward()
from torch import nn
import torch.nn.functional as F
import torch.distributed as dist
from torch.nn.parallel import DistributedDataParallel as DDP
# use of FlexAttention contributed by @KoszarskyB
from torch.nn.attention.flex_attention import BlockMask, flex_attention

# -----------------------------------------------------------------------------
# Muon optimizer

@torch.compile
def zeropower_via_newtonschulz5(G, steps):
    """
    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a
    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose
    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at
    zero even beyond the point where the iteration no longer converges all the way to one everywhere
    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T
    where S' is diagonal with S_{ii}' ~ Uniform(0.5, 1.5), which turns out not to hurt model
    performance at all relative to UV^T, where USV^T = G is the SVD.
    """
    assert len(G.shape) == 2
    a, b, c = (3.4445, -4.7750,  2.0315)
    X = G.bfloat16()
    if G.size(0) > G.size(1):
        X = X.T

    # Ensure spectral norm is at most 1
    X = X / (X.norm() + 1e-7)
    # Perform the NS iterations
    for _ in range(steps):
        A = X @ X.T
        B = b * A + c * A @ A # adapted from suggestion by @jxbz, @leloykun, and @YouJiacheng
        X = a * X + B @ X
    
    if G.size(0) > G.size(1):
        X = X.T
    return X

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Some warnings:
    - This optimizer assumes that all parameters passed in are 2D.
    - It should not be used for the embedding layer, the final fully connected layer, or any {0,1}-D
    parameters; those should all be optimized by a standard method (e.g., AdamW).
    - To use it with 4D convolutional filters, it works well to just flatten their last 3 dimensions.
    - We believe it is unlikely to work well for training with small batch size.
    - We believe it may not work well for finetuning pretrained models, but we haven't tested this.
    - We have not yet tried this optimizer for training scenarios larger than NanoGPT (124M).

    Arguments:
        lr: The learning rate used by the internal SGD.
        momentum: The momentum used by the internal SGD.
        nesterov: Whether to use Nesterov-style momentum in the internal SGD. (recommended)
        ns_steps: The number of Newton-Schulz iteration steps to use.
    """
    def __init__(self, params, lr=0.02, momentum=0.95, nesterov=True, ns_steps=5):
        self.world_size = int(os.environ['WORLD_SIZE'])
        self.rank = int(os.environ['RANK'])
        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, ns_steps=ns_steps)
        assert all(isinstance(p, torch.Tensor) for p in params)
        sizes = {p.numel() for p in params}
        param_groups = [dict(params=[p for p in params if p.numel() == size],
                             update_buffer=[torch.empty(size, device='cuda', dtype=torch.bfloat16) for _ in range(self.world_size)])
                        for size in sizes]
        super().__init__(param_groups, defaults)

    def step(self):

        for group in self.param_groups:

            lr = group['lr']
            momentum = group['momentum']
            nesterov = group['nesterov']
            ns_steps = group['ns_steps']
            update_buffers = group['update_buffer']
            # generate weight updates in distributed fashion
            params = group['params']
            handle = None
            params_world = None
            def update_prev():
                if params_world is None:
                    return
                assert handle is not None
                handle.wait()
                for p_world, g_world in zip(params_world, update_buffers):
                    p_world.data.add_(
                        g_world.view_as(p_world),
                        alpha=-lr * max(1, p_world.size(0) / p_world.size(1)) ** 0.5,
                    )
            for base_i in range(len(params))[::self.world_size]:
                if base_i + rank < len(params):
                    p = params[base_i + self.rank]
                    g = p.grad
                    assert g is not None
                    state = self.state[p]
                    if 'momentum_buffer' not in state:
                        state['momentum_buffer'] = torch.zeros_like(g)
                    buf = state['momentum_buffer']
                    buf.lerp_(g, 1 - momentum)
                    g = g.lerp_(buf, momentum) if nesterov else buf
                    g = zeropower_via_newtonschulz5(g, steps=ns_steps).flatten()
                else:
                    g = update_buffers[rank]
                update_prev() # async all_gather instead of sync all_reduce by @YouJiacheng
                handle = dist.all_gather(update_buffers, g, async_op=True)
                params_world = params[base_i : base_i + self.world_size]
            update_prev()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the GPT-2 model

def norm(x):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):

    def __init__(self, in_features, out_features):
        super().__init__(in_features, out_features, bias=False)

    def forward(self, x):
        return F.linear(x, self.weight.type_as(x))

class Rotary(nn.Module):

    def __init__(self, dim, max_seq_len=65536):
        super().__init__()
        # half-truncate RoPE by @YouJiacheng
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=dim//4, dtype=torch.float32)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(dim//4)])
        t = torch.arange(max_seq_len, dtype=torch.float32)
        theta = torch.einsum('i,j -> ij', t, angular_freq)
        self.cos = nn.Buffer(theta.cos(), persistent=False)
        self.sin = nn.Buffer(theta.sin(), persistent=False)

    def forward(self, x):
        cos, sin = self.cos[None, :x.size(-3), None, :], self.sin[None, :x.size(-3), None, :]
        x1, x2 = x.float().chunk(2, dim=-1)
        y1 = x1 * cos + x2 * sin
        y2 = x1 * (-sin) + x2 * cos
        return torch.cat((y1, y2), 3).type_as(x)

class CausalSelfAttention(nn.Module):

    def __init__(self, dim, num_heads, layer_id):
        super().__init__()
        assert dim % num_heads == 0
        self.num_heads = num_heads
        self.layer_id = layer_id
        self.c_q = CastedLinear(dim, dim)
        self.c_k = CastedLinear(dim, dim)
        self.c_v = CastedLinear(dim, dim)
        self.lambdas = nn.Parameter(torch.tensor([0.5, 0.5]))
        self.rotary = Rotary(dim // num_heads) # dim // num_heads = head_dim
        self.c_proj = CastedLinear(dim, dim)
        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977
        # self.attn_scale = nn.Parameter(torch.tensor(1.0 / (dim // num_heads) ** 0.5))
        # self.attn_scale = nn.Parameter(torch.tensor(0.20))
        self.attn_scale = 0.13 + 0.01 * min(layer_id, 11 - layer_id)

    def forward(self, x, ve, block_mask):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, 'Must use batch size = 1 for FlexAttention'
        q = self.c_q(x).view(B, T, self.num_heads, -1)
        k = self.c_k(x).view(B, T, self.num_heads, -1)
        v = self.c_v(x).view(B, T, self.num_heads, -1)
        if ve is not None:
            v = self.lambdas[0] * v + self.lambdas[1] * ve.view_as(v) # @KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = self.lambdas[0] * v
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = self.rotary(q), self.rotary(k)
        # y = flex_attention(q.transpose(1, 2) * self.attn_scale, k.transpose(1, 2), v.transpose(1, 2), block_mask=block_mask, scale=1.)
        y = flex_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), block_mask=block_mask, scale=self.attn_scale)
        y = y.transpose(1, 2).contiguous().view_as(x) # re-assemble all head outputs side by side
        y = self.c_proj(y)
        return y

class MLP(nn.Module):

    def __init__(self, dim):
        super().__init__()
        self.c_fc = CastedLinear(dim, 4 * dim)
        self.c_proj = CastedLinear(4 * dim, dim)
        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977

    def forward(self, x):
        x = self.c_fc(x)
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = self.c_proj(x)
        return x

class Block(nn.Module):

    def __init__(self, model_dim, num_heads, layer_id, use_attn=True):
        super().__init__()
        self.attn = CausalSelfAttention(model_dim, num_heads, layer_id) if use_attn else None
        self.mlp = MLP(model_dim)
        self.lambdas = nn.Parameter(torch.tensor([1., 0.]))

    def forward(self, x, ve, x0, block_mask):
        x = self.lambdas[0] * x + self.lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), ve, block_mask)
        x = x + self.mlp(norm(x))
        return x

class ValueEmbedding(nn.Module):
    def __init__(self, vocab_size, model_dim):
        super().__init__()
        self.embed = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])

    def forward(self, inputs):
        ve = [emb(inputs).bfloat16() for emb in self.embed]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        ve = [ve[0], ve[1], ve[2], None, None, None, None, None, None, ve[0], ve[1], ve[2]]
        return ve

# -----------------------------------------------------------------------------
# The main GPT-2 model

class GPT(nn.Module):

    def __init__(self, vocab_size, num_layers, num_heads, model_dim):
        super().__init__()
        self.embed = nn.Embedding(vocab_size, model_dim)
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.blocks = nn.ModuleList([Block(model_dim, num_heads, layer_id=layer_id, use_attn=(layer_id != 7))
                                     for layer_id in range(num_layers)])
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual learning
        # U-net structure on token value embeddings by @leloykun
        self.value_embeds = ValueEmbedding(vocab_size, model_dim)
        self.lm_head = CastedLinear(model_dim, vocab_size)
        self.lm_head.weight.data.zero_() # @Grad62304977
        # U-net design by @brendanh0gan
        self.num_encoder_layers = num_layers // 2 # Half of the layers for encoder
        self.num_decoder_layers = num_layers - self.num_encoder_layers # Remaining for decoder
        # Add learnable skip connection weights for decoder layers
        self.skip_weights = nn.Parameter(torch.ones(self.num_decoder_layers))

    def forward(self, inputs, targets, sliding_window_num_blocks):
        BLOCK_SIZE = 128
        seq_len = len(inputs)
        assert seq_len % BLOCK_SIZE == 0
        total_num_blocks = seq_len // BLOCK_SIZE
        assert inputs.ndim == 1
        docs = (inputs == 50256).cumsum(0)
        docs_low = docs.view(-1, BLOCK_SIZE)[:, 0].contiguous()
        docs_high = docs.view(-1, BLOCK_SIZE)[:, -1].contiguous()

        def document_causal(b, h, q_idx, kv_idx):
            causal_mask = q_idx >= kv_idx
            document_mask = docs[q_idx] == docs[kv_idx]
            return causal_mask & document_mask

        def dense_to_ordered(dense_mask):
            num_blocks = dense_mask.sum(dim=-1, dtype=torch.int32)
            indices = dense_mask.argsort(dim=-1, descending=True, stable=True).to(torch.int32)
            return num_blocks[None, None].contiguous(), indices[None, None].contiguous()

        def create_doc_swc_block_mask(sliding_window_num_blocks):
            kv_idx = block_idx = torch.arange(total_num_blocks, dtype=torch.int32, device='cuda')
            q_idx = block_idx[:, None]
            causal_bm = q_idx >= kv_idx
            causal_full_bm = q_idx > kv_idx
            window_bm = q_idx - kv_idx < sliding_window_num_blocks
            window_full_bm = window_bm # block-wise sliding window by @YouJiacheng
            # document_bm = (docs_low[q_idx] <= docs_high[kv_idx]) & (docs_low[kv_idx] <= docs_high[q_idx])
            document_bm = (docs_low[:, None] <= docs_high) & (docs_low <= docs_high[:, None])
            document_full_bm = (docs_low[:, None] == docs_high) & (docs_low == docs_high[:, None])
            nonzero_bm = causal_bm & window_bm & document_bm
            full_bm  = causal_full_bm & window_full_bm & document_full_bm
            kv_num_blocks, kv_indices = dense_to_ordered(nonzero_bm & ~full_bm)
            full_kv_num_blocks, full_kv_indices = dense_to_ordered(full_bm)
            return BlockMask.from_kv_blocks(
                kv_num_blocks,
                kv_indices,
                full_kv_num_blocks,
                full_kv_indices,
                BLOCK_SIZE=BLOCK_SIZE,
                mask_mod=document_causal,
            )

        block_mask = create_doc_swc_block_mask(sliding_window_num_blocks)

        x0 = norm(self.embed(inputs[None]).bfloat16()) # use of norm here by @Grad62304977
        x = x0
        ve = self.value_embeds(inputs)
        assert len(ve) == len(self.blocks)
        ve_enc, ve_dec = ve[:self.num_encoder_layers], ve[self.num_encoder_layers:]

        # Store outputs for U-Net skip connections
        skip_connections = []
        # Encoder pass - process only the first half of the blocks
        for i in range(self.num_encoder_layers):
            x = self.blocks[i](x, ve_enc[i], x0, block_mask)
            skip_connections.append(x)
        # Decoder pass - process the remaining blocks with weighted skip connections
        for i in range(self.num_decoder_layers):
            x = x + self.skip_weights[i] * skip_connections.pop()
            # U-net structure on token value embeddings by @leloykun
            x = self.blocks[self.num_encoder_layers + i](x, ve_dec[i], x0, block_mask)

        x = norm(x)
        logits = self.lm_head(x)
        logits = 15 * torch.tanh(logits / 15) # @Grad62304977 added tanh softcapping, @KoszarskyB reduced it from 30 to 15
        logits = logits.float()
        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets)
        return loss

# -----------------------------------------------------------------------------
# Our own simple Distributed Data Loader

def _load_data_shard(path):
    # only reads the header, returns header data
    # header is 256 int32
    header = torch.from_file(path, False, 256, dtype=torch.int32)
    assert header[0] == 20240520, 'magic number mismatch in the data .bin file'
    assert header[1] == 1, 'unsupported version'
    num_tokens = int(header[2]) # number of tokens (claimed)
    with open(path, 'rb', buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, 'number of tokens read does not match header'
    return tokens

class DistributedDataLoader:

    def __init__(self, filename_pattern):
        self.rank = int(os.environ['RANK'])
        self.world_size = int(os.environ['WORLD_SIZE'])
        self.files = sorted(glob.glob(filename_pattern))
        self.reset()

    def reset(self):
        self.current_shard = -1
        self.advance()

    def advance(self):
        self.current_shard = (self.current_shard + 1) % len(self.files)
        self.current_position = 0
        self.tokens = _load_data_shard(self.files[self.current_shard])

    def next_batch(self, batch_size):
        assert batch_size % self.world_size == 0
        device_batch_size = batch_size // self.world_size
        # load next shard if necessary
        if self.current_position + batch_size + 1 >= len(self.tokens):
            self.advance()
        pos = self.current_position + self.rank * device_batch_size
        device_batch_tokens = self.tokens[pos:pos+device_batch_size+1]
        # advance current position
        self.current_position += batch_size
        inputs = device_batch_tokens[:-1].to(device='cuda', dtype=torch.int32, non_blocking=True)
        targets = device_batch_tokens[1:].to(device='cuda', dtype=torch.int64, non_blocking=True)
        return inputs, targets

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files = 'data/fineweb10B/fineweb_train_*.bin' # input .bin to train on
    val_files = 'data/fineweb10B/fineweb_val_*.bin' # input .bin to eval validation loss on
    val_tokens = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # optimization
    batch_size = 8*64*1024 # batch size in tokens
    num_iterations = 1375 # number of iterations to run
    cooldown_frac = 0.4 # fraction of training spent cooling down the learning rate
    bf16_embeds = True
    # evaluation and logging
    val_loss_every = 125 # every how many steps to evaluate val loss? 0 for only at the end
    # implementation
    max_device_batch_size = 64*1024 # batch size per device in tokens
    save_checkpoint = False
args = Hyperparameters()

micro_bs = args.max_device_batch_size

# set up DDP (distributed data parallel). torchrun sets this env variable
rank = int(os.environ['RANK'])
local_rank = int(os.environ['LOCAL_RANK'])
world_size = int(os.environ['WORLD_SIZE'])
assert torch.cuda.is_available()
torch.cuda.set_device(local_rank)
dist.init_process_group(backend='nccl', device_id=torch.device(local_rank))
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = uuid.uuid4()
    os.makedirs('logs', exist_ok=True)
    logfile = f'logs/{run_id}.txt'
    print(logfile)

def print0(s, console=False):
    if master_process:
        with open(logfile, 'a') as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0('='*100)
# log information about the hardware/software environment this is running on
print0(f'Running Python {sys.version}')
print0(f'Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}')
print0(subprocess.run(['nvidia-smi'], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout)
print0('='*100)

# load data
train_loader = DistributedDataLoader(args.train_files)
val_loader = DistributedDataLoader(args.val_files)
print0(f'Training dataloader files: {train_loader.files}')
print0(f'Validation dataloader files: {val_loader.files}')
print0('='*100)

# there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency. suggested to me by @Grad62304977.
# this originates from Karpathy's experiments.
model = GPT(vocab_size=50304, num_layers=12, num_heads=6, model_dim=768)
model = model.cuda()
if args.bf16_embeds:
    for m in model.modules():
        if isinstance(m, nn.Embedding):
            m.bfloat16()
model: GPT = torch.compile(model)
ddp_model = DDP(model, device_ids=[local_rank], broadcast_buffers=False, gradient_as_bucket_view=True)

# collect the parameters to optimize
hidden_matrix_params = [p for p in model.blocks.parameters() if p.ndim == 2]
embed_params = [model.embed.weight, *model.value_embeds.parameters()]
scalar_params = [p for n, p in model.named_parameters() if p.ndim < 2 and "attn_scale" not in n]
attn_scale_params = [p for n, p in model.named_parameters() if p.ndim < 2 and "attn_scale" in n]
head_params = [model.lm_head.weight]

# init the optimizer(s)
optimizer1 = torch.optim.Adam([dict(params=embed_params, lr=0.6),
                               dict(params=head_params, lr=0.008),
                               dict(params=scalar_params, lr=0.04),
                               dict(params=attn_scale_params, lr=0.01)],
                              betas=(0.8, 0.95), fused=True)
optimizer2 = Muon(hidden_matrix_params, lr=0.05, momentum=0.95)
optimizers = [optimizer1, optimizer2]

# learning rate schedule: stable then decay
def get_lr(it):
    t = 1 - it / args.num_iterations # time remaining in training
    assert 1 >= t > 0
    # 1) constant lr for first part of training
    if t >= args.cooldown_frac:
        return 1.0
    # 2) then linear cooldown
    else:
        return t / args.cooldown_frac
schedulers = [torch.optim.lr_scheduler.LambdaLR(opt, get_lr) for opt in optimizers]

# sliding window size schedule: linear increase over training in chunks of 128 from 128 -> 1792. By @fernbear.bsky.social
def get_sliding_window_blocks(it):
    x = it / args.num_iterations # training progress
    assert 0 <= x <= 1
    return int(((1 - x) * 128 + x * 1856) // 128)
sliding_window_num_blocks = torch.tensor(1, dtype=torch.int32, device='cuda')

# Start training loop
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    # This effectively ignores timing first 10 steps, which are slower for weird reasons.
    # Alternately, and slightly more correctly in terms of benchmarking, we could do 10
    # steps with dummy data first, and then re-initialize the model and reset the loader.
    if step == 10:
        training_time_ms = 0
        t0 = time.perf_counter()
    timed_steps = float('nan') if step <= 11 else (step - 10) + 1 # <= 11 to avoid bug in val

    sliding_window_num_blocks.copy_(get_sliding_window_blocks(step))

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        # run validation batches
        model.eval()
        val_loader.reset()
        val_loss = 0.0
        # calculate the number of steps to take in the val loop.
        val_batch_size = world_size * micro_bs
        assert args.val_tokens % val_batch_size == 0
        val_steps = args.val_tokens // val_batch_size
        for _ in range(val_steps):
            with torch.no_grad():
                inputs_val, targets_val = val_loader.next_batch(val_batch_size)
                val_loss += ddp_model(inputs_val, targets_val, sliding_window_num_blocks)
        # Print attention scales
        for n, p in model.named_parameters():
            if p.ndim < 2 and "attn_scale" in n:
                print0(f'{n}: {p.item()}', console=True)
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        val_loss /= val_steps
        # logging
        print0(f'step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/(timed_steps-1):.2f}ms', console=True)
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f'logs/{run_id}', exist_ok=True)
            torch.save(log, f'logs/{run_id}/state_step{step:06d}.pt')
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    model.train()
    batch_size = args.batch_size
    assert batch_size % world_size == 0
    inputs_train, targets_train = train_loader.next_batch(batch_size)
    assert len(inputs_train) <= micro_bs or len(inputs_train) % micro_bs == 0
    for micro_inputs_train, micro_targets_train in zip(inputs_train.split(micro_bs), targets_train.split(micro_bs)):
        ddp_model(micro_inputs_train, micro_targets_train, sliding_window_num_blocks).backward()
    # momentum warmup for Muon
    frac = min(step/300, 1)
    for group in optimizer2.param_groups:
        group['momentum'] = (1 - frac) * 0.85 + frac * 0.95
    # step the optimizers and schedulers
    for opt, sched in zip(optimizers, schedulers):
        opt.step()
        if step != train_steps-1:
            sched.step()
    # null the gradients
    model.zero_grad(set_to_none=True)
    # logging
    approx_time = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f'step:{step+1}/{train_steps} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms', console=True)

print0(f'peak memory consumption: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB')
dist.destroy_process_group()

====================================================================================================
Running Python 3.10.12 (main, Sep 11 2024, 15:47:36) [GCC 11.4.0]
Running PyTorch 2.6.0.dev20241231+cu126 compiled for CUDA 12.6
Tue Jan 14 15:49:13 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.183.06             Driver Version: 535.183.06   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:19:00.0 Off |                    0 |
| N/A   38C    P0             126W / 700W |   7713MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:3B:00.0 Off |                    0 |
| N/A   32C    P0             114W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:4C:00.0 Off |                    0 |
| N/A   29C    P0             118W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   37C    P0             129W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:9B:00.0 Off |                    0 |
| N/A   38C    P0             123W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:BB:00.0 Off |                    0 |
| N/A   30C    P0             115W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:CB:00.0 Off |                    0 |
| N/A   36C    P0             116W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:DB:00.0 Off |                    0 |
| N/A   29C    P0             117W / 700W |   3211MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
Training dataloader files: ['data/fineweb10B/fineweb_train_000001.bin', 'data/fineweb10B/fineweb_train_000002.bin', 'data/fineweb10B/fineweb_train_000003.bin', 'data/fineweb10B/fineweb_train_000004.bin', 'data/fineweb10B/fineweb_train_000005.bin', 'data/fineweb10B/fineweb_train_000006.bin', 'data/fineweb10B/fineweb_train_000007.bin', 'data/fineweb10B/fineweb_train_000008.bin', 'data/fineweb10B/fineweb_train_000009.bin']
Validation dataloader files: ['data/fineweb10B/fineweb_val_000000.bin']
====================================================================================================
step:0/1375 val_loss:10.8258 train_time:0ms step_avg:nanms
step:1/1375 train_time:27464ms step_avg:nanms
step:2/1375 train_time:27551ms step_avg:nanms
step:3/1375 train_time:27732ms step_avg:nanms
step:4/1375 train_time:27865ms step_avg:nanms
step:5/1375 train_time:27998ms step_avg:nanms
step:6/1375 train_time:28131ms step_avg:nanms
step:7/1375 train_time:28264ms step_avg:nanms
step:8/1375 train_time:28396ms step_avg:nanms
step:9/1375 train_time:28530ms step_avg:nanms
step:10/1375 train_time:28670ms step_avg:nanms
step:11/1375 train_time:136ms step_avg:nanms
step:12/1375 train_time:272ms step_avg:nanms
step:13/1375 train_time:406ms step_avg:135.48ms
step:14/1375 train_time:540ms step_avg:135.03ms
step:15/1375 train_time:675ms step_avg:135.10ms
step:16/1375 train_time:809ms step_avg:134.86ms
step:17/1375 train_time:945ms step_avg:135.06ms
step:18/1375 train_time:1084ms step_avg:135.47ms
step:19/1375 train_time:1221ms step_avg:135.62ms
step:20/1375 train_time:1358ms step_avg:135.83ms
step:21/1375 train_time:1492ms step_avg:135.63ms
step:22/1375 train_time:1628ms step_avg:135.65ms
step:23/1375 train_time:1762ms step_avg:135.55ms
step:24/1375 train_time:1897ms step_avg:135.49ms
step:25/1375 train_time:2032ms step_avg:135.46ms
step:26/1375 train_time:2168ms step_avg:135.52ms
step:27/1375 train_time:2305ms step_avg:135.56ms
step:28/1375 train_time:2440ms step_avg:135.54ms
step:29/1375 train_time:2576ms step_avg:135.58ms
step:30/1375 train_time:2712ms step_avg:135.61ms
step:31/1375 train_time:2845ms step_avg:135.49ms
step:32/1375 train_time:2981ms step_avg:135.50ms
step:33/1375 train_time:3116ms step_avg:135.50ms
step:34/1375 train_time:3252ms step_avg:135.50ms
step:35/1375 train_time:3387ms step_avg:135.48ms
step:36/1375 train_time:3523ms step_avg:135.50ms
step:37/1375 train_time:3658ms step_avg:135.49ms
step:38/1375 train_time:3794ms step_avg:135.51ms
step:39/1375 train_time:3929ms step_avg:135.48ms
step:40/1375 train_time:4064ms step_avg:135.46ms
step:41/1375 train_time:4200ms step_avg:135.47ms
step:42/1375 train_time:4336ms step_avg:135.50ms
step:43/1375 train_time:4472ms step_avg:135.51ms
step:44/1375 train_time:4607ms step_avg:135.51ms
step:45/1375 train_time:4742ms step_avg:135.49ms
step:46/1375 train_time:4880ms step_avg:135.55ms
step:47/1375 train_time:5015ms step_avg:135.54ms
step:48/1375 train_time:5149ms step_avg:135.50ms
step:49/1375 train_time:5284ms step_avg:135.49ms
step:50/1375 train_time:5418ms step_avg:135.45ms
step:51/1375 train_time:5554ms step_avg:135.45ms
step:52/1375 train_time:5689ms step_avg:135.45ms
step:53/1375 train_time:5825ms step_avg:135.46ms
step:54/1375 train_time:5960ms step_avg:135.45ms
step:55/1375 train_time:6095ms step_avg:135.44ms
step:56/1375 train_time:6230ms step_avg:135.43ms
step:57/1375 train_time:6365ms step_avg:135.44ms
step:58/1375 train_time:6501ms step_avg:135.44ms
step:59/1375 train_time:6638ms step_avg:135.47ms
step:60/1375 train_time:6773ms step_avg:135.47ms
step:61/1375 train_time:6908ms step_avg:135.46ms
step:62/1375 train_time:7044ms step_avg:135.45ms
step:63/1375 train_time:7179ms step_avg:135.45ms
step:64/1375 train_time:7314ms step_avg:135.45ms
step:65/1375 train_time:7449ms step_avg:135.43ms
step:66/1375 train_time:7583ms step_avg:135.41ms
step:67/1375 train_time:7720ms step_avg:135.43ms
step:68/1375 train_time:7855ms step_avg:135.44ms
step:69/1375 train_time:7991ms step_avg:135.44ms
step:70/1375 train_time:8128ms step_avg:135.46ms
step:71/1375 train_time:8263ms step_avg:135.46ms
step:72/1375 train_time:8399ms step_avg:135.47ms
step:73/1375 train_time:8536ms step_avg:135.49ms
step:74/1375 train_time:8671ms step_avg:135.48ms
step:75/1375 train_time:8806ms step_avg:135.47ms
step:76/1375 train_time:8941ms step_avg:135.48ms
step:77/1375 train_time:9078ms step_avg:135.49ms
step:78/1375 train_time:9214ms step_avg:135.49ms
step:79/1375 train_time:9349ms step_avg:135.49ms
step:80/1375 train_time:9483ms step_avg:135.47ms
step:81/1375 train_time:9620ms step_avg:135.49ms
step:82/1375 train_time:9754ms step_avg:135.47ms
step:83/1375 train_time:9888ms step_avg:135.46ms
step:84/1375 train_time:10025ms step_avg:135.47ms
step:85/1375 train_time:10160ms step_avg:135.46ms
step:86/1375 train_time:10295ms step_avg:135.45ms
step:87/1375 train_time:10430ms step_avg:135.45ms
step:88/1375 train_time:10567ms step_avg:135.48ms
step:89/1375 train_time:10704ms step_avg:135.50ms
step:90/1375 train_time:10839ms step_avg:135.48ms
step:91/1375 train_time:10975ms step_avg:135.49ms
step:92/1375 train_time:11111ms step_avg:135.50ms
step:93/1375 train_time:11246ms step_avg:135.49ms
step:94/1375 train_time:11381ms step_avg:135.49ms
step:95/1375 train_time:11516ms step_avg:135.48ms
step:96/1375 train_time:11652ms step_avg:135.49ms
step:97/1375 train_time:11787ms step_avg:135.48ms
step:98/1375 train_time:11923ms step_avg:135.49ms
step:99/1375 train_time:12058ms step_avg:135.48ms
step:100/1375 train_time:12194ms step_avg:135.49ms
step:101/1375 train_time:12330ms step_avg:135.49ms
step:102/1375 train_time:12465ms step_avg:135.49ms
step:103/1375 train_time:12604ms step_avg:135.52ms
step:104/1375 train_time:12743ms step_avg:135.57ms
step:105/1375 train_time:12882ms step_avg:135.60ms
step:106/1375 train_time:13021ms step_avg:135.64ms
step:107/1375 train_time:13158ms step_avg:135.65ms
step:108/1375 train_time:13297ms step_avg:135.68ms
step:109/1375 train_time:13435ms step_avg:135.70ms
step:110/1375 train_time:13574ms step_avg:135.74ms
step:111/1375 train_time:13713ms step_avg:135.77ms
step:112/1375 train_time:13853ms step_avg:135.82ms
step:113/1375 train_time:13992ms step_avg:135.85ms
step:114/1375 train_time:14133ms step_avg:135.89ms
step:115/1375 train_time:14270ms step_avg:135.91ms
step:116/1375 train_time:14408ms step_avg:135.92ms
step:117/1375 train_time:14546ms step_avg:135.94ms
step:118/1375 train_time:14687ms step_avg:135.99ms
step:119/1375 train_time:14827ms step_avg:136.03ms
step:120/1375 train_time:14967ms step_avg:136.06ms
step:121/1375 train_time:15106ms step_avg:136.09ms
step:122/1375 train_time:15244ms step_avg:136.11ms
step:123/1375 train_time:15383ms step_avg:136.13ms
step:124/1375 train_time:15521ms step_avg:136.15ms
step:125/1375 train_time:15660ms step_avg:136.18ms
step:125/1375 val_loss:4.3637 train_time:15727ms step_avg:136.75ms
step:126/1375 train_time:15801ms step_avg:136.21ms
step:127/1375 train_time:15945ms step_avg:136.28ms
step:128/1375 train_time:16082ms step_avg:136.29ms
step:129/1375 train_time:16220ms step_avg:136.30ms
step:130/1375 train_time:16358ms step_avg:136.31ms
step:131/1375 train_time:16494ms step_avg:136.31ms
step:132/1375 train_time:16631ms step_avg:136.32ms
step:133/1375 train_time:16773ms step_avg:136.37ms
step:134/1375 train_time:16913ms step_avg:136.40ms
step:135/1375 train_time:17053ms step_avg:136.43ms
step:136/1375 train_time:17193ms step_avg:136.45ms
step:137/1375 train_time:17329ms step_avg:136.45ms
step:138/1375 train_time:17468ms step_avg:136.47ms
step:139/1375 train_time:17606ms step_avg:136.48ms
step:140/1375 train_time:17745ms step_avg:136.50ms
step:141/1375 train_time:17885ms step_avg:136.52ms
step:142/1375 train_time:18024ms step_avg:136.54ms
step:143/1375 train_time:18165ms step_avg:136.58ms
step:144/1375 train_time:18303ms step_avg:136.59ms
step:145/1375 train_time:18441ms step_avg:136.60ms
step:146/1375 train_time:18579ms step_avg:136.61ms
step:147/1375 train_time:18717ms step_avg:136.62ms
step:148/1375 train_time:18858ms step_avg:136.65ms
step:149/1375 train_time:18997ms step_avg:136.67ms
step:150/1375 train_time:19137ms step_avg:136.69ms
step:151/1375 train_time:19276ms step_avg:136.71ms
step:152/1375 train_time:19415ms step_avg:136.73ms
step:153/1375 train_time:19554ms step_avg:136.74ms
step:154/1375 train_time:19691ms step_avg:136.74ms
step:155/1375 train_time:19829ms step_avg:136.75ms
step:156/1375 train_time:19969ms step_avg:136.77ms
step:157/1375 train_time:20108ms step_avg:136.79ms
step:158/1375 train_time:20248ms step_avg:136.81ms
step:159/1375 train_time:20388ms step_avg:136.83ms
step:160/1375 train_time:20526ms step_avg:136.84ms
step:161/1375 train_time:20665ms step_avg:136.86ms
step:162/1375 train_time:20804ms step_avg:136.87ms
step:163/1375 train_time:20943ms step_avg:136.88ms
step:164/1375 train_time:21081ms step_avg:136.89ms
step:165/1375 train_time:21221ms step_avg:136.91ms
step:166/1375 train_time:21361ms step_avg:136.93ms
step:167/1375 train_time:21501ms step_avg:136.95ms
step:168/1375 train_time:21640ms step_avg:136.96ms
step:169/1375 train_time:21778ms step_avg:136.97ms
step:170/1375 train_time:21917ms step_avg:136.98ms
step:171/1375 train_time:22055ms step_avg:136.99ms
step:172/1375 train_time:22195ms step_avg:137.00ms
step:173/1375 train_time:22334ms step_avg:137.02ms
step:174/1375 train_time:22473ms step_avg:137.03ms
step:175/1375 train_time:22612ms step_avg:137.04ms
step:176/1375 train_time:22751ms step_avg:137.05ms
step:177/1375 train_time:22889ms step_avg:137.06ms
step:178/1375 train_time:23028ms step_avg:137.07ms
step:179/1375 train_time:23167ms step_avg:137.08ms
step:180/1375 train_time:23307ms step_avg:137.10ms
step:181/1375 train_time:23447ms step_avg:137.12ms
step:182/1375 train_time:23587ms step_avg:137.13ms
step:183/1375 train_time:23727ms step_avg:137.15ms
step:184/1375 train_time:23867ms step_avg:137.16ms
step:185/1375 train_time:24005ms step_avg:137.17ms
step:186/1375 train_time:24146ms step_avg:137.19ms
step:187/1375 train_time:24286ms step_avg:137.21ms
step:188/1375 train_time:24426ms step_avg:137.23ms
step:189/1375 train_time:24566ms step_avg:137.24ms
step:190/1375 train_time:24704ms step_avg:137.25ms
step:191/1375 train_time:24883ms step_avg:137.47ms
step:192/1375 train_time:25020ms step_avg:137.47ms
step:193/1375 train_time:25158ms step_avg:137.47ms
step:194/1375 train_time:25297ms step_avg:137.48ms
step:195/1375 train_time:25435ms step_avg:137.49ms
step:196/1375 train_time:25573ms step_avg:137.49ms
step:197/1375 train_time:25713ms step_avg:137.50ms
step:198/1375 train_time:25855ms step_avg:137.53ms
step:199/1375 train_time:25994ms step_avg:137.54ms
step:200/1375 train_time:26133ms step_avg:137.54ms
step:201/1375 train_time:26272ms step_avg:137.55ms
step:202/1375 train_time:26412ms step_avg:137.56ms
step:203/1375 train_time:26551ms step_avg:137.57ms
step:204/1375 train_time:26692ms step_avg:137.59ms
step:205/1375 train_time:26832ms step_avg:137.60ms
step:206/1375 train_time:26975ms step_avg:137.63ms
step:207/1375 train_time:27117ms step_avg:137.65ms
step:208/1375 train_time:27258ms step_avg:137.66ms
step:209/1375 train_time:27399ms step_avg:137.68ms
step:210/1375 train_time:27540ms step_avg:137.70ms
step:211/1375 train_time:27680ms step_avg:137.71ms
step:212/1375 train_time:27824ms step_avg:137.74ms
step:213/1375 train_time:27967ms step_avg:137.77ms
step:214/1375 train_time:28109ms step_avg:137.79ms
step:215/1375 train_time:28251ms step_avg:137.81ms
step:216/1375 train_time:28392ms step_avg:137.83ms
step:217/1375 train_time:28534ms step_avg:137.84ms
step:218/1375 train_time:28676ms step_avg:137.86ms
step:219/1375 train_time:28817ms step_avg:137.88ms
step:220/1375 train_time:28958ms step_avg:137.89ms
step:221/1375 train_time:29100ms step_avg:137.91ms
step:222/1375 train_time:29244ms step_avg:137.94ms
step:223/1375 train_time:29385ms step_avg:137.96ms
step:224/1375 train_time:29527ms step_avg:137.98ms
step:225/1375 train_time:29670ms step_avg:138.00ms
step:226/1375 train_time:29812ms step_avg:138.02ms
step:227/1375 train_time:29954ms step_avg:138.04ms
step:228/1375 train_time:30096ms step_avg:138.05ms
step:229/1375 train_time:30235ms step_avg:138.06ms
step:230/1375 train_time:30377ms step_avg:138.08ms
step:231/1375 train_time:30519ms step_avg:138.09ms
step:232/1375 train_time:30660ms step_avg:138.11ms
step:233/1375 train_time:30802ms step_avg:138.13ms
step:234/1375 train_time:30946ms step_avg:138.15ms
step:235/1375 train_time:31089ms step_avg:138.17ms
step:236/1375 train_time:31230ms step_avg:138.19ms
step:237/1375 train_time:31372ms step_avg:138.20ms
step:238/1375 train_time:31514ms step_avg:138.22ms
step:239/1375 train_time:31656ms step_avg:138.23ms
step:240/1375 train_time:31798ms step_avg:138.25ms
step:241/1375 train_time:31938ms step_avg:138.26ms
step:242/1375 train_time:32079ms step_avg:138.27ms
step:243/1375 train_time:32220ms step_avg:138.28ms
step:244/1375 train_time:32362ms step_avg:138.30ms
step:245/1375 train_time:32504ms step_avg:138.32ms
step:246/1375 train_time:32647ms step_avg:138.34ms
step:247/1375 train_time:32790ms step_avg:138.36ms
step:248/1375 train_time:32933ms step_avg:138.37ms
step:249/1375 train_time:33075ms step_avg:138.39ms
step:250/1375 train_time:33217ms step_avg:138.40ms
step:250/1375 val_loss:3.9569 train_time:33287ms step_avg:138.70ms
step:251/1375 train_time:33362ms step_avg:138.43ms
step:252/1375 train_time:33506ms step_avg:138.46ms
step:253/1375 train_time:33647ms step_avg:138.46ms
step:254/1375 train_time:33788ms step_avg:138.47ms
step:255/1375 train_time:33928ms step_avg:138.48ms
step:256/1375 train_time:34069ms step_avg:138.49ms
step:257/1375 train_time:34211ms step_avg:138.51ms
step:258/1375 train_time:34354ms step_avg:138.53ms
step:259/1375 train_time:34498ms step_avg:138.54ms
step:260/1375 train_time:34640ms step_avg:138.56ms
step:261/1375 train_time:34781ms step_avg:138.57ms
step:262/1375 train_time:34921ms step_avg:138.57ms
step:263/1375 train_time:35063ms step_avg:138.59ms
step:264/1375 train_time:35206ms step_avg:138.61ms
step:265/1375 train_time:35348ms step_avg:138.62ms
step:266/1375 train_time:35492ms step_avg:138.64ms
step:267/1375 train_time:35635ms step_avg:138.66ms
step:268/1375 train_time:35778ms step_avg:138.67ms
step:269/1375 train_time:35919ms step_avg:138.68ms
step:270/1375 train_time:36061ms step_avg:138.70ms
step:271/1375 train_time:36202ms step_avg:138.71ms
step:272/1375 train_time:36344ms step_avg:138.72ms
step:273/1375 train_time:36487ms step_avg:138.73ms
step:274/1375 train_time:36628ms step_avg:138.74ms
step:275/1375 train_time:36771ms step_avg:138.76ms
step:276/1375 train_time:36913ms step_avg:138.77ms
step:277/1375 train_time:37055ms step_avg:138.78ms
step:278/1375 train_time:37196ms step_avg:138.79ms
step:279/1375 train_time:37338ms step_avg:138.80ms
step:280/1375 train_time:37481ms step_avg:138.82ms
step:281/1375 train_time:37623ms step_avg:138.83ms
step:282/1375 train_time:37766ms step_avg:138.85ms
step:283/1375 train_time:37908ms step_avg:138.86ms
step:284/1375 train_time:38049ms step_avg:138.87ms
step:285/1375 train_time:38190ms step_avg:138.87ms
step:286/1375 train_time:38332ms step_avg:138.88ms
step:287/1375 train_time:38475ms step_avg:138.90ms
step:288/1375 train_time:38617ms step_avg:138.91ms
step:289/1375 train_time:38761ms step_avg:138.93ms
step:290/1375 train_time:38902ms step_avg:138.94ms
step:291/1375 train_time:39043ms step_avg:138.94ms
step:292/1375 train_time:39185ms step_avg:138.95ms
step:293/1375 train_time:39327ms step_avg:138.97ms
step:294/1375 train_time:39469ms step_avg:138.98ms
step:295/1375 train_time:39612ms step_avg:138.99ms
step:296/1375 train_time:39755ms step_avg:139.00ms
step:297/1375 train_time:39896ms step_avg:139.01ms
step:298/1375 train_time:40038ms step_avg:139.02ms
step:299/1375 train_time:40180ms step_avg:139.03ms
step:300/1375 train_time:40323ms step_avg:139.05ms
step:301/1375 train_time:40465ms step_avg:139.05ms
step:302/1375 train_time:40607ms step_avg:139.07ms
step:303/1375 train_time:40750ms step_avg:139.08ms
step:304/1375 train_time:40892ms step_avg:139.09ms
step:305/1375 train_time:41033ms step_avg:139.10ms
step:306/1375 train_time:41174ms step_avg:139.10ms
step:307/1375 train_time:41318ms step_avg:139.12ms
step:308/1375 train_time:41464ms step_avg:139.14ms
step:309/1375 train_time:41607ms step_avg:139.15ms
step:310/1375 train_time:41752ms step_avg:139.17ms
step:311/1375 train_time:41897ms step_avg:139.19ms
step:312/1375 train_time:42042ms step_avg:139.21ms
step:313/1375 train_time:42185ms step_avg:139.22ms
step:314/1375 train_time:42328ms step_avg:139.24ms
step:315/1375 train_time:42472ms step_avg:139.25ms
step:316/1375 train_time:42617ms step_avg:139.27ms
step:317/1375 train_time:42761ms step_avg:139.29ms
step:318/1375 train_time:42905ms step_avg:139.30ms
step:319/1375 train_time:43048ms step_avg:139.31ms
step:320/1375 train_time:43193ms step_avg:139.33ms
step:321/1375 train_time:43337ms step_avg:139.35ms
step:322/1375 train_time:43482ms step_avg:139.37ms
step:323/1375 train_time:43626ms step_avg:139.38ms
step:324/1375 train_time:43768ms step_avg:139.39ms
step:325/1375 train_time:43913ms step_avg:139.41ms
step:326/1375 train_time:44058ms step_avg:139.42ms
step:327/1375 train_time:44202ms step_avg:139.44ms
step:328/1375 train_time:44345ms step_avg:139.45ms
step:329/1375 train_time:44490ms step_avg:139.47ms
step:330/1375 train_time:44633ms step_avg:139.48ms
step:331/1375 train_time:44779ms step_avg:139.50ms
step:332/1375 train_time:44924ms step_avg:139.51ms
step:333/1375 train_time:45068ms step_avg:139.53ms
step:334/1375 train_time:45211ms step_avg:139.54ms
step:335/1375 train_time:45355ms step_avg:139.55ms
step:336/1375 train_time:45500ms step_avg:139.57ms
step:337/1375 train_time:45643ms step_avg:139.58ms
step:338/1375 train_time:45788ms step_avg:139.60ms
step:339/1375 train_time:45932ms step_avg:139.61ms
step:340/1375 train_time:46076ms step_avg:139.62ms
step:341/1375 train_time:46220ms step_avg:139.64ms
step:342/1375 train_time:46364ms step_avg:139.65ms
step:343/1375 train_time:46507ms step_avg:139.66ms
step:344/1375 train_time:46650ms step_avg:139.67ms
step:345/1375 train_time:46795ms step_avg:139.69ms
step:346/1375 train_time:46940ms step_avg:139.70ms
step:347/1375 train_time:47085ms step_avg:139.72ms
step:348/1375 train_time:47229ms step_avg:139.73ms
step:349/1375 train_time:47374ms step_avg:139.75ms
step:350/1375 train_time:47521ms step_avg:139.77ms
step:351/1375 train_time:47664ms step_avg:139.78ms
step:352/1375 train_time:47807ms step_avg:139.79ms
step:353/1375 train_time:47952ms step_avg:139.80ms
step:354/1375 train_time:48095ms step_avg:139.81ms
step:355/1375 train_time:48241ms step_avg:139.83ms
step:356/1375 train_time:48387ms step_avg:139.85ms
step:357/1375 train_time:48530ms step_avg:139.86ms
step:358/1375 train_time:48676ms step_avg:139.87ms
step:359/1375 train_time:48820ms step_avg:139.89ms
step:360/1375 train_time:48965ms step_avg:139.90ms
step:361/1375 train_time:49108ms step_avg:139.91ms
step:362/1375 train_time:49250ms step_avg:139.92ms
step:363/1375 train_time:49395ms step_avg:139.93ms
step:364/1375 train_time:49542ms step_avg:139.95ms
step:365/1375 train_time:49687ms step_avg:139.96ms
step:366/1375 train_time:49829ms step_avg:139.97ms
step:367/1375 train_time:49975ms step_avg:139.99ms
step:368/1375 train_time:50120ms step_avg:140.00ms
step:369/1375 train_time:50264ms step_avg:140.01ms
step:370/1375 train_time:50408ms step_avg:140.02ms
step:371/1375 train_time:50550ms step_avg:140.03ms
step:372/1375 train_time:50695ms step_avg:140.04ms
step:373/1375 train_time:50839ms step_avg:140.05ms
step:374/1375 train_time:50985ms step_avg:140.07ms
step:375/1375 train_time:51129ms step_avg:140.08ms
step:375/1375 val_loss:3.7760 train_time:51199ms step_avg:140.27ms
step:376/1375 train_time:51277ms step_avg:140.10ms
step:377/1375 train_time:51422ms step_avg:140.12ms
step:378/1375 train_time:51566ms step_avg:140.12ms
step:379/1375 train_time:51708ms step_avg:140.13ms
step:380/1375 train_time:51852ms step_avg:140.14ms
step:381/1375 train_time:52035ms step_avg:140.26ms
step:382/1375 train_time:52179ms step_avg:140.27ms
step:383/1375 train_time:52322ms step_avg:140.27ms
step:384/1375 train_time:52464ms step_avg:140.28ms
step:385/1375 train_time:52607ms step_avg:140.29ms
step:386/1375 train_time:52750ms step_avg:140.29ms
step:387/1375 train_time:52895ms step_avg:140.31ms
step:388/1375 train_time:53044ms step_avg:140.33ms
step:389/1375 train_time:53186ms step_avg:140.33ms
step:390/1375 train_time:53330ms step_avg:140.34ms
step:391/1375 train_time:53475ms step_avg:140.35ms
step:392/1375 train_time:53619ms step_avg:140.36ms
step:393/1375 train_time:53762ms step_avg:140.37ms
step:394/1375 train_time:53905ms step_avg:140.38ms
step:395/1375 train_time:54051ms step_avg:140.39ms
step:396/1375 train_time:54196ms step_avg:140.40ms
step:397/1375 train_time:54340ms step_avg:140.41ms
step:398/1375 train_time:54484ms step_avg:140.42ms
step:399/1375 train_time:54627ms step_avg:140.43ms
step:400/1375 train_time:54771ms step_avg:140.44ms
step:401/1375 train_time:54916ms step_avg:140.45ms
step:402/1375 train_time:55061ms step_avg:140.46ms
step:403/1375 train_time:55205ms step_avg:140.47ms
step:404/1375 train_time:55349ms step_avg:140.48ms
step:405/1375 train_time:55492ms step_avg:140.49ms
step:406/1375 train_time:55637ms step_avg:140.50ms
step:407/1375 train_time:55782ms step_avg:140.51ms
step:408/1375 train_time:55926ms step_avg:140.52ms
step:409/1375 train_time:56072ms step_avg:140.53ms
step:410/1375 train_time:56221ms step_avg:140.55ms
step:411/1375 train_time:56366ms step_avg:140.56ms
step:412/1375 train_time:56511ms step_avg:140.57ms
step:413/1375 train_time:56659ms step_avg:140.59ms
step:414/1375 train_time:56804ms step_avg:140.60ms
step:415/1375 train_time:56949ms step_avg:140.62ms
step:416/1375 train_time:57097ms step_avg:140.63ms
step:417/1375 train_time:57243ms step_avg:140.65ms
step:418/1375 train_time:57388ms step_avg:140.66ms
step:419/1375 train_time:57533ms step_avg:140.67ms
step:420/1375 train_time:57681ms step_avg:140.69ms
step:421/1375 train_time:57827ms step_avg:140.70ms
step:422/1375 train_time:57973ms step_avg:140.71ms
step:423/1375 train_time:58120ms step_avg:140.73ms
step:424/1375 train_time:58265ms step_avg:140.74ms
step:425/1375 train_time:58410ms step_avg:140.75ms
step:426/1375 train_time:58558ms step_avg:140.76ms
step:427/1375 train_time:58702ms step_avg:140.77ms
step:428/1375 train_time:58850ms step_avg:140.79ms
step:429/1375 train_time:58995ms step_avg:140.80ms
step:430/1375 train_time:59141ms step_avg:140.81ms
step:431/1375 train_time:59287ms step_avg:140.82ms
step:432/1375 train_time:59434ms step_avg:140.84ms
step:433/1375 train_time:59580ms step_avg:140.85ms
step:434/1375 train_time:59724ms step_avg:140.86ms
step:435/1375 train_time:59870ms step_avg:140.87ms
step:436/1375 train_time:60016ms step_avg:140.88ms
step:437/1375 train_time:60163ms step_avg:140.90ms
step:438/1375 train_time:60308ms step_avg:140.91ms
step:439/1375 train_time:60454ms step_avg:140.92ms
step:440/1375 train_time:60599ms step_avg:140.93ms
step:441/1375 train_time:60745ms step_avg:140.94ms
step:442/1375 train_time:60890ms step_avg:140.95ms
step:443/1375 train_time:61037ms step_avg:140.96ms
step:444/1375 train_time:61184ms step_avg:140.98ms
step:445/1375 train_time:61329ms step_avg:140.99ms
step:446/1375 train_time:61477ms step_avg:141.00ms
step:447/1375 train_time:61623ms step_avg:141.01ms
step:448/1375 train_time:61769ms step_avg:141.03ms
step:449/1375 train_time:61916ms step_avg:141.04ms
step:450/1375 train_time:62063ms step_avg:141.05ms
step:451/1375 train_time:62209ms step_avg:141.06ms
step:452/1375 train_time:62357ms step_avg:141.08ms
step:453/1375 train_time:62503ms step_avg:141.09ms
step:454/1375 train_time:62648ms step_avg:141.10ms
step:455/1375 train_time:62794ms step_avg:141.11ms
step:456/1375 train_time:62940ms step_avg:141.12ms
step:457/1375 train_time:63086ms step_avg:141.13ms
step:458/1375 train_time:63231ms step_avg:141.14ms
step:459/1375 train_time:63381ms step_avg:141.16ms
step:460/1375 train_time:63525ms step_avg:141.17ms
step:461/1375 train_time:63672ms step_avg:141.18ms
step:462/1375 train_time:63820ms step_avg:141.19ms
step:463/1375 train_time:63965ms step_avg:141.20ms
step:464/1375 train_time:64110ms step_avg:141.21ms
step:465/1375 train_time:64257ms step_avg:141.22ms
step:466/1375 train_time:64403ms step_avg:141.23ms
step:467/1375 train_time:64549ms step_avg:141.24ms
step:468/1375 train_time:64696ms step_avg:141.26ms
step:469/1375 train_time:64842ms step_avg:141.27ms
step:470/1375 train_time:64987ms step_avg:141.28ms
step:471/1375 train_time:65132ms step_avg:141.29ms
step:472/1375 train_time:65279ms step_avg:141.30ms
step:473/1375 train_time:65424ms step_avg:141.31ms
step:474/1375 train_time:65570ms step_avg:141.31ms
step:475/1375 train_time:65716ms step_avg:141.32ms
step:476/1375 train_time:65864ms step_avg:141.34ms
step:477/1375 train_time:66009ms step_avg:141.35ms
step:478/1375 train_time:66157ms step_avg:141.36ms
step:479/1375 train_time:66303ms step_avg:141.37ms
step:480/1375 train_time:66447ms step_avg:141.38ms
step:481/1375 train_time:66593ms step_avg:141.39ms
step:482/1375 train_time:66740ms step_avg:141.40ms
step:483/1375 train_time:66886ms step_avg:141.41ms
step:484/1375 train_time:67034ms step_avg:141.42ms
step:485/1375 train_time:67181ms step_avg:141.43ms
step:486/1375 train_time:67325ms step_avg:141.44ms
step:487/1375 train_time:67470ms step_avg:141.45ms
step:488/1375 train_time:67619ms step_avg:141.46ms
step:489/1375 train_time:67764ms step_avg:141.47ms
step:490/1375 train_time:67911ms step_avg:141.48ms
step:491/1375 train_time:68058ms step_avg:141.49ms
step:492/1375 train_time:68203ms step_avg:141.50ms
step:493/1375 train_time:68348ms step_avg:141.51ms
step:494/1375 train_time:68494ms step_avg:141.52ms
step:495/1375 train_time:68641ms step_avg:141.53ms
step:496/1375 train_time:68787ms step_avg:141.54ms
step:497/1375 train_time:68932ms step_avg:141.54ms
step:498/1375 train_time:69079ms step_avg:141.56ms
step:499/1375 train_time:69224ms step_avg:141.56ms
step:500/1375 train_time:69370ms step_avg:141.57ms
step:500/1375 val_loss:3.6589 train_time:69443ms step_avg:141.72ms
step:501/1375 train_time:69518ms step_avg:141.58ms
step:502/1375 train_time:69670ms step_avg:141.61ms
step:503/1375 train_time:69816ms step_avg:141.62ms
step:504/1375 train_time:69964ms step_avg:141.63ms
step:505/1375 train_time:70109ms step_avg:141.63ms
step:506/1375 train_time:70253ms step_avg:141.64ms
step:507/1375 train_time:70397ms step_avg:141.64ms
step:508/1375 train_time:70546ms step_avg:141.66ms
step:509/1375 train_time:70693ms step_avg:141.67ms
step:510/1375 train_time:70840ms step_avg:141.68ms
step:511/1375 train_time:70987ms step_avg:141.69ms
step:512/1375 train_time:71136ms step_avg:141.71ms
step:513/1375 train_time:71285ms step_avg:141.72ms
step:514/1375 train_time:71433ms step_avg:141.73ms
step:515/1375 train_time:71580ms step_avg:141.74ms
step:516/1375 train_time:71727ms step_avg:141.75ms
step:517/1375 train_time:71875ms step_avg:141.76ms
step:518/1375 train_time:72023ms step_avg:141.78ms
step:519/1375 train_time:72171ms step_avg:141.79ms
step:520/1375 train_time:72318ms step_avg:141.80ms
step:521/1375 train_time:72466ms step_avg:141.81ms
step:522/1375 train_time:72612ms step_avg:141.82ms
step:523/1375 train_time:72761ms step_avg:141.83ms
step:524/1375 train_time:72909ms step_avg:141.85ms
step:525/1375 train_time:73056ms step_avg:141.86ms
step:526/1375 train_time:73204ms step_avg:141.87ms
step:527/1375 train_time:73352ms step_avg:141.88ms
step:528/1375 train_time:73499ms step_avg:141.89ms
step:529/1375 train_time:73649ms step_avg:141.91ms
step:530/1375 train_time:73795ms step_avg:141.91ms
step:531/1375 train_time:73942ms step_avg:141.92ms
step:532/1375 train_time:74091ms step_avg:141.94ms
step:533/1375 train_time:74238ms step_avg:141.95ms
step:534/1375 train_time:74385ms step_avg:141.96ms
step:535/1375 train_time:74532ms step_avg:141.97ms
step:536/1375 train_time:74681ms step_avg:141.98ms
step:537/1375 train_time:74830ms step_avg:141.99ms
step:538/1375 train_time:74979ms step_avg:142.01ms
step:539/1375 train_time:75128ms step_avg:142.02ms
step:540/1375 train_time:75275ms step_avg:142.03ms
step:541/1375 train_time:75422ms step_avg:142.04ms
step:542/1375 train_time:75571ms step_avg:142.05ms
step:543/1375 train_time:75717ms step_avg:142.06ms
step:544/1375 train_time:75868ms step_avg:142.07ms
step:545/1375 train_time:76014ms step_avg:142.08ms
step:546/1375 train_time:76163ms step_avg:142.10ms
step:547/1375 train_time:76310ms step_avg:142.10ms
step:548/1375 train_time:76459ms step_avg:142.12ms
step:549/1375 train_time:76609ms step_avg:142.13ms
step:550/1375 train_time:76756ms step_avg:142.14ms
step:551/1375 train_time:76903ms step_avg:142.15ms
step:552/1375 train_time:77052ms step_avg:142.16ms
step:553/1375 train_time:77198ms step_avg:142.17ms
step:554/1375 train_time:77347ms step_avg:142.18ms
step:555/1375 train_time:77494ms step_avg:142.19ms
step:556/1375 train_time:77641ms step_avg:142.20ms
step:557/1375 train_time:77790ms step_avg:142.21ms
step:558/1375 train_time:77938ms step_avg:142.22ms
step:559/1375 train_time:78084ms step_avg:142.23ms
step:560/1375 train_time:78231ms step_avg:142.24ms
step:561/1375 train_time:78378ms step_avg:142.25ms
step:562/1375 train_time:78526ms step_avg:142.26ms
step:563/1375 train_time:78673ms step_avg:142.27ms
step:564/1375 train_time:78819ms step_avg:142.27ms
step:565/1375 train_time:78969ms step_avg:142.29ms
step:566/1375 train_time:79116ms step_avg:142.29ms
step:567/1375 train_time:79265ms step_avg:142.31ms
step:568/1375 train_time:79412ms step_avg:142.32ms
step:569/1375 train_time:79561ms step_avg:142.33ms
step:570/1375 train_time:79709ms step_avg:142.34ms
step:571/1375 train_time:79893ms step_avg:142.41ms
step:572/1375 train_time:80039ms step_avg:142.42ms
step:573/1375 train_time:80187ms step_avg:142.43ms
step:574/1375 train_time:80335ms step_avg:142.44ms
step:575/1375 train_time:80482ms step_avg:142.45ms
step:576/1375 train_time:80627ms step_avg:142.45ms
step:577/1375 train_time:80775ms step_avg:142.46ms
step:578/1375 train_time:80925ms step_avg:142.47ms
step:579/1375 train_time:81072ms step_avg:142.48ms
step:580/1375 train_time:81219ms step_avg:142.49ms
step:581/1375 train_time:81367ms step_avg:142.50ms
step:582/1375 train_time:81513ms step_avg:142.51ms
step:583/1375 train_time:81659ms step_avg:142.51ms
step:584/1375 train_time:81810ms step_avg:142.53ms
step:585/1375 train_time:81956ms step_avg:142.53ms
step:586/1375 train_time:82105ms step_avg:142.54ms
step:587/1375 train_time:82253ms step_avg:142.55ms
step:588/1375 train_time:82399ms step_avg:142.56ms
step:589/1375 train_time:82549ms step_avg:142.57ms
step:590/1375 train_time:82696ms step_avg:142.58ms
step:591/1375 train_time:82846ms step_avg:142.59ms
step:592/1375 train_time:82993ms step_avg:142.60ms
step:593/1375 train_time:83143ms step_avg:142.61ms
step:594/1375 train_time:83290ms step_avg:142.62ms
step:595/1375 train_time:83437ms step_avg:142.63ms
step:596/1375 train_time:83587ms step_avg:142.64ms
step:597/1375 train_time:83733ms step_avg:142.65ms
step:598/1375 train_time:83882ms step_avg:142.66ms
step:599/1375 train_time:84031ms step_avg:142.67ms
step:600/1375 train_time:84178ms step_avg:142.67ms
step:601/1375 train_time:84326ms step_avg:142.68ms
step:602/1375 train_time:84473ms step_avg:142.69ms
step:603/1375 train_time:84621ms step_avg:142.70ms
step:604/1375 train_time:84770ms step_avg:142.71ms
step:605/1375 train_time:84917ms step_avg:142.72ms
step:606/1375 train_time:85067ms step_avg:142.73ms
step:607/1375 train_time:85213ms step_avg:142.74ms
step:608/1375 train_time:85363ms step_avg:142.75ms
step:609/1375 train_time:85509ms step_avg:142.75ms
step:610/1375 train_time:85656ms step_avg:142.76ms
step:611/1375 train_time:85803ms step_avg:142.77ms
step:612/1375 train_time:85952ms step_avg:142.78ms
step:613/1375 train_time:86100ms step_avg:142.79ms
step:614/1375 train_time:86251ms step_avg:142.80ms
step:615/1375 train_time:86398ms step_avg:142.81ms
step:616/1375 train_time:86549ms step_avg:142.82ms
step:617/1375 train_time:86697ms step_avg:142.83ms
step:618/1375 train_time:86847ms step_avg:142.84ms
step:619/1375 train_time:86996ms step_avg:142.85ms
step:620/1375 train_time:87148ms step_avg:142.87ms
step:621/1375 train_time:87296ms step_avg:142.87ms
step:622/1375 train_time:87446ms step_avg:142.89ms
step:623/1375 train_time:87593ms step_avg:142.89ms
step:624/1375 train_time:87743ms step_avg:142.90ms
step:625/1375 train_time:87891ms step_avg:142.91ms
step:625/1375 val_loss:3.5771 train_time:87965ms step_avg:143.03ms
step:626/1375 train_time:88041ms step_avg:142.92ms
step:627/1375 train_time:88193ms step_avg:142.94ms
step:628/1375 train_time:88340ms step_avg:142.95ms
step:629/1375 train_time:88490ms step_avg:142.96ms
step:630/1375 train_time:88637ms step_avg:142.96ms
step:631/1375 train_time:88785ms step_avg:142.97ms
step:632/1375 train_time:88935ms step_avg:142.98ms
step:633/1375 train_time:89086ms step_avg:143.00ms
step:634/1375 train_time:89236ms step_avg:143.01ms
step:635/1375 train_time:89385ms step_avg:143.02ms
step:636/1375 train_time:89534ms step_avg:143.03ms
step:637/1375 train_time:89682ms step_avg:143.03ms
step:638/1375 train_time:89831ms step_avg:143.04ms
step:639/1375 train_time:89979ms step_avg:143.05ms
step:640/1375 train_time:90132ms step_avg:143.07ms
step:641/1375 train_time:90279ms step_avg:143.07ms
step:642/1375 train_time:90431ms step_avg:143.09ms
step:643/1375 train_time:90581ms step_avg:143.10ms
step:644/1375 train_time:90731ms step_avg:143.11ms
step:645/1375 train_time:90880ms step_avg:143.12ms
step:646/1375 train_time:91031ms step_avg:143.13ms
step:647/1375 train_time:91178ms step_avg:143.14ms
step:648/1375 train_time:91332ms step_avg:143.15ms
step:649/1375 train_time:91481ms step_avg:143.16ms
step:650/1375 train_time:91632ms step_avg:143.17ms
step:651/1375 train_time:91781ms step_avg:143.18ms
step:652/1375 train_time:91932ms step_avg:143.20ms
step:653/1375 train_time:92080ms step_avg:143.20ms
step:654/1375 train_time:92232ms step_avg:143.22ms
step:655/1375 train_time:92380ms step_avg:143.22ms
step:656/1375 train_time:92531ms step_avg:143.24ms
step:657/1375 train_time:92678ms step_avg:143.24ms
step:658/1375 train_time:92830ms step_avg:143.26ms
step:659/1375 train_time:92979ms step_avg:143.27ms
step:660/1375 train_time:93129ms step_avg:143.28ms
step:661/1375 train_time:93276ms step_avg:143.28ms
step:662/1375 train_time:93428ms step_avg:143.29ms
step:663/1375 train_time:93576ms step_avg:143.30ms
step:664/1375 train_time:93729ms step_avg:143.32ms
step:665/1375 train_time:93878ms step_avg:143.32ms
step:666/1375 train_time:94027ms step_avg:143.33ms
step:667/1375 train_time:94175ms step_avg:143.34ms
step:668/1375 train_time:94327ms step_avg:143.35ms
step:669/1375 train_time:94475ms step_avg:143.36ms
step:670/1375 train_time:94627ms step_avg:143.37ms
step:671/1375 train_time:94776ms step_avg:143.38ms
step:672/1375 train_time:94926ms step_avg:143.39ms
step:673/1375 train_time:95074ms step_avg:143.40ms
step:674/1375 train_time:95224ms step_avg:143.41ms
step:675/1375 train_time:95373ms step_avg:143.42ms
step:676/1375 train_time:95523ms step_avg:143.43ms
step:677/1375 train_time:95672ms step_avg:143.44ms
step:678/1375 train_time:95820ms step_avg:143.44ms
step:679/1375 train_time:95971ms step_avg:143.45ms
step:680/1375 train_time:96118ms step_avg:143.46ms
step:681/1375 train_time:96268ms step_avg:143.47ms
step:682/1375 train_time:96418ms step_avg:143.48ms
step:683/1375 train_time:96569ms step_avg:143.49ms
step:684/1375 train_time:96717ms step_avg:143.50ms
step:685/1375 train_time:96866ms step_avg:143.50ms
step:686/1375 train_time:97013ms step_avg:143.51ms
step:687/1375 train_time:97161ms step_avg:143.52ms
step:688/1375 train_time:97313ms step_avg:143.53ms
step:689/1375 train_time:97462ms step_avg:143.54ms
step:690/1375 train_time:97615ms step_avg:143.55ms
step:691/1375 train_time:97764ms step_avg:143.56ms
step:692/1375 train_time:97913ms step_avg:143.57ms
step:693/1375 train_time:98061ms step_avg:143.57ms
step:694/1375 train_time:98211ms step_avg:143.58ms
step:695/1375 train_time:98359ms step_avg:143.59ms
step:696/1375 train_time:98511ms step_avg:143.60ms
step:697/1375 train_time:98660ms step_avg:143.61ms
step:698/1375 train_time:98812ms step_avg:143.62ms
step:699/1375 train_time:98961ms step_avg:143.63ms
step:700/1375 train_time:99110ms step_avg:143.64ms
step:701/1375 train_time:99258ms step_avg:143.64ms
step:702/1375 train_time:99411ms step_avg:143.66ms
step:703/1375 train_time:99559ms step_avg:143.66ms
step:704/1375 train_time:99708ms step_avg:143.67ms
step:705/1375 train_time:99856ms step_avg:143.68ms
step:706/1375 train_time:100010ms step_avg:143.69ms
step:707/1375 train_time:100158ms step_avg:143.70ms
step:708/1375 train_time:100308ms step_avg:143.71ms
step:709/1375 train_time:100457ms step_avg:143.71ms
step:710/1375 train_time:100606ms step_avg:143.72ms
step:711/1375 train_time:100755ms step_avg:143.73ms
step:712/1375 train_time:100906ms step_avg:143.74ms
step:713/1375 train_time:101056ms step_avg:143.75ms
step:714/1375 train_time:101207ms step_avg:143.76ms
step:715/1375 train_time:101356ms step_avg:143.77ms
step:716/1375 train_time:101507ms step_avg:143.78ms
step:717/1375 train_time:101656ms step_avg:143.78ms
step:718/1375 train_time:101805ms step_avg:143.79ms
step:719/1375 train_time:101953ms step_avg:143.80ms
step:720/1375 train_time:102106ms step_avg:143.81ms
step:721/1375 train_time:102256ms step_avg:143.82ms
step:722/1375 train_time:102411ms step_avg:143.84ms
step:723/1375 train_time:102560ms step_avg:143.84ms
step:724/1375 train_time:102712ms step_avg:143.85ms
step:725/1375 train_time:102862ms step_avg:143.86ms
step:726/1375 train_time:103013ms step_avg:143.87ms
step:727/1375 train_time:103165ms step_avg:143.88ms
step:728/1375 train_time:103315ms step_avg:143.89ms
step:729/1375 train_time:103466ms step_avg:143.90ms
step:730/1375 train_time:103617ms step_avg:143.91ms
step:731/1375 train_time:103770ms step_avg:143.93ms
step:732/1375 train_time:103917ms step_avg:143.93ms
step:733/1375 train_time:104069ms step_avg:143.94ms
step:734/1375 train_time:104218ms step_avg:143.95ms
step:735/1375 train_time:104370ms step_avg:143.96ms
step:736/1375 train_time:104521ms step_avg:143.97ms
step:737/1375 train_time:104672ms step_avg:143.98ms
step:738/1375 train_time:104823ms step_avg:143.99ms
step:739/1375 train_time:104975ms step_avg:144.00ms
step:740/1375 train_time:105127ms step_avg:144.01ms
step:741/1375 train_time:105276ms step_avg:144.02ms
step:742/1375 train_time:105427ms step_avg:144.03ms
step:743/1375 train_time:105576ms step_avg:144.03ms
step:744/1375 train_time:105729ms step_avg:144.04ms
step:745/1375 train_time:105879ms step_avg:144.05ms
step:746/1375 train_time:106030ms step_avg:144.06ms
step:747/1375 train_time:106178ms step_avg:144.07ms
step:748/1375 train_time:106332ms step_avg:144.08ms
step:749/1375 train_time:106482ms step_avg:144.09ms
step:750/1375 train_time:106633ms step_avg:144.10ms
step:750/1375 val_loss:3.5222 train_time:106708ms step_avg:144.20ms
step:751/1375 train_time:106786ms step_avg:144.11ms
step:752/1375 train_time:106935ms step_avg:144.12ms
step:753/1375 train_time:107085ms step_avg:144.13ms
step:754/1375 train_time:107234ms step_avg:144.13ms
step:755/1375 train_time:107383ms step_avg:144.14ms
step:756/1375 train_time:107532ms step_avg:144.14ms
step:757/1375 train_time:107686ms step_avg:144.16ms
step:758/1375 train_time:107836ms step_avg:144.17ms
step:759/1375 train_time:107987ms step_avg:144.17ms
step:760/1375 train_time:108136ms step_avg:144.18ms
step:761/1375 train_time:108325ms step_avg:144.24ms
step:762/1375 train_time:108475ms step_avg:144.25ms
step:763/1375 train_time:108625ms step_avg:144.26ms
step:764/1375 train_time:108775ms step_avg:144.26ms
step:765/1375 train_time:108925ms step_avg:144.27ms
step:766/1375 train_time:109076ms step_avg:144.28ms
step:767/1375 train_time:109228ms step_avg:144.29ms
step:768/1375 train_time:109379ms step_avg:144.30ms
step:769/1375 train_time:109531ms step_avg:144.31ms
step:770/1375 train_time:109683ms step_avg:144.32ms
step:771/1375 train_time:109833ms step_avg:144.33ms
step:772/1375 train_time:109985ms step_avg:144.34ms
step:773/1375 train_time:110135ms step_avg:144.35ms
step:774/1375 train_time:110287ms step_avg:144.35ms
step:775/1375 train_time:110438ms step_avg:144.36ms
step:776/1375 train_time:110590ms step_avg:144.37ms
step:777/1375 train_time:110743ms step_avg:144.38ms
step:778/1375 train_time:110891ms step_avg:144.39ms
step:779/1375 train_time:111041ms step_avg:144.40ms
step:780/1375 train_time:111193ms step_avg:144.41ms
step:781/1375 train_time:111346ms step_avg:144.42ms
step:782/1375 train_time:111497ms step_avg:144.43ms
step:783/1375 train_time:111648ms step_avg:144.43ms
step:784/1375 train_time:111798ms step_avg:144.44ms
step:785/1375 train_time:111948ms step_avg:144.45ms
step:786/1375 train_time:112098ms step_avg:144.46ms
step:787/1375 train_time:112249ms step_avg:144.46ms
step:788/1375 train_time:112400ms step_avg:144.47ms
step:789/1375 train_time:112548ms step_avg:144.48ms
step:790/1375 train_time:112698ms step_avg:144.49ms
step:791/1375 train_time:112848ms step_avg:144.49ms
step:792/1375 train_time:113000ms step_avg:144.50ms
step:793/1375 train_time:113150ms step_avg:144.51ms
step:794/1375 train_time:113303ms step_avg:144.52ms
step:795/1375 train_time:113455ms step_avg:144.53ms
step:796/1375 train_time:113606ms step_avg:144.54ms
step:797/1375 train_time:113755ms step_avg:144.54ms
step:798/1375 train_time:113907ms step_avg:144.55ms
step:799/1375 train_time:114061ms step_avg:144.56ms
step:800/1375 train_time:114210ms step_avg:144.57ms
step:801/1375 train_time:114362ms step_avg:144.58ms
step:802/1375 train_time:114514ms step_avg:144.59ms
step:803/1375 train_time:114664ms step_avg:144.60ms
step:804/1375 train_time:114814ms step_avg:144.60ms
step:805/1375 train_time:114969ms step_avg:144.62ms
step:806/1375 train_time:115120ms step_avg:144.62ms
step:807/1375 train_time:115269ms step_avg:144.63ms
step:808/1375 train_time:115421ms step_avg:144.64ms
step:809/1375 train_time:115570ms step_avg:144.64ms
step:810/1375 train_time:115723ms step_avg:144.65ms
step:811/1375 train_time:115873ms step_avg:144.66ms
step:812/1375 train_time:116025ms step_avg:144.67ms
step:813/1375 train_time:116174ms step_avg:144.68ms
step:814/1375 train_time:116326ms step_avg:144.68ms
step:815/1375 train_time:116476ms step_avg:144.69ms
step:816/1375 train_time:116628ms step_avg:144.70ms
step:817/1375 train_time:116779ms step_avg:144.71ms
step:818/1375 train_time:116930ms step_avg:144.71ms
step:819/1375 train_time:117083ms step_avg:144.73ms
step:820/1375 train_time:117237ms step_avg:144.74ms
step:821/1375 train_time:117388ms step_avg:144.75ms
step:822/1375 train_time:117540ms step_avg:144.75ms
step:823/1375 train_time:117691ms step_avg:144.76ms
step:824/1375 train_time:117843ms step_avg:144.77ms
step:825/1375 train_time:117997ms step_avg:144.78ms
step:826/1375 train_time:118150ms step_avg:144.79ms
step:827/1375 train_time:118303ms step_avg:144.80ms
step:828/1375 train_time:118454ms step_avg:144.81ms
step:829/1375 train_time:118606ms step_avg:144.82ms
step:830/1375 train_time:118756ms step_avg:144.82ms
step:831/1375 train_time:118908ms step_avg:144.83ms
step:832/1375 train_time:119062ms step_avg:144.84ms
step:833/1375 train_time:119213ms step_avg:144.85ms
step:834/1375 train_time:119366ms step_avg:144.86ms
step:835/1375 train_time:119517ms step_avg:144.87ms
step:836/1375 train_time:119671ms step_avg:144.88ms
step:837/1375 train_time:119823ms step_avg:144.89ms
step:838/1375 train_time:119974ms step_avg:144.90ms
step:839/1375 train_time:120126ms step_avg:144.90ms
step:840/1375 train_time:120277ms step_avg:144.91ms
step:841/1375 train_time:120429ms step_avg:144.92ms
step:842/1375 train_time:120584ms step_avg:144.93ms
step:843/1375 train_time:120733ms step_avg:144.94ms
step:844/1375 train_time:120886ms step_avg:144.95ms
step:845/1375 train_time:121036ms step_avg:144.95ms
step:846/1375 train_time:121189ms step_avg:144.96ms
step:847/1375 train_time:121341ms step_avg:144.97ms
step:848/1375 train_time:121492ms step_avg:144.98ms
step:849/1375 train_time:121645ms step_avg:144.99ms
step:850/1375 train_time:121798ms step_avg:145.00ms
step:851/1375 train_time:121949ms step_avg:145.00ms
step:852/1375 train_time:122101ms step_avg:145.01ms
step:853/1375 train_time:122251ms step_avg:145.02ms
step:854/1375 train_time:122404ms step_avg:145.03ms
step:855/1375 train_time:122557ms step_avg:145.04ms
step:856/1375 train_time:122706ms step_avg:145.04ms
step:857/1375 train_time:122860ms step_avg:145.05ms
step:858/1375 train_time:123015ms step_avg:145.07ms
step:859/1375 train_time:123167ms step_avg:145.07ms
step:860/1375 train_time:123320ms step_avg:145.08ms
step:861/1375 train_time:123472ms step_avg:145.09ms
step:862/1375 train_time:123626ms step_avg:145.10ms
step:863/1375 train_time:123777ms step_avg:145.11ms
step:864/1375 train_time:123928ms step_avg:145.11ms
step:865/1375 train_time:124078ms step_avg:145.12ms
step:866/1375 train_time:124238ms step_avg:145.14ms
step:867/1375 train_time:124389ms step_avg:145.14ms
step:868/1375 train_time:124539ms step_avg:145.15ms
step:869/1375 train_time:124690ms step_avg:145.16ms
step:870/1375 train_time:124844ms step_avg:145.17ms
step:871/1375 train_time:124996ms step_avg:145.17ms
step:872/1375 train_time:125148ms step_avg:145.18ms
step:873/1375 train_time:125301ms step_avg:145.19ms
step:874/1375 train_time:125452ms step_avg:145.20ms
step:875/1375 train_time:125603ms step_avg:145.21ms
step:875/1375 val_loss:3.4708 train_time:125679ms step_avg:145.29ms
step:876/1375 train_time:125755ms step_avg:145.21ms
step:877/1375 train_time:125912ms step_avg:145.23ms
step:878/1375 train_time:126062ms step_avg:145.23ms
step:879/1375 train_time:126214ms step_avg:145.24ms
step:880/1375 train_time:126366ms step_avg:145.25ms
step:881/1375 train_time:126515ms step_avg:145.25ms
step:882/1375 train_time:126671ms step_avg:145.26ms
step:883/1375 train_time:126823ms step_avg:145.27ms
step:884/1375 train_time:126974ms step_avg:145.28ms
step:885/1375 train_time:127125ms step_avg:145.29ms
step:886/1375 train_time:127280ms step_avg:145.30ms
step:887/1375 train_time:127432ms step_avg:145.30ms
step:888/1375 train_time:127585ms step_avg:145.31ms
step:889/1375 train_time:127738ms step_avg:145.32ms
step:890/1375 train_time:127890ms step_avg:145.33ms
step:891/1375 train_time:128042ms step_avg:145.34ms
step:892/1375 train_time:128195ms step_avg:145.35ms
step:893/1375 train_time:128346ms step_avg:145.35ms
step:894/1375 train_time:128498ms step_avg:145.36ms
step:895/1375 train_time:128653ms step_avg:145.37ms
step:896/1375 train_time:128807ms step_avg:145.38ms
step:897/1375 train_time:128959ms step_avg:145.39ms
step:898/1375 train_time:129112ms step_avg:145.40ms
step:899/1375 train_time:129264ms step_avg:145.40ms
step:900/1375 train_time:129414ms step_avg:145.41ms
step:901/1375 train_time:129567ms step_avg:145.42ms
step:902/1375 train_time:129715ms step_avg:145.42ms
step:903/1375 train_time:129869ms step_avg:145.43ms
step:904/1375 train_time:130021ms step_avg:145.44ms
step:905/1375 train_time:130173ms step_avg:145.45ms
step:906/1375 train_time:130325ms step_avg:145.45ms
step:907/1375 train_time:130479ms step_avg:145.46ms
step:908/1375 train_time:130632ms step_avg:145.47ms
step:909/1375 train_time:130785ms step_avg:145.48ms
step:910/1375 train_time:130943ms step_avg:145.49ms
step:911/1375 train_time:131094ms step_avg:145.50ms
step:912/1375 train_time:131245ms step_avg:145.50ms
step:913/1375 train_time:131399ms step_avg:145.51ms
step:914/1375 train_time:131552ms step_avg:145.52ms
step:915/1375 train_time:131706ms step_avg:145.53ms
step:916/1375 train_time:131856ms step_avg:145.54ms
step:917/1375 train_time:132010ms step_avg:145.55ms
step:918/1375 train_time:132162ms step_avg:145.55ms
step:919/1375 train_time:132317ms step_avg:145.56ms
step:920/1375 train_time:132471ms step_avg:145.57ms
step:921/1375 train_time:132623ms step_avg:145.58ms
step:922/1375 train_time:132782ms step_avg:145.59ms
step:923/1375 train_time:132932ms step_avg:145.60ms
step:924/1375 train_time:133084ms step_avg:145.61ms
step:925/1375 train_time:133238ms step_avg:145.62ms
step:926/1375 train_time:133393ms step_avg:145.63ms
step:927/1375 train_time:133545ms step_avg:145.63ms
step:928/1375 train_time:133698ms step_avg:145.64ms
step:929/1375 train_time:133854ms step_avg:145.65ms
step:930/1375 train_time:134009ms step_avg:145.66ms
step:931/1375 train_time:134160ms step_avg:145.67ms
step:932/1375 train_time:134313ms step_avg:145.68ms
step:933/1375 train_time:134467ms step_avg:145.68ms
step:934/1375 train_time:134620ms step_avg:145.69ms
step:935/1375 train_time:134774ms step_avg:145.70ms
step:936/1375 train_time:134927ms step_avg:145.71ms
step:937/1375 train_time:135085ms step_avg:145.72ms
step:938/1375 train_time:135238ms step_avg:145.73ms
step:939/1375 train_time:135393ms step_avg:145.74ms
step:940/1375 train_time:135546ms step_avg:145.75ms
step:941/1375 train_time:135699ms step_avg:145.76ms
step:942/1375 train_time:135852ms step_avg:145.76ms
step:943/1375 train_time:136007ms step_avg:145.77ms
step:944/1375 train_time:136165ms step_avg:145.79ms
step:945/1375 train_time:136319ms step_avg:145.80ms
step:946/1375 train_time:136475ms step_avg:145.81ms
step:947/1375 train_time:136628ms step_avg:145.81ms
step:948/1375 train_time:136780ms step_avg:145.82ms
step:949/1375 train_time:136934ms step_avg:145.83ms
step:950/1375 train_time:137087ms step_avg:145.84ms
step:951/1375 train_time:137281ms step_avg:145.89ms
step:952/1375 train_time:137432ms step_avg:145.89ms
step:953/1375 train_time:137583ms step_avg:145.90ms
step:954/1375 train_time:137736ms step_avg:145.91ms
step:955/1375 train_time:137887ms step_avg:145.91ms
step:956/1375 train_time:138040ms step_avg:145.92ms
step:957/1375 train_time:138195ms step_avg:145.93ms
step:958/1375 train_time:138353ms step_avg:145.94ms
step:959/1375 train_time:138508ms step_avg:145.95ms
step:960/1375 train_time:138663ms step_avg:145.96ms
step:961/1375 train_time:138817ms step_avg:145.97ms
step:962/1375 train_time:138970ms step_avg:145.98ms
step:963/1375 train_time:139129ms step_avg:145.99ms
step:964/1375 train_time:139283ms step_avg:146.00ms
step:965/1375 train_time:139435ms step_avg:146.00ms
step:966/1375 train_time:139585ms step_avg:146.01ms
step:967/1375 train_time:139736ms step_avg:146.01ms
step:968/1375 train_time:139888ms step_avg:146.02ms
step:969/1375 train_time:140042ms step_avg:146.03ms
step:970/1375 train_time:140194ms step_avg:146.04ms
step:971/1375 train_time:140348ms step_avg:146.04ms
step:972/1375 train_time:140499ms step_avg:146.05ms
step:973/1375 train_time:140651ms step_avg:146.05ms
step:974/1375 train_time:140804ms step_avg:146.06ms
step:975/1375 train_time:140956ms step_avg:146.07ms
step:976/1375 train_time:141111ms step_avg:146.08ms
step:977/1375 train_time:141261ms step_avg:146.08ms
step:978/1375 train_time:141415ms step_avg:146.09ms
step:979/1375 train_time:141569ms step_avg:146.10ms
step:980/1375 train_time:141720ms step_avg:146.10ms
step:981/1375 train_time:141871ms step_avg:146.11ms
step:982/1375 train_time:142024ms step_avg:146.12ms
step:983/1375 train_time:142176ms step_avg:146.12ms
step:984/1375 train_time:142329ms step_avg:146.13ms
step:985/1375 train_time:142482ms step_avg:146.14ms
step:986/1375 train_time:142637ms step_avg:146.14ms
step:987/1375 train_time:142788ms step_avg:146.15ms
step:988/1375 train_time:142940ms step_avg:146.16ms
step:989/1375 train_time:143093ms step_avg:146.16ms
step:990/1375 train_time:143248ms step_avg:146.17ms
step:991/1375 train_time:143398ms step_avg:146.18ms
step:992/1375 train_time:143555ms step_avg:146.19ms
step:993/1375 train_time:143717ms step_avg:146.20ms
step:994/1375 train_time:143871ms step_avg:146.21ms
step:995/1375 train_time:144022ms step_avg:146.22ms
step:996/1375 train_time:144174ms step_avg:146.22ms
step:997/1375 train_time:144326ms step_avg:146.23ms
step:998/1375 train_time:144477ms step_avg:146.23ms
step:999/1375 train_time:144631ms step_avg:146.24ms
step:1000/1375 train_time:144784ms step_avg:146.25ms
step:1000/1375 val_loss:3.4051 train_time:144860ms step_avg:146.32ms
step:1001/1375 train_time:144937ms step_avg:146.25ms
step:1002/1375 train_time:145093ms step_avg:146.26ms
step:1003/1375 train_time:145246ms step_avg:146.27ms
step:1004/1375 train_time:145400ms step_avg:146.28ms
step:1005/1375 train_time:145553ms step_avg:146.28ms
step:1006/1375 train_time:145704ms step_avg:146.29ms
step:1007/1375 train_time:145860ms step_avg:146.30ms
step:1008/1375 train_time:146015ms step_avg:146.31ms
step:1009/1375 train_time:146174ms step_avg:146.32ms
step:1010/1375 train_time:146326ms step_avg:146.33ms
step:1011/1375 train_time:146478ms step_avg:146.33ms
step:1012/1375 train_time:146630ms step_avg:146.34ms
step:1013/1375 train_time:146783ms step_avg:146.34ms
step:1014/1375 train_time:146937ms step_avg:146.35ms
step:1015/1375 train_time:147092ms step_avg:146.36ms
step:1016/1375 train_time:147245ms step_avg:146.37ms
step:1017/1375 train_time:147398ms step_avg:146.37ms
step:1018/1375 train_time:147549ms step_avg:146.38ms
step:1019/1375 train_time:147703ms step_avg:146.39ms
step:1020/1375 train_time:147858ms step_avg:146.39ms
step:1021/1375 train_time:148012ms step_avg:146.40ms
step:1022/1375 train_time:148165ms step_avg:146.41ms
step:1023/1375 train_time:148320ms step_avg:146.42ms
step:1024/1375 train_time:148472ms step_avg:146.42ms
step:1025/1375 train_time:148627ms step_avg:146.43ms
step:1026/1375 train_time:148778ms step_avg:146.44ms
step:1027/1375 train_time:148932ms step_avg:146.44ms
step:1028/1375 train_time:149088ms step_avg:146.45ms
step:1029/1375 train_time:149244ms step_avg:146.46ms
step:1030/1375 train_time:149399ms step_avg:146.47ms
step:1031/1375 train_time:149552ms step_avg:146.48ms
step:1032/1375 train_time:149706ms step_avg:146.48ms
step:1033/1375 train_time:149860ms step_avg:146.49ms
step:1034/1375 train_time:150015ms step_avg:146.50ms
step:1035/1375 train_time:150172ms step_avg:146.51ms
step:1036/1375 train_time:150324ms step_avg:146.51ms
step:1037/1375 train_time:150480ms step_avg:146.52ms
step:1038/1375 train_time:150636ms step_avg:146.53ms
step:1039/1375 train_time:150789ms step_avg:146.54ms
step:1040/1375 train_time:150941ms step_avg:146.55ms
step:1041/1375 train_time:151096ms step_avg:146.55ms
step:1042/1375 train_time:151248ms step_avg:146.56ms
step:1043/1375 train_time:151401ms step_avg:146.56ms
step:1044/1375 train_time:151556ms step_avg:146.57ms
step:1045/1375 train_time:151714ms step_avg:146.58ms
step:1046/1375 train_time:151866ms step_avg:146.59ms
step:1047/1375 train_time:152019ms step_avg:146.60ms
step:1048/1375 train_time:152177ms step_avg:146.61ms
step:1049/1375 train_time:152331ms step_avg:146.61ms
step:1050/1375 train_time:152490ms step_avg:146.62ms
step:1051/1375 train_time:152647ms step_avg:146.63ms
step:1052/1375 train_time:152800ms step_avg:146.64ms
step:1053/1375 train_time:152953ms step_avg:146.65ms
step:1054/1375 train_time:153110ms step_avg:146.66ms
step:1055/1375 train_time:153263ms step_avg:146.66ms
step:1056/1375 train_time:153417ms step_avg:146.67ms
step:1057/1375 train_time:153573ms step_avg:146.68ms
step:1058/1375 train_time:153729ms step_avg:146.69ms
step:1059/1375 train_time:153885ms step_avg:146.70ms
step:1060/1375 train_time:154040ms step_avg:146.71ms
step:1061/1375 train_time:154193ms step_avg:146.71ms
step:1062/1375 train_time:154350ms step_avg:146.72ms
step:1063/1375 train_time:154503ms step_avg:146.73ms
step:1064/1375 train_time:154656ms step_avg:146.73ms
step:1065/1375 train_time:154811ms step_avg:146.74ms
step:1066/1375 train_time:154970ms step_avg:146.75ms
step:1067/1375 train_time:155123ms step_avg:146.76ms
step:1068/1375 train_time:155278ms step_avg:146.77ms
step:1069/1375 train_time:155440ms step_avg:146.78ms
step:1070/1375 train_time:155593ms step_avg:146.79ms
step:1071/1375 train_time:155749ms step_avg:146.79ms
step:1072/1375 train_time:155900ms step_avg:146.80ms
step:1073/1375 train_time:156053ms step_avg:146.80ms
step:1074/1375 train_time:156206ms step_avg:146.81ms
step:1075/1375 train_time:156360ms step_avg:146.82ms
step:1076/1375 train_time:156515ms step_avg:146.82ms
step:1077/1375 train_time:156668ms step_avg:146.83ms
step:1078/1375 train_time:156826ms step_avg:146.84ms
step:1079/1375 train_time:156986ms step_avg:146.85ms
step:1080/1375 train_time:157139ms step_avg:146.86ms
step:1081/1375 train_time:157294ms step_avg:146.87ms
step:1082/1375 train_time:157446ms step_avg:146.87ms
step:1083/1375 train_time:157600ms step_avg:146.88ms
step:1084/1375 train_time:157756ms step_avg:146.89ms
step:1085/1375 train_time:157908ms step_avg:146.89ms
step:1086/1375 train_time:158064ms step_avg:146.90ms
step:1087/1375 train_time:158221ms step_avg:146.91ms
step:1088/1375 train_time:158375ms step_avg:146.92ms
step:1089/1375 train_time:158533ms step_avg:146.93ms
step:1090/1375 train_time:158690ms step_avg:146.94ms
step:1091/1375 train_time:158847ms step_avg:146.94ms
step:1092/1375 train_time:158999ms step_avg:146.95ms
step:1093/1375 train_time:159156ms step_avg:146.96ms
step:1094/1375 train_time:159310ms step_avg:146.96ms
step:1095/1375 train_time:159463ms step_avg:146.97ms
step:1096/1375 train_time:159621ms step_avg:146.98ms
step:1097/1375 train_time:159776ms step_avg:146.99ms
step:1098/1375 train_time:159929ms step_avg:146.99ms
step:1099/1375 train_time:160082ms step_avg:147.00ms
step:1100/1375 train_time:160235ms step_avg:147.00ms
step:1101/1375 train_time:160389ms step_avg:147.01ms
step:1102/1375 train_time:160546ms step_avg:147.02ms
step:1103/1375 train_time:160701ms step_avg:147.03ms
step:1104/1375 train_time:160854ms step_avg:147.03ms
step:1105/1375 train_time:161010ms step_avg:147.04ms
step:1106/1375 train_time:161164ms step_avg:147.05ms
step:1107/1375 train_time:161318ms step_avg:147.05ms
step:1108/1375 train_time:161475ms step_avg:147.06ms
step:1109/1375 train_time:161627ms step_avg:147.07ms
step:1110/1375 train_time:161781ms step_avg:147.07ms
step:1111/1375 train_time:161939ms step_avg:147.08ms
step:1112/1375 train_time:162094ms step_avg:147.09ms
step:1113/1375 train_time:162249ms step_avg:147.10ms
step:1114/1375 train_time:162406ms step_avg:147.11ms
step:1115/1375 train_time:162561ms step_avg:147.11ms
step:1116/1375 train_time:162714ms step_avg:147.12ms
step:1117/1375 train_time:162872ms step_avg:147.13ms
step:1118/1375 train_time:163032ms step_avg:147.14ms
step:1119/1375 train_time:163185ms step_avg:147.15ms
step:1120/1375 train_time:163338ms step_avg:147.15ms
step:1121/1375 train_time:163493ms step_avg:147.16ms
step:1122/1375 train_time:163645ms step_avg:147.16ms
step:1123/1375 train_time:163798ms step_avg:147.17ms
step:1124/1375 train_time:163957ms step_avg:147.18ms
step:1125/1375 train_time:164113ms step_avg:147.19ms
step:1125/1375 val_loss:3.3518 train_time:164191ms step_avg:147.26ms
step:1126/1375 train_time:164268ms step_avg:147.19ms
step:1127/1375 train_time:164424ms step_avg:147.20ms
step:1128/1375 train_time:164579ms step_avg:147.21ms
step:1129/1375 train_time:164739ms step_avg:147.22ms
step:1130/1375 train_time:164892ms step_avg:147.23ms
step:1131/1375 train_time:165052ms step_avg:147.24ms
step:1132/1375 train_time:165208ms step_avg:147.24ms
step:1133/1375 train_time:165365ms step_avg:147.25ms
step:1134/1375 train_time:165520ms step_avg:147.26ms
step:1135/1375 train_time:165674ms step_avg:147.27ms
step:1136/1375 train_time:165834ms step_avg:147.28ms
step:1137/1375 train_time:165987ms step_avg:147.28ms
step:1138/1375 train_time:166142ms step_avg:147.29ms
step:1139/1375 train_time:166299ms step_avg:147.30ms
step:1140/1375 train_time:166454ms step_avg:147.30ms
step:1141/1375 train_time:166647ms step_avg:147.34ms
step:1142/1375 train_time:166803ms step_avg:147.35ms
step:1143/1375 train_time:166961ms step_avg:147.36ms
step:1144/1375 train_time:167116ms step_avg:147.37ms
step:1145/1375 train_time:167267ms step_avg:147.37ms
step:1146/1375 train_time:167424ms step_avg:147.38ms
step:1147/1375 train_time:167580ms step_avg:147.39ms
step:1148/1375 train_time:167735ms step_avg:147.39ms
step:1149/1375 train_time:167892ms step_avg:147.40ms
step:1150/1375 train_time:168045ms step_avg:147.41ms
step:1151/1375 train_time:168202ms step_avg:147.42ms
step:1152/1375 train_time:168357ms step_avg:147.42ms
step:1153/1375 train_time:168514ms step_avg:147.43ms
step:1154/1375 train_time:168668ms step_avg:147.44ms
step:1155/1375 train_time:168823ms step_avg:147.44ms
step:1156/1375 train_time:168983ms step_avg:147.45ms
step:1157/1375 train_time:169141ms step_avg:147.46ms
step:1158/1375 train_time:169295ms step_avg:147.47ms
step:1159/1375 train_time:169450ms step_avg:147.48ms
step:1160/1375 train_time:169603ms step_avg:147.48ms
step:1161/1375 train_time:169759ms step_avg:147.49ms
step:1162/1375 train_time:169916ms step_avg:147.50ms
step:1163/1375 train_time:170072ms step_avg:147.50ms
step:1164/1375 train_time:170226ms step_avg:147.51ms
step:1165/1375 train_time:170379ms step_avg:147.51ms
step:1166/1375 train_time:170534ms step_avg:147.52ms
step:1167/1375 train_time:170688ms step_avg:147.53ms
step:1168/1375 train_time:170843ms step_avg:147.53ms
step:1169/1375 train_time:170999ms step_avg:147.54ms
step:1170/1375 train_time:171153ms step_avg:147.55ms
step:1171/1375 train_time:171311ms step_avg:147.55ms
step:1172/1375 train_time:171466ms step_avg:147.56ms
step:1173/1375 train_time:171622ms step_avg:147.57ms
step:1174/1375 train_time:171784ms step_avg:147.58ms
step:1175/1375 train_time:171943ms step_avg:147.59ms
step:1176/1375 train_time:172101ms step_avg:147.60ms
step:1177/1375 train_time:172264ms step_avg:147.61ms
step:1178/1375 train_time:172420ms step_avg:147.62ms
step:1179/1375 train_time:172575ms step_avg:147.63ms
step:1180/1375 train_time:172737ms step_avg:147.64ms
step:1181/1375 train_time:172893ms step_avg:147.65ms
step:1182/1375 train_time:173046ms step_avg:147.65ms
step:1183/1375 train_time:173199ms step_avg:147.66ms
step:1184/1375 train_time:173353ms step_avg:147.66ms
step:1185/1375 train_time:173511ms step_avg:147.67ms
step:1186/1375 train_time:173665ms step_avg:147.67ms
step:1187/1375 train_time:173830ms step_avg:147.69ms
step:1188/1375 train_time:173983ms step_avg:147.69ms
step:1189/1375 train_time:174143ms step_avg:147.70ms
step:1190/1375 train_time:174301ms step_avg:147.71ms
step:1191/1375 train_time:174457ms step_avg:147.72ms
step:1192/1375 train_time:174610ms step_avg:147.72ms
step:1193/1375 train_time:174765ms step_avg:147.73ms
step:1194/1375 train_time:174920ms step_avg:147.74ms
step:1195/1375 train_time:175076ms step_avg:147.74ms
step:1196/1375 train_time:175232ms step_avg:147.75ms
step:1197/1375 train_time:175387ms step_avg:147.76ms
step:1198/1375 train_time:175548ms step_avg:147.77ms
step:1199/1375 train_time:175702ms step_avg:147.77ms
step:1200/1375 train_time:175855ms step_avg:147.78ms
step:1201/1375 train_time:176011ms step_avg:147.78ms
step:1202/1375 train_time:176179ms step_avg:147.80ms
step:1203/1375 train_time:176339ms step_avg:147.81ms
step:1204/1375 train_time:176496ms step_avg:147.82ms
step:1205/1375 train_time:176650ms step_avg:147.82ms
step:1206/1375 train_time:176807ms step_avg:147.83ms
step:1207/1375 train_time:176962ms step_avg:147.84ms
step:1208/1375 train_time:177117ms step_avg:147.84ms
step:1209/1375 train_time:177272ms step_avg:147.85ms
step:1210/1375 train_time:177430ms step_avg:147.86ms
step:1211/1375 train_time:177587ms step_avg:147.87ms
step:1212/1375 train_time:177744ms step_avg:147.87ms
step:1213/1375 train_time:177899ms step_avg:147.88ms
step:1214/1375 train_time:178054ms step_avg:147.89ms
step:1215/1375 train_time:178210ms step_avg:147.89ms
step:1216/1375 train_time:178363ms step_avg:147.90ms
step:1217/1375 train_time:178520ms step_avg:147.90ms
step:1218/1375 train_time:178674ms step_avg:147.91ms
step:1219/1375 train_time:178828ms step_avg:147.91ms
step:1220/1375 train_time:178981ms step_avg:147.92ms
step:1221/1375 train_time:179135ms step_avg:147.92ms
step:1222/1375 train_time:179292ms step_avg:147.93ms
step:1223/1375 train_time:179449ms step_avg:147.94ms
step:1224/1375 train_time:179606ms step_avg:147.95ms
step:1225/1375 train_time:179763ms step_avg:147.95ms
step:1226/1375 train_time:179918ms step_avg:147.96ms
step:1227/1375 train_time:180077ms step_avg:147.97ms
step:1228/1375 train_time:180231ms step_avg:147.97ms
step:1229/1375 train_time:180387ms step_avg:147.98ms
step:1230/1375 train_time:180548ms step_avg:147.99ms
step:1231/1375 train_time:180706ms step_avg:148.00ms
step:1232/1375 train_time:180864ms step_avg:148.01ms
step:1233/1375 train_time:181022ms step_avg:148.01ms
step:1234/1375 train_time:181176ms step_avg:148.02ms
step:1235/1375 train_time:181331ms step_avg:148.03ms
step:1236/1375 train_time:181487ms step_avg:148.03ms
step:1237/1375 train_time:181644ms step_avg:148.04ms
step:1238/1375 train_time:181808ms step_avg:148.05ms
step:1239/1375 train_time:181963ms step_avg:148.06ms
step:1240/1375 train_time:182122ms step_avg:148.07ms
step:1241/1375 train_time:182283ms step_avg:148.08ms
step:1242/1375 train_time:182437ms step_avg:148.08ms
step:1243/1375 train_time:182597ms step_avg:148.09ms
step:1244/1375 train_time:182752ms step_avg:148.10ms
step:1245/1375 train_time:182910ms step_avg:148.11ms
step:1246/1375 train_time:183064ms step_avg:148.11ms
step:1247/1375 train_time:183224ms step_avg:148.12ms
step:1248/1375 train_time:183378ms step_avg:148.12ms
step:1249/1375 train_time:183531ms step_avg:148.13ms
step:1250/1375 train_time:183687ms step_avg:148.14ms
step:1250/1375 val_loss:3.3059 train_time:183768ms step_avg:148.20ms
step:1251/1375 train_time:183848ms step_avg:148.15ms
step:1252/1375 train_time:184002ms step_avg:148.15ms
step:1253/1375 train_time:184157ms step_avg:148.16ms
step:1254/1375 train_time:184310ms step_avg:148.16ms
step:1255/1375 train_time:184478ms step_avg:148.17ms
step:1256/1375 train_time:184633ms step_avg:148.18ms
step:1257/1375 train_time:184787ms step_avg:148.19ms
step:1258/1375 train_time:184946ms step_avg:148.19ms
step:1259/1375 train_time:185104ms step_avg:148.20ms
step:1260/1375 train_time:185256ms step_avg:148.20ms
step:1261/1375 train_time:185414ms step_avg:148.21ms
step:1262/1375 train_time:185572ms step_avg:148.22ms
step:1263/1375 train_time:185728ms step_avg:148.23ms
step:1264/1375 train_time:185882ms step_avg:148.23ms
step:1265/1375 train_time:186037ms step_avg:148.24ms
step:1266/1375 train_time:186195ms step_avg:148.24ms
step:1267/1375 train_time:186352ms step_avg:148.25ms
step:1268/1375 train_time:186509ms step_avg:148.26ms
step:1269/1375 train_time:186669ms step_avg:148.27ms
step:1270/1375 train_time:186824ms step_avg:148.27ms
step:1271/1375 train_time:186981ms step_avg:148.28ms
step:1272/1375 train_time:187136ms step_avg:148.29ms
step:1273/1375 train_time:187290ms step_avg:148.29ms
step:1274/1375 train_time:187445ms step_avg:148.30ms
step:1275/1375 train_time:187601ms step_avg:148.30ms
step:1276/1375 train_time:187753ms step_avg:148.30ms
step:1277/1375 train_time:187909ms step_avg:148.31ms
step:1278/1375 train_time:188061ms step_avg:148.31ms
step:1279/1375 train_time:188219ms step_avg:148.32ms
step:1280/1375 train_time:188381ms step_avg:148.33ms
step:1281/1375 train_time:188535ms step_avg:148.34ms
step:1282/1375 train_time:188690ms step_avg:148.34ms
step:1283/1375 train_time:188847ms step_avg:148.35ms
step:1284/1375 train_time:189007ms step_avg:148.36ms
step:1285/1375 train_time:189160ms step_avg:148.36ms
step:1286/1375 train_time:189320ms step_avg:148.37ms
step:1287/1375 train_time:189475ms step_avg:148.37ms
step:1288/1375 train_time:189632ms step_avg:148.38ms
step:1289/1375 train_time:189794ms step_avg:148.39ms
step:1290/1375 train_time:189954ms step_avg:148.40ms
step:1291/1375 train_time:190112ms step_avg:148.41ms
step:1292/1375 train_time:190271ms step_avg:148.42ms
step:1293/1375 train_time:190430ms step_avg:148.43ms
step:1294/1375 train_time:190585ms step_avg:148.43ms
step:1295/1375 train_time:190741ms step_avg:148.44ms
step:1296/1375 train_time:190897ms step_avg:148.44ms
step:1297/1375 train_time:191055ms step_avg:148.45ms
step:1298/1375 train_time:191213ms step_avg:148.46ms
step:1299/1375 train_time:191370ms step_avg:148.46ms
step:1300/1375 train_time:191523ms step_avg:148.47ms
step:1301/1375 train_time:191677ms step_avg:148.47ms
step:1302/1375 train_time:191836ms step_avg:148.48ms
step:1303/1375 train_time:191993ms step_avg:148.49ms
step:1304/1375 train_time:192152ms step_avg:148.49ms
step:1305/1375 train_time:192308ms step_avg:148.50ms
step:1306/1375 train_time:192467ms step_avg:148.51ms
step:1307/1375 train_time:192618ms step_avg:148.51ms
step:1308/1375 train_time:192775ms step_avg:148.52ms
step:1309/1375 train_time:192931ms step_avg:148.52ms
step:1310/1375 train_time:193085ms step_avg:148.53ms
step:1311/1375 train_time:193240ms step_avg:148.53ms
step:1312/1375 train_time:193393ms step_avg:148.54ms
step:1313/1375 train_time:193547ms step_avg:148.54ms
step:1314/1375 train_time:193704ms step_avg:148.55ms
step:1315/1375 train_time:193860ms step_avg:148.55ms
step:1316/1375 train_time:194014ms step_avg:148.56ms
step:1317/1375 train_time:194170ms step_avg:148.56ms
step:1318/1375 train_time:194332ms step_avg:148.57ms
step:1319/1375 train_time:194488ms step_avg:148.58ms
step:1320/1375 train_time:194643ms step_avg:148.58ms
step:1321/1375 train_time:194800ms step_avg:148.59ms
step:1322/1375 train_time:194961ms step_avg:148.60ms
step:1323/1375 train_time:195118ms step_avg:148.60ms
step:1324/1375 train_time:195274ms step_avg:148.61ms
step:1325/1375 train_time:195430ms step_avg:148.62ms
step:1326/1375 train_time:195590ms step_avg:148.62ms
step:1327/1375 train_time:195746ms step_avg:148.63ms
step:1328/1375 train_time:195900ms step_avg:148.63ms
step:1329/1375 train_time:196076ms step_avg:148.65ms
step:1330/1375 train_time:196235ms step_avg:148.66ms
step:1331/1375 train_time:196430ms step_avg:148.70ms
step:1332/1375 train_time:196593ms step_avg:148.71ms
step:1333/1375 train_time:196750ms step_avg:148.72ms
step:1334/1375 train_time:196906ms step_avg:148.72ms
step:1335/1375 train_time:197058ms step_avg:148.72ms
step:1336/1375 train_time:197223ms step_avg:148.73ms
step:1337/1375 train_time:197383ms step_avg:148.74ms
step:1338/1375 train_time:197541ms step_avg:148.75ms
step:1339/1375 train_time:197699ms step_avg:148.76ms
step:1340/1375 train_time:197856ms step_avg:148.76ms
step:1341/1375 train_time:198010ms step_avg:148.77ms
step:1342/1375 train_time:198169ms step_avg:148.78ms
step:1343/1375 train_time:198325ms step_avg:148.78ms
step:1344/1375 train_time:198479ms step_avg:148.79ms
step:1345/1375 train_time:198637ms step_avg:148.79ms
step:1346/1375 train_time:198795ms step_avg:148.80ms
step:1347/1375 train_time:198952ms step_avg:148.80ms
step:1348/1375 train_time:199108ms step_avg:148.81ms
step:1349/1375 train_time:199265ms step_avg:148.82ms
step:1350/1375 train_time:199419ms step_avg:148.82ms
step:1351/1375 train_time:199577ms step_avg:148.83ms
step:1352/1375 train_time:199739ms step_avg:148.84ms
step:1353/1375 train_time:199899ms step_avg:148.85ms
step:1354/1375 train_time:200056ms step_avg:148.85ms
step:1355/1375 train_time:200212ms step_avg:148.86ms
step:1356/1375 train_time:200367ms step_avg:148.86ms
step:1357/1375 train_time:200524ms step_avg:148.87ms
step:1358/1375 train_time:200684ms step_avg:148.88ms
step:1359/1375 train_time:200843ms step_avg:148.88ms
step:1360/1375 train_time:201006ms step_avg:148.89ms
step:1361/1375 train_time:201165ms step_avg:148.90ms
step:1362/1375 train_time:201324ms step_avg:148.91ms
step:1363/1375 train_time:201486ms step_avg:148.92ms
step:1364/1375 train_time:201642ms step_avg:148.92ms
step:1365/1375 train_time:201796ms step_avg:148.93ms
step:1366/1375 train_time:201952ms step_avg:148.93ms
step:1367/1375 train_time:202112ms step_avg:148.94ms
step:1368/1375 train_time:202270ms step_avg:148.95ms
step:1369/1375 train_time:202434ms step_avg:148.96ms
step:1370/1375 train_time:202593ms step_avg:148.97ms
step:1371/1375 train_time:202749ms step_avg:148.97ms
step:1372/1375 train_time:202911ms step_avg:148.98ms
step:1373/1375 train_time:203068ms step_avg:148.99ms
step:1374/1375 train_time:203228ms step_avg:148.99ms
step:1375/1375 train_time:203382ms step_avg:149.00ms
step:1375/1375 val_loss:3.2805 train_time:203458ms step_avg:149.05ms
peak memory consumption: 31565 MiB

import os
import sys
with open(sys.argv[0]) as f:
    code = f.read() # read the code of this file ASAP, for logging
import uuid
import time
import glob
import subprocess
import contextlib
from dataclasses import dataclass

import torch
torch.empty(1, device='cuda', requires_grad=True).backward()
from torch import nn
import torch.nn.functional as F
import torch.distributed as dist
from torch.nn.parallel import DistributedDataParallel as DDP
# use of FlexAttention contributed by @KoszarskyB
from torch.nn.attention.flex_attention import BlockMask, flex_attention

# -----------------------------------------------------------------------------
# Muon optimizer

@torch.compile
def zeropower_via_newtonschulz5(G, steps):
    """
    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a
    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose
    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at
    zero even beyond the point where the iteration no longer converges all the way to one everywhere
    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T
    where S' is diagonal with S_{ii}' ~ Uniform(0.5, 1.5), which turns out not to hurt model
    performance at all relative to UV^T, where USV^T = G is the SVD.
    """
    assert len(G.shape) == 2
    a, b, c = (3.4445, -4.7750,  2.0315)
    X = G.bfloat16()
    if G.size(0) > G.size(1):
        X = X.T

    # Ensure spectral norm is at most 1
    X = X / (X.norm() + 1e-7)
    # Perform the NS iterations
    for _ in range(steps):
        A = X @ X.T
        B = b * A + c * A @ A # adapted from suggestion by @jxbz, @leloykun, and @YouJiacheng
        X = a * X + B @ X
    
    if G.size(0) > G.size(1):
        X = X.T
    return X

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Some warnings:
    - This optimizer assumes that all parameters passed in are 2D.
    - It should not be used for the embedding layer, the final fully connected layer, or any {0,1}-D
    parameters; those should all be optimized by a standard method (e.g., AdamW).
    - To use it with 4D convolutional filters, it works well to just flatten their last 3 dimensions.
    - We believe it is unlikely to work well for training with small batch size.
    - We believe it may not work well for finetuning pretrained models, but we haven't tested this.
    - We have not yet tried this optimizer for training scenarios larger than NanoGPT (124M).

    Arguments:
        lr: The learning rate used by the internal SGD.
        momentum: The momentum used by the internal SGD.
        nesterov: Whether to use Nesterov-style momentum in the internal SGD. (recommended)
        ns_steps: The number of Newton-Schulz iteration steps to use.
    """
    def __init__(self, params, lr=0.02, momentum=0.95, nesterov=True, ns_steps=5):
        self.world_size = int(os.environ['WORLD_SIZE'])
        self.rank = int(os.environ['RANK'])
        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, ns_steps=ns_steps)
        assert all(isinstance(p, torch.Tensor) for p in params)
        sizes = {p.numel() for p in params}
        param_groups = [dict(params=[p for p in params if p.numel() == size],
                             update_buffer=[torch.empty(size, device='cuda', dtype=torch.bfloat16) for _ in range(self.world_size)])
                        for size in sizes]
        super().__init__(param_groups, defaults)

    def step(self):

        for group in self.param_groups:

            lr = group['lr']
            momentum = group['momentum']
            nesterov = group['nesterov']
            ns_steps = group['ns_steps']
            update_buffers = group['update_buffer']
            # generate weight updates in distributed fashion
            params = group['params']
            handle = None
            params_world = None
            def update_prev():
                if params_world is None:
                    return
                assert handle is not None
                handle.wait()
                for p_world, g_world in zip(params_world, update_buffers):
                    p_world.data.add_(
                        g_world.view_as(p_world),
                        alpha=-lr * max(1, p_world.size(0) / p_world.size(1)) ** 0.5,
                    )
            for base_i in range(len(params))[::self.world_size]:
                if base_i + rank < len(params):
                    p = params[base_i + self.rank]
                    g = p.grad
                    assert g is not None
                    state = self.state[p]
                    if 'momentum_buffer' not in state:
                        state['momentum_buffer'] = torch.zeros_like(g)
                    buf = state['momentum_buffer']
                    buf.lerp_(g, 1 - momentum)
                    g = g.lerp_(buf, momentum) if nesterov else buf
                    g = zeropower_via_newtonschulz5(g, steps=ns_steps).flatten()
                else:
                    g = update_buffers[rank]
                update_prev() # async all_gather instead of sync all_reduce by @YouJiacheng
                handle = dist.all_gather(update_buffers, g, async_op=True)
                params_world = params[base_i : base_i + self.world_size]
            update_prev()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the GPT-2 model

def norm(x):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):

    def __init__(self, in_features, out_features):
        super().__init__(in_features, out_features, bias=False)

    def forward(self, x):
        return F.linear(x, self.weight.type_as(x))

class Rotary(nn.Module):

    def __init__(self, dim, max_seq_len=65536):
        super().__init__()
        # half-truncate RoPE by @YouJiacheng
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=dim//4, dtype=torch.float32)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(dim//4)])
        t = torch.arange(max_seq_len, dtype=torch.float32)
        theta = torch.einsum('i,j -> ij', t, angular_freq)
        self.cos = nn.Buffer(theta.cos(), persistent=False)
        self.sin = nn.Buffer(theta.sin(), persistent=False)

    def forward(self, x):
        cos, sin = self.cos[None, :x.size(-3), None, :], self.sin[None, :x.size(-3), None, :]
        x1, x2 = x.float().chunk(2, dim=-1)
        y1 = x1 * cos + x2 * sin
        y2 = x1 * (-sin) + x2 * cos
        return torch.cat((y1, y2), 3).type_as(x)

class CausalSelfAttention(nn.Module):

    def __init__(self, dim, num_heads, layer_id):
        super().__init__()
        assert dim % num_heads == 0
        self.num_heads = num_heads
        self.layer_id = layer_id
        self.c_q = CastedLinear(dim, dim)
        self.c_k = CastedLinear(dim, dim)
        self.c_v = CastedLinear(dim, dim)
        self.lambdas = nn.Parameter(torch.tensor([0.5, 0.5]))
        self.rotary = Rotary(dim // num_heads) # dim // num_heads = head_dim
        self.c_proj = CastedLinear(dim, dim)
        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977
        # self.attn_scale = nn.Parameter(torch.tensor(1.0 / (dim // num_heads) ** 0.5))
        # self.attn_scale = nn.Parameter(torch.tensor(0.20))
        self.attn_scale = 0.13 + 0.01 * min(layer_id, 11 - layer_id)

    def forward(self, x, ve, block_mask):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, 'Must use batch size = 1 for FlexAttention'
        q = self.c_q(x).view(B, T, self.num_heads, -1)
        k = self.c_k(x).view(B, T, self.num_heads, -1)
        v = self.c_v(x).view(B, T, self.num_heads, -1)
        if ve is not None:
            v = self.lambdas[0] * v + self.lambdas[1] * ve.view_as(v) # @KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = self.lambdas[0] * v
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = self.rotary(q), self.rotary(k)
        # y = flex_attention(q.transpose(1, 2) * self.attn_scale, k.transpose(1, 2), v.transpose(1, 2), block_mask=block_mask, scale=1.)
        y = flex_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), block_mask=block_mask, scale=self.attn_scale)
        y = y.transpose(1, 2).contiguous().view_as(x) # re-assemble all head outputs side by side
        y = self.c_proj(y)
        return y

class MLP(nn.Module):

    def __init__(self, dim):
        super().__init__()
        self.c_fc = CastedLinear(dim, 4 * dim)
        self.c_proj = CastedLinear(4 * dim, dim)
        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977

    def forward(self, x):
        x = self.c_fc(x)
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = self.c_proj(x)
        return x

class Block(nn.Module):

    def __init__(self, model_dim, num_heads, layer_id, use_attn=True):
        super().__init__()
        self.attn = CausalSelfAttention(model_dim, num_heads, layer_id) if use_attn else None
        self.mlp = MLP(model_dim)
        self.lambdas = nn.Parameter(torch.tensor([1., 0.]))

    def forward(self, x, ve, x0, block_mask):
        x = self.lambdas[0] * x + self.lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), ve, block_mask)
        x = x + self.mlp(norm(x))
        return x

class ValueEmbedding(nn.Module):
    def __init__(self, vocab_size, model_dim):
        super().__init__()
        self.embed = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])

    def forward(self, inputs):
        ve = [emb(inputs).bfloat16() for emb in self.embed]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        ve = [ve[0], ve[1], ve[2], None, None, None, None, None, None, ve[0], ve[1], ve[2]]
        return ve

# -----------------------------------------------------------------------------
# The main GPT-2 model

class GPT(nn.Module):

    def __init__(self, vocab_size, num_layers, num_heads, model_dim):
        super().__init__()
        self.embed = nn.Embedding(vocab_size, model_dim)
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.blocks = nn.ModuleList([Block(model_dim, num_heads, layer_id=layer_id, use_attn=(layer_id != 7))
                                     for layer_id in range(num_layers)])
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual learning
        # U-net structure on token value embeddings by @leloykun
        self.value_embeds = ValueEmbedding(vocab_size, model_dim)
        self.lm_head = CastedLinear(model_dim, vocab_size)
        self.lm_head.weight.data.zero_() # @Grad62304977
        # U-net design by @brendanh0gan
        self.num_encoder_layers = num_layers // 2 # Half of the layers for encoder
        self.num_decoder_layers = num_layers - self.num_encoder_layers # Remaining for decoder
        # Add learnable skip connection weights for decoder layers
        self.skip_weights = nn.Parameter(torch.ones(self.num_decoder_layers))

    def forward(self, inputs, targets, sliding_window_num_blocks):
        BLOCK_SIZE = 128
        seq_len = len(inputs)
        assert seq_len % BLOCK_SIZE == 0
        total_num_blocks = seq_len // BLOCK_SIZE
        assert inputs.ndim == 1
        docs = (inputs == 50256).cumsum(0)
        docs_low = docs.view(-1, BLOCK_SIZE)[:, 0].contiguous()
        docs_high = docs.view(-1, BLOCK_SIZE)[:, -1].contiguous()

        def document_causal(b, h, q_idx, kv_idx):
            causal_mask = q_idx >= kv_idx
            document_mask = docs[q_idx] == docs[kv_idx]
            return causal_mask & document_mask

        def dense_to_ordered(dense_mask):
            num_blocks = dense_mask.sum(dim=-1, dtype=torch.int32)
            indices = dense_mask.argsort(dim=-1, descending=True, stable=True).to(torch.int32)
            return num_blocks[None, None].contiguous(), indices[None, None].contiguous()

        def create_doc_swc_block_mask(sliding_window_num_blocks):
            kv_idx = block_idx = torch.arange(total_num_blocks, dtype=torch.int32, device='cuda')
            q_idx = block_idx[:, None]
            causal_bm = q_idx >= kv_idx
            causal_full_bm = q_idx > kv_idx
            window_bm = q_idx - kv_idx < sliding_window_num_blocks
            window_full_bm = window_bm # block-wise sliding window by @YouJiacheng
            # document_bm = (docs_low[q_idx] <= docs_high[kv_idx]) & (docs_low[kv_idx] <= docs_high[q_idx])
            document_bm = (docs_low[:, None] <= docs_high) & (docs_low <= docs_high[:, None])
            document_full_bm = (docs_low[:, None] == docs_high) & (docs_low == docs_high[:, None])
            nonzero_bm = causal_bm & window_bm & document_bm
            full_bm  = causal_full_bm & window_full_bm & document_full_bm
            kv_num_blocks, kv_indices = dense_to_ordered(nonzero_bm & ~full_bm)
            full_kv_num_blocks, full_kv_indices = dense_to_ordered(full_bm)
            return BlockMask.from_kv_blocks(
                kv_num_blocks,
                kv_indices,
                full_kv_num_blocks,
                full_kv_indices,
                BLOCK_SIZE=BLOCK_SIZE,
                mask_mod=document_causal,
            )

        block_mask = create_doc_swc_block_mask(sliding_window_num_blocks)

        x0 = norm(self.embed(inputs[None]).bfloat16()) # use of norm here by @Grad62304977
        x = x0
        ve = self.value_embeds(inputs)
        assert len(ve) == len(self.blocks)
        ve_enc, ve_dec = ve[:self.num_encoder_layers], ve[self.num_encoder_layers:]

        # Store outputs for U-Net skip connections
        skip_connections = []
        # Encoder pass - process only the first half of the blocks
        for i in range(self.num_encoder_layers):
            x = self.blocks[i](x, ve_enc[i], x0, block_mask)
            skip_connections.append(x)
        # Decoder pass - process the remaining blocks with weighted skip connections
        for i in range(self.num_decoder_layers):
            x = x + self.skip_weights[i] * skip_connections.pop()
            # U-net structure on token value embeddings by @leloykun
            x = self.blocks[self.num_encoder_layers + i](x, ve_dec[i], x0, block_mask)

        x = norm(x)
        logits = self.lm_head(x)
        logits = 15 * torch.tanh(logits / 15) # @Grad62304977 added tanh softcapping, @KoszarskyB reduced it from 30 to 15
        logits = logits.float()
        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets)
        return loss

# -----------------------------------------------------------------------------
# Our own simple Distributed Data Loader

def _load_data_shard(path):
    # only reads the header, returns header data
    # header is 256 int32
    header = torch.from_file(path, False, 256, dtype=torch.int32)
    assert header[0] == 20240520, 'magic number mismatch in the data .bin file'
    assert header[1] == 1, 'unsupported version'
    num_tokens = int(header[2]) # number of tokens (claimed)
    with open(path, 'rb', buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, 'number of tokens read does not match header'
    return tokens

class DistributedDataLoader:

    def __init__(self, filename_pattern):
        self.rank = int(os.environ['RANK'])
        self.world_size = int(os.environ['WORLD_SIZE'])
        self.files = sorted(glob.glob(filename_pattern))
        self.reset()

    def reset(self):
        self.current_shard = -1
        self.advance()

    def advance(self):
        self.current_shard = (self.current_shard + 1) % len(self.files)
        self.current_position = 0
        self.tokens = _load_data_shard(self.files[self.current_shard])

    def next_batch(self, batch_size):
        assert batch_size % self.world_size == 0
        device_batch_size = batch_size // self.world_size
        # load next shard if necessary
        if self.current_position + batch_size + 1 >= len(self.tokens):
            self.advance()
        pos = self.current_position + self.rank * device_batch_size
        device_batch_tokens = self.tokens[pos:pos+device_batch_size+1]
        # advance current position
        self.current_position += batch_size
        inputs = device_batch_tokens[:-1].to(device='cuda', dtype=torch.int32, non_blocking=True)
        targets = device_batch_tokens[1:].to(device='cuda', dtype=torch.int64, non_blocking=True)
        return inputs, targets

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files = 'data/fineweb10B/fineweb_train_*.bin' # input .bin to train on
    val_files = 'data/fineweb10B/fineweb_val_*.bin' # input .bin to eval validation loss on
    val_tokens = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # optimization
    batch_size = 8*64*1024 # batch size in tokens
    num_iterations = 1375 # number of iterations to run
    cooldown_frac = 0.4 # fraction of training spent cooling down the learning rate
    bf16_embeds = True
    # evaluation and logging
    val_loss_every = 125 # every how many steps to evaluate val loss? 0 for only at the end
    # implementation
    max_device_batch_size = 64*1024 # batch size per device in tokens
    save_checkpoint = False
args = Hyperparameters()

micro_bs = args.max_device_batch_size

# set up DDP (distributed data parallel). torchrun sets this env variable
rank = int(os.environ['RANK'])
local_rank = int(os.environ['LOCAL_RANK'])
world_size = int(os.environ['WORLD_SIZE'])
assert torch.cuda.is_available()
torch.cuda.set_device(local_rank)
dist.init_process_group(backend='nccl', device_id=torch.device(local_rank))
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = uuid.uuid4()
    os.makedirs('logs', exist_ok=True)
    logfile = f'logs/{run_id}.txt'
    print(logfile)

def print0(s, console=False):
    if master_process:
        with open(logfile, 'a') as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0('='*100)
# log information about the hardware/software environment this is running on
print0(f'Running Python {sys.version}')
print0(f'Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}')
print0(subprocess.run(['nvidia-smi'], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout)
print0('='*100)

# load data
train_loader = DistributedDataLoader(args.train_files)
val_loader = DistributedDataLoader(args.val_files)
print0(f'Training dataloader files: {train_loader.files}')
print0(f'Validation dataloader files: {val_loader.files}')
print0('='*100)

# there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency. suggested to me by @Grad62304977.
# this originates from Karpathy's experiments.
model = GPT(vocab_size=50304, num_layers=12, num_heads=6, model_dim=768)
model = model.cuda()
if args.bf16_embeds:
    for m in model.modules():
        if isinstance(m, nn.Embedding):
            m.bfloat16()
model: GPT = torch.compile(model)
ddp_model = DDP(model, device_ids=[local_rank], broadcast_buffers=False, gradient_as_bucket_view=True)

# collect the parameters to optimize
hidden_matrix_params = [p for p in model.blocks.parameters() if p.ndim == 2]
embed_params = [model.embed.weight, *model.value_embeds.parameters()]
scalar_params = [p for n, p in model.named_parameters() if p.ndim < 2 and "attn_scale" not in n]
attn_scale_params = [p for n, p in model.named_parameters() if p.ndim < 2 and "attn_scale" in n]
head_params = [model.lm_head.weight]

# init the optimizer(s)
optimizer1 = torch.optim.Adam([dict(params=embed_params, lr=0.6),
                               dict(params=head_params, lr=0.008),
                               dict(params=scalar_params, lr=0.04),
                               dict(params=attn_scale_params, lr=0.01)],
                              betas=(0.8, 0.95), fused=True)
optimizer2 = Muon(hidden_matrix_params, lr=0.05, momentum=0.95)
optimizers = [optimizer1, optimizer2]

# learning rate schedule: stable then decay
def get_lr(it):
    t = 1 - it / args.num_iterations # time remaining in training
    assert 1 >= t > 0
    # 1) constant lr for first part of training
    if t >= args.cooldown_frac:
        return 1.0
    # 2) then linear cooldown
    else:
        return t / args.cooldown_frac
schedulers = [torch.optim.lr_scheduler.LambdaLR(opt, get_lr) for opt in optimizers]

# sliding window size schedule: linear increase over training in chunks of 128 from 128 -> 1792. By @fernbear.bsky.social
def get_sliding_window_blocks(it):
    x = it / args.num_iterations # training progress
    assert 0 <= x <= 1
    return int(((1 - x) * 128 + x * 1856) // 128)
sliding_window_num_blocks = torch.tensor(1, dtype=torch.int32, device='cuda')

# Start training loop
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    # This effectively ignores timing first 10 steps, which are slower for weird reasons.
    # Alternately, and slightly more correctly in terms of benchmarking, we could do 10
    # steps with dummy data first, and then re-initialize the model and reset the loader.
    if step == 10:
        training_time_ms = 0
        t0 = time.perf_counter()
    timed_steps = float('nan') if step <= 11 else (step - 10) + 1 # <= 11 to avoid bug in val

    sliding_window_num_blocks.copy_(get_sliding_window_blocks(step))

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        # run validation batches
        model.eval()
        val_loader.reset()
        val_loss = 0.0
        # calculate the number of steps to take in the val loop.
        val_batch_size = world_size * micro_bs
        assert args.val_tokens % val_batch_size == 0
        val_steps = args.val_tokens // val_batch_size
        for _ in range(val_steps):
            with torch.no_grad():
                inputs_val, targets_val = val_loader.next_batch(val_batch_size)
                val_loss += ddp_model(inputs_val, targets_val, sliding_window_num_blocks)
        # Print attention scales
        for n, p in model.named_parameters():
            if p.ndim < 2 and "attn_scale" in n:
                print0(f'{n}: {p.item()}', console=True)
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        val_loss /= val_steps
        # logging
        print0(f'step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/(timed_steps-1):.2f}ms', console=True)
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f'logs/{run_id}', exist_ok=True)
            torch.save(log, f'logs/{run_id}/state_step{step:06d}.pt')
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    model.train()
    batch_size = args.batch_size
    assert batch_size % world_size == 0
    inputs_train, targets_train = train_loader.next_batch(batch_size)
    assert len(inputs_train) <= micro_bs or len(inputs_train) % micro_bs == 0
    for micro_inputs_train, micro_targets_train in zip(inputs_train.split(micro_bs), targets_train.split(micro_bs)):
        ddp_model(micro_inputs_train, micro_targets_train, sliding_window_num_blocks).backward()
    # momentum warmup for Muon
    frac = min(step/300, 1)
    for group in optimizer2.param_groups:
        group['momentum'] = (1 - frac) * 0.85 + frac * 0.95
    # step the optimizers and schedulers
    for opt, sched in zip(optimizers, schedulers):
        opt.step()
        if step != train_steps-1:
            sched.step()
    # null the gradients
    model.zero_grad(set_to_none=True)
    # logging
    approx_time = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f'step:{step+1}/{train_steps} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms', console=True)

print0(f'peak memory consumption: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB')
dist.destroy_process_group()

====================================================================================================
Running Python 3.10.12 (main, Sep 11 2024, 15:47:36) [GCC 11.4.0]
Running PyTorch 2.6.0.dev20241231+cu126 compiled for CUDA 12.6
Tue Jan 14 15:34:39 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.183.06             Driver Version: 535.183.06   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:19:00.0 Off |                    0 |
| N/A   38C    P0             126W / 700W |   7713MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:3B:00.0 Off |                    0 |
| N/A   32C    P0             114W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:4C:00.0 Off |                    0 |
| N/A   29C    P0             117W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   37C    P0             128W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:9B:00.0 Off |                    0 |
| N/A   38C    P0             123W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:BB:00.0 Off |                    0 |
| N/A   30C    P0             114W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:CB:00.0 Off |                    0 |
| N/A   36C    P0             116W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:DB:00.0 Off |                    0 |
| N/A   29C    P0             117W / 700W |   3211MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
Training dataloader files: ['data/fineweb10B/fineweb_train_000001.bin', 'data/fineweb10B/fineweb_train_000002.bin', 'data/fineweb10B/fineweb_train_000003.bin', 'data/fineweb10B/fineweb_train_000004.bin', 'data/fineweb10B/fineweb_train_000005.bin', 'data/fineweb10B/fineweb_train_000006.bin', 'data/fineweb10B/fineweb_train_000007.bin', 'data/fineweb10B/fineweb_train_000008.bin', 'data/fineweb10B/fineweb_train_000009.bin']
Validation dataloader files: ['data/fineweb10B/fineweb_val_000000.bin']
====================================================================================================
step:0/1375 val_loss:10.8258 train_time:0ms step_avg:nanms
step:1/1375 train_time:27759ms step_avg:nanms
step:2/1375 train_time:27827ms step_avg:nanms
step:3/1375 train_time:28015ms step_avg:nanms
step:4/1375 train_time:28149ms step_avg:nanms
step:5/1375 train_time:28282ms step_avg:nanms
step:6/1375 train_time:28417ms step_avg:nanms
step:7/1375 train_time:28549ms step_avg:nanms
step:8/1375 train_time:28682ms step_avg:nanms
step:9/1375 train_time:28815ms step_avg:nanms
step:10/1375 train_time:28953ms step_avg:nanms
step:11/1375 train_time:138ms step_avg:nanms
step:12/1375 train_time:272ms step_avg:nanms
step:13/1375 train_time:408ms step_avg:135.90ms
step:14/1375 train_time:542ms step_avg:135.45ms
step:15/1375 train_time:679ms step_avg:135.83ms
step:16/1375 train_time:812ms step_avg:135.28ms
step:17/1375 train_time:947ms step_avg:135.26ms
step:18/1375 train_time:1083ms step_avg:135.36ms
step:19/1375 train_time:1220ms step_avg:135.56ms
step:20/1375 train_time:1356ms step_avg:135.56ms
step:21/1375 train_time:1493ms step_avg:135.75ms
step:22/1375 train_time:1627ms step_avg:135.62ms
step:23/1375 train_time:1763ms step_avg:135.61ms
step:24/1375 train_time:1898ms step_avg:135.56ms
step:25/1375 train_time:2034ms step_avg:135.59ms
step:26/1375 train_time:2169ms step_avg:135.56ms
step:27/1375 train_time:2305ms step_avg:135.57ms
step:28/1375 train_time:2440ms step_avg:135.58ms
step:29/1375 train_time:2578ms step_avg:135.69ms
step:30/1375 train_time:2713ms step_avg:135.63ms
step:31/1375 train_time:2848ms step_avg:135.60ms
step:32/1375 train_time:2982ms step_avg:135.56ms
step:33/1375 train_time:3119ms step_avg:135.59ms
step:34/1375 train_time:3256ms step_avg:135.65ms
step:35/1375 train_time:3392ms step_avg:135.68ms
step:36/1375 train_time:3526ms step_avg:135.61ms
step:37/1375 train_time:3660ms step_avg:135.56ms
step:38/1375 train_time:3798ms step_avg:135.63ms
step:39/1375 train_time:3932ms step_avg:135.60ms
step:40/1375 train_time:4067ms step_avg:135.55ms
step:41/1375 train_time:4202ms step_avg:135.54ms
step:42/1375 train_time:4338ms step_avg:135.56ms
step:43/1375 train_time:4474ms step_avg:135.57ms
step:44/1375 train_time:4608ms step_avg:135.54ms
step:45/1375 train_time:4743ms step_avg:135.52ms
step:46/1375 train_time:4878ms step_avg:135.51ms
step:47/1375 train_time:5014ms step_avg:135.50ms
step:48/1375 train_time:5148ms step_avg:135.47ms
step:49/1375 train_time:5283ms step_avg:135.47ms
step:50/1375 train_time:5420ms step_avg:135.49ms
step:51/1375 train_time:5555ms step_avg:135.48ms
step:52/1375 train_time:5691ms step_avg:135.49ms
step:53/1375 train_time:5826ms step_avg:135.48ms
step:54/1375 train_time:5960ms step_avg:135.46ms
step:55/1375 train_time:6097ms step_avg:135.50ms
step:56/1375 train_time:6232ms step_avg:135.48ms
step:57/1375 train_time:6367ms step_avg:135.48ms
step:58/1375 train_time:6503ms step_avg:135.47ms
step:59/1375 train_time:6638ms step_avg:135.47ms
step:60/1375 train_time:6772ms step_avg:135.44ms
step:61/1375 train_time:6907ms step_avg:135.43ms
step:62/1375 train_time:7042ms step_avg:135.43ms
step:63/1375 train_time:7178ms step_avg:135.44ms
step:64/1375 train_time:7313ms step_avg:135.43ms
step:65/1375 train_time:7448ms step_avg:135.41ms
step:66/1375 train_time:7583ms step_avg:135.41ms
step:67/1375 train_time:7720ms step_avg:135.44ms
step:68/1375 train_time:7857ms step_avg:135.47ms
step:69/1375 train_time:7991ms step_avg:135.44ms
step:70/1375 train_time:8127ms step_avg:135.44ms
step:71/1375 train_time:8262ms step_avg:135.44ms
step:72/1375 train_time:8399ms step_avg:135.47ms
step:73/1375 train_time:8535ms step_avg:135.48ms
step:74/1375 train_time:8670ms step_avg:135.47ms
step:75/1375 train_time:8806ms step_avg:135.48ms
step:76/1375 train_time:8941ms step_avg:135.47ms
step:77/1375 train_time:9079ms step_avg:135.50ms
step:78/1375 train_time:9213ms step_avg:135.49ms
step:79/1375 train_time:9348ms step_avg:135.48ms
step:80/1375 train_time:9482ms step_avg:135.45ms
step:81/1375 train_time:9619ms step_avg:135.48ms
step:82/1375 train_time:9754ms step_avg:135.47ms
step:83/1375 train_time:9889ms step_avg:135.46ms
step:84/1375 train_time:10024ms step_avg:135.46ms
step:85/1375 train_time:10160ms step_avg:135.46ms
step:86/1375 train_time:10296ms step_avg:135.47ms
step:87/1375 train_time:10430ms step_avg:135.46ms
step:88/1375 train_time:10567ms step_avg:135.47ms
step:89/1375 train_time:10701ms step_avg:135.46ms
step:90/1375 train_time:10838ms step_avg:135.47ms
step:91/1375 train_time:10975ms step_avg:135.49ms
step:92/1375 train_time:11110ms step_avg:135.49ms
step:93/1375 train_time:11245ms step_avg:135.48ms
step:94/1375 train_time:11382ms step_avg:135.50ms
step:95/1375 train_time:11518ms step_avg:135.51ms
step:96/1375 train_time:11652ms step_avg:135.49ms
step:97/1375 train_time:11787ms step_avg:135.48ms
step:98/1375 train_time:11923ms step_avg:135.48ms
step:99/1375 train_time:12059ms step_avg:135.49ms
step:100/1375 train_time:12196ms step_avg:135.51ms
step:101/1375 train_time:12329ms step_avg:135.49ms
step:102/1375 train_time:12466ms step_avg:135.50ms
step:103/1375 train_time:12602ms step_avg:135.50ms
step:104/1375 train_time:12741ms step_avg:135.54ms
step:105/1375 train_time:12880ms step_avg:135.58ms
step:106/1375 train_time:13020ms step_avg:135.62ms
step:107/1375 train_time:13159ms step_avg:135.66ms
step:108/1375 train_time:13297ms step_avg:135.68ms
step:109/1375 train_time:13435ms step_avg:135.70ms
step:110/1375 train_time:13573ms step_avg:135.73ms
step:111/1375 train_time:13714ms step_avg:135.78ms
step:112/1375 train_time:13853ms step_avg:135.81ms
step:113/1375 train_time:13991ms step_avg:135.84ms
step:114/1375 train_time:14131ms step_avg:135.88ms
step:115/1375 train_time:14270ms step_avg:135.91ms
step:116/1375 train_time:14409ms step_avg:135.94ms
step:117/1375 train_time:14549ms step_avg:135.97ms
step:118/1375 train_time:14687ms step_avg:135.99ms
step:119/1375 train_time:14827ms step_avg:136.03ms
step:120/1375 train_time:14965ms step_avg:136.04ms
step:121/1375 train_time:15104ms step_avg:136.08ms
step:122/1375 train_time:15244ms step_avg:136.11ms
step:123/1375 train_time:15383ms step_avg:136.13ms
step:124/1375 train_time:15522ms step_avg:136.16ms
step:125/1375 train_time:15661ms step_avg:136.19ms
step:125/1375 val_loss:4.3711 train_time:15731ms step_avg:136.79ms
step:126/1375 train_time:15805ms step_avg:136.25ms
step:127/1375 train_time:15950ms step_avg:136.33ms
step:128/1375 train_time:16091ms step_avg:136.36ms
step:129/1375 train_time:16229ms step_avg:136.38ms
step:130/1375 train_time:16368ms step_avg:136.40ms
step:131/1375 train_time:16504ms step_avg:136.40ms
step:132/1375 train_time:16642ms step_avg:136.41ms
step:133/1375 train_time:16781ms step_avg:136.43ms
step:134/1375 train_time:16924ms step_avg:136.48ms
step:135/1375 train_time:17061ms step_avg:136.49ms
step:136/1375 train_time:17200ms step_avg:136.51ms
step:137/1375 train_time:17339ms step_avg:136.53ms
step:138/1375 train_time:17477ms step_avg:136.54ms
step:139/1375 train_time:17614ms step_avg:136.55ms
step:140/1375 train_time:17754ms step_avg:136.57ms
step:141/1375 train_time:17894ms step_avg:136.59ms
step:142/1375 train_time:18033ms step_avg:136.61ms
step:143/1375 train_time:18172ms step_avg:136.63ms
step:144/1375 train_time:18312ms step_avg:136.66ms
step:145/1375 train_time:18452ms step_avg:136.68ms
step:146/1375 train_time:18590ms step_avg:136.69ms
step:147/1375 train_time:18729ms step_avg:136.71ms
step:148/1375 train_time:18870ms step_avg:136.74ms
step:149/1375 train_time:19010ms step_avg:136.76ms
step:150/1375 train_time:19150ms step_avg:136.79ms
step:151/1375 train_time:19288ms step_avg:136.79ms
step:152/1375 train_time:19427ms step_avg:136.81ms
step:153/1375 train_time:19566ms step_avg:136.82ms
step:154/1375 train_time:19705ms step_avg:136.84ms
step:155/1375 train_time:19843ms step_avg:136.85ms
step:156/1375 train_time:19984ms step_avg:136.87ms
step:157/1375 train_time:20123ms step_avg:136.89ms
step:158/1375 train_time:20262ms step_avg:136.91ms
step:159/1375 train_time:20402ms step_avg:136.93ms
step:160/1375 train_time:20540ms step_avg:136.93ms
step:161/1375 train_time:20678ms step_avg:136.94ms
step:162/1375 train_time:20816ms step_avg:136.95ms
step:163/1375 train_time:20956ms step_avg:136.96ms
step:164/1375 train_time:21095ms step_avg:136.98ms
step:165/1375 train_time:21235ms step_avg:137.00ms
step:166/1375 train_time:21375ms step_avg:137.02ms
step:167/1375 train_time:21513ms step_avg:137.03ms
step:168/1375 train_time:21653ms step_avg:137.04ms
step:169/1375 train_time:21791ms step_avg:137.05ms
step:170/1375 train_time:21930ms step_avg:137.06ms
step:171/1375 train_time:22069ms step_avg:137.08ms
step:172/1375 train_time:22209ms step_avg:137.09ms
step:173/1375 train_time:22349ms step_avg:137.11ms
step:174/1375 train_time:22488ms step_avg:137.12ms
step:175/1375 train_time:22626ms step_avg:137.13ms
step:176/1375 train_time:22767ms step_avg:137.15ms
step:177/1375 train_time:22906ms step_avg:137.16ms
step:178/1375 train_time:23045ms step_avg:137.17ms
step:179/1375 train_time:23185ms step_avg:137.19ms
step:180/1375 train_time:23324ms step_avg:137.20ms
step:181/1375 train_time:23462ms step_avg:137.20ms
step:182/1375 train_time:23600ms step_avg:137.21ms
step:183/1375 train_time:23739ms step_avg:137.22ms
step:184/1375 train_time:23878ms step_avg:137.23ms
step:185/1375 train_time:24017ms step_avg:137.24ms
step:186/1375 train_time:24157ms step_avg:137.25ms
step:187/1375 train_time:24295ms step_avg:137.26ms
step:188/1375 train_time:24437ms step_avg:137.29ms
step:189/1375 train_time:24577ms step_avg:137.30ms
step:190/1375 train_time:24717ms step_avg:137.31ms
step:191/1375 train_time:24912ms step_avg:137.63ms
step:192/1375 train_time:25049ms step_avg:137.63ms
step:193/1375 train_time:25187ms step_avg:137.64ms
step:194/1375 train_time:25325ms step_avg:137.64ms
step:195/1375 train_time:25464ms step_avg:137.64ms
step:196/1375 train_time:25601ms step_avg:137.64ms
step:197/1375 train_time:25740ms step_avg:137.65ms
step:198/1375 train_time:25883ms step_avg:137.68ms
step:199/1375 train_time:26025ms step_avg:137.70ms
step:200/1375 train_time:26164ms step_avg:137.70ms
step:201/1375 train_time:26301ms step_avg:137.70ms
step:202/1375 train_time:26439ms step_avg:137.70ms
step:203/1375 train_time:26577ms step_avg:137.70ms
step:204/1375 train_time:26715ms step_avg:137.71ms
step:205/1375 train_time:26856ms step_avg:137.72ms
step:206/1375 train_time:26998ms step_avg:137.75ms
step:207/1375 train_time:27142ms step_avg:137.78ms
step:208/1375 train_time:27283ms step_avg:137.79ms
step:209/1375 train_time:27424ms step_avg:137.81ms
step:210/1375 train_time:27564ms step_avg:137.82ms
step:211/1375 train_time:27705ms step_avg:137.84ms
step:212/1375 train_time:27848ms step_avg:137.86ms
step:213/1375 train_time:27990ms step_avg:137.88ms
step:214/1375 train_time:28133ms step_avg:137.91ms
step:215/1375 train_time:28274ms step_avg:137.92ms
step:216/1375 train_time:28415ms step_avg:137.94ms
step:217/1375 train_time:28558ms step_avg:137.96ms
step:218/1375 train_time:28698ms step_avg:137.97ms
step:219/1375 train_time:28842ms step_avg:138.00ms
step:220/1375 train_time:28984ms step_avg:138.02ms
step:221/1375 train_time:29126ms step_avg:138.04ms
step:222/1375 train_time:29267ms step_avg:138.05ms
step:223/1375 train_time:29410ms step_avg:138.07ms
step:224/1375 train_time:29552ms step_avg:138.09ms
step:225/1375 train_time:29692ms step_avg:138.10ms
step:226/1375 train_time:29835ms step_avg:138.13ms
step:227/1375 train_time:29977ms step_avg:138.14ms
step:228/1375 train_time:30119ms step_avg:138.16ms
step:229/1375 train_time:30263ms step_avg:138.18ms
step:230/1375 train_time:30405ms step_avg:138.20ms
step:231/1375 train_time:30547ms step_avg:138.22ms
step:232/1375 train_time:30688ms step_avg:138.23ms
step:233/1375 train_time:30829ms step_avg:138.25ms
step:234/1375 train_time:30971ms step_avg:138.26ms
step:235/1375 train_time:31113ms step_avg:138.28ms
step:236/1375 train_time:31254ms step_avg:138.29ms
step:237/1375 train_time:31396ms step_avg:138.31ms
step:238/1375 train_time:31541ms step_avg:138.34ms
step:239/1375 train_time:31682ms step_avg:138.35ms
step:240/1375 train_time:31824ms step_avg:138.37ms
step:241/1375 train_time:31967ms step_avg:138.38ms
step:242/1375 train_time:32109ms step_avg:138.40ms
step:243/1375 train_time:32252ms step_avg:138.42ms
step:244/1375 train_time:32392ms step_avg:138.43ms
step:245/1375 train_time:32535ms step_avg:138.45ms
step:246/1375 train_time:32677ms step_avg:138.46ms
step:247/1375 train_time:32820ms step_avg:138.48ms
step:248/1375 train_time:32961ms step_avg:138.49ms
step:249/1375 train_time:33104ms step_avg:138.51ms
step:250/1375 train_time:33248ms step_avg:138.53ms
step:250/1375 val_loss:3.9535 train_time:33318ms step_avg:138.82ms
step:251/1375 train_time:33392ms step_avg:138.56ms
step:252/1375 train_time:33537ms step_avg:138.58ms
step:253/1375 train_time:33680ms step_avg:138.60ms
step:254/1375 train_time:33822ms step_avg:138.61ms
step:255/1375 train_time:33963ms step_avg:138.63ms
step:256/1375 train_time:34103ms step_avg:138.63ms
step:257/1375 train_time:34244ms step_avg:138.64ms
step:258/1375 train_time:34389ms step_avg:138.66ms
step:259/1375 train_time:34531ms step_avg:138.68ms
step:260/1375 train_time:34674ms step_avg:138.70ms
step:261/1375 train_time:34816ms step_avg:138.71ms
step:262/1375 train_time:34956ms step_avg:138.71ms
step:263/1375 train_time:35097ms step_avg:138.72ms
step:264/1375 train_time:35237ms step_avg:138.73ms
step:265/1375 train_time:35378ms step_avg:138.74ms
step:266/1375 train_time:35521ms step_avg:138.75ms
step:267/1375 train_time:35664ms step_avg:138.77ms
step:268/1375 train_time:35806ms step_avg:138.78ms
step:269/1375 train_time:35950ms step_avg:138.80ms
step:270/1375 train_time:36090ms step_avg:138.81ms
step:271/1375 train_time:36232ms step_avg:138.82ms
step:272/1375 train_time:36373ms step_avg:138.83ms
step:273/1375 train_time:36515ms step_avg:138.84ms
step:274/1375 train_time:36659ms step_avg:138.86ms
step:275/1375 train_time:36801ms step_avg:138.87ms
step:276/1375 train_time:36942ms step_avg:138.88ms
step:277/1375 train_time:37084ms step_avg:138.89ms
step:278/1375 train_time:37225ms step_avg:138.90ms
step:279/1375 train_time:37367ms step_avg:138.91ms
step:280/1375 train_time:37507ms step_avg:138.91ms
step:281/1375 train_time:37650ms step_avg:138.93ms
step:282/1375 train_time:37793ms step_avg:138.94ms
step:283/1375 train_time:37935ms step_avg:138.96ms
step:284/1375 train_time:38076ms step_avg:138.96ms
step:285/1375 train_time:38218ms step_avg:138.97ms
step:286/1375 train_time:38359ms step_avg:138.98ms
step:287/1375 train_time:38501ms step_avg:138.99ms
step:288/1375 train_time:38644ms step_avg:139.01ms
step:289/1375 train_time:38786ms step_avg:139.02ms
step:290/1375 train_time:38929ms step_avg:139.03ms
step:291/1375 train_time:39071ms step_avg:139.04ms
step:292/1375 train_time:39213ms step_avg:139.05ms
step:293/1375 train_time:39356ms step_avg:139.07ms
step:294/1375 train_time:39498ms step_avg:139.08ms
step:295/1375 train_time:39642ms step_avg:139.09ms
step:296/1375 train_time:39784ms step_avg:139.10ms
step:297/1375 train_time:39926ms step_avg:139.12ms
step:298/1375 train_time:40069ms step_avg:139.13ms
step:299/1375 train_time:40211ms step_avg:139.14ms
step:300/1375 train_time:40353ms step_avg:139.15ms
step:301/1375 train_time:40494ms step_avg:139.15ms
step:302/1375 train_time:40635ms step_avg:139.16ms
step:303/1375 train_time:40778ms step_avg:139.17ms
step:304/1375 train_time:40921ms step_avg:139.19ms
step:305/1375 train_time:41062ms step_avg:139.19ms
step:306/1375 train_time:41203ms step_avg:139.20ms
step:307/1375 train_time:41347ms step_avg:139.21ms
step:308/1375 train_time:41491ms step_avg:139.23ms
step:309/1375 train_time:41635ms step_avg:139.25ms
step:310/1375 train_time:41780ms step_avg:139.27ms
step:311/1375 train_time:41924ms step_avg:139.28ms
step:312/1375 train_time:42070ms step_avg:139.30ms
step:313/1375 train_time:42213ms step_avg:139.32ms
step:314/1375 train_time:42357ms step_avg:139.33ms
step:315/1375 train_time:42500ms step_avg:139.34ms
step:316/1375 train_time:42645ms step_avg:139.36ms
step:317/1375 train_time:42789ms step_avg:139.38ms
step:318/1375 train_time:42934ms step_avg:139.39ms
step:319/1375 train_time:43078ms step_avg:139.41ms
step:320/1375 train_time:43222ms step_avg:139.43ms
step:321/1375 train_time:43367ms step_avg:139.44ms
step:322/1375 train_time:43511ms step_avg:139.46ms
step:323/1375 train_time:43654ms step_avg:139.47ms
step:324/1375 train_time:43798ms step_avg:139.48ms
step:325/1375 train_time:43943ms step_avg:139.50ms
step:326/1375 train_time:44087ms step_avg:139.52ms
step:327/1375 train_time:44230ms step_avg:139.53ms
step:328/1375 train_time:44374ms step_avg:139.54ms
step:329/1375 train_time:44517ms step_avg:139.55ms
step:330/1375 train_time:44660ms step_avg:139.56ms
step:331/1375 train_time:44803ms step_avg:139.57ms
step:332/1375 train_time:44948ms step_avg:139.59ms
step:333/1375 train_time:45091ms step_avg:139.60ms
step:334/1375 train_time:45237ms step_avg:139.62ms
step:335/1375 train_time:45383ms step_avg:139.64ms
step:336/1375 train_time:45527ms step_avg:139.65ms
step:337/1375 train_time:45670ms step_avg:139.66ms
step:338/1375 train_time:45813ms step_avg:139.67ms
step:339/1375 train_time:45957ms step_avg:139.69ms
step:340/1375 train_time:46100ms step_avg:139.70ms
step:341/1375 train_time:46244ms step_avg:139.71ms
step:342/1375 train_time:46388ms step_avg:139.72ms
step:343/1375 train_time:46531ms step_avg:139.73ms
step:344/1375 train_time:46674ms step_avg:139.74ms
step:345/1375 train_time:46818ms step_avg:139.75ms
step:346/1375 train_time:46963ms step_avg:139.77ms
step:347/1375 train_time:47106ms step_avg:139.78ms
step:348/1375 train_time:47251ms step_avg:139.80ms
step:349/1375 train_time:47395ms step_avg:139.81ms
step:350/1375 train_time:47539ms step_avg:139.82ms
step:351/1375 train_time:47682ms step_avg:139.83ms
step:352/1375 train_time:47826ms step_avg:139.84ms
step:353/1375 train_time:47971ms step_avg:139.86ms
step:354/1375 train_time:48115ms step_avg:139.87ms
step:355/1375 train_time:48260ms step_avg:139.88ms
step:356/1375 train_time:48402ms step_avg:139.89ms
step:357/1375 train_time:48548ms step_avg:139.91ms
step:358/1375 train_time:48692ms step_avg:139.92ms
step:359/1375 train_time:48835ms step_avg:139.93ms
step:360/1375 train_time:48979ms step_avg:139.94ms
step:361/1375 train_time:49125ms step_avg:139.96ms
step:362/1375 train_time:49268ms step_avg:139.97ms
step:363/1375 train_time:49412ms step_avg:139.98ms
step:364/1375 train_time:49557ms step_avg:139.99ms
step:365/1375 train_time:49700ms step_avg:140.00ms
step:366/1375 train_time:49847ms step_avg:140.02ms
step:367/1375 train_time:49992ms step_avg:140.03ms
step:368/1375 train_time:50136ms step_avg:140.04ms
step:369/1375 train_time:50279ms step_avg:140.05ms
step:370/1375 train_time:50425ms step_avg:140.07ms
step:371/1375 train_time:50569ms step_avg:140.08ms
step:372/1375 train_time:50713ms step_avg:140.09ms
step:373/1375 train_time:50856ms step_avg:140.10ms
step:374/1375 train_time:50999ms step_avg:140.11ms
step:375/1375 train_time:51143ms step_avg:140.12ms
step:375/1375 val_loss:3.7733 train_time:51214ms step_avg:140.31ms
step:376/1375 train_time:51289ms step_avg:140.13ms
step:377/1375 train_time:51435ms step_avg:140.15ms
step:378/1375 train_time:51582ms step_avg:140.17ms
step:379/1375 train_time:51725ms step_avg:140.18ms
step:380/1375 train_time:51868ms step_avg:140.18ms
step:381/1375 train_time:52063ms step_avg:140.33ms
step:382/1375 train_time:52204ms step_avg:140.33ms
step:383/1375 train_time:52346ms step_avg:140.34ms
step:384/1375 train_time:52489ms step_avg:140.34ms
step:385/1375 train_time:52631ms step_avg:140.35ms
step:386/1375 train_time:52775ms step_avg:140.36ms
step:387/1375 train_time:52920ms step_avg:140.37ms
step:388/1375 train_time:53066ms step_avg:140.39ms
step:389/1375 train_time:53210ms step_avg:140.39ms
step:390/1375 train_time:53353ms step_avg:140.40ms
step:391/1375 train_time:53497ms step_avg:140.41ms
step:392/1375 train_time:53642ms step_avg:140.42ms
step:393/1375 train_time:53785ms step_avg:140.43ms
step:394/1375 train_time:53929ms step_avg:140.44ms
step:395/1375 train_time:54074ms step_avg:140.45ms
step:396/1375 train_time:54218ms step_avg:140.46ms
step:397/1375 train_time:54362ms step_avg:140.47ms
step:398/1375 train_time:54505ms step_avg:140.48ms
step:399/1375 train_time:54649ms step_avg:140.49ms
step:400/1375 train_time:54793ms step_avg:140.49ms
step:401/1375 train_time:54937ms step_avg:140.50ms
step:402/1375 train_time:55081ms step_avg:140.51ms
step:403/1375 train_time:55225ms step_avg:140.52ms
step:404/1375 train_time:55368ms step_avg:140.53ms
step:405/1375 train_time:55512ms step_avg:140.54ms
step:406/1375 train_time:55656ms step_avg:140.55ms
step:407/1375 train_time:55802ms step_avg:140.56ms
step:408/1375 train_time:55946ms step_avg:140.57ms
step:409/1375 train_time:56091ms step_avg:140.58ms
step:410/1375 train_time:56236ms step_avg:140.59ms
step:411/1375 train_time:56383ms step_avg:140.61ms
step:412/1375 train_time:56526ms step_avg:140.61ms
step:413/1375 train_time:56671ms step_avg:140.62ms
step:414/1375 train_time:56817ms step_avg:140.64ms
step:415/1375 train_time:56964ms step_avg:140.65ms
step:416/1375 train_time:57109ms step_avg:140.66ms
step:417/1375 train_time:57257ms step_avg:140.68ms
step:418/1375 train_time:57403ms step_avg:140.69ms
step:419/1375 train_time:57548ms step_avg:140.70ms
step:420/1375 train_time:57693ms step_avg:140.71ms
step:421/1375 train_time:57839ms step_avg:140.73ms
step:422/1375 train_time:57984ms step_avg:140.74ms
step:423/1375 train_time:58130ms step_avg:140.75ms
step:424/1375 train_time:58275ms step_avg:140.76ms
step:425/1375 train_time:58422ms step_avg:140.78ms
step:426/1375 train_time:58568ms step_avg:140.79ms
step:427/1375 train_time:58713ms step_avg:140.80ms
step:428/1375 train_time:58859ms step_avg:140.81ms
step:429/1375 train_time:59006ms step_avg:140.82ms
step:430/1375 train_time:59152ms step_avg:140.84ms
step:431/1375 train_time:59297ms step_avg:140.85ms
step:432/1375 train_time:59443ms step_avg:140.86ms
step:433/1375 train_time:59589ms step_avg:140.87ms
step:434/1375 train_time:59734ms step_avg:140.88ms
step:435/1375 train_time:59880ms step_avg:140.90ms
step:436/1375 train_time:60024ms step_avg:140.90ms
step:437/1375 train_time:60171ms step_avg:140.92ms
step:438/1375 train_time:60317ms step_avg:140.93ms
step:439/1375 train_time:60464ms step_avg:140.94ms
step:440/1375 train_time:60608ms step_avg:140.95ms
step:441/1375 train_time:60754ms step_avg:140.96ms
step:442/1375 train_time:60900ms step_avg:140.97ms
step:443/1375 train_time:61047ms step_avg:140.99ms
step:444/1375 train_time:61193ms step_avg:141.00ms
step:445/1375 train_time:61339ms step_avg:141.01ms
step:446/1375 train_time:61486ms step_avg:141.02ms
step:447/1375 train_time:61631ms step_avg:141.03ms
step:448/1375 train_time:61777ms step_avg:141.04ms
step:449/1375 train_time:61923ms step_avg:141.05ms
step:450/1375 train_time:62069ms step_avg:141.07ms
step:451/1375 train_time:62216ms step_avg:141.08ms
step:452/1375 train_time:62363ms step_avg:141.09ms
step:453/1375 train_time:62509ms step_avg:141.10ms
step:454/1375 train_time:62655ms step_avg:141.11ms
step:455/1375 train_time:62801ms step_avg:141.13ms
step:456/1375 train_time:62947ms step_avg:141.14ms
step:457/1375 train_time:63092ms step_avg:141.15ms
step:458/1375 train_time:63239ms step_avg:141.16ms
step:459/1375 train_time:63384ms step_avg:141.17ms
step:460/1375 train_time:63531ms step_avg:141.18ms
step:461/1375 train_time:63677ms step_avg:141.19ms
step:462/1375 train_time:63823ms step_avg:141.20ms
step:463/1375 train_time:63968ms step_avg:141.21ms
step:464/1375 train_time:64114ms step_avg:141.22ms
step:465/1375 train_time:64260ms step_avg:141.23ms
step:466/1375 train_time:64406ms step_avg:141.24ms
step:467/1375 train_time:64553ms step_avg:141.25ms
step:468/1375 train_time:64699ms step_avg:141.26ms
step:469/1375 train_time:64846ms step_avg:141.28ms
step:470/1375 train_time:64991ms step_avg:141.28ms
step:471/1375 train_time:65137ms step_avg:141.29ms
step:472/1375 train_time:65284ms step_avg:141.31ms
step:473/1375 train_time:65428ms step_avg:141.31ms
step:474/1375 train_time:65574ms step_avg:141.32ms
step:475/1375 train_time:65720ms step_avg:141.33ms
step:476/1375 train_time:65867ms step_avg:141.34ms
step:477/1375 train_time:66013ms step_avg:141.35ms
step:478/1375 train_time:66158ms step_avg:141.36ms
step:479/1375 train_time:66303ms step_avg:141.37ms
step:480/1375 train_time:66448ms step_avg:141.38ms
step:481/1375 train_time:66594ms step_avg:141.39ms
step:482/1375 train_time:66742ms step_avg:141.40ms
step:483/1375 train_time:66889ms step_avg:141.41ms
step:484/1375 train_time:67035ms step_avg:141.42ms
step:485/1375 train_time:67180ms step_avg:141.43ms
step:486/1375 train_time:67324ms step_avg:141.44ms
step:487/1375 train_time:67469ms step_avg:141.44ms
step:488/1375 train_time:67615ms step_avg:141.45ms
step:489/1375 train_time:67762ms step_avg:141.47ms
step:490/1375 train_time:67907ms step_avg:141.47ms
step:491/1375 train_time:68055ms step_avg:141.49ms
step:492/1375 train_time:68200ms step_avg:141.49ms
step:493/1375 train_time:68345ms step_avg:141.50ms
step:494/1375 train_time:68490ms step_avg:141.51ms
step:495/1375 train_time:68637ms step_avg:141.52ms
step:496/1375 train_time:68783ms step_avg:141.53ms
step:497/1375 train_time:68928ms step_avg:141.54ms
step:498/1375 train_time:69072ms step_avg:141.54ms
step:499/1375 train_time:69219ms step_avg:141.55ms
step:500/1375 train_time:69364ms step_avg:141.56ms
step:500/1375 val_loss:3.6560 train_time:69436ms step_avg:141.71ms
step:501/1375 train_time:69510ms step_avg:141.57ms
step:502/1375 train_time:69658ms step_avg:141.58ms
step:503/1375 train_time:69803ms step_avg:141.59ms
step:504/1375 train_time:69948ms step_avg:141.59ms
step:505/1375 train_time:70093ms step_avg:141.60ms
step:506/1375 train_time:70238ms step_avg:141.61ms
step:507/1375 train_time:70383ms step_avg:141.62ms
step:508/1375 train_time:70532ms step_avg:141.63ms
step:509/1375 train_time:70681ms step_avg:141.65ms
step:510/1375 train_time:70826ms step_avg:141.65ms
step:511/1375 train_time:70971ms step_avg:141.66ms
step:512/1375 train_time:71119ms step_avg:141.67ms
step:513/1375 train_time:71266ms step_avg:141.68ms
step:514/1375 train_time:71414ms step_avg:141.70ms
step:515/1375 train_time:71562ms step_avg:141.71ms
step:516/1375 train_time:71710ms step_avg:141.72ms
step:517/1375 train_time:71857ms step_avg:141.73ms
step:518/1375 train_time:72004ms step_avg:141.74ms
step:519/1375 train_time:72151ms step_avg:141.75ms
step:520/1375 train_time:72298ms step_avg:141.76ms
step:521/1375 train_time:72446ms step_avg:141.77ms
step:522/1375 train_time:72594ms step_avg:141.79ms
step:523/1375 train_time:72743ms step_avg:141.80ms
step:524/1375 train_time:72891ms step_avg:141.81ms
step:525/1375 train_time:73038ms step_avg:141.82ms
step:526/1375 train_time:73185ms step_avg:141.83ms
step:527/1375 train_time:73332ms step_avg:141.84ms
step:528/1375 train_time:73479ms step_avg:141.85ms
step:529/1375 train_time:73626ms step_avg:141.86ms
step:530/1375 train_time:73773ms step_avg:141.87ms
step:531/1375 train_time:73922ms step_avg:141.88ms
step:532/1375 train_time:74069ms step_avg:141.89ms
step:533/1375 train_time:74216ms step_avg:141.90ms
step:534/1375 train_time:74362ms step_avg:141.91ms
step:535/1375 train_time:74508ms step_avg:141.92ms
step:536/1375 train_time:74657ms step_avg:141.93ms
step:537/1375 train_time:74804ms step_avg:141.94ms
step:538/1375 train_time:74951ms step_avg:141.95ms
step:539/1375 train_time:75100ms step_avg:141.97ms
step:540/1375 train_time:75246ms step_avg:141.97ms
step:541/1375 train_time:75394ms step_avg:141.98ms
step:542/1375 train_time:75542ms step_avg:142.00ms
step:543/1375 train_time:75688ms step_avg:142.00ms
step:544/1375 train_time:75835ms step_avg:142.01ms
step:545/1375 train_time:75982ms step_avg:142.02ms
step:546/1375 train_time:76131ms step_avg:142.03ms
step:547/1375 train_time:76278ms step_avg:142.04ms
step:548/1375 train_time:76425ms step_avg:142.05ms
step:549/1375 train_time:76573ms step_avg:142.07ms
step:550/1375 train_time:76723ms step_avg:142.08ms
step:551/1375 train_time:76869ms step_avg:142.09ms
step:552/1375 train_time:77016ms step_avg:142.10ms
step:553/1375 train_time:77163ms step_avg:142.11ms
step:554/1375 train_time:77311ms step_avg:142.12ms
step:555/1375 train_time:77459ms step_avg:142.13ms
step:556/1375 train_time:77605ms step_avg:142.13ms
step:557/1375 train_time:77753ms step_avg:142.14ms
step:558/1375 train_time:77901ms step_avg:142.15ms
step:559/1375 train_time:78047ms step_avg:142.16ms
step:560/1375 train_time:78195ms step_avg:142.17ms
step:561/1375 train_time:78342ms step_avg:142.18ms
step:562/1375 train_time:78488ms step_avg:142.19ms
step:563/1375 train_time:78636ms step_avg:142.20ms
step:564/1375 train_time:78783ms step_avg:142.21ms
step:565/1375 train_time:78930ms step_avg:142.22ms
step:566/1375 train_time:79077ms step_avg:142.22ms
step:567/1375 train_time:79225ms step_avg:142.23ms
step:568/1375 train_time:79372ms step_avg:142.24ms
step:569/1375 train_time:79521ms step_avg:142.26ms
step:570/1375 train_time:79668ms step_avg:142.26ms
step:571/1375 train_time:79859ms step_avg:142.35ms
step:572/1375 train_time:80004ms step_avg:142.36ms
step:573/1375 train_time:80151ms step_avg:142.36ms
step:574/1375 train_time:80298ms step_avg:142.37ms
step:575/1375 train_time:80445ms step_avg:142.38ms
step:576/1375 train_time:80590ms step_avg:142.39ms
step:577/1375 train_time:80739ms step_avg:142.40ms
step:578/1375 train_time:80887ms step_avg:142.41ms
step:579/1375 train_time:81034ms step_avg:142.41ms
step:580/1375 train_time:81182ms step_avg:142.42ms
step:581/1375 train_time:81328ms step_avg:142.43ms
step:582/1375 train_time:81473ms step_avg:142.44ms
step:583/1375 train_time:81620ms step_avg:142.44ms
step:584/1375 train_time:81769ms step_avg:142.45ms
step:585/1375 train_time:81916ms step_avg:142.46ms
step:586/1375 train_time:82064ms step_avg:142.47ms
step:587/1375 train_time:82211ms step_avg:142.48ms
step:588/1375 train_time:82358ms step_avg:142.49ms
step:589/1375 train_time:82506ms step_avg:142.50ms
step:590/1375 train_time:82653ms step_avg:142.50ms
step:591/1375 train_time:82801ms step_avg:142.51ms
step:592/1375 train_time:82949ms step_avg:142.52ms
step:593/1375 train_time:83098ms step_avg:142.54ms
step:594/1375 train_time:83244ms step_avg:142.54ms
step:595/1375 train_time:83391ms step_avg:142.55ms
step:596/1375 train_time:83540ms step_avg:142.56ms
step:597/1375 train_time:83686ms step_avg:142.57ms
step:598/1375 train_time:83834ms step_avg:142.57ms
step:599/1375 train_time:83982ms step_avg:142.58ms
step:600/1375 train_time:84129ms step_avg:142.59ms
step:601/1375 train_time:84275ms step_avg:142.60ms
step:602/1375 train_time:84422ms step_avg:142.61ms
step:603/1375 train_time:84570ms step_avg:142.61ms
step:604/1375 train_time:84718ms step_avg:142.62ms
step:605/1375 train_time:84865ms step_avg:142.63ms
step:606/1375 train_time:85014ms step_avg:142.64ms
step:607/1375 train_time:85161ms step_avg:142.65ms
step:608/1375 train_time:85306ms step_avg:142.65ms
step:609/1375 train_time:85453ms step_avg:142.66ms
step:610/1375 train_time:85601ms step_avg:142.67ms
step:611/1375 train_time:85747ms step_avg:142.67ms
step:612/1375 train_time:85896ms step_avg:142.69ms
step:613/1375 train_time:86045ms step_avg:142.69ms
step:614/1375 train_time:86195ms step_avg:142.71ms
step:615/1375 train_time:86344ms step_avg:142.72ms
step:616/1375 train_time:86493ms step_avg:142.73ms
step:617/1375 train_time:86643ms step_avg:142.74ms
step:618/1375 train_time:86791ms step_avg:142.75ms
step:619/1375 train_time:86940ms step_avg:142.76ms
step:620/1375 train_time:87089ms step_avg:142.77ms
step:621/1375 train_time:87239ms step_avg:142.78ms
step:622/1375 train_time:87389ms step_avg:142.79ms
step:623/1375 train_time:87538ms step_avg:142.80ms
step:624/1375 train_time:87687ms step_avg:142.81ms
step:625/1375 train_time:87836ms step_avg:142.82ms
step:625/1375 val_loss:3.5731 train_time:87910ms step_avg:142.94ms
step:626/1375 train_time:87986ms step_avg:142.83ms
step:627/1375 train_time:88134ms step_avg:142.84ms
step:628/1375 train_time:88282ms step_avg:142.85ms
step:629/1375 train_time:88430ms step_avg:142.86ms
step:630/1375 train_time:88578ms step_avg:142.87ms
step:631/1375 train_time:88725ms step_avg:142.87ms
step:632/1375 train_time:88875ms step_avg:142.89ms
step:633/1375 train_time:89025ms step_avg:142.90ms
step:634/1375 train_time:89176ms step_avg:142.91ms
step:635/1375 train_time:89323ms step_avg:142.92ms
step:636/1375 train_time:89472ms step_avg:142.93ms
step:637/1375 train_time:89620ms step_avg:142.93ms
step:638/1375 train_time:89767ms step_avg:142.94ms
step:639/1375 train_time:89916ms step_avg:142.95ms
step:640/1375 train_time:90065ms step_avg:142.96ms
step:641/1375 train_time:90215ms step_avg:142.97ms
step:642/1375 train_time:90365ms step_avg:142.98ms
step:643/1375 train_time:90515ms step_avg:142.99ms
step:644/1375 train_time:90662ms step_avg:143.00ms
step:645/1375 train_time:90811ms step_avg:143.01ms
step:646/1375 train_time:90960ms step_avg:143.02ms
step:647/1375 train_time:91109ms step_avg:143.03ms
step:648/1375 train_time:91261ms step_avg:143.04ms
step:649/1375 train_time:91411ms step_avg:143.05ms
step:650/1375 train_time:91562ms step_avg:143.07ms
step:651/1375 train_time:91710ms step_avg:143.07ms
step:652/1375 train_time:91859ms step_avg:143.08ms
step:653/1375 train_time:92007ms step_avg:143.09ms
step:654/1375 train_time:92157ms step_avg:143.10ms
step:655/1375 train_time:92306ms step_avg:143.11ms
step:656/1375 train_time:92455ms step_avg:143.12ms
step:657/1375 train_time:92602ms step_avg:143.13ms
step:658/1375 train_time:92753ms step_avg:143.14ms
step:659/1375 train_time:92899ms step_avg:143.14ms
step:660/1375 train_time:93048ms step_avg:143.15ms
step:661/1375 train_time:93200ms step_avg:143.16ms
step:662/1375 train_time:93350ms step_avg:143.18ms
step:663/1375 train_time:93498ms step_avg:143.18ms
step:664/1375 train_time:93650ms step_avg:143.20ms
step:665/1375 train_time:93799ms step_avg:143.20ms
step:666/1375 train_time:93946ms step_avg:143.21ms
step:667/1375 train_time:94094ms step_avg:143.22ms
step:668/1375 train_time:94244ms step_avg:143.23ms
step:669/1375 train_time:94394ms step_avg:143.24ms
step:670/1375 train_time:94541ms step_avg:143.24ms
step:671/1375 train_time:94692ms step_avg:143.26ms
step:672/1375 train_time:94840ms step_avg:143.26ms
step:673/1375 train_time:94989ms step_avg:143.27ms
step:674/1375 train_time:95138ms step_avg:143.28ms
step:675/1375 train_time:95287ms step_avg:143.29ms
step:676/1375 train_time:95435ms step_avg:143.30ms
step:677/1375 train_time:95582ms step_avg:143.30ms
step:678/1375 train_time:95730ms step_avg:143.31ms
step:679/1375 train_time:95880ms step_avg:143.32ms
step:680/1375 train_time:96028ms step_avg:143.33ms
step:681/1375 train_time:96178ms step_avg:143.34ms
step:682/1375 train_time:96327ms step_avg:143.34ms
step:683/1375 train_time:96475ms step_avg:143.35ms
step:684/1375 train_time:96624ms step_avg:143.36ms
step:685/1375 train_time:96773ms step_avg:143.37ms
step:686/1375 train_time:96920ms step_avg:143.37ms
step:687/1375 train_time:97068ms step_avg:143.38ms
step:688/1375 train_time:97218ms step_avg:143.39ms
step:689/1375 train_time:97367ms step_avg:143.40ms
step:690/1375 train_time:97519ms step_avg:143.41ms
step:691/1375 train_time:97666ms step_avg:143.42ms
step:692/1375 train_time:97815ms step_avg:143.42ms
step:693/1375 train_time:97964ms step_avg:143.43ms
step:694/1375 train_time:98114ms step_avg:143.44ms
step:695/1375 train_time:98260ms step_avg:143.44ms
step:696/1375 train_time:98411ms step_avg:143.46ms
step:697/1375 train_time:98558ms step_avg:143.46ms
step:698/1375 train_time:98706ms step_avg:143.47ms
step:699/1375 train_time:98855ms step_avg:143.48ms
step:700/1375 train_time:99003ms step_avg:143.48ms
step:701/1375 train_time:99151ms step_avg:143.49ms
step:702/1375 train_time:99299ms step_avg:143.50ms
step:703/1375 train_time:99449ms step_avg:143.50ms
step:704/1375 train_time:99598ms step_avg:143.51ms
step:705/1375 train_time:99749ms step_avg:143.52ms
step:706/1375 train_time:99901ms step_avg:143.54ms
step:707/1375 train_time:100051ms step_avg:143.54ms
step:708/1375 train_time:100198ms step_avg:143.55ms
step:709/1375 train_time:100348ms step_avg:143.56ms
step:710/1375 train_time:100497ms step_avg:143.57ms
step:711/1375 train_time:100647ms step_avg:143.58ms
step:712/1375 train_time:100798ms step_avg:143.59ms
step:713/1375 train_time:100949ms step_avg:143.60ms
step:714/1375 train_time:101099ms step_avg:143.61ms
step:715/1375 train_time:101250ms step_avg:143.62ms
step:716/1375 train_time:101400ms step_avg:143.63ms
step:717/1375 train_time:101550ms step_avg:143.64ms
step:718/1375 train_time:101699ms step_avg:143.64ms
step:719/1375 train_time:101849ms step_avg:143.65ms
step:720/1375 train_time:102000ms step_avg:143.66ms
step:721/1375 train_time:102150ms step_avg:143.67ms
step:722/1375 train_time:102300ms step_avg:143.68ms
step:723/1375 train_time:102451ms step_avg:143.69ms
step:724/1375 train_time:102601ms step_avg:143.70ms
step:725/1375 train_time:102752ms step_avg:143.71ms
step:726/1375 train_time:102900ms step_avg:143.72ms
step:727/1375 train_time:103053ms step_avg:143.73ms
step:728/1375 train_time:103201ms step_avg:143.73ms
step:729/1375 train_time:103350ms step_avg:143.74ms
step:730/1375 train_time:103501ms step_avg:143.75ms
step:731/1375 train_time:103651ms step_avg:143.76ms
step:732/1375 train_time:103799ms step_avg:143.77ms
step:733/1375 train_time:103950ms step_avg:143.78ms
step:734/1375 train_time:104101ms step_avg:143.79ms
step:735/1375 train_time:104253ms step_avg:143.80ms
step:736/1375 train_time:104403ms step_avg:143.81ms
step:737/1375 train_time:104555ms step_avg:143.82ms
step:738/1375 train_time:104703ms step_avg:143.82ms
step:739/1375 train_time:104856ms step_avg:143.83ms
step:740/1375 train_time:105006ms step_avg:143.84ms
step:741/1375 train_time:105157ms step_avg:143.85ms
step:742/1375 train_time:105308ms step_avg:143.86ms
step:743/1375 train_time:105459ms step_avg:143.87ms
step:744/1375 train_time:105608ms step_avg:143.88ms
step:745/1375 train_time:105762ms step_avg:143.89ms
step:746/1375 train_time:105911ms step_avg:143.90ms
step:747/1375 train_time:106060ms step_avg:143.91ms
step:748/1375 train_time:106211ms step_avg:143.92ms
step:749/1375 train_time:106360ms step_avg:143.92ms
step:750/1375 train_time:106511ms step_avg:143.93ms
step:750/1375 val_loss:3.5195 train_time:106587ms step_avg:144.04ms
step:751/1375 train_time:106664ms step_avg:143.95ms
step:752/1375 train_time:106817ms step_avg:143.96ms
step:753/1375 train_time:106967ms step_avg:143.97ms
step:754/1375 train_time:107114ms step_avg:143.97ms
step:755/1375 train_time:107263ms step_avg:143.98ms
step:756/1375 train_time:107412ms step_avg:143.98ms
step:757/1375 train_time:107565ms step_avg:144.00ms
step:758/1375 train_time:107716ms step_avg:144.01ms
step:759/1375 train_time:107869ms step_avg:144.02ms
step:760/1375 train_time:108017ms step_avg:144.02ms
step:761/1375 train_time:108208ms step_avg:144.08ms
step:762/1375 train_time:108356ms step_avg:144.09ms
step:763/1375 train_time:108506ms step_avg:144.10ms
step:764/1375 train_time:108654ms step_avg:144.10ms
step:765/1375 train_time:108803ms step_avg:144.11ms
step:766/1375 train_time:108952ms step_avg:144.12ms
step:767/1375 train_time:109104ms step_avg:144.13ms
step:768/1375 train_time:109257ms step_avg:144.14ms
step:769/1375 train_time:109409ms step_avg:144.15ms
step:770/1375 train_time:109559ms step_avg:144.16ms
step:771/1375 train_time:109709ms step_avg:144.16ms
step:772/1375 train_time:109857ms step_avg:144.17ms
step:773/1375 train_time:110008ms step_avg:144.18ms
step:774/1375 train_time:110157ms step_avg:144.18ms
step:775/1375 train_time:110307ms step_avg:144.19ms
step:776/1375 train_time:110459ms step_avg:144.20ms
step:777/1375 train_time:110611ms step_avg:144.21ms
step:778/1375 train_time:110761ms step_avg:144.22ms
step:779/1375 train_time:110910ms step_avg:144.23ms
step:780/1375 train_time:111062ms step_avg:144.24ms
step:781/1375 train_time:111211ms step_avg:144.24ms
step:782/1375 train_time:111360ms step_avg:144.25ms
step:783/1375 train_time:111510ms step_avg:144.26ms
step:784/1375 train_time:111662ms step_avg:144.27ms
step:785/1375 train_time:111811ms step_avg:144.27ms
step:786/1375 train_time:111962ms step_avg:144.28ms
step:787/1375 train_time:112112ms step_avg:144.29ms
step:788/1375 train_time:112263ms step_avg:144.30ms
step:789/1375 train_time:112411ms step_avg:144.30ms
step:790/1375 train_time:112561ms step_avg:144.31ms
step:791/1375 train_time:112710ms step_avg:144.31ms
step:792/1375 train_time:112862ms step_avg:144.32ms
step:793/1375 train_time:113010ms step_avg:144.33ms
step:794/1375 train_time:113160ms step_avg:144.34ms
step:795/1375 train_time:113311ms step_avg:144.35ms
step:796/1375 train_time:113463ms step_avg:144.35ms
step:797/1375 train_time:113612ms step_avg:144.36ms
step:798/1375 train_time:113763ms step_avg:144.37ms
step:799/1375 train_time:113916ms step_avg:144.38ms
step:800/1375 train_time:114067ms step_avg:144.39ms
step:801/1375 train_time:114215ms step_avg:144.39ms
step:802/1375 train_time:114367ms step_avg:144.40ms
step:803/1375 train_time:114516ms step_avg:144.41ms
step:804/1375 train_time:114666ms step_avg:144.42ms
step:805/1375 train_time:114820ms step_avg:144.43ms
step:806/1375 train_time:114969ms step_avg:144.43ms
step:807/1375 train_time:115117ms step_avg:144.44ms
step:808/1375 train_time:115267ms step_avg:144.45ms
step:809/1375 train_time:115415ms step_avg:144.45ms
step:810/1375 train_time:115566ms step_avg:144.46ms
step:811/1375 train_time:115716ms step_avg:144.46ms
step:812/1375 train_time:115868ms step_avg:144.47ms
step:813/1375 train_time:116015ms step_avg:144.48ms
step:814/1375 train_time:116165ms step_avg:144.48ms
step:815/1375 train_time:116314ms step_avg:144.49ms
step:816/1375 train_time:116468ms step_avg:144.50ms
step:817/1375 train_time:116619ms step_avg:144.51ms
step:818/1375 train_time:116770ms step_avg:144.52ms
step:819/1375 train_time:116923ms step_avg:144.53ms
step:820/1375 train_time:117074ms step_avg:144.54ms
step:821/1375 train_time:117225ms step_avg:144.54ms
step:822/1375 train_time:117374ms step_avg:144.55ms
step:823/1375 train_time:117527ms step_avg:144.56ms
step:824/1375 train_time:117678ms step_avg:144.57ms
step:825/1375 train_time:117832ms step_avg:144.58ms
step:826/1375 train_time:117983ms step_avg:144.59ms
step:827/1375 train_time:118132ms step_avg:144.59ms
step:828/1375 train_time:118283ms step_avg:144.60ms
step:829/1375 train_time:118433ms step_avg:144.61ms
step:830/1375 train_time:118585ms step_avg:144.62ms
step:831/1375 train_time:118737ms step_avg:144.62ms
step:832/1375 train_time:118889ms step_avg:144.63ms
step:833/1375 train_time:119042ms step_avg:144.64ms
step:834/1375 train_time:119191ms step_avg:144.65ms
step:835/1375 train_time:119344ms step_avg:144.66ms
step:836/1375 train_time:119495ms step_avg:144.67ms
step:837/1375 train_time:119646ms step_avg:144.67ms
step:838/1375 train_time:119798ms step_avg:144.68ms
step:839/1375 train_time:119950ms step_avg:144.69ms
step:840/1375 train_time:120102ms step_avg:144.70ms
step:841/1375 train_time:120252ms step_avg:144.71ms
step:842/1375 train_time:120404ms step_avg:144.72ms
step:843/1375 train_time:120554ms step_avg:144.72ms
step:844/1375 train_time:120705ms step_avg:144.73ms
step:845/1375 train_time:120854ms step_avg:144.74ms
step:846/1375 train_time:121005ms step_avg:144.74ms
step:847/1375 train_time:121158ms step_avg:144.75ms
step:848/1375 train_time:121309ms step_avg:144.76ms
step:849/1375 train_time:121460ms step_avg:144.77ms
step:850/1375 train_time:121612ms step_avg:144.78ms
step:851/1375 train_time:121765ms step_avg:144.79ms
step:852/1375 train_time:121916ms step_avg:144.79ms
step:853/1375 train_time:122066ms step_avg:144.80ms
step:854/1375 train_time:122217ms step_avg:144.81ms
step:855/1375 train_time:122369ms step_avg:144.82ms
step:856/1375 train_time:122518ms step_avg:144.82ms
step:857/1375 train_time:122670ms step_avg:144.83ms
step:858/1375 train_time:122825ms step_avg:144.84ms
step:859/1375 train_time:122977ms step_avg:144.85ms
step:860/1375 train_time:123129ms step_avg:144.86ms
step:861/1375 train_time:123283ms step_avg:144.87ms
step:862/1375 train_time:123434ms step_avg:144.88ms
step:863/1375 train_time:123586ms step_avg:144.88ms
step:864/1375 train_time:123736ms step_avg:144.89ms
step:865/1375 train_time:123887ms step_avg:144.90ms
step:866/1375 train_time:124042ms step_avg:144.91ms
step:867/1375 train_time:124193ms step_avg:144.92ms
step:868/1375 train_time:124344ms step_avg:144.92ms
step:869/1375 train_time:124493ms step_avg:144.93ms
step:870/1375 train_time:124649ms step_avg:144.94ms
step:871/1375 train_time:124800ms step_avg:144.95ms
step:872/1375 train_time:124951ms step_avg:144.95ms
step:873/1375 train_time:125102ms step_avg:144.96ms
step:874/1375 train_time:125253ms step_avg:144.97ms
step:875/1375 train_time:125404ms step_avg:144.98ms
step:875/1375 val_loss:3.4676 train_time:125479ms step_avg:145.06ms
step:876/1375 train_time:125555ms step_avg:144.98ms
step:877/1375 train_time:125711ms step_avg:144.99ms
step:878/1375 train_time:125862ms step_avg:145.00ms
step:879/1375 train_time:126013ms step_avg:145.01ms
step:880/1375 train_time:126164ms step_avg:145.02ms
step:881/1375 train_time:126314ms step_avg:145.02ms
step:882/1375 train_time:126469ms step_avg:145.03ms
step:883/1375 train_time:126619ms step_avg:145.04ms
step:884/1375 train_time:126773ms step_avg:145.05ms
step:885/1375 train_time:126923ms step_avg:145.06ms
step:886/1375 train_time:127078ms step_avg:145.07ms
step:887/1375 train_time:127229ms step_avg:145.07ms
step:888/1375 train_time:127380ms step_avg:145.08ms
step:889/1375 train_time:127534ms step_avg:145.09ms
step:890/1375 train_time:127684ms step_avg:145.10ms
step:891/1375 train_time:127835ms step_avg:145.10ms
step:892/1375 train_time:127989ms step_avg:145.11ms
step:893/1375 train_time:128138ms step_avg:145.12ms
step:894/1375 train_time:128292ms step_avg:145.13ms
step:895/1375 train_time:128447ms step_avg:145.14ms
step:896/1375 train_time:128597ms step_avg:145.14ms
step:897/1375 train_time:128749ms step_avg:145.15ms
step:898/1375 train_time:128900ms step_avg:145.16ms
step:899/1375 train_time:129051ms step_avg:145.16ms
step:900/1375 train_time:129201ms step_avg:145.17ms
step:901/1375 train_time:129356ms step_avg:145.18ms
step:902/1375 train_time:129506ms step_avg:145.19ms
step:903/1375 train_time:129657ms step_avg:145.19ms
step:904/1375 train_time:129809ms step_avg:145.20ms
step:905/1375 train_time:129959ms step_avg:145.21ms
step:906/1375 train_time:130111ms step_avg:145.21ms
step:907/1375 train_time:130268ms step_avg:145.23ms
step:908/1375 train_time:130418ms step_avg:145.23ms
step:909/1375 train_time:130573ms step_avg:145.24ms
step:910/1375 train_time:130730ms step_avg:145.26ms
step:911/1375 train_time:130880ms step_avg:145.26ms
step:912/1375 train_time:131030ms step_avg:145.27ms
step:913/1375 train_time:131184ms step_avg:145.28ms
step:914/1375 train_time:131335ms step_avg:145.28ms
step:915/1375 train_time:131488ms step_avg:145.29ms
step:916/1375 train_time:131638ms step_avg:145.30ms
step:917/1375 train_time:131791ms step_avg:145.30ms
step:918/1375 train_time:131943ms step_avg:145.31ms
step:919/1375 train_time:132100ms step_avg:145.32ms
step:920/1375 train_time:132251ms step_avg:145.33ms
step:921/1375 train_time:132403ms step_avg:145.34ms
step:922/1375 train_time:132560ms step_avg:145.35ms
step:923/1375 train_time:132713ms step_avg:145.36ms
step:924/1375 train_time:132866ms step_avg:145.37ms
step:925/1375 train_time:133019ms step_avg:145.38ms
step:926/1375 train_time:133172ms step_avg:145.38ms
step:927/1375 train_time:133322ms step_avg:145.39ms
step:928/1375 train_time:133476ms step_avg:145.40ms
step:929/1375 train_time:133631ms step_avg:145.41ms
step:930/1375 train_time:133783ms step_avg:145.42ms
step:931/1375 train_time:133935ms step_avg:145.42ms
step:932/1375 train_time:134085ms step_avg:145.43ms
step:933/1375 train_time:134238ms step_avg:145.44ms
step:934/1375 train_time:134391ms step_avg:145.45ms
step:935/1375 train_time:134544ms step_avg:145.45ms
step:936/1375 train_time:134697ms step_avg:145.46ms
step:937/1375 train_time:134855ms step_avg:145.47ms
step:938/1375 train_time:135008ms step_avg:145.48ms
step:939/1375 train_time:135161ms step_avg:145.49ms
step:940/1375 train_time:135316ms step_avg:145.50ms
step:941/1375 train_time:135468ms step_avg:145.51ms
step:942/1375 train_time:135618ms step_avg:145.51ms
step:943/1375 train_time:135774ms step_avg:145.52ms
step:944/1375 train_time:135932ms step_avg:145.54ms
step:945/1375 train_time:136086ms step_avg:145.55ms
step:946/1375 train_time:136239ms step_avg:145.55ms
step:947/1375 train_time:136392ms step_avg:145.56ms
step:948/1375 train_time:136544ms step_avg:145.57ms
step:949/1375 train_time:136697ms step_avg:145.58ms
step:950/1375 train_time:136851ms step_avg:145.59ms
step:951/1375 train_time:137047ms step_avg:145.64ms
step:952/1375 train_time:137198ms step_avg:145.65ms
step:953/1375 train_time:137352ms step_avg:145.65ms
step:954/1375 train_time:137504ms step_avg:145.66ms
step:955/1375 train_time:137654ms step_avg:145.67ms
step:956/1375 train_time:137806ms step_avg:145.67ms
step:957/1375 train_time:137959ms step_avg:145.68ms
step:958/1375 train_time:138116ms step_avg:145.69ms
step:959/1375 train_time:138273ms step_avg:145.70ms
step:960/1375 train_time:138426ms step_avg:145.71ms
step:961/1375 train_time:138578ms step_avg:145.72ms
step:962/1375 train_time:138730ms step_avg:145.72ms
step:963/1375 train_time:138885ms step_avg:145.73ms
step:964/1375 train_time:139038ms step_avg:145.74ms
step:965/1375 train_time:139190ms step_avg:145.75ms
step:966/1375 train_time:139341ms step_avg:145.75ms
step:967/1375 train_time:139494ms step_avg:145.76ms
step:968/1375 train_time:139644ms step_avg:145.77ms
step:969/1375 train_time:139799ms step_avg:145.78ms
step:970/1375 train_time:139951ms step_avg:145.78ms
step:971/1375 train_time:140104ms step_avg:145.79ms
step:972/1375 train_time:140256ms step_avg:145.80ms
step:973/1375 train_time:140407ms step_avg:145.80ms
step:974/1375 train_time:140558ms step_avg:145.81ms
step:975/1375 train_time:140713ms step_avg:145.82ms
step:976/1375 train_time:140864ms step_avg:145.82ms
step:977/1375 train_time:141016ms step_avg:145.83ms
step:978/1375 train_time:141171ms step_avg:145.84ms
step:979/1375 train_time:141321ms step_avg:145.84ms
step:980/1375 train_time:141474ms step_avg:145.85ms
step:981/1375 train_time:141623ms step_avg:145.85ms
step:982/1375 train_time:141775ms step_avg:145.86ms
step:983/1375 train_time:141926ms step_avg:145.86ms
step:984/1375 train_time:142077ms step_avg:145.87ms
step:985/1375 train_time:142231ms step_avg:145.88ms
step:986/1375 train_time:142385ms step_avg:145.89ms
step:987/1375 train_time:142536ms step_avg:145.89ms
step:988/1375 train_time:142688ms step_avg:145.90ms
step:989/1375 train_time:142838ms step_avg:145.90ms
step:990/1375 train_time:142992ms step_avg:145.91ms
step:991/1375 train_time:143145ms step_avg:145.92ms
step:992/1375 train_time:143303ms step_avg:145.93ms
step:993/1375 train_time:143466ms step_avg:145.95ms
step:994/1375 train_time:143618ms step_avg:145.95ms
step:995/1375 train_time:143770ms step_avg:145.96ms
step:996/1375 train_time:143920ms step_avg:145.96ms
step:997/1375 train_time:144074ms step_avg:145.97ms
step:998/1375 train_time:144225ms step_avg:145.98ms
step:999/1375 train_time:144379ms step_avg:145.98ms
step:1000/1375 train_time:144530ms step_avg:145.99ms
step:1000/1375 val_loss:3.4017 train_time:144608ms step_avg:146.07ms
step:1001/1375 train_time:144684ms step_avg:146.00ms
step:1002/1375 train_time:144839ms step_avg:146.01ms
step:1003/1375 train_time:144992ms step_avg:146.01ms
step:1004/1375 train_time:145147ms step_avg:146.02ms
step:1005/1375 train_time:145299ms step_avg:146.03ms
step:1006/1375 train_time:145450ms step_avg:146.03ms
step:1007/1375 train_time:145605ms step_avg:146.04ms
step:1008/1375 train_time:145759ms step_avg:146.05ms
step:1009/1375 train_time:145920ms step_avg:146.07ms
step:1010/1375 train_time:146072ms step_avg:146.07ms
step:1011/1375 train_time:146225ms step_avg:146.08ms
step:1012/1375 train_time:146375ms step_avg:146.08ms
step:1013/1375 train_time:146529ms step_avg:146.09ms
step:1014/1375 train_time:146682ms step_avg:146.10ms
step:1015/1375 train_time:146835ms step_avg:146.10ms
step:1016/1375 train_time:146989ms step_avg:146.11ms
step:1017/1375 train_time:147143ms step_avg:146.12ms
step:1018/1375 train_time:147294ms step_avg:146.12ms
step:1019/1375 train_time:147448ms step_avg:146.13ms
step:1020/1375 train_time:147604ms step_avg:146.14ms
step:1021/1375 train_time:147756ms step_avg:146.15ms
step:1022/1375 train_time:147911ms step_avg:146.16ms
step:1023/1375 train_time:148066ms step_avg:146.17ms
step:1024/1375 train_time:148218ms step_avg:146.17ms
step:1025/1375 train_time:148372ms step_avg:146.18ms
step:1026/1375 train_time:148524ms step_avg:146.18ms
step:1027/1375 train_time:148678ms step_avg:146.19ms
step:1028/1375 train_time:148834ms step_avg:146.20ms
step:1029/1375 train_time:148989ms step_avg:146.21ms
step:1030/1375 train_time:149144ms step_avg:146.22ms
step:1031/1375 train_time:149294ms step_avg:146.22ms
step:1032/1375 train_time:149448ms step_avg:146.23ms
step:1033/1375 train_time:149601ms step_avg:146.24ms
step:1034/1375 train_time:149753ms step_avg:146.24ms
step:1035/1375 train_time:149909ms step_avg:146.25ms
step:1036/1375 train_time:150065ms step_avg:146.26ms
step:1037/1375 train_time:150220ms step_avg:146.27ms
step:1038/1375 train_time:150375ms step_avg:146.28ms
step:1039/1375 train_time:150527ms step_avg:146.28ms
step:1040/1375 train_time:150678ms step_avg:146.29ms
step:1041/1375 train_time:150833ms step_avg:146.30ms
step:1042/1375 train_time:150984ms step_avg:146.30ms
step:1043/1375 train_time:151135ms step_avg:146.31ms
step:1044/1375 train_time:151290ms step_avg:146.32ms
step:1045/1375 train_time:151448ms step_avg:146.33ms
step:1046/1375 train_time:151602ms step_avg:146.33ms
step:1047/1375 train_time:151753ms step_avg:146.34ms
step:1048/1375 train_time:151909ms step_avg:146.35ms
step:1049/1375 train_time:152064ms step_avg:146.36ms
step:1050/1375 train_time:152221ms step_avg:146.37ms
step:1051/1375 train_time:152377ms step_avg:146.38ms
step:1052/1375 train_time:152531ms step_avg:146.38ms
step:1053/1375 train_time:152683ms step_avg:146.39ms
step:1054/1375 train_time:152837ms step_avg:146.40ms
step:1055/1375 train_time:152992ms step_avg:146.40ms
step:1056/1375 train_time:153147ms step_avg:146.41ms
step:1057/1375 train_time:153303ms step_avg:146.42ms
step:1058/1375 train_time:153459ms step_avg:146.43ms
step:1059/1375 train_time:153615ms step_avg:146.44ms
step:1060/1375 train_time:153771ms step_avg:146.45ms
step:1061/1375 train_time:153924ms step_avg:146.45ms
step:1062/1375 train_time:154076ms step_avg:146.46ms
step:1063/1375 train_time:154230ms step_avg:146.47ms
step:1064/1375 train_time:154383ms step_avg:146.47ms
step:1065/1375 train_time:154536ms step_avg:146.48ms
step:1066/1375 train_time:154692ms step_avg:146.49ms
step:1067/1375 train_time:154849ms step_avg:146.50ms
step:1068/1375 train_time:155001ms step_avg:146.50ms
step:1069/1375 train_time:155161ms step_avg:146.52ms
step:1070/1375 train_time:155312ms step_avg:146.52ms
step:1071/1375 train_time:155468ms step_avg:146.53ms
step:1072/1375 train_time:155620ms step_avg:146.53ms
step:1073/1375 train_time:155773ms step_avg:146.54ms
step:1074/1375 train_time:155926ms step_avg:146.55ms
step:1075/1375 train_time:156079ms step_avg:146.55ms
step:1076/1375 train_time:156232ms step_avg:146.56ms
step:1077/1375 train_time:156385ms step_avg:146.57ms
step:1078/1375 train_time:156543ms step_avg:146.58ms
step:1079/1375 train_time:156701ms step_avg:146.59ms
step:1080/1375 train_time:156856ms step_avg:146.59ms
step:1081/1375 train_time:157009ms step_avg:146.60ms
step:1082/1375 train_time:157162ms step_avg:146.61ms
step:1083/1375 train_time:157316ms step_avg:146.61ms
step:1084/1375 train_time:157473ms step_avg:146.62ms
step:1085/1375 train_time:157625ms step_avg:146.63ms
step:1086/1375 train_time:157779ms step_avg:146.63ms
step:1087/1375 train_time:157933ms step_avg:146.64ms
step:1088/1375 train_time:158087ms step_avg:146.65ms
step:1089/1375 train_time:158246ms step_avg:146.66ms
step:1090/1375 train_time:158405ms step_avg:146.67ms
step:1091/1375 train_time:158559ms step_avg:146.68ms
step:1092/1375 train_time:158711ms step_avg:146.68ms
step:1093/1375 train_time:158865ms step_avg:146.69ms
step:1094/1375 train_time:159017ms step_avg:146.70ms
step:1095/1375 train_time:159170ms step_avg:146.70ms
step:1096/1375 train_time:159330ms step_avg:146.71ms
step:1097/1375 train_time:159484ms step_avg:146.72ms
step:1098/1375 train_time:159639ms step_avg:146.73ms
step:1099/1375 train_time:159791ms step_avg:146.73ms
step:1100/1375 train_time:159944ms step_avg:146.74ms
step:1101/1375 train_time:160096ms step_avg:146.74ms
step:1102/1375 train_time:160251ms step_avg:146.75ms
step:1103/1375 train_time:160405ms step_avg:146.76ms
step:1104/1375 train_time:160556ms step_avg:146.76ms
step:1105/1375 train_time:160710ms step_avg:146.77ms
step:1106/1375 train_time:160864ms step_avg:146.77ms
step:1107/1375 train_time:161017ms step_avg:146.78ms
step:1108/1375 train_time:161175ms step_avg:146.79ms
step:1109/1375 train_time:161327ms step_avg:146.79ms
step:1110/1375 train_time:161481ms step_avg:146.80ms
step:1111/1375 train_time:161634ms step_avg:146.81ms
step:1112/1375 train_time:161789ms step_avg:146.81ms
step:1113/1375 train_time:161944ms step_avg:146.82ms
step:1114/1375 train_time:162098ms step_avg:146.83ms
step:1115/1375 train_time:162254ms step_avg:146.84ms
step:1116/1375 train_time:162407ms step_avg:146.84ms
step:1117/1375 train_time:162565ms step_avg:146.85ms
step:1118/1375 train_time:162723ms step_avg:146.86ms
step:1119/1375 train_time:162876ms step_avg:146.87ms
step:1120/1375 train_time:163030ms step_avg:146.87ms
step:1121/1375 train_time:163184ms step_avg:146.88ms
step:1122/1375 train_time:163336ms step_avg:146.88ms
step:1123/1375 train_time:163489ms step_avg:146.89ms
step:1124/1375 train_time:163650ms step_avg:146.90ms
step:1125/1375 train_time:163807ms step_avg:146.91ms
step:1125/1375 val_loss:3.3487 train_time:163883ms step_avg:146.98ms
step:1126/1375 train_time:163959ms step_avg:146.92ms
step:1127/1375 train_time:164117ms step_avg:146.93ms
step:1128/1375 train_time:164274ms step_avg:146.94ms
step:1129/1375 train_time:164433ms step_avg:146.95ms
step:1130/1375 train_time:164586ms step_avg:146.95ms
step:1131/1375 train_time:164742ms step_avg:146.96ms
step:1132/1375 train_time:164897ms step_avg:146.97ms
step:1133/1375 train_time:165053ms step_avg:146.97ms
step:1134/1375 train_time:165208ms step_avg:146.98ms
step:1135/1375 train_time:165361ms step_avg:146.99ms
step:1136/1375 train_time:165522ms step_avg:147.00ms
step:1137/1375 train_time:165675ms step_avg:147.01ms
step:1138/1375 train_time:165831ms step_avg:147.01ms
step:1139/1375 train_time:165986ms step_avg:147.02ms
step:1140/1375 train_time:166142ms step_avg:147.03ms
step:1141/1375 train_time:166338ms step_avg:147.07ms
step:1142/1375 train_time:166490ms step_avg:147.08ms
step:1143/1375 train_time:166651ms step_avg:147.09ms
step:1144/1375 train_time:166805ms step_avg:147.09ms
step:1145/1375 train_time:166958ms step_avg:147.10ms
step:1146/1375 train_time:167115ms step_avg:147.11ms
step:1147/1375 train_time:167269ms step_avg:147.11ms
step:1148/1375 train_time:167423ms step_avg:147.12ms
step:1149/1375 train_time:167578ms step_avg:147.13ms
step:1150/1375 train_time:167732ms step_avg:147.13ms
step:1151/1375 train_time:167887ms step_avg:147.14ms
step:1152/1375 train_time:168043ms step_avg:147.15ms
step:1153/1375 train_time:168200ms step_avg:147.16ms
step:1154/1375 train_time:168354ms step_avg:147.16ms
step:1155/1375 train_time:168507ms step_avg:147.17ms
step:1156/1375 train_time:168666ms step_avg:147.18ms
step:1157/1375 train_time:168825ms step_avg:147.19ms
step:1158/1375 train_time:168979ms step_avg:147.19ms
step:1159/1375 train_time:169134ms step_avg:147.20ms
step:1160/1375 train_time:169286ms step_avg:147.21ms
step:1161/1375 train_time:169441ms step_avg:147.21ms
step:1162/1375 train_time:169596ms step_avg:147.22ms
step:1163/1375 train_time:169751ms step_avg:147.23ms
step:1164/1375 train_time:169905ms step_avg:147.23ms
step:1165/1375 train_time:170058ms step_avg:147.24ms
step:1166/1375 train_time:170213ms step_avg:147.24ms
step:1167/1375 train_time:170365ms step_avg:147.25ms
step:1168/1375 train_time:170522ms step_avg:147.26ms
step:1169/1375 train_time:170677ms step_avg:147.26ms
step:1170/1375 train_time:170831ms step_avg:147.27ms
step:1171/1375 train_time:170987ms step_avg:147.28ms
step:1172/1375 train_time:171142ms step_avg:147.28ms
step:1173/1375 train_time:171297ms step_avg:147.29ms
step:1174/1375 train_time:171459ms step_avg:147.30ms
step:1175/1375 train_time:171617ms step_avg:147.31ms
step:1176/1375 train_time:171778ms step_avg:147.32ms
step:1177/1375 train_time:171939ms step_avg:147.33ms
step:1178/1375 train_time:172094ms step_avg:147.34ms
step:1179/1375 train_time:172247ms step_avg:147.35ms
step:1180/1375 train_time:172408ms step_avg:147.36ms
step:1181/1375 train_time:172563ms step_avg:147.36ms
step:1182/1375 train_time:172717ms step_avg:147.37ms
step:1183/1375 train_time:172872ms step_avg:147.38ms
step:1184/1375 train_time:173028ms step_avg:147.38ms
step:1185/1375 train_time:173186ms step_avg:147.39ms
step:1186/1375 train_time:173340ms step_avg:147.40ms
step:1187/1375 train_time:173503ms step_avg:147.41ms
step:1188/1375 train_time:173656ms step_avg:147.42ms
step:1189/1375 train_time:173812ms step_avg:147.42ms
step:1190/1375 train_time:173967ms step_avg:147.43ms
step:1191/1375 train_time:174127ms step_avg:147.44ms
step:1192/1375 train_time:174279ms step_avg:147.44ms
step:1193/1375 train_time:174437ms step_avg:147.45ms
step:1194/1375 train_time:174593ms step_avg:147.46ms
step:1195/1375 train_time:174745ms step_avg:147.46ms
step:1196/1375 train_time:174899ms step_avg:147.47ms
step:1197/1375 train_time:175056ms step_avg:147.48ms
step:1198/1375 train_time:175216ms step_avg:147.49ms
step:1199/1375 train_time:175371ms step_avg:147.49ms
step:1200/1375 train_time:175524ms step_avg:147.50ms
step:1201/1375 train_time:175679ms step_avg:147.51ms
step:1202/1375 train_time:175843ms step_avg:147.52ms
step:1203/1375 train_time:176001ms step_avg:147.53ms
step:1204/1375 train_time:176157ms step_avg:147.54ms
step:1205/1375 train_time:176312ms step_avg:147.54ms
step:1206/1375 train_time:176467ms step_avg:147.55ms
step:1207/1375 train_time:176621ms step_avg:147.55ms
step:1208/1375 train_time:176777ms step_avg:147.56ms
step:1209/1375 train_time:176932ms step_avg:147.57ms
step:1210/1375 train_time:177092ms step_avg:147.58ms
step:1211/1375 train_time:177248ms step_avg:147.58ms
step:1212/1375 train_time:177401ms step_avg:147.59ms
step:1213/1375 train_time:177557ms step_avg:147.60ms
step:1214/1375 train_time:177712ms step_avg:147.60ms
step:1215/1375 train_time:177868ms step_avg:147.61ms
step:1216/1375 train_time:178021ms step_avg:147.61ms
step:1217/1375 train_time:178176ms step_avg:147.62ms
step:1218/1375 train_time:178330ms step_avg:147.62ms
step:1219/1375 train_time:178481ms step_avg:147.63ms
step:1220/1375 train_time:178634ms step_avg:147.63ms
step:1221/1375 train_time:178787ms step_avg:147.64ms
step:1222/1375 train_time:178942ms step_avg:147.64ms
step:1223/1375 train_time:179098ms step_avg:147.65ms
step:1224/1375 train_time:179255ms step_avg:147.66ms
step:1225/1375 train_time:179411ms step_avg:147.66ms
step:1226/1375 train_time:179565ms step_avg:147.67ms
step:1227/1375 train_time:179724ms step_avg:147.68ms
step:1228/1375 train_time:179880ms step_avg:147.69ms
step:1229/1375 train_time:180035ms step_avg:147.69ms
step:1230/1375 train_time:180195ms step_avg:147.70ms
step:1231/1375 train_time:180353ms step_avg:147.71ms
step:1232/1375 train_time:180510ms step_avg:147.72ms
step:1233/1375 train_time:180664ms step_avg:147.72ms
step:1234/1375 train_time:180817ms step_avg:147.73ms
step:1235/1375 train_time:180974ms step_avg:147.73ms
step:1236/1375 train_time:181128ms step_avg:147.74ms
step:1237/1375 train_time:181282ms step_avg:147.74ms
step:1238/1375 train_time:181445ms step_avg:147.76ms
step:1239/1375 train_time:181601ms step_avg:147.76ms
step:1240/1375 train_time:181759ms step_avg:147.77ms
step:1241/1375 train_time:181922ms step_avg:147.78ms
step:1242/1375 train_time:182078ms step_avg:147.79ms
step:1243/1375 train_time:182237ms step_avg:147.80ms
step:1244/1375 train_time:182390ms step_avg:147.80ms
step:1245/1375 train_time:182545ms step_avg:147.81ms
step:1246/1375 train_time:182699ms step_avg:147.82ms
step:1247/1375 train_time:182858ms step_avg:147.82ms
step:1248/1375 train_time:183012ms step_avg:147.83ms
step:1249/1375 train_time:183165ms step_avg:147.83ms
step:1250/1375 train_time:183318ms step_avg:147.84ms
step:1250/1375 val_loss:3.3033 train_time:183400ms step_avg:147.90ms
step:1251/1375 train_time:183479ms step_avg:147.85ms
step:1252/1375 train_time:183634ms step_avg:147.85ms
step:1253/1375 train_time:183789ms step_avg:147.86ms
step:1254/1375 train_time:183941ms step_avg:147.86ms
step:1255/1375 train_time:184109ms step_avg:147.88ms
step:1256/1375 train_time:184263ms step_avg:147.88ms
step:1257/1375 train_time:184421ms step_avg:147.89ms
step:1258/1375 train_time:184579ms step_avg:147.90ms
step:1259/1375 train_time:184736ms step_avg:147.91ms
step:1260/1375 train_time:184889ms step_avg:147.91ms
step:1261/1375 train_time:185045ms step_avg:147.92ms
step:1262/1375 train_time:185204ms step_avg:147.93ms
step:1263/1375 train_time:185361ms step_avg:147.93ms
step:1264/1375 train_time:185516ms step_avg:147.94ms
step:1265/1375 train_time:185670ms step_avg:147.94ms
step:1266/1375 train_time:185824ms step_avg:147.95ms
step:1267/1375 train_time:185980ms step_avg:147.96ms
step:1268/1375 train_time:186137ms step_avg:147.96ms
step:1269/1375 train_time:186297ms step_avg:147.97ms
step:1270/1375 train_time:186453ms step_avg:147.98ms
step:1271/1375 train_time:186610ms step_avg:147.99ms
step:1272/1375 train_time:186764ms step_avg:147.99ms
step:1273/1375 train_time:186918ms step_avg:148.00ms
step:1274/1375 train_time:187074ms step_avg:148.00ms
step:1275/1375 train_time:187229ms step_avg:148.01ms
step:1276/1375 train_time:187381ms step_avg:148.01ms
step:1277/1375 train_time:187538ms step_avg:148.02ms
step:1278/1375 train_time:187692ms step_avg:148.02ms
step:1279/1375 train_time:187849ms step_avg:148.03ms
step:1280/1375 train_time:188013ms step_avg:148.04ms
step:1281/1375 train_time:188171ms step_avg:148.05ms
step:1282/1375 train_time:188323ms step_avg:148.05ms
step:1283/1375 train_time:188481ms step_avg:148.06ms
step:1284/1375 train_time:188639ms step_avg:148.07ms
step:1285/1375 train_time:188794ms step_avg:148.07ms
step:1286/1375 train_time:188949ms step_avg:148.08ms
step:1287/1375 train_time:189104ms step_avg:148.08ms
step:1288/1375 train_time:189262ms step_avg:148.09ms
step:1289/1375 train_time:189424ms step_avg:148.10ms
step:1290/1375 train_time:189583ms step_avg:148.11ms
step:1291/1375 train_time:189744ms step_avg:148.12ms
step:1292/1375 train_time:189900ms step_avg:148.13ms
step:1293/1375 train_time:190058ms step_avg:148.14ms
step:1294/1375 train_time:190215ms step_avg:148.14ms
step:1295/1375 train_time:190369ms step_avg:148.15ms
step:1296/1375 train_time:190528ms step_avg:148.16ms
step:1297/1375 train_time:190685ms step_avg:148.16ms
step:1298/1375 train_time:190840ms step_avg:148.17ms
step:1299/1375 train_time:190995ms step_avg:148.17ms
step:1300/1375 train_time:191150ms step_avg:148.18ms
step:1301/1375 train_time:191303ms step_avg:148.18ms
step:1302/1375 train_time:191461ms step_avg:148.19ms
step:1303/1375 train_time:191619ms step_avg:148.20ms
step:1304/1375 train_time:191779ms step_avg:148.21ms
step:1305/1375 train_time:191934ms step_avg:148.21ms
step:1306/1375 train_time:192091ms step_avg:148.22ms
step:1307/1375 train_time:192243ms step_avg:148.22ms
step:1308/1375 train_time:192400ms step_avg:148.23ms
step:1309/1375 train_time:192555ms step_avg:148.23ms
step:1310/1375 train_time:192709ms step_avg:148.24ms
step:1311/1375 train_time:192864ms step_avg:148.24ms
step:1312/1375 train_time:193018ms step_avg:148.25ms
step:1313/1375 train_time:193172ms step_avg:148.25ms
step:1314/1375 train_time:193328ms step_avg:148.26ms
step:1315/1375 train_time:193486ms step_avg:148.27ms
step:1316/1375 train_time:193639ms step_avg:148.27ms
step:1317/1375 train_time:193793ms step_avg:148.27ms
step:1318/1375 train_time:193954ms step_avg:148.28ms
step:1319/1375 train_time:194109ms step_avg:148.29ms
step:1320/1375 train_time:194264ms step_avg:148.29ms
step:1321/1375 train_time:194421ms step_avg:148.30ms
step:1322/1375 train_time:194579ms step_avg:148.31ms
step:1323/1375 train_time:194734ms step_avg:148.31ms
step:1324/1375 train_time:194889ms step_avg:148.32ms
step:1325/1375 train_time:195045ms step_avg:148.32ms
step:1326/1375 train_time:195205ms step_avg:148.33ms
step:1327/1375 train_time:195360ms step_avg:148.34ms
step:1328/1375 train_time:195518ms step_avg:148.34ms
step:1329/1375 train_time:195691ms step_avg:148.36ms
step:1330/1375 train_time:195850ms step_avg:148.37ms
step:1331/1375 train_time:196047ms step_avg:148.41ms
step:1332/1375 train_time:196211ms step_avg:148.42ms
step:1333/1375 train_time:196367ms step_avg:148.43ms
step:1334/1375 train_time:196522ms step_avg:148.43ms
step:1335/1375 train_time:196675ms step_avg:148.43ms
step:1336/1375 train_time:196838ms step_avg:148.45ms
step:1337/1375 train_time:196996ms step_avg:148.45ms
step:1338/1375 train_time:197156ms step_avg:148.46ms
step:1339/1375 train_time:197316ms step_avg:148.47ms
step:1340/1375 train_time:197474ms step_avg:148.48ms
step:1341/1375 train_time:197628ms step_avg:148.48ms
step:1342/1375 train_time:197786ms step_avg:148.49ms
step:1343/1375 train_time:197941ms step_avg:148.49ms
step:1344/1375 train_time:198096ms step_avg:148.50ms
step:1345/1375 train_time:198254ms step_avg:148.50ms
step:1346/1375 train_time:198410ms step_avg:148.51ms
step:1347/1375 train_time:198568ms step_avg:148.52ms
step:1348/1375 train_time:198724ms step_avg:148.52ms
step:1349/1375 train_time:198880ms step_avg:148.53ms
step:1350/1375 train_time:199034ms step_avg:148.53ms
step:1351/1375 train_time:199188ms step_avg:148.54ms
step:1352/1375 train_time:199352ms step_avg:148.55ms
step:1353/1375 train_time:199514ms step_avg:148.56ms
step:1354/1375 train_time:199669ms step_avg:148.56ms
step:1355/1375 train_time:199826ms step_avg:148.57ms
step:1356/1375 train_time:199981ms step_avg:148.57ms
step:1357/1375 train_time:200138ms step_avg:148.58ms
step:1358/1375 train_time:200296ms step_avg:148.59ms
step:1359/1375 train_time:200455ms step_avg:148.60ms
step:1360/1375 train_time:200614ms step_avg:148.60ms
step:1361/1375 train_time:200775ms step_avg:148.61ms
step:1362/1375 train_time:200933ms step_avg:148.62ms
step:1363/1375 train_time:201097ms step_avg:148.63ms
step:1364/1375 train_time:201252ms step_avg:148.64ms
step:1365/1375 train_time:201404ms step_avg:148.64ms
step:1366/1375 train_time:201560ms step_avg:148.64ms
step:1367/1375 train_time:201719ms step_avg:148.65ms
step:1368/1375 train_time:201876ms step_avg:148.66ms
step:1369/1375 train_time:202038ms step_avg:148.67ms
step:1370/1375 train_time:202198ms step_avg:148.68ms
step:1371/1375 train_time:202353ms step_avg:148.68ms
step:1372/1375 train_time:202515ms step_avg:148.69ms
step:1373/1375 train_time:202670ms step_avg:148.69ms
step:1374/1375 train_time:202827ms step_avg:148.70ms
step:1375/1375 train_time:202983ms step_avg:148.71ms
step:1375/1375 val_loss:3.2778 train_time:203062ms step_avg:148.76ms
peak memory consumption: 31563 MiB

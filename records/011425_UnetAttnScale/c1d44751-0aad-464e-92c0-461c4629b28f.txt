import os
import sys
with open(sys.argv[0]) as f:
    code = f.read() # read the code of this file ASAP, for logging
import uuid
import time
import glob
import subprocess
import contextlib
from dataclasses import dataclass

import torch
torch.empty(1, device='cuda', requires_grad=True).backward()
from torch import nn
import torch.nn.functional as F
import torch.distributed as dist
from torch.nn.parallel import DistributedDataParallel as DDP
# use of FlexAttention contributed by @KoszarskyB
from torch.nn.attention.flex_attention import BlockMask, flex_attention

# -----------------------------------------------------------------------------
# Muon optimizer

@torch.compile
def zeropower_via_newtonschulz5(G, steps):
    """
    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a
    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose
    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at
    zero even beyond the point where the iteration no longer converges all the way to one everywhere
    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T
    where S' is diagonal with S_{ii}' ~ Uniform(0.5, 1.5), which turns out not to hurt model
    performance at all relative to UV^T, where USV^T = G is the SVD.
    """
    assert len(G.shape) == 2
    a, b, c = (3.4445, -4.7750,  2.0315)
    X = G.bfloat16()
    if G.size(0) > G.size(1):
        X = X.T

    # Ensure spectral norm is at most 1
    X = X / (X.norm() + 1e-7)
    # Perform the NS iterations
    for _ in range(steps):
        A = X @ X.T
        B = b * A + c * A @ A # adapted from suggestion by @jxbz, @leloykun, and @YouJiacheng
        X = a * X + B @ X
    
    if G.size(0) > G.size(1):
        X = X.T
    return X

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Some warnings:
    - This optimizer assumes that all parameters passed in are 2D.
    - It should not be used for the embedding layer, the final fully connected layer, or any {0,1}-D
    parameters; those should all be optimized by a standard method (e.g., AdamW).
    - To use it with 4D convolutional filters, it works well to just flatten their last 3 dimensions.
    - We believe it is unlikely to work well for training with small batch size.
    - We believe it may not work well for finetuning pretrained models, but we haven't tested this.
    - We have not yet tried this optimizer for training scenarios larger than NanoGPT (124M).

    Arguments:
        lr: The learning rate used by the internal SGD.
        momentum: The momentum used by the internal SGD.
        nesterov: Whether to use Nesterov-style momentum in the internal SGD. (recommended)
        ns_steps: The number of Newton-Schulz iteration steps to use.
    """
    def __init__(self, params, lr=0.02, momentum=0.95, nesterov=True, ns_steps=5):
        self.world_size = int(os.environ['WORLD_SIZE'])
        self.rank = int(os.environ['RANK'])
        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, ns_steps=ns_steps)
        assert all(isinstance(p, torch.Tensor) for p in params)
        sizes = {p.numel() for p in params}
        param_groups = [dict(params=[p for p in params if p.numel() == size],
                             update_buffer=[torch.empty(size, device='cuda', dtype=torch.bfloat16) for _ in range(self.world_size)])
                        for size in sizes]
        super().__init__(param_groups, defaults)

    def step(self):

        for group in self.param_groups:

            lr = group['lr']
            momentum = group['momentum']
            nesterov = group['nesterov']
            ns_steps = group['ns_steps']
            update_buffers = group['update_buffer']
            # generate weight updates in distributed fashion
            params = group['params']
            handle = None
            params_world = None
            def update_prev():
                if params_world is None:
                    return
                assert handle is not None
                handle.wait()
                for p_world, g_world in zip(params_world, update_buffers):
                    p_world.data.add_(
                        g_world.view_as(p_world),
                        alpha=-lr * max(1, p_world.size(0) / p_world.size(1)) ** 0.5,
                    )
            for base_i in range(len(params))[::self.world_size]:
                if base_i + rank < len(params):
                    p = params[base_i + self.rank]
                    g = p.grad
                    assert g is not None
                    state = self.state[p]
                    if 'momentum_buffer' not in state:
                        state['momentum_buffer'] = torch.zeros_like(g)
                    buf = state['momentum_buffer']
                    buf.lerp_(g, 1 - momentum)
                    g = g.lerp_(buf, momentum) if nesterov else buf
                    g = zeropower_via_newtonschulz5(g, steps=ns_steps).flatten()
                else:
                    g = update_buffers[rank]
                update_prev() # async all_gather instead of sync all_reduce by @YouJiacheng
                handle = dist.all_gather(update_buffers, g, async_op=True)
                params_world = params[base_i : base_i + self.world_size]
            update_prev()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the GPT-2 model

def norm(x):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):

    def __init__(self, in_features, out_features):
        super().__init__(in_features, out_features, bias=False)

    def forward(self, x):
        return F.linear(x, self.weight.type_as(x))

class Rotary(nn.Module):

    def __init__(self, dim, max_seq_len=65536):
        super().__init__()
        # half-truncate RoPE by @YouJiacheng
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=dim//4, dtype=torch.float32)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(dim//4)])
        t = torch.arange(max_seq_len, dtype=torch.float32)
        theta = torch.einsum('i,j -> ij', t, angular_freq)
        self.cos = nn.Buffer(theta.cos(), persistent=False)
        self.sin = nn.Buffer(theta.sin(), persistent=False)

    def forward(self, x):
        cos, sin = self.cos[None, :x.size(-3), None, :], self.sin[None, :x.size(-3), None, :]
        x1, x2 = x.float().chunk(2, dim=-1)
        y1 = x1 * cos + x2 * sin
        y2 = x1 * (-sin) + x2 * cos
        return torch.cat((y1, y2), 3).type_as(x)

class CausalSelfAttention(nn.Module):

    def __init__(self, dim, num_heads, layer_id):
        super().__init__()
        assert dim % num_heads == 0
        self.num_heads = num_heads
        self.layer_id = layer_id
        self.c_q = CastedLinear(dim, dim)
        self.c_k = CastedLinear(dim, dim)
        self.c_v = CastedLinear(dim, dim)
        self.lambdas = nn.Parameter(torch.tensor([0.5, 0.5]))
        self.rotary = Rotary(dim // num_heads) # dim // num_heads = head_dim
        self.c_proj = CastedLinear(dim, dim)
        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977
        # self.attn_scale = nn.Parameter(torch.tensor(1.0 / (dim // num_heads) ** 0.5))
        # self.attn_scale = nn.Parameter(torch.tensor(0.20))
        self.attn_scale = 0.13 + 0.01 * min(layer_id, 11 - layer_id)  # unet pattern attention scale by @leloykun

    def forward(self, x, ve, block_mask):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, 'Must use batch size = 1 for FlexAttention'
        q = self.c_q(x).view(B, T, self.num_heads, -1)
        k = self.c_k(x).view(B, T, self.num_heads, -1)
        v = self.c_v(x).view(B, T, self.num_heads, -1)
        if ve is not None:
            v = self.lambdas[0] * v + self.lambdas[1] * ve.view_as(v) # @KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = self.lambdas[0] * v
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = self.rotary(q), self.rotary(k)
        # y = flex_attention(q.transpose(1, 2) * self.attn_scale, k.transpose(1, 2), v.transpose(1, 2), block_mask=block_mask, scale=1.)
        y = flex_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), block_mask=block_mask, scale=self.attn_scale)
        y = y.transpose(1, 2).contiguous().view_as(x) # re-assemble all head outputs side by side
        y = self.c_proj(y)
        return y

class MLP(nn.Module):

    def __init__(self, dim):
        super().__init__()
        self.c_fc = CastedLinear(dim, 4 * dim)
        self.c_proj = CastedLinear(4 * dim, dim)
        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977

    def forward(self, x):
        x = self.c_fc(x)
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = self.c_proj(x)
        return x

class Block(nn.Module):

    def __init__(self, model_dim, num_heads, layer_id, use_attn=True):
        super().__init__()
        self.attn = CausalSelfAttention(model_dim, num_heads, layer_id) if use_attn else None
        self.mlp = MLP(model_dim)
        self.lambdas = nn.Parameter(torch.tensor([1., 0.]))

    def forward(self, x, ve, x0, block_mask):
        x = self.lambdas[0] * x + self.lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), ve, block_mask)
        x = x + self.mlp(norm(x))
        return x

class ValueEmbedding(nn.Module):
    def __init__(self, vocab_size, model_dim):
        super().__init__()
        self.embed = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])

    def forward(self, inputs):
        ve = [emb(inputs).bfloat16() for emb in self.embed]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        ve = [ve[0], ve[1], ve[2], None, None, None, None, None, None, ve[0], ve[1], ve[2]]
        return ve

# -----------------------------------------------------------------------------
# The main GPT-2 model

class GPT(nn.Module):

    def __init__(self, vocab_size, num_layers, num_heads, model_dim):
        super().__init__()
        self.embed = nn.Embedding(vocab_size, model_dim)
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.blocks = nn.ModuleList([Block(model_dim, num_heads, layer_id=layer_id, use_attn=(layer_id != 7))
                                     for layer_id in range(num_layers)])
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual learning
        # U-net structure on token value embeddings by @leloykun
        self.value_embeds = ValueEmbedding(vocab_size, model_dim)
        self.lm_head = CastedLinear(model_dim, vocab_size)
        self.lm_head.weight.data.zero_() # @Grad62304977
        # U-net design by @brendanh0gan
        self.num_encoder_layers = num_layers // 2 # Half of the layers for encoder
        self.num_decoder_layers = num_layers - self.num_encoder_layers # Remaining for decoder
        # Add learnable skip connection weights for decoder layers
        self.skip_weights = nn.Parameter(torch.ones(self.num_decoder_layers))

    def forward(self, inputs, targets, sliding_window_num_blocks):
        BLOCK_SIZE = 128
        seq_len = len(inputs)
        assert seq_len % BLOCK_SIZE == 0
        total_num_blocks = seq_len // BLOCK_SIZE
        assert inputs.ndim == 1
        docs = (inputs == 50256).cumsum(0)
        docs_low = docs.view(-1, BLOCK_SIZE)[:, 0].contiguous()
        docs_high = docs.view(-1, BLOCK_SIZE)[:, -1].contiguous()

        def document_causal(b, h, q_idx, kv_idx):
            causal_mask = q_idx >= kv_idx
            document_mask = docs[q_idx] == docs[kv_idx]
            return causal_mask & document_mask

        def dense_to_ordered(dense_mask):
            num_blocks = dense_mask.sum(dim=-1, dtype=torch.int32)
            indices = dense_mask.argsort(dim=-1, descending=True, stable=True).to(torch.int32)
            return num_blocks[None, None].contiguous(), indices[None, None].contiguous()

        def create_doc_swc_block_mask(sliding_window_num_blocks):
            kv_idx = block_idx = torch.arange(total_num_blocks, dtype=torch.int32, device='cuda')
            q_idx = block_idx[:, None]
            causal_bm = q_idx >= kv_idx
            causal_full_bm = q_idx > kv_idx
            window_bm = q_idx - kv_idx < sliding_window_num_blocks
            window_full_bm = window_bm # block-wise sliding window by @YouJiacheng
            # document_bm = (docs_low[q_idx] <= docs_high[kv_idx]) & (docs_low[kv_idx] <= docs_high[q_idx])
            document_bm = (docs_low[:, None] <= docs_high) & (docs_low <= docs_high[:, None])
            document_full_bm = (docs_low[:, None] == docs_high) & (docs_low == docs_high[:, None])
            nonzero_bm = causal_bm & window_bm & document_bm
            full_bm  = causal_full_bm & window_full_bm & document_full_bm
            kv_num_blocks, kv_indices = dense_to_ordered(nonzero_bm & ~full_bm)
            full_kv_num_blocks, full_kv_indices = dense_to_ordered(full_bm)
            return BlockMask.from_kv_blocks(
                kv_num_blocks,
                kv_indices,
                full_kv_num_blocks,
                full_kv_indices,
                BLOCK_SIZE=BLOCK_SIZE,
                mask_mod=document_causal,
            )

        block_mask = create_doc_swc_block_mask(sliding_window_num_blocks)

        x0 = norm(self.embed(inputs[None]).bfloat16()) # use of norm here by @Grad62304977
        x = x0
        ve = self.value_embeds(inputs)
        assert len(ve) == len(self.blocks)
        ve_enc, ve_dec = ve[:self.num_encoder_layers], ve[self.num_encoder_layers:]

        # Store outputs for U-Net skip connections
        skip_connections = []
        # Encoder pass - process only the first half of the blocks
        for i in range(self.num_encoder_layers):
            x = self.blocks[i](x, ve_enc[i], x0, block_mask)
            skip_connections.append(x)
        # Decoder pass - process the remaining blocks with weighted skip connections
        for i in range(self.num_decoder_layers):
            x = x + self.skip_weights[i] * skip_connections.pop()
            # U-net structure on token value embeddings by @leloykun
            x = self.blocks[self.num_encoder_layers + i](x, ve_dec[i], x0, block_mask)

        x = norm(x)
        logits = self.lm_head(x)
        logits = 15 * torch.tanh(logits / 15) # @Grad62304977 added tanh softcapping, @KoszarskyB reduced it from 30 to 15
        logits = logits.float()
        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets)
        return loss

# -----------------------------------------------------------------------------
# Our own simple Distributed Data Loader

def _load_data_shard(path):
    # only reads the header, returns header data
    # header is 256 int32
    header = torch.from_file(path, False, 256, dtype=torch.int32)
    assert header[0] == 20240520, 'magic number mismatch in the data .bin file'
    assert header[1] == 1, 'unsupported version'
    num_tokens = int(header[2]) # number of tokens (claimed)
    with open(path, 'rb', buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, 'number of tokens read does not match header'
    return tokens

class DistributedDataLoader:

    def __init__(self, filename_pattern):
        self.rank = int(os.environ['RANK'])
        self.world_size = int(os.environ['WORLD_SIZE'])
        self.files = sorted(glob.glob(filename_pattern))
        self.reset()

    def reset(self):
        self.current_shard = -1
        self.advance()

    def advance(self):
        self.current_shard = (self.current_shard + 1) % len(self.files)
        self.current_position = 0
        self.tokens = _load_data_shard(self.files[self.current_shard])

    def next_batch(self, batch_size):
        assert batch_size % self.world_size == 0
        device_batch_size = batch_size // self.world_size
        # load next shard if necessary
        if self.current_position + batch_size + 1 >= len(self.tokens):
            self.advance()
        pos = self.current_position + self.rank * device_batch_size
        device_batch_tokens = self.tokens[pos:pos+device_batch_size+1]
        # advance current position
        self.current_position += batch_size
        inputs = device_batch_tokens[:-1].to(device='cuda', dtype=torch.int32, non_blocking=True)
        targets = device_batch_tokens[1:].to(device='cuda', dtype=torch.int64, non_blocking=True)
        return inputs, targets

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files = 'data/fineweb10B/fineweb_train_*.bin' # input .bin to train on
    val_files = 'data/fineweb10B/fineweb_val_*.bin' # input .bin to eval validation loss on
    val_tokens = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # optimization
    batch_size = 8*64*1024 # batch size in tokens
    num_iterations = 1375 # number of iterations to run
    cooldown_frac = 0.4 # fraction of training spent cooling down the learning rate
    bf16_embeds = True
    # evaluation and logging
    val_loss_every = 125 # every how many steps to evaluate val loss? 0 for only at the end
    # implementation
    max_device_batch_size = 64*1024 # batch size per device in tokens
    save_checkpoint = False
args = Hyperparameters()

micro_bs = args.max_device_batch_size

# set up DDP (distributed data parallel). torchrun sets this env variable
rank = int(os.environ['RANK'])
local_rank = int(os.environ['LOCAL_RANK'])
world_size = int(os.environ['WORLD_SIZE'])
assert torch.cuda.is_available()
torch.cuda.set_device(local_rank)
dist.init_process_group(backend='nccl', device_id=torch.device(local_rank))
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = uuid.uuid4()
    os.makedirs('logs', exist_ok=True)
    logfile = f'logs/{run_id}.txt'
    print(logfile)

def print0(s, console=False):
    if master_process:
        with open(logfile, 'a') as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0('='*100)
# log information about the hardware/software environment this is running on
print0(f'Running Python {sys.version}')
print0(f'Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}')
print0(subprocess.run(['nvidia-smi'], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout)
print0('='*100)

# load data
train_loader = DistributedDataLoader(args.train_files)
val_loader = DistributedDataLoader(args.val_files)
print0(f'Training dataloader files: {train_loader.files}')
print0(f'Validation dataloader files: {val_loader.files}')
print0('='*100)

# there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency. suggested to me by @Grad62304977.
# this originates from Karpathy's experiments.
model = GPT(vocab_size=50304, num_layers=12, num_heads=6, model_dim=768)
model = model.cuda()
if args.bf16_embeds:
    for m in model.modules():
        if isinstance(m, nn.Embedding):
            m.bfloat16()
model: GPT = torch.compile(model)
ddp_model = DDP(model, device_ids=[local_rank], broadcast_buffers=False, gradient_as_bucket_view=True)

# collect the parameters to optimize
hidden_matrix_params = [p for p in model.blocks.parameters() if p.ndim == 2]
embed_params = [model.embed.weight, *model.value_embeds.parameters()]
scalar_params = [p for n, p in model.named_parameters() if p.ndim < 2 and "attn_scale" not in n]
attn_scale_params = [p for n, p in model.named_parameters() if p.ndim < 2 and "attn_scale" in n]
head_params = [model.lm_head.weight]

# init the optimizer(s)
optimizer1 = torch.optim.Adam([dict(params=embed_params, lr=0.6),
                               dict(params=head_params, lr=0.008),
                               dict(params=scalar_params, lr=0.04),
                               dict(params=attn_scale_params, lr=0.01)],
                              betas=(0.8, 0.95), fused=True)
optimizer2 = Muon(hidden_matrix_params, lr=0.05, momentum=0.95)
optimizers = [optimizer1, optimizer2]

# learning rate schedule: stable then decay
def get_lr(it):
    t = 1 - it / args.num_iterations # time remaining in training
    assert 1 >= t > 0
    # 1) constant lr for first part of training
    if t >= args.cooldown_frac:
        return 1.0
    # 2) then linear cooldown
    else:
        return t / args.cooldown_frac
schedulers = [torch.optim.lr_scheduler.LambdaLR(opt, get_lr) for opt in optimizers]

# sliding window size schedule: linear increase over training in chunks of 128 from 128 -> 1792. By @fernbear.bsky.social
def get_sliding_window_blocks(it):
    x = it / args.num_iterations # training progress
    assert 0 <= x <= 1
    return int(((1 - x) * 128 + x * 1856) // 128)
sliding_window_num_blocks = torch.tensor(1, dtype=torch.int32, device='cuda')

# Start training loop
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    # This effectively ignores timing first 10 steps, which are slower for weird reasons.
    # Alternately, and slightly more correctly in terms of benchmarking, we could do 10
    # steps with dummy data first, and then re-initialize the model and reset the loader.
    if step == 10:
        training_time_ms = 0
        t0 = time.perf_counter()
    timed_steps = float('nan') if step <= 11 else (step - 10) + 1 # <= 11 to avoid bug in val

    sliding_window_num_blocks.copy_(get_sliding_window_blocks(step))

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        # run validation batches
        model.eval()
        val_loader.reset()
        val_loss = 0.0
        # calculate the number of steps to take in the val loop.
        val_batch_size = world_size * micro_bs
        assert args.val_tokens % val_batch_size == 0
        val_steps = args.val_tokens // val_batch_size
        for _ in range(val_steps):
            with torch.no_grad():
                inputs_val, targets_val = val_loader.next_batch(val_batch_size)
                val_loss += ddp_model(inputs_val, targets_val, sliding_window_num_blocks)
        # Print attention scales
        for n, p in model.named_parameters():
            if p.ndim < 2 and "attn_scale" in n:
                print0(f'{n}: {p.item()}', console=True)
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        val_loss /= val_steps
        # logging
        print0(f'step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/(timed_steps-1):.2f}ms', console=True)
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f'logs/{run_id}', exist_ok=True)
            torch.save(log, f'logs/{run_id}/state_step{step:06d}.pt')
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    model.train()
    batch_size = args.batch_size
    assert batch_size % world_size == 0
    inputs_train, targets_train = train_loader.next_batch(batch_size)
    assert len(inputs_train) <= micro_bs or len(inputs_train) % micro_bs == 0
    for micro_inputs_train, micro_targets_train in zip(inputs_train.split(micro_bs), targets_train.split(micro_bs)):
        ddp_model(micro_inputs_train, micro_targets_train, sliding_window_num_blocks).backward()
    # momentum warmup for Muon
    frac = min(step/300, 1)
    for group in optimizer2.param_groups:
        group['momentum'] = (1 - frac) * 0.85 + frac * 0.95
    # step the optimizers and schedulers
    for opt, sched in zip(optimizers, schedulers):
        opt.step()
        if step != train_steps-1:
            sched.step()
    # null the gradients
    model.zero_grad(set_to_none=True)
    # logging
    approx_time = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f'step:{step+1}/{train_steps} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms', console=True)

print0(f'peak memory consumption: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB')
dist.destroy_process_group()

====================================================================================================
Running Python 3.10.12 (main, Sep 11 2024, 15:47:36) [GCC 11.4.0]
Running PyTorch 2.6.0.dev20241231+cu126 compiled for CUDA 12.6
Tue Jan 14 16:54:12 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.183.06             Driver Version: 535.183.06   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:19:00.0 Off |                    0 |
| N/A   39C    P0             127W / 700W |   7713MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:3B:00.0 Off |                    0 |
| N/A   33C    P0             114W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:4C:00.0 Off |                    0 |
| N/A   30C    P0             118W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   38C    P0             129W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:9B:00.0 Off |                    0 |
| N/A   39C    P0             124W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:BB:00.0 Off |                    0 |
| N/A   30C    P0             115W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:CB:00.0 Off |                    0 |
| N/A   37C    P0             116W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:DB:00.0 Off |                    0 |
| N/A   30C    P0             117W / 700W |   3211MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
Training dataloader files: ['data/fineweb10B/fineweb_train_000001.bin', 'data/fineweb10B/fineweb_train_000002.bin', 'data/fineweb10B/fineweb_train_000003.bin', 'data/fineweb10B/fineweb_train_000004.bin', 'data/fineweb10B/fineweb_train_000005.bin', 'data/fineweb10B/fineweb_train_000006.bin', 'data/fineweb10B/fineweb_train_000007.bin', 'data/fineweb10B/fineweb_train_000008.bin', 'data/fineweb10B/fineweb_train_000009.bin']
Validation dataloader files: ['data/fineweb10B/fineweb_val_000000.bin']
====================================================================================================
step:0/1375 val_loss:10.8258 train_time:0ms step_avg:nanms
step:1/1375 train_time:27737ms step_avg:nanms
step:2/1375 train_time:27822ms step_avg:nanms
step:3/1375 train_time:28003ms step_avg:nanms
step:4/1375 train_time:28135ms step_avg:nanms
step:5/1375 train_time:28270ms step_avg:nanms
step:6/1375 train_time:28405ms step_avg:nanms
step:7/1375 train_time:28538ms step_avg:nanms
step:8/1375 train_time:28673ms step_avg:nanms
step:9/1375 train_time:28806ms step_avg:nanms
step:10/1375 train_time:28944ms step_avg:nanms
step:11/1375 train_time:136ms step_avg:nanms
step:12/1375 train_time:271ms step_avg:nanms
step:13/1375 train_time:405ms step_avg:135.16ms
step:14/1375 train_time:541ms step_avg:135.32ms
step:15/1375 train_time:675ms step_avg:134.97ms
step:16/1375 train_time:810ms step_avg:135.01ms
step:17/1375 train_time:945ms step_avg:135.01ms
step:18/1375 train_time:1082ms step_avg:135.20ms
step:19/1375 train_time:1218ms step_avg:135.37ms
step:20/1375 train_time:1355ms step_avg:135.50ms
step:21/1375 train_time:1489ms step_avg:135.39ms
step:22/1375 train_time:1626ms step_avg:135.49ms
step:23/1375 train_time:1760ms step_avg:135.40ms
step:24/1375 train_time:1896ms step_avg:135.44ms
step:25/1375 train_time:2033ms step_avg:135.56ms
step:26/1375 train_time:2171ms step_avg:135.66ms
step:27/1375 train_time:2306ms step_avg:135.63ms
step:28/1375 train_time:2442ms step_avg:135.67ms
step:29/1375 train_time:2577ms step_avg:135.63ms
step:30/1375 train_time:2712ms step_avg:135.61ms
step:31/1375 train_time:2848ms step_avg:135.61ms
step:32/1375 train_time:2983ms step_avg:135.58ms
step:33/1375 train_time:3119ms step_avg:135.59ms
step:34/1375 train_time:3257ms step_avg:135.69ms
step:35/1375 train_time:3392ms step_avg:135.70ms
step:36/1375 train_time:3529ms step_avg:135.73ms
step:37/1375 train_time:3662ms step_avg:135.62ms
step:38/1375 train_time:3797ms step_avg:135.59ms
step:39/1375 train_time:3931ms step_avg:135.57ms
step:40/1375 train_time:4067ms step_avg:135.57ms
step:41/1375 train_time:4203ms step_avg:135.58ms
step:42/1375 train_time:4338ms step_avg:135.58ms
step:43/1375 train_time:4474ms step_avg:135.56ms
step:44/1375 train_time:4610ms step_avg:135.58ms
step:45/1375 train_time:4744ms step_avg:135.55ms
step:46/1375 train_time:4880ms step_avg:135.54ms
step:47/1375 train_time:5014ms step_avg:135.53ms
step:48/1375 train_time:5151ms step_avg:135.56ms
step:49/1375 train_time:5286ms step_avg:135.53ms
step:50/1375 train_time:5423ms step_avg:135.57ms
step:51/1375 train_time:5558ms step_avg:135.56ms
step:52/1375 train_time:5692ms step_avg:135.53ms
step:53/1375 train_time:5828ms step_avg:135.54ms
step:54/1375 train_time:5963ms step_avg:135.52ms
step:55/1375 train_time:6100ms step_avg:135.55ms
step:56/1375 train_time:6234ms step_avg:135.53ms
step:57/1375 train_time:6369ms step_avg:135.52ms
step:58/1375 train_time:6505ms step_avg:135.53ms
step:59/1375 train_time:6643ms step_avg:135.57ms
step:60/1375 train_time:6776ms step_avg:135.53ms
step:61/1375 train_time:6912ms step_avg:135.53ms
step:62/1375 train_time:7048ms step_avg:135.54ms
step:63/1375 train_time:7184ms step_avg:135.54ms
step:64/1375 train_time:7319ms step_avg:135.54ms
step:65/1375 train_time:7455ms step_avg:135.55ms
step:66/1375 train_time:7590ms step_avg:135.54ms
step:67/1375 train_time:7727ms step_avg:135.55ms
step:68/1375 train_time:7861ms step_avg:135.54ms
step:69/1375 train_time:7997ms step_avg:135.54ms
step:70/1375 train_time:8132ms step_avg:135.54ms
step:71/1375 train_time:8267ms step_avg:135.52ms
step:72/1375 train_time:8404ms step_avg:135.54ms
step:73/1375 train_time:8539ms step_avg:135.55ms
step:74/1375 train_time:8674ms step_avg:135.53ms
step:75/1375 train_time:8809ms step_avg:135.53ms
step:76/1375 train_time:8945ms step_avg:135.53ms
step:77/1375 train_time:9080ms step_avg:135.52ms
step:78/1375 train_time:9215ms step_avg:135.51ms
step:79/1375 train_time:9350ms step_avg:135.51ms
step:80/1375 train_time:9485ms step_avg:135.50ms
step:81/1375 train_time:9621ms step_avg:135.51ms
step:82/1375 train_time:9757ms step_avg:135.51ms
step:83/1375 train_time:9892ms step_avg:135.50ms
step:84/1375 train_time:10028ms step_avg:135.51ms
step:85/1375 train_time:10164ms step_avg:135.52ms
step:86/1375 train_time:10300ms step_avg:135.53ms
step:87/1375 train_time:10435ms step_avg:135.52ms
step:88/1375 train_time:10571ms step_avg:135.52ms
step:89/1375 train_time:10707ms step_avg:135.53ms
step:90/1375 train_time:10844ms step_avg:135.55ms
step:91/1375 train_time:10978ms step_avg:135.53ms
step:92/1375 train_time:11115ms step_avg:135.55ms
step:93/1375 train_time:11251ms step_avg:135.55ms
step:94/1375 train_time:11387ms step_avg:135.55ms
step:95/1375 train_time:11522ms step_avg:135.55ms
step:96/1375 train_time:11657ms step_avg:135.55ms
step:97/1375 train_time:11793ms step_avg:135.55ms
step:98/1375 train_time:11930ms step_avg:135.56ms
step:99/1375 train_time:12065ms step_avg:135.57ms
step:100/1375 train_time:12200ms step_avg:135.56ms
step:101/1375 train_time:12337ms step_avg:135.57ms
step:102/1375 train_time:12473ms step_avg:135.58ms
step:103/1375 train_time:12609ms step_avg:135.59ms
step:104/1375 train_time:12750ms step_avg:135.63ms
step:105/1375 train_time:12887ms step_avg:135.66ms
step:106/1375 train_time:13028ms step_avg:135.70ms
step:107/1375 train_time:13166ms step_avg:135.73ms
step:108/1375 train_time:13305ms step_avg:135.76ms
step:109/1375 train_time:13443ms step_avg:135.78ms
step:110/1375 train_time:13581ms step_avg:135.81ms
step:111/1375 train_time:13720ms step_avg:135.84ms
step:112/1375 train_time:13858ms step_avg:135.87ms
step:113/1375 train_time:13997ms step_avg:135.90ms
step:114/1375 train_time:14136ms step_avg:135.92ms
step:115/1375 train_time:14275ms step_avg:135.95ms
step:116/1375 train_time:14414ms step_avg:135.98ms
step:117/1375 train_time:14552ms step_avg:136.00ms
step:118/1375 train_time:14689ms step_avg:136.01ms
step:119/1375 train_time:14829ms step_avg:136.05ms
step:120/1375 train_time:14968ms step_avg:136.07ms
step:121/1375 train_time:15107ms step_avg:136.10ms
step:122/1375 train_time:15249ms step_avg:136.15ms
step:123/1375 train_time:15386ms step_avg:136.16ms
step:124/1375 train_time:15525ms step_avg:136.18ms
step:125/1375 train_time:15664ms step_avg:136.21ms
step:125/1375 val_loss:4.3642 train_time:15732ms step_avg:136.80ms
step:126/1375 train_time:15806ms step_avg:136.25ms
step:127/1375 train_time:15951ms step_avg:136.33ms
step:128/1375 train_time:16090ms step_avg:136.36ms
step:129/1375 train_time:16227ms step_avg:136.36ms
step:130/1375 train_time:16365ms step_avg:136.37ms
step:131/1375 train_time:16501ms step_avg:136.37ms
step:132/1375 train_time:16638ms step_avg:136.38ms
step:133/1375 train_time:16778ms step_avg:136.41ms
step:134/1375 train_time:16919ms step_avg:136.44ms
step:135/1375 train_time:17058ms step_avg:136.47ms
step:136/1375 train_time:17197ms step_avg:136.49ms
step:137/1375 train_time:17336ms step_avg:136.50ms
step:138/1375 train_time:17475ms step_avg:136.52ms
step:139/1375 train_time:17612ms step_avg:136.53ms
step:140/1375 train_time:17749ms step_avg:136.53ms
step:141/1375 train_time:17890ms step_avg:136.57ms
step:142/1375 train_time:18029ms step_avg:136.58ms
step:143/1375 train_time:18169ms step_avg:136.61ms
step:144/1375 train_time:18309ms step_avg:136.64ms
step:145/1375 train_time:18448ms step_avg:136.65ms
step:146/1375 train_time:18587ms step_avg:136.67ms
step:147/1375 train_time:18725ms step_avg:136.68ms
step:148/1375 train_time:18864ms step_avg:136.69ms
step:149/1375 train_time:19002ms step_avg:136.71ms
step:150/1375 train_time:19141ms step_avg:136.72ms
step:151/1375 train_time:19280ms step_avg:136.73ms
step:152/1375 train_time:19420ms step_avg:136.76ms
step:153/1375 train_time:19558ms step_avg:136.77ms
step:154/1375 train_time:19696ms step_avg:136.78ms
step:155/1375 train_time:19835ms step_avg:136.79ms
step:156/1375 train_time:19974ms step_avg:136.81ms
step:157/1375 train_time:20114ms step_avg:136.83ms
step:158/1375 train_time:20253ms step_avg:136.85ms
step:159/1375 train_time:20392ms step_avg:136.86ms
step:160/1375 train_time:20529ms step_avg:136.86ms
step:161/1375 train_time:20669ms step_avg:136.88ms
step:162/1375 train_time:20807ms step_avg:136.89ms
step:163/1375 train_time:20948ms step_avg:136.92ms
step:164/1375 train_time:21088ms step_avg:136.93ms
step:165/1375 train_time:21226ms step_avg:136.94ms
step:166/1375 train_time:21366ms step_avg:136.96ms
step:167/1375 train_time:21505ms step_avg:136.97ms
step:168/1375 train_time:21643ms step_avg:136.98ms
step:169/1375 train_time:21782ms step_avg:136.99ms
step:170/1375 train_time:21920ms step_avg:137.00ms
step:171/1375 train_time:22060ms step_avg:137.02ms
step:172/1375 train_time:22199ms step_avg:137.03ms
step:173/1375 train_time:22338ms step_avg:137.04ms
step:174/1375 train_time:22477ms step_avg:137.06ms
step:175/1375 train_time:22617ms step_avg:137.07ms
step:176/1375 train_time:22757ms step_avg:137.09ms
step:177/1375 train_time:22896ms step_avg:137.10ms
step:178/1375 train_time:23036ms step_avg:137.12ms
step:179/1375 train_time:23175ms step_avg:137.13ms
step:180/1375 train_time:23314ms step_avg:137.14ms
step:181/1375 train_time:23453ms step_avg:137.15ms
step:182/1375 train_time:23592ms step_avg:137.16ms
step:183/1375 train_time:23731ms step_avg:137.17ms
step:184/1375 train_time:23871ms step_avg:137.19ms
step:185/1375 train_time:24011ms step_avg:137.20ms
step:186/1375 train_time:24151ms step_avg:137.22ms
step:187/1375 train_time:24291ms step_avg:137.24ms
step:188/1375 train_time:24429ms step_avg:137.24ms
step:189/1375 train_time:24571ms step_avg:137.27ms
step:190/1375 train_time:24710ms step_avg:137.28ms
step:191/1375 train_time:24887ms step_avg:137.50ms
step:192/1375 train_time:25023ms step_avg:137.49ms
step:193/1375 train_time:25161ms step_avg:137.49ms
step:194/1375 train_time:25299ms step_avg:137.50ms
step:195/1375 train_time:25438ms step_avg:137.50ms
step:196/1375 train_time:25575ms step_avg:137.50ms
step:197/1375 train_time:25716ms step_avg:137.52ms
step:198/1375 train_time:25859ms step_avg:137.55ms
step:199/1375 train_time:26000ms step_avg:137.56ms
step:200/1375 train_time:26138ms step_avg:137.57ms
step:201/1375 train_time:26276ms step_avg:137.57ms
step:202/1375 train_time:26414ms step_avg:137.57ms
step:203/1375 train_time:26553ms step_avg:137.58ms
step:204/1375 train_time:26693ms step_avg:137.59ms
step:205/1375 train_time:26834ms step_avg:137.61ms
step:206/1375 train_time:26979ms step_avg:137.65ms
step:207/1375 train_time:27120ms step_avg:137.67ms
step:208/1375 train_time:27262ms step_avg:137.69ms
step:209/1375 train_time:27403ms step_avg:137.70ms
step:210/1375 train_time:27543ms step_avg:137.72ms
step:211/1375 train_time:27686ms step_avg:137.74ms
step:212/1375 train_time:27828ms step_avg:137.76ms
step:213/1375 train_time:27972ms step_avg:137.79ms
step:214/1375 train_time:28114ms step_avg:137.82ms
step:215/1375 train_time:28256ms step_avg:137.84ms
step:216/1375 train_time:28396ms step_avg:137.85ms
step:217/1375 train_time:28537ms step_avg:137.86ms
step:218/1375 train_time:28679ms step_avg:137.88ms
step:219/1375 train_time:28821ms step_avg:137.90ms
step:220/1375 train_time:28964ms step_avg:137.92ms
step:221/1375 train_time:29106ms step_avg:137.95ms
step:222/1375 train_time:29249ms step_avg:137.97ms
step:223/1375 train_time:29391ms step_avg:137.98ms
step:224/1375 train_time:29530ms step_avg:137.99ms
step:225/1375 train_time:29673ms step_avg:138.01ms
step:226/1375 train_time:29814ms step_avg:138.03ms
step:227/1375 train_time:29956ms step_avg:138.05ms
step:228/1375 train_time:30097ms step_avg:138.06ms
step:229/1375 train_time:30240ms step_avg:138.08ms
step:230/1375 train_time:30383ms step_avg:138.10ms
step:231/1375 train_time:30524ms step_avg:138.12ms
step:232/1375 train_time:30666ms step_avg:138.14ms
step:233/1375 train_time:30808ms step_avg:138.15ms
step:234/1375 train_time:30951ms step_avg:138.17ms
step:235/1375 train_time:31093ms step_avg:138.19ms
step:236/1375 train_time:31233ms step_avg:138.20ms
step:237/1375 train_time:31376ms step_avg:138.22ms
step:238/1375 train_time:31517ms step_avg:138.23ms
step:239/1375 train_time:31659ms step_avg:138.25ms
step:240/1375 train_time:31800ms step_avg:138.26ms
step:241/1375 train_time:31942ms step_avg:138.28ms
step:242/1375 train_time:32083ms step_avg:138.29ms
step:243/1375 train_time:32226ms step_avg:138.31ms
step:244/1375 train_time:32369ms step_avg:138.33ms
step:245/1375 train_time:32511ms step_avg:138.34ms
step:246/1375 train_time:32652ms step_avg:138.36ms
step:247/1375 train_time:32794ms step_avg:138.37ms
step:248/1375 train_time:32935ms step_avg:138.38ms
step:249/1375 train_time:33078ms step_avg:138.40ms
step:250/1375 train_time:33221ms step_avg:138.42ms
step:250/1375 val_loss:3.9564 train_time:33292ms step_avg:138.71ms
step:251/1375 train_time:33367ms step_avg:138.45ms
step:252/1375 train_time:33511ms step_avg:138.47ms
step:253/1375 train_time:33653ms step_avg:138.49ms
step:254/1375 train_time:33794ms step_avg:138.50ms
step:255/1375 train_time:33935ms step_avg:138.51ms
step:256/1375 train_time:34075ms step_avg:138.52ms
step:257/1375 train_time:34217ms step_avg:138.53ms
step:258/1375 train_time:34360ms step_avg:138.55ms
step:259/1375 train_time:34505ms step_avg:138.57ms
step:260/1375 train_time:34646ms step_avg:138.58ms
step:261/1375 train_time:34788ms step_avg:138.60ms
step:262/1375 train_time:34929ms step_avg:138.61ms
step:263/1375 train_time:35070ms step_avg:138.61ms
step:264/1375 train_time:35210ms step_avg:138.62ms
step:265/1375 train_time:35351ms step_avg:138.63ms
step:266/1375 train_time:35494ms step_avg:138.65ms
step:267/1375 train_time:35636ms step_avg:138.66ms
step:268/1375 train_time:35778ms step_avg:138.67ms
step:269/1375 train_time:35919ms step_avg:138.68ms
step:270/1375 train_time:36061ms step_avg:138.70ms
step:271/1375 train_time:36203ms step_avg:138.71ms
step:272/1375 train_time:36343ms step_avg:138.71ms
step:273/1375 train_time:36486ms step_avg:138.73ms
step:274/1375 train_time:36629ms step_avg:138.75ms
step:275/1375 train_time:36772ms step_avg:138.76ms
step:276/1375 train_time:36912ms step_avg:138.77ms
step:277/1375 train_time:37053ms step_avg:138.77ms
step:278/1375 train_time:37194ms step_avg:138.78ms
step:279/1375 train_time:37337ms step_avg:138.80ms
step:280/1375 train_time:37479ms step_avg:138.81ms
step:281/1375 train_time:37620ms step_avg:138.82ms
step:282/1375 train_time:37763ms step_avg:138.83ms
step:283/1375 train_time:37906ms step_avg:138.85ms
step:284/1375 train_time:38046ms step_avg:138.85ms
step:285/1375 train_time:38190ms step_avg:138.87ms
step:286/1375 train_time:38332ms step_avg:138.88ms
step:287/1375 train_time:38474ms step_avg:138.90ms
step:288/1375 train_time:38616ms step_avg:138.90ms
step:289/1375 train_time:38757ms step_avg:138.92ms
step:290/1375 train_time:38899ms step_avg:138.93ms
step:291/1375 train_time:39041ms step_avg:138.94ms
step:292/1375 train_time:39184ms step_avg:138.95ms
step:293/1375 train_time:39327ms step_avg:138.96ms
step:294/1375 train_time:39468ms step_avg:138.97ms
step:295/1375 train_time:39610ms step_avg:138.98ms
step:296/1375 train_time:39751ms step_avg:138.99ms
step:297/1375 train_time:39894ms step_avg:139.00ms
step:298/1375 train_time:40035ms step_avg:139.01ms
step:299/1375 train_time:40177ms step_avg:139.02ms
step:300/1375 train_time:40320ms step_avg:139.04ms
step:301/1375 train_time:40464ms step_avg:139.05ms
step:302/1375 train_time:40607ms step_avg:139.06ms
step:303/1375 train_time:40747ms step_avg:139.07ms
step:304/1375 train_time:40890ms step_avg:139.08ms
step:305/1375 train_time:41031ms step_avg:139.09ms
step:306/1375 train_time:41172ms step_avg:139.09ms
step:307/1375 train_time:41316ms step_avg:139.11ms
step:308/1375 train_time:41460ms step_avg:139.13ms
step:309/1375 train_time:41606ms step_avg:139.15ms
step:310/1375 train_time:41748ms step_avg:139.16ms
step:311/1375 train_time:41892ms step_avg:139.18ms
step:312/1375 train_time:42036ms step_avg:139.19ms
step:313/1375 train_time:42180ms step_avg:139.21ms
step:314/1375 train_time:42326ms step_avg:139.23ms
step:315/1375 train_time:42469ms step_avg:139.24ms
step:316/1375 train_time:42614ms step_avg:139.26ms
step:317/1375 train_time:42758ms step_avg:139.28ms
step:318/1375 train_time:42903ms step_avg:139.29ms
step:319/1375 train_time:43046ms step_avg:139.31ms
step:320/1375 train_time:43191ms step_avg:139.32ms
step:321/1375 train_time:43334ms step_avg:139.34ms
step:322/1375 train_time:43481ms step_avg:139.36ms
step:323/1375 train_time:43624ms step_avg:139.37ms
step:324/1375 train_time:43768ms step_avg:139.39ms
step:325/1375 train_time:43911ms step_avg:139.40ms
step:326/1375 train_time:44054ms step_avg:139.41ms
step:327/1375 train_time:44198ms step_avg:139.43ms
step:328/1375 train_time:44343ms step_avg:139.44ms
step:329/1375 train_time:44490ms step_avg:139.47ms
step:330/1375 train_time:44634ms step_avg:139.48ms
step:331/1375 train_time:44779ms step_avg:139.50ms
step:332/1375 train_time:44925ms step_avg:139.52ms
step:333/1375 train_time:45068ms step_avg:139.53ms
step:334/1375 train_time:45212ms step_avg:139.54ms
step:335/1375 train_time:45356ms step_avg:139.56ms
step:336/1375 train_time:45500ms step_avg:139.57ms
step:337/1375 train_time:45644ms step_avg:139.58ms
step:338/1375 train_time:45788ms step_avg:139.60ms
step:339/1375 train_time:45932ms step_avg:139.61ms
step:340/1375 train_time:46075ms step_avg:139.62ms
step:341/1375 train_time:46219ms step_avg:139.63ms
step:342/1375 train_time:46364ms step_avg:139.65ms
step:343/1375 train_time:46509ms step_avg:139.67ms
step:344/1375 train_time:46652ms step_avg:139.68ms
step:345/1375 train_time:46796ms step_avg:139.69ms
step:346/1375 train_time:46941ms step_avg:139.70ms
step:347/1375 train_time:47086ms step_avg:139.72ms
step:348/1375 train_time:47231ms step_avg:139.74ms
step:349/1375 train_time:47375ms step_avg:139.75ms
step:350/1375 train_time:47519ms step_avg:139.76ms
step:351/1375 train_time:47664ms step_avg:139.78ms
step:352/1375 train_time:47809ms step_avg:139.79ms
step:353/1375 train_time:47951ms step_avg:139.80ms
step:354/1375 train_time:48094ms step_avg:139.81ms
step:355/1375 train_time:48239ms step_avg:139.82ms
step:356/1375 train_time:48385ms step_avg:139.84ms
step:357/1375 train_time:48530ms step_avg:139.86ms
step:358/1375 train_time:48674ms step_avg:139.87ms
step:359/1375 train_time:48819ms step_avg:139.88ms
step:360/1375 train_time:48965ms step_avg:139.90ms
step:361/1375 train_time:49107ms step_avg:139.91ms
step:362/1375 train_time:49250ms step_avg:139.92ms
step:363/1375 train_time:49395ms step_avg:139.93ms
step:364/1375 train_time:49537ms step_avg:139.94ms
step:365/1375 train_time:49683ms step_avg:139.95ms
step:366/1375 train_time:49829ms step_avg:139.97ms
step:367/1375 train_time:49972ms step_avg:139.98ms
step:368/1375 train_time:50115ms step_avg:139.99ms
step:369/1375 train_time:50259ms step_avg:140.00ms
step:370/1375 train_time:50405ms step_avg:140.01ms
step:371/1375 train_time:50548ms step_avg:140.02ms
step:372/1375 train_time:50692ms step_avg:140.03ms
step:373/1375 train_time:50836ms step_avg:140.04ms
step:374/1375 train_time:50981ms step_avg:140.06ms
step:375/1375 train_time:51126ms step_avg:140.07ms
step:375/1375 val_loss:3.7756 train_time:51195ms step_avg:140.26ms
step:376/1375 train_time:51271ms step_avg:140.08ms
step:377/1375 train_time:51417ms step_avg:140.10ms
step:378/1375 train_time:51561ms step_avg:140.11ms
step:379/1375 train_time:51705ms step_avg:140.12ms
step:380/1375 train_time:51848ms step_avg:140.13ms
step:381/1375 train_time:52031ms step_avg:140.25ms
step:382/1375 train_time:52174ms step_avg:140.25ms
step:383/1375 train_time:52316ms step_avg:140.26ms
step:384/1375 train_time:52458ms step_avg:140.26ms
step:385/1375 train_time:52602ms step_avg:140.27ms
step:386/1375 train_time:52744ms step_avg:140.28ms
step:387/1375 train_time:52891ms step_avg:140.29ms
step:388/1375 train_time:53040ms step_avg:140.32ms
step:389/1375 train_time:53185ms step_avg:140.33ms
step:390/1375 train_time:53330ms step_avg:140.34ms
step:391/1375 train_time:53474ms step_avg:140.35ms
step:392/1375 train_time:53616ms step_avg:140.36ms
step:393/1375 train_time:53759ms step_avg:140.36ms
step:394/1375 train_time:53903ms step_avg:140.37ms
step:395/1375 train_time:54049ms step_avg:140.39ms
step:396/1375 train_time:54195ms step_avg:140.40ms
step:397/1375 train_time:54338ms step_avg:140.41ms
step:398/1375 train_time:54482ms step_avg:140.42ms
step:399/1375 train_time:54625ms step_avg:140.42ms
step:400/1375 train_time:54769ms step_avg:140.43ms
step:401/1375 train_time:54915ms step_avg:140.45ms
step:402/1375 train_time:55058ms step_avg:140.45ms
step:403/1375 train_time:55204ms step_avg:140.47ms
step:404/1375 train_time:55348ms step_avg:140.48ms
step:405/1375 train_time:55493ms step_avg:140.49ms
step:406/1375 train_time:55636ms step_avg:140.50ms
step:407/1375 train_time:55780ms step_avg:140.50ms
step:408/1375 train_time:55925ms step_avg:140.51ms
step:409/1375 train_time:56071ms step_avg:140.53ms
step:410/1375 train_time:56218ms step_avg:140.54ms
step:411/1375 train_time:56363ms step_avg:140.56ms
step:412/1375 train_time:56510ms step_avg:140.57ms
step:413/1375 train_time:56655ms step_avg:140.58ms
step:414/1375 train_time:56800ms step_avg:140.59ms
step:415/1375 train_time:56947ms step_avg:140.61ms
step:416/1375 train_time:57094ms step_avg:140.63ms
step:417/1375 train_time:57239ms step_avg:140.64ms
step:418/1375 train_time:57386ms step_avg:140.65ms
step:419/1375 train_time:57532ms step_avg:140.67ms
step:420/1375 train_time:57678ms step_avg:140.68ms
step:421/1375 train_time:57824ms step_avg:140.69ms
step:422/1375 train_time:57972ms step_avg:140.71ms
step:423/1375 train_time:58118ms step_avg:140.72ms
step:424/1375 train_time:58265ms step_avg:140.74ms
step:425/1375 train_time:58414ms step_avg:140.76ms
step:426/1375 train_time:58559ms step_avg:140.77ms
step:427/1375 train_time:58704ms step_avg:140.78ms
step:428/1375 train_time:58850ms step_avg:140.79ms
step:429/1375 train_time:58996ms step_avg:140.80ms
step:430/1375 train_time:59141ms step_avg:140.81ms
step:431/1375 train_time:59288ms step_avg:140.83ms
step:432/1375 train_time:59435ms step_avg:140.84ms
step:433/1375 train_time:59580ms step_avg:140.85ms
step:434/1375 train_time:59725ms step_avg:140.86ms
step:435/1375 train_time:59872ms step_avg:140.87ms
step:436/1375 train_time:60017ms step_avg:140.88ms
step:437/1375 train_time:60162ms step_avg:140.89ms
step:438/1375 train_time:60309ms step_avg:140.91ms
step:439/1375 train_time:60453ms step_avg:140.92ms
step:440/1375 train_time:60598ms step_avg:140.93ms
step:441/1375 train_time:60745ms step_avg:140.94ms
step:442/1375 train_time:60891ms step_avg:140.95ms
step:443/1375 train_time:61037ms step_avg:140.96ms
step:444/1375 train_time:61182ms step_avg:140.97ms
step:445/1375 train_time:61329ms step_avg:140.99ms
step:446/1375 train_time:61476ms step_avg:141.00ms
step:447/1375 train_time:61620ms step_avg:141.01ms
step:448/1375 train_time:61767ms step_avg:141.02ms
step:449/1375 train_time:61915ms step_avg:141.04ms
step:450/1375 train_time:62059ms step_avg:141.04ms
step:451/1375 train_time:62207ms step_avg:141.06ms
step:452/1375 train_time:62352ms step_avg:141.07ms
step:453/1375 train_time:62499ms step_avg:141.08ms
step:454/1375 train_time:62646ms step_avg:141.09ms
step:455/1375 train_time:62794ms step_avg:141.11ms
step:456/1375 train_time:62939ms step_avg:141.12ms
step:457/1375 train_time:63085ms step_avg:141.13ms
step:458/1375 train_time:63233ms step_avg:141.14ms
step:459/1375 train_time:63377ms step_avg:141.15ms
step:460/1375 train_time:63524ms step_avg:141.16ms
step:461/1375 train_time:63672ms step_avg:141.18ms
step:462/1375 train_time:63817ms step_avg:141.19ms
step:463/1375 train_time:63963ms step_avg:141.20ms
step:464/1375 train_time:64108ms step_avg:141.21ms
step:465/1375 train_time:64253ms step_avg:141.22ms
step:466/1375 train_time:64400ms step_avg:141.23ms
step:467/1375 train_time:64546ms step_avg:141.24ms
step:468/1375 train_time:64693ms step_avg:141.25ms
step:469/1375 train_time:64839ms step_avg:141.26ms
step:470/1375 train_time:64984ms step_avg:141.27ms
step:471/1375 train_time:65130ms step_avg:141.28ms
step:472/1375 train_time:65276ms step_avg:141.29ms
step:473/1375 train_time:65420ms step_avg:141.30ms
step:474/1375 train_time:65567ms step_avg:141.31ms
step:475/1375 train_time:65714ms step_avg:141.32ms
step:476/1375 train_time:65858ms step_avg:141.33ms
step:477/1375 train_time:66005ms step_avg:141.34ms
step:478/1375 train_time:66150ms step_avg:141.35ms
step:479/1375 train_time:66298ms step_avg:141.36ms
step:480/1375 train_time:66444ms step_avg:141.37ms
step:481/1375 train_time:66592ms step_avg:141.38ms
step:482/1375 train_time:66737ms step_avg:141.39ms
step:483/1375 train_time:66882ms step_avg:141.40ms
step:484/1375 train_time:67028ms step_avg:141.41ms
step:485/1375 train_time:67174ms step_avg:141.42ms
step:486/1375 train_time:67321ms step_avg:141.43ms
step:487/1375 train_time:67467ms step_avg:141.44ms
step:488/1375 train_time:67615ms step_avg:141.45ms
step:489/1375 train_time:67759ms step_avg:141.46ms
step:490/1375 train_time:67906ms step_avg:141.47ms
step:491/1375 train_time:68051ms step_avg:141.48ms
step:492/1375 train_time:68197ms step_avg:141.49ms
step:493/1375 train_time:68342ms step_avg:141.50ms
step:494/1375 train_time:68489ms step_avg:141.51ms
step:495/1375 train_time:68636ms step_avg:141.52ms
step:496/1375 train_time:68781ms step_avg:141.52ms
step:497/1375 train_time:68925ms step_avg:141.53ms
step:498/1375 train_time:69072ms step_avg:141.54ms
step:499/1375 train_time:69218ms step_avg:141.55ms
step:500/1375 train_time:69362ms step_avg:141.56ms
step:500/1375 val_loss:3.6549 train_time:69435ms step_avg:141.70ms
step:501/1375 train_time:69510ms step_avg:141.57ms
step:502/1375 train_time:69658ms step_avg:141.58ms
step:503/1375 train_time:69805ms step_avg:141.59ms
step:504/1375 train_time:69949ms step_avg:141.60ms
step:505/1375 train_time:70093ms step_avg:141.60ms
step:506/1375 train_time:70238ms step_avg:141.61ms
step:507/1375 train_time:70385ms step_avg:141.62ms
step:508/1375 train_time:70533ms step_avg:141.63ms
step:509/1375 train_time:70679ms step_avg:141.64ms
step:510/1375 train_time:70827ms step_avg:141.65ms
step:511/1375 train_time:70972ms step_avg:141.66ms
step:512/1375 train_time:71120ms step_avg:141.67ms
step:513/1375 train_time:71267ms step_avg:141.68ms
step:514/1375 train_time:71414ms step_avg:141.69ms
step:515/1375 train_time:71563ms step_avg:141.71ms
step:516/1375 train_time:71712ms step_avg:141.72ms
step:517/1375 train_time:71859ms step_avg:141.73ms
step:518/1375 train_time:72008ms step_avg:141.75ms
step:519/1375 train_time:72154ms step_avg:141.76ms
step:520/1375 train_time:72301ms step_avg:141.77ms
step:521/1375 train_time:72448ms step_avg:141.78ms
step:522/1375 train_time:72594ms step_avg:141.79ms
step:523/1375 train_time:72741ms step_avg:141.80ms
step:524/1375 train_time:72890ms step_avg:141.81ms
step:525/1375 train_time:73037ms step_avg:141.82ms
step:526/1375 train_time:73186ms step_avg:141.83ms
step:527/1375 train_time:73333ms step_avg:141.84ms
step:528/1375 train_time:73479ms step_avg:141.85ms
step:529/1375 train_time:73626ms step_avg:141.86ms
step:530/1375 train_time:73774ms step_avg:141.87ms
step:531/1375 train_time:73923ms step_avg:141.89ms
step:532/1375 train_time:74070ms step_avg:141.90ms
step:533/1375 train_time:74217ms step_avg:141.91ms
step:534/1375 train_time:74364ms step_avg:141.92ms
step:535/1375 train_time:74511ms step_avg:141.92ms
step:536/1375 train_time:74659ms step_avg:141.94ms
step:537/1375 train_time:74807ms step_avg:141.95ms
step:538/1375 train_time:74954ms step_avg:141.96ms
step:539/1375 train_time:75102ms step_avg:141.97ms
step:540/1375 train_time:75249ms step_avg:141.98ms
step:541/1375 train_time:75395ms step_avg:141.99ms
step:542/1375 train_time:75542ms step_avg:142.00ms
step:543/1375 train_time:75691ms step_avg:142.01ms
step:544/1375 train_time:75838ms step_avg:142.02ms
step:545/1375 train_time:75986ms step_avg:142.03ms
step:546/1375 train_time:76133ms step_avg:142.04ms
step:547/1375 train_time:76281ms step_avg:142.05ms
step:548/1375 train_time:76429ms step_avg:142.06ms
step:549/1375 train_time:76576ms step_avg:142.07ms
step:550/1375 train_time:76724ms step_avg:142.08ms
step:551/1375 train_time:76872ms step_avg:142.09ms
step:552/1375 train_time:77019ms step_avg:142.10ms
step:553/1375 train_time:77167ms step_avg:142.11ms
step:554/1375 train_time:77313ms step_avg:142.12ms
step:555/1375 train_time:77462ms step_avg:142.13ms
step:556/1375 train_time:77609ms step_avg:142.14ms
step:557/1375 train_time:77755ms step_avg:142.15ms
step:558/1375 train_time:77903ms step_avg:142.16ms
step:559/1375 train_time:78049ms step_avg:142.17ms
step:560/1375 train_time:78197ms step_avg:142.18ms
step:561/1375 train_time:78344ms step_avg:142.19ms
step:562/1375 train_time:78492ms step_avg:142.20ms
step:563/1375 train_time:78640ms step_avg:142.21ms
step:564/1375 train_time:78787ms step_avg:142.22ms
step:565/1375 train_time:78934ms step_avg:142.22ms
step:566/1375 train_time:79081ms step_avg:142.23ms
step:567/1375 train_time:79228ms step_avg:142.24ms
step:568/1375 train_time:79374ms step_avg:142.25ms
step:569/1375 train_time:79522ms step_avg:142.26ms
step:570/1375 train_time:79670ms step_avg:142.27ms
step:571/1375 train_time:79857ms step_avg:142.35ms
step:572/1375 train_time:80003ms step_avg:142.35ms
step:573/1375 train_time:80149ms step_avg:142.36ms
step:574/1375 train_time:80298ms step_avg:142.37ms
step:575/1375 train_time:80444ms step_avg:142.38ms
step:576/1375 train_time:80591ms step_avg:142.39ms
step:577/1375 train_time:80739ms step_avg:142.40ms
step:578/1375 train_time:80890ms step_avg:142.41ms
step:579/1375 train_time:81037ms step_avg:142.42ms
step:580/1375 train_time:81186ms step_avg:142.43ms
step:581/1375 train_time:81331ms step_avg:142.44ms
step:582/1375 train_time:81477ms step_avg:142.44ms
step:583/1375 train_time:81623ms step_avg:142.45ms
step:584/1375 train_time:81772ms step_avg:142.46ms
step:585/1375 train_time:81920ms step_avg:142.47ms
step:586/1375 train_time:82070ms step_avg:142.48ms
step:587/1375 train_time:82217ms step_avg:142.49ms
step:588/1375 train_time:82365ms step_avg:142.50ms
step:589/1375 train_time:82512ms step_avg:142.51ms
step:590/1375 train_time:82659ms step_avg:142.51ms
step:591/1375 train_time:82808ms step_avg:142.53ms
step:592/1375 train_time:82956ms step_avg:142.54ms
step:593/1375 train_time:83103ms step_avg:142.54ms
step:594/1375 train_time:83250ms step_avg:142.55ms
step:595/1375 train_time:83397ms step_avg:142.56ms
step:596/1375 train_time:83544ms step_avg:142.57ms
step:597/1375 train_time:83692ms step_avg:142.58ms
step:598/1375 train_time:83840ms step_avg:142.58ms
step:599/1375 train_time:83988ms step_avg:142.59ms
step:600/1375 train_time:84134ms step_avg:142.60ms
step:601/1375 train_time:84281ms step_avg:142.61ms
step:602/1375 train_time:84428ms step_avg:142.61ms
step:603/1375 train_time:84575ms step_avg:142.62ms
step:604/1375 train_time:84723ms step_avg:142.63ms
step:605/1375 train_time:84871ms step_avg:142.64ms
step:606/1375 train_time:85018ms step_avg:142.65ms
step:607/1375 train_time:85166ms step_avg:142.66ms
step:608/1375 train_time:85313ms step_avg:142.66ms
step:609/1375 train_time:85459ms step_avg:142.67ms
step:610/1375 train_time:85607ms step_avg:142.68ms
step:611/1375 train_time:85753ms step_avg:142.68ms
step:612/1375 train_time:85901ms step_avg:142.69ms
step:613/1375 train_time:86050ms step_avg:142.70ms
step:614/1375 train_time:86198ms step_avg:142.71ms
step:615/1375 train_time:86347ms step_avg:142.72ms
step:616/1375 train_time:86495ms step_avg:142.73ms
step:617/1375 train_time:86645ms step_avg:142.74ms
step:618/1375 train_time:86794ms step_avg:142.75ms
step:619/1375 train_time:86946ms step_avg:142.77ms
step:620/1375 train_time:87094ms step_avg:142.78ms
step:621/1375 train_time:87244ms step_avg:142.79ms
step:622/1375 train_time:87392ms step_avg:142.80ms
step:623/1375 train_time:87542ms step_avg:142.81ms
step:624/1375 train_time:87691ms step_avg:142.82ms
step:625/1375 train_time:87839ms step_avg:142.83ms
step:625/1375 val_loss:3.5739 train_time:87915ms step_avg:142.95ms
step:626/1375 train_time:87990ms step_avg:142.84ms
step:627/1375 train_time:88141ms step_avg:142.85ms
step:628/1375 train_time:88288ms step_avg:142.86ms
step:629/1375 train_time:88437ms step_avg:142.87ms
step:630/1375 train_time:88584ms step_avg:142.88ms
step:631/1375 train_time:88731ms step_avg:142.88ms
step:632/1375 train_time:88882ms step_avg:142.90ms
step:633/1375 train_time:89031ms step_avg:142.91ms
step:634/1375 train_time:89182ms step_avg:142.92ms
step:635/1375 train_time:89330ms step_avg:142.93ms
step:636/1375 train_time:89479ms step_avg:142.94ms
step:637/1375 train_time:89626ms step_avg:142.94ms
step:638/1375 train_time:89774ms step_avg:142.95ms
step:639/1375 train_time:89921ms step_avg:142.96ms
step:640/1375 train_time:90072ms step_avg:142.97ms
step:641/1375 train_time:90221ms step_avg:142.98ms
step:642/1375 train_time:90371ms step_avg:142.99ms
step:643/1375 train_time:90520ms step_avg:143.00ms
step:644/1375 train_time:90668ms step_avg:143.01ms
step:645/1375 train_time:90817ms step_avg:143.02ms
step:646/1375 train_time:90966ms step_avg:143.03ms
step:647/1375 train_time:91116ms step_avg:143.04ms
step:648/1375 train_time:91269ms step_avg:143.05ms
step:649/1375 train_time:91418ms step_avg:143.06ms
step:650/1375 train_time:91567ms step_avg:143.07ms
step:651/1375 train_time:91717ms step_avg:143.08ms
step:652/1375 train_time:91866ms step_avg:143.09ms
step:653/1375 train_time:92015ms step_avg:143.10ms
step:654/1375 train_time:92167ms step_avg:143.12ms
step:655/1375 train_time:92315ms step_avg:143.12ms
step:656/1375 train_time:92464ms step_avg:143.13ms
step:657/1375 train_time:92613ms step_avg:143.14ms
step:658/1375 train_time:92763ms step_avg:143.15ms
step:659/1375 train_time:92912ms step_avg:143.16ms
step:660/1375 train_time:93061ms step_avg:143.17ms
step:661/1375 train_time:93212ms step_avg:143.18ms
step:662/1375 train_time:93361ms step_avg:143.19ms
step:663/1375 train_time:93510ms step_avg:143.20ms
step:664/1375 train_time:93662ms step_avg:143.21ms
step:665/1375 train_time:93812ms step_avg:143.22ms
step:666/1375 train_time:93961ms step_avg:143.23ms
step:667/1375 train_time:94108ms step_avg:143.24ms
step:668/1375 train_time:94259ms step_avg:143.25ms
step:669/1375 train_time:94408ms step_avg:143.26ms
step:670/1375 train_time:94558ms step_avg:143.27ms
step:671/1375 train_time:94706ms step_avg:143.28ms
step:672/1375 train_time:94855ms step_avg:143.29ms
step:673/1375 train_time:95004ms step_avg:143.29ms
step:674/1375 train_time:95155ms step_avg:143.31ms
step:675/1375 train_time:95304ms step_avg:143.31ms
step:676/1375 train_time:95453ms step_avg:143.32ms
step:677/1375 train_time:95601ms step_avg:143.33ms
step:678/1375 train_time:95749ms step_avg:143.34ms
step:679/1375 train_time:95898ms step_avg:143.35ms
step:680/1375 train_time:96047ms step_avg:143.35ms
step:681/1375 train_time:96195ms step_avg:143.36ms
step:682/1375 train_time:96343ms step_avg:143.37ms
step:683/1375 train_time:96492ms step_avg:143.38ms
step:684/1375 train_time:96641ms step_avg:143.38ms
step:685/1375 train_time:96790ms step_avg:143.39ms
step:686/1375 train_time:96938ms step_avg:143.40ms
step:687/1375 train_time:97086ms step_avg:143.41ms
step:688/1375 train_time:97238ms step_avg:143.42ms
step:689/1375 train_time:97386ms step_avg:143.43ms
step:690/1375 train_time:97537ms step_avg:143.44ms
step:691/1375 train_time:97685ms step_avg:143.44ms
step:692/1375 train_time:97834ms step_avg:143.45ms
step:693/1375 train_time:97983ms step_avg:143.46ms
step:694/1375 train_time:98132ms step_avg:143.47ms
step:695/1375 train_time:98280ms step_avg:143.47ms
step:696/1375 train_time:98427ms step_avg:143.48ms
step:697/1375 train_time:98577ms step_avg:143.49ms
step:698/1375 train_time:98723ms step_avg:143.49ms
step:699/1375 train_time:98873ms step_avg:143.50ms
step:700/1375 train_time:99021ms step_avg:143.51ms
step:701/1375 train_time:99170ms step_avg:143.52ms
step:702/1375 train_time:99320ms step_avg:143.53ms
step:703/1375 train_time:99469ms step_avg:143.53ms
step:704/1375 train_time:99619ms step_avg:143.54ms
step:705/1375 train_time:99768ms step_avg:143.55ms
step:706/1375 train_time:99922ms step_avg:143.57ms
step:707/1375 train_time:100073ms step_avg:143.58ms
step:708/1375 train_time:100220ms step_avg:143.58ms
step:709/1375 train_time:100369ms step_avg:143.59ms
step:710/1375 train_time:100519ms step_avg:143.60ms
step:711/1375 train_time:100670ms step_avg:143.61ms
step:712/1375 train_time:100820ms step_avg:143.62ms
step:713/1375 train_time:100972ms step_avg:143.63ms
step:714/1375 train_time:101121ms step_avg:143.64ms
step:715/1375 train_time:101272ms step_avg:143.65ms
step:716/1375 train_time:101421ms step_avg:143.66ms
step:717/1375 train_time:101572ms step_avg:143.67ms
step:718/1375 train_time:101723ms step_avg:143.68ms
step:719/1375 train_time:101873ms step_avg:143.68ms
step:720/1375 train_time:102023ms step_avg:143.69ms
step:721/1375 train_time:102175ms step_avg:143.71ms
step:722/1375 train_time:102324ms step_avg:143.71ms
step:723/1375 train_time:102473ms step_avg:143.72ms
step:724/1375 train_time:102624ms step_avg:143.73ms
step:725/1375 train_time:102775ms step_avg:143.74ms
step:726/1375 train_time:102925ms step_avg:143.75ms
step:727/1375 train_time:103077ms step_avg:143.76ms
step:728/1375 train_time:103225ms step_avg:143.77ms
step:729/1375 train_time:103375ms step_avg:143.78ms
step:730/1375 train_time:103526ms step_avg:143.79ms
step:731/1375 train_time:103677ms step_avg:143.80ms
step:732/1375 train_time:103824ms step_avg:143.80ms
step:733/1375 train_time:103978ms step_avg:143.82ms
step:734/1375 train_time:104127ms step_avg:143.82ms
step:735/1375 train_time:104280ms step_avg:143.83ms
step:736/1375 train_time:104430ms step_avg:143.84ms
step:737/1375 train_time:104580ms step_avg:143.85ms
step:738/1375 train_time:104728ms step_avg:143.86ms
step:739/1375 train_time:104880ms step_avg:143.87ms
step:740/1375 train_time:105030ms step_avg:143.88ms
step:741/1375 train_time:105182ms step_avg:143.89ms
step:742/1375 train_time:105332ms step_avg:143.90ms
step:743/1375 train_time:105482ms step_avg:143.90ms
step:744/1375 train_time:105631ms step_avg:143.91ms
step:745/1375 train_time:105784ms step_avg:143.92ms
step:746/1375 train_time:105933ms step_avg:143.93ms
step:747/1375 train_time:106083ms step_avg:143.94ms
step:748/1375 train_time:106235ms step_avg:143.95ms
step:749/1375 train_time:106386ms step_avg:143.96ms
step:750/1375 train_time:106537ms step_avg:143.97ms
step:750/1375 val_loss:3.5200 train_time:106613ms step_avg:144.07ms
step:751/1375 train_time:106691ms step_avg:143.98ms
step:752/1375 train_time:106842ms step_avg:143.99ms
step:753/1375 train_time:106990ms step_avg:144.00ms
step:754/1375 train_time:107140ms step_avg:144.00ms
step:755/1375 train_time:107287ms step_avg:144.01ms
step:756/1375 train_time:107437ms step_avg:144.02ms
step:757/1375 train_time:107590ms step_avg:144.03ms
step:758/1375 train_time:107742ms step_avg:144.04ms
step:759/1375 train_time:107893ms step_avg:144.05ms
step:760/1375 train_time:108043ms step_avg:144.06ms
step:761/1375 train_time:108234ms step_avg:144.12ms
step:762/1375 train_time:108381ms step_avg:144.12ms
step:763/1375 train_time:108530ms step_avg:144.13ms
step:764/1375 train_time:108679ms step_avg:144.14ms
step:765/1375 train_time:108828ms step_avg:144.14ms
step:766/1375 train_time:108979ms step_avg:144.15ms
step:767/1375 train_time:109132ms step_avg:144.16ms
step:768/1375 train_time:109284ms step_avg:144.17ms
step:769/1375 train_time:109436ms step_avg:144.18ms
step:770/1375 train_time:109585ms step_avg:144.19ms
step:771/1375 train_time:109735ms step_avg:144.20ms
step:772/1375 train_time:109884ms step_avg:144.20ms
step:773/1375 train_time:110034ms step_avg:144.21ms
step:774/1375 train_time:110184ms step_avg:144.22ms
step:775/1375 train_time:110336ms step_avg:144.23ms
step:776/1375 train_time:110487ms step_avg:144.24ms
step:777/1375 train_time:110639ms step_avg:144.25ms
step:778/1375 train_time:110787ms step_avg:144.25ms
step:779/1375 train_time:110936ms step_avg:144.26ms
step:780/1375 train_time:111086ms step_avg:144.27ms
step:781/1375 train_time:111238ms step_avg:144.28ms
step:782/1375 train_time:111388ms step_avg:144.28ms
step:783/1375 train_time:111540ms step_avg:144.29ms
step:784/1375 train_time:111689ms step_avg:144.30ms
step:785/1375 train_time:111840ms step_avg:144.31ms
step:786/1375 train_time:111988ms step_avg:144.31ms
step:787/1375 train_time:112140ms step_avg:144.32ms
step:788/1375 train_time:112289ms step_avg:144.33ms
step:789/1375 train_time:112441ms step_avg:144.34ms
step:790/1375 train_time:112590ms step_avg:144.35ms
step:791/1375 train_time:112742ms step_avg:144.36ms
step:792/1375 train_time:112891ms step_avg:144.36ms
step:793/1375 train_time:113040ms step_avg:144.37ms
step:794/1375 train_time:113189ms step_avg:144.37ms
step:795/1375 train_time:113342ms step_avg:144.39ms
step:796/1375 train_time:113492ms step_avg:144.39ms
step:797/1375 train_time:113642ms step_avg:144.40ms
step:798/1375 train_time:113793ms step_avg:144.41ms
step:799/1375 train_time:113947ms step_avg:144.42ms
step:800/1375 train_time:114097ms step_avg:144.43ms
step:801/1375 train_time:114246ms step_avg:144.43ms
step:802/1375 train_time:114398ms step_avg:144.44ms
step:803/1375 train_time:114547ms step_avg:144.45ms
step:804/1375 train_time:114697ms step_avg:144.46ms
step:805/1375 train_time:114852ms step_avg:144.47ms
step:806/1375 train_time:115003ms step_avg:144.48ms
step:807/1375 train_time:115151ms step_avg:144.48ms
step:808/1375 train_time:115302ms step_avg:144.49ms
step:809/1375 train_time:115451ms step_avg:144.49ms
step:810/1375 train_time:115602ms step_avg:144.50ms
step:811/1375 train_time:115751ms step_avg:144.51ms
step:812/1375 train_time:115903ms step_avg:144.52ms
step:813/1375 train_time:116050ms step_avg:144.52ms
step:814/1375 train_time:116202ms step_avg:144.53ms
step:815/1375 train_time:116350ms step_avg:144.53ms
step:816/1375 train_time:116504ms step_avg:144.55ms
step:817/1375 train_time:116654ms step_avg:144.55ms
step:818/1375 train_time:116804ms step_avg:144.56ms
step:819/1375 train_time:116957ms step_avg:144.57ms
step:820/1375 train_time:117111ms step_avg:144.58ms
step:821/1375 train_time:117261ms step_avg:144.59ms
step:822/1375 train_time:117413ms step_avg:144.60ms
step:823/1375 train_time:117565ms step_avg:144.61ms
step:824/1375 train_time:117715ms step_avg:144.61ms
step:825/1375 train_time:117868ms step_avg:144.62ms
step:826/1375 train_time:118023ms step_avg:144.64ms
step:827/1375 train_time:118172ms step_avg:144.64ms
step:828/1375 train_time:118326ms step_avg:144.65ms
step:829/1375 train_time:118477ms step_avg:144.66ms
step:830/1375 train_time:118627ms step_avg:144.67ms
step:831/1375 train_time:118777ms step_avg:144.67ms
step:832/1375 train_time:118930ms step_avg:144.68ms
step:833/1375 train_time:119081ms step_avg:144.69ms
step:834/1375 train_time:119233ms step_avg:144.70ms
step:835/1375 train_time:119385ms step_avg:144.71ms
step:836/1375 train_time:119541ms step_avg:144.72ms
step:837/1375 train_time:119690ms step_avg:144.73ms
step:838/1375 train_time:119842ms step_avg:144.74ms
step:839/1375 train_time:119992ms step_avg:144.74ms
step:840/1375 train_time:120144ms step_avg:144.75ms
step:841/1375 train_time:120296ms step_avg:144.76ms
step:842/1375 train_time:120446ms step_avg:144.77ms
step:843/1375 train_time:120597ms step_avg:144.77ms
step:844/1375 train_time:120747ms step_avg:144.78ms
step:845/1375 train_time:120897ms step_avg:144.79ms
step:846/1375 train_time:121049ms step_avg:144.79ms
step:847/1375 train_time:121204ms step_avg:144.81ms
step:848/1375 train_time:121354ms step_avg:144.81ms
step:849/1375 train_time:121508ms step_avg:144.82ms
step:850/1375 train_time:121661ms step_avg:144.84ms
step:851/1375 train_time:121812ms step_avg:144.84ms
step:852/1375 train_time:121964ms step_avg:144.85ms
step:853/1375 train_time:122114ms step_avg:144.86ms
step:854/1375 train_time:122265ms step_avg:144.86ms
step:855/1375 train_time:122416ms step_avg:144.87ms
step:856/1375 train_time:122566ms step_avg:144.88ms
step:857/1375 train_time:122719ms step_avg:144.89ms
step:858/1375 train_time:122875ms step_avg:144.90ms
step:859/1375 train_time:123027ms step_avg:144.91ms
step:860/1375 train_time:123179ms step_avg:144.92ms
step:861/1375 train_time:123333ms step_avg:144.93ms
step:862/1375 train_time:123485ms step_avg:144.94ms
step:863/1375 train_time:123637ms step_avg:144.94ms
step:864/1375 train_time:123789ms step_avg:144.95ms
step:865/1375 train_time:123940ms step_avg:144.96ms
step:866/1375 train_time:124100ms step_avg:144.98ms
step:867/1375 train_time:124249ms step_avg:144.98ms
step:868/1375 train_time:124401ms step_avg:144.99ms
step:869/1375 train_time:124553ms step_avg:145.00ms
step:870/1375 train_time:124707ms step_avg:145.01ms
step:871/1375 train_time:124860ms step_avg:145.02ms
step:872/1375 train_time:125010ms step_avg:145.02ms
step:873/1375 train_time:125162ms step_avg:145.03ms
step:874/1375 train_time:125315ms step_avg:145.04ms
step:875/1375 train_time:125466ms step_avg:145.05ms
step:875/1375 val_loss:3.4671 train_time:125542ms step_avg:145.14ms
step:876/1375 train_time:125618ms step_avg:145.06ms
step:877/1375 train_time:125772ms step_avg:145.07ms
step:878/1375 train_time:125923ms step_avg:145.07ms
step:879/1375 train_time:126072ms step_avg:145.08ms
step:880/1375 train_time:126224ms step_avg:145.09ms
step:881/1375 train_time:126373ms step_avg:145.09ms
step:882/1375 train_time:126527ms step_avg:145.10ms
step:883/1375 train_time:126680ms step_avg:145.11ms
step:884/1375 train_time:126835ms step_avg:145.12ms
step:885/1375 train_time:126985ms step_avg:145.13ms
step:886/1375 train_time:127137ms step_avg:145.13ms
step:887/1375 train_time:127287ms step_avg:145.14ms
step:888/1375 train_time:127442ms step_avg:145.15ms
step:889/1375 train_time:127595ms step_avg:145.16ms
step:890/1375 train_time:127745ms step_avg:145.16ms
step:891/1375 train_time:127897ms step_avg:145.17ms
step:892/1375 train_time:128048ms step_avg:145.18ms
step:893/1375 train_time:128201ms step_avg:145.19ms
step:894/1375 train_time:128353ms step_avg:145.20ms
step:895/1375 train_time:128508ms step_avg:145.21ms
step:896/1375 train_time:128659ms step_avg:145.21ms
step:897/1375 train_time:128810ms step_avg:145.22ms
step:898/1375 train_time:128963ms step_avg:145.23ms
step:899/1375 train_time:129114ms step_avg:145.24ms
step:900/1375 train_time:129265ms step_avg:145.24ms
step:901/1375 train_time:129419ms step_avg:145.25ms
step:902/1375 train_time:129568ms step_avg:145.26ms
step:903/1375 train_time:129722ms step_avg:145.26ms
step:904/1375 train_time:129875ms step_avg:145.27ms
step:905/1375 train_time:130027ms step_avg:145.28ms
step:906/1375 train_time:130177ms step_avg:145.29ms
step:907/1375 train_time:130334ms step_avg:145.30ms
step:908/1375 train_time:130485ms step_avg:145.31ms
step:909/1375 train_time:130636ms step_avg:145.31ms
step:910/1375 train_time:130795ms step_avg:145.33ms
step:911/1375 train_time:130947ms step_avg:145.33ms
step:912/1375 train_time:131099ms step_avg:145.34ms
step:913/1375 train_time:131252ms step_avg:145.35ms
step:914/1375 train_time:131405ms step_avg:145.36ms
step:915/1375 train_time:131558ms step_avg:145.37ms
step:916/1375 train_time:131710ms step_avg:145.38ms
step:917/1375 train_time:131862ms step_avg:145.38ms
step:918/1375 train_time:132015ms step_avg:145.39ms
step:919/1375 train_time:132170ms step_avg:145.40ms
step:920/1375 train_time:132323ms step_avg:145.41ms
step:921/1375 train_time:132476ms step_avg:145.42ms
step:922/1375 train_time:132630ms step_avg:145.43ms
step:923/1375 train_time:132782ms step_avg:145.44ms
step:924/1375 train_time:132935ms step_avg:145.44ms
step:925/1375 train_time:133090ms step_avg:145.45ms
step:926/1375 train_time:133243ms step_avg:145.46ms
step:927/1375 train_time:133397ms step_avg:145.47ms
step:928/1375 train_time:133549ms step_avg:145.48ms
step:929/1375 train_time:133705ms step_avg:145.49ms
step:930/1375 train_time:133858ms step_avg:145.50ms
step:931/1375 train_time:134009ms step_avg:145.50ms
step:932/1375 train_time:134163ms step_avg:145.51ms
step:933/1375 train_time:134315ms step_avg:145.52ms
step:934/1375 train_time:134468ms step_avg:145.53ms
step:935/1375 train_time:134623ms step_avg:145.54ms
step:936/1375 train_time:134774ms step_avg:145.54ms
step:937/1375 train_time:134931ms step_avg:145.56ms
step:938/1375 train_time:135084ms step_avg:145.56ms
step:939/1375 train_time:135238ms step_avg:145.57ms
step:940/1375 train_time:135391ms step_avg:145.58ms
step:941/1375 train_time:135542ms step_avg:145.59ms
step:942/1375 train_time:135694ms step_avg:145.59ms
step:943/1375 train_time:135849ms step_avg:145.60ms
step:944/1375 train_time:136008ms step_avg:145.62ms
step:945/1375 train_time:136160ms step_avg:145.63ms
step:946/1375 train_time:136314ms step_avg:145.63ms
step:947/1375 train_time:136467ms step_avg:145.64ms
step:948/1375 train_time:136621ms step_avg:145.65ms
step:949/1375 train_time:136777ms step_avg:145.66ms
step:950/1375 train_time:136929ms step_avg:145.67ms
step:951/1375 train_time:137116ms step_avg:145.71ms
step:952/1375 train_time:137266ms step_avg:145.72ms
step:953/1375 train_time:137421ms step_avg:145.73ms
step:954/1375 train_time:137571ms step_avg:145.73ms
step:955/1375 train_time:137724ms step_avg:145.74ms
step:956/1375 train_time:137878ms step_avg:145.75ms
step:957/1375 train_time:138029ms step_avg:145.75ms
step:958/1375 train_time:138189ms step_avg:145.77ms
step:959/1375 train_time:138344ms step_avg:145.78ms
step:960/1375 train_time:138497ms step_avg:145.79ms
step:961/1375 train_time:138647ms step_avg:145.79ms
step:962/1375 train_time:138799ms step_avg:145.80ms
step:963/1375 train_time:138959ms step_avg:145.81ms
step:964/1375 train_time:139114ms step_avg:145.82ms
step:965/1375 train_time:139267ms step_avg:145.83ms
step:966/1375 train_time:139419ms step_avg:145.84ms
step:967/1375 train_time:139570ms step_avg:145.84ms
step:968/1375 train_time:139723ms step_avg:145.85ms
step:969/1375 train_time:139875ms step_avg:145.86ms
step:970/1375 train_time:140026ms step_avg:145.86ms
step:971/1375 train_time:140183ms step_avg:145.87ms
step:972/1375 train_time:140335ms step_avg:145.88ms
step:973/1375 train_time:140487ms step_avg:145.88ms
step:974/1375 train_time:140640ms step_avg:145.89ms
step:975/1375 train_time:140793ms step_avg:145.90ms
step:976/1375 train_time:140945ms step_avg:145.91ms
step:977/1375 train_time:141099ms step_avg:145.91ms
step:978/1375 train_time:141252ms step_avg:145.92ms
step:979/1375 train_time:141404ms step_avg:145.93ms
step:980/1375 train_time:141555ms step_avg:145.93ms
step:981/1375 train_time:141706ms step_avg:145.94ms
step:982/1375 train_time:141859ms step_avg:145.95ms
step:983/1375 train_time:142011ms step_avg:145.95ms
step:984/1375 train_time:142163ms step_avg:145.96ms
step:985/1375 train_time:142316ms step_avg:145.97ms
step:986/1375 train_time:142470ms step_avg:145.97ms
step:987/1375 train_time:142622ms step_avg:145.98ms
step:988/1375 train_time:142774ms step_avg:145.99ms
step:989/1375 train_time:142925ms step_avg:145.99ms
step:990/1375 train_time:143080ms step_avg:146.00ms
step:991/1375 train_time:143233ms step_avg:146.01ms
step:992/1375 train_time:143391ms step_avg:146.02ms
step:993/1375 train_time:143550ms step_avg:146.03ms
step:994/1375 train_time:143702ms step_avg:146.04ms
step:995/1375 train_time:143852ms step_avg:146.04ms
step:996/1375 train_time:144004ms step_avg:146.05ms
step:997/1375 train_time:144153ms step_avg:146.05ms
step:998/1375 train_time:144306ms step_avg:146.06ms
step:999/1375 train_time:144459ms step_avg:146.07ms
step:1000/1375 train_time:144613ms step_avg:146.07ms
step:1000/1375 val_loss:3.4025 train_time:144689ms step_avg:146.15ms
step:1001/1375 train_time:144766ms step_avg:146.08ms
step:1002/1375 train_time:144923ms step_avg:146.09ms
step:1003/1375 train_time:145075ms step_avg:146.10ms
step:1004/1375 train_time:145229ms step_avg:146.11ms
step:1005/1375 train_time:145380ms step_avg:146.11ms
step:1006/1375 train_time:145532ms step_avg:146.12ms
step:1007/1375 train_time:145685ms step_avg:146.12ms
step:1008/1375 train_time:145840ms step_avg:146.13ms
step:1009/1375 train_time:145999ms step_avg:146.14ms
step:1010/1375 train_time:146152ms step_avg:146.15ms
step:1011/1375 train_time:146305ms step_avg:146.16ms
step:1012/1375 train_time:146456ms step_avg:146.16ms
step:1013/1375 train_time:146611ms step_avg:146.17ms
step:1014/1375 train_time:146764ms step_avg:146.18ms
step:1015/1375 train_time:146916ms step_avg:146.19ms
step:1016/1375 train_time:147071ms step_avg:146.19ms
step:1017/1375 train_time:147223ms step_avg:146.20ms
step:1018/1375 train_time:147373ms step_avg:146.20ms
step:1019/1375 train_time:147527ms step_avg:146.21ms
step:1020/1375 train_time:147682ms step_avg:146.22ms
step:1021/1375 train_time:147835ms step_avg:146.23ms
step:1022/1375 train_time:147989ms step_avg:146.23ms
step:1023/1375 train_time:148145ms step_avg:146.24ms
step:1024/1375 train_time:148295ms step_avg:146.25ms
step:1025/1375 train_time:148450ms step_avg:146.26ms
step:1026/1375 train_time:148602ms step_avg:146.26ms
step:1027/1375 train_time:148755ms step_avg:146.27ms
step:1028/1375 train_time:148910ms step_avg:146.28ms
step:1029/1375 train_time:149068ms step_avg:146.29ms
step:1030/1375 train_time:149222ms step_avg:146.30ms
step:1031/1375 train_time:149373ms step_avg:146.30ms
step:1032/1375 train_time:149526ms step_avg:146.31ms
step:1033/1375 train_time:149680ms step_avg:146.31ms
step:1034/1375 train_time:149834ms step_avg:146.32ms
step:1035/1375 train_time:149990ms step_avg:146.33ms
step:1036/1375 train_time:150146ms step_avg:146.34ms
step:1037/1375 train_time:150301ms step_avg:146.35ms
step:1038/1375 train_time:150455ms step_avg:146.36ms
step:1039/1375 train_time:150608ms step_avg:146.36ms
step:1040/1375 train_time:150760ms step_avg:146.37ms
step:1041/1375 train_time:150915ms step_avg:146.38ms
step:1042/1375 train_time:151068ms step_avg:146.38ms
step:1043/1375 train_time:151221ms step_avg:146.39ms
step:1044/1375 train_time:151377ms step_avg:146.40ms
step:1045/1375 train_time:151533ms step_avg:146.41ms
step:1046/1375 train_time:151687ms step_avg:146.42ms
step:1047/1375 train_time:151841ms step_avg:146.42ms
step:1048/1375 train_time:151996ms step_avg:146.43ms
step:1049/1375 train_time:152151ms step_avg:146.44ms
step:1050/1375 train_time:152307ms step_avg:146.45ms
step:1051/1375 train_time:152464ms step_avg:146.46ms
step:1052/1375 train_time:152617ms step_avg:146.47ms
step:1053/1375 train_time:152770ms step_avg:146.47ms
step:1054/1375 train_time:152926ms step_avg:146.48ms
step:1055/1375 train_time:153080ms step_avg:146.49ms
step:1056/1375 train_time:153233ms step_avg:146.49ms
step:1057/1375 train_time:153387ms step_avg:146.50ms
step:1058/1375 train_time:153547ms step_avg:146.51ms
step:1059/1375 train_time:153700ms step_avg:146.52ms
step:1060/1375 train_time:153856ms step_avg:146.53ms
step:1061/1375 train_time:154007ms step_avg:146.53ms
step:1062/1375 train_time:154161ms step_avg:146.54ms
step:1063/1375 train_time:154314ms step_avg:146.55ms
step:1064/1375 train_time:154466ms step_avg:146.55ms
step:1065/1375 train_time:154620ms step_avg:146.56ms
step:1066/1375 train_time:154779ms step_avg:146.57ms
step:1067/1375 train_time:154936ms step_avg:146.58ms
step:1068/1375 train_time:155087ms step_avg:146.59ms
step:1069/1375 train_time:155247ms step_avg:146.60ms
step:1070/1375 train_time:155397ms step_avg:146.60ms
step:1071/1375 train_time:155554ms step_avg:146.61ms
step:1072/1375 train_time:155706ms step_avg:146.62ms
step:1073/1375 train_time:155858ms step_avg:146.62ms
step:1074/1375 train_time:156013ms step_avg:146.63ms
step:1075/1375 train_time:156166ms step_avg:146.64ms
step:1076/1375 train_time:156318ms step_avg:146.64ms
step:1077/1375 train_time:156472ms step_avg:146.65ms
step:1078/1375 train_time:156631ms step_avg:146.66ms
step:1079/1375 train_time:156788ms step_avg:146.67ms
step:1080/1375 train_time:156944ms step_avg:146.68ms
step:1081/1375 train_time:157097ms step_avg:146.68ms
step:1082/1375 train_time:157252ms step_avg:146.69ms
step:1083/1375 train_time:157406ms step_avg:146.70ms
step:1084/1375 train_time:157562ms step_avg:146.71ms
step:1085/1375 train_time:157714ms step_avg:146.71ms
step:1086/1375 train_time:157871ms step_avg:146.72ms
step:1087/1375 train_time:158025ms step_avg:146.73ms
step:1088/1375 train_time:158179ms step_avg:146.73ms
step:1089/1375 train_time:158339ms step_avg:146.75ms
step:1090/1375 train_time:158499ms step_avg:146.76ms
step:1091/1375 train_time:158655ms step_avg:146.77ms
step:1092/1375 train_time:158809ms step_avg:146.77ms
step:1093/1375 train_time:158962ms step_avg:146.78ms
step:1094/1375 train_time:159114ms step_avg:146.78ms
step:1095/1375 train_time:159267ms step_avg:146.79ms
step:1096/1375 train_time:159424ms step_avg:146.80ms
step:1097/1375 train_time:159578ms step_avg:146.81ms
step:1098/1375 train_time:159733ms step_avg:146.81ms
step:1099/1375 train_time:159884ms step_avg:146.82ms
step:1100/1375 train_time:160036ms step_avg:146.82ms
step:1101/1375 train_time:160189ms step_avg:146.83ms
step:1102/1375 train_time:160347ms step_avg:146.84ms
step:1103/1375 train_time:160501ms step_avg:146.84ms
step:1104/1375 train_time:160655ms step_avg:146.85ms
step:1105/1375 train_time:160811ms step_avg:146.86ms
step:1106/1375 train_time:160965ms step_avg:146.87ms
step:1107/1375 train_time:161119ms step_avg:146.87ms
step:1108/1375 train_time:161274ms step_avg:146.88ms
step:1109/1375 train_time:161426ms step_avg:146.88ms
step:1110/1375 train_time:161582ms step_avg:146.89ms
step:1111/1375 train_time:161737ms step_avg:146.90ms
step:1112/1375 train_time:161890ms step_avg:146.91ms
step:1113/1375 train_time:162045ms step_avg:146.91ms
step:1114/1375 train_time:162200ms step_avg:146.92ms
step:1115/1375 train_time:162355ms step_avg:146.93ms
step:1116/1375 train_time:162508ms step_avg:146.93ms
step:1117/1375 train_time:162665ms step_avg:146.94ms
step:1118/1375 train_time:162824ms step_avg:146.95ms
step:1119/1375 train_time:162978ms step_avg:146.96ms
step:1120/1375 train_time:163133ms step_avg:146.97ms
step:1121/1375 train_time:163286ms step_avg:146.97ms
step:1122/1375 train_time:163439ms step_avg:146.98ms
step:1123/1375 train_time:163593ms step_avg:146.98ms
step:1124/1375 train_time:163753ms step_avg:147.00ms
step:1125/1375 train_time:163909ms step_avg:147.00ms
step:1125/1375 val_loss:3.3490 train_time:163986ms step_avg:147.07ms
step:1126/1375 train_time:164063ms step_avg:147.01ms
step:1127/1375 train_time:164220ms step_avg:147.02ms
step:1128/1375 train_time:164374ms step_avg:147.02ms
step:1129/1375 train_time:164532ms step_avg:147.03ms
step:1130/1375 train_time:164687ms step_avg:147.04ms
step:1131/1375 train_time:164843ms step_avg:147.05ms
step:1132/1375 train_time:164995ms step_avg:147.05ms
step:1133/1375 train_time:165152ms step_avg:147.06ms
step:1134/1375 train_time:165309ms step_avg:147.07ms
step:1135/1375 train_time:165462ms step_avg:147.08ms
step:1136/1375 train_time:165623ms step_avg:147.09ms
step:1137/1375 train_time:165776ms step_avg:147.10ms
step:1138/1375 train_time:165929ms step_avg:147.10ms
step:1139/1375 train_time:166085ms step_avg:147.11ms
step:1140/1375 train_time:166240ms step_avg:147.11ms
step:1141/1375 train_time:166439ms step_avg:147.16ms
step:1142/1375 train_time:166593ms step_avg:147.17ms
step:1143/1375 train_time:166750ms step_avg:147.18ms
step:1144/1375 train_time:166907ms step_avg:147.18ms
step:1145/1375 train_time:167059ms step_avg:147.19ms
step:1146/1375 train_time:167217ms step_avg:147.20ms
step:1147/1375 train_time:167371ms step_avg:147.20ms
step:1148/1375 train_time:167527ms step_avg:147.21ms
step:1149/1375 train_time:167682ms step_avg:147.22ms
step:1150/1375 train_time:167835ms step_avg:147.22ms
step:1151/1375 train_time:167990ms step_avg:147.23ms
step:1152/1375 train_time:168146ms step_avg:147.24ms
step:1153/1375 train_time:168303ms step_avg:147.25ms
step:1154/1375 train_time:168457ms step_avg:147.25ms
step:1155/1375 train_time:168610ms step_avg:147.26ms
step:1156/1375 train_time:168771ms step_avg:147.27ms
step:1157/1375 train_time:168927ms step_avg:147.28ms
step:1158/1375 train_time:169082ms step_avg:147.28ms
step:1159/1375 train_time:169235ms step_avg:147.29ms
step:1160/1375 train_time:169389ms step_avg:147.29ms
step:1161/1375 train_time:169544ms step_avg:147.30ms
step:1162/1375 train_time:169702ms step_avg:147.31ms
step:1163/1375 train_time:169857ms step_avg:147.32ms
step:1164/1375 train_time:170011ms step_avg:147.32ms
step:1165/1375 train_time:170164ms step_avg:147.33ms
step:1166/1375 train_time:170319ms step_avg:147.33ms
step:1167/1375 train_time:170472ms step_avg:147.34ms
step:1168/1375 train_time:170626ms step_avg:147.35ms
step:1169/1375 train_time:170782ms step_avg:147.35ms
step:1170/1375 train_time:170937ms step_avg:147.36ms
step:1171/1375 train_time:171093ms step_avg:147.37ms
step:1172/1375 train_time:171248ms step_avg:147.37ms
step:1173/1375 train_time:171401ms step_avg:147.38ms
step:1174/1375 train_time:171563ms step_avg:147.39ms
step:1175/1375 train_time:171720ms step_avg:147.40ms
step:1176/1375 train_time:171880ms step_avg:147.41ms
step:1177/1375 train_time:172039ms step_avg:147.42ms
step:1178/1375 train_time:172195ms step_avg:147.43ms
step:1179/1375 train_time:172349ms step_avg:147.43ms
step:1180/1375 train_time:172512ms step_avg:147.45ms
step:1181/1375 train_time:172669ms step_avg:147.45ms
step:1182/1375 train_time:172820ms step_avg:147.46ms
step:1183/1375 train_time:172975ms step_avg:147.46ms
step:1184/1375 train_time:173130ms step_avg:147.47ms
step:1185/1375 train_time:173290ms step_avg:147.48ms
step:1186/1375 train_time:173445ms step_avg:147.49ms
step:1187/1375 train_time:173608ms step_avg:147.50ms
step:1188/1375 train_time:173761ms step_avg:147.51ms
step:1189/1375 train_time:173918ms step_avg:147.51ms
step:1190/1375 train_time:174076ms step_avg:147.52ms
step:1191/1375 train_time:174233ms step_avg:147.53ms
step:1192/1375 train_time:174387ms step_avg:147.54ms
step:1193/1375 train_time:174540ms step_avg:147.54ms
step:1194/1375 train_time:174695ms step_avg:147.55ms
step:1195/1375 train_time:174849ms step_avg:147.55ms
step:1196/1375 train_time:175005ms step_avg:147.56ms
step:1197/1375 train_time:175163ms step_avg:147.57ms
step:1198/1375 train_time:175321ms step_avg:147.58ms
step:1199/1375 train_time:175475ms step_avg:147.58ms
step:1200/1375 train_time:175629ms step_avg:147.59ms
step:1201/1375 train_time:175785ms step_avg:147.59ms
step:1202/1375 train_time:175951ms step_avg:147.61ms
step:1203/1375 train_time:176111ms step_avg:147.62ms
step:1204/1375 train_time:176267ms step_avg:147.63ms
step:1205/1375 train_time:176421ms step_avg:147.63ms
step:1206/1375 train_time:176575ms step_avg:147.64ms
step:1207/1375 train_time:176730ms step_avg:147.64ms
step:1208/1375 train_time:176886ms step_avg:147.65ms
step:1209/1375 train_time:177041ms step_avg:147.66ms
step:1210/1375 train_time:177200ms step_avg:147.67ms
step:1211/1375 train_time:177357ms step_avg:147.67ms
step:1212/1375 train_time:177510ms step_avg:147.68ms
step:1213/1375 train_time:177665ms step_avg:147.68ms
step:1214/1375 train_time:177821ms step_avg:147.69ms
step:1215/1375 train_time:177975ms step_avg:147.70ms
step:1216/1375 train_time:178128ms step_avg:147.70ms
step:1217/1375 train_time:178283ms step_avg:147.71ms
step:1218/1375 train_time:178436ms step_avg:147.71ms
step:1219/1375 train_time:178590ms step_avg:147.72ms
step:1220/1375 train_time:178745ms step_avg:147.72ms
step:1221/1375 train_time:178898ms step_avg:147.73ms
step:1222/1375 train_time:179055ms step_avg:147.73ms
step:1223/1375 train_time:179212ms step_avg:147.74ms
step:1224/1375 train_time:179368ms step_avg:147.75ms
step:1225/1375 train_time:179526ms step_avg:147.76ms
step:1226/1375 train_time:179682ms step_avg:147.76ms
step:1227/1375 train_time:179840ms step_avg:147.77ms
step:1228/1375 train_time:179993ms step_avg:147.78ms
step:1229/1375 train_time:180148ms step_avg:147.78ms
step:1230/1375 train_time:180309ms step_avg:147.79ms
step:1231/1375 train_time:180466ms step_avg:147.80ms
step:1232/1375 train_time:180626ms step_avg:147.81ms
step:1233/1375 train_time:180782ms step_avg:147.82ms
step:1234/1375 train_time:180936ms step_avg:147.82ms
step:1235/1375 train_time:181093ms step_avg:147.83ms
step:1236/1375 train_time:181248ms step_avg:147.84ms
step:1237/1375 train_time:181403ms step_avg:147.84ms
step:1238/1375 train_time:181567ms step_avg:147.86ms
step:1239/1375 train_time:181722ms step_avg:147.86ms
step:1240/1375 train_time:181881ms step_avg:147.87ms
step:1241/1375 train_time:182044ms step_avg:147.88ms
step:1242/1375 train_time:182202ms step_avg:147.89ms
step:1243/1375 train_time:182361ms step_avg:147.90ms
step:1244/1375 train_time:182514ms step_avg:147.90ms
step:1245/1375 train_time:182669ms step_avg:147.91ms
step:1246/1375 train_time:182823ms step_avg:147.91ms
step:1247/1375 train_time:182982ms step_avg:147.92ms
step:1248/1375 train_time:183137ms step_avg:147.93ms
step:1249/1375 train_time:183291ms step_avg:147.93ms
step:1250/1375 train_time:183446ms step_avg:147.94ms
step:1250/1375 val_loss:3.3040 train_time:183526ms step_avg:148.00ms
step:1251/1375 train_time:183605ms step_avg:147.95ms
step:1252/1375 train_time:183760ms step_avg:147.95ms
step:1253/1375 train_time:183916ms step_avg:147.96ms
step:1254/1375 train_time:184068ms step_avg:147.96ms
step:1255/1375 train_time:184238ms step_avg:147.98ms
step:1256/1375 train_time:184392ms step_avg:147.99ms
step:1257/1375 train_time:184546ms step_avg:147.99ms
step:1258/1375 train_time:184705ms step_avg:148.00ms
step:1259/1375 train_time:184863ms step_avg:148.01ms
step:1260/1375 train_time:185017ms step_avg:148.01ms
step:1261/1375 train_time:185175ms step_avg:148.02ms
step:1262/1375 train_time:185333ms step_avg:148.03ms
step:1263/1375 train_time:185490ms step_avg:148.04ms
step:1264/1375 train_time:185644ms step_avg:148.04ms
step:1265/1375 train_time:185801ms step_avg:148.05ms
step:1266/1375 train_time:185956ms step_avg:148.05ms
step:1267/1375 train_time:186112ms step_avg:148.06ms
step:1268/1375 train_time:186266ms step_avg:148.07ms
step:1269/1375 train_time:186428ms step_avg:148.08ms
step:1270/1375 train_time:186583ms step_avg:148.08ms
step:1271/1375 train_time:186739ms step_avg:148.09ms
step:1272/1375 train_time:186894ms step_avg:148.09ms
step:1273/1375 train_time:187048ms step_avg:148.10ms
step:1274/1375 train_time:187204ms step_avg:148.10ms
step:1275/1375 train_time:187359ms step_avg:148.11ms
step:1276/1375 train_time:187515ms step_avg:148.12ms
step:1277/1375 train_time:187672ms step_avg:148.12ms
step:1278/1375 train_time:187825ms step_avg:148.13ms
step:1279/1375 train_time:187982ms step_avg:148.13ms
step:1280/1375 train_time:188143ms step_avg:148.14ms
step:1281/1375 train_time:188299ms step_avg:148.15ms
step:1282/1375 train_time:188452ms step_avg:148.15ms
step:1283/1375 train_time:188609ms step_avg:148.16ms
step:1284/1375 train_time:188768ms step_avg:148.17ms
step:1285/1375 train_time:188924ms step_avg:148.18ms
step:1286/1375 train_time:189080ms step_avg:148.18ms
step:1287/1375 train_time:189234ms step_avg:148.19ms
step:1288/1375 train_time:189388ms step_avg:148.19ms
step:1289/1375 train_time:189549ms step_avg:148.20ms
step:1290/1375 train_time:189709ms step_avg:148.21ms
step:1291/1375 train_time:189871ms step_avg:148.22ms
step:1292/1375 train_time:190027ms step_avg:148.23ms
step:1293/1375 train_time:190186ms step_avg:148.24ms
step:1294/1375 train_time:190341ms step_avg:148.24ms
step:1295/1375 train_time:190496ms step_avg:148.25ms
step:1296/1375 train_time:190652ms step_avg:148.25ms
step:1297/1375 train_time:190812ms step_avg:148.26ms
step:1298/1375 train_time:190968ms step_avg:148.27ms
step:1299/1375 train_time:191123ms step_avg:148.27ms
step:1300/1375 train_time:191279ms step_avg:148.28ms
step:1301/1375 train_time:191433ms step_avg:148.28ms
step:1302/1375 train_time:191589ms step_avg:148.29ms
step:1303/1375 train_time:191747ms step_avg:148.30ms
step:1304/1375 train_time:191906ms step_avg:148.30ms
step:1305/1375 train_time:192061ms step_avg:148.31ms
step:1306/1375 train_time:192219ms step_avg:148.32ms
step:1307/1375 train_time:192372ms step_avg:148.32ms
step:1308/1375 train_time:192528ms step_avg:148.33ms
step:1309/1375 train_time:192683ms step_avg:148.33ms
step:1310/1375 train_time:192838ms step_avg:148.34ms
step:1311/1375 train_time:192991ms step_avg:148.34ms
step:1312/1375 train_time:193143ms step_avg:148.34ms
step:1313/1375 train_time:193299ms step_avg:148.35ms
step:1314/1375 train_time:193455ms step_avg:148.35ms
step:1315/1375 train_time:193612ms step_avg:148.36ms
step:1316/1375 train_time:193765ms step_avg:148.37ms
step:1317/1375 train_time:193919ms step_avg:148.37ms
step:1318/1375 train_time:194082ms step_avg:148.38ms
step:1319/1375 train_time:194239ms step_avg:148.39ms
step:1320/1375 train_time:194394ms step_avg:148.39ms
step:1321/1375 train_time:194550ms step_avg:148.40ms
step:1322/1375 train_time:194712ms step_avg:148.41ms
step:1323/1375 train_time:194867ms step_avg:148.41ms
step:1324/1375 train_time:195022ms step_avg:148.42ms
step:1325/1375 train_time:195178ms step_avg:148.42ms
step:1326/1375 train_time:195339ms step_avg:148.43ms
step:1327/1375 train_time:195493ms step_avg:148.44ms
step:1328/1375 train_time:195649ms step_avg:148.44ms
step:1329/1375 train_time:195824ms step_avg:148.46ms
step:1330/1375 train_time:195983ms step_avg:148.47ms
step:1331/1375 train_time:196180ms step_avg:148.51ms
step:1332/1375 train_time:196341ms step_avg:148.52ms
step:1333/1375 train_time:196499ms step_avg:148.53ms
step:1334/1375 train_time:196655ms step_avg:148.53ms
step:1335/1375 train_time:196807ms step_avg:148.53ms
step:1336/1375 train_time:196974ms step_avg:148.55ms
step:1337/1375 train_time:197131ms step_avg:148.55ms
step:1338/1375 train_time:197288ms step_avg:148.56ms
step:1339/1375 train_time:197445ms step_avg:148.57ms
step:1340/1375 train_time:197603ms step_avg:148.57ms
step:1341/1375 train_time:197757ms step_avg:148.58ms
step:1342/1375 train_time:197918ms step_avg:148.59ms
step:1343/1375 train_time:198072ms step_avg:148.59ms
step:1344/1375 train_time:198226ms step_avg:148.60ms
step:1345/1375 train_time:198383ms step_avg:148.60ms
step:1346/1375 train_time:198539ms step_avg:148.61ms
step:1347/1375 train_time:198698ms step_avg:148.61ms
step:1348/1375 train_time:198854ms step_avg:148.62ms
step:1349/1375 train_time:199011ms step_avg:148.63ms
step:1350/1375 train_time:199164ms step_avg:148.63ms
step:1351/1375 train_time:199322ms step_avg:148.64ms
step:1352/1375 train_time:199484ms step_avg:148.65ms
step:1353/1375 train_time:199643ms step_avg:148.65ms
step:1354/1375 train_time:199802ms step_avg:148.66ms
step:1355/1375 train_time:199958ms step_avg:148.67ms
step:1356/1375 train_time:200115ms step_avg:148.67ms
step:1357/1375 train_time:200274ms step_avg:148.68ms
step:1358/1375 train_time:200437ms step_avg:148.69ms
step:1359/1375 train_time:200595ms step_avg:148.70ms
step:1360/1375 train_time:200755ms step_avg:148.71ms
step:1361/1375 train_time:200913ms step_avg:148.71ms
step:1362/1375 train_time:201072ms step_avg:148.72ms
step:1363/1375 train_time:201235ms step_avg:148.73ms
step:1364/1375 train_time:201389ms step_avg:148.74ms
step:1365/1375 train_time:201542ms step_avg:148.74ms
step:1366/1375 train_time:201700ms step_avg:148.75ms
step:1367/1375 train_time:201857ms step_avg:148.75ms
step:1368/1375 train_time:202014ms step_avg:148.76ms
step:1369/1375 train_time:202177ms step_avg:148.77ms
step:1370/1375 train_time:202336ms step_avg:148.78ms
step:1371/1375 train_time:202490ms step_avg:148.78ms
step:1372/1375 train_time:202653ms step_avg:148.79ms
step:1373/1375 train_time:202808ms step_avg:148.80ms
step:1374/1375 train_time:202967ms step_avg:148.80ms
step:1375/1375 train_time:203123ms step_avg:148.81ms
step:1375/1375 val_loss:3.2786 train_time:203198ms step_avg:148.86ms
peak memory consumption: 31565 MiB

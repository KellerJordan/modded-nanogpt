import os
import sys
with open(sys.argv[0]) as f:
    code = f.read() # read the code of this file ASAP, for logging
import uuid
import time
import glob
import subprocess
import contextlib
from dataclasses import dataclass

import torch
torch.empty(1, device='cuda', requires_grad=True).backward()
from torch import nn
import torch.nn.functional as F
import torch.distributed as dist
from torch.nn.parallel import DistributedDataParallel as DDP
# use of FlexAttention contributed by @KoszarskyB
from torch.nn.attention.flex_attention import BlockMask, flex_attention

# -----------------------------------------------------------------------------
# Muon optimizer

@torch.compile
def zeropower_via_newtonschulz5(G, steps):
    """
    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a
    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose
    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at
    zero even beyond the point where the iteration no longer converges all the way to one everywhere
    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T
    where S' is diagonal with S_{ii}' ~ Uniform(0.5, 1.5), which turns out not to hurt model
    performance at all relative to UV^T, where USV^T = G is the SVD.
    """
    assert len(G.shape) == 2
    a, b, c = (3.4445, -4.7750,  2.0315)
    X = G.bfloat16()
    if G.size(0) > G.size(1):
        X = X.T

    # Ensure spectral norm is at most 1
    X = X / (X.norm() + 1e-7)
    # Perform the NS iterations
    for _ in range(steps):
        A = X @ X.T
        B = b * A + c * A @ A # adapted from suggestion by @jxbz, @leloykun, and @YouJiacheng
        X = a * X + B @ X
    
    if G.size(0) > G.size(1):
        X = X.T
    return X

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Some warnings:
    - This optimizer assumes that all parameters passed in are 2D.
    - It should not be used for the embedding layer, the final fully connected layer, or any {0,1}-D
    parameters; those should all be optimized by a standard method (e.g., AdamW).
    - To use it with 4D convolutional filters, it works well to just flatten their last 3 dimensions.
    - We believe it is unlikely to work well for training with small batch size.
    - We believe it may not work well for finetuning pretrained models, but we haven't tested this.
    - We have not yet tried this optimizer for training scenarios larger than NanoGPT (124M).

    Arguments:
        lr: The learning rate used by the internal SGD.
        momentum: The momentum used by the internal SGD.
        nesterov: Whether to use Nesterov-style momentum in the internal SGD. (recommended)
        ns_steps: The number of Newton-Schulz iteration steps to use.
    """
    def __init__(self, params, lr=0.02, momentum=0.95, nesterov=True, ns_steps=5):
        self.world_size = int(os.environ['WORLD_SIZE'])
        self.rank = int(os.environ['RANK'])
        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, ns_steps=ns_steps)
        assert all(isinstance(p, torch.Tensor) for p in params)
        sizes = {p.numel() for p in params}
        param_groups = [dict(params=[p for p in params if p.numel() == size],
                             update_buffer=[torch.empty(size, device='cuda', dtype=torch.bfloat16) for _ in range(self.world_size)])
                        for size in sizes]
        super().__init__(param_groups, defaults)

    def step(self):

        for group in self.param_groups:

            lr = group['lr']
            momentum = group['momentum']
            nesterov = group['nesterov']
            ns_steps = group['ns_steps']
            update_buffers = group['update_buffer']
            # generate weight updates in distributed fashion
            params = group['params']
            handle = None
            params_world = None
            def update_prev():
                if params_world is None:
                    return
                assert handle is not None
                handle.wait()
                for p_world, g_world in zip(params_world, update_buffers):
                    p_world.data.add_(
                        g_world.view_as(p_world),
                        alpha=-lr * max(1, p_world.size(0) / p_world.size(1)) ** 0.5,
                    )
            for base_i in range(len(params))[::self.world_size]:
                if base_i + rank < len(params):
                    p = params[base_i + self.rank]
                    g = p.grad
                    assert g is not None
                    state = self.state[p]
                    if 'momentum_buffer' not in state:
                        state['momentum_buffer'] = torch.zeros_like(g)
                    buf = state['momentum_buffer']
                    buf.lerp_(g, 1 - momentum)
                    g = g.lerp_(buf, momentum) if nesterov else buf
                    g = zeropower_via_newtonschulz5(g, steps=ns_steps).flatten()
                else:
                    g = update_buffers[rank]
                update_prev() # async all_gather instead of sync all_reduce by @YouJiacheng
                handle = dist.all_gather(update_buffers, g, async_op=True)
                params_world = params[base_i : base_i + self.world_size]
            update_prev()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the GPT-2 model

def norm(x):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):

    def __init__(self, in_features, out_features):
        super().__init__(in_features, out_features, bias=False)

    def forward(self, x):
        return F.linear(x, self.weight.type_as(x))

class Rotary(nn.Module):

    def __init__(self, dim, max_seq_len=65536):
        super().__init__()
        # half-truncate RoPE by @YouJiacheng
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=dim//4, dtype=torch.float32)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(dim//4)])
        t = torch.arange(max_seq_len, dtype=torch.float32)
        theta = torch.einsum('i,j -> ij', t, angular_freq)
        self.cos = nn.Buffer(theta.cos(), persistent=False)
        self.sin = nn.Buffer(theta.sin(), persistent=False)

    def forward(self, x):
        cos, sin = self.cos[None, :x.size(-3), None, :], self.sin[None, :x.size(-3), None, :]
        x1, x2 = x.float().chunk(2, dim=-1)
        y1 = x1 * cos + x2 * sin
        y2 = x1 * (-sin) + x2 * cos
        return torch.cat((y1, y2), 3).type_as(x)

class CausalSelfAttention(nn.Module):

    def __init__(self, dim, num_heads, layer_id):
        super().__init__()
        assert dim % num_heads == 0
        self.num_heads = num_heads
        self.layer_id = layer_id
        self.c_q = CastedLinear(dim, dim)
        self.c_k = CastedLinear(dim, dim)
        self.c_v = CastedLinear(dim, dim)
        self.lambdas = nn.Parameter(torch.tensor([0.5, 0.5]))
        self.rotary = Rotary(dim // num_heads) # dim // num_heads = head_dim
        self.c_proj = CastedLinear(dim, dim)
        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977
        # self.attn_scale = nn.Parameter(torch.tensor(1.0 / (dim // num_heads) ** 0.5))
        # self.attn_scale = nn.Parameter(torch.tensor(0.20))
        self.attn_scale = 0.13 + 0.01 * min(layer_id, 11 - layer_id)

    def forward(self, x, ve, block_mask):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, 'Must use batch size = 1 for FlexAttention'
        q = self.c_q(x).view(B, T, self.num_heads, -1)
        k = self.c_k(x).view(B, T, self.num_heads, -1)
        v = self.c_v(x).view(B, T, self.num_heads, -1)
        if ve is not None:
            v = self.lambdas[0] * v + self.lambdas[1] * ve.view_as(v) # @KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = self.lambdas[0] * v
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = self.rotary(q), self.rotary(k)
        # y = flex_attention(q.transpose(1, 2) * self.attn_scale, k.transpose(1, 2), v.transpose(1, 2), block_mask=block_mask, scale=1.)
        y = flex_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), block_mask=block_mask, scale=self.attn_scale)
        y = y.transpose(1, 2).contiguous().view_as(x) # re-assemble all head outputs side by side
        y = self.c_proj(y)
        return y

class MLP(nn.Module):

    def __init__(self, dim):
        super().__init__()
        self.c_fc = CastedLinear(dim, 4 * dim)
        self.c_proj = CastedLinear(4 * dim, dim)
        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977

    def forward(self, x):
        x = self.c_fc(x)
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = self.c_proj(x)
        return x

class Block(nn.Module):

    def __init__(self, model_dim, num_heads, layer_id, use_attn=True):
        super().__init__()
        self.attn = CausalSelfAttention(model_dim, num_heads, layer_id) if use_attn else None
        self.mlp = MLP(model_dim)
        self.lambdas = nn.Parameter(torch.tensor([1., 0.]))

    def forward(self, x, ve, x0, block_mask):
        x = self.lambdas[0] * x + self.lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), ve, block_mask)
        x = x + self.mlp(norm(x))
        return x

class ValueEmbedding(nn.Module):
    def __init__(self, vocab_size, model_dim):
        super().__init__()
        self.embed = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])

    def forward(self, inputs):
        ve = [emb(inputs).bfloat16() for emb in self.embed]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        ve = [ve[0], ve[1], ve[2], None, None, None, None, None, None, ve[0], ve[1], ve[2]]
        return ve

# -----------------------------------------------------------------------------
# The main GPT-2 model

class GPT(nn.Module):

    def __init__(self, vocab_size, num_layers, num_heads, model_dim):
        super().__init__()
        self.embed = nn.Embedding(vocab_size, model_dim)
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.blocks = nn.ModuleList([Block(model_dim, num_heads, layer_id=layer_id, use_attn=(layer_id != 7))
                                     for layer_id in range(num_layers)])
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual learning
        # U-net structure on token value embeddings by @leloykun
        self.value_embeds = ValueEmbedding(vocab_size, model_dim)
        self.lm_head = CastedLinear(model_dim, vocab_size)
        self.lm_head.weight.data.zero_() # @Grad62304977
        # U-net design by @brendanh0gan
        self.num_encoder_layers = num_layers // 2 # Half of the layers for encoder
        self.num_decoder_layers = num_layers - self.num_encoder_layers # Remaining for decoder
        # Add learnable skip connection weights for decoder layers
        self.skip_weights = nn.Parameter(torch.ones(self.num_decoder_layers))

    def forward(self, inputs, targets, sliding_window_num_blocks):
        BLOCK_SIZE = 128
        seq_len = len(inputs)
        assert seq_len % BLOCK_SIZE == 0
        total_num_blocks = seq_len // BLOCK_SIZE
        assert inputs.ndim == 1
        docs = (inputs == 50256).cumsum(0)
        docs_low = docs.view(-1, BLOCK_SIZE)[:, 0].contiguous()
        docs_high = docs.view(-1, BLOCK_SIZE)[:, -1].contiguous()

        def document_causal(b, h, q_idx, kv_idx):
            causal_mask = q_idx >= kv_idx
            document_mask = docs[q_idx] == docs[kv_idx]
            return causal_mask & document_mask

        def dense_to_ordered(dense_mask):
            num_blocks = dense_mask.sum(dim=-1, dtype=torch.int32)
            indices = dense_mask.argsort(dim=-1, descending=True, stable=True).to(torch.int32)
            return num_blocks[None, None].contiguous(), indices[None, None].contiguous()

        def create_doc_swc_block_mask(sliding_window_num_blocks):
            kv_idx = block_idx = torch.arange(total_num_blocks, dtype=torch.int32, device='cuda')
            q_idx = block_idx[:, None]
            causal_bm = q_idx >= kv_idx
            causal_full_bm = q_idx > kv_idx
            window_bm = q_idx - kv_idx < sliding_window_num_blocks
            window_full_bm = window_bm # block-wise sliding window by @YouJiacheng
            # document_bm = (docs_low[q_idx] <= docs_high[kv_idx]) & (docs_low[kv_idx] <= docs_high[q_idx])
            document_bm = (docs_low[:, None] <= docs_high) & (docs_low <= docs_high[:, None])
            document_full_bm = (docs_low[:, None] == docs_high) & (docs_low == docs_high[:, None])
            nonzero_bm = causal_bm & window_bm & document_bm
            full_bm  = causal_full_bm & window_full_bm & document_full_bm
            kv_num_blocks, kv_indices = dense_to_ordered(nonzero_bm & ~full_bm)
            full_kv_num_blocks, full_kv_indices = dense_to_ordered(full_bm)
            return BlockMask.from_kv_blocks(
                kv_num_blocks,
                kv_indices,
                full_kv_num_blocks,
                full_kv_indices,
                BLOCK_SIZE=BLOCK_SIZE,
                mask_mod=document_causal,
            )

        block_mask = create_doc_swc_block_mask(sliding_window_num_blocks)

        x0 = norm(self.embed(inputs[None]).bfloat16()) # use of norm here by @Grad62304977
        x = x0
        ve = self.value_embeds(inputs)
        assert len(ve) == len(self.blocks)
        ve_enc, ve_dec = ve[:self.num_encoder_layers], ve[self.num_encoder_layers:]

        # Store outputs for U-Net skip connections
        skip_connections = []
        # Encoder pass - process only the first half of the blocks
        for i in range(self.num_encoder_layers):
            x = self.blocks[i](x, ve_enc[i], x0, block_mask)
            skip_connections.append(x)
        # Decoder pass - process the remaining blocks with weighted skip connections
        for i in range(self.num_decoder_layers):
            x = x + self.skip_weights[i] * skip_connections.pop()
            # U-net structure on token value embeddings by @leloykun
            x = self.blocks[self.num_encoder_layers + i](x, ve_dec[i], x0, block_mask)

        x = norm(x)
        logits = self.lm_head(x)
        logits = 15 * torch.tanh(logits / 15) # @Grad62304977 added tanh softcapping, @KoszarskyB reduced it from 30 to 15
        logits = logits.float()
        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets)
        return loss

# -----------------------------------------------------------------------------
# Our own simple Distributed Data Loader

def _load_data_shard(path):
    # only reads the header, returns header data
    # header is 256 int32
    header = torch.from_file(path, False, 256, dtype=torch.int32)
    assert header[0] == 20240520, 'magic number mismatch in the data .bin file'
    assert header[1] == 1, 'unsupported version'
    num_tokens = int(header[2]) # number of tokens (claimed)
    with open(path, 'rb', buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, 'number of tokens read does not match header'
    return tokens

class DistributedDataLoader:

    def __init__(self, filename_pattern):
        self.rank = int(os.environ['RANK'])
        self.world_size = int(os.environ['WORLD_SIZE'])
        self.files = sorted(glob.glob(filename_pattern))
        self.reset()

    def reset(self):
        self.current_shard = -1
        self.advance()

    def advance(self):
        self.current_shard = (self.current_shard + 1) % len(self.files)
        self.current_position = 0
        self.tokens = _load_data_shard(self.files[self.current_shard])

    def next_batch(self, batch_size):
        assert batch_size % self.world_size == 0
        device_batch_size = batch_size // self.world_size
        # load next shard if necessary
        if self.current_position + batch_size + 1 >= len(self.tokens):
            self.advance()
        pos = self.current_position + self.rank * device_batch_size
        device_batch_tokens = self.tokens[pos:pos+device_batch_size+1]
        # advance current position
        self.current_position += batch_size
        inputs = device_batch_tokens[:-1].to(device='cuda', dtype=torch.int32, non_blocking=True)
        targets = device_batch_tokens[1:].to(device='cuda', dtype=torch.int64, non_blocking=True)
        return inputs, targets

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files = 'data/fineweb10B/fineweb_train_*.bin' # input .bin to train on
    val_files = 'data/fineweb10B/fineweb_val_*.bin' # input .bin to eval validation loss on
    val_tokens = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # optimization
    batch_size = 8*64*1024 # batch size in tokens
    num_iterations = 1370 # number of iterations to run
    cooldown_frac = 0.4 # fraction of training spent cooling down the learning rate
    bf16_embeds = True
    # evaluation and logging
    val_loss_every = 125 # every how many steps to evaluate val loss? 0 for only at the end
    # implementation
    max_device_batch_size = 64*1024 # batch size per device in tokens
    save_checkpoint = False
args = Hyperparameters()

micro_bs = args.max_device_batch_size

# set up DDP (distributed data parallel). torchrun sets this env variable
rank = int(os.environ['RANK'])
local_rank = int(os.environ['LOCAL_RANK'])
world_size = int(os.environ['WORLD_SIZE'])
assert torch.cuda.is_available()
torch.cuda.set_device(local_rank)
dist.init_process_group(backend='nccl', device_id=torch.device(local_rank))
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = uuid.uuid4()
    os.makedirs('logs', exist_ok=True)
    logfile = f'logs/{run_id}.txt'
    print(logfile)

def print0(s, console=False):
    if master_process:
        with open(logfile, 'a') as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0('='*100)
# log information about the hardware/software environment this is running on
print0(f'Running Python {sys.version}')
print0(f'Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}')
print0(subprocess.run(['nvidia-smi'], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout)
print0('='*100)

# load data
train_loader = DistributedDataLoader(args.train_files)
val_loader = DistributedDataLoader(args.val_files)
print0(f'Training dataloader files: {train_loader.files}')
print0(f'Validation dataloader files: {val_loader.files}')
print0('='*100)

# there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency. suggested to me by @Grad62304977.
# this originates from Karpathy's experiments.
model = GPT(vocab_size=50304, num_layers=12, num_heads=6, model_dim=768)
model = model.cuda()
if args.bf16_embeds:
    for m in model.modules():
        if isinstance(m, nn.Embedding):
            m.bfloat16()
model: GPT = torch.compile(model)
ddp_model = DDP(model, device_ids=[local_rank], broadcast_buffers=False, gradient_as_bucket_view=True)

# collect the parameters to optimize
hidden_matrix_params = [p for p in model.blocks.parameters() if p.ndim == 2]
embed_params = [model.embed.weight, *model.value_embeds.parameters()]
scalar_params = [p for n, p in model.named_parameters() if p.ndim < 2 and "attn_scale" not in n]
attn_scale_params = [p for n, p in model.named_parameters() if p.ndim < 2 and "attn_scale" in n]
head_params = [model.lm_head.weight]

# init the optimizer(s)
optimizer1 = torch.optim.Adam([dict(params=embed_params, lr=0.6),
                               dict(params=head_params, lr=0.008),
                               dict(params=scalar_params, lr=0.04),
                               dict(params=attn_scale_params, lr=0.01)],
                              betas=(0.8, 0.95), fused=True)
optimizer2 = Muon(hidden_matrix_params, lr=0.05, momentum=0.95)
optimizers = [optimizer1, optimizer2]

# learning rate schedule: stable then decay
def get_lr(it):
    t = 1 - it / args.num_iterations # time remaining in training
    assert 1 >= t > 0
    # 1) constant lr for first part of training
    if t >= args.cooldown_frac:
        return 1.0
    # 2) then linear cooldown
    else:
        return t / args.cooldown_frac
schedulers = [torch.optim.lr_scheduler.LambdaLR(opt, get_lr) for opt in optimizers]

# sliding window size schedule: linear increase over training in chunks of 128 from 128 -> 1792. By @fernbear.bsky.social
def get_sliding_window_blocks(it):
    x = it / args.num_iterations # training progress
    assert 0 <= x <= 1
    return int(((1 - x) * 128 + x * 1856) // 128)
sliding_window_num_blocks = torch.tensor(1, dtype=torch.int32, device='cuda')

# Start training loop
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    # This effectively ignores timing first 10 steps, which are slower for weird reasons.
    # Alternately, and slightly more correctly in terms of benchmarking, we could do 10
    # steps with dummy data first, and then re-initialize the model and reset the loader.
    if step == 10:
        training_time_ms = 0
        t0 = time.perf_counter()
    timed_steps = float('nan') if step <= 11 else (step - 10) + 1 # <= 11 to avoid bug in val

    sliding_window_num_blocks.copy_(get_sliding_window_blocks(step))

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        # run validation batches
        model.eval()
        val_loader.reset()
        val_loss = 0.0
        # calculate the number of steps to take in the val loop.
        val_batch_size = world_size * micro_bs
        assert args.val_tokens % val_batch_size == 0
        val_steps = args.val_tokens // val_batch_size
        for _ in range(val_steps):
            with torch.no_grad():
                inputs_val, targets_val = val_loader.next_batch(val_batch_size)
                val_loss += ddp_model(inputs_val, targets_val, sliding_window_num_blocks)
        # Print attention scales
        for n, p in model.named_parameters():
            if p.ndim < 2 and "attn_scale" in n:
                print0(f'{n}: {p.item()}', console=True)
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        val_loss /= val_steps
        # logging
        print0(f'step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/(timed_steps-1):.2f}ms', console=True)
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f'logs/{run_id}', exist_ok=True)
            torch.save(log, f'logs/{run_id}/state_step{step:06d}.pt')
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    model.train()
    batch_size = args.batch_size
    assert batch_size % world_size == 0
    inputs_train, targets_train = train_loader.next_batch(batch_size)
    assert len(inputs_train) <= micro_bs or len(inputs_train) % micro_bs == 0
    for micro_inputs_train, micro_targets_train in zip(inputs_train.split(micro_bs), targets_train.split(micro_bs)):
        ddp_model(micro_inputs_train, micro_targets_train, sliding_window_num_blocks).backward()
    # momentum warmup for Muon
    frac = min(step/300, 1)
    for group in optimizer2.param_groups:
        group['momentum'] = (1 - frac) * 0.85 + frac * 0.95
    # step the optimizers and schedulers
    for opt, sched in zip(optimizers, schedulers):
        opt.step()
        if step != train_steps-1:
            sched.step()
    # null the gradients
    model.zero_grad(set_to_none=True)
    # logging
    approx_time = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f'step:{step+1}/{train_steps} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms', console=True)

print0(f'peak memory consumption: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB')
dist.destroy_process_group()

====================================================================================================
Running Python 3.10.12 (main, Sep 11 2024, 15:47:36) [GCC 11.4.0]
Running PyTorch 2.6.0.dev20241231+cu126 compiled for CUDA 12.6
Tue Jan 14 14:18:01 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.183.06             Driver Version: 535.183.06   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:19:00.0 Off |                    0 |
| N/A   29C    P0             118W / 700W |   7713MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:3B:00.0 Off |                    0 |
| N/A   28C    P0             111W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:4C:00.0 Off |                    0 |
| N/A   25C    P0             115W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   28C    P0             121W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:9B:00.0 Off |                    0 |
| N/A   30C    P0             118W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:BB:00.0 Off |                    0 |
| N/A   27C    P0             112W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:CB:00.0 Off |                    0 |
| N/A   28C    P0             110W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:DB:00.0 Off |                    0 |
| N/A   26C    P0             114W / 700W |   3211MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
Training dataloader files: ['data/fineweb10B/fineweb_train_000001.bin', 'data/fineweb10B/fineweb_train_000002.bin', 'data/fineweb10B/fineweb_train_000003.bin', 'data/fineweb10B/fineweb_train_000004.bin', 'data/fineweb10B/fineweb_train_000005.bin', 'data/fineweb10B/fineweb_train_000006.bin', 'data/fineweb10B/fineweb_train_000007.bin', 'data/fineweb10B/fineweb_train_000008.bin', 'data/fineweb10B/fineweb_train_000009.bin']
Validation dataloader files: ['data/fineweb10B/fineweb_val_000000.bin']
====================================================================================================
step:0/1370 val_loss:10.8258 train_time:0ms step_avg:nanms
step:1/1370 train_time:52713ms step_avg:nanms
step:2/1370 train_time:52790ms step_avg:nanms
step:3/1370 train_time:52975ms step_avg:nanms
step:4/1370 train_time:53109ms step_avg:nanms
step:5/1370 train_time:53243ms step_avg:nanms
step:6/1370 train_time:53375ms step_avg:nanms
step:7/1370 train_time:53508ms step_avg:nanms
step:8/1370 train_time:53640ms step_avg:nanms
step:9/1370 train_time:53773ms step_avg:nanms
step:10/1370 train_time:53913ms step_avg:nanms
step:11/1370 train_time:135ms step_avg:nanms
step:12/1370 train_time:270ms step_avg:nanms
step:13/1370 train_time:404ms step_avg:134.64ms
step:14/1370 train_time:538ms step_avg:134.60ms
step:15/1370 train_time:673ms step_avg:134.50ms
step:16/1370 train_time:808ms step_avg:134.60ms
step:17/1370 train_time:943ms step_avg:134.65ms
step:18/1370 train_time:1077ms step_avg:134.63ms
step:19/1370 train_time:1213ms step_avg:134.77ms
step:20/1370 train_time:1348ms step_avg:134.84ms
step:21/1370 train_time:1483ms step_avg:134.84ms
step:22/1370 train_time:1618ms step_avg:134.83ms
step:23/1370 train_time:1752ms step_avg:134.75ms
step:24/1370 train_time:1888ms step_avg:134.82ms
step:25/1370 train_time:2021ms step_avg:134.75ms
step:26/1370 train_time:2158ms step_avg:134.86ms
step:27/1370 train_time:2295ms step_avg:134.98ms
step:28/1370 train_time:2429ms step_avg:134.96ms
step:29/1370 train_time:2563ms step_avg:134.92ms
step:30/1370 train_time:2700ms step_avg:134.99ms
step:31/1370 train_time:2833ms step_avg:134.88ms
step:32/1370 train_time:2967ms step_avg:134.88ms
step:33/1370 train_time:3104ms step_avg:134.98ms
step:34/1370 train_time:3241ms step_avg:135.04ms
step:35/1370 train_time:3374ms step_avg:134.96ms
step:36/1370 train_time:3509ms step_avg:134.98ms
step:37/1370 train_time:3644ms step_avg:134.98ms
step:38/1370 train_time:3781ms step_avg:135.03ms
step:39/1370 train_time:3915ms step_avg:135.00ms
step:40/1370 train_time:4050ms step_avg:135.00ms
step:41/1370 train_time:4184ms step_avg:134.98ms
step:42/1370 train_time:4319ms step_avg:134.98ms
step:43/1370 train_time:4453ms step_avg:134.94ms
step:44/1370 train_time:4588ms step_avg:134.95ms
step:45/1370 train_time:4723ms step_avg:134.95ms
step:46/1370 train_time:4858ms step_avg:134.93ms
step:47/1370 train_time:4992ms step_avg:134.92ms
step:48/1370 train_time:5127ms step_avg:134.92ms
step:49/1370 train_time:5262ms step_avg:134.92ms
step:50/1370 train_time:5398ms step_avg:134.94ms
step:51/1370 train_time:5530ms step_avg:134.89ms
step:52/1370 train_time:5666ms step_avg:134.90ms
step:53/1370 train_time:5802ms step_avg:134.93ms
step:54/1370 train_time:5938ms step_avg:134.95ms
step:55/1370 train_time:6072ms step_avg:134.93ms
step:56/1370 train_time:6208ms step_avg:134.95ms
step:57/1370 train_time:6343ms step_avg:134.96ms
step:58/1370 train_time:6479ms step_avg:134.98ms
step:59/1370 train_time:6612ms step_avg:134.94ms
step:60/1370 train_time:6747ms step_avg:134.94ms
step:61/1370 train_time:6882ms step_avg:134.95ms
step:62/1370 train_time:7019ms step_avg:134.98ms
step:63/1370 train_time:7155ms step_avg:135.01ms
step:64/1370 train_time:7290ms step_avg:134.99ms
step:65/1370 train_time:7425ms step_avg:135.01ms
step:66/1370 train_time:7560ms step_avg:134.99ms
step:67/1370 train_time:7697ms step_avg:135.03ms
step:68/1370 train_time:7830ms step_avg:135.00ms
step:69/1370 train_time:7966ms step_avg:135.01ms
step:70/1370 train_time:8101ms step_avg:135.02ms
step:71/1370 train_time:8236ms step_avg:135.02ms
step:72/1370 train_time:8369ms step_avg:134.98ms
step:73/1370 train_time:8504ms step_avg:134.99ms
step:74/1370 train_time:8639ms step_avg:134.99ms
step:75/1370 train_time:8773ms step_avg:134.97ms
step:76/1370 train_time:8910ms step_avg:134.99ms
step:77/1370 train_time:9047ms step_avg:135.02ms
step:78/1370 train_time:9183ms step_avg:135.04ms
step:79/1370 train_time:9318ms step_avg:135.05ms
step:80/1370 train_time:9452ms step_avg:135.03ms
step:81/1370 train_time:9586ms step_avg:135.02ms
step:82/1370 train_time:9722ms step_avg:135.02ms
step:83/1370 train_time:9856ms step_avg:135.01ms
step:84/1370 train_time:9992ms step_avg:135.03ms
step:85/1370 train_time:10127ms step_avg:135.03ms
step:86/1370 train_time:10264ms step_avg:135.05ms
step:87/1370 train_time:10399ms step_avg:135.06ms
step:88/1370 train_time:10534ms step_avg:135.05ms
step:89/1370 train_time:10667ms step_avg:135.03ms
step:90/1370 train_time:10805ms step_avg:135.06ms
step:91/1370 train_time:10941ms step_avg:135.07ms
step:92/1370 train_time:11077ms step_avg:135.09ms
step:93/1370 train_time:11212ms step_avg:135.09ms
step:94/1370 train_time:11348ms step_avg:135.09ms
step:95/1370 train_time:11484ms step_avg:135.10ms
step:96/1370 train_time:11620ms step_avg:135.12ms
step:97/1370 train_time:11754ms step_avg:135.10ms
step:98/1370 train_time:11889ms step_avg:135.10ms
step:99/1370 train_time:12024ms step_avg:135.10ms
step:100/1370 train_time:12162ms step_avg:135.13ms
step:101/1370 train_time:12297ms step_avg:135.13ms
step:102/1370 train_time:12431ms step_avg:135.12ms
step:103/1370 train_time:12568ms step_avg:135.14ms
step:104/1370 train_time:12707ms step_avg:135.18ms
step:105/1370 train_time:12844ms step_avg:135.20ms
step:106/1370 train_time:12981ms step_avg:135.22ms
step:107/1370 train_time:13122ms step_avg:135.28ms
step:108/1370 train_time:13260ms step_avg:135.31ms
step:109/1370 train_time:13399ms step_avg:135.34ms
step:110/1370 train_time:13537ms step_avg:135.37ms
step:111/1370 train_time:13674ms step_avg:135.39ms
step:112/1370 train_time:13813ms step_avg:135.43ms
step:113/1370 train_time:13951ms step_avg:135.45ms
step:114/1370 train_time:14091ms step_avg:135.49ms
step:115/1370 train_time:14230ms step_avg:135.52ms
step:116/1370 train_time:14368ms step_avg:135.55ms
step:117/1370 train_time:14506ms step_avg:135.57ms
step:118/1370 train_time:14643ms step_avg:135.58ms
step:119/1370 train_time:14782ms step_avg:135.61ms
step:120/1370 train_time:14921ms step_avg:135.65ms
step:121/1370 train_time:15060ms step_avg:135.68ms
step:122/1370 train_time:15200ms step_avg:135.71ms
step:123/1370 train_time:15337ms step_avg:135.73ms
step:124/1370 train_time:15475ms step_avg:135.74ms
step:125/1370 train_time:15613ms step_avg:135.76ms
step:125/1370 val_loss:4.3661 train_time:15680ms step_avg:136.35ms
step:126/1370 train_time:15754ms step_avg:135.81ms
step:127/1370 train_time:15895ms step_avg:135.86ms
step:128/1370 train_time:16034ms step_avg:135.88ms
step:129/1370 train_time:16170ms step_avg:135.88ms
step:130/1370 train_time:16307ms step_avg:135.89ms
step:131/1370 train_time:16444ms step_avg:135.90ms
step:132/1370 train_time:16582ms step_avg:135.92ms
step:133/1370 train_time:16722ms step_avg:135.95ms
step:134/1370 train_time:16861ms step_avg:135.98ms
step:135/1370 train_time:17001ms step_avg:136.00ms
step:136/1370 train_time:17139ms step_avg:136.02ms
step:137/1370 train_time:17277ms step_avg:136.04ms
step:138/1370 train_time:17415ms step_avg:136.06ms
step:139/1370 train_time:17553ms step_avg:136.07ms
step:140/1370 train_time:17692ms step_avg:136.09ms
step:141/1370 train_time:17833ms step_avg:136.13ms
step:142/1370 train_time:17971ms step_avg:136.14ms
step:143/1370 train_time:18109ms step_avg:136.16ms
step:144/1370 train_time:18247ms step_avg:136.17ms
step:145/1370 train_time:18385ms step_avg:136.18ms
step:146/1370 train_time:18523ms step_avg:136.20ms
step:147/1370 train_time:18661ms step_avg:136.21ms
step:148/1370 train_time:18803ms step_avg:136.25ms
step:149/1370 train_time:18942ms step_avg:136.27ms
step:150/1370 train_time:19081ms step_avg:136.29ms
step:151/1370 train_time:19219ms step_avg:136.30ms
step:152/1370 train_time:19358ms step_avg:136.32ms
step:153/1370 train_time:19495ms step_avg:136.33ms
step:154/1370 train_time:19635ms step_avg:136.35ms
step:155/1370 train_time:19772ms step_avg:136.36ms
step:156/1370 train_time:19911ms step_avg:136.38ms
step:157/1370 train_time:20051ms step_avg:136.40ms
step:158/1370 train_time:20189ms step_avg:136.41ms
step:159/1370 train_time:20328ms step_avg:136.43ms
step:160/1370 train_time:20466ms step_avg:136.44ms
step:161/1370 train_time:20605ms step_avg:136.46ms
step:162/1370 train_time:20744ms step_avg:136.48ms
step:163/1370 train_time:20883ms step_avg:136.49ms
step:164/1370 train_time:21022ms step_avg:136.51ms
step:165/1370 train_time:21161ms step_avg:136.53ms
step:166/1370 train_time:21300ms step_avg:136.54ms
step:167/1370 train_time:21439ms step_avg:136.55ms
step:168/1370 train_time:21579ms step_avg:136.57ms
step:169/1370 train_time:21718ms step_avg:136.59ms
step:170/1370 train_time:21858ms step_avg:136.61ms
step:171/1370 train_time:21996ms step_avg:136.62ms
step:172/1370 train_time:22136ms step_avg:136.64ms
step:173/1370 train_time:22275ms step_avg:136.66ms
step:174/1370 train_time:22412ms step_avg:136.66ms
step:175/1370 train_time:22551ms step_avg:136.67ms
step:176/1370 train_time:22690ms step_avg:136.69ms
step:177/1370 train_time:22829ms step_avg:136.70ms
step:178/1370 train_time:22969ms step_avg:136.72ms
step:179/1370 train_time:23107ms step_avg:136.73ms
step:180/1370 train_time:23247ms step_avg:136.74ms
step:181/1370 train_time:23385ms step_avg:136.76ms
step:182/1370 train_time:23524ms step_avg:136.77ms
step:183/1370 train_time:23662ms step_avg:136.78ms
step:184/1370 train_time:23801ms step_avg:136.79ms
step:185/1370 train_time:23941ms step_avg:136.81ms
step:186/1370 train_time:24080ms step_avg:136.82ms
step:187/1370 train_time:24218ms step_avg:136.83ms
step:188/1370 train_time:24358ms step_avg:136.84ms
step:189/1370 train_time:24496ms step_avg:136.85ms
step:190/1370 train_time:24637ms step_avg:136.87ms
step:191/1370 train_time:24818ms step_avg:137.12ms
step:192/1370 train_time:24956ms step_avg:137.12ms
step:193/1370 train_time:25093ms step_avg:137.12ms
step:194/1370 train_time:25231ms step_avg:137.13ms
step:195/1370 train_time:25368ms step_avg:137.12ms
step:196/1370 train_time:25505ms step_avg:137.13ms
step:197/1370 train_time:25644ms step_avg:137.13ms
step:198/1370 train_time:25787ms step_avg:137.16ms
step:199/1370 train_time:25928ms step_avg:137.18ms
step:200/1370 train_time:26067ms step_avg:137.19ms
step:201/1370 train_time:26205ms step_avg:137.20ms
step:202/1370 train_time:26345ms step_avg:137.21ms
step:203/1370 train_time:26483ms step_avg:137.22ms
step:204/1370 train_time:26624ms step_avg:137.24ms
step:205/1370 train_time:26767ms step_avg:137.27ms
step:206/1370 train_time:26911ms step_avg:137.30ms
step:207/1370 train_time:27052ms step_avg:137.32ms
step:208/1370 train_time:27193ms step_avg:137.34ms
step:209/1370 train_time:27335ms step_avg:137.36ms
step:210/1370 train_time:27475ms step_avg:137.38ms
step:211/1370 train_time:27618ms step_avg:137.40ms
step:212/1370 train_time:27761ms step_avg:137.43ms
step:213/1370 train_time:27904ms step_avg:137.46ms
step:214/1370 train_time:28046ms step_avg:137.48ms
step:215/1370 train_time:28187ms step_avg:137.50ms
step:216/1370 train_time:28330ms step_avg:137.53ms
step:217/1370 train_time:28470ms step_avg:137.54ms
step:218/1370 train_time:28612ms step_avg:137.56ms
step:219/1370 train_time:28754ms step_avg:137.58ms
step:220/1370 train_time:28895ms step_avg:137.59ms
step:221/1370 train_time:29038ms step_avg:137.62ms
step:222/1370 train_time:29181ms step_avg:137.65ms
step:223/1370 train_time:29322ms step_avg:137.66ms
step:224/1370 train_time:29463ms step_avg:137.68ms
step:225/1370 train_time:29604ms step_avg:137.69ms
step:226/1370 train_time:29747ms step_avg:137.72ms
step:227/1370 train_time:29887ms step_avg:137.73ms
step:228/1370 train_time:30031ms step_avg:137.76ms
step:229/1370 train_time:30173ms step_avg:137.78ms
step:230/1370 train_time:30315ms step_avg:137.79ms
step:231/1370 train_time:30457ms step_avg:137.81ms
step:232/1370 train_time:30597ms step_avg:137.82ms
step:233/1370 train_time:30739ms step_avg:137.84ms
step:234/1370 train_time:30880ms step_avg:137.86ms
step:235/1370 train_time:31021ms step_avg:137.87ms
step:236/1370 train_time:31162ms step_avg:137.89ms
step:237/1370 train_time:31303ms step_avg:137.90ms
step:238/1370 train_time:31446ms step_avg:137.92ms
step:239/1370 train_time:31587ms step_avg:137.94ms
step:240/1370 train_time:31730ms step_avg:137.96ms
step:241/1370 train_time:31872ms step_avg:137.98ms
step:242/1370 train_time:32014ms step_avg:137.99ms
step:243/1370 train_time:32156ms step_avg:138.01ms
step:244/1370 train_time:32298ms step_avg:138.03ms
step:245/1370 train_time:32440ms step_avg:138.04ms
step:246/1370 train_time:32580ms step_avg:138.05ms
step:247/1370 train_time:32721ms step_avg:138.06ms
step:248/1370 train_time:32862ms step_avg:138.08ms
step:249/1370 train_time:33003ms step_avg:138.09ms
step:250/1370 train_time:33145ms step_avg:138.11ms
step:250/1370 val_loss:3.9559 train_time:33216ms step_avg:138.40ms
step:251/1370 train_time:33291ms step_avg:138.14ms
step:252/1370 train_time:33435ms step_avg:138.16ms
step:253/1370 train_time:33575ms step_avg:138.17ms
step:254/1370 train_time:33718ms step_avg:138.19ms
step:255/1370 train_time:33858ms step_avg:138.20ms
step:256/1370 train_time:33998ms step_avg:138.20ms
step:257/1370 train_time:34139ms step_avg:138.21ms
step:258/1370 train_time:34283ms step_avg:138.24ms
step:259/1370 train_time:34426ms step_avg:138.26ms
step:260/1370 train_time:34568ms step_avg:138.27ms
step:261/1370 train_time:34709ms step_avg:138.28ms
step:262/1370 train_time:34851ms step_avg:138.30ms
step:263/1370 train_time:34991ms step_avg:138.30ms
step:264/1370 train_time:35132ms step_avg:138.32ms
step:265/1370 train_time:35275ms step_avg:138.33ms
step:266/1370 train_time:35419ms step_avg:138.35ms
step:267/1370 train_time:35562ms step_avg:138.38ms
step:268/1370 train_time:35706ms step_avg:138.40ms
step:269/1370 train_time:35847ms step_avg:138.41ms
step:270/1370 train_time:35988ms step_avg:138.41ms
step:271/1370 train_time:36129ms step_avg:138.42ms
step:272/1370 train_time:36270ms step_avg:138.43ms
step:273/1370 train_time:36412ms step_avg:138.45ms
step:274/1370 train_time:36554ms step_avg:138.46ms
step:275/1370 train_time:36696ms step_avg:138.47ms
step:276/1370 train_time:36839ms step_avg:138.49ms
step:277/1370 train_time:36981ms step_avg:138.50ms
step:278/1370 train_time:37122ms step_avg:138.52ms
step:279/1370 train_time:37266ms step_avg:138.54ms
step:280/1370 train_time:37409ms step_avg:138.55ms
step:281/1370 train_time:37550ms step_avg:138.56ms
step:282/1370 train_time:37692ms step_avg:138.57ms
step:283/1370 train_time:37835ms step_avg:138.59ms
step:284/1370 train_time:37978ms step_avg:138.60ms
step:285/1370 train_time:38120ms step_avg:138.62ms
step:286/1370 train_time:38262ms step_avg:138.63ms
step:287/1370 train_time:38403ms step_avg:138.64ms
step:288/1370 train_time:38544ms step_avg:138.65ms
step:289/1370 train_time:38684ms step_avg:138.65ms
step:290/1370 train_time:38826ms step_avg:138.67ms
step:291/1370 train_time:38969ms step_avg:138.68ms
step:292/1370 train_time:39110ms step_avg:138.69ms
step:293/1370 train_time:39253ms step_avg:138.70ms
step:294/1370 train_time:39394ms step_avg:138.71ms
step:295/1370 train_time:39535ms step_avg:138.72ms
step:296/1370 train_time:39677ms step_avg:138.73ms
step:297/1370 train_time:39819ms step_avg:138.74ms
step:298/1370 train_time:39961ms step_avg:138.75ms
step:299/1370 train_time:40104ms step_avg:138.77ms
step:300/1370 train_time:40246ms step_avg:138.78ms
step:301/1370 train_time:40388ms step_avg:138.79ms
step:302/1370 train_time:40530ms step_avg:138.80ms
step:303/1370 train_time:40671ms step_avg:138.81ms
step:304/1370 train_time:40812ms step_avg:138.82ms
step:305/1370 train_time:40954ms step_avg:138.83ms
step:306/1370 train_time:41096ms step_avg:138.84ms
step:307/1370 train_time:41242ms step_avg:138.86ms
step:308/1370 train_time:41386ms step_avg:138.88ms
step:309/1370 train_time:41529ms step_avg:138.89ms
step:310/1370 train_time:41673ms step_avg:138.91ms
step:311/1370 train_time:41818ms step_avg:138.93ms
step:312/1370 train_time:41963ms step_avg:138.95ms
step:313/1370 train_time:42106ms step_avg:138.96ms
step:314/1370 train_time:42250ms step_avg:138.98ms
step:315/1370 train_time:42392ms step_avg:138.99ms
step:316/1370 train_time:42535ms step_avg:139.00ms
step:317/1370 train_time:42682ms step_avg:139.03ms
step:318/1370 train_time:42826ms step_avg:139.05ms
step:319/1370 train_time:42970ms step_avg:139.06ms
step:320/1370 train_time:43114ms step_avg:139.08ms
step:321/1370 train_time:43259ms step_avg:139.10ms
step:322/1370 train_time:43403ms step_avg:139.11ms
step:323/1370 train_time:43547ms step_avg:139.13ms
step:324/1370 train_time:43689ms step_avg:139.14ms
step:325/1370 train_time:43834ms step_avg:139.16ms
step:326/1370 train_time:43978ms step_avg:139.17ms
step:327/1370 train_time:44122ms step_avg:139.19ms
step:328/1370 train_time:44267ms step_avg:139.21ms
step:329/1370 train_time:44410ms step_avg:139.22ms
step:330/1370 train_time:44554ms step_avg:139.23ms
step:331/1370 train_time:44697ms step_avg:139.24ms
step:332/1370 train_time:44842ms step_avg:139.26ms
step:333/1370 train_time:44989ms step_avg:139.28ms
step:334/1370 train_time:45131ms step_avg:139.29ms
step:335/1370 train_time:45277ms step_avg:139.31ms
step:336/1370 train_time:45421ms step_avg:139.33ms
step:337/1370 train_time:45567ms step_avg:139.35ms
step:338/1370 train_time:45708ms step_avg:139.35ms
step:339/1370 train_time:45852ms step_avg:139.37ms
step:340/1370 train_time:45994ms step_avg:139.38ms
step:341/1370 train_time:46139ms step_avg:139.39ms
step:342/1370 train_time:46284ms step_avg:139.41ms
step:343/1370 train_time:46427ms step_avg:139.42ms
step:344/1370 train_time:46571ms step_avg:139.43ms
step:345/1370 train_time:46716ms step_avg:139.45ms
step:346/1370 train_time:46861ms step_avg:139.47ms
step:347/1370 train_time:47005ms step_avg:139.48ms
step:348/1370 train_time:47149ms step_avg:139.49ms
step:349/1370 train_time:47292ms step_avg:139.50ms
step:350/1370 train_time:47435ms step_avg:139.52ms
step:351/1370 train_time:47579ms step_avg:139.53ms
step:352/1370 train_time:47722ms step_avg:139.54ms
step:353/1370 train_time:47869ms step_avg:139.56ms
step:354/1370 train_time:48012ms step_avg:139.57ms
step:355/1370 train_time:48156ms step_avg:139.58ms
step:356/1370 train_time:48299ms step_avg:139.59ms
step:357/1370 train_time:48445ms step_avg:139.61ms
step:358/1370 train_time:48589ms step_avg:139.62ms
step:359/1370 train_time:48731ms step_avg:139.63ms
step:360/1370 train_time:48877ms step_avg:139.65ms
step:361/1370 train_time:49021ms step_avg:139.66ms
step:362/1370 train_time:49165ms step_avg:139.67ms
step:363/1370 train_time:49308ms step_avg:139.68ms
step:364/1370 train_time:49452ms step_avg:139.70ms
step:365/1370 train_time:49596ms step_avg:139.71ms
step:366/1370 train_time:49741ms step_avg:139.72ms
step:367/1370 train_time:49886ms step_avg:139.74ms
step:368/1370 train_time:50029ms step_avg:139.75ms
step:369/1370 train_time:50173ms step_avg:139.76ms
step:370/1370 train_time:50317ms step_avg:139.77ms
step:371/1370 train_time:50461ms step_avg:139.78ms
step:372/1370 train_time:50605ms step_avg:139.79ms
step:373/1370 train_time:50749ms step_avg:139.80ms
step:374/1370 train_time:50892ms step_avg:139.81ms
step:375/1370 train_time:51035ms step_avg:139.82ms
step:375/1370 val_loss:3.7741 train_time:51107ms step_avg:140.02ms
step:376/1370 train_time:51182ms step_avg:139.84ms
step:377/1370 train_time:51327ms step_avg:139.86ms
step:378/1370 train_time:51473ms step_avg:139.87ms
step:379/1370 train_time:51617ms step_avg:139.88ms
step:380/1370 train_time:51761ms step_avg:139.89ms
step:381/1370 train_time:51944ms step_avg:140.01ms
step:382/1370 train_time:52086ms step_avg:140.02ms
step:383/1370 train_time:52228ms step_avg:140.02ms
step:384/1370 train_time:52372ms step_avg:140.03ms
step:385/1370 train_time:52516ms step_avg:140.04ms
step:386/1370 train_time:52660ms step_avg:140.05ms
step:387/1370 train_time:52803ms step_avg:140.06ms
step:388/1370 train_time:52950ms step_avg:140.08ms
step:389/1370 train_time:53093ms step_avg:140.09ms
step:390/1370 train_time:53238ms step_avg:140.10ms
step:391/1370 train_time:53381ms step_avg:140.11ms
step:392/1370 train_time:53524ms step_avg:140.12ms
step:393/1370 train_time:53667ms step_avg:140.12ms
step:394/1370 train_time:53812ms step_avg:140.14ms
step:395/1370 train_time:53958ms step_avg:140.15ms
step:396/1370 train_time:54102ms step_avg:140.16ms
step:397/1370 train_time:54246ms step_avg:140.17ms
step:398/1370 train_time:54388ms step_avg:140.18ms
step:399/1370 train_time:54533ms step_avg:140.19ms
step:400/1370 train_time:54677ms step_avg:140.20ms
step:401/1370 train_time:54822ms step_avg:140.21ms
step:402/1370 train_time:54968ms step_avg:140.22ms
step:403/1370 train_time:55113ms step_avg:140.24ms
step:404/1370 train_time:55258ms step_avg:140.25ms
step:405/1370 train_time:55401ms step_avg:140.26ms
step:406/1370 train_time:55545ms step_avg:140.27ms
step:407/1370 train_time:55688ms step_avg:140.27ms
step:408/1370 train_time:55835ms step_avg:140.29ms
step:409/1370 train_time:55981ms step_avg:140.30ms
step:410/1370 train_time:56126ms step_avg:140.32ms
step:411/1370 train_time:56274ms step_avg:140.33ms
step:412/1370 train_time:56419ms step_avg:140.35ms
step:413/1370 train_time:56565ms step_avg:140.36ms
step:414/1370 train_time:56710ms step_avg:140.37ms
step:415/1370 train_time:56858ms step_avg:140.39ms
step:416/1370 train_time:57003ms step_avg:140.40ms
step:417/1370 train_time:57151ms step_avg:140.42ms
step:418/1370 train_time:57297ms step_avg:140.43ms
step:419/1370 train_time:57443ms step_avg:140.45ms
step:420/1370 train_time:57588ms step_avg:140.46ms
step:421/1370 train_time:57736ms step_avg:140.48ms
step:422/1370 train_time:57882ms step_avg:140.49ms
step:423/1370 train_time:58028ms step_avg:140.50ms
step:424/1370 train_time:58175ms step_avg:140.52ms
step:425/1370 train_time:58322ms step_avg:140.54ms
step:426/1370 train_time:58466ms step_avg:140.54ms
step:427/1370 train_time:58610ms step_avg:140.55ms
step:428/1370 train_time:58758ms step_avg:140.57ms
step:429/1370 train_time:58902ms step_avg:140.58ms
step:430/1370 train_time:59048ms step_avg:140.59ms
step:431/1370 train_time:59193ms step_avg:140.60ms
step:432/1370 train_time:59341ms step_avg:140.62ms
step:433/1370 train_time:59485ms step_avg:140.63ms
step:434/1370 train_time:59631ms step_avg:140.64ms
step:435/1370 train_time:59779ms step_avg:140.66ms
step:436/1370 train_time:59924ms step_avg:140.67ms
step:437/1370 train_time:60069ms step_avg:140.68ms
step:438/1370 train_time:60215ms step_avg:140.69ms
step:439/1370 train_time:60362ms step_avg:140.70ms
step:440/1370 train_time:60507ms step_avg:140.71ms
step:441/1370 train_time:60656ms step_avg:140.73ms
step:442/1370 train_time:60801ms step_avg:140.74ms
step:443/1370 train_time:60947ms step_avg:140.76ms
step:444/1370 train_time:61092ms step_avg:140.76ms
step:445/1370 train_time:61240ms step_avg:140.78ms
step:446/1370 train_time:61385ms step_avg:140.79ms
step:447/1370 train_time:61531ms step_avg:140.80ms
step:448/1370 train_time:61679ms step_avg:140.82ms
step:449/1370 train_time:61824ms step_avg:140.83ms
step:450/1370 train_time:61969ms step_avg:140.84ms
step:451/1370 train_time:62117ms step_avg:140.85ms
step:452/1370 train_time:62263ms step_avg:140.87ms
step:453/1370 train_time:62409ms step_avg:140.88ms
step:454/1370 train_time:62556ms step_avg:140.89ms
step:455/1370 train_time:62701ms step_avg:140.90ms
step:456/1370 train_time:62847ms step_avg:140.91ms
step:457/1370 train_time:62992ms step_avg:140.92ms
step:458/1370 train_time:63140ms step_avg:140.94ms
step:459/1370 train_time:63284ms step_avg:140.95ms
step:460/1370 train_time:63431ms step_avg:140.96ms
step:461/1370 train_time:63578ms step_avg:140.97ms
step:462/1370 train_time:63724ms step_avg:140.98ms
step:463/1370 train_time:63869ms step_avg:140.99ms
step:464/1370 train_time:64015ms step_avg:141.00ms
step:465/1370 train_time:64162ms step_avg:141.01ms
step:466/1370 train_time:64305ms step_avg:141.02ms
step:467/1370 train_time:64451ms step_avg:141.03ms
step:468/1370 train_time:64597ms step_avg:141.04ms
step:469/1370 train_time:64743ms step_avg:141.05ms
step:470/1370 train_time:64887ms step_avg:141.06ms
step:471/1370 train_time:65033ms step_avg:141.07ms
step:472/1370 train_time:65180ms step_avg:141.08ms
step:473/1370 train_time:65324ms step_avg:141.09ms
step:474/1370 train_time:65471ms step_avg:141.10ms
step:475/1370 train_time:65617ms step_avg:141.11ms
step:476/1370 train_time:65764ms step_avg:141.13ms
step:477/1370 train_time:65909ms step_avg:141.13ms
step:478/1370 train_time:66055ms step_avg:141.14ms
step:479/1370 train_time:66200ms step_avg:141.15ms
step:480/1370 train_time:66345ms step_avg:141.16ms
step:481/1370 train_time:66491ms step_avg:141.17ms
step:482/1370 train_time:66639ms step_avg:141.18ms
step:483/1370 train_time:66784ms step_avg:141.19ms
step:484/1370 train_time:66929ms step_avg:141.20ms
step:485/1370 train_time:67075ms step_avg:141.21ms
step:486/1370 train_time:67222ms step_avg:141.22ms
step:487/1370 train_time:67367ms step_avg:141.23ms
step:488/1370 train_time:67512ms step_avg:141.24ms
step:489/1370 train_time:67660ms step_avg:141.25ms
step:490/1370 train_time:67806ms step_avg:141.26ms
step:491/1370 train_time:67954ms step_avg:141.28ms
step:492/1370 train_time:68100ms step_avg:141.29ms
step:493/1370 train_time:68245ms step_avg:141.29ms
step:494/1370 train_time:68389ms step_avg:141.30ms
step:495/1370 train_time:68537ms step_avg:141.31ms
step:496/1370 train_time:68683ms step_avg:141.32ms
step:497/1370 train_time:68827ms step_avg:141.33ms
step:498/1370 train_time:68975ms step_avg:141.34ms
step:499/1370 train_time:69122ms step_avg:141.35ms
step:500/1370 train_time:69267ms step_avg:141.36ms
step:500/1370 val_loss:3.6557 train_time:69338ms step_avg:141.51ms
step:501/1370 train_time:69414ms step_avg:141.37ms
step:502/1370 train_time:69562ms step_avg:141.39ms
step:503/1370 train_time:69709ms step_avg:141.40ms
step:504/1370 train_time:69853ms step_avg:141.40ms
step:505/1370 train_time:69998ms step_avg:141.41ms
step:506/1370 train_time:70143ms step_avg:141.42ms
step:507/1370 train_time:70288ms step_avg:141.42ms
step:508/1370 train_time:70436ms step_avg:141.44ms
step:509/1370 train_time:70582ms step_avg:141.45ms
step:510/1370 train_time:70730ms step_avg:141.46ms
step:511/1370 train_time:70876ms step_avg:141.47ms
step:512/1370 train_time:71027ms step_avg:141.49ms
step:513/1370 train_time:71172ms step_avg:141.50ms
step:514/1370 train_time:71319ms step_avg:141.51ms
step:515/1370 train_time:71469ms step_avg:141.52ms
step:516/1370 train_time:71616ms step_avg:141.53ms
step:517/1370 train_time:71764ms step_avg:141.55ms
step:518/1370 train_time:71911ms step_avg:141.56ms
step:519/1370 train_time:72058ms step_avg:141.57ms
step:520/1370 train_time:72207ms step_avg:141.58ms
step:521/1370 train_time:72353ms step_avg:141.59ms
step:522/1370 train_time:72500ms step_avg:141.60ms
step:523/1370 train_time:72649ms step_avg:141.62ms
step:524/1370 train_time:72794ms step_avg:141.62ms
step:525/1370 train_time:72943ms step_avg:141.64ms
step:526/1370 train_time:73090ms step_avg:141.65ms
step:527/1370 train_time:73238ms step_avg:141.66ms
step:528/1370 train_time:73386ms step_avg:141.67ms
step:529/1370 train_time:73532ms step_avg:141.68ms
step:530/1370 train_time:73678ms step_avg:141.69ms
step:531/1370 train_time:73827ms step_avg:141.70ms
step:532/1370 train_time:73974ms step_avg:141.71ms
step:533/1370 train_time:74123ms step_avg:141.73ms
step:534/1370 train_time:74270ms step_avg:141.74ms
step:535/1370 train_time:74416ms step_avg:141.75ms
step:536/1370 train_time:74566ms step_avg:141.76ms
step:537/1370 train_time:74712ms step_avg:141.77ms
step:538/1370 train_time:74859ms step_avg:141.78ms
step:539/1370 train_time:75009ms step_avg:141.79ms
step:540/1370 train_time:75156ms step_avg:141.80ms
step:541/1370 train_time:75304ms step_avg:141.82ms
step:542/1370 train_time:75451ms step_avg:141.83ms
step:543/1370 train_time:75597ms step_avg:141.83ms
step:544/1370 train_time:75746ms step_avg:141.85ms
step:545/1370 train_time:75892ms step_avg:141.85ms
step:546/1370 train_time:76039ms step_avg:141.86ms
step:547/1370 train_time:76186ms step_avg:141.87ms
step:548/1370 train_time:76335ms step_avg:141.89ms
step:549/1370 train_time:76482ms step_avg:141.90ms
step:550/1370 train_time:76631ms step_avg:141.91ms
step:551/1370 train_time:76777ms step_avg:141.92ms
step:552/1370 train_time:76926ms step_avg:141.93ms
step:553/1370 train_time:77072ms step_avg:141.94ms
step:554/1370 train_time:77218ms step_avg:141.94ms
step:555/1370 train_time:77366ms step_avg:141.96ms
step:556/1370 train_time:77512ms step_avg:141.96ms
step:557/1370 train_time:77660ms step_avg:141.98ms
step:558/1370 train_time:77810ms step_avg:141.99ms
step:559/1370 train_time:77956ms step_avg:142.00ms
step:560/1370 train_time:78103ms step_avg:142.01ms
step:561/1370 train_time:78250ms step_avg:142.01ms
step:562/1370 train_time:78396ms step_avg:142.02ms
step:563/1370 train_time:78546ms step_avg:142.04ms
step:564/1370 train_time:78692ms step_avg:142.04ms
step:565/1370 train_time:78841ms step_avg:142.06ms
step:566/1370 train_time:78988ms step_avg:142.07ms
step:567/1370 train_time:79135ms step_avg:142.07ms
step:568/1370 train_time:79281ms step_avg:142.08ms
step:569/1370 train_time:79430ms step_avg:142.09ms
step:570/1370 train_time:79577ms step_avg:142.10ms
step:571/1370 train_time:79758ms step_avg:142.17ms
step:572/1370 train_time:79906ms step_avg:142.18ms
step:573/1370 train_time:80053ms step_avg:142.19ms
step:574/1370 train_time:80202ms step_avg:142.20ms
step:575/1370 train_time:80350ms step_avg:142.21ms
step:576/1370 train_time:80495ms step_avg:142.22ms
step:577/1370 train_time:80644ms step_avg:142.23ms
step:578/1370 train_time:80791ms step_avg:142.24ms
step:579/1370 train_time:80940ms step_avg:142.25ms
step:580/1370 train_time:81089ms step_avg:142.26ms
step:581/1370 train_time:81235ms step_avg:142.27ms
step:582/1370 train_time:81383ms step_avg:142.28ms
step:583/1370 train_time:81530ms step_avg:142.29ms
step:584/1370 train_time:81677ms step_avg:142.30ms
step:585/1370 train_time:81828ms step_avg:142.31ms
step:586/1370 train_time:81975ms step_avg:142.32ms
step:587/1370 train_time:82124ms step_avg:142.33ms
step:588/1370 train_time:82271ms step_avg:142.34ms
step:589/1370 train_time:82418ms step_avg:142.34ms
step:590/1370 train_time:82566ms step_avg:142.36ms
step:591/1370 train_time:82713ms step_avg:142.36ms
step:592/1370 train_time:82862ms step_avg:142.38ms
step:593/1370 train_time:83011ms step_avg:142.39ms
step:594/1370 train_time:83158ms step_avg:142.39ms
step:595/1370 train_time:83306ms step_avg:142.40ms
step:596/1370 train_time:83452ms step_avg:142.41ms
step:597/1370 train_time:83599ms step_avg:142.42ms
step:598/1370 train_time:83747ms step_avg:142.43ms
step:599/1370 train_time:83892ms step_avg:142.43ms
step:600/1370 train_time:84042ms step_avg:142.44ms
step:601/1370 train_time:84189ms step_avg:142.45ms
step:602/1370 train_time:84336ms step_avg:142.46ms
step:603/1370 train_time:84484ms step_avg:142.47ms
step:604/1370 train_time:84631ms step_avg:142.48ms
step:605/1370 train_time:84778ms step_avg:142.48ms
step:606/1370 train_time:84928ms step_avg:142.50ms
step:607/1370 train_time:85075ms step_avg:142.50ms
step:608/1370 train_time:85224ms step_avg:142.52ms
step:609/1370 train_time:85371ms step_avg:142.52ms
step:610/1370 train_time:85517ms step_avg:142.53ms
step:611/1370 train_time:85667ms step_avg:142.54ms
step:612/1370 train_time:85814ms step_avg:142.55ms
step:613/1370 train_time:85965ms step_avg:142.56ms
step:614/1370 train_time:86113ms step_avg:142.57ms
step:615/1370 train_time:86260ms step_avg:142.58ms
step:616/1370 train_time:86410ms step_avg:142.59ms
step:617/1370 train_time:86558ms step_avg:142.60ms
step:618/1370 train_time:86709ms step_avg:142.61ms
step:619/1370 train_time:86858ms step_avg:142.62ms
step:620/1370 train_time:87008ms step_avg:142.64ms
step:621/1370 train_time:87155ms step_avg:142.64ms
step:622/1370 train_time:87306ms step_avg:142.66ms
step:623/1370 train_time:87453ms step_avg:142.66ms
step:624/1370 train_time:87602ms step_avg:142.67ms
step:625/1370 train_time:87750ms step_avg:142.68ms
step:625/1370 val_loss:3.5731 train_time:87824ms step_avg:142.80ms
step:626/1370 train_time:87901ms step_avg:142.70ms
step:627/1370 train_time:88052ms step_avg:142.71ms
step:628/1370 train_time:88200ms step_avg:142.72ms
step:629/1370 train_time:88349ms step_avg:142.73ms
step:630/1370 train_time:88498ms step_avg:142.74ms
step:631/1370 train_time:88644ms step_avg:142.74ms
step:632/1370 train_time:88795ms step_avg:142.76ms
step:633/1370 train_time:88944ms step_avg:142.77ms
step:634/1370 train_time:89094ms step_avg:142.78ms
step:635/1370 train_time:89240ms step_avg:142.78ms
step:636/1370 train_time:89390ms step_avg:142.80ms
step:637/1370 train_time:89539ms step_avg:142.81ms
step:638/1370 train_time:89685ms step_avg:142.81ms
step:639/1370 train_time:89835ms step_avg:142.82ms
step:640/1370 train_time:89984ms step_avg:142.83ms
step:641/1370 train_time:90135ms step_avg:142.84ms
step:642/1370 train_time:90283ms step_avg:142.85ms
step:643/1370 train_time:90434ms step_avg:142.86ms
step:644/1370 train_time:90580ms step_avg:142.87ms
step:645/1370 train_time:90730ms step_avg:142.88ms
step:646/1370 train_time:90879ms step_avg:142.89ms
step:647/1370 train_time:91028ms step_avg:142.90ms
step:648/1370 train_time:91181ms step_avg:142.92ms
step:649/1370 train_time:91331ms step_avg:142.93ms
step:650/1370 train_time:91480ms step_avg:142.94ms
step:651/1370 train_time:91630ms step_avg:142.95ms
step:652/1370 train_time:91780ms step_avg:142.96ms
step:653/1370 train_time:91928ms step_avg:142.97ms
step:654/1370 train_time:92080ms step_avg:142.98ms
step:655/1370 train_time:92227ms step_avg:142.99ms
step:656/1370 train_time:92378ms step_avg:143.00ms
step:657/1370 train_time:92526ms step_avg:143.01ms
step:658/1370 train_time:92677ms step_avg:143.02ms
step:659/1370 train_time:92826ms step_avg:143.03ms
step:660/1370 train_time:92975ms step_avg:143.04ms
step:661/1370 train_time:93123ms step_avg:143.05ms
step:662/1370 train_time:93273ms step_avg:143.06ms
step:663/1370 train_time:93420ms step_avg:143.06ms
step:664/1370 train_time:93570ms step_avg:143.07ms
step:665/1370 train_time:93719ms step_avg:143.08ms
step:666/1370 train_time:93866ms step_avg:143.09ms
step:667/1370 train_time:94017ms step_avg:143.10ms
step:668/1370 train_time:94165ms step_avg:143.11ms
step:669/1370 train_time:94317ms step_avg:143.12ms
step:670/1370 train_time:94465ms step_avg:143.13ms
step:671/1370 train_time:94616ms step_avg:143.14ms
step:672/1370 train_time:94762ms step_avg:143.15ms
step:673/1370 train_time:94914ms step_avg:143.16ms
step:674/1370 train_time:95061ms step_avg:143.16ms
step:675/1370 train_time:95212ms step_avg:143.18ms
step:676/1370 train_time:95362ms step_avg:143.19ms
step:677/1370 train_time:95511ms step_avg:143.19ms
step:678/1370 train_time:95658ms step_avg:143.20ms
step:679/1370 train_time:95807ms step_avg:143.21ms
step:680/1370 train_time:95957ms step_avg:143.22ms
step:681/1370 train_time:96106ms step_avg:143.23ms
step:682/1370 train_time:96255ms step_avg:143.24ms
step:683/1370 train_time:96403ms step_avg:143.24ms
step:684/1370 train_time:96551ms step_avg:143.25ms
step:685/1370 train_time:96701ms step_avg:143.26ms
step:686/1370 train_time:96848ms step_avg:143.27ms
step:687/1370 train_time:96997ms step_avg:143.28ms
step:688/1370 train_time:97146ms step_avg:143.28ms
step:689/1370 train_time:97295ms step_avg:143.29ms
step:690/1370 train_time:97444ms step_avg:143.30ms
step:691/1370 train_time:97594ms step_avg:143.31ms
step:692/1370 train_time:97741ms step_avg:143.32ms
step:693/1370 train_time:97889ms step_avg:143.32ms
step:694/1370 train_time:98038ms step_avg:143.33ms
step:695/1370 train_time:98185ms step_avg:143.34ms
step:696/1370 train_time:98338ms step_avg:143.35ms
step:697/1370 train_time:98487ms step_avg:143.36ms
step:698/1370 train_time:98636ms step_avg:143.37ms
step:699/1370 train_time:98783ms step_avg:143.37ms
step:700/1370 train_time:98933ms step_avg:143.38ms
step:701/1370 train_time:99081ms step_avg:143.39ms
step:702/1370 train_time:99234ms step_avg:143.40ms
step:703/1370 train_time:99381ms step_avg:143.41ms
step:704/1370 train_time:99533ms step_avg:143.42ms
step:705/1370 train_time:99681ms step_avg:143.43ms
step:706/1370 train_time:99834ms step_avg:143.44ms
step:707/1370 train_time:99981ms step_avg:143.44ms
step:708/1370 train_time:100129ms step_avg:143.45ms
step:709/1370 train_time:100279ms step_avg:143.46ms
step:710/1370 train_time:100427ms step_avg:143.47ms
step:711/1370 train_time:100579ms step_avg:143.48ms
step:712/1370 train_time:100730ms step_avg:143.49ms
step:713/1370 train_time:100881ms step_avg:143.50ms
step:714/1370 train_time:101032ms step_avg:143.51ms
step:715/1370 train_time:101182ms step_avg:143.52ms
step:716/1370 train_time:101336ms step_avg:143.53ms
step:717/1370 train_time:101484ms step_avg:143.54ms
step:718/1370 train_time:101634ms step_avg:143.55ms
step:719/1370 train_time:101782ms step_avg:143.56ms
step:720/1370 train_time:101935ms step_avg:143.57ms
step:721/1370 train_time:102084ms step_avg:143.58ms
step:722/1370 train_time:102238ms step_avg:143.59ms
step:723/1370 train_time:102387ms step_avg:143.60ms
step:724/1370 train_time:102540ms step_avg:143.61ms
step:725/1370 train_time:102689ms step_avg:143.62ms
step:726/1370 train_time:102839ms step_avg:143.63ms
step:727/1370 train_time:102990ms step_avg:143.64ms
step:728/1370 train_time:103140ms step_avg:143.65ms
step:729/1370 train_time:103288ms step_avg:143.66ms
step:730/1370 train_time:103440ms step_avg:143.67ms
step:731/1370 train_time:103590ms step_avg:143.68ms
step:732/1370 train_time:103739ms step_avg:143.68ms
step:733/1370 train_time:103888ms step_avg:143.69ms
step:734/1370 train_time:104040ms step_avg:143.70ms
step:735/1370 train_time:104192ms step_avg:143.71ms
step:736/1370 train_time:104341ms step_avg:143.72ms
step:737/1370 train_time:104493ms step_avg:143.73ms
step:738/1370 train_time:104642ms step_avg:143.74ms
step:739/1370 train_time:104794ms step_avg:143.75ms
step:740/1370 train_time:104943ms step_avg:143.76ms
step:741/1370 train_time:105095ms step_avg:143.77ms
step:742/1370 train_time:105244ms step_avg:143.78ms
step:743/1370 train_time:105396ms step_avg:143.79ms
step:744/1370 train_time:105544ms step_avg:143.79ms
step:745/1370 train_time:105698ms step_avg:143.81ms
step:746/1370 train_time:105845ms step_avg:143.81ms
step:747/1370 train_time:105996ms step_avg:143.82ms
step:748/1370 train_time:106145ms step_avg:143.83ms
step:749/1370 train_time:106298ms step_avg:143.84ms
step:750/1370 train_time:106447ms step_avg:143.85ms
step:750/1370 val_loss:3.5201 train_time:106524ms step_avg:143.95ms
step:751/1370 train_time:106600ms step_avg:143.86ms
step:752/1370 train_time:106754ms step_avg:143.87ms
step:753/1370 train_time:106902ms step_avg:143.88ms
step:754/1370 train_time:107052ms step_avg:143.89ms
step:755/1370 train_time:107201ms step_avg:143.89ms
step:756/1370 train_time:107352ms step_avg:143.90ms
step:757/1370 train_time:107502ms step_avg:143.91ms
step:758/1370 train_time:107655ms step_avg:143.92ms
step:759/1370 train_time:107805ms step_avg:143.93ms
step:760/1370 train_time:107954ms step_avg:143.94ms
step:761/1370 train_time:108138ms step_avg:143.99ms
step:762/1370 train_time:108289ms step_avg:144.00ms
step:763/1370 train_time:108438ms step_avg:144.01ms
step:764/1370 train_time:108589ms step_avg:144.02ms
step:765/1370 train_time:108736ms step_avg:144.02ms
step:766/1370 train_time:108889ms step_avg:144.03ms
step:767/1370 train_time:109039ms step_avg:144.04ms
step:768/1370 train_time:109193ms step_avg:144.05ms
step:769/1370 train_time:109342ms step_avg:144.06ms
step:770/1370 train_time:109494ms step_avg:144.07ms
step:771/1370 train_time:109643ms step_avg:144.08ms
step:772/1370 train_time:109793ms step_avg:144.09ms
step:773/1370 train_time:109943ms step_avg:144.09ms
step:774/1370 train_time:110094ms step_avg:144.10ms
step:775/1370 train_time:110244ms step_avg:144.11ms
step:776/1370 train_time:110395ms step_avg:144.12ms
step:777/1370 train_time:110547ms step_avg:144.13ms
step:778/1370 train_time:110695ms step_avg:144.13ms
step:779/1370 train_time:110845ms step_avg:144.14ms
step:780/1370 train_time:110995ms step_avg:144.15ms
step:781/1370 train_time:111150ms step_avg:144.16ms
step:782/1370 train_time:111298ms step_avg:144.17ms
step:783/1370 train_time:111450ms step_avg:144.18ms
step:784/1370 train_time:111599ms step_avg:144.18ms
step:785/1370 train_time:111750ms step_avg:144.19ms
step:786/1370 train_time:111900ms step_avg:144.20ms
step:787/1370 train_time:112052ms step_avg:144.21ms
step:788/1370 train_time:112200ms step_avg:144.22ms
step:789/1370 train_time:112353ms step_avg:144.23ms
step:790/1370 train_time:112501ms step_avg:144.23ms
step:791/1370 train_time:112652ms step_avg:144.24ms
step:792/1370 train_time:112802ms step_avg:144.25ms
step:793/1370 train_time:112952ms step_avg:144.26ms
step:794/1370 train_time:113101ms step_avg:144.26ms
step:795/1370 train_time:113255ms step_avg:144.27ms
step:796/1370 train_time:113408ms step_avg:144.28ms
step:797/1370 train_time:113558ms step_avg:144.29ms
step:798/1370 train_time:113709ms step_avg:144.30ms
step:799/1370 train_time:113862ms step_avg:144.31ms
step:800/1370 train_time:114012ms step_avg:144.32ms
step:801/1370 train_time:114160ms step_avg:144.32ms
step:802/1370 train_time:114312ms step_avg:144.33ms
step:803/1370 train_time:114460ms step_avg:144.34ms
step:804/1370 train_time:114609ms step_avg:144.34ms
step:805/1370 train_time:114764ms step_avg:144.36ms
step:806/1370 train_time:114913ms step_avg:144.36ms
step:807/1370 train_time:115062ms step_avg:144.37ms
step:808/1370 train_time:115212ms step_avg:144.38ms
step:809/1370 train_time:115362ms step_avg:144.38ms
step:810/1370 train_time:115512ms step_avg:144.39ms
step:811/1370 train_time:115664ms step_avg:144.40ms
step:812/1370 train_time:115814ms step_avg:144.41ms
step:813/1370 train_time:115963ms step_avg:144.41ms
step:814/1370 train_time:116115ms step_avg:144.42ms
step:815/1370 train_time:116266ms step_avg:144.43ms
step:816/1370 train_time:116419ms step_avg:144.44ms
step:817/1370 train_time:116573ms step_avg:144.45ms
step:818/1370 train_time:116722ms step_avg:144.46ms
step:819/1370 train_time:116875ms step_avg:144.47ms
step:820/1370 train_time:117029ms step_avg:144.48ms
step:821/1370 train_time:117177ms step_avg:144.48ms
step:822/1370 train_time:117329ms step_avg:144.49ms
step:823/1370 train_time:117478ms step_avg:144.50ms
step:824/1370 train_time:117632ms step_avg:144.51ms
step:825/1370 train_time:117784ms step_avg:144.52ms
step:826/1370 train_time:117937ms step_avg:144.53ms
step:827/1370 train_time:118086ms step_avg:144.54ms
step:828/1370 train_time:118239ms step_avg:144.55ms
step:829/1370 train_time:118392ms step_avg:144.56ms
step:830/1370 train_time:118542ms step_avg:144.56ms
step:831/1370 train_time:118695ms step_avg:144.57ms
step:832/1370 train_time:118849ms step_avg:144.59ms
step:833/1370 train_time:118999ms step_avg:144.59ms
step:834/1370 train_time:119151ms step_avg:144.60ms
step:835/1370 train_time:119301ms step_avg:144.61ms
step:836/1370 train_time:119455ms step_avg:144.62ms
step:837/1370 train_time:119607ms step_avg:144.63ms
step:838/1370 train_time:119758ms step_avg:144.64ms
step:839/1370 train_time:119910ms step_avg:144.64ms
step:840/1370 train_time:120060ms step_avg:144.65ms
step:841/1370 train_time:120210ms step_avg:144.66ms
step:842/1370 train_time:120364ms step_avg:144.67ms
step:843/1370 train_time:120513ms step_avg:144.67ms
step:844/1370 train_time:120664ms step_avg:144.68ms
step:845/1370 train_time:120815ms step_avg:144.69ms
step:846/1370 train_time:120968ms step_avg:144.70ms
step:847/1370 train_time:121120ms step_avg:144.71ms
step:848/1370 train_time:121272ms step_avg:144.72ms
step:849/1370 train_time:121424ms step_avg:144.72ms
step:850/1370 train_time:121577ms step_avg:144.73ms
step:851/1370 train_time:121732ms step_avg:144.75ms
step:852/1370 train_time:121884ms step_avg:144.75ms
step:853/1370 train_time:122034ms step_avg:144.76ms
step:854/1370 train_time:122185ms step_avg:144.77ms
step:855/1370 train_time:122335ms step_avg:144.78ms
step:856/1370 train_time:122485ms step_avg:144.78ms
step:857/1370 train_time:122637ms step_avg:144.79ms
step:858/1370 train_time:122795ms step_avg:144.80ms
step:859/1370 train_time:122945ms step_avg:144.81ms
step:860/1370 train_time:123096ms step_avg:144.82ms
step:861/1370 train_time:123249ms step_avg:144.83ms
step:862/1370 train_time:123400ms step_avg:144.84ms
step:863/1370 train_time:123553ms step_avg:144.85ms
step:864/1370 train_time:123704ms step_avg:144.85ms
step:865/1370 train_time:123855ms step_avg:144.86ms
step:866/1370 train_time:124010ms step_avg:144.87ms
step:867/1370 train_time:124160ms step_avg:144.88ms
step:868/1370 train_time:124312ms step_avg:144.89ms
step:869/1370 train_time:124465ms step_avg:144.90ms
step:870/1370 train_time:124619ms step_avg:144.91ms
step:871/1370 train_time:124771ms step_avg:144.91ms
step:872/1370 train_time:124921ms step_avg:144.92ms
step:873/1370 train_time:125073ms step_avg:144.93ms
step:874/1370 train_time:125223ms step_avg:144.93ms
step:875/1370 train_time:125375ms step_avg:144.94ms
step:875/1370 val_loss:3.4656 train_time:125449ms step_avg:145.03ms
step:876/1370 train_time:125527ms step_avg:144.95ms
step:877/1370 train_time:125681ms step_avg:144.96ms
step:878/1370 train_time:125832ms step_avg:144.97ms
step:879/1370 train_time:125984ms step_avg:144.98ms
step:880/1370 train_time:126136ms step_avg:144.98ms
step:881/1370 train_time:126285ms step_avg:144.99ms
step:882/1370 train_time:126440ms step_avg:145.00ms
step:883/1370 train_time:126591ms step_avg:145.01ms
step:884/1370 train_time:126744ms step_avg:145.02ms
step:885/1370 train_time:126895ms step_avg:145.02ms
step:886/1370 train_time:127050ms step_avg:145.03ms
step:887/1370 train_time:127202ms step_avg:145.04ms
step:888/1370 train_time:127355ms step_avg:145.05ms
step:889/1370 train_time:127508ms step_avg:145.06ms
step:890/1370 train_time:127661ms step_avg:145.07ms
step:891/1370 train_time:127813ms step_avg:145.08ms
step:892/1370 train_time:127965ms step_avg:145.08ms
step:893/1370 train_time:128117ms step_avg:145.09ms
step:894/1370 train_time:128270ms step_avg:145.10ms
step:895/1370 train_time:128425ms step_avg:145.11ms
step:896/1370 train_time:128578ms step_avg:145.12ms
step:897/1370 train_time:128727ms step_avg:145.13ms
step:898/1370 train_time:128881ms step_avg:145.14ms
step:899/1370 train_time:129029ms step_avg:145.14ms
step:900/1370 train_time:129181ms step_avg:145.15ms
step:901/1370 train_time:129335ms step_avg:145.16ms
step:902/1370 train_time:129485ms step_avg:145.16ms
step:903/1370 train_time:129639ms step_avg:145.17ms
step:904/1370 train_time:129791ms step_avg:145.18ms
step:905/1370 train_time:129943ms step_avg:145.19ms
step:906/1370 train_time:130095ms step_avg:145.20ms
step:907/1370 train_time:130250ms step_avg:145.21ms
step:908/1370 train_time:130403ms step_avg:145.21ms
step:909/1370 train_time:130555ms step_avg:145.22ms
step:910/1370 train_time:130715ms step_avg:145.24ms
step:911/1370 train_time:130865ms step_avg:145.24ms
step:912/1370 train_time:131018ms step_avg:145.25ms
step:913/1370 train_time:131171ms step_avg:145.26ms
step:914/1370 train_time:131322ms step_avg:145.27ms
step:915/1370 train_time:131476ms step_avg:145.28ms
step:916/1370 train_time:131629ms step_avg:145.29ms
step:917/1370 train_time:131782ms step_avg:145.29ms
step:918/1370 train_time:131934ms step_avg:145.30ms
step:919/1370 train_time:132093ms step_avg:145.32ms
step:920/1370 train_time:132246ms step_avg:145.32ms
step:921/1370 train_time:132398ms step_avg:145.33ms
step:922/1370 train_time:132552ms step_avg:145.34ms
step:923/1370 train_time:132703ms step_avg:145.35ms
step:924/1370 train_time:132857ms step_avg:145.36ms
step:925/1370 train_time:133010ms step_avg:145.37ms
step:926/1370 train_time:133164ms step_avg:145.38ms
step:927/1370 train_time:133318ms step_avg:145.39ms
step:928/1370 train_time:133470ms step_avg:145.39ms
step:929/1370 train_time:133625ms step_avg:145.40ms
step:930/1370 train_time:133781ms step_avg:145.41ms
step:931/1370 train_time:133932ms step_avg:145.42ms
step:932/1370 train_time:134084ms step_avg:145.43ms
step:933/1370 train_time:134237ms step_avg:145.44ms
step:934/1370 train_time:134388ms step_avg:145.44ms
step:935/1370 train_time:134543ms step_avg:145.45ms
step:936/1370 train_time:134695ms step_avg:145.46ms
step:937/1370 train_time:134851ms step_avg:145.47ms
step:938/1370 train_time:135005ms step_avg:145.48ms
step:939/1370 train_time:135159ms step_avg:145.49ms
step:940/1370 train_time:135310ms step_avg:145.49ms
step:941/1370 train_time:135464ms step_avg:145.50ms
step:942/1370 train_time:135615ms step_avg:145.51ms
step:943/1370 train_time:135770ms step_avg:145.52ms
step:944/1370 train_time:135930ms step_avg:145.54ms
step:945/1370 train_time:136083ms step_avg:145.54ms
step:946/1370 train_time:136237ms step_avg:145.55ms
step:947/1370 train_time:136389ms step_avg:145.56ms
step:948/1370 train_time:136544ms step_avg:145.57ms
step:949/1370 train_time:136701ms step_avg:145.58ms
step:950/1370 train_time:136852ms step_avg:145.59ms
step:951/1370 train_time:137059ms step_avg:145.65ms
step:952/1370 train_time:137209ms step_avg:145.66ms
step:953/1370 train_time:137361ms step_avg:145.66ms
step:954/1370 train_time:137514ms step_avg:145.67ms
step:955/1370 train_time:137664ms step_avg:145.68ms
step:956/1370 train_time:137819ms step_avg:145.69ms
step:957/1370 train_time:137972ms step_avg:145.69ms
step:958/1370 train_time:138131ms step_avg:145.71ms
step:959/1370 train_time:138286ms step_avg:145.72ms
step:960/1370 train_time:138440ms step_avg:145.73ms
step:961/1370 train_time:138592ms step_avg:145.73ms
step:962/1370 train_time:138744ms step_avg:145.74ms
step:963/1370 train_time:138903ms step_avg:145.75ms
step:964/1370 train_time:139054ms step_avg:145.76ms
step:965/1370 train_time:139206ms step_avg:145.77ms
step:966/1370 train_time:139357ms step_avg:145.77ms
step:967/1370 train_time:139509ms step_avg:145.78ms
step:968/1370 train_time:139659ms step_avg:145.78ms
step:969/1370 train_time:139816ms step_avg:145.79ms
step:970/1370 train_time:139967ms step_avg:145.80ms
step:971/1370 train_time:140122ms step_avg:145.81ms
step:972/1370 train_time:140274ms step_avg:145.81ms
step:973/1370 train_time:140424ms step_avg:145.82ms
step:974/1370 train_time:140579ms step_avg:145.83ms
step:975/1370 train_time:140732ms step_avg:145.84ms
step:976/1370 train_time:140884ms step_avg:145.84ms
step:977/1370 train_time:141039ms step_avg:145.85ms
step:978/1370 train_time:141191ms step_avg:145.86ms
step:979/1370 train_time:141343ms step_avg:145.86ms
step:980/1370 train_time:141494ms step_avg:145.87ms
step:981/1370 train_time:141644ms step_avg:145.87ms
step:982/1370 train_time:141798ms step_avg:145.88ms
step:983/1370 train_time:141948ms step_avg:145.89ms
step:984/1370 train_time:142101ms step_avg:145.89ms
step:985/1370 train_time:142254ms step_avg:145.90ms
step:986/1370 train_time:142408ms step_avg:145.91ms
step:987/1370 train_time:142558ms step_avg:145.91ms
step:988/1370 train_time:142711ms step_avg:145.92ms
step:989/1370 train_time:142862ms step_avg:145.93ms
step:990/1370 train_time:143018ms step_avg:145.94ms
step:991/1370 train_time:143168ms step_avg:145.94ms
step:992/1370 train_time:143324ms step_avg:145.95ms
step:993/1370 train_time:143487ms step_avg:145.97ms
step:994/1370 train_time:143640ms step_avg:145.98ms
step:995/1370 train_time:143789ms step_avg:145.98ms
step:996/1370 train_time:143942ms step_avg:145.99ms
step:997/1370 train_time:144094ms step_avg:145.99ms
step:998/1370 train_time:144246ms step_avg:146.00ms
step:999/1370 train_time:144400ms step_avg:146.01ms
step:1000/1370 train_time:144551ms step_avg:146.01ms
step:1000/1370 val_loss:3.4007 train_time:144627ms step_avg:146.09ms
step:1001/1370 train_time:144704ms step_avg:146.02ms
step:1002/1370 train_time:144861ms step_avg:146.03ms
step:1003/1370 train_time:145015ms step_avg:146.04ms
step:1004/1370 train_time:145170ms step_avg:146.05ms
step:1005/1370 train_time:145323ms step_avg:146.05ms
step:1006/1370 train_time:145473ms step_avg:146.06ms
step:1007/1370 train_time:145631ms step_avg:146.07ms
step:1008/1370 train_time:145786ms step_avg:146.08ms
step:1009/1370 train_time:145946ms step_avg:146.09ms
step:1010/1370 train_time:146095ms step_avg:146.10ms
step:1011/1370 train_time:146248ms step_avg:146.10ms
step:1012/1370 train_time:146399ms step_avg:146.11ms
step:1013/1370 train_time:146551ms step_avg:146.11ms
step:1014/1370 train_time:146705ms step_avg:146.12ms
step:1015/1370 train_time:146860ms step_avg:146.13ms
step:1016/1370 train_time:147012ms step_avg:146.13ms
step:1017/1370 train_time:147167ms step_avg:146.14ms
step:1018/1370 train_time:147319ms step_avg:146.15ms
step:1019/1370 train_time:147472ms step_avg:146.16ms
step:1020/1370 train_time:147628ms step_avg:146.17ms
step:1021/1370 train_time:147784ms step_avg:146.18ms
step:1022/1370 train_time:147937ms step_avg:146.18ms
step:1023/1370 train_time:148090ms step_avg:146.19ms
step:1024/1370 train_time:148244ms step_avg:146.20ms
step:1025/1370 train_time:148397ms step_avg:146.20ms
step:1026/1370 train_time:148549ms step_avg:146.21ms
step:1027/1370 train_time:148702ms step_avg:146.22ms
step:1028/1370 train_time:148857ms step_avg:146.22ms
step:1029/1370 train_time:149012ms step_avg:146.23ms
step:1030/1370 train_time:149167ms step_avg:146.24ms
step:1031/1370 train_time:149318ms step_avg:146.25ms
step:1032/1370 train_time:149470ms step_avg:146.25ms
step:1033/1370 train_time:149625ms step_avg:146.26ms
step:1034/1370 train_time:149778ms step_avg:146.27ms
step:1035/1370 train_time:149933ms step_avg:146.28ms
step:1036/1370 train_time:150089ms step_avg:146.29ms
step:1037/1370 train_time:150247ms step_avg:146.30ms
step:1038/1370 train_time:150401ms step_avg:146.30ms
step:1039/1370 train_time:150554ms step_avg:146.31ms
step:1040/1370 train_time:150707ms step_avg:146.32ms
step:1041/1370 train_time:150863ms step_avg:146.33ms
step:1042/1370 train_time:151012ms step_avg:146.33ms
step:1043/1370 train_time:151167ms step_avg:146.34ms
step:1044/1370 train_time:151321ms step_avg:146.35ms
step:1045/1370 train_time:151476ms step_avg:146.35ms
step:1046/1370 train_time:151627ms step_avg:146.36ms
step:1047/1370 train_time:151781ms step_avg:146.37ms
step:1048/1370 train_time:151936ms step_avg:146.37ms
step:1049/1370 train_time:152093ms step_avg:146.38ms
step:1050/1370 train_time:152250ms step_avg:146.39ms
step:1051/1370 train_time:152405ms step_avg:146.40ms
step:1052/1370 train_time:152559ms step_avg:146.41ms
step:1053/1370 train_time:152710ms step_avg:146.41ms
step:1054/1370 train_time:152866ms step_avg:146.42ms
step:1055/1370 train_time:153018ms step_avg:146.43ms
step:1056/1370 train_time:153173ms step_avg:146.44ms
step:1057/1370 train_time:153329ms step_avg:146.45ms
step:1058/1370 train_time:153487ms step_avg:146.46ms
step:1059/1370 train_time:153643ms step_avg:146.47ms
step:1060/1370 train_time:153796ms step_avg:146.47ms
step:1061/1370 train_time:153948ms step_avg:146.48ms
step:1062/1370 train_time:154103ms step_avg:146.49ms
step:1063/1370 train_time:154259ms step_avg:146.49ms
step:1064/1370 train_time:154412ms step_avg:146.50ms
step:1065/1370 train_time:154569ms step_avg:146.51ms
step:1066/1370 train_time:154728ms step_avg:146.52ms
step:1067/1370 train_time:154886ms step_avg:146.53ms
step:1068/1370 train_time:155037ms step_avg:146.54ms
step:1069/1370 train_time:155196ms step_avg:146.55ms
step:1070/1370 train_time:155348ms step_avg:146.56ms
step:1071/1370 train_time:155505ms step_avg:146.56ms
step:1072/1370 train_time:155658ms step_avg:146.57ms
step:1073/1370 train_time:155810ms step_avg:146.58ms
step:1074/1370 train_time:155964ms step_avg:146.58ms
step:1075/1370 train_time:156118ms step_avg:146.59ms
step:1076/1370 train_time:156270ms step_avg:146.59ms
step:1077/1370 train_time:156424ms step_avg:146.60ms
step:1078/1370 train_time:156584ms step_avg:146.61ms
step:1079/1370 train_time:156742ms step_avg:146.63ms
step:1080/1370 train_time:156897ms step_avg:146.63ms
step:1081/1370 train_time:157051ms step_avg:146.64ms
step:1082/1370 train_time:157204ms step_avg:146.65ms
step:1083/1370 train_time:157358ms step_avg:146.65ms
step:1084/1370 train_time:157516ms step_avg:146.66ms
step:1085/1370 train_time:157669ms step_avg:146.67ms
step:1086/1370 train_time:157825ms step_avg:146.68ms
step:1087/1370 train_time:157982ms step_avg:146.69ms
step:1088/1370 train_time:158135ms step_avg:146.69ms
step:1089/1370 train_time:158293ms step_avg:146.70ms
step:1090/1370 train_time:158453ms step_avg:146.72ms
step:1091/1370 train_time:158607ms step_avg:146.72ms
step:1092/1370 train_time:158761ms step_avg:146.73ms
step:1093/1370 train_time:158915ms step_avg:146.74ms
step:1094/1370 train_time:159068ms step_avg:146.74ms
step:1095/1370 train_time:159221ms step_avg:146.75ms
step:1096/1370 train_time:159376ms step_avg:146.76ms
step:1097/1370 train_time:159531ms step_avg:146.76ms
step:1098/1370 train_time:159685ms step_avg:146.77ms
step:1099/1370 train_time:159836ms step_avg:146.77ms
step:1100/1370 train_time:159988ms step_avg:146.78ms
step:1101/1370 train_time:160142ms step_avg:146.78ms
step:1102/1370 train_time:160296ms step_avg:146.79ms
step:1103/1370 train_time:160450ms step_avg:146.80ms
step:1104/1370 train_time:160603ms step_avg:146.80ms
step:1105/1370 train_time:160761ms step_avg:146.81ms
step:1106/1370 train_time:160912ms step_avg:146.82ms
step:1107/1370 train_time:161067ms step_avg:146.83ms
step:1108/1370 train_time:161225ms step_avg:146.83ms
step:1109/1370 train_time:161377ms step_avg:146.84ms
step:1110/1370 train_time:161530ms step_avg:146.85ms
step:1111/1370 train_time:161687ms step_avg:146.86ms
step:1112/1370 train_time:161840ms step_avg:146.86ms
step:1113/1370 train_time:161995ms step_avg:146.87ms
step:1114/1370 train_time:162153ms step_avg:146.88ms
step:1115/1370 train_time:162307ms step_avg:146.88ms
step:1116/1370 train_time:162458ms step_avg:146.89ms
step:1117/1370 train_time:162612ms step_avg:146.89ms
step:1118/1370 train_time:162772ms step_avg:146.91ms
step:1119/1370 train_time:162927ms step_avg:146.91ms
step:1120/1370 train_time:163081ms step_avg:146.92ms
step:1121/1370 train_time:163235ms step_avg:146.93ms
step:1122/1370 train_time:163390ms step_avg:146.93ms
step:1123/1370 train_time:163544ms step_avg:146.94ms
step:1124/1370 train_time:163700ms step_avg:146.95ms
step:1125/1370 train_time:163858ms step_avg:146.96ms
step:1125/1370 val_loss:3.3473 train_time:163937ms step_avg:147.03ms
step:1126/1370 train_time:164014ms step_avg:146.97ms
step:1127/1370 train_time:164169ms step_avg:146.97ms
step:1128/1370 train_time:164324ms step_avg:146.98ms
step:1129/1370 train_time:164484ms step_avg:146.99ms
step:1130/1370 train_time:164637ms step_avg:147.00ms
step:1131/1370 train_time:164794ms step_avg:147.01ms
step:1132/1370 train_time:164947ms step_avg:147.01ms
step:1133/1370 train_time:165101ms step_avg:147.02ms
step:1134/1370 train_time:165255ms step_avg:147.02ms
step:1135/1370 train_time:165409ms step_avg:147.03ms
step:1136/1370 train_time:165568ms step_avg:147.04ms
step:1137/1370 train_time:165720ms step_avg:147.05ms
step:1138/1370 train_time:165876ms step_avg:147.05ms
step:1139/1370 train_time:166030ms step_avg:147.06ms
step:1140/1370 train_time:166185ms step_avg:147.07ms
step:1141/1370 train_time:166378ms step_avg:147.11ms
step:1142/1370 train_time:166531ms step_avg:147.11ms
step:1143/1370 train_time:166692ms step_avg:147.12ms
step:1144/1370 train_time:166846ms step_avg:147.13ms
step:1145/1370 train_time:166996ms step_avg:147.13ms
step:1146/1370 train_time:167153ms step_avg:147.14ms
step:1147/1370 train_time:167308ms step_avg:147.15ms
step:1148/1370 train_time:167463ms step_avg:147.16ms
step:1149/1370 train_time:167619ms step_avg:147.16ms
step:1150/1370 train_time:167773ms step_avg:147.17ms
step:1151/1370 train_time:167928ms step_avg:147.18ms
step:1152/1370 train_time:168082ms step_avg:147.18ms
step:1153/1370 train_time:168241ms step_avg:147.19ms
step:1154/1370 train_time:168394ms step_avg:147.20ms
step:1155/1370 train_time:168548ms step_avg:147.20ms
step:1156/1370 train_time:168707ms step_avg:147.21ms
step:1157/1370 train_time:168864ms step_avg:147.22ms
step:1158/1370 train_time:169019ms step_avg:147.23ms
step:1159/1370 train_time:169175ms step_avg:147.24ms
step:1160/1370 train_time:169327ms step_avg:147.24ms
step:1161/1370 train_time:169484ms step_avg:147.25ms
step:1162/1370 train_time:169638ms step_avg:147.26ms
step:1163/1370 train_time:169795ms step_avg:147.26ms
step:1164/1370 train_time:169950ms step_avg:147.27ms
step:1165/1370 train_time:170103ms step_avg:147.28ms
step:1166/1370 train_time:170259ms step_avg:147.28ms
step:1167/1370 train_time:170414ms step_avg:147.29ms
step:1168/1370 train_time:170568ms step_avg:147.30ms
step:1169/1370 train_time:170723ms step_avg:147.30ms
step:1170/1370 train_time:170877ms step_avg:147.31ms
step:1171/1370 train_time:171031ms step_avg:147.31ms
step:1172/1370 train_time:171185ms step_avg:147.32ms
step:1173/1370 train_time:171341ms step_avg:147.33ms
step:1174/1370 train_time:171504ms step_avg:147.34ms
step:1175/1370 train_time:171661ms step_avg:147.35ms
step:1176/1370 train_time:171822ms step_avg:147.36ms
step:1177/1370 train_time:171985ms step_avg:147.37ms
step:1178/1370 train_time:172138ms step_avg:147.38ms
step:1179/1370 train_time:172295ms step_avg:147.39ms
step:1180/1370 train_time:172455ms step_avg:147.40ms
step:1181/1370 train_time:172608ms step_avg:147.40ms
step:1182/1370 train_time:172759ms step_avg:147.41ms
step:1183/1370 train_time:172915ms step_avg:147.41ms
step:1184/1370 train_time:173071ms step_avg:147.42ms
step:1185/1370 train_time:173230ms step_avg:147.43ms
step:1186/1370 train_time:173384ms step_avg:147.44ms
step:1187/1370 train_time:173546ms step_avg:147.45ms
step:1188/1370 train_time:173698ms step_avg:147.45ms
step:1189/1370 train_time:173858ms step_avg:147.46ms
step:1190/1370 train_time:174014ms step_avg:147.47ms
step:1191/1370 train_time:174171ms step_avg:147.48ms
step:1192/1370 train_time:174322ms step_avg:147.48ms
step:1193/1370 train_time:174476ms step_avg:147.49ms
step:1194/1370 train_time:174630ms step_avg:147.49ms
step:1195/1370 train_time:174785ms step_avg:147.50ms
step:1196/1370 train_time:174941ms step_avg:147.51ms
step:1197/1370 train_time:175098ms step_avg:147.51ms
step:1198/1370 train_time:175258ms step_avg:147.52ms
step:1199/1370 train_time:175413ms step_avg:147.53ms
step:1200/1370 train_time:175567ms step_avg:147.53ms
step:1201/1370 train_time:175720ms step_avg:147.54ms
step:1202/1370 train_time:175889ms step_avg:147.56ms
step:1203/1370 train_time:176048ms step_avg:147.57ms
step:1204/1370 train_time:176203ms step_avg:147.57ms
step:1205/1370 train_time:176358ms step_avg:147.58ms
step:1206/1370 train_time:176514ms step_avg:147.59ms
step:1207/1370 train_time:176668ms step_avg:147.59ms
step:1208/1370 train_time:176824ms step_avg:147.60ms
step:1209/1370 train_time:176979ms step_avg:147.61ms
step:1210/1370 train_time:177138ms step_avg:147.61ms
step:1211/1370 train_time:177294ms step_avg:147.62ms
step:1212/1370 train_time:177449ms step_avg:147.63ms
step:1213/1370 train_time:177603ms step_avg:147.63ms
step:1214/1370 train_time:177761ms step_avg:147.64ms
step:1215/1370 train_time:177918ms step_avg:147.65ms
step:1216/1370 train_time:178071ms step_avg:147.65ms
step:1217/1370 train_time:178225ms step_avg:147.66ms
step:1218/1370 train_time:178381ms step_avg:147.67ms
step:1219/1370 train_time:178535ms step_avg:147.67ms
step:1220/1370 train_time:178689ms step_avg:147.68ms
step:1221/1370 train_time:178843ms step_avg:147.68ms
step:1222/1370 train_time:178999ms step_avg:147.69ms
step:1223/1370 train_time:179156ms step_avg:147.70ms
step:1224/1370 train_time:179314ms step_avg:147.71ms
step:1225/1370 train_time:179472ms step_avg:147.71ms
step:1226/1370 train_time:179627ms step_avg:147.72ms
step:1227/1370 train_time:179783ms step_avg:147.73ms
step:1228/1370 train_time:179938ms step_avg:147.73ms
step:1229/1370 train_time:180095ms step_avg:147.74ms
step:1230/1370 train_time:180255ms step_avg:147.75ms
step:1231/1370 train_time:180414ms step_avg:147.76ms
step:1232/1370 train_time:180571ms step_avg:147.77ms
step:1233/1370 train_time:180726ms step_avg:147.77ms
step:1234/1370 train_time:180881ms step_avg:147.78ms
step:1235/1370 train_time:181037ms step_avg:147.79ms
step:1236/1370 train_time:181194ms step_avg:147.79ms
step:1237/1370 train_time:181351ms step_avg:147.80ms
step:1238/1370 train_time:181514ms step_avg:147.81ms
step:1239/1370 train_time:181669ms step_avg:147.82ms
step:1240/1370 train_time:181827ms step_avg:147.83ms
step:1241/1370 train_time:181987ms step_avg:147.84ms
step:1242/1370 train_time:182142ms step_avg:147.84ms
step:1243/1370 train_time:182303ms step_avg:147.85ms
step:1244/1370 train_time:182459ms step_avg:147.86ms
step:1245/1370 train_time:182617ms step_avg:147.87ms
step:1246/1370 train_time:182773ms step_avg:147.87ms
step:1247/1370 train_time:182929ms step_avg:147.88ms
step:1248/1370 train_time:183083ms step_avg:147.89ms
step:1249/1370 train_time:183237ms step_avg:147.89ms
step:1250/1370 train_time:183396ms step_avg:147.90ms
step:1250/1370 val_loss:3.3020 train_time:183476ms step_avg:147.96ms
step:1251/1370 train_time:183556ms step_avg:147.91ms
step:1252/1370 train_time:183710ms step_avg:147.91ms
step:1253/1370 train_time:183864ms step_avg:147.92ms
step:1254/1370 train_time:184018ms step_avg:147.92ms
step:1255/1370 train_time:184185ms step_avg:147.94ms
step:1256/1370 train_time:184340ms step_avg:147.95ms
step:1257/1370 train_time:184494ms step_avg:147.95ms
step:1258/1370 train_time:184652ms step_avg:147.96ms
step:1259/1370 train_time:184809ms step_avg:147.97ms
step:1260/1370 train_time:184962ms step_avg:147.97ms
step:1261/1370 train_time:185120ms step_avg:147.98ms
step:1262/1370 train_time:185278ms step_avg:147.99ms
step:1263/1370 train_time:185435ms step_avg:147.99ms
step:1264/1370 train_time:185588ms step_avg:148.00ms
step:1265/1370 train_time:185743ms step_avg:148.00ms
step:1266/1370 train_time:185902ms step_avg:148.01ms
step:1267/1370 train_time:186058ms step_avg:148.02ms
step:1268/1370 train_time:186216ms step_avg:148.03ms
step:1269/1370 train_time:186378ms step_avg:148.04ms
step:1270/1370 train_time:186532ms step_avg:148.04ms
step:1271/1370 train_time:186689ms step_avg:148.05ms
step:1272/1370 train_time:186843ms step_avg:148.05ms
step:1273/1370 train_time:186998ms step_avg:148.06ms
step:1274/1370 train_time:187153ms step_avg:148.06ms
step:1275/1370 train_time:187308ms step_avg:148.07ms
step:1276/1370 train_time:187462ms step_avg:148.07ms
step:1277/1370 train_time:187619ms step_avg:148.08ms
step:1278/1370 train_time:187771ms step_avg:148.08ms
step:1279/1370 train_time:187927ms step_avg:148.09ms
step:1280/1370 train_time:188090ms step_avg:148.10ms
step:1281/1370 train_time:188247ms step_avg:148.11ms
step:1282/1370 train_time:188401ms step_avg:148.11ms
step:1283/1370 train_time:188559ms step_avg:148.12ms
step:1284/1370 train_time:188717ms step_avg:148.13ms
step:1285/1370 train_time:188870ms step_avg:148.13ms
step:1286/1370 train_time:189026ms step_avg:148.14ms
step:1287/1370 train_time:189181ms step_avg:148.14ms
step:1288/1370 train_time:189336ms step_avg:148.15ms
step:1289/1370 train_time:189498ms step_avg:148.16ms
step:1290/1370 train_time:189658ms step_avg:148.17ms
step:1291/1370 train_time:189817ms step_avg:148.18ms
step:1292/1370 train_time:189976ms step_avg:148.19ms
step:1293/1370 train_time:190134ms step_avg:148.20ms
step:1294/1370 train_time:190290ms step_avg:148.20ms
step:1295/1370 train_time:190448ms step_avg:148.21ms
step:1296/1370 train_time:190605ms step_avg:148.22ms
step:1297/1370 train_time:190765ms step_avg:148.22ms
step:1298/1370 train_time:190922ms step_avg:148.23ms
step:1299/1370 train_time:191075ms step_avg:148.24ms
step:1300/1370 train_time:191228ms step_avg:148.24ms
step:1301/1370 train_time:191382ms step_avg:148.24ms
step:1302/1370 train_time:191539ms step_avg:148.25ms
step:1303/1370 train_time:191697ms step_avg:148.26ms
step:1304/1370 train_time:191854ms step_avg:148.26ms
step:1305/1370 train_time:192009ms step_avg:148.27ms
step:1306/1370 train_time:192168ms step_avg:148.28ms
step:1307/1370 train_time:192321ms step_avg:148.28ms
step:1308/1370 train_time:192479ms step_avg:148.29ms
step:1309/1370 train_time:192633ms step_avg:148.29ms
step:1310/1370 train_time:192787ms step_avg:148.30ms
step:1311/1370 train_time:192942ms step_avg:148.30ms
step:1312/1370 train_time:193097ms step_avg:148.31ms
step:1313/1370 train_time:193251ms step_avg:148.31ms
step:1314/1370 train_time:193406ms step_avg:148.32ms
step:1315/1370 train_time:193561ms step_avg:148.32ms
step:1316/1370 train_time:193715ms step_avg:148.33ms
step:1317/1370 train_time:193869ms step_avg:148.33ms
step:1318/1370 train_time:194031ms step_avg:148.34ms
step:1319/1370 train_time:194188ms step_avg:148.35ms
step:1320/1370 train_time:194344ms step_avg:148.35ms
step:1321/1370 train_time:194503ms step_avg:148.36ms
step:1322/1370 train_time:194663ms step_avg:148.37ms
step:1323/1370 train_time:194820ms step_avg:148.38ms
step:1324/1370 train_time:194976ms step_avg:148.38ms
step:1325/1370 train_time:195132ms step_avg:148.39ms
step:1326/1370 train_time:195293ms step_avg:148.40ms
step:1327/1370 train_time:195448ms step_avg:148.40ms
step:1328/1370 train_time:195602ms step_avg:148.41ms
step:1329/1370 train_time:195777ms step_avg:148.43ms
step:1330/1370 train_time:195936ms step_avg:148.44ms
step:1331/1370 train_time:196131ms step_avg:148.47ms
step:1332/1370 train_time:196289ms step_avg:148.48ms
step:1333/1370 train_time:196445ms step_avg:148.48ms
step:1334/1370 train_time:196602ms step_avg:148.49ms
step:1335/1370 train_time:196755ms step_avg:148.49ms
step:1336/1370 train_time:196920ms step_avg:148.51ms
step:1337/1370 train_time:197079ms step_avg:148.51ms
step:1338/1370 train_time:197235ms step_avg:148.52ms
step:1339/1370 train_time:197394ms step_avg:148.53ms
step:1340/1370 train_time:197552ms step_avg:148.54ms
step:1341/1370 train_time:197705ms step_avg:148.54ms
step:1342/1370 train_time:197865ms step_avg:148.55ms
step:1343/1370 train_time:198021ms step_avg:148.55ms
step:1344/1370 train_time:198175ms step_avg:148.56ms
step:1345/1370 train_time:198329ms step_avg:148.56ms
step:1346/1370 train_time:198485ms step_avg:148.57ms
step:1347/1370 train_time:198644ms step_avg:148.57ms
step:1348/1370 train_time:198800ms step_avg:148.58ms
step:1349/1370 train_time:198957ms step_avg:148.59ms
step:1350/1370 train_time:199112ms step_avg:148.59ms
step:1351/1370 train_time:199266ms step_avg:148.60ms
step:1352/1370 train_time:199431ms step_avg:148.61ms
step:1353/1370 train_time:199591ms step_avg:148.62ms
step:1354/1370 train_time:199751ms step_avg:148.62ms
step:1355/1370 train_time:199909ms step_avg:148.63ms
step:1356/1370 train_time:200064ms step_avg:148.64ms
step:1357/1370 train_time:200222ms step_avg:148.64ms
step:1358/1370 train_time:200380ms step_avg:148.65ms
step:1359/1370 train_time:200534ms step_avg:148.65ms
step:1360/1370 train_time:200693ms step_avg:148.66ms
step:1361/1370 train_time:200851ms step_avg:148.67ms
step:1362/1370 train_time:201010ms step_avg:148.68ms
step:1363/1370 train_time:201173ms step_avg:148.69ms
step:1364/1370 train_time:201328ms step_avg:148.69ms
step:1365/1370 train_time:201484ms step_avg:148.70ms
step:1366/1370 train_time:201641ms step_avg:148.70ms
step:1367/1370 train_time:201798ms step_avg:148.71ms
step:1368/1370 train_time:201953ms step_avg:148.71ms
step:1369/1370 train_time:202115ms step_avg:148.72ms
step:1370/1370 train_time:202274ms step_avg:148.73ms
step:1370/1370 val_loss:3.2781 train_time:202352ms step_avg:148.79ms
peak memory consumption: 31565 MiB

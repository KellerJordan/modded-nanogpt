import os
import sys
with open(sys.argv[0]) as f:
    code = f.read() # read the code of this file ASAP, for logging
import uuid
import time
import glob
import subprocess
import contextlib
from dataclasses import dataclass

import torch
torch.empty(1, device='cuda', requires_grad=True).backward()
from torch import nn
import torch.nn.functional as F
import torch.distributed as dist
from torch.nn.parallel import DistributedDataParallel as DDP
# use of FlexAttention contributed by @KoszarskyB
from torch.nn.attention.flex_attention import BlockMask, flex_attention

# -----------------------------------------------------------------------------
# Muon optimizer

@torch.compile
def zeropower_via_newtonschulz5(G, steps):
    """
    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a
    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose
    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at
    zero even beyond the point where the iteration no longer converges all the way to one everywhere
    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T
    where S' is diagonal with S_{ii}' ~ Uniform(0.5, 1.5), which turns out not to hurt model
    performance at all relative to UV^T, where USV^T = G is the SVD.
    """
    assert len(G.shape) == 2
    a, b, c = (3.4445, -4.7750,  2.0315)
    X = G.bfloat16()
    if G.size(0) > G.size(1):
        X = X.T

    # Ensure spectral norm is at most 1
    X = X / (X.norm() + 1e-7)
    # Perform the NS iterations
    for _ in range(steps):
        A = X @ X.T
        B = b * A + c * A @ A # adapted from suggestion by @jxbz, @leloykun, and @YouJiacheng
        X = a * X + B @ X
    
    if G.size(0) > G.size(1):
        X = X.T
    return X

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Some warnings:
    - This optimizer assumes that all parameters passed in are 2D.
    - It should not be used for the embedding layer, the final fully connected layer, or any {0,1}-D
    parameters; those should all be optimized by a standard method (e.g., AdamW).
    - To use it with 4D convolutional filters, it works well to just flatten their last 3 dimensions.
    - We believe it is unlikely to work well for training with small batch size.
    - We believe it may not work well for finetuning pretrained models, but we haven't tested this.
    - We have not yet tried this optimizer for training scenarios larger than NanoGPT (124M).

    Arguments:
        lr: The learning rate used by the internal SGD.
        momentum: The momentum used by the internal SGD.
        nesterov: Whether to use Nesterov-style momentum in the internal SGD. (recommended)
        ns_steps: The number of Newton-Schulz iteration steps to use.
    """
    def __init__(self, params, lr=0.02, momentum=0.95, nesterov=True, ns_steps=5):
        self.world_size = int(os.environ['WORLD_SIZE'])
        self.rank = int(os.environ['RANK'])
        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, ns_steps=ns_steps)
        assert all(isinstance(p, torch.Tensor) for p in params)
        sizes = {p.numel() for p in params}
        param_groups = [dict(params=[p for p in params if p.numel() == size],
                             update_buffer=[torch.empty(size, device='cuda', dtype=torch.bfloat16) for _ in range(self.world_size)])
                        for size in sizes]
        super().__init__(param_groups, defaults)

    def step(self):

        for group in self.param_groups:

            lr = group['lr']
            momentum = group['momentum']
            nesterov = group['nesterov']
            ns_steps = group['ns_steps']
            update_buffers = group['update_buffer']
            # generate weight updates in distributed fashion
            params = group['params']
            handle = None
            params_world = None
            def update_prev():
                if params_world is None:
                    return
                assert handle is not None
                handle.wait()
                for p_world, g_world in zip(params_world, update_buffers):
                    p_world.data.add_(
                        g_world.view_as(p_world),
                        alpha=-lr * max(1, p_world.size(0) / p_world.size(1)) ** 0.5,
                    )
            for base_i in range(len(params))[::self.world_size]:
                if base_i + rank < len(params):
                    p = params[base_i + self.rank]
                    g = p.grad
                    assert g is not None
                    state = self.state[p]
                    if 'momentum_buffer' not in state:
                        state['momentum_buffer'] = torch.zeros_like(g)
                    buf = state['momentum_buffer']
                    buf.lerp_(g, 1 - momentum)
                    g = g.lerp_(buf, momentum) if nesterov else buf
                    g = zeropower_via_newtonschulz5(g, steps=ns_steps).flatten()
                else:
                    g = update_buffers[rank]
                update_prev() # async all_gather instead of sync all_reduce by @YouJiacheng
                handle = dist.all_gather(update_buffers, g, async_op=True)
                params_world = params[base_i : base_i + self.world_size]
            update_prev()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the GPT-2 model

def norm(x):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):

    def __init__(self, in_features, out_features):
        super().__init__(in_features, out_features, bias=False)

    def forward(self, x):
        return F.linear(x, self.weight.type_as(x))

class Rotary(nn.Module):

    def __init__(self, dim, max_seq_len=65536):
        super().__init__()
        # half-truncate RoPE by @YouJiacheng
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=dim//4, dtype=torch.float32)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(dim//4)])
        t = torch.arange(max_seq_len, dtype=torch.float32)
        theta = torch.einsum('i,j -> ij', t, angular_freq)
        self.cos = nn.Buffer(theta.cos(), persistent=False)
        self.sin = nn.Buffer(theta.sin(), persistent=False)

    def forward(self, x):
        cos, sin = self.cos[None, :x.size(-3), None, :], self.sin[None, :x.size(-3), None, :]
        x1, x2 = x.float().chunk(2, dim=-1)
        y1 = x1 * cos + x2 * sin
        y2 = x1 * (-sin) + x2 * cos
        return torch.cat((y1, y2), 3).type_as(x)

class CausalSelfAttention(nn.Module):

    def __init__(self, dim, num_heads, layer_id):
        super().__init__()
        assert dim % num_heads == 0
        self.num_heads = num_heads
        self.layer_id = layer_id
        self.c_q = CastedLinear(dim, dim)
        self.c_k = CastedLinear(dim, dim)
        self.c_v = CastedLinear(dim, dim)
        self.lambdas = nn.Parameter(torch.tensor([0.5, 0.5]))
        self.rotary = Rotary(dim // num_heads) # dim // num_heads = head_dim
        self.c_proj = CastedLinear(dim, dim)
        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977
        # self.attn_scale = nn.Parameter(torch.tensor(1.0 / (dim // num_heads) ** 0.5))
        # self.attn_scale = nn.Parameter(torch.tensor(0.20))
        self.attn_scale = 0.13 + 0.01 * min(layer_id, 11 - layer_id)

    def forward(self, x, ve, block_mask):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, 'Must use batch size = 1 for FlexAttention'
        q = self.c_q(x).view(B, T, self.num_heads, -1)
        k = self.c_k(x).view(B, T, self.num_heads, -1)
        v = self.c_v(x).view(B, T, self.num_heads, -1)
        if ve is not None:
            v = self.lambdas[0] * v + self.lambdas[1] * ve.view_as(v) # @KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = self.lambdas[0] * v
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = self.rotary(q), self.rotary(k)
        # y = flex_attention(q.transpose(1, 2) * self.attn_scale, k.transpose(1, 2), v.transpose(1, 2), block_mask=block_mask, scale=1.)
        y = flex_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), block_mask=block_mask, scale=self.attn_scale)
        y = y.transpose(1, 2).contiguous().view_as(x) # re-assemble all head outputs side by side
        y = self.c_proj(y)
        return y

class MLP(nn.Module):

    def __init__(self, dim):
        super().__init__()
        self.c_fc = CastedLinear(dim, 4 * dim)
        self.c_proj = CastedLinear(4 * dim, dim)
        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977

    def forward(self, x):
        x = self.c_fc(x)
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = self.c_proj(x)
        return x

class Block(nn.Module):

    def __init__(self, model_dim, num_heads, layer_id, use_attn=True):
        super().__init__()
        self.attn = CausalSelfAttention(model_dim, num_heads, layer_id) if use_attn else None
        self.mlp = MLP(model_dim)
        self.lambdas = nn.Parameter(torch.tensor([1., 0.]))

    def forward(self, x, ve, x0, block_mask):
        x = self.lambdas[0] * x + self.lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), ve, block_mask)
        x = x + self.mlp(norm(x))
        return x

class ValueEmbedding(nn.Module):
    def __init__(self, vocab_size, model_dim):
        super().__init__()
        self.embed = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])

    def forward(self, inputs):
        ve = [emb(inputs).bfloat16() for emb in self.embed]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        ve = [ve[0], ve[1], ve[2], None, None, None, None, None, None, ve[0], ve[1], ve[2]]
        return ve

# -----------------------------------------------------------------------------
# The main GPT-2 model

class GPT(nn.Module):

    def __init__(self, vocab_size, num_layers, num_heads, model_dim):
        super().__init__()
        self.embed = nn.Embedding(vocab_size, model_dim)
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.blocks = nn.ModuleList([Block(model_dim, num_heads, layer_id=layer_id, use_attn=(layer_id != 7))
                                     for layer_id in range(num_layers)])
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual learning
        # U-net structure on token value embeddings by @leloykun
        self.value_embeds = ValueEmbedding(vocab_size, model_dim)
        self.lm_head = CastedLinear(model_dim, vocab_size)
        self.lm_head.weight.data.zero_() # @Grad62304977
        # U-net design by @brendanh0gan
        self.num_encoder_layers = num_layers // 2 # Half of the layers for encoder
        self.num_decoder_layers = num_layers - self.num_encoder_layers # Remaining for decoder
        # Add learnable skip connection weights for decoder layers
        self.skip_weights = nn.Parameter(torch.ones(self.num_decoder_layers))

    def forward(self, inputs, targets, sliding_window_num_blocks):
        BLOCK_SIZE = 128
        seq_len = len(inputs)
        assert seq_len % BLOCK_SIZE == 0
        total_num_blocks = seq_len // BLOCK_SIZE
        assert inputs.ndim == 1
        docs = (inputs == 50256).cumsum(0)
        docs_low = docs.view(-1, BLOCK_SIZE)[:, 0].contiguous()
        docs_high = docs.view(-1, BLOCK_SIZE)[:, -1].contiguous()

        def document_causal(b, h, q_idx, kv_idx):
            causal_mask = q_idx >= kv_idx
            document_mask = docs[q_idx] == docs[kv_idx]
            return causal_mask & document_mask

        def dense_to_ordered(dense_mask):
            num_blocks = dense_mask.sum(dim=-1, dtype=torch.int32)
            indices = dense_mask.argsort(dim=-1, descending=True, stable=True).to(torch.int32)
            return num_blocks[None, None].contiguous(), indices[None, None].contiguous()

        def create_doc_swc_block_mask(sliding_window_num_blocks):
            kv_idx = block_idx = torch.arange(total_num_blocks, dtype=torch.int32, device='cuda')
            q_idx = block_idx[:, None]
            causal_bm = q_idx >= kv_idx
            causal_full_bm = q_idx > kv_idx
            window_bm = q_idx - kv_idx < sliding_window_num_blocks
            window_full_bm = window_bm # block-wise sliding window by @YouJiacheng
            # document_bm = (docs_low[q_idx] <= docs_high[kv_idx]) & (docs_low[kv_idx] <= docs_high[q_idx])
            document_bm = (docs_low[:, None] <= docs_high) & (docs_low <= docs_high[:, None])
            document_full_bm = (docs_low[:, None] == docs_high) & (docs_low == docs_high[:, None])
            nonzero_bm = causal_bm & window_bm & document_bm
            full_bm  = causal_full_bm & window_full_bm & document_full_bm
            kv_num_blocks, kv_indices = dense_to_ordered(nonzero_bm & ~full_bm)
            full_kv_num_blocks, full_kv_indices = dense_to_ordered(full_bm)
            return BlockMask.from_kv_blocks(
                kv_num_blocks,
                kv_indices,
                full_kv_num_blocks,
                full_kv_indices,
                BLOCK_SIZE=BLOCK_SIZE,
                mask_mod=document_causal,
            )

        block_mask = create_doc_swc_block_mask(sliding_window_num_blocks)

        x0 = norm(self.embed(inputs[None]).bfloat16()) # use of norm here by @Grad62304977
        x = x0
        ve = self.value_embeds(inputs)
        assert len(ve) == len(self.blocks)
        ve_enc, ve_dec = ve[:self.num_encoder_layers], ve[self.num_encoder_layers:]

        # Store outputs for U-Net skip connections
        skip_connections = []
        # Encoder pass - process only the first half of the blocks
        for i in range(self.num_encoder_layers):
            x = self.blocks[i](x, ve_enc[i], x0, block_mask)
            skip_connections.append(x)
        # Decoder pass - process the remaining blocks with weighted skip connections
        for i in range(self.num_decoder_layers):
            x = x + self.skip_weights[i] * skip_connections.pop()
            # U-net structure on token value embeddings by @leloykun
            x = self.blocks[self.num_encoder_layers + i](x, ve_dec[i], x0, block_mask)

        x = norm(x)
        logits = self.lm_head(x)
        logits = 15 * torch.tanh(logits / 15) # @Grad62304977 added tanh softcapping, @KoszarskyB reduced it from 30 to 15
        logits = logits.float()
        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets)
        return loss

# -----------------------------------------------------------------------------
# Our own simple Distributed Data Loader

def _load_data_shard(path):
    # only reads the header, returns header data
    # header is 256 int32
    header = torch.from_file(path, False, 256, dtype=torch.int32)
    assert header[0] == 20240520, 'magic number mismatch in the data .bin file'
    assert header[1] == 1, 'unsupported version'
    num_tokens = int(header[2]) # number of tokens (claimed)
    with open(path, 'rb', buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, 'number of tokens read does not match header'
    return tokens

class DistributedDataLoader:

    def __init__(self, filename_pattern):
        self.rank = int(os.environ['RANK'])
        self.world_size = int(os.environ['WORLD_SIZE'])
        self.files = sorted(glob.glob(filename_pattern))
        self.reset()

    def reset(self):
        self.current_shard = -1
        self.advance()

    def advance(self):
        self.current_shard = (self.current_shard + 1) % len(self.files)
        self.current_position = 0
        self.tokens = _load_data_shard(self.files[self.current_shard])

    def next_batch(self, batch_size):
        assert batch_size % self.world_size == 0
        device_batch_size = batch_size // self.world_size
        # load next shard if necessary
        if self.current_position + batch_size + 1 >= len(self.tokens):
            self.advance()
        pos = self.current_position + self.rank * device_batch_size
        device_batch_tokens = self.tokens[pos:pos+device_batch_size+1]
        # advance current position
        self.current_position += batch_size
        inputs = device_batch_tokens[:-1].to(device='cuda', dtype=torch.int32, non_blocking=True)
        targets = device_batch_tokens[1:].to(device='cuda', dtype=torch.int64, non_blocking=True)
        return inputs, targets

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files = 'data/fineweb10B/fineweb_train_*.bin' # input .bin to train on
    val_files = 'data/fineweb10B/fineweb_val_*.bin' # input .bin to eval validation loss on
    val_tokens = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # optimization
    batch_size = 8*64*1024 # batch size in tokens
    num_iterations = 1375 # number of iterations to run
    cooldown_frac = 0.4 # fraction of training spent cooling down the learning rate
    bf16_embeds = True
    # evaluation and logging
    val_loss_every = 125 # every how many steps to evaluate val loss? 0 for only at the end
    # implementation
    max_device_batch_size = 64*1024 # batch size per device in tokens
    save_checkpoint = False
args = Hyperparameters()

micro_bs = args.max_device_batch_size

# set up DDP (distributed data parallel). torchrun sets this env variable
rank = int(os.environ['RANK'])
local_rank = int(os.environ['LOCAL_RANK'])
world_size = int(os.environ['WORLD_SIZE'])
assert torch.cuda.is_available()
torch.cuda.set_device(local_rank)
dist.init_process_group(backend='nccl', device_id=torch.device(local_rank))
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = uuid.uuid4()
    os.makedirs('logs', exist_ok=True)
    logfile = f'logs/{run_id}.txt'
    print(logfile)

def print0(s, console=False):
    if master_process:
        with open(logfile, 'a') as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0('='*100)
# log information about the hardware/software environment this is running on
print0(f'Running Python {sys.version}')
print0(f'Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}')
print0(subprocess.run(['nvidia-smi'], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout)
print0('='*100)

# load data
train_loader = DistributedDataLoader(args.train_files)
val_loader = DistributedDataLoader(args.val_files)
print0(f'Training dataloader files: {train_loader.files}')
print0(f'Validation dataloader files: {val_loader.files}')
print0('='*100)

# there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency. suggested to me by @Grad62304977.
# this originates from Karpathy's experiments.
model = GPT(vocab_size=50304, num_layers=12, num_heads=6, model_dim=768)
model = model.cuda()
if args.bf16_embeds:
    for m in model.modules():
        if isinstance(m, nn.Embedding):
            m.bfloat16()
model: GPT = torch.compile(model)
ddp_model = DDP(model, device_ids=[local_rank], broadcast_buffers=False, gradient_as_bucket_view=True)

# collect the parameters to optimize
hidden_matrix_params = [p for p in model.blocks.parameters() if p.ndim == 2]
embed_params = [model.embed.weight, *model.value_embeds.parameters()]
scalar_params = [p for n, p in model.named_parameters() if p.ndim < 2 and "attn_scale" not in n]
attn_scale_params = [p for n, p in model.named_parameters() if p.ndim < 2 and "attn_scale" in n]
head_params = [model.lm_head.weight]

# init the optimizer(s)
optimizer1 = torch.optim.Adam([dict(params=embed_params, lr=0.6),
                               dict(params=head_params, lr=0.008),
                               dict(params=scalar_params, lr=0.04),
                               dict(params=attn_scale_params, lr=0.01)],
                              betas=(0.8, 0.95), fused=True)
optimizer2 = Muon(hidden_matrix_params, lr=0.05, momentum=0.95)
optimizers = [optimizer1, optimizer2]

# learning rate schedule: stable then decay
def get_lr(it):
    t = 1 - it / args.num_iterations # time remaining in training
    assert 1 >= t > 0
    # 1) constant lr for first part of training
    if t >= args.cooldown_frac:
        return 1.0
    # 2) then linear cooldown
    else:
        return t / args.cooldown_frac
schedulers = [torch.optim.lr_scheduler.LambdaLR(opt, get_lr) for opt in optimizers]

# sliding window size schedule: linear increase over training in chunks of 128 from 128 -> 1792. By @fernbear.bsky.social
def get_sliding_window_blocks(it):
    x = it / args.num_iterations # training progress
    assert 0 <= x <= 1
    return int(((1 - x) * 128 + x * 1856) // 128)
sliding_window_num_blocks = torch.tensor(1, dtype=torch.int32, device='cuda')

# Start training loop
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    # This effectively ignores timing first 10 steps, which are slower for weird reasons.
    # Alternately, and slightly more correctly in terms of benchmarking, we could do 10
    # steps with dummy data first, and then re-initialize the model and reset the loader.
    if step == 10:
        training_time_ms = 0
        t0 = time.perf_counter()
    timed_steps = float('nan') if step <= 11 else (step - 10) + 1 # <= 11 to avoid bug in val

    sliding_window_num_blocks.copy_(get_sliding_window_blocks(step))

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        # run validation batches
        model.eval()
        val_loader.reset()
        val_loss = 0.0
        # calculate the number of steps to take in the val loop.
        val_batch_size = world_size * micro_bs
        assert args.val_tokens % val_batch_size == 0
        val_steps = args.val_tokens // val_batch_size
        for _ in range(val_steps):
            with torch.no_grad():
                inputs_val, targets_val = val_loader.next_batch(val_batch_size)
                val_loss += ddp_model(inputs_val, targets_val, sliding_window_num_blocks)
        # Print attention scales
        for n, p in model.named_parameters():
            if p.ndim < 2 and "attn_scale" in n:
                print0(f'{n}: {p.item()}', console=True)
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        val_loss /= val_steps
        # logging
        print0(f'step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/(timed_steps-1):.2f}ms', console=True)
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f'logs/{run_id}', exist_ok=True)
            torch.save(log, f'logs/{run_id}/state_step{step:06d}.pt')
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    model.train()
    batch_size = args.batch_size
    assert batch_size % world_size == 0
    inputs_train, targets_train = train_loader.next_batch(batch_size)
    assert len(inputs_train) <= micro_bs or len(inputs_train) % micro_bs == 0
    for micro_inputs_train, micro_targets_train in zip(inputs_train.split(micro_bs), targets_train.split(micro_bs)):
        ddp_model(micro_inputs_train, micro_targets_train, sliding_window_num_blocks).backward()
    # momentum warmup for Muon
    frac = min(step/300, 1)
    for group in optimizer2.param_groups:
        group['momentum'] = (1 - frac) * 0.85 + frac * 0.95
    # step the optimizers and schedulers
    for opt, sched in zip(optimizers, schedulers):
        opt.step()
        if step != train_steps-1:
            sched.step()
    # null the gradients
    model.zero_grad(set_to_none=True)
    # logging
    approx_time = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f'step:{step+1}/{train_steps} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms', console=True)

print0(f'peak memory consumption: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB')
dist.destroy_process_group()

====================================================================================================
Running Python 3.10.12 (main, Sep 11 2024, 15:47:36) [GCC 11.4.0]
Running PyTorch 2.6.0.dev20241231+cu126 compiled for CUDA 12.6
Tue Jan 14 14:29:38 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.183.06             Driver Version: 535.183.06   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:19:00.0 Off |                    0 |
| N/A   30C    P0             119W / 700W |   7713MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:3B:00.0 Off |                    0 |
| N/A   29C    P0             112W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:4C:00.0 Off |                    0 |
| N/A   25C    P0             116W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   29C    P0             122W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:9B:00.0 Off |                    0 |
| N/A   30C    P0             118W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:BB:00.0 Off |                    0 |
| N/A   26C    P0             112W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:CB:00.0 Off |                    0 |
| N/A   29C    P0             111W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:DB:00.0 Off |                    0 |
| N/A   26C    P0             114W / 700W |   3211MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
Training dataloader files: ['data/fineweb10B/fineweb_train_000001.bin', 'data/fineweb10B/fineweb_train_000002.bin', 'data/fineweb10B/fineweb_train_000003.bin', 'data/fineweb10B/fineweb_train_000004.bin', 'data/fineweb10B/fineweb_train_000005.bin', 'data/fineweb10B/fineweb_train_000006.bin', 'data/fineweb10B/fineweb_train_000007.bin', 'data/fineweb10B/fineweb_train_000008.bin', 'data/fineweb10B/fineweb_train_000009.bin']
Validation dataloader files: ['data/fineweb10B/fineweb_val_000000.bin']
====================================================================================================
step:0/1375 val_loss:10.8258 train_time:0ms step_avg:nanms
step:1/1375 train_time:27621ms step_avg:nanms
step:2/1375 train_time:27754ms step_avg:nanms
step:3/1375 train_time:27936ms step_avg:nanms
step:4/1375 train_time:28070ms step_avg:nanms
step:5/1375 train_time:28207ms step_avg:nanms
step:6/1375 train_time:28340ms step_avg:nanms
step:7/1375 train_time:28473ms step_avg:nanms
step:8/1375 train_time:28606ms step_avg:nanms
step:9/1375 train_time:28737ms step_avg:nanms
step:10/1375 train_time:28877ms step_avg:nanms
step:11/1375 train_time:134ms step_avg:nanms
step:12/1375 train_time:271ms step_avg:nanms
step:13/1375 train_time:407ms step_avg:135.72ms
step:14/1375 train_time:540ms step_avg:135.11ms
step:15/1375 train_time:674ms step_avg:134.83ms
step:16/1375 train_time:808ms step_avg:134.61ms
step:17/1375 train_time:942ms step_avg:134.60ms
step:18/1375 train_time:1079ms step_avg:134.84ms
step:19/1375 train_time:1216ms step_avg:135.07ms
step:20/1375 train_time:1351ms step_avg:135.08ms
step:21/1375 train_time:1486ms step_avg:135.10ms
step:22/1375 train_time:1620ms step_avg:134.99ms
step:23/1375 train_time:1753ms step_avg:134.82ms
step:24/1375 train_time:1889ms step_avg:134.92ms
step:25/1375 train_time:2023ms step_avg:134.88ms
step:26/1375 train_time:2159ms step_avg:134.91ms
step:27/1375 train_time:2294ms step_avg:134.96ms
step:28/1375 train_time:2431ms step_avg:135.06ms
step:29/1375 train_time:2565ms step_avg:135.00ms
step:30/1375 train_time:2699ms step_avg:134.97ms
step:31/1375 train_time:2834ms step_avg:134.93ms
step:32/1375 train_time:2968ms step_avg:134.90ms
step:33/1375 train_time:3103ms step_avg:134.89ms
step:34/1375 train_time:3237ms step_avg:134.90ms
step:35/1375 train_time:3373ms step_avg:134.91ms
step:36/1375 train_time:3509ms step_avg:134.97ms
step:37/1375 train_time:3644ms step_avg:134.95ms
step:38/1375 train_time:3778ms step_avg:134.94ms
step:39/1375 train_time:3913ms step_avg:134.94ms
step:40/1375 train_time:4047ms step_avg:134.90ms
step:41/1375 train_time:4182ms step_avg:134.89ms
step:42/1375 train_time:4317ms step_avg:134.91ms
step:43/1375 train_time:4452ms step_avg:134.90ms
step:44/1375 train_time:4587ms step_avg:134.90ms
step:45/1375 train_time:4723ms step_avg:134.94ms
step:46/1375 train_time:4857ms step_avg:134.91ms
step:47/1375 train_time:4992ms step_avg:134.91ms
step:48/1375 train_time:5126ms step_avg:134.89ms
step:49/1375 train_time:5261ms step_avg:134.90ms
step:50/1375 train_time:5396ms step_avg:134.90ms
step:51/1375 train_time:5531ms step_avg:134.91ms
step:52/1375 train_time:5667ms step_avg:134.93ms
step:53/1375 train_time:5804ms step_avg:134.99ms
step:54/1375 train_time:5939ms step_avg:134.97ms
step:55/1375 train_time:6075ms step_avg:134.99ms
step:56/1375 train_time:6209ms step_avg:134.99ms
step:57/1375 train_time:6345ms step_avg:135.00ms
step:58/1375 train_time:6480ms step_avg:135.00ms
step:59/1375 train_time:6615ms step_avg:135.00ms
step:60/1375 train_time:6749ms step_avg:134.97ms
step:61/1375 train_time:6883ms step_avg:134.97ms
step:62/1375 train_time:7019ms step_avg:134.99ms
step:63/1375 train_time:7153ms step_avg:134.97ms
step:64/1375 train_time:7290ms step_avg:135.00ms
step:65/1375 train_time:7425ms step_avg:135.00ms
step:66/1375 train_time:7560ms step_avg:134.99ms
step:67/1375 train_time:7695ms step_avg:135.01ms
step:68/1375 train_time:7829ms step_avg:134.99ms
step:69/1375 train_time:7964ms step_avg:134.98ms
step:70/1375 train_time:8098ms step_avg:134.96ms
step:71/1375 train_time:8233ms step_avg:134.97ms
step:72/1375 train_time:8368ms step_avg:134.97ms
step:73/1375 train_time:8503ms step_avg:134.97ms
step:74/1375 train_time:8638ms step_avg:134.97ms
step:75/1375 train_time:8772ms step_avg:134.96ms
step:76/1375 train_time:8909ms step_avg:134.98ms
step:77/1375 train_time:9043ms step_avg:134.97ms
step:78/1375 train_time:9177ms step_avg:134.96ms
step:79/1375 train_time:9313ms step_avg:134.97ms
step:80/1375 train_time:9449ms step_avg:134.99ms
step:81/1375 train_time:9584ms step_avg:134.98ms
step:82/1375 train_time:9718ms step_avg:134.97ms
step:83/1375 train_time:9853ms step_avg:134.97ms
step:84/1375 train_time:9989ms step_avg:134.98ms
step:85/1375 train_time:10123ms step_avg:134.98ms
step:86/1375 train_time:10258ms step_avg:134.97ms
step:87/1375 train_time:10395ms step_avg:135.00ms
step:88/1375 train_time:10529ms step_avg:134.98ms
step:89/1375 train_time:10664ms step_avg:134.98ms
step:90/1375 train_time:10800ms step_avg:135.00ms
step:91/1375 train_time:10936ms step_avg:135.02ms
step:92/1375 train_time:11071ms step_avg:135.02ms
step:93/1375 train_time:11207ms step_avg:135.03ms
step:94/1375 train_time:11341ms step_avg:135.01ms
step:95/1375 train_time:11476ms step_avg:135.01ms
step:96/1375 train_time:11611ms step_avg:135.02ms
step:97/1375 train_time:11747ms step_avg:135.03ms
step:98/1375 train_time:11883ms step_avg:135.03ms
step:99/1375 train_time:12018ms step_avg:135.04ms
step:100/1375 train_time:12153ms step_avg:135.04ms
step:101/1375 train_time:12289ms step_avg:135.05ms
step:102/1375 train_time:12424ms step_avg:135.05ms
step:103/1375 train_time:12560ms step_avg:135.05ms
step:104/1375 train_time:12698ms step_avg:135.09ms
step:105/1375 train_time:12836ms step_avg:135.12ms
step:106/1375 train_time:12975ms step_avg:135.16ms
step:107/1375 train_time:13115ms step_avg:135.20ms
step:108/1375 train_time:13252ms step_avg:135.22ms
step:109/1375 train_time:13392ms step_avg:135.27ms
step:110/1375 train_time:13530ms step_avg:135.30ms
step:111/1375 train_time:13666ms step_avg:135.31ms
step:112/1375 train_time:13804ms step_avg:135.33ms
step:113/1375 train_time:13941ms step_avg:135.35ms
step:114/1375 train_time:14079ms step_avg:135.38ms
step:115/1375 train_time:14218ms step_avg:135.41ms
step:116/1375 train_time:14357ms step_avg:135.44ms
step:117/1375 train_time:14498ms step_avg:135.49ms
step:118/1375 train_time:14637ms step_avg:135.52ms
step:119/1375 train_time:14776ms step_avg:135.56ms
step:120/1375 train_time:14915ms step_avg:135.59ms
step:121/1375 train_time:15054ms step_avg:135.62ms
step:122/1375 train_time:15195ms step_avg:135.67ms
step:123/1375 train_time:15335ms step_avg:135.71ms
step:124/1375 train_time:15474ms step_avg:135.74ms
step:125/1375 train_time:15614ms step_avg:135.77ms
step:125/1375 val_loss:4.3762 train_time:15680ms step_avg:136.35ms
step:126/1375 train_time:15753ms step_avg:135.80ms
step:127/1375 train_time:15893ms step_avg:135.83ms
step:128/1375 train_time:16031ms step_avg:135.86ms
step:129/1375 train_time:16169ms step_avg:135.88ms
step:130/1375 train_time:16307ms step_avg:135.89ms
step:131/1375 train_time:16445ms step_avg:135.91ms
step:132/1375 train_time:16584ms step_avg:135.94ms
step:133/1375 train_time:16727ms step_avg:135.99ms
step:134/1375 train_time:16868ms step_avg:136.03ms
step:135/1375 train_time:17005ms step_avg:136.04ms
step:136/1375 train_time:17144ms step_avg:136.06ms
step:137/1375 train_time:17282ms step_avg:136.08ms
step:138/1375 train_time:17420ms step_avg:136.10ms
step:139/1375 train_time:17560ms step_avg:136.12ms
step:140/1375 train_time:17698ms step_avg:136.14ms
step:141/1375 train_time:17837ms step_avg:136.16ms
step:142/1375 train_time:17977ms step_avg:136.19ms
step:143/1375 train_time:18115ms step_avg:136.21ms
step:144/1375 train_time:18256ms step_avg:136.24ms
step:145/1375 train_time:18393ms step_avg:136.25ms
step:146/1375 train_time:18532ms step_avg:136.26ms
step:147/1375 train_time:18671ms step_avg:136.29ms
step:148/1375 train_time:18810ms step_avg:136.31ms
step:149/1375 train_time:18951ms step_avg:136.34ms
step:150/1375 train_time:19090ms step_avg:136.36ms
step:151/1375 train_time:19230ms step_avg:136.38ms
step:152/1375 train_time:19369ms step_avg:136.40ms
step:153/1375 train_time:19506ms step_avg:136.41ms
step:154/1375 train_time:19646ms step_avg:136.43ms
step:155/1375 train_time:19787ms step_avg:136.46ms
step:156/1375 train_time:19927ms step_avg:136.48ms
step:157/1375 train_time:20067ms step_avg:136.51ms
step:158/1375 train_time:20204ms step_avg:136.51ms
step:159/1375 train_time:20346ms step_avg:136.55ms
step:160/1375 train_time:20485ms step_avg:136.57ms
step:161/1375 train_time:20624ms step_avg:136.58ms
step:162/1375 train_time:20765ms step_avg:136.61ms
step:163/1375 train_time:20903ms step_avg:136.62ms
step:164/1375 train_time:21042ms step_avg:136.64ms
step:165/1375 train_time:21182ms step_avg:136.66ms
step:166/1375 train_time:21321ms step_avg:136.68ms
step:167/1375 train_time:21461ms step_avg:136.69ms
step:168/1375 train_time:21601ms step_avg:136.71ms
step:169/1375 train_time:21741ms step_avg:136.74ms
step:170/1375 train_time:21881ms step_avg:136.76ms
step:171/1375 train_time:22021ms step_avg:136.78ms
step:172/1375 train_time:22161ms step_avg:136.80ms
step:173/1375 train_time:22301ms step_avg:136.81ms
step:174/1375 train_time:22439ms step_avg:136.82ms
step:175/1375 train_time:22578ms step_avg:136.84ms
step:176/1375 train_time:22718ms step_avg:136.85ms
step:177/1375 train_time:22858ms step_avg:136.87ms
step:178/1375 train_time:22997ms step_avg:136.89ms
step:179/1375 train_time:23136ms step_avg:136.90ms
step:180/1375 train_time:23275ms step_avg:136.91ms
step:181/1375 train_time:23416ms step_avg:136.94ms
step:182/1375 train_time:23555ms step_avg:136.95ms
step:183/1375 train_time:23694ms step_avg:136.96ms
step:184/1375 train_time:23835ms step_avg:136.98ms
step:185/1375 train_time:23975ms step_avg:137.00ms
step:186/1375 train_time:24114ms step_avg:137.01ms
step:187/1375 train_time:24254ms step_avg:137.03ms
step:188/1375 train_time:24393ms step_avg:137.04ms
step:189/1375 train_time:24531ms step_avg:137.05ms
step:190/1375 train_time:24672ms step_avg:137.06ms
step:191/1375 train_time:24849ms step_avg:137.29ms
step:192/1375 train_time:24989ms step_avg:137.30ms
step:193/1375 train_time:25127ms step_avg:137.30ms
step:194/1375 train_time:25265ms step_avg:137.31ms
step:195/1375 train_time:25403ms step_avg:137.31ms
step:196/1375 train_time:25541ms step_avg:137.32ms
step:197/1375 train_time:25681ms step_avg:137.33ms
step:198/1375 train_time:25827ms step_avg:137.38ms
step:199/1375 train_time:25968ms step_avg:137.40ms
step:200/1375 train_time:26105ms step_avg:137.40ms
step:201/1375 train_time:26246ms step_avg:137.41ms
step:202/1375 train_time:26385ms step_avg:137.42ms
step:203/1375 train_time:26525ms step_avg:137.44ms
step:204/1375 train_time:26665ms step_avg:137.45ms
step:205/1375 train_time:26805ms step_avg:137.46ms
step:206/1375 train_time:26950ms step_avg:137.50ms
step:207/1375 train_time:27091ms step_avg:137.52ms
step:208/1375 train_time:27232ms step_avg:137.53ms
step:209/1375 train_time:27374ms step_avg:137.56ms
step:210/1375 train_time:27515ms step_avg:137.57ms
step:211/1375 train_time:27657ms step_avg:137.60ms
step:212/1375 train_time:27799ms step_avg:137.62ms
step:213/1375 train_time:27942ms step_avg:137.64ms
step:214/1375 train_time:28083ms step_avg:137.66ms
step:215/1375 train_time:28226ms step_avg:137.69ms
step:216/1375 train_time:28367ms step_avg:137.71ms
step:217/1375 train_time:28508ms step_avg:137.72ms
step:218/1375 train_time:28650ms step_avg:137.74ms
step:219/1375 train_time:28792ms step_avg:137.76ms
step:220/1375 train_time:28935ms step_avg:137.79ms
step:221/1375 train_time:29077ms step_avg:137.81ms
step:222/1375 train_time:29220ms step_avg:137.83ms
step:223/1375 train_time:29364ms step_avg:137.86ms
step:224/1375 train_time:29505ms step_avg:137.87ms
step:225/1375 train_time:29648ms step_avg:137.90ms
step:226/1375 train_time:29790ms step_avg:137.92ms
step:227/1375 train_time:29933ms step_avg:137.94ms
step:228/1375 train_time:30076ms step_avg:137.96ms
step:229/1375 train_time:30217ms step_avg:137.98ms
step:230/1375 train_time:30359ms step_avg:138.00ms
step:231/1375 train_time:30500ms step_avg:138.01ms
step:232/1375 train_time:30641ms step_avg:138.02ms
step:233/1375 train_time:30783ms step_avg:138.04ms
step:234/1375 train_time:30926ms step_avg:138.06ms
step:235/1375 train_time:31069ms step_avg:138.09ms
step:236/1375 train_time:31209ms step_avg:138.09ms
step:237/1375 train_time:31351ms step_avg:138.11ms
step:238/1375 train_time:31493ms step_avg:138.13ms
step:239/1375 train_time:31634ms step_avg:138.14ms
step:240/1375 train_time:31775ms step_avg:138.15ms
step:241/1375 train_time:31917ms step_avg:138.17ms
step:242/1375 train_time:32059ms step_avg:138.19ms
step:243/1375 train_time:32200ms step_avg:138.20ms
step:244/1375 train_time:32343ms step_avg:138.22ms
step:245/1375 train_time:32486ms step_avg:138.24ms
step:246/1375 train_time:32627ms step_avg:138.25ms
step:247/1375 train_time:32768ms step_avg:138.26ms
step:248/1375 train_time:32909ms step_avg:138.27ms
step:249/1375 train_time:33052ms step_avg:138.29ms
step:250/1375 train_time:33193ms step_avg:138.30ms
step:250/1375 val_loss:3.9592 train_time:33261ms step_avg:138.59ms
step:251/1375 train_time:33335ms step_avg:138.32ms
step:252/1375 train_time:33480ms step_avg:138.35ms
step:253/1375 train_time:33622ms step_avg:138.36ms
step:254/1375 train_time:33763ms step_avg:138.37ms
step:255/1375 train_time:33904ms step_avg:138.38ms
step:256/1375 train_time:34045ms step_avg:138.39ms
step:257/1375 train_time:34187ms step_avg:138.41ms
step:258/1375 train_time:34330ms step_avg:138.43ms
step:259/1375 train_time:34474ms step_avg:138.45ms
step:260/1375 train_time:34616ms step_avg:138.46ms
step:261/1375 train_time:34757ms step_avg:138.47ms
step:262/1375 train_time:34897ms step_avg:138.48ms
step:263/1375 train_time:35037ms step_avg:138.49ms
step:264/1375 train_time:35180ms step_avg:138.50ms
step:265/1375 train_time:35322ms step_avg:138.52ms
step:266/1375 train_time:35465ms step_avg:138.53ms
step:267/1375 train_time:35607ms step_avg:138.55ms
step:268/1375 train_time:35750ms step_avg:138.57ms
step:269/1375 train_time:35893ms step_avg:138.58ms
step:270/1375 train_time:36032ms step_avg:138.58ms
step:271/1375 train_time:36174ms step_avg:138.60ms
step:272/1375 train_time:36315ms step_avg:138.61ms
step:273/1375 train_time:36456ms step_avg:138.62ms
step:274/1375 train_time:36599ms step_avg:138.63ms
step:275/1375 train_time:36741ms step_avg:138.65ms
step:276/1375 train_time:36882ms step_avg:138.65ms
step:277/1375 train_time:37024ms step_avg:138.67ms
step:278/1375 train_time:37165ms step_avg:138.68ms
step:279/1375 train_time:37306ms step_avg:138.68ms
step:280/1375 train_time:37447ms step_avg:138.69ms
step:281/1375 train_time:37589ms step_avg:138.71ms
step:282/1375 train_time:37731ms step_avg:138.72ms
step:283/1375 train_time:37873ms step_avg:138.73ms
step:284/1375 train_time:38015ms step_avg:138.74ms
step:285/1375 train_time:38156ms step_avg:138.75ms
step:286/1375 train_time:38298ms step_avg:138.76ms
step:287/1375 train_time:38439ms step_avg:138.77ms
step:288/1375 train_time:38583ms step_avg:138.79ms
step:289/1375 train_time:38725ms step_avg:138.80ms
step:290/1375 train_time:38867ms step_avg:138.81ms
step:291/1375 train_time:39007ms step_avg:138.82ms
step:292/1375 train_time:39149ms step_avg:138.83ms
step:293/1375 train_time:39293ms step_avg:138.84ms
step:294/1375 train_time:39433ms step_avg:138.85ms
step:295/1375 train_time:39577ms step_avg:138.87ms
step:296/1375 train_time:39718ms step_avg:138.87ms
step:297/1375 train_time:39860ms step_avg:138.89ms
step:298/1375 train_time:40002ms step_avg:138.90ms
step:299/1375 train_time:40144ms step_avg:138.91ms
step:300/1375 train_time:40285ms step_avg:138.91ms
step:301/1375 train_time:40426ms step_avg:138.92ms
step:302/1375 train_time:40569ms step_avg:138.94ms
step:303/1375 train_time:40711ms step_avg:138.95ms
step:304/1375 train_time:40854ms step_avg:138.96ms
step:305/1375 train_time:40997ms step_avg:138.97ms
step:306/1375 train_time:41139ms step_avg:138.98ms
step:307/1375 train_time:41281ms step_avg:138.99ms
step:308/1375 train_time:41426ms step_avg:139.01ms
step:309/1375 train_time:41569ms step_avg:139.03ms
step:310/1375 train_time:41714ms step_avg:139.05ms
step:311/1375 train_time:41857ms step_avg:139.06ms
step:312/1375 train_time:42000ms step_avg:139.07ms
step:313/1375 train_time:42146ms step_avg:139.10ms
step:314/1375 train_time:42290ms step_avg:139.11ms
step:315/1375 train_time:42434ms step_avg:139.13ms
step:316/1375 train_time:42578ms step_avg:139.14ms
step:317/1375 train_time:42720ms step_avg:139.15ms
step:318/1375 train_time:42863ms step_avg:139.17ms
step:319/1375 train_time:43006ms step_avg:139.18ms
step:320/1375 train_time:43153ms step_avg:139.20ms
step:321/1375 train_time:43298ms step_avg:139.22ms
step:322/1375 train_time:43440ms step_avg:139.23ms
step:323/1375 train_time:43585ms step_avg:139.25ms
step:324/1375 train_time:43729ms step_avg:139.26ms
step:325/1375 train_time:43874ms step_avg:139.28ms
step:326/1375 train_time:44018ms step_avg:139.30ms
step:327/1375 train_time:44163ms step_avg:139.31ms
step:328/1375 train_time:44309ms step_avg:139.34ms
step:329/1375 train_time:44454ms step_avg:139.36ms
step:330/1375 train_time:44598ms step_avg:139.37ms
step:331/1375 train_time:44741ms step_avg:139.38ms
step:332/1375 train_time:44884ms step_avg:139.39ms
step:333/1375 train_time:45028ms step_avg:139.40ms
step:334/1375 train_time:45172ms step_avg:139.42ms
step:335/1375 train_time:45317ms step_avg:139.44ms
step:336/1375 train_time:45459ms step_avg:139.45ms
step:337/1375 train_time:45606ms step_avg:139.47ms
step:338/1375 train_time:45751ms step_avg:139.48ms
step:339/1375 train_time:45896ms step_avg:139.50ms
step:340/1375 train_time:46038ms step_avg:139.51ms
step:341/1375 train_time:46181ms step_avg:139.52ms
step:342/1375 train_time:46326ms step_avg:139.54ms
step:343/1375 train_time:46470ms step_avg:139.55ms
step:344/1375 train_time:46615ms step_avg:139.56ms
step:345/1375 train_time:46758ms step_avg:139.58ms
step:346/1375 train_time:46903ms step_avg:139.59ms
step:347/1375 train_time:47046ms step_avg:139.60ms
step:348/1375 train_time:47190ms step_avg:139.62ms
step:349/1375 train_time:47333ms step_avg:139.63ms
step:350/1375 train_time:47479ms step_avg:139.64ms
step:351/1375 train_time:47623ms step_avg:139.66ms
step:352/1375 train_time:47767ms step_avg:139.67ms
step:353/1375 train_time:47911ms step_avg:139.68ms
step:354/1375 train_time:48055ms step_avg:139.69ms
step:355/1375 train_time:48199ms step_avg:139.71ms
step:356/1375 train_time:48341ms step_avg:139.72ms
step:357/1375 train_time:48485ms step_avg:139.73ms
step:358/1375 train_time:48629ms step_avg:139.74ms
step:359/1375 train_time:48774ms step_avg:139.75ms
step:360/1375 train_time:48920ms step_avg:139.77ms
step:361/1375 train_time:49064ms step_avg:139.78ms
step:362/1375 train_time:49208ms step_avg:139.79ms
step:363/1375 train_time:49353ms step_avg:139.81ms
step:364/1375 train_time:49497ms step_avg:139.82ms
step:365/1375 train_time:49640ms step_avg:139.83ms
step:366/1375 train_time:49788ms step_avg:139.85ms
step:367/1375 train_time:49931ms step_avg:139.86ms
step:368/1375 train_time:50076ms step_avg:139.88ms
step:369/1375 train_time:50220ms step_avg:139.89ms
step:370/1375 train_time:50365ms step_avg:139.90ms
step:371/1375 train_time:50509ms step_avg:139.91ms
step:372/1375 train_time:50655ms step_avg:139.93ms
step:373/1375 train_time:50798ms step_avg:139.94ms
step:374/1375 train_time:50943ms step_avg:139.95ms
step:375/1375 train_time:51087ms step_avg:139.96ms
step:375/1375 val_loss:3.7741 train_time:51156ms step_avg:140.15ms
step:376/1375 train_time:51230ms step_avg:139.97ms
step:377/1375 train_time:51374ms step_avg:139.98ms
step:378/1375 train_time:51520ms step_avg:140.00ms
step:379/1375 train_time:51664ms step_avg:140.01ms
step:380/1375 train_time:51808ms step_avg:140.02ms
step:381/1375 train_time:51989ms step_avg:140.13ms
step:382/1375 train_time:52136ms step_avg:140.15ms
step:383/1375 train_time:52279ms step_avg:140.16ms
step:384/1375 train_time:52421ms step_avg:140.16ms
step:385/1375 train_time:52563ms step_avg:140.17ms
step:386/1375 train_time:52707ms step_avg:140.18ms
step:387/1375 train_time:52849ms step_avg:140.18ms
step:388/1375 train_time:52998ms step_avg:140.21ms
step:389/1375 train_time:53142ms step_avg:140.22ms
step:390/1375 train_time:53287ms step_avg:140.23ms
step:391/1375 train_time:53429ms step_avg:140.23ms
step:392/1375 train_time:53573ms step_avg:140.24ms
step:393/1375 train_time:53718ms step_avg:140.26ms
step:394/1375 train_time:53864ms step_avg:140.27ms
step:395/1375 train_time:54009ms step_avg:140.28ms
step:396/1375 train_time:54154ms step_avg:140.29ms
step:397/1375 train_time:54300ms step_avg:140.31ms
step:398/1375 train_time:54443ms step_avg:140.32ms
step:399/1375 train_time:54587ms step_avg:140.33ms
step:400/1375 train_time:54730ms step_avg:140.33ms
step:401/1375 train_time:54872ms step_avg:140.34ms
step:402/1375 train_time:55017ms step_avg:140.35ms
step:403/1375 train_time:55161ms step_avg:140.36ms
step:404/1375 train_time:55305ms step_avg:140.37ms
step:405/1375 train_time:55449ms step_avg:140.38ms
step:406/1375 train_time:55593ms step_avg:140.39ms
step:407/1375 train_time:55737ms step_avg:140.39ms
step:408/1375 train_time:55881ms step_avg:140.40ms
step:409/1375 train_time:56025ms step_avg:140.41ms
step:410/1375 train_time:56171ms step_avg:140.43ms
step:411/1375 train_time:56317ms step_avg:140.44ms
step:412/1375 train_time:56464ms step_avg:140.46ms
step:413/1375 train_time:56610ms step_avg:140.47ms
step:414/1375 train_time:56756ms step_avg:140.48ms
step:415/1375 train_time:56903ms step_avg:140.50ms
step:416/1375 train_time:57047ms step_avg:140.51ms
step:417/1375 train_time:57193ms step_avg:140.52ms
step:418/1375 train_time:57339ms step_avg:140.54ms
step:419/1375 train_time:57484ms step_avg:140.55ms
step:420/1375 train_time:57630ms step_avg:140.56ms
step:421/1375 train_time:57774ms step_avg:140.57ms
step:422/1375 train_time:57920ms step_avg:140.58ms
step:423/1375 train_time:58067ms step_avg:140.60ms
step:424/1375 train_time:58212ms step_avg:140.61ms
step:425/1375 train_time:58357ms step_avg:140.62ms
step:426/1375 train_time:58503ms step_avg:140.63ms
step:427/1375 train_time:58646ms step_avg:140.64ms
step:428/1375 train_time:58794ms step_avg:140.65ms
step:429/1375 train_time:58940ms step_avg:140.67ms
step:430/1375 train_time:59087ms step_avg:140.68ms
step:431/1375 train_time:59231ms step_avg:140.69ms
step:432/1375 train_time:59377ms step_avg:140.70ms
step:433/1375 train_time:59525ms step_avg:140.72ms
step:434/1375 train_time:59670ms step_avg:140.73ms
step:435/1375 train_time:59815ms step_avg:140.74ms
step:436/1375 train_time:59961ms step_avg:140.75ms
step:437/1375 train_time:60108ms step_avg:140.77ms
step:438/1375 train_time:60253ms step_avg:140.78ms
step:439/1375 train_time:60400ms step_avg:140.79ms
step:440/1375 train_time:60545ms step_avg:140.80ms
step:441/1375 train_time:60691ms step_avg:140.81ms
step:442/1375 train_time:60836ms step_avg:140.83ms
step:443/1375 train_time:60984ms step_avg:140.84ms
step:444/1375 train_time:61130ms step_avg:140.85ms
step:445/1375 train_time:61275ms step_avg:140.86ms
step:446/1375 train_time:61420ms step_avg:140.87ms
step:447/1375 train_time:61568ms step_avg:140.89ms
step:448/1375 train_time:61712ms step_avg:140.89ms
step:449/1375 train_time:61858ms step_avg:140.91ms
step:450/1375 train_time:62005ms step_avg:140.92ms
step:451/1375 train_time:62149ms step_avg:140.93ms
step:452/1375 train_time:62295ms step_avg:140.94ms
step:453/1375 train_time:62442ms step_avg:140.95ms
step:454/1375 train_time:62589ms step_avg:140.97ms
step:455/1375 train_time:62734ms step_avg:140.98ms
step:456/1375 train_time:62880ms step_avg:140.99ms
step:457/1375 train_time:63027ms step_avg:141.00ms
step:458/1375 train_time:63171ms step_avg:141.01ms
step:459/1375 train_time:63316ms step_avg:141.02ms
step:460/1375 train_time:63465ms step_avg:141.03ms
step:461/1375 train_time:63611ms step_avg:141.04ms
step:462/1375 train_time:63757ms step_avg:141.06ms
step:463/1375 train_time:63904ms step_avg:141.07ms
step:464/1375 train_time:64048ms step_avg:141.08ms
step:465/1375 train_time:64194ms step_avg:141.09ms
step:466/1375 train_time:64340ms step_avg:141.10ms
step:467/1375 train_time:64488ms step_avg:141.11ms
step:468/1375 train_time:64632ms step_avg:141.12ms
step:469/1375 train_time:64779ms step_avg:141.13ms
step:470/1375 train_time:64926ms step_avg:141.14ms
step:471/1375 train_time:65071ms step_avg:141.15ms
step:472/1375 train_time:65217ms step_avg:141.16ms
step:473/1375 train_time:65364ms step_avg:141.17ms
step:474/1375 train_time:65509ms step_avg:141.18ms
step:475/1375 train_time:65654ms step_avg:141.19ms
step:476/1375 train_time:65800ms step_avg:141.20ms
step:477/1375 train_time:65945ms step_avg:141.21ms
step:478/1375 train_time:66091ms step_avg:141.22ms
step:479/1375 train_time:66236ms step_avg:141.23ms
step:480/1375 train_time:66382ms step_avg:141.24ms
step:481/1375 train_time:66528ms step_avg:141.25ms
step:482/1375 train_time:66674ms step_avg:141.26ms
step:483/1375 train_time:66819ms step_avg:141.27ms
step:484/1375 train_time:66967ms step_avg:141.28ms
step:485/1375 train_time:67111ms step_avg:141.29ms
step:486/1375 train_time:67257ms step_avg:141.30ms
step:487/1375 train_time:67404ms step_avg:141.31ms
step:488/1375 train_time:67548ms step_avg:141.31ms
step:489/1375 train_time:67693ms step_avg:141.32ms
step:490/1375 train_time:67840ms step_avg:141.33ms
step:491/1375 train_time:67988ms step_avg:141.35ms
step:492/1375 train_time:68133ms step_avg:141.36ms
step:493/1375 train_time:68278ms step_avg:141.36ms
step:494/1375 train_time:68426ms step_avg:141.38ms
step:495/1375 train_time:68571ms step_avg:141.38ms
step:496/1375 train_time:68719ms step_avg:141.40ms
step:497/1375 train_time:68865ms step_avg:141.41ms
step:498/1375 train_time:69011ms step_avg:141.42ms
step:499/1375 train_time:69156ms step_avg:141.42ms
step:500/1375 train_time:69302ms step_avg:141.43ms
step:500/1375 val_loss:3.6562 train_time:69375ms step_avg:141.58ms
step:501/1375 train_time:69449ms step_avg:141.44ms
step:502/1375 train_time:69596ms step_avg:141.46ms
step:503/1375 train_time:69741ms step_avg:141.46ms
step:504/1375 train_time:69886ms step_avg:141.47ms
step:505/1375 train_time:70031ms step_avg:141.48ms
step:506/1375 train_time:70175ms step_avg:141.48ms
step:507/1375 train_time:70324ms step_avg:141.50ms
step:508/1375 train_time:70472ms step_avg:141.51ms
step:509/1375 train_time:70619ms step_avg:141.52ms
step:510/1375 train_time:70764ms step_avg:141.53ms
step:511/1375 train_time:70910ms step_avg:141.54ms
step:512/1375 train_time:71060ms step_avg:141.55ms
step:513/1375 train_time:71205ms step_avg:141.56ms
step:514/1375 train_time:71353ms step_avg:141.57ms
step:515/1375 train_time:71504ms step_avg:141.59ms
step:516/1375 train_time:71651ms step_avg:141.60ms
step:517/1375 train_time:71798ms step_avg:141.61ms
step:518/1375 train_time:71944ms step_avg:141.62ms
step:519/1375 train_time:72092ms step_avg:141.64ms
step:520/1375 train_time:72239ms step_avg:141.64ms
step:521/1375 train_time:72386ms step_avg:141.66ms
step:522/1375 train_time:72534ms step_avg:141.67ms
step:523/1375 train_time:72681ms step_avg:141.68ms
step:524/1375 train_time:72829ms step_avg:141.69ms
step:525/1375 train_time:72977ms step_avg:141.70ms
step:526/1375 train_time:73124ms step_avg:141.71ms
step:527/1375 train_time:73271ms step_avg:141.72ms
step:528/1375 train_time:73420ms step_avg:141.74ms
step:529/1375 train_time:73566ms step_avg:141.74ms
step:530/1375 train_time:73713ms step_avg:141.76ms
step:531/1375 train_time:73862ms step_avg:141.77ms
step:532/1375 train_time:74010ms step_avg:141.78ms
step:533/1375 train_time:74158ms step_avg:141.79ms
step:534/1375 train_time:74305ms step_avg:141.80ms
step:535/1375 train_time:74452ms step_avg:141.81ms
step:536/1375 train_time:74601ms step_avg:141.83ms
step:537/1375 train_time:74747ms step_avg:141.83ms
step:538/1375 train_time:74896ms step_avg:141.85ms
step:539/1375 train_time:75044ms step_avg:141.86ms
step:540/1375 train_time:75192ms step_avg:141.87ms
step:541/1375 train_time:75338ms step_avg:141.88ms
step:542/1375 train_time:75485ms step_avg:141.89ms
step:543/1375 train_time:75632ms step_avg:141.90ms
step:544/1375 train_time:75779ms step_avg:141.91ms
step:545/1375 train_time:75926ms step_avg:141.92ms
step:546/1375 train_time:76074ms step_avg:141.93ms
step:547/1375 train_time:76223ms step_avg:141.94ms
step:548/1375 train_time:76369ms step_avg:141.95ms
step:549/1375 train_time:76518ms step_avg:141.96ms
step:550/1375 train_time:76666ms step_avg:141.97ms
step:551/1375 train_time:76813ms step_avg:141.98ms
step:552/1375 train_time:76962ms step_avg:142.00ms
step:553/1375 train_time:77109ms step_avg:142.00ms
step:554/1375 train_time:77256ms step_avg:142.01ms
step:555/1375 train_time:77404ms step_avg:142.03ms
step:556/1375 train_time:77550ms step_avg:142.03ms
step:557/1375 train_time:77699ms step_avg:142.05ms
step:558/1375 train_time:77844ms step_avg:142.05ms
step:559/1375 train_time:77991ms step_avg:142.06ms
step:560/1375 train_time:78139ms step_avg:142.07ms
step:561/1375 train_time:78287ms step_avg:142.08ms
step:562/1375 train_time:78435ms step_avg:142.09ms
step:563/1375 train_time:78584ms step_avg:142.10ms
step:564/1375 train_time:78730ms step_avg:142.11ms
step:565/1375 train_time:78878ms step_avg:142.12ms
step:566/1375 train_time:79025ms step_avg:142.13ms
step:567/1375 train_time:79170ms step_avg:142.14ms
step:568/1375 train_time:79318ms step_avg:142.15ms
step:569/1375 train_time:79466ms step_avg:142.16ms
step:570/1375 train_time:79615ms step_avg:142.17ms
step:571/1375 train_time:79800ms step_avg:142.25ms
step:572/1375 train_time:79948ms step_avg:142.26ms
step:573/1375 train_time:80093ms step_avg:142.26ms
step:574/1375 train_time:80241ms step_avg:142.27ms
step:575/1375 train_time:80388ms step_avg:142.28ms
step:576/1375 train_time:80534ms step_avg:142.29ms
step:577/1375 train_time:80682ms step_avg:142.30ms
step:578/1375 train_time:80832ms step_avg:142.31ms
step:579/1375 train_time:80981ms step_avg:142.32ms
step:580/1375 train_time:81128ms step_avg:142.33ms
step:581/1375 train_time:81274ms step_avg:142.34ms
step:582/1375 train_time:81421ms step_avg:142.34ms
step:583/1375 train_time:81566ms step_avg:142.35ms
step:584/1375 train_time:81717ms step_avg:142.36ms
step:585/1375 train_time:81866ms step_avg:142.38ms
step:586/1375 train_time:82015ms step_avg:142.39ms
step:587/1375 train_time:82163ms step_avg:142.40ms
step:588/1375 train_time:82309ms step_avg:142.40ms
step:589/1375 train_time:82457ms step_avg:142.41ms
step:590/1375 train_time:82603ms step_avg:142.42ms
step:591/1375 train_time:82750ms step_avg:142.43ms
step:592/1375 train_time:82900ms step_avg:142.44ms
step:593/1375 train_time:83047ms step_avg:142.45ms
step:594/1375 train_time:83195ms step_avg:142.46ms
step:595/1375 train_time:83342ms step_avg:142.46ms
step:596/1375 train_time:83489ms step_avg:142.47ms
step:597/1375 train_time:83636ms step_avg:142.48ms
step:598/1375 train_time:83784ms step_avg:142.49ms
step:599/1375 train_time:83931ms step_avg:142.50ms
step:600/1375 train_time:84079ms step_avg:142.51ms
step:601/1375 train_time:84225ms step_avg:142.51ms
step:602/1375 train_time:84372ms step_avg:142.52ms
step:603/1375 train_time:84522ms step_avg:142.53ms
step:604/1375 train_time:84669ms step_avg:142.54ms
step:605/1375 train_time:84818ms step_avg:142.55ms
step:606/1375 train_time:84966ms step_avg:142.56ms
step:607/1375 train_time:85113ms step_avg:142.57ms
step:608/1375 train_time:85262ms step_avg:142.58ms
step:609/1375 train_time:85408ms step_avg:142.58ms
step:610/1375 train_time:85554ms step_avg:142.59ms
step:611/1375 train_time:85703ms step_avg:142.60ms
step:612/1375 train_time:85850ms step_avg:142.61ms
step:613/1375 train_time:85999ms step_avg:142.62ms
step:614/1375 train_time:86145ms step_avg:142.62ms
step:615/1375 train_time:86294ms step_avg:142.64ms
step:616/1375 train_time:86442ms step_avg:142.64ms
step:617/1375 train_time:86590ms step_avg:142.65ms
step:618/1375 train_time:86738ms step_avg:142.66ms
step:619/1375 train_time:86891ms step_avg:142.68ms
step:620/1375 train_time:87039ms step_avg:142.69ms
step:621/1375 train_time:87188ms step_avg:142.70ms
step:622/1375 train_time:87336ms step_avg:142.71ms
step:623/1375 train_time:87485ms step_avg:142.72ms
step:624/1375 train_time:87634ms step_avg:142.73ms
step:625/1375 train_time:87784ms step_avg:142.74ms
step:625/1375 val_loss:3.5743 train_time:87858ms step_avg:142.86ms
step:626/1375 train_time:87933ms step_avg:142.75ms
step:627/1375 train_time:88085ms step_avg:142.76ms
step:628/1375 train_time:88232ms step_avg:142.77ms
step:629/1375 train_time:88382ms step_avg:142.78ms
step:630/1375 train_time:88528ms step_avg:142.79ms
step:631/1375 train_time:88677ms step_avg:142.80ms
step:632/1375 train_time:88824ms step_avg:142.80ms
step:633/1375 train_time:88975ms step_avg:142.82ms
step:634/1375 train_time:89124ms step_avg:142.83ms
step:635/1375 train_time:89271ms step_avg:142.83ms
step:636/1375 train_time:89419ms step_avg:142.84ms
step:637/1375 train_time:89568ms step_avg:142.85ms
step:638/1375 train_time:89716ms step_avg:142.86ms
step:639/1375 train_time:89865ms step_avg:142.87ms
step:640/1375 train_time:90013ms step_avg:142.88ms
step:641/1375 train_time:90161ms step_avg:142.89ms
step:642/1375 train_time:90311ms step_avg:142.90ms
step:643/1375 train_time:90459ms step_avg:142.91ms
step:644/1375 train_time:90607ms step_avg:142.91ms
step:645/1375 train_time:90755ms step_avg:142.92ms
step:646/1375 train_time:90905ms step_avg:142.93ms
step:647/1375 train_time:91052ms step_avg:142.94ms
step:648/1375 train_time:91205ms step_avg:142.95ms
step:649/1375 train_time:91355ms step_avg:142.97ms
step:650/1375 train_time:91506ms step_avg:142.98ms
step:651/1375 train_time:91655ms step_avg:142.99ms
step:652/1375 train_time:91804ms step_avg:143.00ms
step:653/1375 train_time:91951ms step_avg:143.00ms
step:654/1375 train_time:92103ms step_avg:143.02ms
step:655/1375 train_time:92250ms step_avg:143.02ms
step:656/1375 train_time:92401ms step_avg:143.03ms
step:657/1375 train_time:92549ms step_avg:143.04ms
step:658/1375 train_time:92699ms step_avg:143.05ms
step:659/1375 train_time:92847ms step_avg:143.06ms
step:660/1375 train_time:92995ms step_avg:143.07ms
step:661/1375 train_time:93145ms step_avg:143.08ms
step:662/1375 train_time:93293ms step_avg:143.09ms
step:663/1375 train_time:93440ms step_avg:143.09ms
step:664/1375 train_time:93591ms step_avg:143.11ms
step:665/1375 train_time:93741ms step_avg:143.12ms
step:666/1375 train_time:93889ms step_avg:143.12ms
step:667/1375 train_time:94037ms step_avg:143.13ms
step:668/1375 train_time:94185ms step_avg:143.14ms
step:669/1375 train_time:94333ms step_avg:143.15ms
step:670/1375 train_time:94482ms step_avg:143.15ms
step:671/1375 train_time:94630ms step_avg:143.16ms
step:672/1375 train_time:94779ms step_avg:143.17ms
step:673/1375 train_time:94928ms step_avg:143.18ms
step:674/1375 train_time:95077ms step_avg:143.19ms
step:675/1375 train_time:95226ms step_avg:143.20ms
step:676/1375 train_time:95374ms step_avg:143.20ms
step:677/1375 train_time:95524ms step_avg:143.21ms
step:678/1375 train_time:95670ms step_avg:143.22ms
step:679/1375 train_time:95820ms step_avg:143.23ms
step:680/1375 train_time:95969ms step_avg:143.24ms
step:681/1375 train_time:96118ms step_avg:143.25ms
step:682/1375 train_time:96267ms step_avg:143.25ms
step:683/1375 train_time:96414ms step_avg:143.26ms
step:684/1375 train_time:96564ms step_avg:143.27ms
step:685/1375 train_time:96712ms step_avg:143.28ms
step:686/1375 train_time:96862ms step_avg:143.29ms
step:687/1375 train_time:97009ms step_avg:143.29ms
step:688/1375 train_time:97160ms step_avg:143.30ms
step:689/1375 train_time:97309ms step_avg:143.31ms
step:690/1375 train_time:97459ms step_avg:143.32ms
step:691/1375 train_time:97608ms step_avg:143.33ms
step:692/1375 train_time:97756ms step_avg:143.34ms
step:693/1375 train_time:97904ms step_avg:143.34ms
step:694/1375 train_time:98052ms step_avg:143.35ms
step:695/1375 train_time:98201ms step_avg:143.36ms
step:696/1375 train_time:98349ms step_avg:143.37ms
step:697/1375 train_time:98500ms step_avg:143.38ms
step:698/1375 train_time:98648ms step_avg:143.38ms
step:699/1375 train_time:98797ms step_avg:143.39ms
step:700/1375 train_time:98945ms step_avg:143.40ms
step:701/1375 train_time:99093ms step_avg:143.40ms
step:702/1375 train_time:99244ms step_avg:143.42ms
step:703/1375 train_time:99392ms step_avg:143.42ms
step:704/1375 train_time:99541ms step_avg:143.43ms
step:705/1375 train_time:99691ms step_avg:143.44ms
step:706/1375 train_time:99844ms step_avg:143.45ms
step:707/1375 train_time:99991ms step_avg:143.46ms
step:708/1375 train_time:100141ms step_avg:143.47ms
step:709/1375 train_time:100291ms step_avg:143.48ms
step:710/1375 train_time:100441ms step_avg:143.49ms
step:711/1375 train_time:100591ms step_avg:143.50ms
step:712/1375 train_time:100742ms step_avg:143.51ms
step:713/1375 train_time:100891ms step_avg:143.51ms
step:714/1375 train_time:101039ms step_avg:143.52ms
step:715/1375 train_time:101188ms step_avg:143.53ms
step:716/1375 train_time:101339ms step_avg:143.54ms
step:717/1375 train_time:101490ms step_avg:143.55ms
step:718/1375 train_time:101640ms step_avg:143.56ms
step:719/1375 train_time:101790ms step_avg:143.57ms
step:720/1375 train_time:101941ms step_avg:143.58ms
step:721/1375 train_time:102089ms step_avg:143.59ms
step:722/1375 train_time:102240ms step_avg:143.60ms
step:723/1375 train_time:102388ms step_avg:143.60ms
step:724/1375 train_time:102538ms step_avg:143.61ms
step:725/1375 train_time:102688ms step_avg:143.62ms
step:726/1375 train_time:102838ms step_avg:143.63ms
step:727/1375 train_time:102990ms step_avg:143.64ms
step:728/1375 train_time:103139ms step_avg:143.65ms
step:729/1375 train_time:103287ms step_avg:143.65ms
step:730/1375 train_time:103439ms step_avg:143.67ms
step:731/1375 train_time:103588ms step_avg:143.67ms
step:732/1375 train_time:103737ms step_avg:143.68ms
step:733/1375 train_time:103887ms step_avg:143.69ms
step:734/1375 train_time:104037ms step_avg:143.70ms
step:735/1375 train_time:104188ms step_avg:143.71ms
step:736/1375 train_time:104339ms step_avg:143.72ms
step:737/1375 train_time:104490ms step_avg:143.73ms
step:738/1375 train_time:104640ms step_avg:143.74ms
step:739/1375 train_time:104791ms step_avg:143.75ms
step:740/1375 train_time:104943ms step_avg:143.76ms
step:741/1375 train_time:105095ms step_avg:143.77ms
step:742/1375 train_time:105245ms step_avg:143.78ms
step:743/1375 train_time:105393ms step_avg:143.78ms
step:744/1375 train_time:105543ms step_avg:143.79ms
step:745/1375 train_time:105694ms step_avg:143.80ms
step:746/1375 train_time:105844ms step_avg:143.81ms
step:747/1375 train_time:105992ms step_avg:143.82ms
step:748/1375 train_time:106143ms step_avg:143.82ms
step:749/1375 train_time:106293ms step_avg:143.83ms
step:750/1375 train_time:106444ms step_avg:143.84ms
step:750/1375 val_loss:3.5215 train_time:106520ms step_avg:143.95ms
step:751/1375 train_time:106596ms step_avg:143.85ms
step:752/1375 train_time:106748ms step_avg:143.86ms
step:753/1375 train_time:106896ms step_avg:143.87ms
step:754/1375 train_time:107045ms step_avg:143.88ms
step:755/1375 train_time:107194ms step_avg:143.88ms
step:756/1375 train_time:107344ms step_avg:143.89ms
step:757/1375 train_time:107496ms step_avg:143.90ms
step:758/1375 train_time:107648ms step_avg:143.91ms
step:759/1375 train_time:107797ms step_avg:143.92ms
step:760/1375 train_time:107949ms step_avg:143.93ms
step:761/1375 train_time:108135ms step_avg:143.99ms
step:762/1375 train_time:108286ms step_avg:144.00ms
step:763/1375 train_time:108434ms step_avg:144.00ms
step:764/1375 train_time:108584ms step_avg:144.01ms
step:765/1375 train_time:108733ms step_avg:144.02ms
step:766/1375 train_time:108885ms step_avg:144.03ms
step:767/1375 train_time:109037ms step_avg:144.04ms
step:768/1375 train_time:109189ms step_avg:144.05ms
step:769/1375 train_time:109340ms step_avg:144.06ms
step:770/1375 train_time:109490ms step_avg:144.07ms
step:771/1375 train_time:109640ms step_avg:144.07ms
step:772/1375 train_time:109788ms step_avg:144.08ms
step:773/1375 train_time:109939ms step_avg:144.09ms
step:774/1375 train_time:110090ms step_avg:144.10ms
step:775/1375 train_time:110242ms step_avg:144.11ms
step:776/1375 train_time:110393ms step_avg:144.12ms
step:777/1375 train_time:110544ms step_avg:144.13ms
step:778/1375 train_time:110694ms step_avg:144.13ms
step:779/1375 train_time:110843ms step_avg:144.14ms
step:780/1375 train_time:110994ms step_avg:144.15ms
step:781/1375 train_time:111146ms step_avg:144.16ms
step:782/1375 train_time:111295ms step_avg:144.16ms
step:783/1375 train_time:111444ms step_avg:144.17ms
step:784/1375 train_time:111595ms step_avg:144.18ms
step:785/1375 train_time:111745ms step_avg:144.19ms
step:786/1375 train_time:111894ms step_avg:144.19ms
step:787/1375 train_time:112044ms step_avg:144.20ms
step:788/1375 train_time:112195ms step_avg:144.21ms
step:789/1375 train_time:112345ms step_avg:144.22ms
step:790/1375 train_time:112494ms step_avg:144.22ms
step:791/1375 train_time:112644ms step_avg:144.23ms
step:792/1375 train_time:112794ms step_avg:144.24ms
step:793/1375 train_time:112943ms step_avg:144.24ms
step:794/1375 train_time:113094ms step_avg:144.25ms
step:795/1375 train_time:113246ms step_avg:144.26ms
step:796/1375 train_time:113395ms step_avg:144.27ms
step:797/1375 train_time:113546ms step_avg:144.28ms
step:798/1375 train_time:113697ms step_avg:144.28ms
step:799/1375 train_time:113852ms step_avg:144.30ms
step:800/1375 train_time:114000ms step_avg:144.30ms
step:801/1375 train_time:114151ms step_avg:144.31ms
step:802/1375 train_time:114301ms step_avg:144.32ms
step:803/1375 train_time:114449ms step_avg:144.32ms
step:804/1375 train_time:114598ms step_avg:144.33ms
step:805/1375 train_time:114753ms step_avg:144.34ms
step:806/1375 train_time:114902ms step_avg:144.35ms
step:807/1375 train_time:115052ms step_avg:144.36ms
step:808/1375 train_time:115203ms step_avg:144.36ms
step:809/1375 train_time:115353ms step_avg:144.37ms
step:810/1375 train_time:115501ms step_avg:144.38ms
step:811/1375 train_time:115653ms step_avg:144.39ms
step:812/1375 train_time:115802ms step_avg:144.39ms
step:813/1375 train_time:115953ms step_avg:144.40ms
step:814/1375 train_time:116100ms step_avg:144.40ms
step:815/1375 train_time:116250ms step_avg:144.41ms
step:816/1375 train_time:116401ms step_avg:144.42ms
step:817/1375 train_time:116553ms step_avg:144.43ms
step:818/1375 train_time:116704ms step_avg:144.44ms
step:819/1375 train_time:116856ms step_avg:144.45ms
step:820/1375 train_time:117012ms step_avg:144.46ms
step:821/1375 train_time:117161ms step_avg:144.46ms
step:822/1375 train_time:117313ms step_avg:144.47ms
step:823/1375 train_time:117462ms step_avg:144.48ms
step:824/1375 train_time:117614ms step_avg:144.49ms
step:825/1375 train_time:117767ms step_avg:144.50ms
step:826/1375 train_time:117921ms step_avg:144.51ms
step:827/1375 train_time:118072ms step_avg:144.52ms
step:828/1375 train_time:118224ms step_avg:144.53ms
step:829/1375 train_time:118375ms step_avg:144.54ms
step:830/1375 train_time:118526ms step_avg:144.54ms
step:831/1375 train_time:118676ms step_avg:144.55ms
step:832/1375 train_time:118829ms step_avg:144.56ms
step:833/1375 train_time:118983ms step_avg:144.57ms
step:834/1375 train_time:119134ms step_avg:144.58ms
step:835/1375 train_time:119286ms step_avg:144.59ms
step:836/1375 train_time:119438ms step_avg:144.60ms
step:837/1375 train_time:119589ms step_avg:144.61ms
step:838/1375 train_time:119740ms step_avg:144.61ms
step:839/1375 train_time:119891ms step_avg:144.62ms
step:840/1375 train_time:120041ms step_avg:144.63ms
step:841/1375 train_time:120192ms step_avg:144.64ms
step:842/1375 train_time:120344ms step_avg:144.64ms
step:843/1375 train_time:120493ms step_avg:144.65ms
step:844/1375 train_time:120645ms step_avg:144.66ms
step:845/1375 train_time:120795ms step_avg:144.66ms
step:846/1375 train_time:120949ms step_avg:144.68ms
step:847/1375 train_time:121101ms step_avg:144.68ms
step:848/1375 train_time:121252ms step_avg:144.69ms
step:849/1375 train_time:121404ms step_avg:144.70ms
step:850/1375 train_time:121557ms step_avg:144.71ms
step:851/1375 train_time:121710ms step_avg:144.72ms
step:852/1375 train_time:121861ms step_avg:144.73ms
step:853/1375 train_time:122010ms step_avg:144.73ms
step:854/1375 train_time:122161ms step_avg:144.74ms
step:855/1375 train_time:122311ms step_avg:144.75ms
step:856/1375 train_time:122461ms step_avg:144.75ms
step:857/1375 train_time:122614ms step_avg:144.76ms
step:858/1375 train_time:122770ms step_avg:144.78ms
step:859/1375 train_time:122920ms step_avg:144.78ms
step:860/1375 train_time:123072ms step_avg:144.79ms
step:861/1375 train_time:123223ms step_avg:144.80ms
step:862/1375 train_time:123375ms step_avg:144.81ms
step:863/1375 train_time:123528ms step_avg:144.82ms
step:864/1375 train_time:123679ms step_avg:144.82ms
step:865/1375 train_time:123830ms step_avg:144.83ms
step:866/1375 train_time:123986ms step_avg:144.84ms
step:867/1375 train_time:124137ms step_avg:144.85ms
step:868/1375 train_time:124287ms step_avg:144.86ms
step:869/1375 train_time:124438ms step_avg:144.86ms
step:870/1375 train_time:124594ms step_avg:144.88ms
step:871/1375 train_time:124746ms step_avg:144.89ms
step:872/1375 train_time:124896ms step_avg:144.89ms
step:873/1375 train_time:125048ms step_avg:144.90ms
step:874/1375 train_time:125200ms step_avg:144.91ms
step:875/1375 train_time:125353ms step_avg:144.92ms
step:875/1375 val_loss:3.4688 train_time:125428ms step_avg:145.00ms
step:876/1375 train_time:125503ms step_avg:144.92ms
step:877/1375 train_time:125656ms step_avg:144.93ms
step:878/1375 train_time:125806ms step_avg:144.94ms
step:879/1375 train_time:125957ms step_avg:144.94ms
step:880/1375 train_time:126107ms step_avg:144.95ms
step:881/1375 train_time:126257ms step_avg:144.96ms
step:882/1375 train_time:126413ms step_avg:144.97ms
step:883/1375 train_time:126564ms step_avg:144.98ms
step:884/1375 train_time:126717ms step_avg:144.99ms
step:885/1375 train_time:126868ms step_avg:144.99ms
step:886/1375 train_time:127021ms step_avg:145.00ms
step:887/1375 train_time:127172ms step_avg:145.01ms
step:888/1375 train_time:127324ms step_avg:145.02ms
step:889/1375 train_time:127478ms step_avg:145.03ms
step:890/1375 train_time:127627ms step_avg:145.03ms
step:891/1375 train_time:127780ms step_avg:145.04ms
step:892/1375 train_time:127932ms step_avg:145.05ms
step:893/1375 train_time:128084ms step_avg:145.06ms
step:894/1375 train_time:128235ms step_avg:145.06ms
step:895/1375 train_time:128390ms step_avg:145.07ms
step:896/1375 train_time:128540ms step_avg:145.08ms
step:897/1375 train_time:128691ms step_avg:145.09ms
step:898/1375 train_time:128844ms step_avg:145.09ms
step:899/1375 train_time:128996ms step_avg:145.10ms
step:900/1375 train_time:129147ms step_avg:145.11ms
step:901/1375 train_time:129301ms step_avg:145.12ms
step:902/1375 train_time:129450ms step_avg:145.12ms
step:903/1375 train_time:129603ms step_avg:145.13ms
step:904/1375 train_time:129755ms step_avg:145.14ms
step:905/1375 train_time:129906ms step_avg:145.15ms
step:906/1375 train_time:130057ms step_avg:145.15ms
step:907/1375 train_time:130213ms step_avg:145.17ms
step:908/1375 train_time:130363ms step_avg:145.17ms
step:909/1375 train_time:130518ms step_avg:145.18ms
step:910/1375 train_time:130673ms step_avg:145.19ms
step:911/1375 train_time:130823ms step_avg:145.20ms
step:912/1375 train_time:130976ms step_avg:145.21ms
step:913/1375 train_time:131128ms step_avg:145.21ms
step:914/1375 train_time:131280ms step_avg:145.22ms
step:915/1375 train_time:131435ms step_avg:145.23ms
step:916/1375 train_time:131586ms step_avg:145.24ms
step:917/1375 train_time:131738ms step_avg:145.25ms
step:918/1375 train_time:131890ms step_avg:145.25ms
step:919/1375 train_time:132046ms step_avg:145.27ms
step:920/1375 train_time:132199ms step_avg:145.27ms
step:921/1375 train_time:132352ms step_avg:145.28ms
step:922/1375 train_time:132507ms step_avg:145.29ms
step:923/1375 train_time:132659ms step_avg:145.30ms
step:924/1375 train_time:132813ms step_avg:145.31ms
step:925/1375 train_time:132967ms step_avg:145.32ms
step:926/1375 train_time:133120ms step_avg:145.33ms
step:927/1375 train_time:133273ms step_avg:145.34ms
step:928/1375 train_time:133426ms step_avg:145.34ms
step:929/1375 train_time:133582ms step_avg:145.36ms
step:930/1375 train_time:133737ms step_avg:145.37ms
step:931/1375 train_time:133889ms step_avg:145.37ms
step:932/1375 train_time:134041ms step_avg:145.38ms
step:933/1375 train_time:134194ms step_avg:145.39ms
step:934/1375 train_time:134346ms step_avg:145.40ms
step:935/1375 train_time:134501ms step_avg:145.41ms
step:936/1375 train_time:134652ms step_avg:145.41ms
step:937/1375 train_time:134809ms step_avg:145.43ms
step:938/1375 train_time:134960ms step_avg:145.43ms
step:939/1375 train_time:135114ms step_avg:145.44ms
step:940/1375 train_time:135266ms step_avg:145.45ms
step:941/1375 train_time:135419ms step_avg:145.46ms
step:942/1375 train_time:135573ms step_avg:145.46ms
step:943/1375 train_time:135727ms step_avg:145.47ms
step:944/1375 train_time:135888ms step_avg:145.49ms
step:945/1375 train_time:136041ms step_avg:145.50ms
step:946/1375 train_time:136194ms step_avg:145.51ms
step:947/1375 train_time:136347ms step_avg:145.51ms
step:948/1375 train_time:136499ms step_avg:145.52ms
step:949/1375 train_time:136653ms step_avg:145.53ms
step:950/1375 train_time:136808ms step_avg:145.54ms
step:951/1375 train_time:137003ms step_avg:145.59ms
step:952/1375 train_time:137154ms step_avg:145.60ms
step:953/1375 train_time:137307ms step_avg:145.61ms
step:954/1375 train_time:137458ms step_avg:145.61ms
step:955/1375 train_time:137609ms step_avg:145.62ms
step:956/1375 train_time:137762ms step_avg:145.63ms
step:957/1375 train_time:137918ms step_avg:145.64ms
step:958/1375 train_time:138078ms step_avg:145.65ms
step:959/1375 train_time:138233ms step_avg:145.66ms
step:960/1375 train_time:138388ms step_avg:145.67ms
step:961/1375 train_time:138539ms step_avg:145.68ms
step:962/1375 train_time:138691ms step_avg:145.68ms
step:963/1375 train_time:138851ms step_avg:145.70ms
step:964/1375 train_time:139006ms step_avg:145.71ms
step:965/1375 train_time:139158ms step_avg:145.71ms
step:966/1375 train_time:139312ms step_avg:145.72ms
step:967/1375 train_time:139463ms step_avg:145.73ms
step:968/1375 train_time:139615ms step_avg:145.74ms
step:969/1375 train_time:139767ms step_avg:145.74ms
step:970/1375 train_time:139918ms step_avg:145.75ms
step:971/1375 train_time:140074ms step_avg:145.76ms
step:972/1375 train_time:140225ms step_avg:145.76ms
step:973/1375 train_time:140377ms step_avg:145.77ms
step:974/1375 train_time:140531ms step_avg:145.78ms
step:975/1375 train_time:140685ms step_avg:145.79ms
step:976/1375 train_time:140838ms step_avg:145.80ms
step:977/1375 train_time:140990ms step_avg:145.80ms
step:978/1375 train_time:141141ms step_avg:145.81ms
step:979/1375 train_time:141294ms step_avg:145.81ms
step:980/1375 train_time:141449ms step_avg:145.82ms
step:981/1375 train_time:141599ms step_avg:145.83ms
step:982/1375 train_time:141751ms step_avg:145.83ms
step:983/1375 train_time:141902ms step_avg:145.84ms
step:984/1375 train_time:142053ms step_avg:145.85ms
step:985/1375 train_time:142206ms step_avg:145.85ms
step:986/1375 train_time:142361ms step_avg:145.86ms
step:987/1375 train_time:142514ms step_avg:145.87ms
step:988/1375 train_time:142666ms step_avg:145.88ms
step:989/1375 train_time:142817ms step_avg:145.88ms
step:990/1375 train_time:142972ms step_avg:145.89ms
step:991/1375 train_time:143122ms step_avg:145.89ms
step:992/1375 train_time:143281ms step_avg:145.91ms
step:993/1375 train_time:143442ms step_avg:145.92ms
step:994/1375 train_time:143593ms step_avg:145.93ms
step:995/1375 train_time:143744ms step_avg:145.93ms
step:996/1375 train_time:143895ms step_avg:145.94ms
step:997/1375 train_time:144045ms step_avg:145.94ms
step:998/1375 train_time:144198ms step_avg:145.95ms
step:999/1375 train_time:144352ms step_avg:145.96ms
step:1000/1375 train_time:144505ms step_avg:145.96ms
step:1000/1375 val_loss:3.4038 train_time:144580ms step_avg:146.04ms
step:1001/1375 train_time:144659ms step_avg:145.97ms
step:1002/1375 train_time:144812ms step_avg:145.98ms
step:1003/1375 train_time:144966ms step_avg:145.99ms
step:1004/1375 train_time:145119ms step_avg:146.00ms
step:1005/1375 train_time:145271ms step_avg:146.00ms
step:1006/1375 train_time:145421ms step_avg:146.01ms
step:1007/1375 train_time:145579ms step_avg:146.02ms
step:1008/1375 train_time:145732ms step_avg:146.02ms
step:1009/1375 train_time:145895ms step_avg:146.04ms
step:1010/1375 train_time:146046ms step_avg:146.05ms
step:1011/1375 train_time:146199ms step_avg:146.05ms
step:1012/1375 train_time:146348ms step_avg:146.06ms
step:1013/1375 train_time:146502ms step_avg:146.06ms
step:1014/1375 train_time:146655ms step_avg:146.07ms
step:1015/1375 train_time:146809ms step_avg:146.08ms
step:1016/1375 train_time:146964ms step_avg:146.09ms
step:1017/1375 train_time:147116ms step_avg:146.09ms
step:1018/1375 train_time:147267ms step_avg:146.10ms
step:1019/1375 train_time:147421ms step_avg:146.11ms
step:1020/1375 train_time:147577ms step_avg:146.12ms
step:1021/1375 train_time:147728ms step_avg:146.12ms
step:1022/1375 train_time:147881ms step_avg:146.13ms
step:1023/1375 train_time:148035ms step_avg:146.14ms
step:1024/1375 train_time:148187ms step_avg:146.14ms
step:1025/1375 train_time:148341ms step_avg:146.15ms
step:1026/1375 train_time:148494ms step_avg:146.16ms
step:1027/1375 train_time:148646ms step_avg:146.16ms
step:1028/1375 train_time:148803ms step_avg:146.17ms
step:1029/1375 train_time:148959ms step_avg:146.18ms
step:1030/1375 train_time:149112ms step_avg:146.19ms
step:1031/1375 train_time:149263ms step_avg:146.19ms
step:1032/1375 train_time:149414ms step_avg:146.20ms
step:1033/1375 train_time:149569ms step_avg:146.21ms
step:1034/1375 train_time:149722ms step_avg:146.21ms
step:1035/1375 train_time:149879ms step_avg:146.22ms
step:1036/1375 train_time:150033ms step_avg:146.23ms
step:1037/1375 train_time:150187ms step_avg:146.24ms
step:1038/1375 train_time:150342ms step_avg:146.25ms
step:1039/1375 train_time:150494ms step_avg:146.25ms
step:1040/1375 train_time:150647ms step_avg:146.26ms
step:1041/1375 train_time:150802ms step_avg:146.27ms
step:1042/1375 train_time:150954ms step_avg:146.27ms
step:1043/1375 train_time:151105ms step_avg:146.28ms
step:1044/1375 train_time:151261ms step_avg:146.29ms
step:1045/1375 train_time:151415ms step_avg:146.29ms
step:1046/1375 train_time:151567ms step_avg:146.30ms
step:1047/1375 train_time:151719ms step_avg:146.31ms
step:1048/1375 train_time:151875ms step_avg:146.31ms
step:1049/1375 train_time:152029ms step_avg:146.32ms
step:1050/1375 train_time:152186ms step_avg:146.33ms
step:1051/1375 train_time:152343ms step_avg:146.34ms
step:1052/1375 train_time:152497ms step_avg:146.35ms
step:1053/1375 train_time:152649ms step_avg:146.36ms
step:1054/1375 train_time:152803ms step_avg:146.36ms
step:1055/1375 train_time:152957ms step_avg:146.37ms
step:1056/1375 train_time:153111ms step_avg:146.38ms
step:1057/1375 train_time:153265ms step_avg:146.39ms
step:1058/1375 train_time:153421ms step_avg:146.39ms
step:1059/1375 train_time:153576ms step_avg:146.40ms
step:1060/1375 train_time:153731ms step_avg:146.41ms
step:1061/1375 train_time:153884ms step_avg:146.42ms
step:1062/1375 train_time:154038ms step_avg:146.42ms
step:1063/1375 train_time:154190ms step_avg:146.43ms
step:1064/1375 train_time:154343ms step_avg:146.44ms
step:1065/1375 train_time:154499ms step_avg:146.44ms
step:1066/1375 train_time:154656ms step_avg:146.45ms
step:1067/1375 train_time:154813ms step_avg:146.46ms
step:1068/1375 train_time:154966ms step_avg:146.47ms
step:1069/1375 train_time:155125ms step_avg:146.48ms
step:1070/1375 train_time:155278ms step_avg:146.49ms
step:1071/1375 train_time:155434ms step_avg:146.50ms
step:1072/1375 train_time:155585ms step_avg:146.50ms
step:1073/1375 train_time:155739ms step_avg:146.51ms
step:1074/1375 train_time:155892ms step_avg:146.52ms
step:1075/1375 train_time:156045ms step_avg:146.52ms
step:1076/1375 train_time:156199ms step_avg:146.53ms
step:1077/1375 train_time:156352ms step_avg:146.53ms
step:1078/1375 train_time:156510ms step_avg:146.55ms
step:1079/1375 train_time:156669ms step_avg:146.56ms
step:1080/1375 train_time:156823ms step_avg:146.56ms
step:1081/1375 train_time:156976ms step_avg:146.57ms
step:1082/1375 train_time:157128ms step_avg:146.57ms
step:1083/1375 train_time:157281ms step_avg:146.58ms
step:1084/1375 train_time:157439ms step_avg:146.59ms
step:1085/1375 train_time:157591ms step_avg:146.60ms
step:1086/1375 train_time:157747ms step_avg:146.61ms
step:1087/1375 train_time:157902ms step_avg:146.61ms
step:1088/1375 train_time:158055ms step_avg:146.62ms
step:1089/1375 train_time:158212ms step_avg:146.63ms
step:1090/1375 train_time:158371ms step_avg:146.64ms
step:1091/1375 train_time:158525ms step_avg:146.65ms
step:1092/1375 train_time:158678ms step_avg:146.65ms
step:1093/1375 train_time:158834ms step_avg:146.66ms
step:1094/1375 train_time:158987ms step_avg:146.67ms
step:1095/1375 train_time:159140ms step_avg:146.67ms
step:1096/1375 train_time:159297ms step_avg:146.68ms
step:1097/1375 train_time:159452ms step_avg:146.69ms
step:1098/1375 train_time:159606ms step_avg:146.70ms
step:1099/1375 train_time:159759ms step_avg:146.70ms
step:1100/1375 train_time:159910ms step_avg:146.71ms
step:1101/1375 train_time:160063ms step_avg:146.71ms
step:1102/1375 train_time:160220ms step_avg:146.72ms
step:1103/1375 train_time:160372ms step_avg:146.73ms
step:1104/1375 train_time:160525ms step_avg:146.73ms
step:1105/1375 train_time:160683ms step_avg:146.74ms
step:1106/1375 train_time:160838ms step_avg:146.75ms
step:1107/1375 train_time:160990ms step_avg:146.75ms
step:1108/1375 train_time:161146ms step_avg:146.76ms
step:1109/1375 train_time:161299ms step_avg:146.77ms
step:1110/1375 train_time:161453ms step_avg:146.78ms
step:1111/1375 train_time:161609ms step_avg:146.78ms
step:1112/1375 train_time:161764ms step_avg:146.79ms
step:1113/1375 train_time:161916ms step_avg:146.80ms
step:1114/1375 train_time:162074ms step_avg:146.81ms
step:1115/1375 train_time:162228ms step_avg:146.81ms
step:1116/1375 train_time:162380ms step_avg:146.82ms
step:1117/1375 train_time:162538ms step_avg:146.83ms
step:1118/1375 train_time:162697ms step_avg:146.84ms
step:1119/1375 train_time:162849ms step_avg:146.84ms
step:1120/1375 train_time:163003ms step_avg:146.85ms
step:1121/1375 train_time:163157ms step_avg:146.86ms
step:1122/1375 train_time:163311ms step_avg:146.86ms
step:1123/1375 train_time:163466ms step_avg:146.87ms
step:1124/1375 train_time:163621ms step_avg:146.88ms
step:1125/1375 train_time:163777ms step_avg:146.89ms
step:1125/1375 val_loss:3.3500 train_time:163854ms step_avg:146.95ms
step:1126/1375 train_time:163931ms step_avg:146.89ms
step:1127/1375 train_time:164088ms step_avg:146.90ms
step:1128/1375 train_time:164244ms step_avg:146.91ms
step:1129/1375 train_time:164402ms step_avg:146.92ms
step:1130/1375 train_time:164555ms step_avg:146.92ms
step:1131/1375 train_time:164712ms step_avg:146.93ms
step:1132/1375 train_time:164865ms step_avg:146.94ms
step:1133/1375 train_time:165020ms step_avg:146.95ms
step:1134/1375 train_time:165177ms step_avg:146.95ms
step:1135/1375 train_time:165330ms step_avg:146.96ms
step:1136/1375 train_time:165487ms step_avg:146.97ms
step:1137/1375 train_time:165640ms step_avg:146.97ms
step:1138/1375 train_time:165796ms step_avg:146.98ms
step:1139/1375 train_time:165951ms step_avg:146.99ms
step:1140/1375 train_time:166107ms step_avg:147.00ms
step:1141/1375 train_time:166300ms step_avg:147.04ms
step:1142/1375 train_time:166455ms step_avg:147.05ms
step:1143/1375 train_time:166614ms step_avg:147.06ms
step:1144/1375 train_time:166769ms step_avg:147.06ms
step:1145/1375 train_time:166921ms step_avg:147.07ms
step:1146/1375 train_time:167076ms step_avg:147.07ms
step:1147/1375 train_time:167231ms step_avg:147.08ms
step:1148/1375 train_time:167387ms step_avg:147.09ms
step:1149/1375 train_time:167543ms step_avg:147.10ms
step:1150/1375 train_time:167697ms step_avg:147.10ms
step:1151/1375 train_time:167852ms step_avg:147.11ms
step:1152/1375 train_time:168007ms step_avg:147.12ms
step:1153/1375 train_time:168162ms step_avg:147.12ms
step:1154/1375 train_time:168315ms step_avg:147.13ms
step:1155/1375 train_time:168469ms step_avg:147.13ms
step:1156/1375 train_time:168629ms step_avg:147.15ms
step:1157/1375 train_time:168788ms step_avg:147.16ms
step:1158/1375 train_time:168942ms step_avg:147.16ms
step:1159/1375 train_time:169095ms step_avg:147.17ms
step:1160/1375 train_time:169248ms step_avg:147.17ms
step:1161/1375 train_time:169403ms step_avg:147.18ms
step:1162/1375 train_time:169559ms step_avg:147.19ms
step:1163/1375 train_time:169714ms step_avg:147.19ms
step:1164/1375 train_time:169868ms step_avg:147.20ms
step:1165/1375 train_time:170021ms step_avg:147.20ms
step:1166/1375 train_time:170178ms step_avg:147.21ms
step:1167/1375 train_time:170330ms step_avg:147.22ms
step:1168/1375 train_time:170487ms step_avg:147.23ms
step:1169/1375 train_time:170642ms step_avg:147.23ms
step:1170/1375 train_time:170799ms step_avg:147.24ms
step:1171/1375 train_time:170956ms step_avg:147.25ms
step:1172/1375 train_time:171112ms step_avg:147.26ms
step:1173/1375 train_time:171269ms step_avg:147.26ms
step:1174/1375 train_time:171432ms step_avg:147.28ms
step:1175/1375 train_time:171588ms step_avg:147.29ms
step:1176/1375 train_time:171746ms step_avg:147.30ms
step:1177/1375 train_time:171908ms step_avg:147.31ms
step:1178/1375 train_time:172062ms step_avg:147.31ms
step:1179/1375 train_time:172218ms step_avg:147.32ms
step:1180/1375 train_time:172385ms step_avg:147.34ms
step:1181/1375 train_time:172538ms step_avg:147.34ms
step:1182/1375 train_time:172692ms step_avg:147.35ms
step:1183/1375 train_time:172848ms step_avg:147.36ms
step:1184/1375 train_time:173003ms step_avg:147.36ms
step:1185/1375 train_time:173163ms step_avg:147.37ms
step:1186/1375 train_time:173317ms step_avg:147.38ms
step:1187/1375 train_time:173477ms step_avg:147.39ms
step:1188/1375 train_time:173632ms step_avg:147.40ms
step:1189/1375 train_time:173789ms step_avg:147.40ms
step:1190/1375 train_time:173943ms step_avg:147.41ms
step:1191/1375 train_time:174101ms step_avg:147.42ms
step:1192/1375 train_time:174253ms step_avg:147.42ms
step:1193/1375 train_time:174408ms step_avg:147.43ms
step:1194/1375 train_time:174565ms step_avg:147.44ms
step:1195/1375 train_time:174720ms step_avg:147.44ms
step:1196/1375 train_time:174874ms step_avg:147.45ms
step:1197/1375 train_time:175030ms step_avg:147.46ms
step:1198/1375 train_time:175189ms step_avg:147.47ms
step:1199/1375 train_time:175343ms step_avg:147.47ms
step:1200/1375 train_time:175497ms step_avg:147.48ms
step:1201/1375 train_time:175653ms step_avg:147.48ms
step:1202/1375 train_time:175821ms step_avg:147.50ms
step:1203/1375 train_time:175981ms step_avg:147.51ms
step:1204/1375 train_time:176137ms step_avg:147.52ms
step:1205/1375 train_time:176292ms step_avg:147.52ms
step:1206/1375 train_time:176446ms step_avg:147.53ms
step:1207/1375 train_time:176599ms step_avg:147.53ms
step:1208/1375 train_time:176758ms step_avg:147.54ms
step:1209/1375 train_time:176913ms step_avg:147.55ms
step:1210/1375 train_time:177072ms step_avg:147.56ms
step:1211/1375 train_time:177228ms step_avg:147.57ms
step:1212/1375 train_time:177382ms step_avg:147.57ms
step:1213/1375 train_time:177536ms step_avg:147.58ms
step:1214/1375 train_time:177692ms step_avg:147.59ms
step:1215/1375 train_time:177849ms step_avg:147.59ms
step:1216/1375 train_time:178001ms step_avg:147.60ms
step:1217/1375 train_time:178155ms step_avg:147.60ms
step:1218/1375 train_time:178308ms step_avg:147.61ms
step:1219/1375 train_time:178460ms step_avg:147.61ms
step:1220/1375 train_time:178613ms step_avg:147.61ms
step:1221/1375 train_time:178766ms step_avg:147.62ms
step:1222/1375 train_time:178921ms step_avg:147.62ms
step:1223/1375 train_time:179077ms step_avg:147.63ms
step:1224/1375 train_time:179234ms step_avg:147.64ms
step:1225/1375 train_time:179389ms step_avg:147.65ms
step:1226/1375 train_time:179545ms step_avg:147.65ms
step:1227/1375 train_time:179702ms step_avg:147.66ms
step:1228/1375 train_time:179856ms step_avg:147.66ms
step:1229/1375 train_time:180010ms step_avg:147.67ms
step:1230/1375 train_time:180170ms step_avg:147.68ms
step:1231/1375 train_time:180327ms step_avg:147.69ms
step:1232/1375 train_time:180490ms step_avg:147.70ms
step:1233/1375 train_time:180646ms step_avg:147.71ms
step:1234/1375 train_time:180801ms step_avg:147.71ms
step:1235/1375 train_time:180959ms step_avg:147.72ms
step:1236/1375 train_time:181114ms step_avg:147.73ms
step:1237/1375 train_time:181267ms step_avg:147.73ms
step:1238/1375 train_time:181432ms step_avg:147.75ms
step:1239/1375 train_time:181589ms step_avg:147.75ms
step:1240/1375 train_time:181748ms step_avg:147.76ms
step:1241/1375 train_time:181908ms step_avg:147.77ms
step:1242/1375 train_time:182064ms step_avg:147.78ms
step:1243/1375 train_time:182225ms step_avg:147.79ms
step:1244/1375 train_time:182380ms step_avg:147.80ms
step:1245/1375 train_time:182535ms step_avg:147.80ms
step:1246/1375 train_time:182690ms step_avg:147.81ms
step:1247/1375 train_time:182846ms step_avg:147.81ms
step:1248/1375 train_time:183000ms step_avg:147.82ms
step:1249/1375 train_time:183153ms step_avg:147.82ms
step:1250/1375 train_time:183308ms step_avg:147.83ms
step:1250/1375 val_loss:3.3046 train_time:183388ms step_avg:147.89ms
step:1251/1375 train_time:183469ms step_avg:147.84ms
step:1252/1375 train_time:183623ms step_avg:147.84ms
step:1253/1375 train_time:183778ms step_avg:147.85ms
step:1254/1375 train_time:183931ms step_avg:147.85ms
step:1255/1375 train_time:184096ms step_avg:147.87ms
step:1256/1375 train_time:184251ms step_avg:147.87ms
step:1257/1375 train_time:184406ms step_avg:147.88ms
step:1258/1375 train_time:184566ms step_avg:147.89ms
step:1259/1375 train_time:184723ms step_avg:147.90ms
step:1260/1375 train_time:184875ms step_avg:147.90ms
step:1261/1375 train_time:185032ms step_avg:147.91ms
step:1262/1375 train_time:185191ms step_avg:147.92ms
step:1263/1375 train_time:185347ms step_avg:147.92ms
step:1264/1375 train_time:185499ms step_avg:147.93ms
step:1265/1375 train_time:185653ms step_avg:147.93ms
step:1266/1375 train_time:185811ms step_avg:147.94ms
step:1267/1375 train_time:185970ms step_avg:147.95ms
step:1268/1375 train_time:186127ms step_avg:147.95ms
step:1269/1375 train_time:186289ms step_avg:147.97ms
step:1270/1375 train_time:186445ms step_avg:147.97ms
step:1271/1375 train_time:186601ms step_avg:147.98ms
step:1272/1375 train_time:186754ms step_avg:147.98ms
step:1273/1375 train_time:186909ms step_avg:147.99ms
step:1274/1375 train_time:187065ms step_avg:147.99ms
step:1275/1375 train_time:187220ms step_avg:148.00ms
step:1276/1375 train_time:187372ms step_avg:148.00ms
step:1277/1375 train_time:187530ms step_avg:148.01ms
step:1278/1375 train_time:187684ms step_avg:148.02ms
step:1279/1375 train_time:187841ms step_avg:148.02ms
step:1280/1375 train_time:188006ms step_avg:148.04ms
step:1281/1375 train_time:188162ms step_avg:148.04ms
step:1282/1375 train_time:188316ms step_avg:148.05ms
step:1283/1375 train_time:188473ms step_avg:148.05ms
step:1284/1375 train_time:188632ms step_avg:148.06ms
step:1285/1375 train_time:188787ms step_avg:148.07ms
step:1286/1375 train_time:188941ms step_avg:148.07ms
step:1287/1375 train_time:189097ms step_avg:148.08ms
step:1288/1375 train_time:189252ms step_avg:148.08ms
step:1289/1375 train_time:189414ms step_avg:148.10ms
step:1290/1375 train_time:189575ms step_avg:148.11ms
step:1291/1375 train_time:189733ms step_avg:148.11ms
step:1292/1375 train_time:189892ms step_avg:148.12ms
step:1293/1375 train_time:190050ms step_avg:148.13ms
step:1294/1375 train_time:190204ms step_avg:148.13ms
step:1295/1375 train_time:190360ms step_avg:148.14ms
step:1296/1375 train_time:190517ms step_avg:148.15ms
step:1297/1375 train_time:190676ms step_avg:148.16ms
step:1298/1375 train_time:190832ms step_avg:148.16ms
step:1299/1375 train_time:190988ms step_avg:148.17ms
step:1300/1375 train_time:191144ms step_avg:148.17ms
step:1301/1375 train_time:191297ms step_avg:148.18ms
step:1302/1375 train_time:191454ms step_avg:148.18ms
step:1303/1375 train_time:191612ms step_avg:148.19ms
step:1304/1375 train_time:191772ms step_avg:148.20ms
step:1305/1375 train_time:191926ms step_avg:148.21ms
step:1306/1375 train_time:192084ms step_avg:148.21ms
step:1307/1375 train_time:192237ms step_avg:148.22ms
step:1308/1375 train_time:192395ms step_avg:148.22ms
step:1309/1375 train_time:192551ms step_avg:148.23ms
step:1310/1375 train_time:192706ms step_avg:148.24ms
step:1311/1375 train_time:192859ms step_avg:148.24ms
step:1312/1375 train_time:193011ms step_avg:148.24ms
step:1313/1375 train_time:193168ms step_avg:148.25ms
step:1314/1375 train_time:193323ms step_avg:148.25ms
step:1315/1375 train_time:193481ms step_avg:148.26ms
step:1316/1375 train_time:193636ms step_avg:148.27ms
step:1317/1375 train_time:193790ms step_avg:148.27ms
step:1318/1375 train_time:193950ms step_avg:148.28ms
step:1319/1375 train_time:194104ms step_avg:148.28ms
step:1320/1375 train_time:194260ms step_avg:148.29ms
step:1321/1375 train_time:194417ms step_avg:148.30ms
step:1322/1375 train_time:194577ms step_avg:148.31ms
step:1323/1375 train_time:194732ms step_avg:148.31ms
step:1324/1375 train_time:194887ms step_avg:148.32ms
step:1325/1375 train_time:195043ms step_avg:148.32ms
step:1326/1375 train_time:195202ms step_avg:148.33ms
step:1327/1375 train_time:195356ms step_avg:148.33ms
step:1328/1375 train_time:195512ms step_avg:148.34ms
step:1329/1375 train_time:195687ms step_avg:148.36ms
step:1330/1375 train_time:195847ms step_avg:148.37ms
step:1331/1375 train_time:196043ms step_avg:148.40ms
step:1332/1375 train_time:196210ms step_avg:148.42ms
step:1333/1375 train_time:196369ms step_avg:148.43ms
step:1334/1375 train_time:196524ms step_avg:148.43ms
step:1335/1375 train_time:196677ms step_avg:148.44ms
step:1336/1375 train_time:196843ms step_avg:148.45ms
step:1337/1375 train_time:197001ms step_avg:148.46ms
step:1338/1375 train_time:197157ms step_avg:148.46ms
step:1339/1375 train_time:197314ms step_avg:148.47ms
step:1340/1375 train_time:197472ms step_avg:148.47ms
step:1341/1375 train_time:197626ms step_avg:148.48ms
step:1342/1375 train_time:197784ms step_avg:148.49ms
step:1343/1375 train_time:197939ms step_avg:148.49ms
step:1344/1375 train_time:198093ms step_avg:148.50ms
step:1345/1375 train_time:198249ms step_avg:148.50ms
step:1346/1375 train_time:198405ms step_avg:148.51ms
step:1347/1375 train_time:198565ms step_avg:148.52ms
step:1348/1375 train_time:198720ms step_avg:148.52ms
step:1349/1375 train_time:198876ms step_avg:148.53ms
step:1350/1375 train_time:199031ms step_avg:148.53ms
step:1351/1375 train_time:199188ms step_avg:148.54ms
step:1352/1375 train_time:199351ms step_avg:148.55ms
step:1353/1375 train_time:199511ms step_avg:148.56ms
step:1354/1375 train_time:199672ms step_avg:148.57ms
step:1355/1375 train_time:199830ms step_avg:148.57ms
step:1356/1375 train_time:199987ms step_avg:148.58ms
step:1357/1375 train_time:200145ms step_avg:148.59ms
step:1358/1375 train_time:200305ms step_avg:148.59ms
step:1359/1375 train_time:200461ms step_avg:148.60ms
step:1360/1375 train_time:200622ms step_avg:148.61ms
step:1361/1375 train_time:200781ms step_avg:148.62ms
step:1362/1375 train_time:200939ms step_avg:148.62ms
step:1363/1375 train_time:201103ms step_avg:148.63ms
step:1364/1375 train_time:201257ms step_avg:148.64ms
step:1365/1375 train_time:201410ms step_avg:148.64ms
step:1366/1375 train_time:201569ms step_avg:148.65ms
step:1367/1375 train_time:201724ms step_avg:148.65ms
step:1368/1375 train_time:201882ms step_avg:148.66ms
step:1369/1375 train_time:202049ms step_avg:148.68ms
step:1370/1375 train_time:202209ms step_avg:148.68ms
step:1371/1375 train_time:202363ms step_avg:148.69ms
step:1372/1375 train_time:202527ms step_avg:148.70ms
step:1373/1375 train_time:202681ms step_avg:148.70ms
step:1374/1375 train_time:202840ms step_avg:148.71ms
step:1375/1375 train_time:202996ms step_avg:148.72ms
step:1375/1375 val_loss:3.2791 train_time:203072ms step_avg:148.77ms
peak memory consumption: 31565 MiB

import os
import sys
with open(sys.argv[0]) as f:
    code = f.read() # read the code of this file ASAP, for logging
import uuid
import time
import glob
import subprocess
import contextlib
from dataclasses import dataclass

import torch
torch.empty(1, device='cuda', requires_grad=True).backward()
from torch import nn
import torch.nn.functional as F
import torch.distributed as dist
from torch.nn.parallel import DistributedDataParallel as DDP
# use of FlexAttention contributed by @KoszarskyB
from torch.nn.attention.flex_attention import BlockMask, flex_attention

# -----------------------------------------------------------------------------
# Muon optimizer

@torch.compile
def zeropower_via_newtonschulz5(G, steps):
    """
    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a
    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose
    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at
    zero even beyond the point where the iteration no longer converges all the way to one everywhere
    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T
    where S' is diagonal with S_{ii}' ~ Uniform(0.5, 1.5), which turns out not to hurt model
    performance at all relative to UV^T, where USV^T = G is the SVD.
    """
    assert len(G.shape) == 2
    a, b, c = (3.4445, -4.7750,  2.0315)
    X = G.bfloat16()
    if G.size(0) > G.size(1):
        X = X.T

    # Ensure spectral norm is at most 1
    X = X / (X.norm() + 1e-7)
    # Perform the NS iterations
    for _ in range(steps):
        A = X @ X.T
        B = b * A + c * A @ A # adapted from suggestion by @jxbz, @leloykun, and @YouJiacheng
        X = a * X + B @ X
    
    if G.size(0) > G.size(1):
        X = X.T
    return X

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Some warnings:
    - This optimizer assumes that all parameters passed in are 2D.
    - It should not be used for the embedding layer, the final fully connected layer, or any {0,1}-D
    parameters; those should all be optimized by a standard method (e.g., AdamW).
    - To use it with 4D convolutional filters, it works well to just flatten their last 3 dimensions.
    - We believe it is unlikely to work well for training with small batch size.
    - We believe it may not work well for finetuning pretrained models, but we haven't tested this.
    - We have not yet tried this optimizer for training scenarios larger than NanoGPT (124M).

    Arguments:
        lr: The learning rate used by the internal SGD.
        momentum: The momentum used by the internal SGD.
        nesterov: Whether to use Nesterov-style momentum in the internal SGD. (recommended)
        ns_steps: The number of Newton-Schulz iteration steps to use.
    """
    def __init__(self, params, lr=0.02, momentum=0.95, nesterov=True, ns_steps=5):
        self.world_size = int(os.environ['WORLD_SIZE'])
        self.rank = int(os.environ['RANK'])
        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, ns_steps=ns_steps)
        assert all(isinstance(p, torch.Tensor) for p in params)
        sizes = {p.numel() for p in params}
        param_groups = [dict(params=[p for p in params if p.numel() == size],
                             update_buffer=[torch.empty(size, device='cuda', dtype=torch.bfloat16) for _ in range(self.world_size)])
                        for size in sizes]
        super().__init__(param_groups, defaults)

    def step(self):

        for group in self.param_groups:

            lr = group['lr']
            momentum = group['momentum']
            nesterov = group['nesterov']
            ns_steps = group['ns_steps']
            update_buffers = group['update_buffer']
            # generate weight updates in distributed fashion
            params = group['params']
            handle = None
            params_world = None
            def update_prev():
                if params_world is None:
                    return
                assert handle is not None
                handle.wait()
                for p_world, g_world in zip(params_world, update_buffers):
                    p_world.data.add_(
                        g_world.view_as(p_world),
                        alpha=-lr * max(1, p_world.size(0) / p_world.size(1)) ** 0.5,
                    )
            for base_i in range(len(params))[::self.world_size]:
                if base_i + rank < len(params):
                    p = params[base_i + self.rank]
                    g = p.grad
                    assert g is not None
                    state = self.state[p]
                    if 'momentum_buffer' not in state:
                        state['momentum_buffer'] = torch.zeros_like(g)
                    buf = state['momentum_buffer']
                    buf.lerp_(g, 1 - momentum)
                    g = g.lerp_(buf, momentum) if nesterov else buf
                    g = zeropower_via_newtonschulz5(g, steps=ns_steps).flatten()
                else:
                    g = update_buffers[rank]
                update_prev() # async all_gather instead of sync all_reduce by @YouJiacheng
                handle = dist.all_gather(update_buffers, g, async_op=True)
                params_world = params[base_i : base_i + self.world_size]
            update_prev()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the GPT-2 model

def norm(x):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):

    def __init__(self, in_features, out_features):
        super().__init__(in_features, out_features, bias=False)

    def forward(self, x):
        return F.linear(x, self.weight.type_as(x))

class Rotary(nn.Module):

    def __init__(self, dim, max_seq_len=65536):
        super().__init__()
        # half-truncate RoPE by @YouJiacheng
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=dim//4, dtype=torch.float32)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(dim//4)])
        t = torch.arange(max_seq_len, dtype=torch.float32)
        theta = torch.einsum('i,j -> ij', t, angular_freq)
        self.cos = nn.Buffer(theta.cos(), persistent=False)
        self.sin = nn.Buffer(theta.sin(), persistent=False)

    def forward(self, x):
        cos, sin = self.cos[None, :x.size(-3), None, :], self.sin[None, :x.size(-3), None, :]
        x1, x2 = x.float().chunk(2, dim=-1)
        y1 = x1 * cos + x2 * sin
        y2 = x1 * (-sin) + x2 * cos
        return torch.cat((y1, y2), 3).type_as(x)

class CausalSelfAttention(nn.Module):

    def __init__(self, dim, num_heads, layer_id):
        super().__init__()
        assert dim % num_heads == 0
        self.num_heads = num_heads
        self.layer_id = layer_id
        self.c_q = CastedLinear(dim, dim)
        self.c_k = CastedLinear(dim, dim)
        self.c_v = CastedLinear(dim, dim)
        self.lambdas = nn.Parameter(torch.tensor([0.5, 0.5]))
        self.rotary = Rotary(dim // num_heads) # dim // num_heads = head_dim
        self.c_proj = CastedLinear(dim, dim)
        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977
        # self.attn_scale = nn.Parameter(torch.tensor(1.0 / (dim // num_heads) ** 0.5))
        # self.attn_scale = nn.Parameter(torch.tensor(0.20))
        self.attn_scale = 0.13 + 0.01 * min(layer_id, 11 - layer_id)  # unet pattern attention scale by @leloykun

    def forward(self, x, ve, block_mask):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, 'Must use batch size = 1 for FlexAttention'
        q = self.c_q(x).view(B, T, self.num_heads, -1)
        k = self.c_k(x).view(B, T, self.num_heads, -1)
        v = self.c_v(x).view(B, T, self.num_heads, -1)
        if ve is not None:
            v = self.lambdas[0] * v + self.lambdas[1] * ve.view_as(v) # @KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = self.lambdas[0] * v
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = self.rotary(q), self.rotary(k)
        # y = flex_attention(q.transpose(1, 2) * self.attn_scale, k.transpose(1, 2), v.transpose(1, 2), block_mask=block_mask, scale=1.)
        y = flex_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), block_mask=block_mask, scale=self.attn_scale)
        y = y.transpose(1, 2).contiguous().view_as(x) # re-assemble all head outputs side by side
        y = self.c_proj(y)
        return y

class MLP(nn.Module):

    def __init__(self, dim):
        super().__init__()
        self.c_fc = CastedLinear(dim, 4 * dim)
        self.c_proj = CastedLinear(4 * dim, dim)
        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977

    def forward(self, x):
        x = self.c_fc(x)
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = self.c_proj(x)
        return x

class Block(nn.Module):

    def __init__(self, model_dim, num_heads, layer_id, use_attn=True):
        super().__init__()
        self.attn = CausalSelfAttention(model_dim, num_heads, layer_id) if use_attn else None
        self.mlp = MLP(model_dim)
        self.lambdas = nn.Parameter(torch.tensor([1., 0.]))

    def forward(self, x, ve, x0, block_mask):
        x = self.lambdas[0] * x + self.lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), ve, block_mask)
        x = x + self.mlp(norm(x))
        return x

class ValueEmbedding(nn.Module):
    def __init__(self, vocab_size, model_dim):
        super().__init__()
        self.embed = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])

    def forward(self, inputs):
        ve = [emb(inputs).bfloat16() for emb in self.embed]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        ve = [ve[0], ve[1], ve[2], None, None, None, None, None, None, ve[0], ve[1], ve[2]]
        return ve

# -----------------------------------------------------------------------------
# The main GPT-2 model

class GPT(nn.Module):

    def __init__(self, vocab_size, num_layers, num_heads, model_dim):
        super().__init__()
        self.embed = nn.Embedding(vocab_size, model_dim)
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.blocks = nn.ModuleList([Block(model_dim, num_heads, layer_id=layer_id, use_attn=(layer_id != 7))
                                     for layer_id in range(num_layers)])
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual learning
        # U-net structure on token value embeddings by @leloykun
        self.value_embeds = ValueEmbedding(vocab_size, model_dim)
        self.lm_head = CastedLinear(model_dim, vocab_size)
        self.lm_head.weight.data.zero_() # @Grad62304977
        # U-net design by @brendanh0gan
        self.num_encoder_layers = num_layers // 2 # Half of the layers for encoder
        self.num_decoder_layers = num_layers - self.num_encoder_layers # Remaining for decoder
        # Add learnable skip connection weights for decoder layers
        self.skip_weights = nn.Parameter(torch.ones(self.num_decoder_layers))

    def forward(self, inputs, targets, sliding_window_num_blocks):
        BLOCK_SIZE = 128
        seq_len = len(inputs)
        assert seq_len % BLOCK_SIZE == 0
        total_num_blocks = seq_len // BLOCK_SIZE
        assert inputs.ndim == 1
        docs = (inputs == 50256).cumsum(0)
        docs_low = docs.view(-1, BLOCK_SIZE)[:, 0].contiguous()
        docs_high = docs.view(-1, BLOCK_SIZE)[:, -1].contiguous()

        def document_causal(b, h, q_idx, kv_idx):
            causal_mask = q_idx >= kv_idx
            document_mask = docs[q_idx] == docs[kv_idx]
            return causal_mask & document_mask

        def dense_to_ordered(dense_mask):
            num_blocks = dense_mask.sum(dim=-1, dtype=torch.int32)
            indices = dense_mask.argsort(dim=-1, descending=True, stable=True).to(torch.int32)
            return num_blocks[None, None].contiguous(), indices[None, None].contiguous()

        def create_doc_swc_block_mask(sliding_window_num_blocks):
            kv_idx = block_idx = torch.arange(total_num_blocks, dtype=torch.int32, device='cuda')
            q_idx = block_idx[:, None]
            causal_bm = q_idx >= kv_idx
            causal_full_bm = q_idx > kv_idx
            window_bm = q_idx - kv_idx < sliding_window_num_blocks
            window_full_bm = window_bm # block-wise sliding window by @YouJiacheng
            # document_bm = (docs_low[q_idx] <= docs_high[kv_idx]) & (docs_low[kv_idx] <= docs_high[q_idx])
            document_bm = (docs_low[:, None] <= docs_high) & (docs_low <= docs_high[:, None])
            document_full_bm = (docs_low[:, None] == docs_high) & (docs_low == docs_high[:, None])
            nonzero_bm = causal_bm & window_bm & document_bm
            full_bm  = causal_full_bm & window_full_bm & document_full_bm
            kv_num_blocks, kv_indices = dense_to_ordered(nonzero_bm & ~full_bm)
            full_kv_num_blocks, full_kv_indices = dense_to_ordered(full_bm)
            return BlockMask.from_kv_blocks(
                kv_num_blocks,
                kv_indices,
                full_kv_num_blocks,
                full_kv_indices,
                BLOCK_SIZE=BLOCK_SIZE,
                mask_mod=document_causal,
            )

        block_mask = create_doc_swc_block_mask(sliding_window_num_blocks)

        x0 = norm(self.embed(inputs[None]).bfloat16()) # use of norm here by @Grad62304977
        x = x0
        ve = self.value_embeds(inputs)
        assert len(ve) == len(self.blocks)
        ve_enc, ve_dec = ve[:self.num_encoder_layers], ve[self.num_encoder_layers:]

        # Store outputs for U-Net skip connections
        skip_connections = []
        # Encoder pass - process only the first half of the blocks
        for i in range(self.num_encoder_layers):
            x = self.blocks[i](x, ve_enc[i], x0, block_mask)
            skip_connections.append(x)
        # Decoder pass - process the remaining blocks with weighted skip connections
        for i in range(self.num_decoder_layers):
            x = x + self.skip_weights[i] * skip_connections.pop()
            # U-net structure on token value embeddings by @leloykun
            x = self.blocks[self.num_encoder_layers + i](x, ve_dec[i], x0, block_mask)

        x = norm(x)
        logits = self.lm_head(x)
        logits = 15 * torch.tanh(logits / 15) # @Grad62304977 added tanh softcapping, @KoszarskyB reduced it from 30 to 15
        logits = logits.float()
        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets)
        return loss

# -----------------------------------------------------------------------------
# Our own simple Distributed Data Loader

def _load_data_shard(path):
    # only reads the header, returns header data
    # header is 256 int32
    header = torch.from_file(path, False, 256, dtype=torch.int32)
    assert header[0] == 20240520, 'magic number mismatch in the data .bin file'
    assert header[1] == 1, 'unsupported version'
    num_tokens = int(header[2]) # number of tokens (claimed)
    with open(path, 'rb', buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, 'number of tokens read does not match header'
    return tokens

class DistributedDataLoader:

    def __init__(self, filename_pattern):
        self.rank = int(os.environ['RANK'])
        self.world_size = int(os.environ['WORLD_SIZE'])
        self.files = sorted(glob.glob(filename_pattern))
        self.reset()

    def reset(self):
        self.current_shard = -1
        self.advance()

    def advance(self):
        self.current_shard = (self.current_shard + 1) % len(self.files)
        self.current_position = 0
        self.tokens = _load_data_shard(self.files[self.current_shard])

    def next_batch(self, batch_size):
        assert batch_size % self.world_size == 0
        device_batch_size = batch_size // self.world_size
        # load next shard if necessary
        if self.current_position + batch_size + 1 >= len(self.tokens):
            self.advance()
        pos = self.current_position + self.rank * device_batch_size
        device_batch_tokens = self.tokens[pos:pos+device_batch_size+1]
        # advance current position
        self.current_position += batch_size
        inputs = device_batch_tokens[:-1].to(device='cuda', dtype=torch.int32, non_blocking=True)
        targets = device_batch_tokens[1:].to(device='cuda', dtype=torch.int64, non_blocking=True)
        return inputs, targets

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files = 'data/fineweb10B/fineweb_train_*.bin' # input .bin to train on
    val_files = 'data/fineweb10B/fineweb_val_*.bin' # input .bin to eval validation loss on
    val_tokens = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # optimization
    batch_size = 8*64*1024 # batch size in tokens
    num_iterations = 1375 # number of iterations to run
    cooldown_frac = 0.4 # fraction of training spent cooling down the learning rate
    bf16_embeds = True
    # evaluation and logging
    val_loss_every = 125 # every how many steps to evaluate val loss? 0 for only at the end
    # implementation
    max_device_batch_size = 64*1024 # batch size per device in tokens
    save_checkpoint = False
args = Hyperparameters()

micro_bs = args.max_device_batch_size

# set up DDP (distributed data parallel). torchrun sets this env variable
rank = int(os.environ['RANK'])
local_rank = int(os.environ['LOCAL_RANK'])
world_size = int(os.environ['WORLD_SIZE'])
assert torch.cuda.is_available()
torch.cuda.set_device(local_rank)
dist.init_process_group(backend='nccl', device_id=torch.device(local_rank))
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = uuid.uuid4()
    os.makedirs('logs', exist_ok=True)
    logfile = f'logs/{run_id}.txt'
    print(logfile)

def print0(s, console=False):
    if master_process:
        with open(logfile, 'a') as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0('='*100)
# log information about the hardware/software environment this is running on
print0(f'Running Python {sys.version}')
print0(f'Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}')
print0(subprocess.run(['nvidia-smi'], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout)
print0('='*100)

# load data
train_loader = DistributedDataLoader(args.train_files)
val_loader = DistributedDataLoader(args.val_files)
print0(f'Training dataloader files: {train_loader.files}')
print0(f'Validation dataloader files: {val_loader.files}')
print0('='*100)

# there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency. suggested to me by @Grad62304977.
# this originates from Karpathy's experiments.
model = GPT(vocab_size=50304, num_layers=12, num_heads=6, model_dim=768)
model = model.cuda()
if args.bf16_embeds:
    for m in model.modules():
        if isinstance(m, nn.Embedding):
            m.bfloat16()
model: GPT = torch.compile(model)
ddp_model = DDP(model, device_ids=[local_rank], broadcast_buffers=False, gradient_as_bucket_view=True)

# collect the parameters to optimize
hidden_matrix_params = [p for p in model.blocks.parameters() if p.ndim == 2]
embed_params = [model.embed.weight, *model.value_embeds.parameters()]
scalar_params = [p for n, p in model.named_parameters() if p.ndim < 2 and "attn_scale" not in n]
attn_scale_params = [p for n, p in model.named_parameters() if p.ndim < 2 and "attn_scale" in n]
head_params = [model.lm_head.weight]

# init the optimizer(s)
optimizer1 = torch.optim.Adam([dict(params=embed_params, lr=0.6),
                               dict(params=head_params, lr=0.008),
                               dict(params=scalar_params, lr=0.04),
                               dict(params=attn_scale_params, lr=0.01)],
                              betas=(0.8, 0.95), fused=True)
optimizer2 = Muon(hidden_matrix_params, lr=0.05, momentum=0.95)
optimizers = [optimizer1, optimizer2]

# learning rate schedule: stable then decay
def get_lr(it):
    t = 1 - it / args.num_iterations # time remaining in training
    assert 1 >= t > 0
    # 1) constant lr for first part of training
    if t >= args.cooldown_frac:
        return 1.0
    # 2) then linear cooldown
    else:
        return t / args.cooldown_frac
schedulers = [torch.optim.lr_scheduler.LambdaLR(opt, get_lr) for opt in optimizers]

# sliding window size schedule: linear increase over training in chunks of 128 from 128 -> 1792. By @fernbear.bsky.social
def get_sliding_window_blocks(it):
    x = it / args.num_iterations # training progress
    assert 0 <= x <= 1
    return int(((1 - x) * 128 + x * 1856) // 128)
sliding_window_num_blocks = torch.tensor(1, dtype=torch.int32, device='cuda')

# Start training loop
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    # This effectively ignores timing first 10 steps, which are slower for weird reasons.
    # Alternately, and slightly more correctly in terms of benchmarking, we could do 10
    # steps with dummy data first, and then re-initialize the model and reset the loader.
    if step == 10:
        training_time_ms = 0
        t0 = time.perf_counter()
    timed_steps = float('nan') if step <= 11 else (step - 10) + 1 # <= 11 to avoid bug in val

    sliding_window_num_blocks.copy_(get_sliding_window_blocks(step))

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        # run validation batches
        model.eval()
        val_loader.reset()
        val_loss = 0.0
        # calculate the number of steps to take in the val loop.
        val_batch_size = world_size * micro_bs
        assert args.val_tokens % val_batch_size == 0
        val_steps = args.val_tokens // val_batch_size
        for _ in range(val_steps):
            with torch.no_grad():
                inputs_val, targets_val = val_loader.next_batch(val_batch_size)
                val_loss += ddp_model(inputs_val, targets_val, sliding_window_num_blocks)
        # Print attention scales
        for n, p in model.named_parameters():
            if p.ndim < 2 and "attn_scale" in n:
                print0(f'{n}: {p.item()}', console=True)
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        val_loss /= val_steps
        # logging
        print0(f'step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/(timed_steps-1):.2f}ms', console=True)
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f'logs/{run_id}', exist_ok=True)
            torch.save(log, f'logs/{run_id}/state_step{step:06d}.pt')
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    model.train()
    batch_size = args.batch_size
    assert batch_size % world_size == 0
    inputs_train, targets_train = train_loader.next_batch(batch_size)
    assert len(inputs_train) <= micro_bs or len(inputs_train) % micro_bs == 0
    for micro_inputs_train, micro_targets_train in zip(inputs_train.split(micro_bs), targets_train.split(micro_bs)):
        ddp_model(micro_inputs_train, micro_targets_train, sliding_window_num_blocks).backward()
    # momentum warmup for Muon
    frac = min(step/300, 1)
    for group in optimizer2.param_groups:
        group['momentum'] = (1 - frac) * 0.85 + frac * 0.95
    # step the optimizers and schedulers
    for opt, sched in zip(optimizers, schedulers):
        opt.step()
        if step != train_steps-1:
            sched.step()
    # null the gradients
    model.zero_grad(set_to_none=True)
    # logging
    approx_time = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f'step:{step+1}/{train_steps} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms', console=True)

print0(f'peak memory consumption: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB')
dist.destroy_process_group()

====================================================================================================
Running Python 3.10.12 (main, Sep 11 2024, 15:47:36) [GCC 11.4.0]
Running PyTorch 2.6.0.dev20241231+cu126 compiled for CUDA 12.6
Tue Jan 14 16:25:04 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.183.06             Driver Version: 535.183.06   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:19:00.0 Off |                    0 |
| N/A   40C    P0             127W / 700W |   7713MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:3B:00.0 Off |                    0 |
| N/A   33C    P0             114W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:4C:00.0 Off |                    0 |
| N/A   30C    P0             118W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   39C    P0             131W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:9B:00.0 Off |                    0 |
| N/A   39C    P0             124W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:BB:00.0 Off |                    0 |
| N/A   31C    P0             115W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:CB:00.0 Off |                    0 |
| N/A   38C    P0             117W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:DB:00.0 Off |                    0 |
| N/A   30C    P0             117W / 700W |   3211MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
Training dataloader files: ['data/fineweb10B/fineweb_train_000001.bin', 'data/fineweb10B/fineweb_train_000002.bin', 'data/fineweb10B/fineweb_train_000003.bin', 'data/fineweb10B/fineweb_train_000004.bin', 'data/fineweb10B/fineweb_train_000005.bin', 'data/fineweb10B/fineweb_train_000006.bin', 'data/fineweb10B/fineweb_train_000007.bin', 'data/fineweb10B/fineweb_train_000008.bin', 'data/fineweb10B/fineweb_train_000009.bin']
Validation dataloader files: ['data/fineweb10B/fineweb_val_000000.bin']
====================================================================================================
step:0/1375 val_loss:10.8258 train_time:0ms step_avg:nanms
step:1/1375 train_time:27488ms step_avg:nanms
step:2/1375 train_time:27567ms step_avg:nanms
step:3/1375 train_time:27757ms step_avg:nanms
step:4/1375 train_time:27889ms step_avg:nanms
step:5/1375 train_time:28023ms step_avg:nanms
step:6/1375 train_time:28156ms step_avg:nanms
step:7/1375 train_time:28289ms step_avg:nanms
step:8/1375 train_time:28422ms step_avg:nanms
step:9/1375 train_time:28555ms step_avg:nanms
step:10/1375 train_time:28694ms step_avg:nanms
step:11/1375 train_time:136ms step_avg:nanms
step:12/1375 train_time:272ms step_avg:nanms
step:13/1375 train_time:407ms step_avg:135.51ms
step:14/1375 train_time:543ms step_avg:135.76ms
step:15/1375 train_time:676ms step_avg:135.11ms
step:16/1375 train_time:810ms step_avg:134.92ms
step:17/1375 train_time:948ms step_avg:135.40ms
step:18/1375 train_time:1085ms step_avg:135.57ms
step:19/1375 train_time:1220ms step_avg:135.58ms
step:20/1375 train_time:1355ms step_avg:135.52ms
step:21/1375 train_time:1491ms step_avg:135.53ms
step:22/1375 train_time:1627ms step_avg:135.56ms
step:23/1375 train_time:1762ms step_avg:135.50ms
step:24/1375 train_time:1899ms step_avg:135.62ms
step:25/1375 train_time:2035ms step_avg:135.67ms
step:26/1375 train_time:2172ms step_avg:135.72ms
step:27/1375 train_time:2305ms step_avg:135.59ms
step:28/1375 train_time:2441ms step_avg:135.64ms
step:29/1375 train_time:2576ms step_avg:135.56ms
step:30/1375 train_time:2710ms step_avg:135.52ms
step:31/1375 train_time:2848ms step_avg:135.61ms
step:32/1375 train_time:2983ms step_avg:135.58ms
step:33/1375 train_time:3119ms step_avg:135.60ms
step:34/1375 train_time:3255ms step_avg:135.62ms
step:35/1375 train_time:3390ms step_avg:135.59ms
step:36/1375 train_time:3526ms step_avg:135.60ms
step:37/1375 train_time:3662ms step_avg:135.65ms
step:38/1375 train_time:3799ms step_avg:135.69ms
step:39/1375 train_time:3934ms step_avg:135.66ms
step:40/1375 train_time:4070ms step_avg:135.66ms
step:41/1375 train_time:4205ms step_avg:135.66ms
step:42/1375 train_time:4341ms step_avg:135.64ms
step:43/1375 train_time:4475ms step_avg:135.61ms
step:44/1375 train_time:4610ms step_avg:135.59ms
step:45/1375 train_time:4747ms step_avg:135.62ms
step:46/1375 train_time:4883ms step_avg:135.63ms
step:47/1375 train_time:5018ms step_avg:135.62ms
step:48/1375 train_time:5155ms step_avg:135.65ms
step:49/1375 train_time:5291ms step_avg:135.66ms
step:50/1375 train_time:5426ms step_avg:135.65ms
step:51/1375 train_time:5560ms step_avg:135.61ms
step:52/1375 train_time:5695ms step_avg:135.60ms
step:53/1375 train_time:5831ms step_avg:135.60ms
step:54/1375 train_time:5966ms step_avg:135.60ms
step:55/1375 train_time:6100ms step_avg:135.56ms
step:56/1375 train_time:6237ms step_avg:135.58ms
step:57/1375 train_time:6372ms step_avg:135.58ms
step:58/1375 train_time:6507ms step_avg:135.57ms
step:59/1375 train_time:6644ms step_avg:135.59ms
step:60/1375 train_time:6779ms step_avg:135.57ms
step:61/1375 train_time:6913ms step_avg:135.56ms
step:62/1375 train_time:7050ms step_avg:135.58ms
step:63/1375 train_time:7185ms step_avg:135.57ms
step:64/1375 train_time:7319ms step_avg:135.54ms
step:65/1375 train_time:7455ms step_avg:135.54ms
step:66/1375 train_time:7590ms step_avg:135.54ms
step:67/1375 train_time:7725ms step_avg:135.53ms
step:68/1375 train_time:7862ms step_avg:135.54ms
step:69/1375 train_time:7997ms step_avg:135.54ms
step:70/1375 train_time:8132ms step_avg:135.53ms
step:71/1375 train_time:8268ms step_avg:135.54ms
step:72/1375 train_time:8404ms step_avg:135.55ms
step:73/1375 train_time:8540ms step_avg:135.56ms
step:74/1375 train_time:8674ms step_avg:135.54ms
step:75/1375 train_time:8809ms step_avg:135.52ms
step:76/1375 train_time:8947ms step_avg:135.55ms
step:77/1375 train_time:9081ms step_avg:135.53ms
step:78/1375 train_time:9215ms step_avg:135.52ms
step:79/1375 train_time:9351ms step_avg:135.52ms
step:80/1375 train_time:9486ms step_avg:135.51ms
step:81/1375 train_time:9621ms step_avg:135.51ms
step:82/1375 train_time:9756ms step_avg:135.50ms
step:83/1375 train_time:9893ms step_avg:135.52ms
step:84/1375 train_time:10029ms step_avg:135.53ms
step:85/1375 train_time:10165ms step_avg:135.53ms
step:86/1375 train_time:10299ms step_avg:135.51ms
step:87/1375 train_time:10435ms step_avg:135.52ms
step:88/1375 train_time:10570ms step_avg:135.52ms
step:89/1375 train_time:10705ms step_avg:135.50ms
step:90/1375 train_time:10839ms step_avg:135.49ms
step:91/1375 train_time:10974ms step_avg:135.49ms
step:92/1375 train_time:11110ms step_avg:135.48ms
step:93/1375 train_time:11247ms step_avg:135.50ms
step:94/1375 train_time:11382ms step_avg:135.50ms
step:95/1375 train_time:11518ms step_avg:135.50ms
step:96/1375 train_time:11654ms step_avg:135.52ms
step:97/1375 train_time:11789ms step_avg:135.50ms
step:98/1375 train_time:11924ms step_avg:135.50ms
step:99/1375 train_time:12060ms step_avg:135.51ms
step:100/1375 train_time:12196ms step_avg:135.51ms
step:101/1375 train_time:12332ms step_avg:135.52ms
step:102/1375 train_time:12469ms step_avg:135.53ms
step:103/1375 train_time:12607ms step_avg:135.56ms
step:104/1375 train_time:12748ms step_avg:135.61ms
step:105/1375 train_time:12885ms step_avg:135.64ms
step:106/1375 train_time:13025ms step_avg:135.67ms
step:107/1375 train_time:13163ms step_avg:135.70ms
step:108/1375 train_time:13300ms step_avg:135.71ms
step:109/1375 train_time:13438ms step_avg:135.74ms
step:110/1375 train_time:13577ms step_avg:135.77ms
step:111/1375 train_time:13717ms step_avg:135.81ms
step:112/1375 train_time:13857ms step_avg:135.85ms
step:113/1375 train_time:13996ms step_avg:135.89ms
step:114/1375 train_time:14136ms step_avg:135.92ms
step:115/1375 train_time:14275ms step_avg:135.95ms
step:116/1375 train_time:14412ms step_avg:135.96ms
step:117/1375 train_time:14553ms step_avg:136.01ms
step:118/1375 train_time:14692ms step_avg:136.04ms
step:119/1375 train_time:14832ms step_avg:136.07ms
step:120/1375 train_time:14970ms step_avg:136.09ms
step:121/1375 train_time:15108ms step_avg:136.11ms
step:122/1375 train_time:15248ms step_avg:136.14ms
step:123/1375 train_time:15385ms step_avg:136.15ms
step:124/1375 train_time:15524ms step_avg:136.17ms
step:125/1375 train_time:15662ms step_avg:136.19ms
step:125/1375 val_loss:4.3779 train_time:15728ms step_avg:136.77ms
step:126/1375 train_time:15804ms step_avg:136.24ms
step:127/1375 train_time:15944ms step_avg:136.27ms
step:128/1375 train_time:16082ms step_avg:136.29ms
step:129/1375 train_time:16221ms step_avg:136.31ms
step:130/1375 train_time:16358ms step_avg:136.32ms
step:131/1375 train_time:16497ms step_avg:136.34ms
step:132/1375 train_time:16636ms step_avg:136.36ms
step:133/1375 train_time:16778ms step_avg:136.41ms
step:134/1375 train_time:16918ms step_avg:136.44ms
step:135/1375 train_time:17058ms step_avg:136.47ms
step:136/1375 train_time:17198ms step_avg:136.49ms
step:137/1375 train_time:17337ms step_avg:136.51ms
step:138/1375 train_time:17477ms step_avg:136.54ms
step:139/1375 train_time:17615ms step_avg:136.55ms
step:140/1375 train_time:17756ms step_avg:136.58ms
step:141/1375 train_time:17897ms step_avg:136.62ms
step:142/1375 train_time:18037ms step_avg:136.64ms
step:143/1375 train_time:18179ms step_avg:136.68ms
step:144/1375 train_time:18318ms step_avg:136.70ms
step:145/1375 train_time:18457ms step_avg:136.72ms
step:146/1375 train_time:18597ms step_avg:136.74ms
step:147/1375 train_time:18737ms step_avg:136.76ms
step:148/1375 train_time:18878ms step_avg:136.79ms
step:149/1375 train_time:19017ms step_avg:136.81ms
step:150/1375 train_time:19157ms step_avg:136.84ms
step:151/1375 train_time:19298ms step_avg:136.87ms
step:152/1375 train_time:19438ms step_avg:136.89ms
step:153/1375 train_time:19578ms step_avg:136.91ms
step:154/1375 train_time:19717ms step_avg:136.92ms
step:155/1375 train_time:19857ms step_avg:136.94ms
step:156/1375 train_time:20000ms step_avg:136.98ms
step:157/1375 train_time:20140ms step_avg:137.01ms
step:158/1375 train_time:20280ms step_avg:137.03ms
step:159/1375 train_time:20419ms step_avg:137.04ms
step:160/1375 train_time:20558ms step_avg:137.05ms
step:161/1375 train_time:20698ms step_avg:137.07ms
step:162/1375 train_time:20837ms step_avg:137.09ms
step:163/1375 train_time:20978ms step_avg:137.11ms
step:164/1375 train_time:21118ms step_avg:137.13ms
step:165/1375 train_time:21257ms step_avg:137.14ms
step:166/1375 train_time:21398ms step_avg:137.17ms
step:167/1375 train_time:21538ms step_avg:137.18ms
step:168/1375 train_time:21677ms step_avg:137.20ms
step:169/1375 train_time:21816ms step_avg:137.21ms
step:170/1375 train_time:21955ms step_avg:137.22ms
step:171/1375 train_time:22096ms step_avg:137.24ms
step:172/1375 train_time:22237ms step_avg:137.27ms
step:173/1375 train_time:22378ms step_avg:137.29ms
step:174/1375 train_time:22518ms step_avg:137.30ms
step:175/1375 train_time:22657ms step_avg:137.32ms
step:176/1375 train_time:22799ms step_avg:137.34ms
step:177/1375 train_time:22939ms step_avg:137.36ms
step:178/1375 train_time:23079ms step_avg:137.37ms
step:179/1375 train_time:23219ms step_avg:137.39ms
step:180/1375 train_time:23358ms step_avg:137.40ms
step:181/1375 train_time:23499ms step_avg:137.42ms
step:182/1375 train_time:23639ms step_avg:137.44ms
step:183/1375 train_time:23779ms step_avg:137.45ms
step:184/1375 train_time:23919ms step_avg:137.46ms
step:185/1375 train_time:24058ms step_avg:137.47ms
step:186/1375 train_time:24199ms step_avg:137.50ms
step:187/1375 train_time:24338ms step_avg:137.50ms
step:188/1375 train_time:24479ms step_avg:137.52ms
step:189/1375 train_time:24620ms step_avg:137.54ms
step:190/1375 train_time:24758ms step_avg:137.55ms
step:191/1375 train_time:24941ms step_avg:137.80ms
step:192/1375 train_time:25079ms step_avg:137.79ms
step:193/1375 train_time:25217ms step_avg:137.80ms
step:194/1375 train_time:25354ms step_avg:137.79ms
step:195/1375 train_time:25493ms step_avg:137.80ms
step:196/1375 train_time:25631ms step_avg:137.80ms
step:197/1375 train_time:25771ms step_avg:137.82ms
step:198/1375 train_time:25914ms step_avg:137.84ms
step:199/1375 train_time:26053ms step_avg:137.85ms
step:200/1375 train_time:26192ms step_avg:137.85ms
step:201/1375 train_time:26330ms step_avg:137.85ms
step:202/1375 train_time:26469ms step_avg:137.86ms
step:203/1375 train_time:26607ms step_avg:137.86ms
step:204/1375 train_time:26747ms step_avg:137.87ms
step:205/1375 train_time:26890ms step_avg:137.90ms
step:206/1375 train_time:27034ms step_avg:137.93ms
step:207/1375 train_time:27176ms step_avg:137.95ms
step:208/1375 train_time:27317ms step_avg:137.96ms
step:209/1375 train_time:27457ms step_avg:137.97ms
step:210/1375 train_time:27600ms step_avg:138.00ms
step:211/1375 train_time:27742ms step_avg:138.02ms
step:212/1375 train_time:27885ms step_avg:138.04ms
step:213/1375 train_time:28028ms step_avg:138.07ms
step:214/1375 train_time:28171ms step_avg:138.09ms
step:215/1375 train_time:28314ms step_avg:138.12ms
step:216/1375 train_time:28454ms step_avg:138.13ms
step:217/1375 train_time:28596ms step_avg:138.14ms
step:218/1375 train_time:28737ms step_avg:138.16ms
step:219/1375 train_time:28881ms step_avg:138.19ms
step:220/1375 train_time:29023ms step_avg:138.20ms
step:221/1375 train_time:29166ms step_avg:138.23ms
step:222/1375 train_time:29307ms step_avg:138.24ms
step:223/1375 train_time:29450ms step_avg:138.26ms
step:224/1375 train_time:29593ms step_avg:138.29ms
step:225/1375 train_time:29735ms step_avg:138.30ms
step:226/1375 train_time:29877ms step_avg:138.32ms
step:227/1375 train_time:30019ms step_avg:138.34ms
step:228/1375 train_time:30160ms step_avg:138.35ms
step:229/1375 train_time:30304ms step_avg:138.37ms
step:230/1375 train_time:30444ms step_avg:138.38ms
step:231/1375 train_time:30588ms step_avg:138.41ms
step:232/1375 train_time:30728ms step_avg:138.41ms
step:233/1375 train_time:30872ms step_avg:138.44ms
step:234/1375 train_time:31014ms step_avg:138.46ms
step:235/1375 train_time:31156ms step_avg:138.47ms
step:236/1375 train_time:31298ms step_avg:138.49ms
step:237/1375 train_time:31441ms step_avg:138.51ms
step:238/1375 train_time:31583ms step_avg:138.52ms
step:239/1375 train_time:31723ms step_avg:138.53ms
step:240/1375 train_time:31866ms step_avg:138.55ms
step:241/1375 train_time:32010ms step_avg:138.57ms
step:242/1375 train_time:32151ms step_avg:138.58ms
step:243/1375 train_time:32294ms step_avg:138.60ms
step:244/1375 train_time:32437ms step_avg:138.62ms
step:245/1375 train_time:32579ms step_avg:138.63ms
step:246/1375 train_time:32720ms step_avg:138.64ms
step:247/1375 train_time:32862ms step_avg:138.66ms
step:248/1375 train_time:33004ms step_avg:138.67ms
step:249/1375 train_time:33147ms step_avg:138.69ms
step:250/1375 train_time:33290ms step_avg:138.71ms
step:250/1375 val_loss:3.9611 train_time:33360ms step_avg:139.00ms
step:251/1375 train_time:33435ms step_avg:138.73ms
step:252/1375 train_time:33577ms step_avg:138.75ms
step:253/1375 train_time:33719ms step_avg:138.76ms
step:254/1375 train_time:33859ms step_avg:138.77ms
step:255/1375 train_time:34001ms step_avg:138.78ms
step:256/1375 train_time:34141ms step_avg:138.78ms
step:257/1375 train_time:34283ms step_avg:138.80ms
step:258/1375 train_time:34430ms step_avg:138.83ms
step:259/1375 train_time:34571ms step_avg:138.84ms
step:260/1375 train_time:34714ms step_avg:138.86ms
step:261/1375 train_time:34857ms step_avg:138.87ms
step:262/1375 train_time:34999ms step_avg:138.88ms
step:263/1375 train_time:35139ms step_avg:138.89ms
step:264/1375 train_time:35281ms step_avg:138.90ms
step:265/1375 train_time:35425ms step_avg:138.92ms
step:266/1375 train_time:35568ms step_avg:138.94ms
step:267/1375 train_time:35712ms step_avg:138.96ms
step:268/1375 train_time:35853ms step_avg:138.97ms
step:269/1375 train_time:35995ms step_avg:138.98ms
step:270/1375 train_time:36136ms step_avg:138.99ms
step:271/1375 train_time:36278ms step_avg:138.99ms
step:272/1375 train_time:36421ms step_avg:139.01ms
step:273/1375 train_time:36564ms step_avg:139.02ms
step:274/1375 train_time:36708ms step_avg:139.04ms
step:275/1375 train_time:36850ms step_avg:139.06ms
step:276/1375 train_time:36991ms step_avg:139.06ms
step:277/1375 train_time:37132ms step_avg:139.07ms
step:278/1375 train_time:37273ms step_avg:139.08ms
step:279/1375 train_time:37416ms step_avg:139.09ms
step:280/1375 train_time:37559ms step_avg:139.11ms
step:281/1375 train_time:37702ms step_avg:139.12ms
step:282/1375 train_time:37843ms step_avg:139.13ms
step:283/1375 train_time:37987ms step_avg:139.15ms
step:284/1375 train_time:38127ms step_avg:139.15ms
step:285/1375 train_time:38270ms step_avg:139.16ms
step:286/1375 train_time:38413ms step_avg:139.18ms
step:287/1375 train_time:38556ms step_avg:139.19ms
step:288/1375 train_time:38699ms step_avg:139.21ms
step:289/1375 train_time:38840ms step_avg:139.21ms
step:290/1375 train_time:38984ms step_avg:139.23ms
step:291/1375 train_time:39125ms step_avg:139.24ms
step:292/1375 train_time:39268ms step_avg:139.25ms
step:293/1375 train_time:39412ms step_avg:139.26ms
step:294/1375 train_time:39553ms step_avg:139.27ms
step:295/1375 train_time:39696ms step_avg:139.29ms
step:296/1375 train_time:39838ms step_avg:139.29ms
step:297/1375 train_time:39980ms step_avg:139.30ms
step:298/1375 train_time:40120ms step_avg:139.31ms
step:299/1375 train_time:40263ms step_avg:139.32ms
step:300/1375 train_time:40406ms step_avg:139.33ms
step:301/1375 train_time:40547ms step_avg:139.34ms
step:302/1375 train_time:40689ms step_avg:139.35ms
step:303/1375 train_time:40831ms step_avg:139.35ms
step:304/1375 train_time:40972ms step_avg:139.36ms
step:305/1375 train_time:41114ms step_avg:139.37ms
step:306/1375 train_time:41257ms step_avg:139.38ms
step:307/1375 train_time:41400ms step_avg:139.39ms
step:308/1375 train_time:41543ms step_avg:139.41ms
step:309/1375 train_time:41689ms step_avg:139.43ms
step:310/1375 train_time:41832ms step_avg:139.44ms
step:311/1375 train_time:41976ms step_avg:139.46ms
step:312/1375 train_time:42120ms step_avg:139.47ms
step:313/1375 train_time:42265ms step_avg:139.49ms
step:314/1375 train_time:42411ms step_avg:139.51ms
step:315/1375 train_time:42556ms step_avg:139.53ms
step:316/1375 train_time:42700ms step_avg:139.54ms
step:317/1375 train_time:42844ms step_avg:139.56ms
step:318/1375 train_time:42990ms step_avg:139.58ms
step:319/1375 train_time:43134ms step_avg:139.59ms
step:320/1375 train_time:43278ms step_avg:139.61ms
step:321/1375 train_time:43423ms step_avg:139.62ms
step:322/1375 train_time:43568ms step_avg:139.64ms
step:323/1375 train_time:43713ms step_avg:139.66ms
step:324/1375 train_time:43857ms step_avg:139.67ms
step:325/1375 train_time:44002ms step_avg:139.69ms
step:326/1375 train_time:44146ms step_avg:139.70ms
step:327/1375 train_time:44289ms step_avg:139.71ms
step:328/1375 train_time:44433ms step_avg:139.73ms
step:329/1375 train_time:44578ms step_avg:139.74ms
step:330/1375 train_time:44722ms step_avg:139.76ms
step:331/1375 train_time:44866ms step_avg:139.77ms
step:332/1375 train_time:45011ms step_avg:139.79ms
step:333/1375 train_time:45155ms step_avg:139.80ms
step:334/1375 train_time:45301ms step_avg:139.82ms
step:335/1375 train_time:45444ms step_avg:139.83ms
step:336/1375 train_time:45590ms step_avg:139.85ms
step:337/1375 train_time:45733ms step_avg:139.86ms
step:338/1375 train_time:45877ms step_avg:139.87ms
step:339/1375 train_time:46023ms step_avg:139.89ms
step:340/1375 train_time:46168ms step_avg:139.90ms
step:341/1375 train_time:46313ms step_avg:139.92ms
step:342/1375 train_time:46457ms step_avg:139.93ms
step:343/1375 train_time:46601ms step_avg:139.94ms
step:344/1375 train_time:46747ms step_avg:139.96ms
step:345/1375 train_time:46892ms step_avg:139.98ms
step:346/1375 train_time:47035ms step_avg:139.99ms
step:347/1375 train_time:47181ms step_avg:140.00ms
step:348/1375 train_time:47326ms step_avg:140.02ms
step:349/1375 train_time:47469ms step_avg:140.03ms
step:350/1375 train_time:47615ms step_avg:140.04ms
step:351/1375 train_time:47757ms step_avg:140.05ms
step:352/1375 train_time:47903ms step_avg:140.07ms
step:353/1375 train_time:48048ms step_avg:140.08ms
step:354/1375 train_time:48192ms step_avg:140.09ms
step:355/1375 train_time:48335ms step_avg:140.10ms
step:356/1375 train_time:48482ms step_avg:140.12ms
step:357/1375 train_time:48628ms step_avg:140.14ms
step:358/1375 train_time:48771ms step_avg:140.15ms
step:359/1375 train_time:48916ms step_avg:140.16ms
step:360/1375 train_time:49062ms step_avg:140.18ms
step:361/1375 train_time:49208ms step_avg:140.19ms
step:362/1375 train_time:49351ms step_avg:140.20ms
step:363/1375 train_time:49494ms step_avg:140.21ms
step:364/1375 train_time:49638ms step_avg:140.22ms
step:365/1375 train_time:49782ms step_avg:140.23ms
step:366/1375 train_time:49927ms step_avg:140.24ms
step:367/1375 train_time:50071ms step_avg:140.25ms
step:368/1375 train_time:50216ms step_avg:140.27ms
step:369/1375 train_time:50361ms step_avg:140.28ms
step:370/1375 train_time:50506ms step_avg:140.29ms
step:371/1375 train_time:50649ms step_avg:140.30ms
step:372/1375 train_time:50794ms step_avg:140.31ms
step:373/1375 train_time:50938ms step_avg:140.32ms
step:374/1375 train_time:51083ms step_avg:140.34ms
step:375/1375 train_time:51229ms step_avg:140.35ms
step:375/1375 val_loss:3.7754 train_time:51298ms step_avg:140.54ms
step:376/1375 train_time:51373ms step_avg:140.36ms
step:377/1375 train_time:51520ms step_avg:140.38ms
step:378/1375 train_time:51666ms step_avg:140.40ms
step:379/1375 train_time:51810ms step_avg:140.41ms
step:380/1375 train_time:51953ms step_avg:140.41ms
step:381/1375 train_time:52142ms step_avg:140.55ms
step:382/1375 train_time:52285ms step_avg:140.55ms
step:383/1375 train_time:52428ms step_avg:140.56ms
step:384/1375 train_time:52570ms step_avg:140.56ms
step:385/1375 train_time:52715ms step_avg:140.57ms
step:386/1375 train_time:52859ms step_avg:140.58ms
step:387/1375 train_time:53006ms step_avg:140.60ms
step:388/1375 train_time:53150ms step_avg:140.61ms
step:389/1375 train_time:53296ms step_avg:140.62ms
step:390/1375 train_time:53438ms step_avg:140.63ms
step:391/1375 train_time:53583ms step_avg:140.64ms
step:392/1375 train_time:53727ms step_avg:140.65ms
step:393/1375 train_time:53869ms step_avg:140.65ms
step:394/1375 train_time:54016ms step_avg:140.67ms
step:395/1375 train_time:54161ms step_avg:140.68ms
step:396/1375 train_time:54308ms step_avg:140.69ms
step:397/1375 train_time:54452ms step_avg:140.70ms
step:398/1375 train_time:54596ms step_avg:140.71ms
step:399/1375 train_time:54738ms step_avg:140.72ms
step:400/1375 train_time:54884ms step_avg:140.73ms
step:401/1375 train_time:55028ms step_avg:140.74ms
step:402/1375 train_time:55173ms step_avg:140.75ms
step:403/1375 train_time:55318ms step_avg:140.76ms
step:404/1375 train_time:55463ms step_avg:140.77ms
step:405/1375 train_time:55607ms step_avg:140.78ms
step:406/1375 train_time:55750ms step_avg:140.78ms
step:407/1375 train_time:55893ms step_avg:140.79ms
step:408/1375 train_time:56038ms step_avg:140.80ms
step:409/1375 train_time:56185ms step_avg:140.82ms
step:410/1375 train_time:56331ms step_avg:140.83ms
step:411/1375 train_time:56478ms step_avg:140.84ms
step:412/1375 train_time:56625ms step_avg:140.86ms
step:413/1375 train_time:56769ms step_avg:140.87ms
step:414/1375 train_time:56914ms step_avg:140.88ms
step:415/1375 train_time:57060ms step_avg:140.89ms
step:416/1375 train_time:57207ms step_avg:140.90ms
step:417/1375 train_time:57352ms step_avg:140.91ms
step:418/1375 train_time:57501ms step_avg:140.93ms
step:419/1375 train_time:57646ms step_avg:140.94ms
step:420/1375 train_time:57793ms step_avg:140.96ms
step:421/1375 train_time:57938ms step_avg:140.97ms
step:422/1375 train_time:58084ms step_avg:140.98ms
step:423/1375 train_time:58231ms step_avg:140.99ms
step:424/1375 train_time:58377ms step_avg:141.01ms
step:425/1375 train_time:58524ms step_avg:141.02ms
step:426/1375 train_time:58670ms step_avg:141.03ms
step:427/1375 train_time:58816ms step_avg:141.05ms
step:428/1375 train_time:58961ms step_avg:141.05ms
step:429/1375 train_time:59108ms step_avg:141.07ms
step:430/1375 train_time:59253ms step_avg:141.08ms
step:431/1375 train_time:59401ms step_avg:141.10ms
step:432/1375 train_time:59546ms step_avg:141.10ms
step:433/1375 train_time:59691ms step_avg:141.11ms
step:434/1375 train_time:59836ms step_avg:141.12ms
step:435/1375 train_time:59983ms step_avg:141.14ms
step:436/1375 train_time:60129ms step_avg:141.15ms
step:437/1375 train_time:60275ms step_avg:141.16ms
step:438/1375 train_time:60421ms step_avg:141.17ms
step:439/1375 train_time:60567ms step_avg:141.18ms
step:440/1375 train_time:60713ms step_avg:141.19ms
step:441/1375 train_time:60859ms step_avg:141.20ms
step:442/1375 train_time:61006ms step_avg:141.22ms
step:443/1375 train_time:61151ms step_avg:141.23ms
step:444/1375 train_time:61297ms step_avg:141.24ms
step:445/1375 train_time:61443ms step_avg:141.25ms
step:446/1375 train_time:61590ms step_avg:141.26ms
step:447/1375 train_time:61736ms step_avg:141.27ms
step:448/1375 train_time:61882ms step_avg:141.28ms
step:449/1375 train_time:62029ms step_avg:141.30ms
step:450/1375 train_time:62174ms step_avg:141.30ms
step:451/1375 train_time:62321ms step_avg:141.32ms
step:452/1375 train_time:62465ms step_avg:141.32ms
step:453/1375 train_time:62613ms step_avg:141.34ms
step:454/1375 train_time:62760ms step_avg:141.35ms
step:455/1375 train_time:62907ms step_avg:141.36ms
step:456/1375 train_time:63053ms step_avg:141.37ms
step:457/1375 train_time:63200ms step_avg:141.39ms
step:458/1375 train_time:63346ms step_avg:141.40ms
step:459/1375 train_time:63492ms step_avg:141.41ms
step:460/1375 train_time:63638ms step_avg:141.42ms
step:461/1375 train_time:63786ms step_avg:141.43ms
step:462/1375 train_time:63931ms step_avg:141.44ms
step:463/1375 train_time:64077ms step_avg:141.45ms
step:464/1375 train_time:64224ms step_avg:141.46ms
step:465/1375 train_time:64369ms step_avg:141.47ms
step:466/1375 train_time:64515ms step_avg:141.48ms
step:467/1375 train_time:64661ms step_avg:141.49ms
step:468/1375 train_time:64809ms step_avg:141.50ms
step:469/1375 train_time:64953ms step_avg:141.51ms
step:470/1375 train_time:65099ms step_avg:141.52ms
step:471/1375 train_time:65245ms step_avg:141.53ms
step:472/1375 train_time:65391ms step_avg:141.54ms
step:473/1375 train_time:65536ms step_avg:141.55ms
step:474/1375 train_time:65684ms step_avg:141.56ms
step:475/1375 train_time:65829ms step_avg:141.57ms
step:476/1375 train_time:65976ms step_avg:141.58ms
step:477/1375 train_time:66122ms step_avg:141.59ms
step:478/1375 train_time:66268ms step_avg:141.60ms
step:479/1375 train_time:66414ms step_avg:141.61ms
step:480/1375 train_time:66560ms step_avg:141.62ms
step:481/1375 train_time:66706ms step_avg:141.63ms
step:482/1375 train_time:66850ms step_avg:141.63ms
step:483/1375 train_time:66996ms step_avg:141.64ms
step:484/1375 train_time:67141ms step_avg:141.65ms
step:485/1375 train_time:67288ms step_avg:141.66ms
step:486/1375 train_time:67434ms step_avg:141.67ms
step:487/1375 train_time:67580ms step_avg:141.68ms
step:488/1375 train_time:67727ms step_avg:141.69ms
step:489/1375 train_time:67872ms step_avg:141.70ms
step:490/1375 train_time:68020ms step_avg:141.71ms
step:491/1375 train_time:68165ms step_avg:141.71ms
step:492/1375 train_time:68311ms step_avg:141.72ms
step:493/1375 train_time:68457ms step_avg:141.73ms
step:494/1375 train_time:68604ms step_avg:141.74ms
step:495/1375 train_time:68749ms step_avg:141.75ms
step:496/1375 train_time:68895ms step_avg:141.76ms
step:497/1375 train_time:69039ms step_avg:141.76ms
step:498/1375 train_time:69186ms step_avg:141.78ms
step:499/1375 train_time:69331ms step_avg:141.78ms
step:500/1375 train_time:69479ms step_avg:141.79ms
step:500/1375 val_loss:3.6566 train_time:69552ms step_avg:141.94ms
step:501/1375 train_time:69626ms step_avg:141.80ms
step:502/1375 train_time:69776ms step_avg:141.82ms
step:503/1375 train_time:69921ms step_avg:141.83ms
step:504/1375 train_time:70065ms step_avg:141.83ms
step:505/1375 train_time:70210ms step_avg:141.84ms
step:506/1375 train_time:70355ms step_avg:141.84ms
step:507/1375 train_time:70501ms step_avg:141.85ms
step:508/1375 train_time:70651ms step_avg:141.87ms
step:509/1375 train_time:70798ms step_avg:141.88ms
step:510/1375 train_time:70943ms step_avg:141.89ms
step:511/1375 train_time:71088ms step_avg:141.89ms
step:512/1375 train_time:71236ms step_avg:141.90ms
step:513/1375 train_time:71382ms step_avg:141.91ms
step:514/1375 train_time:71532ms step_avg:141.93ms
step:515/1375 train_time:71682ms step_avg:141.95ms
step:516/1375 train_time:71831ms step_avg:141.96ms
step:517/1375 train_time:71980ms step_avg:141.97ms
step:518/1375 train_time:72126ms step_avg:141.98ms
step:519/1375 train_time:72275ms step_avg:141.99ms
step:520/1375 train_time:72421ms step_avg:142.00ms
step:521/1375 train_time:72569ms step_avg:142.01ms
step:522/1375 train_time:72718ms step_avg:142.03ms
step:523/1375 train_time:72865ms step_avg:142.04ms
step:524/1375 train_time:73015ms step_avg:142.05ms
step:525/1375 train_time:73162ms step_avg:142.06ms
step:526/1375 train_time:73309ms step_avg:142.07ms
step:527/1375 train_time:73456ms step_avg:142.08ms
step:528/1375 train_time:73604ms step_avg:142.09ms
step:529/1375 train_time:73751ms step_avg:142.10ms
step:530/1375 train_time:73900ms step_avg:142.11ms
step:531/1375 train_time:74047ms step_avg:142.13ms
step:532/1375 train_time:74195ms step_avg:142.14ms
step:533/1375 train_time:74341ms step_avg:142.14ms
step:534/1375 train_time:74489ms step_avg:142.16ms
step:535/1375 train_time:74637ms step_avg:142.17ms
step:536/1375 train_time:74786ms step_avg:142.18ms
step:537/1375 train_time:74933ms step_avg:142.19ms
step:538/1375 train_time:75080ms step_avg:142.20ms
step:539/1375 train_time:75229ms step_avg:142.21ms
step:540/1375 train_time:75375ms step_avg:142.22ms
step:541/1375 train_time:75522ms step_avg:142.23ms
step:542/1375 train_time:75669ms step_avg:142.23ms
step:543/1375 train_time:75817ms step_avg:142.25ms
step:544/1375 train_time:75964ms step_avg:142.25ms
step:545/1375 train_time:76111ms step_avg:142.26ms
step:546/1375 train_time:76260ms step_avg:142.28ms
step:547/1375 train_time:76409ms step_avg:142.29ms
step:548/1375 train_time:76556ms step_avg:142.30ms
step:549/1375 train_time:76705ms step_avg:142.31ms
step:550/1375 train_time:76852ms step_avg:142.32ms
step:551/1375 train_time:76998ms step_avg:142.33ms
step:552/1375 train_time:77146ms step_avg:142.34ms
step:553/1375 train_time:77297ms step_avg:142.35ms
step:554/1375 train_time:77443ms step_avg:142.36ms
step:555/1375 train_time:77592ms step_avg:142.37ms
step:556/1375 train_time:77738ms step_avg:142.38ms
step:557/1375 train_time:77885ms step_avg:142.39ms
step:558/1375 train_time:78032ms step_avg:142.39ms
step:559/1375 train_time:78180ms step_avg:142.40ms
step:560/1375 train_time:78328ms step_avg:142.41ms
step:561/1375 train_time:78476ms step_avg:142.42ms
step:562/1375 train_time:78622ms step_avg:142.43ms
step:563/1375 train_time:78770ms step_avg:142.44ms
step:564/1375 train_time:78918ms step_avg:142.45ms
step:565/1375 train_time:79064ms step_avg:142.46ms
step:566/1375 train_time:79212ms step_avg:142.47ms
step:567/1375 train_time:79360ms step_avg:142.48ms
step:568/1375 train_time:79509ms step_avg:142.49ms
step:569/1375 train_time:79658ms step_avg:142.50ms
step:570/1375 train_time:79806ms step_avg:142.51ms
step:571/1375 train_time:79998ms step_avg:142.60ms
step:572/1375 train_time:80144ms step_avg:142.60ms
step:573/1375 train_time:80291ms step_avg:142.61ms
step:574/1375 train_time:80439ms step_avg:142.62ms
step:575/1375 train_time:80585ms step_avg:142.63ms
step:576/1375 train_time:80731ms step_avg:142.63ms
step:577/1375 train_time:80879ms step_avg:142.64ms
step:578/1375 train_time:81029ms step_avg:142.66ms
step:579/1375 train_time:81178ms step_avg:142.67ms
step:580/1375 train_time:81325ms step_avg:142.68ms
step:581/1375 train_time:81472ms step_avg:142.68ms
step:582/1375 train_time:81617ms step_avg:142.69ms
step:583/1375 train_time:81763ms step_avg:142.69ms
step:584/1375 train_time:81913ms step_avg:142.71ms
step:585/1375 train_time:82062ms step_avg:142.72ms
step:586/1375 train_time:82212ms step_avg:142.73ms
step:587/1375 train_time:82360ms step_avg:142.74ms
step:588/1375 train_time:82508ms step_avg:142.75ms
step:589/1375 train_time:82654ms step_avg:142.75ms
step:590/1375 train_time:82802ms step_avg:142.76ms
step:591/1375 train_time:82949ms step_avg:142.77ms
step:592/1375 train_time:83098ms step_avg:142.78ms
step:593/1375 train_time:83245ms step_avg:142.79ms
step:594/1375 train_time:83394ms step_avg:142.80ms
step:595/1375 train_time:83540ms step_avg:142.80ms
step:596/1375 train_time:83689ms step_avg:142.81ms
step:597/1375 train_time:83835ms step_avg:142.82ms
step:598/1375 train_time:83983ms step_avg:142.83ms
step:599/1375 train_time:84131ms step_avg:142.84ms
step:600/1375 train_time:84280ms step_avg:142.85ms
step:601/1375 train_time:84428ms step_avg:142.86ms
step:602/1375 train_time:84575ms step_avg:142.86ms
step:603/1375 train_time:84722ms step_avg:142.87ms
step:604/1375 train_time:84870ms step_avg:142.88ms
step:605/1375 train_time:85018ms step_avg:142.89ms
step:606/1375 train_time:85165ms step_avg:142.89ms
step:607/1375 train_time:85312ms step_avg:142.90ms
step:608/1375 train_time:85460ms step_avg:142.91ms
step:609/1375 train_time:85608ms step_avg:142.92ms
step:610/1375 train_time:85754ms step_avg:142.92ms
step:611/1375 train_time:85901ms step_avg:142.93ms
step:612/1375 train_time:86050ms step_avg:142.94ms
step:613/1375 train_time:86198ms step_avg:142.95ms
step:614/1375 train_time:86347ms step_avg:142.96ms
step:615/1375 train_time:86495ms step_avg:142.97ms
step:616/1375 train_time:86643ms step_avg:142.98ms
step:617/1375 train_time:86791ms step_avg:142.98ms
step:618/1375 train_time:86939ms step_avg:142.99ms
step:619/1375 train_time:87090ms step_avg:143.00ms
step:620/1375 train_time:87237ms step_avg:143.01ms
step:621/1375 train_time:87386ms step_avg:143.02ms
step:622/1375 train_time:87535ms step_avg:143.03ms
step:623/1375 train_time:87684ms step_avg:143.04ms
step:624/1375 train_time:87833ms step_avg:143.05ms
step:625/1375 train_time:87981ms step_avg:143.06ms
step:625/1375 val_loss:3.5752 train_time:88056ms step_avg:143.18ms
step:626/1375 train_time:88132ms step_avg:143.07ms
step:627/1375 train_time:88281ms step_avg:143.08ms
step:628/1375 train_time:88429ms step_avg:143.09ms
step:629/1375 train_time:88578ms step_avg:143.10ms
step:630/1375 train_time:88725ms step_avg:143.11ms
step:631/1375 train_time:88873ms step_avg:143.11ms
step:632/1375 train_time:89023ms step_avg:143.12ms
step:633/1375 train_time:89174ms step_avg:143.14ms
step:634/1375 train_time:89323ms step_avg:143.15ms
step:635/1375 train_time:89472ms step_avg:143.16ms
step:636/1375 train_time:89622ms step_avg:143.17ms
step:637/1375 train_time:89771ms step_avg:143.18ms
step:638/1375 train_time:89920ms step_avg:143.19ms
step:639/1375 train_time:90069ms step_avg:143.19ms
step:640/1375 train_time:90220ms step_avg:143.21ms
step:641/1375 train_time:90370ms step_avg:143.22ms
step:642/1375 train_time:90519ms step_avg:143.23ms
step:643/1375 train_time:90669ms step_avg:143.24ms
step:644/1375 train_time:90817ms step_avg:143.24ms
step:645/1375 train_time:90968ms step_avg:143.26ms
step:646/1375 train_time:91117ms step_avg:143.27ms
step:647/1375 train_time:91267ms step_avg:143.28ms
step:648/1375 train_time:91421ms step_avg:143.29ms
step:649/1375 train_time:91571ms step_avg:143.30ms
step:650/1375 train_time:91720ms step_avg:143.31ms
step:651/1375 train_time:91868ms step_avg:143.32ms
step:652/1375 train_time:92017ms step_avg:143.33ms
step:653/1375 train_time:92167ms step_avg:143.34ms
step:654/1375 train_time:92316ms step_avg:143.35ms
step:655/1375 train_time:92466ms step_avg:143.36ms
step:656/1375 train_time:92614ms step_avg:143.37ms
step:657/1375 train_time:92763ms step_avg:143.37ms
step:658/1375 train_time:92911ms step_avg:143.38ms
step:659/1375 train_time:93060ms step_avg:143.39ms
step:660/1375 train_time:93208ms step_avg:143.40ms
step:661/1375 train_time:93357ms step_avg:143.41ms
step:662/1375 train_time:93506ms step_avg:143.41ms
step:663/1375 train_time:93654ms step_avg:143.42ms
step:664/1375 train_time:93804ms step_avg:143.43ms
step:665/1375 train_time:93953ms step_avg:143.44ms
step:666/1375 train_time:94101ms step_avg:143.45ms
step:667/1375 train_time:94249ms step_avg:143.45ms
step:668/1375 train_time:94399ms step_avg:143.46ms
step:669/1375 train_time:94549ms step_avg:143.47ms
step:670/1375 train_time:94698ms step_avg:143.48ms
step:671/1375 train_time:94849ms step_avg:143.49ms
step:672/1375 train_time:94997ms step_avg:143.50ms
step:673/1375 train_time:95147ms step_avg:143.51ms
step:674/1375 train_time:95295ms step_avg:143.52ms
step:675/1375 train_time:95445ms step_avg:143.53ms
step:676/1375 train_time:95595ms step_avg:143.54ms
step:677/1375 train_time:95745ms step_avg:143.55ms
step:678/1375 train_time:95893ms step_avg:143.55ms
step:679/1375 train_time:96043ms step_avg:143.56ms
step:680/1375 train_time:96190ms step_avg:143.57ms
step:681/1375 train_time:96339ms step_avg:143.57ms
step:682/1375 train_time:96486ms step_avg:143.58ms
step:683/1375 train_time:96634ms step_avg:143.59ms
step:684/1375 train_time:96787ms step_avg:143.60ms
step:685/1375 train_time:96934ms step_avg:143.61ms
step:686/1375 train_time:97084ms step_avg:143.61ms
step:687/1375 train_time:97231ms step_avg:143.62ms
step:688/1375 train_time:97383ms step_avg:143.63ms
step:689/1375 train_time:97530ms step_avg:143.64ms
step:690/1375 train_time:97682ms step_avg:143.65ms
step:691/1375 train_time:97830ms step_avg:143.66ms
step:692/1375 train_time:97981ms step_avg:143.67ms
step:693/1375 train_time:98129ms step_avg:143.67ms
step:694/1375 train_time:98278ms step_avg:143.68ms
step:695/1375 train_time:98428ms step_avg:143.69ms
step:696/1375 train_time:98575ms step_avg:143.70ms
step:697/1375 train_time:98726ms step_avg:143.71ms
step:698/1375 train_time:98875ms step_avg:143.71ms
step:699/1375 train_time:99023ms step_avg:143.72ms
step:700/1375 train_time:99172ms step_avg:143.73ms
step:701/1375 train_time:99321ms step_avg:143.74ms
step:702/1375 train_time:99473ms step_avg:143.75ms
step:703/1375 train_time:99622ms step_avg:143.75ms
step:704/1375 train_time:99771ms step_avg:143.76ms
step:705/1375 train_time:99921ms step_avg:143.77ms
step:706/1375 train_time:100072ms step_avg:143.78ms
step:707/1375 train_time:100221ms step_avg:143.79ms
step:708/1375 train_time:100369ms step_avg:143.80ms
step:709/1375 train_time:100518ms step_avg:143.80ms
step:710/1375 train_time:100668ms step_avg:143.81ms
step:711/1375 train_time:100816ms step_avg:143.82ms
step:712/1375 train_time:100966ms step_avg:143.83ms
step:713/1375 train_time:101115ms step_avg:143.83ms
step:714/1375 train_time:101266ms step_avg:143.84ms
step:715/1375 train_time:101414ms step_avg:143.85ms
step:716/1375 train_time:101565ms step_avg:143.86ms
step:717/1375 train_time:101713ms step_avg:143.87ms
step:718/1375 train_time:101864ms step_avg:143.88ms
step:719/1375 train_time:102011ms step_avg:143.88ms
step:720/1375 train_time:102164ms step_avg:143.89ms
step:721/1375 train_time:102313ms step_avg:143.90ms
step:722/1375 train_time:102467ms step_avg:143.91ms
step:723/1375 train_time:102615ms step_avg:143.92ms
step:724/1375 train_time:102767ms step_avg:143.93ms
step:725/1375 train_time:102916ms step_avg:143.94ms
step:726/1375 train_time:103065ms step_avg:143.95ms
step:727/1375 train_time:103215ms step_avg:143.95ms
step:728/1375 train_time:103366ms step_avg:143.96ms
step:729/1375 train_time:103513ms step_avg:143.97ms
step:730/1375 train_time:103667ms step_avg:143.98ms
step:731/1375 train_time:103816ms step_avg:143.99ms
step:732/1375 train_time:103965ms step_avg:144.00ms
step:733/1375 train_time:104114ms step_avg:144.00ms
step:734/1375 train_time:104266ms step_avg:144.01ms
step:735/1375 train_time:104415ms step_avg:144.02ms
step:736/1375 train_time:104566ms step_avg:144.03ms
step:737/1375 train_time:104714ms step_avg:144.04ms
step:738/1375 train_time:104866ms step_avg:144.05ms
step:739/1375 train_time:105015ms step_avg:144.05ms
step:740/1375 train_time:105167ms step_avg:144.06ms
step:741/1375 train_time:105316ms step_avg:144.07ms
step:742/1375 train_time:105466ms step_avg:144.08ms
step:743/1375 train_time:105614ms step_avg:144.08ms
step:744/1375 train_time:105764ms step_avg:144.09ms
step:745/1375 train_time:105915ms step_avg:144.10ms
step:746/1375 train_time:106065ms step_avg:144.11ms
step:747/1375 train_time:106213ms step_avg:144.12ms
step:748/1375 train_time:106365ms step_avg:144.13ms
step:749/1375 train_time:106514ms step_avg:144.13ms
step:750/1375 train_time:106665ms step_avg:144.14ms
step:750/1375 val_loss:3.5206 train_time:106740ms step_avg:144.24ms
step:751/1375 train_time:106816ms step_avg:144.15ms
step:752/1375 train_time:106969ms step_avg:144.16ms
step:753/1375 train_time:107117ms step_avg:144.17ms
step:754/1375 train_time:107266ms step_avg:144.17ms
step:755/1375 train_time:107416ms step_avg:144.18ms
step:756/1375 train_time:107565ms step_avg:144.19ms
step:757/1375 train_time:107717ms step_avg:144.20ms
step:758/1375 train_time:107869ms step_avg:144.21ms
step:759/1375 train_time:108019ms step_avg:144.22ms
step:760/1375 train_time:108169ms step_avg:144.23ms
step:761/1375 train_time:108363ms step_avg:144.29ms
step:762/1375 train_time:108513ms step_avg:144.30ms
step:763/1375 train_time:108663ms step_avg:144.31ms
step:764/1375 train_time:108813ms step_avg:144.31ms
step:765/1375 train_time:108963ms step_avg:144.32ms
step:766/1375 train_time:109115ms step_avg:144.33ms
step:767/1375 train_time:109267ms step_avg:144.34ms
step:768/1375 train_time:109419ms step_avg:144.35ms
step:769/1375 train_time:109571ms step_avg:144.36ms
step:770/1375 train_time:109722ms step_avg:144.37ms
step:771/1375 train_time:109872ms step_avg:144.38ms
step:772/1375 train_time:110020ms step_avg:144.38ms
step:773/1375 train_time:110170ms step_avg:144.39ms
step:774/1375 train_time:110321ms step_avg:144.40ms
step:775/1375 train_time:110473ms step_avg:144.41ms
step:776/1375 train_time:110626ms step_avg:144.42ms
step:777/1375 train_time:110777ms step_avg:144.43ms
step:778/1375 train_time:110926ms step_avg:144.43ms
step:779/1375 train_time:111073ms step_avg:144.44ms
step:780/1375 train_time:111224ms step_avg:144.45ms
step:781/1375 train_time:111374ms step_avg:144.45ms
step:782/1375 train_time:111527ms step_avg:144.46ms
step:783/1375 train_time:111676ms step_avg:144.47ms
step:784/1375 train_time:111828ms step_avg:144.48ms
step:785/1375 train_time:111977ms step_avg:144.49ms
step:786/1375 train_time:112129ms step_avg:144.50ms
step:787/1375 train_time:112277ms step_avg:144.50ms
step:788/1375 train_time:112428ms step_avg:144.51ms
step:789/1375 train_time:112577ms step_avg:144.51ms
step:790/1375 train_time:112727ms step_avg:144.52ms
step:791/1375 train_time:112877ms step_avg:144.53ms
step:792/1375 train_time:113028ms step_avg:144.54ms
step:793/1375 train_time:113176ms step_avg:144.54ms
step:794/1375 train_time:113326ms step_avg:144.55ms
step:795/1375 train_time:113477ms step_avg:144.56ms
step:796/1375 train_time:113630ms step_avg:144.57ms
step:797/1375 train_time:113778ms step_avg:144.57ms
step:798/1375 train_time:113929ms step_avg:144.58ms
step:799/1375 train_time:114082ms step_avg:144.59ms
step:800/1375 train_time:114234ms step_avg:144.60ms
step:801/1375 train_time:114385ms step_avg:144.61ms
step:802/1375 train_time:114536ms step_avg:144.62ms
step:803/1375 train_time:114685ms step_avg:144.62ms
step:804/1375 train_time:114833ms step_avg:144.63ms
step:805/1375 train_time:114987ms step_avg:144.64ms
step:806/1375 train_time:115138ms step_avg:144.65ms
step:807/1375 train_time:115286ms step_avg:144.65ms
step:808/1375 train_time:115438ms step_avg:144.66ms
step:809/1375 train_time:115589ms step_avg:144.67ms
step:810/1375 train_time:115738ms step_avg:144.67ms
step:811/1375 train_time:115888ms step_avg:144.68ms
step:812/1375 train_time:116039ms step_avg:144.69ms
step:813/1375 train_time:116188ms step_avg:144.69ms
step:814/1375 train_time:116338ms step_avg:144.70ms
step:815/1375 train_time:116488ms step_avg:144.71ms
step:816/1375 train_time:116642ms step_avg:144.72ms
step:817/1375 train_time:116793ms step_avg:144.73ms
step:818/1375 train_time:116945ms step_avg:144.73ms
step:819/1375 train_time:117097ms step_avg:144.74ms
step:820/1375 train_time:117250ms step_avg:144.75ms
step:821/1375 train_time:117401ms step_avg:144.76ms
step:822/1375 train_time:117552ms step_avg:144.77ms
step:823/1375 train_time:117704ms step_avg:144.78ms
step:824/1375 train_time:117852ms step_avg:144.78ms
step:825/1375 train_time:118007ms step_avg:144.79ms
step:826/1375 train_time:118159ms step_avg:144.80ms
step:827/1375 train_time:118311ms step_avg:144.81ms
step:828/1375 train_time:118462ms step_avg:144.82ms
step:829/1375 train_time:118612ms step_avg:144.83ms
step:830/1375 train_time:118764ms step_avg:144.83ms
step:831/1375 train_time:118915ms step_avg:144.84ms
step:832/1375 train_time:119069ms step_avg:144.85ms
step:833/1375 train_time:119219ms step_avg:144.86ms
step:834/1375 train_time:119370ms step_avg:144.87ms
step:835/1375 train_time:119522ms step_avg:144.88ms
step:836/1375 train_time:119676ms step_avg:144.89ms
step:837/1375 train_time:119828ms step_avg:144.90ms
step:838/1375 train_time:119978ms step_avg:144.90ms
step:839/1375 train_time:120130ms step_avg:144.91ms
step:840/1375 train_time:120280ms step_avg:144.92ms
step:841/1375 train_time:120432ms step_avg:144.92ms
step:842/1375 train_time:120584ms step_avg:144.93ms
step:843/1375 train_time:120734ms step_avg:144.94ms
step:844/1375 train_time:120886ms step_avg:144.95ms
step:845/1375 train_time:121036ms step_avg:144.95ms
step:846/1375 train_time:121188ms step_avg:144.96ms
step:847/1375 train_time:121342ms step_avg:144.97ms
step:848/1375 train_time:121492ms step_avg:144.98ms
step:849/1375 train_time:121646ms step_avg:144.99ms
step:850/1375 train_time:121801ms step_avg:145.00ms
step:851/1375 train_time:121952ms step_avg:145.01ms
step:852/1375 train_time:122106ms step_avg:145.02ms
step:853/1375 train_time:122257ms step_avg:145.03ms
step:854/1375 train_time:122408ms step_avg:145.03ms
step:855/1375 train_time:122558ms step_avg:145.04ms
step:856/1375 train_time:122709ms step_avg:145.05ms
step:857/1375 train_time:122862ms step_avg:145.05ms
step:858/1375 train_time:123018ms step_avg:145.07ms
step:859/1375 train_time:123170ms step_avg:145.08ms
step:860/1375 train_time:123321ms step_avg:145.08ms
step:861/1375 train_time:123473ms step_avg:145.09ms
step:862/1375 train_time:123624ms step_avg:145.10ms
step:863/1375 train_time:123775ms step_avg:145.11ms
step:864/1375 train_time:123928ms step_avg:145.11ms
step:865/1375 train_time:124077ms step_avg:145.12ms
step:866/1375 train_time:124234ms step_avg:145.13ms
step:867/1375 train_time:124385ms step_avg:145.14ms
step:868/1375 train_time:124537ms step_avg:145.15ms
step:869/1375 train_time:124687ms step_avg:145.15ms
step:870/1375 train_time:124841ms step_avg:145.16ms
step:871/1375 train_time:124990ms step_avg:145.17ms
step:872/1375 train_time:125143ms step_avg:145.18ms
step:873/1375 train_time:125293ms step_avg:145.18ms
step:874/1375 train_time:125448ms step_avg:145.19ms
step:875/1375 train_time:125598ms step_avg:145.20ms
step:875/1375 val_loss:3.4677 train_time:125673ms step_avg:145.29ms
step:876/1375 train_time:125750ms step_avg:145.21ms
step:877/1375 train_time:125904ms step_avg:145.22ms
step:878/1375 train_time:126055ms step_avg:145.23ms
step:879/1375 train_time:126206ms step_avg:145.23ms
step:880/1375 train_time:126356ms step_avg:145.24ms
step:881/1375 train_time:126506ms step_avg:145.24ms
step:882/1375 train_time:126661ms step_avg:145.25ms
step:883/1375 train_time:126815ms step_avg:145.26ms
step:884/1375 train_time:126970ms step_avg:145.27ms
step:885/1375 train_time:127121ms step_avg:145.28ms
step:886/1375 train_time:127275ms step_avg:145.29ms
step:887/1375 train_time:127425ms step_avg:145.30ms
step:888/1375 train_time:127577ms step_avg:145.30ms
step:889/1375 train_time:127732ms step_avg:145.31ms
step:890/1375 train_time:127882ms step_avg:145.32ms
step:891/1375 train_time:128032ms step_avg:145.33ms
step:892/1375 train_time:128185ms step_avg:145.33ms
step:893/1375 train_time:128336ms step_avg:145.34ms
step:894/1375 train_time:128488ms step_avg:145.35ms
step:895/1375 train_time:128645ms step_avg:145.36ms
step:896/1375 train_time:128796ms step_avg:145.37ms
step:897/1375 train_time:128948ms step_avg:145.38ms
step:898/1375 train_time:129102ms step_avg:145.39ms
step:899/1375 train_time:129253ms step_avg:145.39ms
step:900/1375 train_time:129404ms step_avg:145.40ms
step:901/1375 train_time:129556ms step_avg:145.41ms
step:902/1375 train_time:129705ms step_avg:145.41ms
step:903/1375 train_time:129857ms step_avg:145.42ms
step:904/1375 train_time:130009ms step_avg:145.42ms
step:905/1375 train_time:130162ms step_avg:145.43ms
step:906/1375 train_time:130314ms step_avg:145.44ms
step:907/1375 train_time:130469ms step_avg:145.45ms
step:908/1375 train_time:130621ms step_avg:145.46ms
step:909/1375 train_time:130773ms step_avg:145.47ms
step:910/1375 train_time:130929ms step_avg:145.48ms
step:911/1375 train_time:131082ms step_avg:145.48ms
step:912/1375 train_time:131231ms step_avg:145.49ms
step:913/1375 train_time:131386ms step_avg:145.50ms
step:914/1375 train_time:131536ms step_avg:145.50ms
step:915/1375 train_time:131687ms step_avg:145.51ms
step:916/1375 train_time:131839ms step_avg:145.52ms
step:917/1375 train_time:131990ms step_avg:145.52ms
step:918/1375 train_time:132144ms step_avg:145.53ms
step:919/1375 train_time:132300ms step_avg:145.54ms
step:920/1375 train_time:132452ms step_avg:145.55ms
step:921/1375 train_time:132605ms step_avg:145.56ms
step:922/1375 train_time:132765ms step_avg:145.58ms
step:923/1375 train_time:132916ms step_avg:145.58ms
step:924/1375 train_time:133071ms step_avg:145.59ms
step:925/1375 train_time:133225ms step_avg:145.60ms
step:926/1375 train_time:133380ms step_avg:145.61ms
step:927/1375 train_time:133531ms step_avg:145.62ms
step:928/1375 train_time:133683ms step_avg:145.62ms
step:929/1375 train_time:133837ms step_avg:145.63ms
step:930/1375 train_time:133991ms step_avg:145.64ms
step:931/1375 train_time:134143ms step_avg:145.65ms
step:932/1375 train_time:134295ms step_avg:145.66ms
step:933/1375 train_time:134447ms step_avg:145.66ms
step:934/1375 train_time:134602ms step_avg:145.67ms
step:935/1375 train_time:134757ms step_avg:145.68ms
step:936/1375 train_time:134910ms step_avg:145.69ms
step:937/1375 train_time:135068ms step_avg:145.70ms
step:938/1375 train_time:135222ms step_avg:145.71ms
step:939/1375 train_time:135375ms step_avg:145.72ms
step:940/1375 train_time:135530ms step_avg:145.73ms
step:941/1375 train_time:135684ms step_avg:145.74ms
step:942/1375 train_time:135836ms step_avg:145.75ms
step:943/1375 train_time:135989ms step_avg:145.75ms
step:944/1375 train_time:136147ms step_avg:145.77ms
step:945/1375 train_time:136301ms step_avg:145.78ms
step:946/1375 train_time:136456ms step_avg:145.79ms
step:947/1375 train_time:136609ms step_avg:145.79ms
step:948/1375 train_time:136763ms step_avg:145.80ms
step:949/1375 train_time:136917ms step_avg:145.81ms
step:950/1375 train_time:137070ms step_avg:145.82ms
step:951/1375 train_time:137268ms step_avg:145.87ms
step:952/1375 train_time:137420ms step_avg:145.88ms
step:953/1375 train_time:137573ms step_avg:145.89ms
step:954/1375 train_time:137724ms step_avg:145.89ms
step:955/1375 train_time:137873ms step_avg:145.90ms
step:956/1375 train_time:138027ms step_avg:145.91ms
step:957/1375 train_time:138183ms step_avg:145.92ms
step:958/1375 train_time:138339ms step_avg:145.93ms
step:959/1375 train_time:138495ms step_avg:145.94ms
step:960/1375 train_time:138648ms step_avg:145.95ms
step:961/1375 train_time:138802ms step_avg:145.95ms
step:962/1375 train_time:138953ms step_avg:145.96ms
step:963/1375 train_time:139111ms step_avg:145.97ms
step:964/1375 train_time:139264ms step_avg:145.98ms
step:965/1375 train_time:139415ms step_avg:145.98ms
step:966/1375 train_time:139570ms step_avg:145.99ms
step:967/1375 train_time:139722ms step_avg:146.00ms
step:968/1375 train_time:139873ms step_avg:146.01ms
step:969/1375 train_time:140026ms step_avg:146.01ms
step:970/1375 train_time:140177ms step_avg:146.02ms
step:971/1375 train_time:140329ms step_avg:146.02ms
step:972/1375 train_time:140483ms step_avg:146.03ms
step:973/1375 train_time:140635ms step_avg:146.04ms
step:974/1375 train_time:140789ms step_avg:146.05ms
step:975/1375 train_time:140944ms step_avg:146.06ms
step:976/1375 train_time:141094ms step_avg:146.06ms
step:977/1375 train_time:141247ms step_avg:146.07ms
step:978/1375 train_time:141400ms step_avg:146.07ms
step:979/1375 train_time:141550ms step_avg:146.08ms
step:980/1375 train_time:141703ms step_avg:146.09ms
step:981/1375 train_time:141854ms step_avg:146.09ms
step:982/1375 train_time:142005ms step_avg:146.10ms
step:983/1375 train_time:142157ms step_avg:146.10ms
step:984/1375 train_time:142309ms step_avg:146.11ms
step:985/1375 train_time:142463ms step_avg:146.12ms
step:986/1375 train_time:142620ms step_avg:146.13ms
step:987/1375 train_time:142770ms step_avg:146.13ms
step:988/1375 train_time:142924ms step_avg:146.14ms
step:989/1375 train_time:143078ms step_avg:146.15ms
step:990/1375 train_time:143231ms step_avg:146.15ms
step:991/1375 train_time:143383ms step_avg:146.16ms
step:992/1375 train_time:143539ms step_avg:146.17ms
step:993/1375 train_time:143702ms step_avg:146.19ms
step:994/1375 train_time:143854ms step_avg:146.19ms
step:995/1375 train_time:144006ms step_avg:146.20ms
step:996/1375 train_time:144156ms step_avg:146.20ms
step:997/1375 train_time:144307ms step_avg:146.21ms
step:998/1375 train_time:144461ms step_avg:146.22ms
step:999/1375 train_time:144614ms step_avg:146.22ms
step:1000/1375 train_time:144767ms step_avg:146.23ms
step:1000/1375 val_loss:3.4026 train_time:144844ms step_avg:146.31ms
step:1001/1375 train_time:144920ms step_avg:146.24ms
step:1002/1375 train_time:145076ms step_avg:146.25ms
step:1003/1375 train_time:145231ms step_avg:146.25ms
step:1004/1375 train_time:145384ms step_avg:146.26ms
step:1005/1375 train_time:145536ms step_avg:146.27ms
step:1006/1375 train_time:145686ms step_avg:146.27ms
step:1007/1375 train_time:145840ms step_avg:146.28ms
step:1008/1375 train_time:145994ms step_avg:146.29ms
step:1009/1375 train_time:146154ms step_avg:146.30ms
step:1010/1375 train_time:146305ms step_avg:146.31ms
step:1011/1375 train_time:146457ms step_avg:146.31ms
step:1012/1375 train_time:146610ms step_avg:146.32ms
step:1013/1375 train_time:146761ms step_avg:146.32ms
step:1014/1375 train_time:146914ms step_avg:146.33ms
step:1015/1375 train_time:147066ms step_avg:146.33ms
step:1016/1375 train_time:147218ms step_avg:146.34ms
step:1017/1375 train_time:147372ms step_avg:146.35ms
step:1018/1375 train_time:147522ms step_avg:146.35ms
step:1019/1375 train_time:147675ms step_avg:146.36ms
step:1020/1375 train_time:147830ms step_avg:146.37ms
step:1021/1375 train_time:147983ms step_avg:146.37ms
step:1022/1375 train_time:148137ms step_avg:146.38ms
step:1023/1375 train_time:148292ms step_avg:146.39ms
step:1024/1375 train_time:148446ms step_avg:146.40ms
step:1025/1375 train_time:148599ms step_avg:146.40ms
step:1026/1375 train_time:148754ms step_avg:146.41ms
step:1027/1375 train_time:148905ms step_avg:146.42ms
step:1028/1375 train_time:149059ms step_avg:146.42ms
step:1029/1375 train_time:149216ms step_avg:146.43ms
step:1030/1375 train_time:149371ms step_avg:146.44ms
step:1031/1375 train_time:149523ms step_avg:146.45ms
step:1032/1375 train_time:149676ms step_avg:146.45ms
step:1033/1375 train_time:149829ms step_avg:146.46ms
step:1034/1375 train_time:149981ms step_avg:146.47ms
step:1035/1375 train_time:150137ms step_avg:146.47ms
step:1036/1375 train_time:150291ms step_avg:146.48ms
step:1037/1375 train_time:150445ms step_avg:146.49ms
step:1038/1375 train_time:150598ms step_avg:146.50ms
step:1039/1375 train_time:150752ms step_avg:146.50ms
step:1040/1375 train_time:150904ms step_avg:146.51ms
step:1041/1375 train_time:151057ms step_avg:146.52ms
step:1042/1375 train_time:151209ms step_avg:146.52ms
step:1043/1375 train_time:151360ms step_avg:146.53ms
step:1044/1375 train_time:151517ms step_avg:146.53ms
step:1045/1375 train_time:151674ms step_avg:146.54ms
step:1046/1375 train_time:151826ms step_avg:146.55ms
step:1047/1375 train_time:151980ms step_avg:146.56ms
step:1048/1375 train_time:152136ms step_avg:146.57ms
step:1049/1375 train_time:152291ms step_avg:146.57ms
step:1050/1375 train_time:152447ms step_avg:146.58ms
step:1051/1375 train_time:152601ms step_avg:146.59ms
step:1052/1375 train_time:152754ms step_avg:146.60ms
step:1053/1375 train_time:152906ms step_avg:146.60ms
step:1054/1375 train_time:153061ms step_avg:146.61ms
step:1055/1375 train_time:153215ms step_avg:146.62ms
step:1056/1375 train_time:153370ms step_avg:146.63ms
step:1057/1375 train_time:153527ms step_avg:146.63ms
step:1058/1375 train_time:153686ms step_avg:146.65ms
step:1059/1375 train_time:153840ms step_avg:146.65ms
step:1060/1375 train_time:153995ms step_avg:146.66ms
step:1061/1375 train_time:154147ms step_avg:146.67ms
step:1062/1375 train_time:154303ms step_avg:146.68ms
step:1063/1375 train_time:154457ms step_avg:146.68ms
step:1064/1375 train_time:154610ms step_avg:146.69ms
step:1065/1375 train_time:154764ms step_avg:146.70ms
step:1066/1375 train_time:154926ms step_avg:146.71ms
step:1067/1375 train_time:155081ms step_avg:146.72ms
step:1068/1375 train_time:155235ms step_avg:146.72ms
step:1069/1375 train_time:155395ms step_avg:146.74ms
step:1070/1375 train_time:155547ms step_avg:146.74ms
step:1071/1375 train_time:155701ms step_avg:146.75ms
step:1072/1375 train_time:155855ms step_avg:146.76ms
step:1073/1375 train_time:156009ms step_avg:146.76ms
step:1074/1375 train_time:156163ms step_avg:146.77ms
step:1075/1375 train_time:156318ms step_avg:146.78ms
step:1076/1375 train_time:156472ms step_avg:146.78ms
step:1077/1375 train_time:156624ms step_avg:146.79ms
step:1078/1375 train_time:156780ms step_avg:146.80ms
step:1079/1375 train_time:156941ms step_avg:146.81ms
step:1080/1375 train_time:157095ms step_avg:146.82ms
step:1081/1375 train_time:157248ms step_avg:146.82ms
step:1082/1375 train_time:157401ms step_avg:146.83ms
step:1083/1375 train_time:157555ms step_avg:146.84ms
step:1084/1375 train_time:157711ms step_avg:146.84ms
step:1085/1375 train_time:157862ms step_avg:146.85ms
step:1086/1375 train_time:158018ms step_avg:146.86ms
step:1087/1375 train_time:158175ms step_avg:146.87ms
step:1088/1375 train_time:158329ms step_avg:146.87ms
step:1089/1375 train_time:158487ms step_avg:146.88ms
step:1090/1375 train_time:158644ms step_avg:146.89ms
step:1091/1375 train_time:158800ms step_avg:146.90ms
step:1092/1375 train_time:158953ms step_avg:146.91ms
step:1093/1375 train_time:159106ms step_avg:146.91ms
step:1094/1375 train_time:159258ms step_avg:146.92ms
step:1095/1375 train_time:159412ms step_avg:146.92ms
step:1096/1375 train_time:159569ms step_avg:146.93ms
step:1097/1375 train_time:159723ms step_avg:146.94ms
step:1098/1375 train_time:159876ms step_avg:146.94ms
step:1099/1375 train_time:160029ms step_avg:146.95ms
step:1100/1375 train_time:160182ms step_avg:146.96ms
step:1101/1375 train_time:160335ms step_avg:146.96ms
step:1102/1375 train_time:160490ms step_avg:146.97ms
step:1103/1375 train_time:160642ms step_avg:146.97ms
step:1104/1375 train_time:160795ms step_avg:146.98ms
step:1105/1375 train_time:160950ms step_avg:146.99ms
step:1106/1375 train_time:161102ms step_avg:146.99ms
step:1107/1375 train_time:161256ms step_avg:147.00ms
step:1108/1375 train_time:161413ms step_avg:147.01ms
step:1109/1375 train_time:161566ms step_avg:147.01ms
step:1110/1375 train_time:161720ms step_avg:147.02ms
step:1111/1375 train_time:161875ms step_avg:147.03ms
step:1112/1375 train_time:162030ms step_avg:147.03ms
step:1113/1375 train_time:162184ms step_avg:147.04ms
step:1114/1375 train_time:162341ms step_avg:147.05ms
step:1115/1375 train_time:162496ms step_avg:147.06ms
step:1116/1375 train_time:162648ms step_avg:147.06ms
step:1117/1375 train_time:162805ms step_avg:147.07ms
step:1118/1375 train_time:162963ms step_avg:147.08ms
step:1119/1375 train_time:163118ms step_avg:147.09ms
step:1120/1375 train_time:163271ms step_avg:147.09ms
step:1121/1375 train_time:163423ms step_avg:147.10ms
step:1122/1375 train_time:163579ms step_avg:147.10ms
step:1123/1375 train_time:163733ms step_avg:147.11ms
step:1124/1375 train_time:163892ms step_avg:147.12ms
step:1125/1375 train_time:164051ms step_avg:147.13ms
step:1125/1375 val_loss:3.3490 train_time:164128ms step_avg:147.20ms
step:1126/1375 train_time:164204ms step_avg:147.14ms
step:1127/1375 train_time:164363ms step_avg:147.15ms
step:1128/1375 train_time:164518ms step_avg:147.15ms
step:1129/1375 train_time:164677ms step_avg:147.16ms
step:1130/1375 train_time:164831ms step_avg:147.17ms
step:1131/1375 train_time:164989ms step_avg:147.18ms
step:1132/1375 train_time:165142ms step_avg:147.19ms
step:1133/1375 train_time:165297ms step_avg:147.19ms
step:1134/1375 train_time:165451ms step_avg:147.20ms
step:1135/1375 train_time:165604ms step_avg:147.20ms
step:1136/1375 train_time:165761ms step_avg:147.21ms
step:1137/1375 train_time:165914ms step_avg:147.22ms
step:1138/1375 train_time:166068ms step_avg:147.22ms
step:1139/1375 train_time:166222ms step_avg:147.23ms
step:1140/1375 train_time:166377ms step_avg:147.24ms
step:1141/1375 train_time:166575ms step_avg:147.28ms
step:1142/1375 train_time:166727ms step_avg:147.29ms
step:1143/1375 train_time:166887ms step_avg:147.30ms
step:1144/1375 train_time:167042ms step_avg:147.30ms
step:1145/1375 train_time:167194ms step_avg:147.31ms
step:1146/1375 train_time:167351ms step_avg:147.32ms
step:1147/1375 train_time:167505ms step_avg:147.32ms
step:1148/1375 train_time:167662ms step_avg:147.33ms
step:1149/1375 train_time:167817ms step_avg:147.34ms
step:1150/1375 train_time:167969ms step_avg:147.34ms
step:1151/1375 train_time:168124ms step_avg:147.35ms
step:1152/1375 train_time:168279ms step_avg:147.35ms
step:1153/1375 train_time:168435ms step_avg:147.36ms
step:1154/1375 train_time:168590ms step_avg:147.37ms
step:1155/1375 train_time:168745ms step_avg:147.38ms
step:1156/1375 train_time:168903ms step_avg:147.39ms
step:1157/1375 train_time:169061ms step_avg:147.39ms
step:1158/1375 train_time:169216ms step_avg:147.40ms
step:1159/1375 train_time:169371ms step_avg:147.41ms
step:1160/1375 train_time:169522ms step_avg:147.41ms
step:1161/1375 train_time:169678ms step_avg:147.42ms
step:1162/1375 train_time:169834ms step_avg:147.43ms
step:1163/1375 train_time:169989ms step_avg:147.43ms
step:1164/1375 train_time:170145ms step_avg:147.44ms
step:1165/1375 train_time:170298ms step_avg:147.44ms
step:1166/1375 train_time:170455ms step_avg:147.45ms
step:1167/1375 train_time:170608ms step_avg:147.46ms
step:1168/1375 train_time:170765ms step_avg:147.47ms
step:1169/1375 train_time:170919ms step_avg:147.47ms
step:1170/1375 train_time:171076ms step_avg:147.48ms
step:1171/1375 train_time:171232ms step_avg:147.49ms
step:1172/1375 train_time:171388ms step_avg:147.49ms
step:1173/1375 train_time:171543ms step_avg:147.50ms
step:1174/1375 train_time:171706ms step_avg:147.51ms
step:1175/1375 train_time:171863ms step_avg:147.52ms
step:1176/1375 train_time:172018ms step_avg:147.53ms
step:1177/1375 train_time:172182ms step_avg:147.54ms
step:1178/1375 train_time:172337ms step_avg:147.55ms
step:1179/1375 train_time:172494ms step_avg:147.56ms
step:1180/1375 train_time:172655ms step_avg:147.57ms
step:1181/1375 train_time:172810ms step_avg:147.57ms
step:1182/1375 train_time:172963ms step_avg:147.58ms
step:1183/1375 train_time:173117ms step_avg:147.58ms
step:1184/1375 train_time:173272ms step_avg:147.59ms
step:1185/1375 train_time:173429ms step_avg:147.60ms
step:1186/1375 train_time:173582ms step_avg:147.60ms
step:1187/1375 train_time:173746ms step_avg:147.62ms
step:1188/1375 train_time:173898ms step_avg:147.62ms
step:1189/1375 train_time:174055ms step_avg:147.63ms
step:1190/1375 train_time:174212ms step_avg:147.64ms
step:1191/1375 train_time:174369ms step_avg:147.65ms
step:1192/1375 train_time:174521ms step_avg:147.65ms
step:1193/1375 train_time:174677ms step_avg:147.66ms
step:1194/1375 train_time:174833ms step_avg:147.66ms
step:1195/1375 train_time:174989ms step_avg:147.67ms
step:1196/1375 train_time:175143ms step_avg:147.68ms
step:1197/1375 train_time:175299ms step_avg:147.68ms
step:1198/1375 train_time:175460ms step_avg:147.69ms
step:1199/1375 train_time:175614ms step_avg:147.70ms
step:1200/1375 train_time:175771ms step_avg:147.71ms
step:1201/1375 train_time:175925ms step_avg:147.71ms
step:1202/1375 train_time:176094ms step_avg:147.73ms
step:1203/1375 train_time:176252ms step_avg:147.74ms
step:1204/1375 train_time:176408ms step_avg:147.75ms
step:1205/1375 train_time:176562ms step_avg:147.75ms
step:1206/1375 train_time:176717ms step_avg:147.76ms
step:1207/1375 train_time:176873ms step_avg:147.76ms
step:1208/1375 train_time:177028ms step_avg:147.77ms
step:1209/1375 train_time:177182ms step_avg:147.77ms
step:1210/1375 train_time:177341ms step_avg:147.78ms
step:1211/1375 train_time:177496ms step_avg:147.79ms
step:1212/1375 train_time:177650ms step_avg:147.80ms
step:1213/1375 train_time:177806ms step_avg:147.80ms
step:1214/1375 train_time:177961ms step_avg:147.81ms
step:1215/1375 train_time:178117ms step_avg:147.81ms
step:1216/1375 train_time:178270ms step_avg:147.82ms
step:1217/1375 train_time:178425ms step_avg:147.83ms
step:1218/1375 train_time:178577ms step_avg:147.83ms
step:1219/1375 train_time:178731ms step_avg:147.83ms
step:1220/1375 train_time:178884ms step_avg:147.84ms
step:1221/1375 train_time:179036ms step_avg:147.84ms
step:1222/1375 train_time:179192ms step_avg:147.85ms
step:1223/1375 train_time:179348ms step_avg:147.86ms
step:1224/1375 train_time:179507ms step_avg:147.86ms
step:1225/1375 train_time:179663ms step_avg:147.87ms
step:1226/1375 train_time:179819ms step_avg:147.88ms
step:1227/1375 train_time:179979ms step_avg:147.89ms
step:1228/1375 train_time:180133ms step_avg:147.89ms
step:1229/1375 train_time:180288ms step_avg:147.90ms
step:1230/1375 train_time:180451ms step_avg:147.91ms
step:1231/1375 train_time:180608ms step_avg:147.92ms
step:1232/1375 train_time:180767ms step_avg:147.93ms
step:1233/1375 train_time:180924ms step_avg:147.93ms
step:1234/1375 train_time:181077ms step_avg:147.94ms
step:1235/1375 train_time:181233ms step_avg:147.95ms
step:1236/1375 train_time:181389ms step_avg:147.95ms
step:1237/1375 train_time:181545ms step_avg:147.96ms
step:1238/1375 train_time:181710ms step_avg:147.97ms
step:1239/1375 train_time:181866ms step_avg:147.98ms
step:1240/1375 train_time:182024ms step_avg:147.99ms
step:1241/1375 train_time:182182ms step_avg:148.00ms
step:1242/1375 train_time:182337ms step_avg:148.00ms
step:1243/1375 train_time:182496ms step_avg:148.01ms
step:1244/1375 train_time:182652ms step_avg:148.02ms
step:1245/1375 train_time:182809ms step_avg:148.02ms
step:1246/1375 train_time:182962ms step_avg:148.03ms
step:1247/1375 train_time:183118ms step_avg:148.03ms
step:1248/1375 train_time:183274ms step_avg:148.04ms
step:1249/1375 train_time:183425ms step_avg:148.04ms
step:1250/1375 train_time:183580ms step_avg:148.05ms
step:1250/1375 val_loss:3.3031 train_time:183661ms step_avg:148.11ms
step:1251/1375 train_time:183740ms step_avg:148.06ms
step:1252/1375 train_time:183893ms step_avg:148.06ms
step:1253/1375 train_time:184046ms step_avg:148.07ms
step:1254/1375 train_time:184199ms step_avg:148.07ms
step:1255/1375 train_time:184366ms step_avg:148.08ms
step:1256/1375 train_time:184521ms step_avg:148.09ms
step:1257/1375 train_time:184675ms step_avg:148.10ms
step:1258/1375 train_time:184833ms step_avg:148.10ms
step:1259/1375 train_time:184990ms step_avg:148.11ms
step:1260/1375 train_time:185144ms step_avg:148.12ms
step:1261/1375 train_time:185301ms step_avg:148.12ms
step:1262/1375 train_time:185461ms step_avg:148.13ms
step:1263/1375 train_time:185617ms step_avg:148.14ms
step:1264/1375 train_time:185770ms step_avg:148.14ms
step:1265/1375 train_time:185925ms step_avg:148.15ms
step:1266/1375 train_time:186081ms step_avg:148.15ms
step:1267/1375 train_time:186237ms step_avg:148.16ms
step:1268/1375 train_time:186395ms step_avg:148.17ms
step:1269/1375 train_time:186556ms step_avg:148.18ms
step:1270/1375 train_time:186711ms step_avg:148.18ms
step:1271/1375 train_time:186869ms step_avg:148.19ms
step:1272/1375 train_time:187022ms step_avg:148.20ms
step:1273/1375 train_time:187176ms step_avg:148.20ms
step:1274/1375 train_time:187330ms step_avg:148.20ms
step:1275/1375 train_time:187486ms step_avg:148.21ms
step:1276/1375 train_time:187639ms step_avg:148.21ms
step:1277/1375 train_time:187797ms step_avg:148.22ms
step:1278/1375 train_time:187951ms step_avg:148.23ms
step:1279/1375 train_time:188108ms step_avg:148.23ms
step:1280/1375 train_time:188271ms step_avg:148.25ms
step:1281/1375 train_time:188427ms step_avg:148.25ms
step:1282/1375 train_time:188581ms step_avg:148.26ms
step:1283/1375 train_time:188735ms step_avg:148.26ms
step:1284/1375 train_time:188896ms step_avg:148.27ms
step:1285/1375 train_time:189050ms step_avg:148.27ms
step:1286/1375 train_time:189208ms step_avg:148.28ms
step:1287/1375 train_time:189363ms step_avg:148.29ms
step:1288/1375 train_time:189520ms step_avg:148.29ms
step:1289/1375 train_time:189682ms step_avg:148.31ms
step:1290/1375 train_time:189840ms step_avg:148.31ms
step:1291/1375 train_time:189999ms step_avg:148.32ms
step:1292/1375 train_time:190155ms step_avg:148.33ms
step:1293/1375 train_time:190314ms step_avg:148.34ms
step:1294/1375 train_time:190469ms step_avg:148.34ms
step:1295/1375 train_time:190626ms step_avg:148.35ms
step:1296/1375 train_time:190784ms step_avg:148.35ms
step:1297/1375 train_time:190943ms step_avg:148.36ms
step:1298/1375 train_time:191100ms step_avg:148.37ms
step:1299/1375 train_time:191253ms step_avg:148.37ms
step:1300/1375 train_time:191408ms step_avg:148.38ms
step:1301/1375 train_time:191563ms step_avg:148.38ms
step:1302/1375 train_time:191723ms step_avg:148.39ms
step:1303/1375 train_time:191880ms step_avg:148.40ms
step:1304/1375 train_time:192039ms step_avg:148.41ms
step:1305/1375 train_time:192194ms step_avg:148.41ms
step:1306/1375 train_time:192351ms step_avg:148.42ms
step:1307/1375 train_time:192506ms step_avg:148.42ms
step:1308/1375 train_time:192663ms step_avg:148.43ms
step:1309/1375 train_time:192819ms step_avg:148.44ms
step:1310/1375 train_time:192971ms step_avg:148.44ms
step:1311/1375 train_time:193125ms step_avg:148.44ms
step:1312/1375 train_time:193278ms step_avg:148.45ms
step:1313/1375 train_time:193432ms step_avg:148.45ms
step:1314/1375 train_time:193588ms step_avg:148.46ms
step:1315/1375 train_time:193744ms step_avg:148.46ms
step:1316/1375 train_time:193897ms step_avg:148.47ms
step:1317/1375 train_time:194050ms step_avg:148.47ms
step:1318/1375 train_time:194211ms step_avg:148.48ms
step:1319/1375 train_time:194367ms step_avg:148.48ms
step:1320/1375 train_time:194525ms step_avg:148.49ms
step:1321/1375 train_time:194682ms step_avg:148.50ms
step:1322/1375 train_time:194843ms step_avg:148.51ms
step:1323/1375 train_time:194999ms step_avg:148.51ms
step:1324/1375 train_time:195152ms step_avg:148.52ms
step:1325/1375 train_time:195308ms step_avg:148.52ms
step:1326/1375 train_time:195471ms step_avg:148.53ms
step:1327/1375 train_time:195628ms step_avg:148.54ms
step:1328/1375 train_time:195783ms step_avg:148.55ms
step:1329/1375 train_time:195958ms step_avg:148.57ms
step:1330/1375 train_time:196117ms step_avg:148.57ms
step:1331/1375 train_time:196314ms step_avg:148.61ms
step:1332/1375 train_time:196477ms step_avg:148.62ms
step:1333/1375 train_time:196634ms step_avg:148.63ms
step:1334/1375 train_time:196790ms step_avg:148.63ms
step:1335/1375 train_time:196943ms step_avg:148.64ms
step:1336/1375 train_time:197108ms step_avg:148.65ms
step:1337/1375 train_time:197267ms step_avg:148.66ms
step:1338/1375 train_time:197424ms step_avg:148.66ms
step:1339/1375 train_time:197585ms step_avg:148.67ms
step:1340/1375 train_time:197744ms step_avg:148.68ms
step:1341/1375 train_time:197898ms step_avg:148.68ms
step:1342/1375 train_time:198056ms step_avg:148.69ms
step:1343/1375 train_time:198210ms step_avg:148.69ms
step:1344/1375 train_time:198365ms step_avg:148.70ms
step:1345/1375 train_time:198523ms step_avg:148.71ms
step:1346/1375 train_time:198680ms step_avg:148.71ms
step:1347/1375 train_time:198837ms step_avg:148.72ms
step:1348/1375 train_time:198992ms step_avg:148.72ms
step:1349/1375 train_time:199149ms step_avg:148.73ms
step:1350/1375 train_time:199305ms step_avg:148.74ms
step:1351/1375 train_time:199460ms step_avg:148.74ms
step:1352/1375 train_time:199625ms step_avg:148.75ms
step:1353/1375 train_time:199787ms step_avg:148.76ms
step:1354/1375 train_time:199945ms step_avg:148.77ms
step:1355/1375 train_time:200103ms step_avg:148.78ms
step:1356/1375 train_time:200258ms step_avg:148.78ms
step:1357/1375 train_time:200414ms step_avg:148.79ms
step:1358/1375 train_time:200573ms step_avg:148.79ms
step:1359/1375 train_time:200731ms step_avg:148.80ms
step:1360/1375 train_time:200892ms step_avg:148.81ms
step:1361/1375 train_time:201052ms step_avg:148.82ms
step:1362/1375 train_time:201211ms step_avg:148.82ms
step:1363/1375 train_time:201374ms step_avg:148.84ms
step:1364/1375 train_time:201529ms step_avg:148.84ms
step:1365/1375 train_time:201682ms step_avg:148.84ms
step:1366/1375 train_time:201839ms step_avg:148.85ms
step:1367/1375 train_time:201996ms step_avg:148.85ms
step:1368/1375 train_time:202152ms step_avg:148.86ms
step:1369/1375 train_time:202315ms step_avg:148.87ms
step:1370/1375 train_time:202474ms step_avg:148.88ms
step:1371/1375 train_time:202629ms step_avg:148.88ms
step:1372/1375 train_time:202792ms step_avg:148.89ms
step:1373/1375 train_time:202947ms step_avg:148.90ms
step:1374/1375 train_time:203106ms step_avg:148.90ms
step:1375/1375 train_time:203260ms step_avg:148.91ms
step:1375/1375 val_loss:3.2777 train_time:203337ms step_avg:148.96ms
peak memory consumption: 31565 MiB

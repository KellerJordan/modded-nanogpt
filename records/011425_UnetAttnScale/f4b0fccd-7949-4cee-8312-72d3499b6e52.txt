import os
import sys
with open(sys.argv[0]) as f:
    code = f.read() # read the code of this file ASAP, for logging
import uuid
import time
import glob
import subprocess
import contextlib
from dataclasses import dataclass

import torch
torch.empty(1, device='cuda', requires_grad=True).backward()
from torch import nn
import torch.nn.functional as F
import torch.distributed as dist
from torch.nn.parallel import DistributedDataParallel as DDP
# use of FlexAttention contributed by @KoszarskyB
from torch.nn.attention.flex_attention import BlockMask, flex_attention

# -----------------------------------------------------------------------------
# Muon optimizer

@torch.compile
def zeropower_via_newtonschulz5(G, steps):
    """
    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a
    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose
    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at
    zero even beyond the point where the iteration no longer converges all the way to one everywhere
    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T
    where S' is diagonal with S_{ii}' ~ Uniform(0.5, 1.5), which turns out not to hurt model
    performance at all relative to UV^T, where USV^T = G is the SVD.
    """
    assert len(G.shape) == 2
    a, b, c = (3.4445, -4.7750,  2.0315)
    X = G.bfloat16()
    if G.size(0) > G.size(1):
        X = X.T

    # Ensure spectral norm is at most 1
    X = X / (X.norm() + 1e-7)
    # Perform the NS iterations
    for _ in range(steps):
        A = X @ X.T
        B = b * A + c * A @ A # adapted from suggestion by @jxbz, @leloykun, and @YouJiacheng
        X = a * X + B @ X
    
    if G.size(0) > G.size(1):
        X = X.T
    return X

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Some warnings:
    - This optimizer assumes that all parameters passed in are 2D.
    - It should not be used for the embedding layer, the final fully connected layer, or any {0,1}-D
    parameters; those should all be optimized by a standard method (e.g., AdamW).
    - To use it with 4D convolutional filters, it works well to just flatten their last 3 dimensions.
    - We believe it is unlikely to work well for training with small batch size.
    - We believe it may not work well for finetuning pretrained models, but we haven't tested this.
    - We have not yet tried this optimizer for training scenarios larger than NanoGPT (124M).

    Arguments:
        lr: The learning rate used by the internal SGD.
        momentum: The momentum used by the internal SGD.
        nesterov: Whether to use Nesterov-style momentum in the internal SGD. (recommended)
        ns_steps: The number of Newton-Schulz iteration steps to use.
    """
    def __init__(self, params, lr=0.02, momentum=0.95, nesterov=True, ns_steps=5):
        self.world_size = int(os.environ['WORLD_SIZE'])
        self.rank = int(os.environ['RANK'])
        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, ns_steps=ns_steps)
        assert all(isinstance(p, torch.Tensor) for p in params)
        sizes = {p.numel() for p in params}
        param_groups = [dict(params=[p for p in params if p.numel() == size],
                             update_buffer=[torch.empty(size, device='cuda', dtype=torch.bfloat16) for _ in range(self.world_size)])
                        for size in sizes]
        super().__init__(param_groups, defaults)

    def step(self):

        for group in self.param_groups:

            lr = group['lr']
            momentum = group['momentum']
            nesterov = group['nesterov']
            ns_steps = group['ns_steps']
            update_buffers = group['update_buffer']
            # generate weight updates in distributed fashion
            params = group['params']
            handle = None
            params_world = None
            def update_prev():
                if params_world is None:
                    return
                assert handle is not None
                handle.wait()
                for p_world, g_world in zip(params_world, update_buffers):
                    p_world.data.add_(
                        g_world.view_as(p_world),
                        alpha=-lr * max(1, p_world.size(0) / p_world.size(1)) ** 0.5,
                    )
            for base_i in range(len(params))[::self.world_size]:
                if base_i + rank < len(params):
                    p = params[base_i + self.rank]
                    g = p.grad
                    assert g is not None
                    state = self.state[p]
                    if 'momentum_buffer' not in state:
                        state['momentum_buffer'] = torch.zeros_like(g)
                    buf = state['momentum_buffer']
                    buf.lerp_(g, 1 - momentum)
                    g = g.lerp_(buf, momentum) if nesterov else buf
                    g = zeropower_via_newtonschulz5(g, steps=ns_steps).flatten()
                else:
                    g = update_buffers[rank]
                update_prev() # async all_gather instead of sync all_reduce by @YouJiacheng
                handle = dist.all_gather(update_buffers, g, async_op=True)
                params_world = params[base_i : base_i + self.world_size]
            update_prev()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the GPT-2 model

def norm(x):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):

    def __init__(self, in_features, out_features):
        super().__init__(in_features, out_features, bias=False)

    def forward(self, x):
        return F.linear(x, self.weight.type_as(x))

class Rotary(nn.Module):

    def __init__(self, dim, max_seq_len=65536):
        super().__init__()
        # half-truncate RoPE by @YouJiacheng
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=dim//4, dtype=torch.float32)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(dim//4)])
        t = torch.arange(max_seq_len, dtype=torch.float32)
        theta = torch.einsum('i,j -> ij', t, angular_freq)
        self.cos = nn.Buffer(theta.cos(), persistent=False)
        self.sin = nn.Buffer(theta.sin(), persistent=False)

    def forward(self, x):
        cos, sin = self.cos[None, :x.size(-3), None, :], self.sin[None, :x.size(-3), None, :]
        x1, x2 = x.float().chunk(2, dim=-1)
        y1 = x1 * cos + x2 * sin
        y2 = x1 * (-sin) + x2 * cos
        return torch.cat((y1, y2), 3).type_as(x)

class CausalSelfAttention(nn.Module):

    def __init__(self, dim, num_heads, layer_id):
        super().__init__()
        assert dim % num_heads == 0
        self.num_heads = num_heads
        self.layer_id = layer_id
        self.c_q = CastedLinear(dim, dim)
        self.c_k = CastedLinear(dim, dim)
        self.c_v = CastedLinear(dim, dim)
        self.lambdas = nn.Parameter(torch.tensor([0.5, 0.5]))
        self.rotary = Rotary(dim // num_heads) # dim // num_heads = head_dim
        self.c_proj = CastedLinear(dim, dim)
        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977
        # self.attn_scale = nn.Parameter(torch.tensor(1.0 / (dim // num_heads) ** 0.5))
        # self.attn_scale = nn.Parameter(torch.tensor(0.20))
        self.attn_scale = 0.13 + 0.01 * min(layer_id, 11 - layer_id)  # unet pattern attention scale by @leloykun

    def forward(self, x, ve, block_mask):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, 'Must use batch size = 1 for FlexAttention'
        q = self.c_q(x).view(B, T, self.num_heads, -1)
        k = self.c_k(x).view(B, T, self.num_heads, -1)
        v = self.c_v(x).view(B, T, self.num_heads, -1)
        if ve is not None:
            v = self.lambdas[0] * v + self.lambdas[1] * ve.view_as(v) # @KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = self.lambdas[0] * v
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = self.rotary(q), self.rotary(k)
        # y = flex_attention(q.transpose(1, 2) * self.attn_scale, k.transpose(1, 2), v.transpose(1, 2), block_mask=block_mask, scale=1.)
        y = flex_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), block_mask=block_mask, scale=self.attn_scale)
        y = y.transpose(1, 2).contiguous().view_as(x) # re-assemble all head outputs side by side
        y = self.c_proj(y)
        return y

class MLP(nn.Module):

    def __init__(self, dim):
        super().__init__()
        self.c_fc = CastedLinear(dim, 4 * dim)
        self.c_proj = CastedLinear(4 * dim, dim)
        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977

    def forward(self, x):
        x = self.c_fc(x)
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = self.c_proj(x)
        return x

class Block(nn.Module):

    def __init__(self, model_dim, num_heads, layer_id, use_attn=True):
        super().__init__()
        self.attn = CausalSelfAttention(model_dim, num_heads, layer_id) if use_attn else None
        self.mlp = MLP(model_dim)
        self.lambdas = nn.Parameter(torch.tensor([1., 0.]))

    def forward(self, x, ve, x0, block_mask):
        x = self.lambdas[0] * x + self.lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), ve, block_mask)
        x = x + self.mlp(norm(x))
        return x

class ValueEmbedding(nn.Module):
    def __init__(self, vocab_size, model_dim):
        super().__init__()
        self.embed = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])

    def forward(self, inputs):
        ve = [emb(inputs).bfloat16() for emb in self.embed]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        ve = [ve[0], ve[1], ve[2], None, None, None, None, None, None, ve[0], ve[1], ve[2]]
        return ve

# -----------------------------------------------------------------------------
# The main GPT-2 model

class GPT(nn.Module):

    def __init__(self, vocab_size, num_layers, num_heads, model_dim):
        super().__init__()
        self.embed = nn.Embedding(vocab_size, model_dim)
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.blocks = nn.ModuleList([Block(model_dim, num_heads, layer_id=layer_id, use_attn=(layer_id != 7))
                                     for layer_id in range(num_layers)])
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual learning
        # U-net structure on token value embeddings by @leloykun
        self.value_embeds = ValueEmbedding(vocab_size, model_dim)
        self.lm_head = CastedLinear(model_dim, vocab_size)
        self.lm_head.weight.data.zero_() # @Grad62304977
        # U-net design by @brendanh0gan
        self.num_encoder_layers = num_layers // 2 # Half of the layers for encoder
        self.num_decoder_layers = num_layers - self.num_encoder_layers # Remaining for decoder
        # Add learnable skip connection weights for decoder layers
        self.skip_weights = nn.Parameter(torch.ones(self.num_decoder_layers))

    def forward(self, inputs, targets, sliding_window_num_blocks):
        BLOCK_SIZE = 128
        seq_len = len(inputs)
        assert seq_len % BLOCK_SIZE == 0
        total_num_blocks = seq_len // BLOCK_SIZE
        assert inputs.ndim == 1
        docs = (inputs == 50256).cumsum(0)
        docs_low = docs.view(-1, BLOCK_SIZE)[:, 0].contiguous()
        docs_high = docs.view(-1, BLOCK_SIZE)[:, -1].contiguous()

        def document_causal(b, h, q_idx, kv_idx):
            causal_mask = q_idx >= kv_idx
            document_mask = docs[q_idx] == docs[kv_idx]
            return causal_mask & document_mask

        def dense_to_ordered(dense_mask):
            num_blocks = dense_mask.sum(dim=-1, dtype=torch.int32)
            indices = dense_mask.argsort(dim=-1, descending=True, stable=True).to(torch.int32)
            return num_blocks[None, None].contiguous(), indices[None, None].contiguous()

        def create_doc_swc_block_mask(sliding_window_num_blocks):
            kv_idx = block_idx = torch.arange(total_num_blocks, dtype=torch.int32, device='cuda')
            q_idx = block_idx[:, None]
            causal_bm = q_idx >= kv_idx
            causal_full_bm = q_idx > kv_idx
            window_bm = q_idx - kv_idx < sliding_window_num_blocks
            window_full_bm = window_bm # block-wise sliding window by @YouJiacheng
            # document_bm = (docs_low[q_idx] <= docs_high[kv_idx]) & (docs_low[kv_idx] <= docs_high[q_idx])
            document_bm = (docs_low[:, None] <= docs_high) & (docs_low <= docs_high[:, None])
            document_full_bm = (docs_low[:, None] == docs_high) & (docs_low == docs_high[:, None])
            nonzero_bm = causal_bm & window_bm & document_bm
            full_bm  = causal_full_bm & window_full_bm & document_full_bm
            kv_num_blocks, kv_indices = dense_to_ordered(nonzero_bm & ~full_bm)
            full_kv_num_blocks, full_kv_indices = dense_to_ordered(full_bm)
            return BlockMask.from_kv_blocks(
                kv_num_blocks,
                kv_indices,
                full_kv_num_blocks,
                full_kv_indices,
                BLOCK_SIZE=BLOCK_SIZE,
                mask_mod=document_causal,
            )

        block_mask = create_doc_swc_block_mask(sliding_window_num_blocks)

        x0 = norm(self.embed(inputs[None]).bfloat16()) # use of norm here by @Grad62304977
        x = x0
        ve = self.value_embeds(inputs)
        assert len(ve) == len(self.blocks)
        ve_enc, ve_dec = ve[:self.num_encoder_layers], ve[self.num_encoder_layers:]

        # Store outputs for U-Net skip connections
        skip_connections = []
        # Encoder pass - process only the first half of the blocks
        for i in range(self.num_encoder_layers):
            x = self.blocks[i](x, ve_enc[i], x0, block_mask)
            skip_connections.append(x)
        # Decoder pass - process the remaining blocks with weighted skip connections
        for i in range(self.num_decoder_layers):
            x = x + self.skip_weights[i] * skip_connections.pop()
            # U-net structure on token value embeddings by @leloykun
            x = self.blocks[self.num_encoder_layers + i](x, ve_dec[i], x0, block_mask)

        x = norm(x)
        logits = self.lm_head(x)
        logits = 15 * torch.tanh(logits / 15) # @Grad62304977 added tanh softcapping, @KoszarskyB reduced it from 30 to 15
        logits = logits.float()
        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets)
        return loss

# -----------------------------------------------------------------------------
# Our own simple Distributed Data Loader

def _load_data_shard(path):
    # only reads the header, returns header data
    # header is 256 int32
    header = torch.from_file(path, False, 256, dtype=torch.int32)
    assert header[0] == 20240520, 'magic number mismatch in the data .bin file'
    assert header[1] == 1, 'unsupported version'
    num_tokens = int(header[2]) # number of tokens (claimed)
    with open(path, 'rb', buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, 'number of tokens read does not match header'
    return tokens

class DistributedDataLoader:

    def __init__(self, filename_pattern):
        self.rank = int(os.environ['RANK'])
        self.world_size = int(os.environ['WORLD_SIZE'])
        self.files = sorted(glob.glob(filename_pattern))
        self.reset()

    def reset(self):
        self.current_shard = -1
        self.advance()

    def advance(self):
        self.current_shard = (self.current_shard + 1) % len(self.files)
        self.current_position = 0
        self.tokens = _load_data_shard(self.files[self.current_shard])

    def next_batch(self, batch_size):
        assert batch_size % self.world_size == 0
        device_batch_size = batch_size // self.world_size
        # load next shard if necessary
        if self.current_position + batch_size + 1 >= len(self.tokens):
            self.advance()
        pos = self.current_position + self.rank * device_batch_size
        device_batch_tokens = self.tokens[pos:pos+device_batch_size+1]
        # advance current position
        self.current_position += batch_size
        inputs = device_batch_tokens[:-1].to(device='cuda', dtype=torch.int32, non_blocking=True)
        targets = device_batch_tokens[1:].to(device='cuda', dtype=torch.int64, non_blocking=True)
        return inputs, targets

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files = 'data/fineweb10B/fineweb_train_*.bin' # input .bin to train on
    val_files = 'data/fineweb10B/fineweb_val_*.bin' # input .bin to eval validation loss on
    val_tokens = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # optimization
    batch_size = 8*64*1024 # batch size in tokens
    num_iterations = 1375 # number of iterations to run
    cooldown_frac = 0.4 # fraction of training spent cooling down the learning rate
    bf16_embeds = True
    # evaluation and logging
    val_loss_every = 125 # every how many steps to evaluate val loss? 0 for only at the end
    # implementation
    max_device_batch_size = 64*1024 # batch size per device in tokens
    save_checkpoint = False
args = Hyperparameters()

micro_bs = args.max_device_batch_size

# set up DDP (distributed data parallel). torchrun sets this env variable
rank = int(os.environ['RANK'])
local_rank = int(os.environ['LOCAL_RANK'])
world_size = int(os.environ['WORLD_SIZE'])
assert torch.cuda.is_available()
torch.cuda.set_device(local_rank)
dist.init_process_group(backend='nccl', device_id=torch.device(local_rank))
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = uuid.uuid4()
    os.makedirs('logs', exist_ok=True)
    logfile = f'logs/{run_id}.txt'
    print(logfile)

def print0(s, console=False):
    if master_process:
        with open(logfile, 'a') as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0('='*100)
# log information about the hardware/software environment this is running on
print0(f'Running Python {sys.version}')
print0(f'Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}')
print0(subprocess.run(['nvidia-smi'], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout)
print0('='*100)

# load data
train_loader = DistributedDataLoader(args.train_files)
val_loader = DistributedDataLoader(args.val_files)
print0(f'Training dataloader files: {train_loader.files}')
print0(f'Validation dataloader files: {val_loader.files}')
print0('='*100)

# there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency. suggested to me by @Grad62304977.
# this originates from Karpathy's experiments.
model = GPT(vocab_size=50304, num_layers=12, num_heads=6, model_dim=768)
model = model.cuda()
if args.bf16_embeds:
    for m in model.modules():
        if isinstance(m, nn.Embedding):
            m.bfloat16()
model: GPT = torch.compile(model)
ddp_model = DDP(model, device_ids=[local_rank], broadcast_buffers=False, gradient_as_bucket_view=True)

# collect the parameters to optimize
hidden_matrix_params = [p for p in model.blocks.parameters() if p.ndim == 2]
embed_params = [model.embed.weight, *model.value_embeds.parameters()]
scalar_params = [p for n, p in model.named_parameters() if p.ndim < 2 and "attn_scale" not in n]
attn_scale_params = [p for n, p in model.named_parameters() if p.ndim < 2 and "attn_scale" in n]
head_params = [model.lm_head.weight]

# init the optimizer(s)
optimizer1 = torch.optim.Adam([dict(params=embed_params, lr=0.6),
                               dict(params=head_params, lr=0.008),
                               dict(params=scalar_params, lr=0.04),
                               dict(params=attn_scale_params, lr=0.01)],
                              betas=(0.8, 0.95), fused=True)
optimizer2 = Muon(hidden_matrix_params, lr=0.05, momentum=0.95)
optimizers = [optimizer1, optimizer2]

# learning rate schedule: stable then decay
def get_lr(it):
    t = 1 - it / args.num_iterations # time remaining in training
    assert 1 >= t > 0
    # 1) constant lr for first part of training
    if t >= args.cooldown_frac:
        return 1.0
    # 2) then linear cooldown
    else:
        return t / args.cooldown_frac
schedulers = [torch.optim.lr_scheduler.LambdaLR(opt, get_lr) for opt in optimizers]

# sliding window size schedule: linear increase over training in chunks of 128 from 128 -> 1792. By @fernbear.bsky.social
def get_sliding_window_blocks(it):
    x = it / args.num_iterations # training progress
    assert 0 <= x <= 1
    return int(((1 - x) * 128 + x * 1856) // 128)
sliding_window_num_blocks = torch.tensor(1, dtype=torch.int32, device='cuda')

# Start training loop
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    # This effectively ignores timing first 10 steps, which are slower for weird reasons.
    # Alternately, and slightly more correctly in terms of benchmarking, we could do 10
    # steps with dummy data first, and then re-initialize the model and reset the loader.
    if step == 10:
        training_time_ms = 0
        t0 = time.perf_counter()
    timed_steps = float('nan') if step <= 11 else (step - 10) + 1 # <= 11 to avoid bug in val

    sliding_window_num_blocks.copy_(get_sliding_window_blocks(step))

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        # run validation batches
        model.eval()
        val_loader.reset()
        val_loss = 0.0
        # calculate the number of steps to take in the val loop.
        val_batch_size = world_size * micro_bs
        assert args.val_tokens % val_batch_size == 0
        val_steps = args.val_tokens // val_batch_size
        for _ in range(val_steps):
            with torch.no_grad():
                inputs_val, targets_val = val_loader.next_batch(val_batch_size)
                val_loss += ddp_model(inputs_val, targets_val, sliding_window_num_blocks)
        # Print attention scales
        for n, p in model.named_parameters():
            if p.ndim < 2 and "attn_scale" in n:
                print0(f'{n}: {p.item()}', console=True)
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        val_loss /= val_steps
        # logging
        print0(f'step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/(timed_steps-1):.2f}ms', console=True)
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f'logs/{run_id}', exist_ok=True)
            torch.save(log, f'logs/{run_id}/state_step{step:06d}.pt')
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    model.train()
    batch_size = args.batch_size
    assert batch_size % world_size == 0
    inputs_train, targets_train = train_loader.next_batch(batch_size)
    assert len(inputs_train) <= micro_bs or len(inputs_train) % micro_bs == 0
    for micro_inputs_train, micro_targets_train in zip(inputs_train.split(micro_bs), targets_train.split(micro_bs)):
        ddp_model(micro_inputs_train, micro_targets_train, sliding_window_num_blocks).backward()
    # momentum warmup for Muon
    frac = min(step/300, 1)
    for group in optimizer2.param_groups:
        group['momentum'] = (1 - frac) * 0.85 + frac * 0.95
    # step the optimizers and schedulers
    for opt, sched in zip(optimizers, schedulers):
        opt.step()
        if step != train_steps-1:
            sched.step()
    # null the gradients
    model.zero_grad(set_to_none=True)
    # logging
    approx_time = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f'step:{step+1}/{train_steps} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms', console=True)

print0(f'peak memory consumption: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB')
dist.destroy_process_group()

====================================================================================================
Running Python 3.10.12 (main, Sep 11 2024, 15:47:36) [GCC 11.4.0]
Running PyTorch 2.6.0.dev20241231+cu126 compiled for CUDA 12.6
Tue Jan 14 16:49:20 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.183.06             Driver Version: 535.183.06   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:19:00.0 Off |                    0 |
| N/A   39C    P0             126W / 700W |   7713MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:3B:00.0 Off |                    0 |
| N/A   32C    P0             114W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:4C:00.0 Off |                    0 |
| N/A   29C    P0             117W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   38C    P0             130W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:9B:00.0 Off |                    0 |
| N/A   39C    P0             123W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:BB:00.0 Off |                    0 |
| N/A   30C    P0             114W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:CB:00.0 Off |                    0 |
| N/A   37C    P0             116W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:DB:00.0 Off |                    0 |
| N/A   30C    P0             117W / 700W |   3211MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
Training dataloader files: ['data/fineweb10B/fineweb_train_000001.bin', 'data/fineweb10B/fineweb_train_000002.bin', 'data/fineweb10B/fineweb_train_000003.bin', 'data/fineweb10B/fineweb_train_000004.bin', 'data/fineweb10B/fineweb_train_000005.bin', 'data/fineweb10B/fineweb_train_000006.bin', 'data/fineweb10B/fineweb_train_000007.bin', 'data/fineweb10B/fineweb_train_000008.bin', 'data/fineweb10B/fineweb_train_000009.bin']
Validation dataloader files: ['data/fineweb10B/fineweb_val_000000.bin']
====================================================================================================
step:0/1375 val_loss:10.8258 train_time:0ms step_avg:nanms
step:1/1375 train_time:27457ms step_avg:nanms
step:2/1375 train_time:27532ms step_avg:nanms
step:3/1375 train_time:27711ms step_avg:nanms
step:4/1375 train_time:27843ms step_avg:nanms
step:5/1375 train_time:27978ms step_avg:nanms
step:6/1375 train_time:28112ms step_avg:nanms
step:7/1375 train_time:28246ms step_avg:nanms
step:8/1375 train_time:28379ms step_avg:nanms
step:9/1375 train_time:28514ms step_avg:nanms
step:10/1375 train_time:28652ms step_avg:nanms
step:11/1375 train_time:137ms step_avg:nanms
step:12/1375 train_time:273ms step_avg:nanms
step:13/1375 train_time:409ms step_avg:136.19ms
step:14/1375 train_time:545ms step_avg:136.15ms
step:15/1375 train_time:679ms step_avg:135.72ms
step:16/1375 train_time:813ms step_avg:135.48ms
step:17/1375 train_time:949ms step_avg:135.60ms
step:18/1375 train_time:1085ms step_avg:135.60ms
step:19/1375 train_time:1221ms step_avg:135.70ms
step:20/1375 train_time:1358ms step_avg:135.77ms
step:21/1375 train_time:1491ms step_avg:135.51ms
step:22/1375 train_time:1626ms step_avg:135.52ms
step:23/1375 train_time:1761ms step_avg:135.46ms
step:24/1375 train_time:1896ms step_avg:135.46ms
step:25/1375 train_time:2032ms step_avg:135.46ms
step:26/1375 train_time:2169ms step_avg:135.54ms
step:27/1375 train_time:2303ms step_avg:135.49ms
step:28/1375 train_time:2440ms step_avg:135.57ms
step:29/1375 train_time:2573ms step_avg:135.44ms
step:30/1375 train_time:2709ms step_avg:135.45ms
step:31/1375 train_time:2845ms step_avg:135.47ms
step:32/1375 train_time:2981ms step_avg:135.48ms
step:33/1375 train_time:3116ms step_avg:135.46ms
step:34/1375 train_time:3253ms step_avg:135.56ms
step:35/1375 train_time:3388ms step_avg:135.53ms
step:36/1375 train_time:3524ms step_avg:135.55ms
step:37/1375 train_time:3659ms step_avg:135.53ms
step:38/1375 train_time:3794ms step_avg:135.49ms
step:39/1375 train_time:3930ms step_avg:135.50ms
step:40/1375 train_time:4064ms step_avg:135.48ms
step:41/1375 train_time:4199ms step_avg:135.45ms
step:42/1375 train_time:4335ms step_avg:135.48ms
step:43/1375 train_time:4470ms step_avg:135.46ms
step:44/1375 train_time:4605ms step_avg:135.44ms
step:45/1375 train_time:4741ms step_avg:135.46ms
step:46/1375 train_time:4874ms step_avg:135.38ms
step:47/1375 train_time:5010ms step_avg:135.41ms
step:48/1375 train_time:5147ms step_avg:135.45ms
step:49/1375 train_time:5282ms step_avg:135.43ms
step:50/1375 train_time:5418ms step_avg:135.46ms
step:51/1375 train_time:5555ms step_avg:135.50ms
step:52/1375 train_time:5689ms step_avg:135.45ms
step:53/1375 train_time:5825ms step_avg:135.47ms
step:54/1375 train_time:5960ms step_avg:135.45ms
step:55/1375 train_time:6094ms step_avg:135.43ms
step:56/1375 train_time:6231ms step_avg:135.45ms
step:57/1375 train_time:6367ms step_avg:135.47ms
step:58/1375 train_time:6503ms step_avg:135.48ms
step:59/1375 train_time:6638ms step_avg:135.47ms
step:60/1375 train_time:6772ms step_avg:135.44ms
step:61/1375 train_time:6908ms step_avg:135.45ms
step:62/1375 train_time:7044ms step_avg:135.46ms
step:63/1375 train_time:7179ms step_avg:135.46ms
step:64/1375 train_time:7315ms step_avg:135.46ms
step:65/1375 train_time:7451ms step_avg:135.47ms
step:66/1375 train_time:7587ms step_avg:135.48ms
step:67/1375 train_time:7722ms step_avg:135.47ms
step:68/1375 train_time:7858ms step_avg:135.48ms
step:69/1375 train_time:7993ms step_avg:135.47ms
step:70/1375 train_time:8128ms step_avg:135.47ms
step:71/1375 train_time:8264ms step_avg:135.47ms
step:72/1375 train_time:8400ms step_avg:135.48ms
step:73/1375 train_time:8537ms step_avg:135.51ms
step:74/1375 train_time:8671ms step_avg:135.48ms
step:75/1375 train_time:8807ms step_avg:135.50ms
step:76/1375 train_time:8945ms step_avg:135.53ms
step:77/1375 train_time:9079ms step_avg:135.50ms
step:78/1375 train_time:9213ms step_avg:135.49ms
step:79/1375 train_time:9349ms step_avg:135.49ms
step:80/1375 train_time:9484ms step_avg:135.49ms
step:81/1375 train_time:9621ms step_avg:135.51ms
step:82/1375 train_time:9757ms step_avg:135.52ms
step:83/1375 train_time:9891ms step_avg:135.50ms
step:84/1375 train_time:10027ms step_avg:135.51ms
step:85/1375 train_time:10163ms step_avg:135.51ms
step:86/1375 train_time:10300ms step_avg:135.52ms
step:87/1375 train_time:10435ms step_avg:135.53ms
step:88/1375 train_time:10571ms step_avg:135.53ms
step:89/1375 train_time:10708ms step_avg:135.55ms
step:90/1375 train_time:10845ms step_avg:135.57ms
step:91/1375 train_time:10980ms step_avg:135.56ms
step:92/1375 train_time:11116ms step_avg:135.56ms
step:93/1375 train_time:11251ms step_avg:135.55ms
step:94/1375 train_time:11388ms step_avg:135.57ms
step:95/1375 train_time:11525ms step_avg:135.59ms
step:96/1375 train_time:11661ms step_avg:135.59ms
step:97/1375 train_time:11796ms step_avg:135.59ms
step:98/1375 train_time:11931ms step_avg:135.59ms
step:99/1375 train_time:12068ms step_avg:135.60ms
step:100/1375 train_time:12204ms step_avg:135.59ms
step:101/1375 train_time:12340ms step_avg:135.60ms
step:102/1375 train_time:12476ms step_avg:135.61ms
step:103/1375 train_time:12614ms step_avg:135.64ms
step:104/1375 train_time:12757ms step_avg:135.71ms
step:105/1375 train_time:12896ms step_avg:135.75ms
step:106/1375 train_time:13036ms step_avg:135.80ms
step:107/1375 train_time:13175ms step_avg:135.82ms
step:108/1375 train_time:13313ms step_avg:135.84ms
step:109/1375 train_time:13452ms step_avg:135.88ms
step:110/1375 train_time:13590ms step_avg:135.90ms
step:111/1375 train_time:13729ms step_avg:135.93ms
step:112/1375 train_time:13870ms step_avg:135.98ms
step:113/1375 train_time:14010ms step_avg:136.02ms
step:114/1375 train_time:14151ms step_avg:136.07ms
step:115/1375 train_time:14289ms step_avg:136.09ms
step:116/1375 train_time:14429ms step_avg:136.12ms
step:117/1375 train_time:14568ms step_avg:136.15ms
step:118/1375 train_time:14707ms step_avg:136.17ms
step:119/1375 train_time:14846ms step_avg:136.20ms
step:120/1375 train_time:14984ms step_avg:136.22ms
step:121/1375 train_time:15125ms step_avg:136.26ms
step:122/1375 train_time:15265ms step_avg:136.30ms
step:123/1375 train_time:15404ms step_avg:136.32ms
step:124/1375 train_time:15544ms step_avg:136.35ms
step:125/1375 train_time:15683ms step_avg:136.37ms
step:125/1375 val_loss:4.3566 train_time:15751ms step_avg:136.97ms
step:126/1375 train_time:15824ms step_avg:136.41ms
step:127/1375 train_time:15969ms step_avg:136.49ms
step:128/1375 train_time:16110ms step_avg:136.52ms
step:129/1375 train_time:16246ms step_avg:136.52ms
step:130/1375 train_time:16384ms step_avg:136.54ms
step:131/1375 train_time:16522ms step_avg:136.55ms
step:132/1375 train_time:16661ms step_avg:136.56ms
step:133/1375 train_time:16802ms step_avg:136.60ms
step:134/1375 train_time:16945ms step_avg:136.66ms
step:135/1375 train_time:17086ms step_avg:136.69ms
step:136/1375 train_time:17224ms step_avg:136.70ms
step:137/1375 train_time:17363ms step_avg:136.72ms
step:138/1375 train_time:17501ms step_avg:136.72ms
step:139/1375 train_time:17640ms step_avg:136.74ms
step:140/1375 train_time:17780ms step_avg:136.77ms
step:141/1375 train_time:17919ms step_avg:136.79ms
step:142/1375 train_time:18059ms step_avg:136.81ms
step:143/1375 train_time:18199ms step_avg:136.84ms
step:144/1375 train_time:18339ms step_avg:136.86ms
step:145/1375 train_time:18477ms step_avg:136.87ms
step:146/1375 train_time:18616ms step_avg:136.88ms
step:147/1375 train_time:18755ms step_avg:136.90ms
step:148/1375 train_time:18896ms step_avg:136.93ms
step:149/1375 train_time:19034ms step_avg:136.93ms
step:150/1375 train_time:19174ms step_avg:136.96ms
step:151/1375 train_time:19313ms step_avg:136.97ms
step:152/1375 train_time:19453ms step_avg:136.99ms
step:153/1375 train_time:19592ms step_avg:137.01ms
step:154/1375 train_time:19733ms step_avg:137.03ms
step:155/1375 train_time:19873ms step_avg:137.05ms
step:156/1375 train_time:20012ms step_avg:137.07ms
step:157/1375 train_time:20152ms step_avg:137.09ms
step:158/1375 train_time:20290ms step_avg:137.10ms
step:159/1375 train_time:20429ms step_avg:137.11ms
step:160/1375 train_time:20570ms step_avg:137.13ms
step:161/1375 train_time:20710ms step_avg:137.15ms
step:162/1375 train_time:20849ms step_avg:137.17ms
step:163/1375 train_time:20989ms step_avg:137.18ms
step:164/1375 train_time:21127ms step_avg:137.19ms
step:165/1375 train_time:21267ms step_avg:137.21ms
step:166/1375 train_time:21406ms step_avg:137.22ms
step:167/1375 train_time:21545ms step_avg:137.23ms
step:168/1375 train_time:21686ms step_avg:137.25ms
step:169/1375 train_time:21825ms step_avg:137.26ms
step:170/1375 train_time:21965ms step_avg:137.28ms
step:171/1375 train_time:22105ms step_avg:137.30ms
step:172/1375 train_time:22245ms step_avg:137.31ms
step:173/1375 train_time:22385ms step_avg:137.33ms
step:174/1375 train_time:22524ms step_avg:137.34ms
step:175/1375 train_time:22665ms step_avg:137.36ms
step:176/1375 train_time:22805ms step_avg:137.38ms
step:177/1375 train_time:22945ms step_avg:137.40ms
step:178/1375 train_time:23086ms step_avg:137.41ms
step:179/1375 train_time:23224ms step_avg:137.42ms
step:180/1375 train_time:23365ms step_avg:137.44ms
step:181/1375 train_time:23504ms step_avg:137.45ms
step:182/1375 train_time:23645ms step_avg:137.47ms
step:183/1375 train_time:23786ms step_avg:137.49ms
step:184/1375 train_time:23924ms step_avg:137.50ms
step:185/1375 train_time:24066ms step_avg:137.52ms
step:186/1375 train_time:24205ms step_avg:137.53ms
step:187/1375 train_time:24345ms step_avg:137.54ms
step:188/1375 train_time:24486ms step_avg:137.56ms
step:189/1375 train_time:24625ms step_avg:137.57ms
step:190/1375 train_time:24765ms step_avg:137.58ms
step:191/1375 train_time:24939ms step_avg:137.79ms
step:192/1375 train_time:25078ms step_avg:137.79ms
step:193/1375 train_time:25215ms step_avg:137.79ms
step:194/1375 train_time:25353ms step_avg:137.79ms
step:195/1375 train_time:25493ms step_avg:137.80ms
step:196/1375 train_time:25630ms step_avg:137.80ms
step:197/1375 train_time:25772ms step_avg:137.82ms
step:198/1375 train_time:25916ms step_avg:137.85ms
step:199/1375 train_time:26056ms step_avg:137.86ms
step:200/1375 train_time:26195ms step_avg:137.87ms
step:201/1375 train_time:26333ms step_avg:137.87ms
step:202/1375 train_time:26472ms step_avg:137.87ms
step:203/1375 train_time:26610ms step_avg:137.88ms
step:204/1375 train_time:26750ms step_avg:137.89ms
step:205/1375 train_time:26893ms step_avg:137.91ms
step:206/1375 train_time:27038ms step_avg:137.95ms
step:207/1375 train_time:27181ms step_avg:137.98ms
step:208/1375 train_time:27323ms step_avg:138.00ms
step:209/1375 train_time:27466ms step_avg:138.02ms
step:210/1375 train_time:27606ms step_avg:138.03ms
step:211/1375 train_time:27747ms step_avg:138.05ms
step:212/1375 train_time:27891ms step_avg:138.08ms
step:213/1375 train_time:28034ms step_avg:138.10ms
step:214/1375 train_time:28176ms step_avg:138.12ms
step:215/1375 train_time:28318ms step_avg:138.14ms
step:216/1375 train_time:28460ms step_avg:138.15ms
step:217/1375 train_time:28600ms step_avg:138.17ms
step:218/1375 train_time:28743ms step_avg:138.19ms
step:219/1375 train_time:28886ms step_avg:138.21ms
step:220/1375 train_time:29028ms step_avg:138.23ms
step:221/1375 train_time:29171ms step_avg:138.25ms
step:222/1375 train_time:29313ms step_avg:138.27ms
step:223/1375 train_time:29455ms step_avg:138.29ms
step:224/1375 train_time:29597ms step_avg:138.31ms
step:225/1375 train_time:29738ms step_avg:138.32ms
step:226/1375 train_time:29881ms step_avg:138.34ms
step:227/1375 train_time:30021ms step_avg:138.34ms
step:228/1375 train_time:30163ms step_avg:138.36ms
step:229/1375 train_time:30306ms step_avg:138.38ms
step:230/1375 train_time:30448ms step_avg:138.40ms
step:231/1375 train_time:30590ms step_avg:138.42ms
step:232/1375 train_time:30731ms step_avg:138.43ms
step:233/1375 train_time:30874ms step_avg:138.45ms
step:234/1375 train_time:31014ms step_avg:138.45ms
step:235/1375 train_time:31155ms step_avg:138.47ms
step:236/1375 train_time:31295ms step_avg:138.48ms
step:237/1375 train_time:31436ms step_avg:138.49ms
step:238/1375 train_time:31578ms step_avg:138.50ms
step:239/1375 train_time:31718ms step_avg:138.51ms
step:240/1375 train_time:31861ms step_avg:138.52ms
step:241/1375 train_time:32001ms step_avg:138.53ms
step:242/1375 train_time:32145ms step_avg:138.56ms
step:243/1375 train_time:32288ms step_avg:138.57ms
step:244/1375 train_time:32429ms step_avg:138.59ms
step:245/1375 train_time:32570ms step_avg:138.60ms
step:246/1375 train_time:32712ms step_avg:138.61ms
step:247/1375 train_time:32854ms step_avg:138.62ms
step:248/1375 train_time:32994ms step_avg:138.63ms
step:249/1375 train_time:33135ms step_avg:138.64ms
step:250/1375 train_time:33278ms step_avg:138.66ms
step:250/1375 val_loss:3.9500 train_time:33347ms step_avg:138.95ms
step:251/1375 train_time:33421ms step_avg:138.68ms
step:252/1375 train_time:33568ms step_avg:138.71ms
step:253/1375 train_time:33710ms step_avg:138.72ms
step:254/1375 train_time:33850ms step_avg:138.73ms
step:255/1375 train_time:33992ms step_avg:138.74ms
step:256/1375 train_time:34132ms step_avg:138.75ms
step:257/1375 train_time:34273ms step_avg:138.76ms
step:258/1375 train_time:34417ms step_avg:138.78ms
step:259/1375 train_time:34562ms step_avg:138.80ms
step:260/1375 train_time:34704ms step_avg:138.82ms
step:261/1375 train_time:34844ms step_avg:138.82ms
step:262/1375 train_time:34986ms step_avg:138.83ms
step:263/1375 train_time:35126ms step_avg:138.84ms
step:264/1375 train_time:35266ms step_avg:138.84ms
step:265/1375 train_time:35409ms step_avg:138.86ms
step:266/1375 train_time:35553ms step_avg:138.88ms
step:267/1375 train_time:35694ms step_avg:138.89ms
step:268/1375 train_time:35836ms step_avg:138.90ms
step:269/1375 train_time:35977ms step_avg:138.91ms
step:270/1375 train_time:36119ms step_avg:138.92ms
step:271/1375 train_time:36260ms step_avg:138.93ms
step:272/1375 train_time:36402ms step_avg:138.94ms
step:273/1375 train_time:36543ms step_avg:138.95ms
step:274/1375 train_time:36685ms step_avg:138.96ms
step:275/1375 train_time:36828ms step_avg:138.97ms
step:276/1375 train_time:36969ms step_avg:138.98ms
step:277/1375 train_time:37110ms step_avg:138.99ms
step:278/1375 train_time:37250ms step_avg:138.99ms
step:279/1375 train_time:37392ms step_avg:139.01ms
step:280/1375 train_time:37534ms step_avg:139.01ms
step:281/1375 train_time:37677ms step_avg:139.03ms
step:282/1375 train_time:37819ms step_avg:139.04ms
step:283/1375 train_time:37961ms step_avg:139.05ms
step:284/1375 train_time:38102ms step_avg:139.06ms
step:285/1375 train_time:38244ms step_avg:139.07ms
step:286/1375 train_time:38388ms step_avg:139.09ms
step:287/1375 train_time:38529ms step_avg:139.10ms
step:288/1375 train_time:38670ms step_avg:139.10ms
step:289/1375 train_time:38812ms step_avg:139.11ms
step:290/1375 train_time:38954ms step_avg:139.12ms
step:291/1375 train_time:39096ms step_avg:139.13ms
step:292/1375 train_time:39238ms step_avg:139.14ms
step:293/1375 train_time:39381ms step_avg:139.16ms
step:294/1375 train_time:39524ms step_avg:139.17ms
step:295/1375 train_time:39667ms step_avg:139.18ms
step:296/1375 train_time:39810ms step_avg:139.20ms
step:297/1375 train_time:39954ms step_avg:139.21ms
step:298/1375 train_time:40096ms step_avg:139.22ms
step:299/1375 train_time:40238ms step_avg:139.23ms
step:300/1375 train_time:40381ms step_avg:139.24ms
step:301/1375 train_time:40522ms step_avg:139.25ms
step:302/1375 train_time:40665ms step_avg:139.26ms
step:303/1375 train_time:40808ms step_avg:139.28ms
step:304/1375 train_time:40949ms step_avg:139.28ms
step:305/1375 train_time:41092ms step_avg:139.30ms
step:306/1375 train_time:41234ms step_avg:139.30ms
step:307/1375 train_time:41376ms step_avg:139.31ms
step:308/1375 train_time:41519ms step_avg:139.33ms
step:309/1375 train_time:41664ms step_avg:139.34ms
step:310/1375 train_time:41808ms step_avg:139.36ms
step:311/1375 train_time:41953ms step_avg:139.38ms
step:312/1375 train_time:42096ms step_avg:139.39ms
step:313/1375 train_time:42239ms step_avg:139.40ms
step:314/1375 train_time:42385ms step_avg:139.42ms
step:315/1375 train_time:42529ms step_avg:139.44ms
step:316/1375 train_time:42672ms step_avg:139.45ms
step:317/1375 train_time:42819ms step_avg:139.48ms
step:318/1375 train_time:42964ms step_avg:139.49ms
step:319/1375 train_time:43108ms step_avg:139.51ms
step:320/1375 train_time:43251ms step_avg:139.52ms
step:321/1375 train_time:43397ms step_avg:139.54ms
step:322/1375 train_time:43542ms step_avg:139.56ms
step:323/1375 train_time:43687ms step_avg:139.58ms
step:324/1375 train_time:43831ms step_avg:139.59ms
step:325/1375 train_time:43974ms step_avg:139.60ms
step:326/1375 train_time:44119ms step_avg:139.62ms
step:327/1375 train_time:44264ms step_avg:139.63ms
step:328/1375 train_time:44408ms step_avg:139.65ms
step:329/1375 train_time:44550ms step_avg:139.66ms
step:330/1375 train_time:44696ms step_avg:139.68ms
step:331/1375 train_time:44840ms step_avg:139.69ms
step:332/1375 train_time:44986ms step_avg:139.71ms
step:333/1375 train_time:45129ms step_avg:139.72ms
step:334/1375 train_time:45272ms step_avg:139.73ms
step:335/1375 train_time:45416ms step_avg:139.74ms
step:336/1375 train_time:45561ms step_avg:139.76ms
step:337/1375 train_time:45707ms step_avg:139.78ms
step:338/1375 train_time:45849ms step_avg:139.78ms
step:339/1375 train_time:45995ms step_avg:139.80ms
step:340/1375 train_time:46139ms step_avg:139.81ms
step:341/1375 train_time:46283ms step_avg:139.83ms
step:342/1375 train_time:46428ms step_avg:139.84ms
step:343/1375 train_time:46570ms step_avg:139.85ms
step:344/1375 train_time:46714ms step_avg:139.86ms
step:345/1375 train_time:46857ms step_avg:139.87ms
step:346/1375 train_time:47002ms step_avg:139.89ms
step:347/1375 train_time:47144ms step_avg:139.89ms
step:348/1375 train_time:47291ms step_avg:139.91ms
step:349/1375 train_time:47434ms step_avg:139.92ms
step:350/1375 train_time:47578ms step_avg:139.94ms
step:351/1375 train_time:47723ms step_avg:139.95ms
step:352/1375 train_time:47868ms step_avg:139.97ms
step:353/1375 train_time:48011ms step_avg:139.97ms
step:354/1375 train_time:48154ms step_avg:139.98ms
step:355/1375 train_time:48298ms step_avg:140.00ms
step:356/1375 train_time:48442ms step_avg:140.01ms
step:357/1375 train_time:48588ms step_avg:140.02ms
step:358/1375 train_time:48731ms step_avg:140.03ms
step:359/1375 train_time:48874ms step_avg:140.04ms
step:360/1375 train_time:49019ms step_avg:140.05ms
step:361/1375 train_time:49163ms step_avg:140.07ms
step:362/1375 train_time:49310ms step_avg:140.09ms
step:363/1375 train_time:49453ms step_avg:140.09ms
step:364/1375 train_time:49600ms step_avg:140.11ms
step:365/1375 train_time:49743ms step_avg:140.12ms
step:366/1375 train_time:49889ms step_avg:140.14ms
step:367/1375 train_time:50032ms step_avg:140.15ms
step:368/1375 train_time:50176ms step_avg:140.16ms
step:369/1375 train_time:50319ms step_avg:140.16ms
step:370/1375 train_time:50464ms step_avg:140.18ms
step:371/1375 train_time:50609ms step_avg:140.19ms
step:372/1375 train_time:50752ms step_avg:140.20ms
step:373/1375 train_time:50895ms step_avg:140.21ms
step:374/1375 train_time:51039ms step_avg:140.22ms
step:375/1375 train_time:51183ms step_avg:140.23ms
step:375/1375 val_loss:3.7703 train_time:51254ms step_avg:140.42ms
step:376/1375 train_time:51328ms step_avg:140.24ms
step:377/1375 train_time:51476ms step_avg:140.26ms
step:378/1375 train_time:51622ms step_avg:140.28ms
step:379/1375 train_time:51764ms step_avg:140.28ms
step:380/1375 train_time:51907ms step_avg:140.29ms
step:381/1375 train_time:52090ms step_avg:140.41ms
step:382/1375 train_time:52233ms step_avg:140.41ms
step:383/1375 train_time:52376ms step_avg:140.42ms
step:384/1375 train_time:52519ms step_avg:140.42ms
step:385/1375 train_time:52662ms step_avg:140.43ms
step:386/1375 train_time:52805ms step_avg:140.44ms
step:387/1375 train_time:52949ms step_avg:140.45ms
step:388/1375 train_time:53101ms step_avg:140.48ms
step:389/1375 train_time:53245ms step_avg:140.49ms
step:390/1375 train_time:53390ms step_avg:140.50ms
step:391/1375 train_time:53534ms step_avg:140.51ms
step:392/1375 train_time:53677ms step_avg:140.52ms
step:393/1375 train_time:53820ms step_avg:140.52ms
step:394/1375 train_time:53963ms step_avg:140.53ms
step:395/1375 train_time:54107ms step_avg:140.54ms
step:396/1375 train_time:54252ms step_avg:140.55ms
step:397/1375 train_time:54397ms step_avg:140.56ms
step:398/1375 train_time:54540ms step_avg:140.57ms
step:399/1375 train_time:54685ms step_avg:140.58ms
step:400/1375 train_time:54828ms step_avg:140.58ms
step:401/1375 train_time:54971ms step_avg:140.59ms
step:402/1375 train_time:55117ms step_avg:140.60ms
step:403/1375 train_time:55262ms step_avg:140.62ms
step:404/1375 train_time:55405ms step_avg:140.62ms
step:405/1375 train_time:55548ms step_avg:140.63ms
step:406/1375 train_time:55693ms step_avg:140.64ms
step:407/1375 train_time:55837ms step_avg:140.65ms
step:408/1375 train_time:55981ms step_avg:140.66ms
step:409/1375 train_time:56126ms step_avg:140.67ms
step:410/1375 train_time:56273ms step_avg:140.68ms
step:411/1375 train_time:56421ms step_avg:140.70ms
step:412/1375 train_time:56565ms step_avg:140.71ms
step:413/1375 train_time:56710ms step_avg:140.72ms
step:414/1375 train_time:56856ms step_avg:140.73ms
step:415/1375 train_time:57002ms step_avg:140.75ms
step:416/1375 train_time:57146ms step_avg:140.75ms
step:417/1375 train_time:57294ms step_avg:140.77ms
step:418/1375 train_time:57441ms step_avg:140.79ms
step:419/1375 train_time:57587ms step_avg:140.80ms
step:420/1375 train_time:57732ms step_avg:140.81ms
step:421/1375 train_time:57879ms step_avg:140.83ms
step:422/1375 train_time:58024ms step_avg:140.84ms
step:423/1375 train_time:58168ms step_avg:140.84ms
step:424/1375 train_time:58313ms step_avg:140.85ms
step:425/1375 train_time:58461ms step_avg:140.87ms
step:426/1375 train_time:58605ms step_avg:140.88ms
step:427/1375 train_time:58749ms step_avg:140.89ms
step:428/1375 train_time:58895ms step_avg:140.90ms
step:429/1375 train_time:59040ms step_avg:140.91ms
step:430/1375 train_time:59186ms step_avg:140.92ms
step:431/1375 train_time:59330ms step_avg:140.93ms
step:432/1375 train_time:59477ms step_avg:140.94ms
step:433/1375 train_time:59623ms step_avg:140.95ms
step:434/1375 train_time:59767ms step_avg:140.96ms
step:435/1375 train_time:59914ms step_avg:140.97ms
step:436/1375 train_time:60061ms step_avg:140.99ms
step:437/1375 train_time:60207ms step_avg:141.00ms
step:438/1375 train_time:60352ms step_avg:141.01ms
step:439/1375 train_time:60499ms step_avg:141.02ms
step:440/1375 train_time:60643ms step_avg:141.03ms
step:441/1375 train_time:60791ms step_avg:141.05ms
step:442/1375 train_time:60937ms step_avg:141.06ms
step:443/1375 train_time:61084ms step_avg:141.07ms
step:444/1375 train_time:61227ms step_avg:141.08ms
step:445/1375 train_time:61374ms step_avg:141.09ms
step:446/1375 train_time:61523ms step_avg:141.11ms
step:447/1375 train_time:61668ms step_avg:141.12ms
step:448/1375 train_time:61814ms step_avg:141.13ms
step:449/1375 train_time:61961ms step_avg:141.14ms
step:450/1375 train_time:62105ms step_avg:141.15ms
step:451/1375 train_time:62250ms step_avg:141.16ms
step:452/1375 train_time:62399ms step_avg:141.17ms
step:453/1375 train_time:62545ms step_avg:141.19ms
step:454/1375 train_time:62689ms step_avg:141.19ms
step:455/1375 train_time:62836ms step_avg:141.21ms
step:456/1375 train_time:62983ms step_avg:141.22ms
step:457/1375 train_time:63128ms step_avg:141.23ms
step:458/1375 train_time:63273ms step_avg:141.23ms
step:459/1375 train_time:63420ms step_avg:141.25ms
step:460/1375 train_time:63565ms step_avg:141.26ms
step:461/1375 train_time:63711ms step_avg:141.27ms
step:462/1375 train_time:63859ms step_avg:141.28ms
step:463/1375 train_time:64004ms step_avg:141.29ms
step:464/1375 train_time:64149ms step_avg:141.30ms
step:465/1375 train_time:64296ms step_avg:141.31ms
step:466/1375 train_time:64442ms step_avg:141.32ms
step:467/1375 train_time:64589ms step_avg:141.33ms
step:468/1375 train_time:64734ms step_avg:141.34ms
step:469/1375 train_time:64881ms step_avg:141.35ms
step:470/1375 train_time:65025ms step_avg:141.36ms
step:471/1375 train_time:65171ms step_avg:141.37ms
step:472/1375 train_time:65318ms step_avg:141.38ms
step:473/1375 train_time:65463ms step_avg:141.39ms
step:474/1375 train_time:65610ms step_avg:141.40ms
step:475/1375 train_time:65756ms step_avg:141.41ms
step:476/1375 train_time:65904ms step_avg:141.42ms
step:477/1375 train_time:66049ms step_avg:141.43ms
step:478/1375 train_time:66195ms step_avg:141.44ms
step:479/1375 train_time:66340ms step_avg:141.45ms
step:480/1375 train_time:66486ms step_avg:141.46ms
step:481/1375 train_time:66632ms step_avg:141.47ms
step:482/1375 train_time:66781ms step_avg:141.49ms
step:483/1375 train_time:66926ms step_avg:141.49ms
step:484/1375 train_time:67072ms step_avg:141.50ms
step:485/1375 train_time:67219ms step_avg:141.51ms
step:486/1375 train_time:67365ms step_avg:141.52ms
step:487/1375 train_time:67509ms step_avg:141.53ms
step:488/1375 train_time:67656ms step_avg:141.54ms
step:489/1375 train_time:67804ms step_avg:141.55ms
step:490/1375 train_time:67949ms step_avg:141.56ms
step:491/1375 train_time:68097ms step_avg:141.57ms
step:492/1375 train_time:68242ms step_avg:141.58ms
step:493/1375 train_time:68387ms step_avg:141.59ms
step:494/1375 train_time:68532ms step_avg:141.60ms
step:495/1375 train_time:68680ms step_avg:141.61ms
step:496/1375 train_time:68824ms step_avg:141.61ms
step:497/1375 train_time:68968ms step_avg:141.62ms
step:498/1375 train_time:69115ms step_avg:141.63ms
step:499/1375 train_time:69263ms step_avg:141.64ms
step:500/1375 train_time:69408ms step_avg:141.65ms
step:500/1375 val_loss:3.6530 train_time:69479ms step_avg:141.79ms
step:501/1375 train_time:69553ms step_avg:141.66ms
step:502/1375 train_time:69701ms step_avg:141.67ms
step:503/1375 train_time:69846ms step_avg:141.67ms
step:504/1375 train_time:69990ms step_avg:141.68ms
step:505/1375 train_time:70134ms step_avg:141.68ms
step:506/1375 train_time:70279ms step_avg:141.69ms
step:507/1375 train_time:70425ms step_avg:141.70ms
step:508/1375 train_time:70576ms step_avg:141.72ms
step:509/1375 train_time:70722ms step_avg:141.73ms
step:510/1375 train_time:70866ms step_avg:141.73ms
step:511/1375 train_time:71012ms step_avg:141.74ms
step:512/1375 train_time:71161ms step_avg:141.75ms
step:513/1375 train_time:71307ms step_avg:141.76ms
step:514/1375 train_time:71457ms step_avg:141.78ms
step:515/1375 train_time:71605ms step_avg:141.79ms
step:516/1375 train_time:71753ms step_avg:141.81ms
step:517/1375 train_time:71902ms step_avg:141.82ms
step:518/1375 train_time:72048ms step_avg:141.83ms
step:519/1375 train_time:72196ms step_avg:141.84ms
step:520/1375 train_time:72341ms step_avg:141.84ms
step:521/1375 train_time:72488ms step_avg:141.85ms
step:522/1375 train_time:72636ms step_avg:141.87ms
step:523/1375 train_time:72783ms step_avg:141.88ms
step:524/1375 train_time:72930ms step_avg:141.89ms
step:525/1375 train_time:73080ms step_avg:141.90ms
step:526/1375 train_time:73227ms step_avg:141.91ms
step:527/1375 train_time:73376ms step_avg:141.93ms
step:528/1375 train_time:73522ms step_avg:141.93ms
step:529/1375 train_time:73668ms step_avg:141.94ms
step:530/1375 train_time:73817ms step_avg:141.96ms
step:531/1375 train_time:73963ms step_avg:141.96ms
step:532/1375 train_time:74109ms step_avg:141.97ms
step:533/1375 train_time:74259ms step_avg:141.99ms
step:534/1375 train_time:74406ms step_avg:142.00ms
step:535/1375 train_time:74553ms step_avg:142.01ms
step:536/1375 train_time:74702ms step_avg:142.02ms
step:537/1375 train_time:74849ms step_avg:142.03ms
step:538/1375 train_time:74998ms step_avg:142.04ms
step:539/1375 train_time:75144ms step_avg:142.05ms
step:540/1375 train_time:75293ms step_avg:142.06ms
step:541/1375 train_time:75441ms step_avg:142.07ms
step:542/1375 train_time:75588ms step_avg:142.08ms
step:543/1375 train_time:75736ms step_avg:142.09ms
step:544/1375 train_time:75883ms step_avg:142.10ms
step:545/1375 train_time:76030ms step_avg:142.11ms
step:546/1375 train_time:76180ms step_avg:142.13ms
step:547/1375 train_time:76325ms step_avg:142.13ms
step:548/1375 train_time:76473ms step_avg:142.14ms
step:549/1375 train_time:76623ms step_avg:142.16ms
step:550/1375 train_time:76769ms step_avg:142.16ms
step:551/1375 train_time:76916ms step_avg:142.17ms
step:552/1375 train_time:77063ms step_avg:142.18ms
step:553/1375 train_time:77211ms step_avg:142.19ms
step:554/1375 train_time:77359ms step_avg:142.20ms
step:555/1375 train_time:77506ms step_avg:142.21ms
step:556/1375 train_time:77654ms step_avg:142.22ms
step:557/1375 train_time:77803ms step_avg:142.24ms
step:558/1375 train_time:77949ms step_avg:142.24ms
step:559/1375 train_time:78096ms step_avg:142.25ms
step:560/1375 train_time:78243ms step_avg:142.26ms
step:561/1375 train_time:78390ms step_avg:142.27ms
step:562/1375 train_time:78538ms step_avg:142.28ms
step:563/1375 train_time:78685ms step_avg:142.29ms
step:564/1375 train_time:78832ms step_avg:142.30ms
step:565/1375 train_time:78979ms step_avg:142.31ms
step:566/1375 train_time:79125ms step_avg:142.31ms
step:567/1375 train_time:79271ms step_avg:142.32ms
step:568/1375 train_time:79420ms step_avg:142.33ms
step:569/1375 train_time:79565ms step_avg:142.34ms
step:570/1375 train_time:79714ms step_avg:142.35ms
step:571/1375 train_time:79904ms step_avg:142.43ms
step:572/1375 train_time:80049ms step_avg:142.44ms
step:573/1375 train_time:80197ms step_avg:142.45ms
step:574/1375 train_time:80347ms step_avg:142.46ms
step:575/1375 train_time:80493ms step_avg:142.46ms
step:576/1375 train_time:80638ms step_avg:142.47ms
step:577/1375 train_time:80786ms step_avg:142.48ms
step:578/1375 train_time:80936ms step_avg:142.49ms
step:579/1375 train_time:81084ms step_avg:142.50ms
step:580/1375 train_time:81230ms step_avg:142.51ms
step:581/1375 train_time:81380ms step_avg:142.52ms
step:582/1375 train_time:81524ms step_avg:142.52ms
step:583/1375 train_time:81671ms step_avg:142.53ms
step:584/1375 train_time:81819ms step_avg:142.54ms
step:585/1375 train_time:81965ms step_avg:142.55ms
step:586/1375 train_time:82114ms step_avg:142.56ms
step:587/1375 train_time:82262ms step_avg:142.57ms
step:588/1375 train_time:82409ms step_avg:142.58ms
step:589/1375 train_time:82557ms step_avg:142.59ms
step:590/1375 train_time:82704ms step_avg:142.59ms
step:591/1375 train_time:82849ms step_avg:142.60ms
step:592/1375 train_time:83000ms step_avg:142.61ms
step:593/1375 train_time:83146ms step_avg:142.62ms
step:594/1375 train_time:83293ms step_avg:142.62ms
step:595/1375 train_time:83439ms step_avg:142.63ms
step:596/1375 train_time:83587ms step_avg:142.64ms
step:597/1375 train_time:83734ms step_avg:142.65ms
step:598/1375 train_time:83883ms step_avg:142.66ms
step:599/1375 train_time:84031ms step_avg:142.67ms
step:600/1375 train_time:84180ms step_avg:142.68ms
step:601/1375 train_time:84326ms step_avg:142.68ms
step:602/1375 train_time:84473ms step_avg:142.69ms
step:603/1375 train_time:84622ms step_avg:142.70ms
step:604/1375 train_time:84767ms step_avg:142.70ms
step:605/1375 train_time:84914ms step_avg:142.71ms
step:606/1375 train_time:85063ms step_avg:142.72ms
step:607/1375 train_time:85209ms step_avg:142.73ms
step:608/1375 train_time:85359ms step_avg:142.74ms
step:609/1375 train_time:85505ms step_avg:142.75ms
step:610/1375 train_time:85653ms step_avg:142.75ms
step:611/1375 train_time:85800ms step_avg:142.76ms
step:612/1375 train_time:85947ms step_avg:142.77ms
step:613/1375 train_time:86095ms step_avg:142.78ms
step:614/1375 train_time:86242ms step_avg:142.78ms
step:615/1375 train_time:86389ms step_avg:142.79ms
step:616/1375 train_time:86537ms step_avg:142.80ms
step:617/1375 train_time:86686ms step_avg:142.81ms
step:618/1375 train_time:86834ms step_avg:142.82ms
step:619/1375 train_time:86985ms step_avg:142.83ms
step:620/1375 train_time:87133ms step_avg:142.84ms
step:621/1375 train_time:87283ms step_avg:142.85ms
step:622/1375 train_time:87432ms step_avg:142.86ms
step:623/1375 train_time:87581ms step_avg:142.87ms
step:624/1375 train_time:87728ms step_avg:142.88ms
step:625/1375 train_time:87878ms step_avg:142.89ms
step:625/1375 val_loss:3.5704 train_time:87953ms step_avg:143.01ms
step:626/1375 train_time:88030ms step_avg:142.91ms
step:627/1375 train_time:88181ms step_avg:142.92ms
step:628/1375 train_time:88328ms step_avg:142.92ms
step:629/1375 train_time:88478ms step_avg:142.94ms
step:630/1375 train_time:88624ms step_avg:142.94ms
step:631/1375 train_time:88771ms step_avg:142.95ms
step:632/1375 train_time:88919ms step_avg:142.96ms
step:633/1375 train_time:89072ms step_avg:142.97ms
step:634/1375 train_time:89221ms step_avg:142.98ms
step:635/1375 train_time:89368ms step_avg:142.99ms
step:636/1375 train_time:89517ms step_avg:143.00ms
step:637/1375 train_time:89664ms step_avg:143.00ms
step:638/1375 train_time:89811ms step_avg:143.01ms
step:639/1375 train_time:89960ms step_avg:143.02ms
step:640/1375 train_time:90111ms step_avg:143.03ms
step:641/1375 train_time:90262ms step_avg:143.05ms
step:642/1375 train_time:90411ms step_avg:143.05ms
step:643/1375 train_time:90560ms step_avg:143.07ms
step:644/1375 train_time:90707ms step_avg:143.07ms
step:645/1375 train_time:90857ms step_avg:143.08ms
step:646/1375 train_time:91004ms step_avg:143.09ms
step:647/1375 train_time:91152ms step_avg:143.10ms
step:648/1375 train_time:91305ms step_avg:143.11ms
step:649/1375 train_time:91456ms step_avg:143.12ms
step:650/1375 train_time:91604ms step_avg:143.13ms
step:651/1375 train_time:91754ms step_avg:143.14ms
step:652/1375 train_time:91901ms step_avg:143.15ms
step:653/1375 train_time:92048ms step_avg:143.15ms
step:654/1375 train_time:92199ms step_avg:143.17ms
step:655/1375 train_time:92347ms step_avg:143.17ms
step:656/1375 train_time:92496ms step_avg:143.18ms
step:657/1375 train_time:92642ms step_avg:143.19ms
step:658/1375 train_time:92790ms step_avg:143.20ms
step:659/1375 train_time:92941ms step_avg:143.21ms
step:660/1375 train_time:93087ms step_avg:143.21ms
step:661/1375 train_time:93238ms step_avg:143.22ms
step:662/1375 train_time:93386ms step_avg:143.23ms
step:663/1375 train_time:93534ms step_avg:143.24ms
step:664/1375 train_time:93684ms step_avg:143.25ms
step:665/1375 train_time:93836ms step_avg:143.26ms
step:666/1375 train_time:93983ms step_avg:143.27ms
step:667/1375 train_time:94132ms step_avg:143.28ms
step:668/1375 train_time:94282ms step_avg:143.29ms
step:669/1375 train_time:94430ms step_avg:143.29ms
step:670/1375 train_time:94580ms step_avg:143.30ms
step:671/1375 train_time:94729ms step_avg:143.31ms
step:672/1375 train_time:94878ms step_avg:143.32ms
step:673/1375 train_time:95024ms step_avg:143.32ms
step:674/1375 train_time:95175ms step_avg:143.34ms
step:675/1375 train_time:95322ms step_avg:143.34ms
step:676/1375 train_time:95473ms step_avg:143.35ms
step:677/1375 train_time:95620ms step_avg:143.36ms
step:678/1375 train_time:95769ms step_avg:143.37ms
step:679/1375 train_time:95919ms step_avg:143.38ms
step:680/1375 train_time:96067ms step_avg:143.38ms
step:681/1375 train_time:96215ms step_avg:143.39ms
step:682/1375 train_time:96364ms step_avg:143.40ms
step:683/1375 train_time:96514ms step_avg:143.41ms
step:684/1375 train_time:96663ms step_avg:143.42ms
step:685/1375 train_time:96812ms step_avg:143.43ms
step:686/1375 train_time:96961ms step_avg:143.43ms
step:687/1375 train_time:97108ms step_avg:143.44ms
step:688/1375 train_time:97259ms step_avg:143.45ms
step:689/1375 train_time:97408ms step_avg:143.46ms
step:690/1375 train_time:97559ms step_avg:143.47ms
step:691/1375 train_time:97706ms step_avg:143.47ms
step:692/1375 train_time:97856ms step_avg:143.48ms
step:693/1375 train_time:98004ms step_avg:143.49ms
step:694/1375 train_time:98154ms step_avg:143.50ms
step:695/1375 train_time:98302ms step_avg:143.51ms
step:696/1375 train_time:98450ms step_avg:143.51ms
step:697/1375 train_time:98600ms step_avg:143.52ms
step:698/1375 train_time:98747ms step_avg:143.53ms
step:699/1375 train_time:98897ms step_avg:143.54ms
step:700/1375 train_time:99043ms step_avg:143.54ms
step:701/1375 train_time:99194ms step_avg:143.55ms
step:702/1375 train_time:99344ms step_avg:143.56ms
step:703/1375 train_time:99492ms step_avg:143.57ms
step:704/1375 train_time:99641ms step_avg:143.58ms
step:705/1375 train_time:99790ms step_avg:143.58ms
step:706/1375 train_time:99942ms step_avg:143.60ms
step:707/1375 train_time:100090ms step_avg:143.60ms
step:708/1375 train_time:100239ms step_avg:143.61ms
step:709/1375 train_time:100387ms step_avg:143.62ms
step:710/1375 train_time:100536ms step_avg:143.62ms
step:711/1375 train_time:100685ms step_avg:143.63ms
step:712/1375 train_time:100838ms step_avg:143.64ms
step:713/1375 train_time:100986ms step_avg:143.65ms
step:714/1375 train_time:101137ms step_avg:143.66ms
step:715/1375 train_time:101285ms step_avg:143.67ms
step:716/1375 train_time:101437ms step_avg:143.68ms
step:717/1375 train_time:101585ms step_avg:143.68ms
step:718/1375 train_time:101735ms step_avg:143.69ms
step:719/1375 train_time:101884ms step_avg:143.70ms
step:720/1375 train_time:102035ms step_avg:143.71ms
step:721/1375 train_time:102185ms step_avg:143.72ms
step:722/1375 train_time:102336ms step_avg:143.73ms
step:723/1375 train_time:102483ms step_avg:143.73ms
step:724/1375 train_time:102635ms step_avg:143.75ms
step:725/1375 train_time:102783ms step_avg:143.75ms
step:726/1375 train_time:102934ms step_avg:143.76ms
step:727/1375 train_time:103085ms step_avg:143.77ms
step:728/1375 train_time:103236ms step_avg:143.78ms
step:729/1375 train_time:103385ms step_avg:143.79ms
step:730/1375 train_time:103535ms step_avg:143.80ms
step:731/1375 train_time:103683ms step_avg:143.81ms
step:732/1375 train_time:103833ms step_avg:143.81ms
step:733/1375 train_time:103983ms step_avg:143.82ms
step:734/1375 train_time:104134ms step_avg:143.83ms
step:735/1375 train_time:104286ms step_avg:143.84ms
step:736/1375 train_time:104438ms step_avg:143.85ms
step:737/1375 train_time:104586ms step_avg:143.86ms
step:738/1375 train_time:104739ms step_avg:143.87ms
step:739/1375 train_time:104889ms step_avg:143.88ms
step:740/1375 train_time:105040ms step_avg:143.89ms
step:741/1375 train_time:105191ms step_avg:143.90ms
step:742/1375 train_time:105341ms step_avg:143.91ms
step:743/1375 train_time:105489ms step_avg:143.91ms
step:744/1375 train_time:105639ms step_avg:143.92ms
step:745/1375 train_time:105790ms step_avg:143.93ms
step:746/1375 train_time:105940ms step_avg:143.94ms
step:747/1375 train_time:106089ms step_avg:143.95ms
step:748/1375 train_time:106240ms step_avg:143.96ms
step:749/1375 train_time:106390ms step_avg:143.96ms
step:750/1375 train_time:106541ms step_avg:143.97ms
step:750/1375 val_loss:3.5183 train_time:106615ms step_avg:144.07ms
step:751/1375 train_time:106692ms step_avg:143.98ms
step:752/1375 train_time:106842ms step_avg:143.99ms
step:753/1375 train_time:106992ms step_avg:144.00ms
step:754/1375 train_time:107140ms step_avg:144.01ms
step:755/1375 train_time:107289ms step_avg:144.01ms
step:756/1375 train_time:107438ms step_avg:144.02ms
step:757/1375 train_time:107590ms step_avg:144.03ms
step:758/1375 train_time:107741ms step_avg:144.04ms
step:759/1375 train_time:107891ms step_avg:144.05ms
step:760/1375 train_time:108040ms step_avg:144.05ms
step:761/1375 train_time:108235ms step_avg:144.12ms
step:762/1375 train_time:108383ms step_avg:144.13ms
step:763/1375 train_time:108533ms step_avg:144.13ms
step:764/1375 train_time:108681ms step_avg:144.14ms
step:765/1375 train_time:108831ms step_avg:144.15ms
step:766/1375 train_time:108981ms step_avg:144.16ms
step:767/1375 train_time:109133ms step_avg:144.17ms
step:768/1375 train_time:109286ms step_avg:144.18ms
step:769/1375 train_time:109437ms step_avg:144.19ms
step:770/1375 train_time:109588ms step_avg:144.19ms
step:771/1375 train_time:109736ms step_avg:144.20ms
step:772/1375 train_time:109886ms step_avg:144.21ms
step:773/1375 train_time:110037ms step_avg:144.22ms
step:774/1375 train_time:110187ms step_avg:144.22ms
step:775/1375 train_time:110339ms step_avg:144.23ms
step:776/1375 train_time:110491ms step_avg:144.24ms
step:777/1375 train_time:110642ms step_avg:144.25ms
step:778/1375 train_time:110793ms step_avg:144.26ms
step:779/1375 train_time:110942ms step_avg:144.27ms
step:780/1375 train_time:111092ms step_avg:144.28ms
step:781/1375 train_time:111244ms step_avg:144.29ms
step:782/1375 train_time:111393ms step_avg:144.29ms
step:783/1375 train_time:111544ms step_avg:144.30ms
step:784/1375 train_time:111694ms step_avg:144.31ms
step:785/1375 train_time:111845ms step_avg:144.32ms
step:786/1375 train_time:111994ms step_avg:144.32ms
step:787/1375 train_time:112144ms step_avg:144.33ms
step:788/1375 train_time:112293ms step_avg:144.34ms
step:789/1375 train_time:112444ms step_avg:144.34ms
step:790/1375 train_time:112592ms step_avg:144.35ms
step:791/1375 train_time:112743ms step_avg:144.36ms
step:792/1375 train_time:112893ms step_avg:144.36ms
step:793/1375 train_time:113043ms step_avg:144.37ms
step:794/1375 train_time:113195ms step_avg:144.38ms
step:795/1375 train_time:113347ms step_avg:144.39ms
step:796/1375 train_time:113495ms step_avg:144.40ms
step:797/1375 train_time:113645ms step_avg:144.40ms
step:798/1375 train_time:113795ms step_avg:144.41ms
step:799/1375 train_time:113950ms step_avg:144.42ms
step:800/1375 train_time:114098ms step_avg:144.43ms
step:801/1375 train_time:114250ms step_avg:144.44ms
step:802/1375 train_time:114399ms step_avg:144.44ms
step:803/1375 train_time:114549ms step_avg:144.45ms
step:804/1375 train_time:114696ms step_avg:144.45ms
step:805/1375 train_time:114851ms step_avg:144.47ms
step:806/1375 train_time:115000ms step_avg:144.47ms
step:807/1375 train_time:115151ms step_avg:144.48ms
step:808/1375 train_time:115300ms step_avg:144.49ms
step:809/1375 train_time:115450ms step_avg:144.49ms
step:810/1375 train_time:115598ms step_avg:144.50ms
step:811/1375 train_time:115751ms step_avg:144.51ms
step:812/1375 train_time:115900ms step_avg:144.51ms
step:813/1375 train_time:116050ms step_avg:144.52ms
step:814/1375 train_time:116199ms step_avg:144.53ms
step:815/1375 train_time:116350ms step_avg:144.53ms
step:816/1375 train_time:116499ms step_avg:144.54ms
step:817/1375 train_time:116651ms step_avg:144.55ms
step:818/1375 train_time:116799ms step_avg:144.55ms
step:819/1375 train_time:116953ms step_avg:144.57ms
step:820/1375 train_time:117107ms step_avg:144.58ms
step:821/1375 train_time:117255ms step_avg:144.58ms
step:822/1375 train_time:117407ms step_avg:144.59ms
step:823/1375 train_time:117557ms step_avg:144.60ms
step:824/1375 train_time:117709ms step_avg:144.61ms
step:825/1375 train_time:117863ms step_avg:144.62ms
step:826/1375 train_time:118017ms step_avg:144.63ms
step:827/1375 train_time:118167ms step_avg:144.64ms
step:828/1375 train_time:118318ms step_avg:144.64ms
step:829/1375 train_time:118470ms step_avg:144.65ms
step:830/1375 train_time:118619ms step_avg:144.66ms
step:831/1375 train_time:118772ms step_avg:144.67ms
step:832/1375 train_time:118925ms step_avg:144.68ms
step:833/1375 train_time:119076ms step_avg:144.68ms
step:834/1375 train_time:119228ms step_avg:144.69ms
step:835/1375 train_time:119380ms step_avg:144.70ms
step:836/1375 train_time:119534ms step_avg:144.71ms
step:837/1375 train_time:119685ms step_avg:144.72ms
step:838/1375 train_time:119837ms step_avg:144.73ms
step:839/1375 train_time:119987ms step_avg:144.74ms
step:840/1375 train_time:120139ms step_avg:144.75ms
step:841/1375 train_time:120291ms step_avg:144.75ms
step:842/1375 train_time:120445ms step_avg:144.77ms
step:843/1375 train_time:120594ms step_avg:144.77ms
step:844/1375 train_time:120745ms step_avg:144.78ms
step:845/1375 train_time:120894ms step_avg:144.78ms
step:846/1375 train_time:121047ms step_avg:144.79ms
step:847/1375 train_time:121198ms step_avg:144.80ms
step:848/1375 train_time:121349ms step_avg:144.81ms
step:849/1375 train_time:121500ms step_avg:144.81ms
step:850/1375 train_time:121654ms step_avg:144.83ms
step:851/1375 train_time:121805ms step_avg:144.83ms
step:852/1375 train_time:121957ms step_avg:144.84ms
step:853/1375 train_time:122108ms step_avg:144.85ms
step:854/1375 train_time:122259ms step_avg:144.86ms
step:855/1375 train_time:122411ms step_avg:144.86ms
step:856/1375 train_time:122560ms step_avg:144.87ms
step:857/1375 train_time:122714ms step_avg:144.88ms
step:858/1375 train_time:122868ms step_avg:144.89ms
step:859/1375 train_time:123018ms step_avg:144.90ms
step:860/1375 train_time:123170ms step_avg:144.91ms
step:861/1375 train_time:123322ms step_avg:144.91ms
step:862/1375 train_time:123473ms step_avg:144.92ms
step:863/1375 train_time:123625ms step_avg:144.93ms
step:864/1375 train_time:123777ms step_avg:144.94ms
step:865/1375 train_time:123927ms step_avg:144.94ms
step:866/1375 train_time:124084ms step_avg:144.96ms
step:867/1375 train_time:124235ms step_avg:144.96ms
step:868/1375 train_time:124384ms step_avg:144.97ms
step:869/1375 train_time:124535ms step_avg:144.98ms
step:870/1375 train_time:124690ms step_avg:144.99ms
step:871/1375 train_time:124840ms step_avg:144.99ms
step:872/1375 train_time:124991ms step_avg:145.00ms
step:873/1375 train_time:125142ms step_avg:145.01ms
step:874/1375 train_time:125293ms step_avg:145.01ms
step:875/1375 train_time:125444ms step_avg:145.02ms
step:875/1375 val_loss:3.4667 train_time:125521ms step_avg:145.11ms
step:876/1375 train_time:125599ms step_avg:145.03ms
step:877/1375 train_time:125752ms step_avg:145.04ms
step:878/1375 train_time:125904ms step_avg:145.05ms
step:879/1375 train_time:126055ms step_avg:145.06ms
step:880/1375 train_time:126207ms step_avg:145.06ms
step:881/1375 train_time:126357ms step_avg:145.07ms
step:882/1375 train_time:126510ms step_avg:145.08ms
step:883/1375 train_time:126661ms step_avg:145.09ms
step:884/1375 train_time:126812ms step_avg:145.09ms
step:885/1375 train_time:126964ms step_avg:145.10ms
step:886/1375 train_time:127119ms step_avg:145.11ms
step:887/1375 train_time:127267ms step_avg:145.12ms
step:888/1375 train_time:127421ms step_avg:145.13ms
step:889/1375 train_time:127574ms step_avg:145.14ms
step:890/1375 train_time:127724ms step_avg:145.14ms
step:891/1375 train_time:127874ms step_avg:145.15ms
step:892/1375 train_time:128027ms step_avg:145.16ms
step:893/1375 train_time:128178ms step_avg:145.16ms
step:894/1375 train_time:128331ms step_avg:145.17ms
step:895/1375 train_time:128485ms step_avg:145.18ms
step:896/1375 train_time:128638ms step_avg:145.19ms
step:897/1375 train_time:128788ms step_avg:145.20ms
step:898/1375 train_time:128942ms step_avg:145.20ms
step:899/1375 train_time:129094ms step_avg:145.21ms
step:900/1375 train_time:129243ms step_avg:145.22ms
step:901/1375 train_time:129398ms step_avg:145.23ms
step:902/1375 train_time:129545ms step_avg:145.23ms
step:903/1375 train_time:129698ms step_avg:145.24ms
step:904/1375 train_time:129849ms step_avg:145.25ms
step:905/1375 train_time:130003ms step_avg:145.25ms
step:906/1375 train_time:130154ms step_avg:145.26ms
step:907/1375 train_time:130309ms step_avg:145.27ms
step:908/1375 train_time:130462ms step_avg:145.28ms
step:909/1375 train_time:130613ms step_avg:145.29ms
step:910/1375 train_time:130770ms step_avg:145.30ms
step:911/1375 train_time:130922ms step_avg:145.31ms
step:912/1375 train_time:131072ms step_avg:145.31ms
step:913/1375 train_time:131224ms step_avg:145.32ms
step:914/1375 train_time:131376ms step_avg:145.33ms
step:915/1375 train_time:131528ms step_avg:145.34ms
step:916/1375 train_time:131681ms step_avg:145.34ms
step:917/1375 train_time:131831ms step_avg:145.35ms
step:918/1375 train_time:131984ms step_avg:145.36ms
step:919/1375 train_time:132140ms step_avg:145.37ms
step:920/1375 train_time:132290ms step_avg:145.37ms
step:921/1375 train_time:132444ms step_avg:145.38ms
step:922/1375 train_time:132600ms step_avg:145.39ms
step:923/1375 train_time:132752ms step_avg:145.40ms
step:924/1375 train_time:132906ms step_avg:145.41ms
step:925/1375 train_time:133060ms step_avg:145.42ms
step:926/1375 train_time:133211ms step_avg:145.43ms
step:927/1375 train_time:133363ms step_avg:145.43ms
step:928/1375 train_time:133516ms step_avg:145.44ms
step:929/1375 train_time:133670ms step_avg:145.45ms
step:930/1375 train_time:133825ms step_avg:145.46ms
step:931/1375 train_time:133977ms step_avg:145.47ms
step:932/1375 train_time:134128ms step_avg:145.48ms
step:933/1375 train_time:134282ms step_avg:145.48ms
step:934/1375 train_time:134433ms step_avg:145.49ms
step:935/1375 train_time:134586ms step_avg:145.50ms
step:936/1375 train_time:134739ms step_avg:145.51ms
step:937/1375 train_time:134896ms step_avg:145.52ms
step:938/1375 train_time:135047ms step_avg:145.52ms
step:939/1375 train_time:135203ms step_avg:145.54ms
step:940/1375 train_time:135356ms step_avg:145.54ms
step:941/1375 train_time:135508ms step_avg:145.55ms
step:942/1375 train_time:135660ms step_avg:145.56ms
step:943/1375 train_time:135813ms step_avg:145.57ms
step:944/1375 train_time:135971ms step_avg:145.58ms
step:945/1375 train_time:136126ms step_avg:145.59ms
step:946/1375 train_time:136279ms step_avg:145.60ms
step:947/1375 train_time:136431ms step_avg:145.60ms
step:948/1375 train_time:136584ms step_avg:145.61ms
step:949/1375 train_time:136736ms step_avg:145.62ms
step:950/1375 train_time:136890ms step_avg:145.63ms
step:951/1375 train_time:137094ms step_avg:145.69ms
step:952/1375 train_time:137244ms step_avg:145.69ms
step:953/1375 train_time:137397ms step_avg:145.70ms
step:954/1375 train_time:137547ms step_avg:145.71ms
step:955/1375 train_time:137699ms step_avg:145.71ms
step:956/1375 train_time:137852ms step_avg:145.72ms
step:957/1375 train_time:138006ms step_avg:145.73ms
step:958/1375 train_time:138166ms step_avg:145.74ms
step:959/1375 train_time:138321ms step_avg:145.75ms
step:960/1375 train_time:138472ms step_avg:145.76ms
step:961/1375 train_time:138624ms step_avg:145.77ms
step:962/1375 train_time:138777ms step_avg:145.77ms
step:963/1375 train_time:138936ms step_avg:145.79ms
step:964/1375 train_time:139090ms step_avg:145.80ms
step:965/1375 train_time:139242ms step_avg:145.80ms
step:966/1375 train_time:139395ms step_avg:145.81ms
step:967/1375 train_time:139545ms step_avg:145.82ms
step:968/1375 train_time:139696ms step_avg:145.82ms
step:969/1375 train_time:139849ms step_avg:145.83ms
step:970/1375 train_time:140004ms step_avg:145.84ms
step:971/1375 train_time:140159ms step_avg:145.85ms
step:972/1375 train_time:140310ms step_avg:145.85ms
step:973/1375 train_time:140461ms step_avg:145.86ms
step:974/1375 train_time:140613ms step_avg:145.86ms
step:975/1375 train_time:140766ms step_avg:145.87ms
step:976/1375 train_time:140919ms step_avg:145.88ms
step:977/1375 train_time:141071ms step_avg:145.88ms
step:978/1375 train_time:141225ms step_avg:145.89ms
step:979/1375 train_time:141376ms step_avg:145.90ms
step:980/1375 train_time:141527ms step_avg:145.90ms
step:981/1375 train_time:141678ms step_avg:145.91ms
step:982/1375 train_time:141830ms step_avg:145.92ms
step:983/1375 train_time:141984ms step_avg:145.92ms
step:984/1375 train_time:142134ms step_avg:145.93ms
step:985/1375 train_time:142286ms step_avg:145.93ms
step:986/1375 train_time:142440ms step_avg:145.94ms
step:987/1375 train_time:142591ms step_avg:145.95ms
step:988/1375 train_time:142743ms step_avg:145.95ms
step:989/1375 train_time:142895ms step_avg:145.96ms
step:990/1375 train_time:143048ms step_avg:145.97ms
step:991/1375 train_time:143202ms step_avg:145.98ms
step:992/1375 train_time:143361ms step_avg:145.99ms
step:993/1375 train_time:143521ms step_avg:146.00ms
step:994/1375 train_time:143671ms step_avg:146.01ms
step:995/1375 train_time:143822ms step_avg:146.01ms
step:996/1375 train_time:143971ms step_avg:146.02ms
step:997/1375 train_time:144124ms step_avg:146.02ms
step:998/1375 train_time:144275ms step_avg:146.03ms
step:999/1375 train_time:144428ms step_avg:146.03ms
step:1000/1375 train_time:144581ms step_avg:146.04ms
step:1000/1375 val_loss:3.4007 train_time:144655ms step_avg:146.12ms
step:1001/1375 train_time:144732ms step_avg:146.05ms
step:1002/1375 train_time:144887ms step_avg:146.06ms
step:1003/1375 train_time:145040ms step_avg:146.06ms
step:1004/1375 train_time:145192ms step_avg:146.07ms
step:1005/1375 train_time:145344ms step_avg:146.07ms
step:1006/1375 train_time:145495ms step_avg:146.08ms
step:1007/1375 train_time:145653ms step_avg:146.09ms
step:1008/1375 train_time:145806ms step_avg:146.10ms
step:1009/1375 train_time:145966ms step_avg:146.11ms
step:1010/1375 train_time:146116ms step_avg:146.12ms
step:1011/1375 train_time:146269ms step_avg:146.12ms
step:1012/1375 train_time:146419ms step_avg:146.13ms
step:1013/1375 train_time:146573ms step_avg:146.13ms
step:1014/1375 train_time:146726ms step_avg:146.14ms
step:1015/1375 train_time:146879ms step_avg:146.15ms
step:1016/1375 train_time:147032ms step_avg:146.16ms
step:1017/1375 train_time:147184ms step_avg:146.16ms
step:1018/1375 train_time:147336ms step_avg:146.17ms
step:1019/1375 train_time:147488ms step_avg:146.17ms
step:1020/1375 train_time:147643ms step_avg:146.18ms
step:1021/1375 train_time:147795ms step_avg:146.19ms
step:1022/1375 train_time:147950ms step_avg:146.20ms
step:1023/1375 train_time:148105ms step_avg:146.20ms
step:1024/1375 train_time:148258ms step_avg:146.21ms
step:1025/1375 train_time:148413ms step_avg:146.22ms
step:1026/1375 train_time:148566ms step_avg:146.23ms
step:1027/1375 train_time:148720ms step_avg:146.23ms
step:1028/1375 train_time:148875ms step_avg:146.24ms
step:1029/1375 train_time:149032ms step_avg:146.25ms
step:1030/1375 train_time:149186ms step_avg:146.26ms
step:1031/1375 train_time:149338ms step_avg:146.27ms
step:1032/1375 train_time:149490ms step_avg:146.27ms
step:1033/1375 train_time:149641ms step_avg:146.28ms
step:1034/1375 train_time:149794ms step_avg:146.28ms
step:1035/1375 train_time:149952ms step_avg:146.29ms
step:1036/1375 train_time:150105ms step_avg:146.30ms
step:1037/1375 train_time:150258ms step_avg:146.31ms
step:1038/1375 train_time:150414ms step_avg:146.32ms
step:1039/1375 train_time:150566ms step_avg:146.32ms
step:1040/1375 train_time:150717ms step_avg:146.33ms
step:1041/1375 train_time:150872ms step_avg:146.34ms
step:1042/1375 train_time:151023ms step_avg:146.34ms
step:1043/1375 train_time:151174ms step_avg:146.34ms
step:1044/1375 train_time:151330ms step_avg:146.35ms
step:1045/1375 train_time:151484ms step_avg:146.36ms
step:1046/1375 train_time:151637ms step_avg:146.37ms
step:1047/1375 train_time:151790ms step_avg:146.37ms
step:1048/1375 train_time:151946ms step_avg:146.38ms
step:1049/1375 train_time:152101ms step_avg:146.39ms
step:1050/1375 train_time:152258ms step_avg:146.40ms
step:1051/1375 train_time:152416ms step_avg:146.41ms
step:1052/1375 train_time:152570ms step_avg:146.42ms
step:1053/1375 train_time:152721ms step_avg:146.42ms
step:1054/1375 train_time:152876ms step_avg:146.43ms
step:1055/1375 train_time:153029ms step_avg:146.44ms
step:1056/1375 train_time:153184ms step_avg:146.45ms
step:1057/1375 train_time:153339ms step_avg:146.46ms
step:1058/1375 train_time:153495ms step_avg:146.47ms
step:1059/1375 train_time:153651ms step_avg:146.47ms
step:1060/1375 train_time:153805ms step_avg:146.48ms
step:1061/1375 train_time:153955ms step_avg:146.48ms
step:1062/1375 train_time:154109ms step_avg:146.49ms
step:1063/1375 train_time:154262ms step_avg:146.50ms
step:1064/1375 train_time:154415ms step_avg:146.50ms
step:1065/1375 train_time:154568ms step_avg:146.51ms
step:1066/1375 train_time:154727ms step_avg:146.52ms
step:1067/1375 train_time:154883ms step_avg:146.53ms
step:1068/1375 train_time:155037ms step_avg:146.54ms
step:1069/1375 train_time:155196ms step_avg:146.55ms
step:1070/1375 train_time:155348ms step_avg:146.55ms
step:1071/1375 train_time:155501ms step_avg:146.56ms
step:1072/1375 train_time:155654ms step_avg:146.57ms
step:1073/1375 train_time:155804ms step_avg:146.57ms
step:1074/1375 train_time:155957ms step_avg:146.58ms
step:1075/1375 train_time:156113ms step_avg:146.58ms
step:1076/1375 train_time:156266ms step_avg:146.59ms
step:1077/1375 train_time:156418ms step_avg:146.60ms
step:1078/1375 train_time:156577ms step_avg:146.61ms
step:1079/1375 train_time:156734ms step_avg:146.62ms
step:1080/1375 train_time:156887ms step_avg:146.62ms
step:1081/1375 train_time:157042ms step_avg:146.63ms
step:1082/1375 train_time:157195ms step_avg:146.64ms
step:1083/1375 train_time:157349ms step_avg:146.64ms
step:1084/1375 train_time:157506ms step_avg:146.65ms
step:1085/1375 train_time:157659ms step_avg:146.66ms
step:1086/1375 train_time:157814ms step_avg:146.67ms
step:1087/1375 train_time:157967ms step_avg:146.67ms
step:1088/1375 train_time:158121ms step_avg:146.68ms
step:1089/1375 train_time:158280ms step_avg:146.69ms
step:1090/1375 train_time:158440ms step_avg:146.70ms
step:1091/1375 train_time:158593ms step_avg:146.71ms
step:1092/1375 train_time:158746ms step_avg:146.71ms
step:1093/1375 train_time:158900ms step_avg:146.72ms
step:1094/1375 train_time:159053ms step_avg:146.73ms
step:1095/1375 train_time:159205ms step_avg:146.73ms
step:1096/1375 train_time:159362ms step_avg:146.74ms
step:1097/1375 train_time:159516ms step_avg:146.75ms
step:1098/1375 train_time:159671ms step_avg:146.76ms
step:1099/1375 train_time:159822ms step_avg:146.76ms
step:1100/1375 train_time:159976ms step_avg:146.77ms
step:1101/1375 train_time:160129ms step_avg:146.77ms
step:1102/1375 train_time:160284ms step_avg:146.78ms
step:1103/1375 train_time:160437ms step_avg:146.79ms
step:1104/1375 train_time:160589ms step_avg:146.79ms
step:1105/1375 train_time:160744ms step_avg:146.80ms
step:1106/1375 train_time:160898ms step_avg:146.80ms
step:1107/1375 train_time:161052ms step_avg:146.81ms
step:1108/1375 train_time:161209ms step_avg:146.82ms
step:1109/1375 train_time:161362ms step_avg:146.83ms
step:1110/1375 train_time:161517ms step_avg:146.83ms
step:1111/1375 train_time:161672ms step_avg:146.84ms
step:1112/1375 train_time:161825ms step_avg:146.85ms
step:1113/1375 train_time:161979ms step_avg:146.85ms
step:1114/1375 train_time:162135ms step_avg:146.86ms
step:1115/1375 train_time:162290ms step_avg:146.87ms
step:1116/1375 train_time:162440ms step_avg:146.87ms
step:1117/1375 train_time:162595ms step_avg:146.88ms
step:1118/1375 train_time:162754ms step_avg:146.89ms
step:1119/1375 train_time:162907ms step_avg:146.90ms
step:1120/1375 train_time:163060ms step_avg:146.90ms
step:1121/1375 train_time:163216ms step_avg:146.91ms
step:1122/1375 train_time:163367ms step_avg:146.91ms
step:1123/1375 train_time:163520ms step_avg:146.92ms
step:1124/1375 train_time:163679ms step_avg:146.93ms
step:1125/1375 train_time:163834ms step_avg:146.94ms
step:1125/1375 val_loss:3.3473 train_time:163911ms step_avg:147.01ms
step:1126/1375 train_time:163988ms step_avg:146.94ms
step:1127/1375 train_time:164145ms step_avg:146.95ms
step:1128/1375 train_time:164299ms step_avg:146.96ms
step:1129/1375 train_time:164458ms step_avg:146.97ms
step:1130/1375 train_time:164612ms step_avg:146.97ms
step:1131/1375 train_time:164768ms step_avg:146.98ms
step:1132/1375 train_time:164921ms step_avg:146.99ms
step:1133/1375 train_time:165076ms step_avg:147.00ms
step:1134/1375 train_time:165231ms step_avg:147.00ms
step:1135/1375 train_time:165385ms step_avg:147.01ms
step:1136/1375 train_time:165544ms step_avg:147.02ms
step:1137/1375 train_time:165696ms step_avg:147.02ms
step:1138/1375 train_time:165851ms step_avg:147.03ms
step:1139/1375 train_time:166005ms step_avg:147.04ms
step:1140/1375 train_time:166160ms step_avg:147.04ms
step:1141/1375 train_time:166360ms step_avg:147.09ms
step:1142/1375 train_time:166513ms step_avg:147.10ms
step:1143/1375 train_time:166671ms step_avg:147.11ms
step:1144/1375 train_time:166825ms step_avg:147.11ms
step:1145/1375 train_time:166974ms step_avg:147.11ms
step:1146/1375 train_time:167130ms step_avg:147.12ms
step:1147/1375 train_time:167286ms step_avg:147.13ms
step:1148/1375 train_time:167440ms step_avg:147.14ms
step:1149/1375 train_time:167596ms step_avg:147.14ms
step:1150/1375 train_time:167750ms step_avg:147.15ms
step:1151/1375 train_time:167906ms step_avg:147.16ms
step:1152/1375 train_time:168060ms step_avg:147.16ms
step:1153/1375 train_time:168217ms step_avg:147.17ms
step:1154/1375 train_time:168370ms step_avg:147.18ms
step:1155/1375 train_time:168524ms step_avg:147.18ms
step:1156/1375 train_time:168684ms step_avg:147.19ms
step:1157/1375 train_time:168842ms step_avg:147.20ms
step:1158/1375 train_time:168998ms step_avg:147.21ms
step:1159/1375 train_time:169153ms step_avg:147.22ms
step:1160/1375 train_time:169305ms step_avg:147.22ms
step:1161/1375 train_time:169460ms step_avg:147.23ms
step:1162/1375 train_time:169615ms step_avg:147.24ms
step:1163/1375 train_time:169772ms step_avg:147.24ms
step:1164/1375 train_time:169924ms step_avg:147.25ms
step:1165/1375 train_time:170077ms step_avg:147.25ms
step:1166/1375 train_time:170233ms step_avg:147.26ms
step:1167/1375 train_time:170386ms step_avg:147.27ms
step:1168/1375 train_time:170539ms step_avg:147.27ms
step:1169/1375 train_time:170697ms step_avg:147.28ms
step:1170/1375 train_time:170852ms step_avg:147.29ms
step:1171/1375 train_time:171009ms step_avg:147.29ms
step:1172/1375 train_time:171163ms step_avg:147.30ms
step:1173/1375 train_time:171317ms step_avg:147.31ms
step:1174/1375 train_time:171482ms step_avg:147.32ms
step:1175/1375 train_time:171638ms step_avg:147.33ms
step:1176/1375 train_time:171798ms step_avg:147.34ms
step:1177/1375 train_time:171961ms step_avg:147.35ms
step:1178/1375 train_time:172113ms step_avg:147.36ms
step:1179/1375 train_time:172268ms step_avg:147.36ms
step:1180/1375 train_time:172429ms step_avg:147.38ms
step:1181/1375 train_time:172585ms step_avg:147.38ms
step:1182/1375 train_time:172735ms step_avg:147.39ms
step:1183/1375 train_time:172890ms step_avg:147.39ms
step:1184/1375 train_time:173048ms step_avg:147.40ms
step:1185/1375 train_time:173205ms step_avg:147.41ms
step:1186/1375 train_time:173359ms step_avg:147.41ms
step:1187/1375 train_time:173525ms step_avg:147.43ms
step:1188/1375 train_time:173678ms step_avg:147.43ms
step:1189/1375 train_time:173834ms step_avg:147.44ms
step:1190/1375 train_time:173993ms step_avg:147.45ms
step:1191/1375 train_time:174149ms step_avg:147.46ms
step:1192/1375 train_time:174299ms step_avg:147.46ms
step:1193/1375 train_time:174454ms step_avg:147.47ms
step:1194/1375 train_time:174608ms step_avg:147.47ms
step:1195/1375 train_time:174762ms step_avg:147.48ms
step:1196/1375 train_time:174919ms step_avg:147.49ms
step:1197/1375 train_time:175077ms step_avg:147.50ms
step:1198/1375 train_time:175237ms step_avg:147.51ms
step:1199/1375 train_time:175393ms step_avg:147.51ms
step:1200/1375 train_time:175547ms step_avg:147.52ms
step:1201/1375 train_time:175700ms step_avg:147.52ms
step:1202/1375 train_time:175870ms step_avg:147.54ms
step:1203/1375 train_time:176029ms step_avg:147.55ms
step:1204/1375 train_time:176185ms step_avg:147.56ms
step:1205/1375 train_time:176341ms step_avg:147.57ms
step:1206/1375 train_time:176497ms step_avg:147.57ms
step:1207/1375 train_time:176652ms step_avg:147.58ms
step:1208/1375 train_time:176808ms step_avg:147.59ms
step:1209/1375 train_time:176963ms step_avg:147.59ms
step:1210/1375 train_time:177123ms step_avg:147.60ms
step:1211/1375 train_time:177277ms step_avg:147.61ms
step:1212/1375 train_time:177431ms step_avg:147.61ms
step:1213/1375 train_time:177588ms step_avg:147.62ms
step:1214/1375 train_time:177746ms step_avg:147.63ms
step:1215/1375 train_time:177900ms step_avg:147.63ms
step:1216/1375 train_time:178053ms step_avg:147.64ms
step:1217/1375 train_time:178208ms step_avg:147.65ms
step:1218/1375 train_time:178359ms step_avg:147.65ms
step:1219/1375 train_time:178512ms step_avg:147.65ms
step:1220/1375 train_time:178665ms step_avg:147.66ms
step:1221/1375 train_time:178819ms step_avg:147.66ms
step:1222/1375 train_time:178976ms step_avg:147.67ms
step:1223/1375 train_time:179132ms step_avg:147.68ms
step:1224/1375 train_time:179289ms step_avg:147.68ms
step:1225/1375 train_time:179447ms step_avg:147.69ms
step:1226/1375 train_time:179601ms step_avg:147.70ms
step:1227/1375 train_time:179761ms step_avg:147.71ms
step:1228/1375 train_time:179915ms step_avg:147.71ms
step:1229/1375 train_time:180071ms step_avg:147.72ms
step:1230/1375 train_time:180230ms step_avg:147.73ms
step:1231/1375 train_time:180386ms step_avg:147.74ms
step:1232/1375 train_time:180544ms step_avg:147.74ms
step:1233/1375 train_time:180701ms step_avg:147.75ms
step:1234/1375 train_time:180855ms step_avg:147.76ms
step:1235/1375 train_time:181011ms step_avg:147.76ms
step:1236/1375 train_time:181165ms step_avg:147.77ms
step:1237/1375 train_time:181319ms step_avg:147.77ms
step:1238/1375 train_time:181484ms step_avg:147.79ms
step:1239/1375 train_time:181639ms step_avg:147.79ms
step:1240/1375 train_time:181800ms step_avg:147.81ms
step:1241/1375 train_time:181964ms step_avg:147.82ms
step:1242/1375 train_time:182119ms step_avg:147.82ms
step:1243/1375 train_time:182278ms step_avg:147.83ms
step:1244/1375 train_time:182433ms step_avg:147.84ms
step:1245/1375 train_time:182587ms step_avg:147.84ms
step:1246/1375 train_time:182741ms step_avg:147.85ms
step:1247/1375 train_time:182900ms step_avg:147.86ms
step:1248/1375 train_time:183055ms step_avg:147.86ms
step:1249/1375 train_time:183210ms step_avg:147.87ms
step:1250/1375 train_time:183363ms step_avg:147.87ms
step:1250/1375 val_loss:3.3021 train_time:183444ms step_avg:147.94ms
step:1251/1375 train_time:183523ms step_avg:147.88ms
step:1252/1375 train_time:183678ms step_avg:147.89ms
step:1253/1375 train_time:183833ms step_avg:147.89ms
step:1254/1375 train_time:183985ms step_avg:147.90ms
step:1255/1375 train_time:184153ms step_avg:147.91ms
step:1256/1375 train_time:184307ms step_avg:147.92ms
step:1257/1375 train_time:184462ms step_avg:147.92ms
step:1258/1375 train_time:184620ms step_avg:147.93ms
step:1259/1375 train_time:184778ms step_avg:147.94ms
step:1260/1375 train_time:184933ms step_avg:147.95ms
step:1261/1375 train_time:185088ms step_avg:147.95ms
step:1262/1375 train_time:185246ms step_avg:147.96ms
step:1263/1375 train_time:185402ms step_avg:147.97ms
step:1264/1375 train_time:185555ms step_avg:147.97ms
step:1265/1375 train_time:185710ms step_avg:147.98ms
step:1266/1375 train_time:185866ms step_avg:147.98ms
step:1267/1375 train_time:186021ms step_avg:147.99ms
step:1268/1375 train_time:186176ms step_avg:147.99ms
step:1269/1375 train_time:186337ms step_avg:148.00ms
step:1270/1375 train_time:186495ms step_avg:148.01ms
step:1271/1375 train_time:186650ms step_avg:148.02ms
step:1272/1375 train_time:186803ms step_avg:148.02ms
step:1273/1375 train_time:186956ms step_avg:148.03ms
step:1274/1375 train_time:187112ms step_avg:148.03ms
step:1275/1375 train_time:187268ms step_avg:148.04ms
step:1276/1375 train_time:187423ms step_avg:148.04ms
step:1277/1375 train_time:187578ms step_avg:148.05ms
step:1278/1375 train_time:187732ms step_avg:148.05ms
step:1279/1375 train_time:187888ms step_avg:148.06ms
step:1280/1375 train_time:188051ms step_avg:148.07ms
step:1281/1375 train_time:188208ms step_avg:148.08ms
step:1282/1375 train_time:188361ms step_avg:148.08ms
step:1283/1375 train_time:188519ms step_avg:148.09ms
step:1284/1375 train_time:188676ms step_avg:148.10ms
step:1285/1375 train_time:188832ms step_avg:148.10ms
step:1286/1375 train_time:188988ms step_avg:148.11ms
step:1287/1375 train_time:189141ms step_avg:148.11ms
step:1288/1375 train_time:189298ms step_avg:148.12ms
step:1289/1375 train_time:189458ms step_avg:148.13ms
step:1290/1375 train_time:189619ms step_avg:148.14ms
step:1291/1375 train_time:189779ms step_avg:148.15ms
step:1292/1375 train_time:189936ms step_avg:148.16ms
step:1293/1375 train_time:190095ms step_avg:148.16ms
step:1294/1375 train_time:190249ms step_avg:148.17ms
step:1295/1375 train_time:190403ms step_avg:148.17ms
step:1296/1375 train_time:190559ms step_avg:148.18ms
step:1297/1375 train_time:190717ms step_avg:148.19ms
step:1298/1375 train_time:190873ms step_avg:148.19ms
step:1299/1375 train_time:191027ms step_avg:148.20ms
step:1300/1375 train_time:191182ms step_avg:148.20ms
step:1301/1375 train_time:191335ms step_avg:148.21ms
step:1302/1375 train_time:191493ms step_avg:148.21ms
step:1303/1375 train_time:191651ms step_avg:148.22ms
step:1304/1375 train_time:191810ms step_avg:148.23ms
step:1305/1375 train_time:191965ms step_avg:148.24ms
step:1306/1375 train_time:192121ms step_avg:148.24ms
step:1307/1375 train_time:192275ms step_avg:148.25ms
step:1308/1375 train_time:192432ms step_avg:148.25ms
step:1309/1375 train_time:192587ms step_avg:148.26ms
step:1310/1375 train_time:192740ms step_avg:148.26ms
step:1311/1375 train_time:192894ms step_avg:148.27ms
step:1312/1375 train_time:193048ms step_avg:148.27ms
step:1313/1375 train_time:193200ms step_avg:148.27ms
step:1314/1375 train_time:193357ms step_avg:148.28ms
step:1315/1375 train_time:193514ms step_avg:148.29ms
step:1316/1375 train_time:193667ms step_avg:148.29ms
step:1317/1375 train_time:193820ms step_avg:148.29ms
step:1318/1375 train_time:193981ms step_avg:148.30ms
step:1319/1375 train_time:194138ms step_avg:148.31ms
step:1320/1375 train_time:194294ms step_avg:148.32ms
step:1321/1375 train_time:194451ms step_avg:148.32ms
step:1322/1375 train_time:194609ms step_avg:148.33ms
step:1323/1375 train_time:194763ms step_avg:148.33ms
step:1324/1375 train_time:194917ms step_avg:148.34ms
step:1325/1375 train_time:195074ms step_avg:148.35ms
step:1326/1375 train_time:195235ms step_avg:148.35ms
step:1327/1375 train_time:195390ms step_avg:148.36ms
step:1328/1375 train_time:195544ms step_avg:148.36ms
step:1329/1375 train_time:195717ms step_avg:148.38ms
step:1330/1375 train_time:195877ms step_avg:148.39ms
step:1331/1375 train_time:196076ms step_avg:148.43ms
step:1332/1375 train_time:196238ms step_avg:148.44ms
step:1333/1375 train_time:196395ms step_avg:148.45ms
step:1334/1375 train_time:196553ms step_avg:148.45ms
step:1335/1375 train_time:196706ms step_avg:148.46ms
step:1336/1375 train_time:196870ms step_avg:148.47ms
step:1337/1375 train_time:197029ms step_avg:148.48ms
step:1338/1375 train_time:197185ms step_avg:148.48ms
step:1339/1375 train_time:197344ms step_avg:148.49ms
step:1340/1375 train_time:197503ms step_avg:148.50ms
step:1341/1375 train_time:197656ms step_avg:148.50ms
step:1342/1375 train_time:197815ms step_avg:148.51ms
step:1343/1375 train_time:197968ms step_avg:148.51ms
step:1344/1375 train_time:198122ms step_avg:148.52ms
step:1345/1375 train_time:198279ms step_avg:148.52ms
step:1346/1375 train_time:198435ms step_avg:148.53ms
step:1347/1375 train_time:198594ms step_avg:148.54ms
step:1348/1375 train_time:198749ms step_avg:148.54ms
step:1349/1375 train_time:198907ms step_avg:148.55ms
step:1350/1375 train_time:199059ms step_avg:148.55ms
step:1351/1375 train_time:199216ms step_avg:148.56ms
step:1352/1375 train_time:199380ms step_avg:148.57ms
step:1353/1375 train_time:199543ms step_avg:148.58ms
step:1354/1375 train_time:199701ms step_avg:148.59ms
step:1355/1375 train_time:199856ms step_avg:148.59ms
step:1356/1375 train_time:200011ms step_avg:148.60ms
step:1357/1375 train_time:200168ms step_avg:148.60ms
step:1358/1375 train_time:200328ms step_avg:148.61ms
step:1359/1375 train_time:200484ms step_avg:148.62ms
step:1360/1375 train_time:200646ms step_avg:148.63ms
step:1361/1375 train_time:200805ms step_avg:148.63ms
step:1362/1375 train_time:200965ms step_avg:148.64ms
step:1363/1375 train_time:201129ms step_avg:148.65ms
step:1364/1375 train_time:201284ms step_avg:148.66ms
step:1365/1375 train_time:201436ms step_avg:148.66ms
step:1366/1375 train_time:201592ms step_avg:148.67ms
step:1367/1375 train_time:201747ms step_avg:148.67ms
step:1368/1375 train_time:201905ms step_avg:148.68ms
step:1369/1375 train_time:202071ms step_avg:148.69ms
step:1370/1375 train_time:202229ms step_avg:148.70ms
step:1371/1375 train_time:202386ms step_avg:148.70ms
step:1372/1375 train_time:202549ms step_avg:148.71ms
step:1373/1375 train_time:202703ms step_avg:148.72ms
step:1374/1375 train_time:202862ms step_avg:148.73ms
step:1375/1375 train_time:203019ms step_avg:148.73ms
step:1375/1375 val_loss:3.2768 train_time:203097ms step_avg:148.79ms
peak memory consumption: 31565 MiB

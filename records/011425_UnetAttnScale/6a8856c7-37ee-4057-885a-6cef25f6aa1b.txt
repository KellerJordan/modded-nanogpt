import os
import sys
with open(sys.argv[0]) as f:
    code = f.read() # read the code of this file ASAP, for logging
import uuid
import time
import glob
import subprocess
import contextlib
from dataclasses import dataclass

import torch
torch.empty(1, device='cuda', requires_grad=True).backward()
from torch import nn
import torch.nn.functional as F
import torch.distributed as dist
from torch.nn.parallel import DistributedDataParallel as DDP
# use of FlexAttention contributed by @KoszarskyB
from torch.nn.attention.flex_attention import BlockMask, flex_attention

# -----------------------------------------------------------------------------
# Muon optimizer

@torch.compile
def zeropower_via_newtonschulz5(G, steps):
    """
    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a
    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose
    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at
    zero even beyond the point where the iteration no longer converges all the way to one everywhere
    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T
    where S' is diagonal with S_{ii}' ~ Uniform(0.5, 1.5), which turns out not to hurt model
    performance at all relative to UV^T, where USV^T = G is the SVD.
    """
    assert len(G.shape) == 2
    a, b, c = (3.4445, -4.7750,  2.0315)
    X = G.bfloat16()
    if G.size(0) > G.size(1):
        X = X.T

    # Ensure spectral norm is at most 1
    X = X / (X.norm() + 1e-7)
    # Perform the NS iterations
    for _ in range(steps):
        A = X @ X.T
        B = b * A + c * A @ A # adapted from suggestion by @jxbz, @leloykun, and @YouJiacheng
        X = a * X + B @ X
    
    if G.size(0) > G.size(1):
        X = X.T
    return X

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Some warnings:
    - This optimizer assumes that all parameters passed in are 2D.
    - It should not be used for the embedding layer, the final fully connected layer, or any {0,1}-D
    parameters; those should all be optimized by a standard method (e.g., AdamW).
    - To use it with 4D convolutional filters, it works well to just flatten their last 3 dimensions.
    - We believe it is unlikely to work well for training with small batch size.
    - We believe it may not work well for finetuning pretrained models, but we haven't tested this.
    - We have not yet tried this optimizer for training scenarios larger than NanoGPT (124M).

    Arguments:
        lr: The learning rate used by the internal SGD.
        momentum: The momentum used by the internal SGD.
        nesterov: Whether to use Nesterov-style momentum in the internal SGD. (recommended)
        ns_steps: The number of Newton-Schulz iteration steps to use.
    """
    def __init__(self, params, lr=0.02, momentum=0.95, nesterov=True, ns_steps=5):
        self.world_size = int(os.environ['WORLD_SIZE'])
        self.rank = int(os.environ['RANK'])
        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, ns_steps=ns_steps)
        assert all(isinstance(p, torch.Tensor) for p in params)
        sizes = {p.numel() for p in params}
        param_groups = [dict(params=[p for p in params if p.numel() == size],
                             update_buffer=[torch.empty(size, device='cuda', dtype=torch.bfloat16) for _ in range(self.world_size)])
                        for size in sizes]
        super().__init__(param_groups, defaults)

    def step(self):

        for group in self.param_groups:

            lr = group['lr']
            momentum = group['momentum']
            nesterov = group['nesterov']
            ns_steps = group['ns_steps']
            update_buffers = group['update_buffer']
            # generate weight updates in distributed fashion
            params = group['params']
            handle = None
            params_world = None
            def update_prev():
                if params_world is None:
                    return
                assert handle is not None
                handle.wait()
                for p_world, g_world in zip(params_world, update_buffers):
                    p_world.data.add_(
                        g_world.view_as(p_world),
                        alpha=-lr * max(1, p_world.size(0) / p_world.size(1)) ** 0.5,
                    )
            for base_i in range(len(params))[::self.world_size]:
                if base_i + rank < len(params):
                    p = params[base_i + self.rank]
                    g = p.grad
                    assert g is not None
                    state = self.state[p]
                    if 'momentum_buffer' not in state:
                        state['momentum_buffer'] = torch.zeros_like(g)
                    buf = state['momentum_buffer']
                    buf.lerp_(g, 1 - momentum)
                    g = g.lerp_(buf, momentum) if nesterov else buf
                    g = zeropower_via_newtonschulz5(g, steps=ns_steps).flatten()
                else:
                    g = update_buffers[rank]
                update_prev() # async all_gather instead of sync all_reduce by @YouJiacheng
                handle = dist.all_gather(update_buffers, g, async_op=True)
                params_world = params[base_i : base_i + self.world_size]
            update_prev()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the GPT-2 model

def norm(x):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):

    def __init__(self, in_features, out_features):
        super().__init__(in_features, out_features, bias=False)

    def forward(self, x):
        return F.linear(x, self.weight.type_as(x))

class Rotary(nn.Module):

    def __init__(self, dim, max_seq_len=65536):
        super().__init__()
        # half-truncate RoPE by @YouJiacheng
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=dim//4, dtype=torch.float32)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(dim//4)])
        t = torch.arange(max_seq_len, dtype=torch.float32)
        theta = torch.einsum('i,j -> ij', t, angular_freq)
        self.cos = nn.Buffer(theta.cos(), persistent=False)
        self.sin = nn.Buffer(theta.sin(), persistent=False)

    def forward(self, x):
        cos, sin = self.cos[None, :x.size(-3), None, :], self.sin[None, :x.size(-3), None, :]
        x1, x2 = x.float().chunk(2, dim=-1)
        y1 = x1 * cos + x2 * sin
        y2 = x1 * (-sin) + x2 * cos
        return torch.cat((y1, y2), 3).type_as(x)

class CausalSelfAttention(nn.Module):

    def __init__(self, dim, num_heads, layer_id):
        super().__init__()
        assert dim % num_heads == 0
        self.num_heads = num_heads
        self.layer_id = layer_id
        self.c_q = CastedLinear(dim, dim)
        self.c_k = CastedLinear(dim, dim)
        self.c_v = CastedLinear(dim, dim)
        self.lambdas = nn.Parameter(torch.tensor([0.5, 0.5]))
        self.rotary = Rotary(dim // num_heads) # dim // num_heads = head_dim
        self.c_proj = CastedLinear(dim, dim)
        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977
        # self.attn_scale = nn.Parameter(torch.tensor(1.0 / (dim // num_heads) ** 0.5))
        # self.attn_scale = nn.Parameter(torch.tensor(0.20))
        self.attn_scale = 0.13 + 0.01 * min(layer_id, 11 - layer_id)

    def forward(self, x, ve, block_mask):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, 'Must use batch size = 1 for FlexAttention'
        q = self.c_q(x).view(B, T, self.num_heads, -1)
        k = self.c_k(x).view(B, T, self.num_heads, -1)
        v = self.c_v(x).view(B, T, self.num_heads, -1)
        if ve is not None:
            v = self.lambdas[0] * v + self.lambdas[1] * ve.view_as(v) # @KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = self.lambdas[0] * v
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = self.rotary(q), self.rotary(k)
        # y = flex_attention(q.transpose(1, 2) * self.attn_scale, k.transpose(1, 2), v.transpose(1, 2), block_mask=block_mask, scale=1.)
        y = flex_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), block_mask=block_mask, scale=self.attn_scale)
        y = y.transpose(1, 2).contiguous().view_as(x) # re-assemble all head outputs side by side
        y = self.c_proj(y)
        return y

class MLP(nn.Module):

    def __init__(self, dim):
        super().__init__()
        self.c_fc = CastedLinear(dim, 4 * dim)
        self.c_proj = CastedLinear(4 * dim, dim)
        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977

    def forward(self, x):
        x = self.c_fc(x)
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = self.c_proj(x)
        return x

class Block(nn.Module):

    def __init__(self, model_dim, num_heads, layer_id, use_attn=True):
        super().__init__()
        self.attn = CausalSelfAttention(model_dim, num_heads, layer_id) if use_attn else None
        self.mlp = MLP(model_dim)
        self.lambdas = nn.Parameter(torch.tensor([1., 0.]))

    def forward(self, x, ve, x0, block_mask):
        x = self.lambdas[0] * x + self.lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), ve, block_mask)
        x = x + self.mlp(norm(x))
        return x

class ValueEmbedding(nn.Module):
    def __init__(self, vocab_size, model_dim):
        super().__init__()
        self.embed = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])

    def forward(self, inputs):
        ve = [emb(inputs).bfloat16() for emb in self.embed]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        ve = [ve[0], ve[1], ve[2], None, None, None, None, None, None, ve[0], ve[1], ve[2]]
        return ve

# -----------------------------------------------------------------------------
# The main GPT-2 model

class GPT(nn.Module):

    def __init__(self, vocab_size, num_layers, num_heads, model_dim):
        super().__init__()
        self.embed = nn.Embedding(vocab_size, model_dim)
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.blocks = nn.ModuleList([Block(model_dim, num_heads, layer_id=layer_id, use_attn=(layer_id != 7))
                                     for layer_id in range(num_layers)])
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual learning
        # U-net structure on token value embeddings by @leloykun
        self.value_embeds = ValueEmbedding(vocab_size, model_dim)
        self.lm_head = CastedLinear(model_dim, vocab_size)
        self.lm_head.weight.data.zero_() # @Grad62304977
        # U-net design by @brendanh0gan
        self.num_encoder_layers = num_layers // 2 # Half of the layers for encoder
        self.num_decoder_layers = num_layers - self.num_encoder_layers # Remaining for decoder
        # Add learnable skip connection weights for decoder layers
        self.skip_weights = nn.Parameter(torch.ones(self.num_decoder_layers))

    def forward(self, inputs, targets, sliding_window_num_blocks):
        BLOCK_SIZE = 128
        seq_len = len(inputs)
        assert seq_len % BLOCK_SIZE == 0
        total_num_blocks = seq_len // BLOCK_SIZE
        assert inputs.ndim == 1
        docs = (inputs == 50256).cumsum(0)
        docs_low = docs.view(-1, BLOCK_SIZE)[:, 0].contiguous()
        docs_high = docs.view(-1, BLOCK_SIZE)[:, -1].contiguous()

        def document_causal(b, h, q_idx, kv_idx):
            causal_mask = q_idx >= kv_idx
            document_mask = docs[q_idx] == docs[kv_idx]
            return causal_mask & document_mask

        def dense_to_ordered(dense_mask):
            num_blocks = dense_mask.sum(dim=-1, dtype=torch.int32)
            indices = dense_mask.argsort(dim=-1, descending=True, stable=True).to(torch.int32)
            return num_blocks[None, None].contiguous(), indices[None, None].contiguous()

        def create_doc_swc_block_mask(sliding_window_num_blocks):
            kv_idx = block_idx = torch.arange(total_num_blocks, dtype=torch.int32, device='cuda')
            q_idx = block_idx[:, None]
            causal_bm = q_idx >= kv_idx
            causal_full_bm = q_idx > kv_idx
            window_bm = q_idx - kv_idx < sliding_window_num_blocks
            window_full_bm = window_bm # block-wise sliding window by @YouJiacheng
            # document_bm = (docs_low[q_idx] <= docs_high[kv_idx]) & (docs_low[kv_idx] <= docs_high[q_idx])
            document_bm = (docs_low[:, None] <= docs_high) & (docs_low <= docs_high[:, None])
            document_full_bm = (docs_low[:, None] == docs_high) & (docs_low == docs_high[:, None])
            nonzero_bm = causal_bm & window_bm & document_bm
            full_bm  = causal_full_bm & window_full_bm & document_full_bm
            kv_num_blocks, kv_indices = dense_to_ordered(nonzero_bm & ~full_bm)
            full_kv_num_blocks, full_kv_indices = dense_to_ordered(full_bm)
            return BlockMask.from_kv_blocks(
                kv_num_blocks,
                kv_indices,
                full_kv_num_blocks,
                full_kv_indices,
                BLOCK_SIZE=BLOCK_SIZE,
                mask_mod=document_causal,
            )

        block_mask = create_doc_swc_block_mask(sliding_window_num_blocks)

        x0 = norm(self.embed(inputs[None]).bfloat16()) # use of norm here by @Grad62304977
        x = x0
        ve = self.value_embeds(inputs)
        assert len(ve) == len(self.blocks)
        ve_enc, ve_dec = ve[:self.num_encoder_layers], ve[self.num_encoder_layers:]

        # Store outputs for U-Net skip connections
        skip_connections = []
        # Encoder pass - process only the first half of the blocks
        for i in range(self.num_encoder_layers):
            x = self.blocks[i](x, ve_enc[i], x0, block_mask)
            skip_connections.append(x)
        # Decoder pass - process the remaining blocks with weighted skip connections
        for i in range(self.num_decoder_layers):
            x = x + self.skip_weights[i] * skip_connections.pop()
            # U-net structure on token value embeddings by @leloykun
            x = self.blocks[self.num_encoder_layers + i](x, ve_dec[i], x0, block_mask)

        x = norm(x)
        logits = self.lm_head(x)
        logits = 15 * torch.tanh(logits / 15) # @Grad62304977 added tanh softcapping, @KoszarskyB reduced it from 30 to 15
        logits = logits.float()
        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets)
        return loss

# -----------------------------------------------------------------------------
# Our own simple Distributed Data Loader

def _load_data_shard(path):
    # only reads the header, returns header data
    # header is 256 int32
    header = torch.from_file(path, False, 256, dtype=torch.int32)
    assert header[0] == 20240520, 'magic number mismatch in the data .bin file'
    assert header[1] == 1, 'unsupported version'
    num_tokens = int(header[2]) # number of tokens (claimed)
    with open(path, 'rb', buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, 'number of tokens read does not match header'
    return tokens

class DistributedDataLoader:

    def __init__(self, filename_pattern):
        self.rank = int(os.environ['RANK'])
        self.world_size = int(os.environ['WORLD_SIZE'])
        self.files = sorted(glob.glob(filename_pattern))
        self.reset()

    def reset(self):
        self.current_shard = -1
        self.advance()

    def advance(self):
        self.current_shard = (self.current_shard + 1) % len(self.files)
        self.current_position = 0
        self.tokens = _load_data_shard(self.files[self.current_shard])

    def next_batch(self, batch_size):
        assert batch_size % self.world_size == 0
        device_batch_size = batch_size // self.world_size
        # load next shard if necessary
        if self.current_position + batch_size + 1 >= len(self.tokens):
            self.advance()
        pos = self.current_position + self.rank * device_batch_size
        device_batch_tokens = self.tokens[pos:pos+device_batch_size+1]
        # advance current position
        self.current_position += batch_size
        inputs = device_batch_tokens[:-1].to(device='cuda', dtype=torch.int32, non_blocking=True)
        targets = device_batch_tokens[1:].to(device='cuda', dtype=torch.int64, non_blocking=True)
        return inputs, targets

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files = 'data/fineweb10B/fineweb_train_*.bin' # input .bin to train on
    val_files = 'data/fineweb10B/fineweb_val_*.bin' # input .bin to eval validation loss on
    val_tokens = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # optimization
    batch_size = 8*64*1024 # batch size in tokens
    num_iterations = 1375 # number of iterations to run
    cooldown_frac = 0.4 # fraction of training spent cooling down the learning rate
    bf16_embeds = True
    # evaluation and logging
    val_loss_every = 125 # every how many steps to evaluate val loss? 0 for only at the end
    # implementation
    max_device_batch_size = 64*1024 # batch size per device in tokens
    save_checkpoint = False
args = Hyperparameters()

micro_bs = args.max_device_batch_size

# set up DDP (distributed data parallel). torchrun sets this env variable
rank = int(os.environ['RANK'])
local_rank = int(os.environ['LOCAL_RANK'])
world_size = int(os.environ['WORLD_SIZE'])
assert torch.cuda.is_available()
torch.cuda.set_device(local_rank)
dist.init_process_group(backend='nccl', device_id=torch.device(local_rank))
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = uuid.uuid4()
    os.makedirs('logs', exist_ok=True)
    logfile = f'logs/{run_id}.txt'
    print(logfile)

def print0(s, console=False):
    if master_process:
        with open(logfile, 'a') as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0('='*100)
# log information about the hardware/software environment this is running on
print0(f'Running Python {sys.version}')
print0(f'Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}')
print0(subprocess.run(['nvidia-smi'], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout)
print0('='*100)

# load data
train_loader = DistributedDataLoader(args.train_files)
val_loader = DistributedDataLoader(args.val_files)
print0(f'Training dataloader files: {train_loader.files}')
print0(f'Validation dataloader files: {val_loader.files}')
print0('='*100)

# there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency. suggested to me by @Grad62304977.
# this originates from Karpathy's experiments.
model = GPT(vocab_size=50304, num_layers=12, num_heads=6, model_dim=768)
model = model.cuda()
if args.bf16_embeds:
    for m in model.modules():
        if isinstance(m, nn.Embedding):
            m.bfloat16()
model: GPT = torch.compile(model)
ddp_model = DDP(model, device_ids=[local_rank], broadcast_buffers=False, gradient_as_bucket_view=True)

# collect the parameters to optimize
hidden_matrix_params = [p for p in model.blocks.parameters() if p.ndim == 2]
embed_params = [model.embed.weight, *model.value_embeds.parameters()]
scalar_params = [p for n, p in model.named_parameters() if p.ndim < 2 and "attn_scale" not in n]
attn_scale_params = [p for n, p in model.named_parameters() if p.ndim < 2 and "attn_scale" in n]
head_params = [model.lm_head.weight]

# init the optimizer(s)
optimizer1 = torch.optim.Adam([dict(params=embed_params, lr=0.6),
                               dict(params=head_params, lr=0.008),
                               dict(params=scalar_params, lr=0.04),
                               dict(params=attn_scale_params, lr=0.01)],
                              betas=(0.8, 0.95), fused=True)
optimizer2 = Muon(hidden_matrix_params, lr=0.05, momentum=0.95)
optimizers = [optimizer1, optimizer2]

# learning rate schedule: stable then decay
def get_lr(it):
    t = 1 - it / args.num_iterations # time remaining in training
    assert 1 >= t > 0
    # 1) constant lr for first part of training
    if t >= args.cooldown_frac:
        return 1.0
    # 2) then linear cooldown
    else:
        return t / args.cooldown_frac
schedulers = [torch.optim.lr_scheduler.LambdaLR(opt, get_lr) for opt in optimizers]

# sliding window size schedule: linear increase over training in chunks of 128 from 128 -> 1792. By @fernbear.bsky.social
def get_sliding_window_blocks(it):
    x = it / args.num_iterations # training progress
    assert 0 <= x <= 1
    return int(((1 - x) * 128 + x * 1856) // 128)
sliding_window_num_blocks = torch.tensor(1, dtype=torch.int32, device='cuda')

# Start training loop
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    # This effectively ignores timing first 10 steps, which are slower for weird reasons.
    # Alternately, and slightly more correctly in terms of benchmarking, we could do 10
    # steps with dummy data first, and then re-initialize the model and reset the loader.
    if step == 10:
        training_time_ms = 0
        t0 = time.perf_counter()
    timed_steps = float('nan') if step <= 11 else (step - 10) + 1 # <= 11 to avoid bug in val

    sliding_window_num_blocks.copy_(get_sliding_window_blocks(step))

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        # run validation batches
        model.eval()
        val_loader.reset()
        val_loss = 0.0
        # calculate the number of steps to take in the val loop.
        val_batch_size = world_size * micro_bs
        assert args.val_tokens % val_batch_size == 0
        val_steps = args.val_tokens // val_batch_size
        for _ in range(val_steps):
            with torch.no_grad():
                inputs_val, targets_val = val_loader.next_batch(val_batch_size)
                val_loss += ddp_model(inputs_val, targets_val, sliding_window_num_blocks)
        # Print attention scales
        for n, p in model.named_parameters():
            if p.ndim < 2 and "attn_scale" in n:
                print0(f'{n}: {p.item()}', console=True)
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        val_loss /= val_steps
        # logging
        print0(f'step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/(timed_steps-1):.2f}ms', console=True)
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f'logs/{run_id}', exist_ok=True)
            torch.save(log, f'logs/{run_id}/state_step{step:06d}.pt')
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    model.train()
    batch_size = args.batch_size
    assert batch_size % world_size == 0
    inputs_train, targets_train = train_loader.next_batch(batch_size)
    assert len(inputs_train) <= micro_bs or len(inputs_train) % micro_bs == 0
    for micro_inputs_train, micro_targets_train in zip(inputs_train.split(micro_bs), targets_train.split(micro_bs)):
        ddp_model(micro_inputs_train, micro_targets_train, sliding_window_num_blocks).backward()
    # momentum warmup for Muon
    frac = min(step/300, 1)
    for group in optimizer2.param_groups:
        group['momentum'] = (1 - frac) * 0.85 + frac * 0.95
    # step the optimizers and schedulers
    for opt, sched in zip(optimizers, schedulers):
        opt.step()
        if step != train_steps-1:
            sched.step()
    # null the gradients
    model.zero_grad(set_to_none=True)
    # logging
    approx_time = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f'step:{step+1}/{train_steps} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms', console=True)

print0(f'peak memory consumption: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB')
dist.destroy_process_group()

====================================================================================================
Running Python 3.10.12 (main, Sep 11 2024, 15:47:36) [GCC 11.4.0]
Running PyTorch 2.6.0.dev20241231+cu126 compiled for CUDA 12.6
Tue Jan 14 15:20:05 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.183.06             Driver Version: 535.183.06   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:19:00.0 Off |                    0 |
| N/A   38C    P0             126W / 700W |   7713MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:3B:00.0 Off |                    0 |
| N/A   31C    P0             114W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:4C:00.0 Off |                    0 |
| N/A   28C    P0             117W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   37C    P0             128W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:9B:00.0 Off |                    0 |
| N/A   38C    P0             123W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:BB:00.0 Off |                    0 |
| N/A   30C    P0             114W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:CB:00.0 Off |                    0 |
| N/A   36C    P0             116W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:DB:00.0 Off |                    0 |
| N/A   29C    P0             117W / 700W |   3211MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
Training dataloader files: ['data/fineweb10B/fineweb_train_000001.bin', 'data/fineweb10B/fineweb_train_000002.bin', 'data/fineweb10B/fineweb_train_000003.bin', 'data/fineweb10B/fineweb_train_000004.bin', 'data/fineweb10B/fineweb_train_000005.bin', 'data/fineweb10B/fineweb_train_000006.bin', 'data/fineweb10B/fineweb_train_000007.bin', 'data/fineweb10B/fineweb_train_000008.bin', 'data/fineweb10B/fineweb_train_000009.bin']
Validation dataloader files: ['data/fineweb10B/fineweb_val_000000.bin']
====================================================================================================
step:0/1375 val_loss:10.8258 train_time:0ms step_avg:nanms
step:1/1375 train_time:27642ms step_avg:nanms
step:2/1375 train_time:27751ms step_avg:nanms
step:3/1375 train_time:27944ms step_avg:nanms
step:4/1375 train_time:28078ms step_avg:nanms
step:5/1375 train_time:28212ms step_avg:nanms
step:6/1375 train_time:28345ms step_avg:nanms
step:7/1375 train_time:28478ms step_avg:nanms
step:8/1375 train_time:28612ms step_avg:nanms
step:9/1375 train_time:28745ms step_avg:nanms
step:10/1375 train_time:28886ms step_avg:nanms
step:11/1375 train_time:137ms step_avg:nanms
step:12/1375 train_time:272ms step_avg:nanms
step:13/1375 train_time:408ms step_avg:135.87ms
step:14/1375 train_time:541ms step_avg:135.23ms
step:15/1375 train_time:676ms step_avg:135.25ms
step:16/1375 train_time:810ms step_avg:134.93ms
step:17/1375 train_time:946ms step_avg:135.10ms
step:18/1375 train_time:1083ms step_avg:135.42ms
step:19/1375 train_time:1221ms step_avg:135.67ms
step:20/1375 train_time:1355ms step_avg:135.50ms
step:21/1375 train_time:1490ms step_avg:135.46ms
step:22/1375 train_time:1625ms step_avg:135.43ms
step:23/1375 train_time:1761ms step_avg:135.43ms
step:24/1375 train_time:1895ms step_avg:135.35ms
step:25/1375 train_time:2032ms step_avg:135.45ms
step:26/1375 train_time:2168ms step_avg:135.47ms
step:27/1375 train_time:2302ms step_avg:135.43ms
step:28/1375 train_time:2439ms step_avg:135.47ms
step:29/1375 train_time:2574ms step_avg:135.48ms
step:30/1375 train_time:2710ms step_avg:135.49ms
step:31/1375 train_time:2843ms step_avg:135.40ms
step:32/1375 train_time:2981ms step_avg:135.48ms
step:33/1375 train_time:3116ms step_avg:135.47ms
step:34/1375 train_time:3252ms step_avg:135.50ms
step:35/1375 train_time:3388ms step_avg:135.50ms
step:36/1375 train_time:3523ms step_avg:135.49ms
step:37/1375 train_time:3659ms step_avg:135.52ms
step:38/1375 train_time:3793ms step_avg:135.47ms
step:39/1375 train_time:3928ms step_avg:135.45ms
step:40/1375 train_time:4063ms step_avg:135.42ms
step:41/1375 train_time:4198ms step_avg:135.43ms
step:42/1375 train_time:4334ms step_avg:135.44ms
step:43/1375 train_time:4471ms step_avg:135.47ms
step:44/1375 train_time:4604ms step_avg:135.41ms
step:45/1375 train_time:4739ms step_avg:135.41ms
step:46/1375 train_time:4875ms step_avg:135.43ms
step:47/1375 train_time:5011ms step_avg:135.43ms
step:48/1375 train_time:5145ms step_avg:135.39ms
step:49/1375 train_time:5282ms step_avg:135.45ms
step:50/1375 train_time:5417ms step_avg:135.43ms
step:51/1375 train_time:5553ms step_avg:135.45ms
step:52/1375 train_time:5688ms step_avg:135.43ms
step:53/1375 train_time:5822ms step_avg:135.40ms
step:54/1375 train_time:5958ms step_avg:135.41ms
step:55/1375 train_time:6092ms step_avg:135.39ms
step:56/1375 train_time:6228ms step_avg:135.40ms
step:57/1375 train_time:6363ms step_avg:135.39ms
step:58/1375 train_time:6500ms step_avg:135.41ms
step:59/1375 train_time:6635ms step_avg:135.42ms
step:60/1375 train_time:6772ms step_avg:135.43ms
step:61/1375 train_time:6906ms step_avg:135.42ms
step:62/1375 train_time:7041ms step_avg:135.39ms
step:63/1375 train_time:7176ms step_avg:135.39ms
step:64/1375 train_time:7313ms step_avg:135.42ms
step:65/1375 train_time:7449ms step_avg:135.43ms
step:66/1375 train_time:7585ms step_avg:135.44ms
step:67/1375 train_time:7720ms step_avg:135.44ms
step:68/1375 train_time:7857ms step_avg:135.46ms
step:69/1375 train_time:7990ms step_avg:135.43ms
step:70/1375 train_time:8124ms step_avg:135.40ms
step:71/1375 train_time:8259ms step_avg:135.39ms
step:72/1375 train_time:8395ms step_avg:135.41ms
step:73/1375 train_time:8531ms step_avg:135.42ms
step:74/1375 train_time:8666ms step_avg:135.41ms
step:75/1375 train_time:8802ms step_avg:135.41ms
step:76/1375 train_time:8936ms step_avg:135.39ms
step:77/1375 train_time:9071ms step_avg:135.39ms
step:78/1375 train_time:9205ms step_avg:135.37ms
step:79/1375 train_time:9341ms step_avg:135.37ms
step:80/1375 train_time:9477ms step_avg:135.38ms
step:81/1375 train_time:9613ms step_avg:135.39ms
step:82/1375 train_time:9749ms step_avg:135.40ms
step:83/1375 train_time:9885ms step_avg:135.41ms
step:84/1375 train_time:10020ms step_avg:135.41ms
step:85/1375 train_time:10156ms step_avg:135.41ms
step:86/1375 train_time:10291ms step_avg:135.41ms
step:87/1375 train_time:10427ms step_avg:135.41ms
step:88/1375 train_time:10562ms step_avg:135.42ms
step:89/1375 train_time:10699ms step_avg:135.43ms
step:90/1375 train_time:10834ms step_avg:135.42ms
step:91/1375 train_time:10971ms step_avg:135.44ms
step:92/1375 train_time:11106ms step_avg:135.44ms
step:93/1375 train_time:11241ms step_avg:135.43ms
step:94/1375 train_time:11378ms step_avg:135.45ms
step:95/1375 train_time:11514ms step_avg:135.46ms
step:96/1375 train_time:11650ms step_avg:135.46ms
step:97/1375 train_time:11786ms step_avg:135.47ms
step:98/1375 train_time:11922ms step_avg:135.48ms
step:99/1375 train_time:12060ms step_avg:135.50ms
step:100/1375 train_time:12196ms step_avg:135.51ms
step:101/1375 train_time:12333ms step_avg:135.53ms
step:102/1375 train_time:12467ms step_avg:135.51ms
step:103/1375 train_time:12603ms step_avg:135.52ms
step:104/1375 train_time:12743ms step_avg:135.57ms
step:105/1375 train_time:12882ms step_avg:135.60ms
step:106/1375 train_time:13021ms step_avg:135.63ms
step:107/1375 train_time:13160ms step_avg:135.67ms
step:108/1375 train_time:13300ms step_avg:135.71ms
step:109/1375 train_time:13440ms step_avg:135.76ms
step:110/1375 train_time:13579ms step_avg:135.79ms
step:111/1375 train_time:13717ms step_avg:135.81ms
step:112/1375 train_time:13856ms step_avg:135.85ms
step:113/1375 train_time:13994ms step_avg:135.87ms
step:114/1375 train_time:14134ms step_avg:135.90ms
step:115/1375 train_time:14273ms step_avg:135.94ms
step:116/1375 train_time:14413ms step_avg:135.97ms
step:117/1375 train_time:14553ms step_avg:136.01ms
step:118/1375 train_time:14691ms step_avg:136.03ms
step:119/1375 train_time:14831ms step_avg:136.06ms
step:120/1375 train_time:14972ms step_avg:136.11ms
step:121/1375 train_time:15112ms step_avg:136.14ms
step:122/1375 train_time:15251ms step_avg:136.17ms
step:123/1375 train_time:15390ms step_avg:136.20ms
step:124/1375 train_time:15530ms step_avg:136.23ms
step:125/1375 train_time:15668ms step_avg:136.25ms
step:125/1375 val_loss:4.3722 train_time:15737ms step_avg:136.84ms
step:126/1375 train_time:15810ms step_avg:136.29ms
step:127/1375 train_time:15953ms step_avg:136.35ms
step:128/1375 train_time:16092ms step_avg:136.37ms
step:129/1375 train_time:16231ms step_avg:136.40ms
step:130/1375 train_time:16370ms step_avg:136.41ms
step:131/1375 train_time:16507ms step_avg:136.42ms
step:132/1375 train_time:16647ms step_avg:136.45ms
step:133/1375 train_time:16788ms step_avg:136.49ms
step:134/1375 train_time:16927ms step_avg:136.51ms
step:135/1375 train_time:17066ms step_avg:136.53ms
step:136/1375 train_time:17206ms step_avg:136.55ms
step:137/1375 train_time:17346ms step_avg:136.58ms
step:138/1375 train_time:17485ms step_avg:136.60ms
step:139/1375 train_time:17624ms step_avg:136.62ms
step:140/1375 train_time:17764ms step_avg:136.65ms
step:141/1375 train_time:17905ms step_avg:136.68ms
step:142/1375 train_time:18045ms step_avg:136.70ms
step:143/1375 train_time:18184ms step_avg:136.72ms
step:144/1375 train_time:18323ms step_avg:136.74ms
step:145/1375 train_time:18462ms step_avg:136.75ms
step:146/1375 train_time:18599ms step_avg:136.76ms
step:147/1375 train_time:18741ms step_avg:136.79ms
step:148/1375 train_time:18879ms step_avg:136.81ms
step:149/1375 train_time:19019ms step_avg:136.83ms
step:150/1375 train_time:19159ms step_avg:136.85ms
step:151/1375 train_time:19298ms step_avg:136.87ms
step:152/1375 train_time:19438ms step_avg:136.89ms
step:153/1375 train_time:19577ms step_avg:136.90ms
step:154/1375 train_time:19716ms step_avg:136.92ms
step:155/1375 train_time:19855ms step_avg:136.93ms
step:156/1375 train_time:19995ms step_avg:136.95ms
step:157/1375 train_time:20134ms step_avg:136.97ms
step:158/1375 train_time:20275ms step_avg:136.99ms
step:159/1375 train_time:20415ms step_avg:137.01ms
step:160/1375 train_time:20555ms step_avg:137.03ms
step:161/1375 train_time:20695ms step_avg:137.05ms
step:162/1375 train_time:20833ms step_avg:137.06ms
step:163/1375 train_time:20975ms step_avg:137.09ms
step:164/1375 train_time:21114ms step_avg:137.10ms
step:165/1375 train_time:21253ms step_avg:137.12ms
step:166/1375 train_time:21393ms step_avg:137.14ms
step:167/1375 train_time:21532ms step_avg:137.15ms
step:168/1375 train_time:21673ms step_avg:137.17ms
step:169/1375 train_time:21812ms step_avg:137.18ms
step:170/1375 train_time:21952ms step_avg:137.20ms
step:171/1375 train_time:22093ms step_avg:137.22ms
step:172/1375 train_time:22233ms step_avg:137.24ms
step:173/1375 train_time:22376ms step_avg:137.27ms
step:174/1375 train_time:22516ms step_avg:137.29ms
step:175/1375 train_time:22655ms step_avg:137.30ms
step:176/1375 train_time:22796ms step_avg:137.32ms
step:177/1375 train_time:22935ms step_avg:137.33ms
step:178/1375 train_time:23076ms step_avg:137.35ms
step:179/1375 train_time:23216ms step_avg:137.37ms
step:180/1375 train_time:23356ms step_avg:137.39ms
step:181/1375 train_time:23496ms step_avg:137.40ms
step:182/1375 train_time:23635ms step_avg:137.41ms
step:183/1375 train_time:23776ms step_avg:137.43ms
step:184/1375 train_time:23916ms step_avg:137.45ms
step:185/1375 train_time:24056ms step_avg:137.46ms
step:186/1375 train_time:24196ms step_avg:137.48ms
step:187/1375 train_time:24335ms step_avg:137.48ms
step:188/1375 train_time:24477ms step_avg:137.51ms
step:189/1375 train_time:24617ms step_avg:137.52ms
step:190/1375 train_time:24756ms step_avg:137.53ms
step:191/1375 train_time:24929ms step_avg:137.73ms
step:192/1375 train_time:25067ms step_avg:137.73ms
step:193/1375 train_time:25206ms step_avg:137.74ms
step:194/1375 train_time:25345ms step_avg:137.74ms
step:195/1375 train_time:25482ms step_avg:137.74ms
step:196/1375 train_time:25620ms step_avg:137.74ms
step:197/1375 train_time:25760ms step_avg:137.75ms
step:198/1375 train_time:25902ms step_avg:137.78ms
step:199/1375 train_time:26043ms step_avg:137.80ms
step:200/1375 train_time:26182ms step_avg:137.80ms
step:201/1375 train_time:26321ms step_avg:137.81ms
step:202/1375 train_time:26460ms step_avg:137.81ms
step:203/1375 train_time:26598ms step_avg:137.82ms
step:204/1375 train_time:26737ms step_avg:137.82ms
step:205/1375 train_time:26878ms step_avg:137.84ms
step:206/1375 train_time:27021ms step_avg:137.86ms
step:207/1375 train_time:27163ms step_avg:137.88ms
step:208/1375 train_time:27307ms step_avg:137.91ms
step:209/1375 train_time:27449ms step_avg:137.93ms
step:210/1375 train_time:27590ms step_avg:137.95ms
step:211/1375 train_time:27731ms step_avg:137.97ms
step:212/1375 train_time:27874ms step_avg:137.99ms
step:213/1375 train_time:28016ms step_avg:138.01ms
step:214/1375 train_time:28159ms step_avg:138.03ms
step:215/1375 train_time:28302ms step_avg:138.06ms
step:216/1375 train_time:28445ms step_avg:138.08ms
step:217/1375 train_time:28588ms step_avg:138.10ms
step:218/1375 train_time:28729ms step_avg:138.12ms
step:219/1375 train_time:28869ms step_avg:138.13ms
step:220/1375 train_time:29011ms step_avg:138.15ms
step:221/1375 train_time:29154ms step_avg:138.17ms
step:222/1375 train_time:29298ms step_avg:138.20ms
step:223/1375 train_time:29441ms step_avg:138.22ms
step:224/1375 train_time:29583ms step_avg:138.24ms
step:225/1375 train_time:29726ms step_avg:138.26ms
step:226/1375 train_time:29867ms step_avg:138.27ms
step:227/1375 train_time:30009ms step_avg:138.29ms
step:228/1375 train_time:30152ms step_avg:138.31ms
step:229/1375 train_time:30296ms step_avg:138.34ms
step:230/1375 train_time:30437ms step_avg:138.35ms
step:231/1375 train_time:30580ms step_avg:138.37ms
step:232/1375 train_time:30722ms step_avg:138.39ms
step:233/1375 train_time:30865ms step_avg:138.41ms
step:234/1375 train_time:31006ms step_avg:138.42ms
step:235/1375 train_time:31148ms step_avg:138.44ms
step:236/1375 train_time:31290ms step_avg:138.45ms
step:237/1375 train_time:31433ms step_avg:138.47ms
step:238/1375 train_time:31577ms step_avg:138.50ms
step:239/1375 train_time:31719ms step_avg:138.51ms
step:240/1375 train_time:31861ms step_avg:138.52ms
step:241/1375 train_time:32002ms step_avg:138.54ms
step:242/1375 train_time:32146ms step_avg:138.56ms
step:243/1375 train_time:32287ms step_avg:138.57ms
step:244/1375 train_time:32430ms step_avg:138.59ms
step:245/1375 train_time:32574ms step_avg:138.61ms
step:246/1375 train_time:32716ms step_avg:138.63ms
step:247/1375 train_time:32857ms step_avg:138.64ms
step:248/1375 train_time:32998ms step_avg:138.65ms
step:249/1375 train_time:33142ms step_avg:138.67ms
step:250/1375 train_time:33285ms step_avg:138.69ms
step:250/1375 val_loss:3.9650 train_time:33355ms step_avg:138.98ms
step:251/1375 train_time:33431ms step_avg:138.72ms
step:252/1375 train_time:33573ms step_avg:138.73ms
step:253/1375 train_time:33716ms step_avg:138.75ms
step:254/1375 train_time:33859ms step_avg:138.77ms
step:255/1375 train_time:34000ms step_avg:138.77ms
step:256/1375 train_time:34140ms step_avg:138.78ms
step:257/1375 train_time:34281ms step_avg:138.79ms
step:258/1375 train_time:34426ms step_avg:138.81ms
step:259/1375 train_time:34569ms step_avg:138.83ms
step:260/1375 train_time:34709ms step_avg:138.84ms
step:261/1375 train_time:34851ms step_avg:138.85ms
step:262/1375 train_time:34992ms step_avg:138.86ms
step:263/1375 train_time:35134ms step_avg:138.87ms
step:264/1375 train_time:35276ms step_avg:138.88ms
step:265/1375 train_time:35420ms step_avg:138.90ms
step:266/1375 train_time:35563ms step_avg:138.92ms
step:267/1375 train_time:35706ms step_avg:138.93ms
step:268/1375 train_time:35847ms step_avg:138.94ms
step:269/1375 train_time:35990ms step_avg:138.96ms
step:270/1375 train_time:36131ms step_avg:138.96ms
step:271/1375 train_time:36272ms step_avg:138.97ms
step:272/1375 train_time:36416ms step_avg:138.99ms
step:273/1375 train_time:36559ms step_avg:139.01ms
step:274/1375 train_time:36701ms step_avg:139.02ms
step:275/1375 train_time:36843ms step_avg:139.03ms
step:276/1375 train_time:36986ms step_avg:139.04ms
step:277/1375 train_time:37129ms step_avg:139.06ms
step:278/1375 train_time:37270ms step_avg:139.07ms
step:279/1375 train_time:37411ms step_avg:139.07ms
step:280/1375 train_time:37554ms step_avg:139.09ms
step:281/1375 train_time:37696ms step_avg:139.10ms
step:282/1375 train_time:37840ms step_avg:139.12ms
step:283/1375 train_time:37981ms step_avg:139.13ms
step:284/1375 train_time:38124ms step_avg:139.14ms
step:285/1375 train_time:38268ms step_avg:139.16ms
step:286/1375 train_time:38409ms step_avg:139.16ms
step:287/1375 train_time:38552ms step_avg:139.18ms
step:288/1375 train_time:38694ms step_avg:139.19ms
step:289/1375 train_time:38837ms step_avg:139.20ms
step:290/1375 train_time:38979ms step_avg:139.21ms
step:291/1375 train_time:39121ms step_avg:139.22ms
step:292/1375 train_time:39262ms step_avg:139.23ms
step:293/1375 train_time:39405ms step_avg:139.24ms
step:294/1375 train_time:39548ms step_avg:139.25ms
step:295/1375 train_time:39693ms step_avg:139.27ms
step:296/1375 train_time:39836ms step_avg:139.29ms
step:297/1375 train_time:39979ms step_avg:139.30ms
step:298/1375 train_time:40121ms step_avg:139.31ms
step:299/1375 train_time:40263ms step_avg:139.32ms
step:300/1375 train_time:40406ms step_avg:139.33ms
step:301/1375 train_time:40546ms step_avg:139.33ms
step:302/1375 train_time:40690ms step_avg:139.35ms
step:303/1375 train_time:40832ms step_avg:139.36ms
step:304/1375 train_time:40976ms step_avg:139.38ms
step:305/1375 train_time:41118ms step_avg:139.38ms
step:306/1375 train_time:41260ms step_avg:139.39ms
step:307/1375 train_time:41402ms step_avg:139.40ms
step:308/1375 train_time:41545ms step_avg:139.41ms
step:309/1375 train_time:41691ms step_avg:139.43ms
step:310/1375 train_time:41835ms step_avg:139.45ms
step:311/1375 train_time:41981ms step_avg:139.47ms
step:312/1375 train_time:42127ms step_avg:139.49ms
step:313/1375 train_time:42270ms step_avg:139.51ms
step:314/1375 train_time:42413ms step_avg:139.52ms
step:315/1375 train_time:42557ms step_avg:139.53ms
step:316/1375 train_time:42703ms step_avg:139.55ms
step:317/1375 train_time:42848ms step_avg:139.57ms
step:318/1375 train_time:42991ms step_avg:139.58ms
step:319/1375 train_time:43138ms step_avg:139.60ms
step:320/1375 train_time:43282ms step_avg:139.62ms
step:321/1375 train_time:43426ms step_avg:139.63ms
step:322/1375 train_time:43570ms step_avg:139.65ms
step:323/1375 train_time:43713ms step_avg:139.66ms
step:324/1375 train_time:43857ms step_avg:139.67ms
step:325/1375 train_time:44003ms step_avg:139.69ms
step:326/1375 train_time:44146ms step_avg:139.70ms
step:327/1375 train_time:44290ms step_avg:139.71ms
step:328/1375 train_time:44433ms step_avg:139.73ms
step:329/1375 train_time:44577ms step_avg:139.74ms
step:330/1375 train_time:44723ms step_avg:139.76ms
step:331/1375 train_time:44868ms step_avg:139.78ms
step:332/1375 train_time:45011ms step_avg:139.79ms
step:333/1375 train_time:45156ms step_avg:139.80ms
step:334/1375 train_time:45300ms step_avg:139.82ms
step:335/1375 train_time:45445ms step_avg:139.83ms
step:336/1375 train_time:45590ms step_avg:139.85ms
step:337/1375 train_time:45734ms step_avg:139.86ms
step:338/1375 train_time:45877ms step_avg:139.87ms
step:339/1375 train_time:46023ms step_avg:139.89ms
step:340/1375 train_time:46169ms step_avg:139.91ms
step:341/1375 train_time:46313ms step_avg:139.92ms
step:342/1375 train_time:46457ms step_avg:139.93ms
step:343/1375 train_time:46602ms step_avg:139.95ms
step:344/1375 train_time:46744ms step_avg:139.95ms
step:345/1375 train_time:46889ms step_avg:139.97ms
step:346/1375 train_time:47032ms step_avg:139.98ms
step:347/1375 train_time:47176ms step_avg:139.99ms
step:348/1375 train_time:47322ms step_avg:140.00ms
step:349/1375 train_time:47467ms step_avg:140.02ms
step:350/1375 train_time:47612ms step_avg:140.03ms
step:351/1375 train_time:47756ms step_avg:140.05ms
step:352/1375 train_time:47902ms step_avg:140.06ms
step:353/1375 train_time:48046ms step_avg:140.08ms
step:354/1375 train_time:48191ms step_avg:140.09ms
step:355/1375 train_time:48335ms step_avg:140.10ms
step:356/1375 train_time:48478ms step_avg:140.11ms
step:357/1375 train_time:48624ms step_avg:140.13ms
step:358/1375 train_time:48768ms step_avg:140.14ms
step:359/1375 train_time:48911ms step_avg:140.15ms
step:360/1375 train_time:49055ms step_avg:140.16ms
step:361/1375 train_time:49201ms step_avg:140.17ms
step:362/1375 train_time:49345ms step_avg:140.18ms
step:363/1375 train_time:49489ms step_avg:140.19ms
step:364/1375 train_time:49632ms step_avg:140.20ms
step:365/1375 train_time:49777ms step_avg:140.22ms
step:366/1375 train_time:49923ms step_avg:140.23ms
step:367/1375 train_time:50067ms step_avg:140.24ms
step:368/1375 train_time:50211ms step_avg:140.25ms
step:369/1375 train_time:50354ms step_avg:140.26ms
step:370/1375 train_time:50498ms step_avg:140.27ms
step:371/1375 train_time:50643ms step_avg:140.28ms
step:372/1375 train_time:50787ms step_avg:140.30ms
step:373/1375 train_time:50930ms step_avg:140.30ms
step:374/1375 train_time:51075ms step_avg:140.31ms
step:375/1375 train_time:51220ms step_avg:140.33ms
step:375/1375 val_loss:3.7811 train_time:51291ms step_avg:140.52ms
step:376/1375 train_time:51367ms step_avg:140.35ms
step:377/1375 train_time:51512ms step_avg:140.36ms
step:378/1375 train_time:51658ms step_avg:140.37ms
step:379/1375 train_time:51801ms step_avg:140.38ms
step:380/1375 train_time:51945ms step_avg:140.39ms
step:381/1375 train_time:52130ms step_avg:140.51ms
step:382/1375 train_time:52272ms step_avg:140.52ms
step:383/1375 train_time:52414ms step_avg:140.52ms
step:384/1375 train_time:52556ms step_avg:140.52ms
step:385/1375 train_time:52698ms step_avg:140.53ms
step:386/1375 train_time:52842ms step_avg:140.54ms
step:387/1375 train_time:52986ms step_avg:140.55ms
step:388/1375 train_time:53133ms step_avg:140.56ms
step:389/1375 train_time:53277ms step_avg:140.57ms
step:390/1375 train_time:53422ms step_avg:140.58ms
step:391/1375 train_time:53567ms step_avg:140.60ms
step:392/1375 train_time:53709ms step_avg:140.60ms
step:393/1375 train_time:53851ms step_avg:140.60ms
step:394/1375 train_time:53996ms step_avg:140.61ms
step:395/1375 train_time:54141ms step_avg:140.63ms
step:396/1375 train_time:54286ms step_avg:140.64ms
step:397/1375 train_time:54430ms step_avg:140.65ms
step:398/1375 train_time:54574ms step_avg:140.65ms
step:399/1375 train_time:54717ms step_avg:140.66ms
step:400/1375 train_time:54862ms step_avg:140.67ms
step:401/1375 train_time:55008ms step_avg:140.69ms
step:402/1375 train_time:55152ms step_avg:140.69ms
step:403/1375 train_time:55296ms step_avg:140.70ms
step:404/1375 train_time:55440ms step_avg:140.71ms
step:405/1375 train_time:55585ms step_avg:140.72ms
step:406/1375 train_time:55730ms step_avg:140.73ms
step:407/1375 train_time:55873ms step_avg:140.74ms
step:408/1375 train_time:56017ms step_avg:140.75ms
step:409/1375 train_time:56162ms step_avg:140.76ms
step:410/1375 train_time:56309ms step_avg:140.77ms
step:411/1375 train_time:56454ms step_avg:140.78ms
step:412/1375 train_time:56599ms step_avg:140.79ms
step:413/1375 train_time:56746ms step_avg:140.81ms
step:414/1375 train_time:56892ms step_avg:140.82ms
step:415/1375 train_time:57039ms step_avg:140.84ms
step:416/1375 train_time:57186ms step_avg:140.85ms
step:417/1375 train_time:57334ms step_avg:140.87ms
step:418/1375 train_time:57479ms step_avg:140.88ms
step:419/1375 train_time:57625ms step_avg:140.89ms
step:420/1375 train_time:57771ms step_avg:140.91ms
step:421/1375 train_time:57916ms step_avg:140.92ms
step:422/1375 train_time:58063ms step_avg:140.93ms
step:423/1375 train_time:58210ms step_avg:140.94ms
step:424/1375 train_time:58355ms step_avg:140.95ms
step:425/1375 train_time:58500ms step_avg:140.96ms
step:426/1375 train_time:58647ms step_avg:140.98ms
step:427/1375 train_time:58793ms step_avg:140.99ms
step:428/1375 train_time:58940ms step_avg:141.00ms
step:429/1375 train_time:59087ms step_avg:141.02ms
step:430/1375 train_time:59233ms step_avg:141.03ms
step:431/1375 train_time:59379ms step_avg:141.04ms
step:432/1375 train_time:59524ms step_avg:141.05ms
step:433/1375 train_time:59671ms step_avg:141.07ms
step:434/1375 train_time:59816ms step_avg:141.07ms
step:435/1375 train_time:59964ms step_avg:141.09ms
step:436/1375 train_time:60110ms step_avg:141.10ms
step:437/1375 train_time:60255ms step_avg:141.11ms
step:438/1375 train_time:60401ms step_avg:141.12ms
step:439/1375 train_time:60548ms step_avg:141.14ms
step:440/1375 train_time:60693ms step_avg:141.15ms
step:441/1375 train_time:60838ms step_avg:141.15ms
step:442/1375 train_time:60985ms step_avg:141.17ms
step:443/1375 train_time:61132ms step_avg:141.18ms
step:444/1375 train_time:61277ms step_avg:141.19ms
step:445/1375 train_time:61425ms step_avg:141.21ms
step:446/1375 train_time:61572ms step_avg:141.22ms
step:447/1375 train_time:61717ms step_avg:141.23ms
step:448/1375 train_time:61864ms step_avg:141.24ms
step:449/1375 train_time:62011ms step_avg:141.25ms
step:450/1375 train_time:62155ms step_avg:141.26ms
step:451/1375 train_time:62301ms step_avg:141.27ms
step:452/1375 train_time:62449ms step_avg:141.29ms
step:453/1375 train_time:62595ms step_avg:141.30ms
step:454/1375 train_time:62741ms step_avg:141.31ms
step:455/1375 train_time:62888ms step_avg:141.32ms
step:456/1375 train_time:63033ms step_avg:141.33ms
step:457/1375 train_time:63178ms step_avg:141.34ms
step:458/1375 train_time:63325ms step_avg:141.35ms
step:459/1375 train_time:63472ms step_avg:141.36ms
step:460/1375 train_time:63617ms step_avg:141.37ms
step:461/1375 train_time:63764ms step_avg:141.38ms
step:462/1375 train_time:63911ms step_avg:141.40ms
step:463/1375 train_time:64055ms step_avg:141.40ms
step:464/1375 train_time:64201ms step_avg:141.41ms
step:465/1375 train_time:64348ms step_avg:141.42ms
step:466/1375 train_time:64494ms step_avg:141.43ms
step:467/1375 train_time:64641ms step_avg:141.45ms
step:468/1375 train_time:64787ms step_avg:141.46ms
step:469/1375 train_time:64932ms step_avg:141.46ms
step:470/1375 train_time:65075ms step_avg:141.47ms
step:471/1375 train_time:65221ms step_avg:141.48ms
step:472/1375 train_time:65371ms step_avg:141.50ms
step:473/1375 train_time:65516ms step_avg:141.50ms
step:474/1375 train_time:65662ms step_avg:141.51ms
step:475/1375 train_time:65809ms step_avg:141.52ms
step:476/1375 train_time:65954ms step_avg:141.53ms
step:477/1375 train_time:66099ms step_avg:141.54ms
step:478/1375 train_time:66244ms step_avg:141.55ms
step:479/1375 train_time:66391ms step_avg:141.56ms
step:480/1375 train_time:66536ms step_avg:141.57ms
step:481/1375 train_time:66685ms step_avg:141.58ms
step:482/1375 train_time:66831ms step_avg:141.59ms
step:483/1375 train_time:66976ms step_avg:141.60ms
step:484/1375 train_time:67121ms step_avg:141.61ms
step:485/1375 train_time:67268ms step_avg:141.62ms
step:486/1375 train_time:67412ms step_avg:141.62ms
step:487/1375 train_time:67557ms step_avg:141.63ms
step:488/1375 train_time:67701ms step_avg:141.63ms
step:489/1375 train_time:67848ms step_avg:141.65ms
step:490/1375 train_time:67996ms step_avg:141.66ms
step:491/1375 train_time:68144ms step_avg:141.67ms
step:492/1375 train_time:68290ms step_avg:141.68ms
step:493/1375 train_time:68434ms step_avg:141.68ms
step:494/1375 train_time:68578ms step_avg:141.69ms
step:495/1375 train_time:68724ms step_avg:141.70ms
step:496/1375 train_time:68872ms step_avg:141.71ms
step:497/1375 train_time:69017ms step_avg:141.72ms
step:498/1375 train_time:69165ms step_avg:141.73ms
step:499/1375 train_time:69311ms step_avg:141.74ms
step:500/1375 train_time:69455ms step_avg:141.75ms
step:500/1375 val_loss:3.6625 train_time:69528ms step_avg:141.89ms
step:501/1375 train_time:69603ms step_avg:141.76ms
step:502/1375 train_time:69750ms step_avg:141.77ms
step:503/1375 train_time:69896ms step_avg:141.78ms
step:504/1375 train_time:70042ms step_avg:141.78ms
step:505/1375 train_time:70187ms step_avg:141.79ms
step:506/1375 train_time:70332ms step_avg:141.80ms
step:507/1375 train_time:70479ms step_avg:141.81ms
step:508/1375 train_time:70628ms step_avg:141.82ms
step:509/1375 train_time:70775ms step_avg:141.83ms
step:510/1375 train_time:70922ms step_avg:141.84ms
step:511/1375 train_time:71067ms step_avg:141.85ms
step:512/1375 train_time:71215ms step_avg:141.86ms
step:513/1375 train_time:71364ms step_avg:141.88ms
step:514/1375 train_time:71509ms step_avg:141.88ms
step:515/1375 train_time:71659ms step_avg:141.90ms
step:516/1375 train_time:71807ms step_avg:141.91ms
step:517/1375 train_time:71956ms step_avg:141.93ms
step:518/1375 train_time:72104ms step_avg:141.94ms
step:519/1375 train_time:72251ms step_avg:141.95ms
step:520/1375 train_time:72399ms step_avg:141.96ms
step:521/1375 train_time:72546ms step_avg:141.97ms
step:522/1375 train_time:72695ms step_avg:141.98ms
step:523/1375 train_time:72843ms step_avg:141.99ms
step:524/1375 train_time:72991ms step_avg:142.01ms
step:525/1375 train_time:73140ms step_avg:142.02ms
step:526/1375 train_time:73288ms step_avg:142.03ms
step:527/1375 train_time:73435ms step_avg:142.04ms
step:528/1375 train_time:73584ms step_avg:142.05ms
step:529/1375 train_time:73730ms step_avg:142.06ms
step:530/1375 train_time:73879ms step_avg:142.07ms
step:531/1375 train_time:74027ms step_avg:142.09ms
step:532/1375 train_time:74175ms step_avg:142.10ms
step:533/1375 train_time:74324ms step_avg:142.11ms
step:534/1375 train_time:74471ms step_avg:142.12ms
step:535/1375 train_time:74620ms step_avg:142.13ms
step:536/1375 train_time:74768ms step_avg:142.14ms
step:537/1375 train_time:74916ms step_avg:142.15ms
step:538/1375 train_time:75064ms step_avg:142.17ms
step:539/1375 train_time:75211ms step_avg:142.18ms
step:540/1375 train_time:75360ms step_avg:142.19ms
step:541/1375 train_time:75506ms step_avg:142.20ms
step:542/1375 train_time:75654ms step_avg:142.21ms
step:543/1375 train_time:75803ms step_avg:142.22ms
step:544/1375 train_time:75947ms step_avg:142.22ms
step:545/1375 train_time:76094ms step_avg:142.23ms
step:546/1375 train_time:76243ms step_avg:142.24ms
step:547/1375 train_time:76390ms step_avg:142.25ms
step:548/1375 train_time:76539ms step_avg:142.27ms
step:549/1375 train_time:76686ms step_avg:142.28ms
step:550/1375 train_time:76834ms step_avg:142.28ms
step:551/1375 train_time:76981ms step_avg:142.29ms
step:552/1375 train_time:77128ms step_avg:142.30ms
step:553/1375 train_time:77276ms step_avg:142.31ms
step:554/1375 train_time:77424ms step_avg:142.32ms
step:555/1375 train_time:77570ms step_avg:142.33ms
step:556/1375 train_time:77718ms step_avg:142.34ms
step:557/1375 train_time:77866ms step_avg:142.35ms
step:558/1375 train_time:78012ms step_avg:142.36ms
step:559/1375 train_time:78160ms step_avg:142.37ms
step:560/1375 train_time:78308ms step_avg:142.38ms
step:561/1375 train_time:78456ms step_avg:142.39ms
step:562/1375 train_time:78604ms step_avg:142.40ms
step:563/1375 train_time:78751ms step_avg:142.41ms
step:564/1375 train_time:78898ms step_avg:142.42ms
step:565/1375 train_time:79045ms step_avg:142.42ms
step:566/1375 train_time:79193ms step_avg:142.43ms
step:567/1375 train_time:79341ms step_avg:142.44ms
step:568/1375 train_time:79488ms step_avg:142.45ms
step:569/1375 train_time:79636ms step_avg:142.46ms
step:570/1375 train_time:79784ms step_avg:142.47ms
step:571/1375 train_time:79970ms step_avg:142.55ms
step:572/1375 train_time:80118ms step_avg:142.56ms
step:573/1375 train_time:80266ms step_avg:142.57ms
step:574/1375 train_time:80413ms step_avg:142.58ms
step:575/1375 train_time:80562ms step_avg:142.59ms
step:576/1375 train_time:80707ms step_avg:142.59ms
step:577/1375 train_time:80854ms step_avg:142.60ms
step:578/1375 train_time:81008ms step_avg:142.62ms
step:579/1375 train_time:81157ms step_avg:142.63ms
step:580/1375 train_time:81305ms step_avg:142.64ms
step:581/1375 train_time:81450ms step_avg:142.65ms
step:582/1375 train_time:81597ms step_avg:142.65ms
step:583/1375 train_time:81744ms step_avg:142.66ms
step:584/1375 train_time:81891ms step_avg:142.67ms
step:585/1375 train_time:82041ms step_avg:142.68ms
step:586/1375 train_time:82189ms step_avg:142.69ms
step:587/1375 train_time:82338ms step_avg:142.70ms
step:588/1375 train_time:82485ms step_avg:142.71ms
step:589/1375 train_time:82631ms step_avg:142.71ms
step:590/1375 train_time:82779ms step_avg:142.72ms
step:591/1375 train_time:82927ms step_avg:142.73ms
step:592/1375 train_time:83077ms step_avg:142.74ms
step:593/1375 train_time:83226ms step_avg:142.75ms
step:594/1375 train_time:83373ms step_avg:142.76ms
step:595/1375 train_time:83522ms step_avg:142.77ms
step:596/1375 train_time:83668ms step_avg:142.78ms
step:597/1375 train_time:83814ms step_avg:142.78ms
step:598/1375 train_time:83964ms step_avg:142.80ms
step:599/1375 train_time:84110ms step_avg:142.80ms
step:600/1375 train_time:84258ms step_avg:142.81ms
step:601/1375 train_time:84405ms step_avg:142.82ms
step:602/1375 train_time:84552ms step_avg:142.82ms
step:603/1375 train_time:84701ms step_avg:142.83ms
step:604/1375 train_time:84846ms step_avg:142.84ms
step:605/1375 train_time:84994ms step_avg:142.85ms
step:606/1375 train_time:85144ms step_avg:142.86ms
step:607/1375 train_time:85291ms step_avg:142.87ms
step:608/1375 train_time:85439ms step_avg:142.87ms
step:609/1375 train_time:85586ms step_avg:142.88ms
step:610/1375 train_time:85732ms step_avg:142.89ms
step:611/1375 train_time:85879ms step_avg:142.89ms
step:612/1375 train_time:86027ms step_avg:142.90ms
step:613/1375 train_time:86175ms step_avg:142.91ms
step:614/1375 train_time:86325ms step_avg:142.92ms
step:615/1375 train_time:86472ms step_avg:142.93ms
step:616/1375 train_time:86623ms step_avg:142.94ms
step:617/1375 train_time:86769ms step_avg:142.95ms
step:618/1375 train_time:86918ms step_avg:142.96ms
step:619/1375 train_time:87069ms step_avg:142.97ms
step:620/1375 train_time:87219ms step_avg:142.98ms
step:621/1375 train_time:87368ms step_avg:142.99ms
step:622/1375 train_time:87518ms step_avg:143.00ms
step:623/1375 train_time:87667ms step_avg:143.01ms
step:624/1375 train_time:87815ms step_avg:143.02ms
step:625/1375 train_time:87965ms step_avg:143.03ms
step:625/1375 val_loss:3.5809 train_time:88039ms step_avg:143.15ms
step:626/1375 train_time:88115ms step_avg:143.04ms
step:627/1375 train_time:88264ms step_avg:143.05ms
step:628/1375 train_time:88412ms step_avg:143.06ms
step:629/1375 train_time:88560ms step_avg:143.07ms
step:630/1375 train_time:88708ms step_avg:143.08ms
step:631/1375 train_time:88856ms step_avg:143.08ms
step:632/1375 train_time:89004ms step_avg:143.09ms
step:633/1375 train_time:89155ms step_avg:143.11ms
step:634/1375 train_time:89302ms step_avg:143.11ms
step:635/1375 train_time:89452ms step_avg:143.12ms
step:636/1375 train_time:89601ms step_avg:143.13ms
step:637/1375 train_time:89749ms step_avg:143.14ms
step:638/1375 train_time:89899ms step_avg:143.15ms
step:639/1375 train_time:90046ms step_avg:143.16ms
step:640/1375 train_time:90197ms step_avg:143.17ms
step:641/1375 train_time:90346ms step_avg:143.18ms
step:642/1375 train_time:90497ms step_avg:143.19ms
step:643/1375 train_time:90645ms step_avg:143.20ms
step:644/1375 train_time:90795ms step_avg:143.21ms
step:645/1375 train_time:90943ms step_avg:143.22ms
step:646/1375 train_time:91092ms step_avg:143.23ms
step:647/1375 train_time:91241ms step_avg:143.24ms
step:648/1375 train_time:91391ms step_avg:143.25ms
step:649/1375 train_time:91541ms step_avg:143.26ms
step:650/1375 train_time:91690ms step_avg:143.27ms
step:651/1375 train_time:91840ms step_avg:143.28ms
step:652/1375 train_time:91988ms step_avg:143.28ms
step:653/1375 train_time:92138ms step_avg:143.29ms
step:654/1375 train_time:92289ms step_avg:143.31ms
step:655/1375 train_time:92438ms step_avg:143.32ms
step:656/1375 train_time:92586ms step_avg:143.32ms
step:657/1375 train_time:92735ms step_avg:143.33ms
step:658/1375 train_time:92881ms step_avg:143.34ms
step:659/1375 train_time:93033ms step_avg:143.35ms
step:660/1375 train_time:93180ms step_avg:143.35ms
step:661/1375 train_time:93331ms step_avg:143.37ms
step:662/1375 train_time:93480ms step_avg:143.37ms
step:663/1375 train_time:93627ms step_avg:143.38ms
step:664/1375 train_time:93779ms step_avg:143.39ms
step:665/1375 train_time:93928ms step_avg:143.40ms
step:666/1375 train_time:94076ms step_avg:143.41ms
step:667/1375 train_time:94223ms step_avg:143.41ms
step:668/1375 train_time:94374ms step_avg:143.42ms
step:669/1375 train_time:94522ms step_avg:143.43ms
step:670/1375 train_time:94673ms step_avg:143.44ms
step:671/1375 train_time:94822ms step_avg:143.45ms
step:672/1375 train_time:94970ms step_avg:143.46ms
step:673/1375 train_time:95119ms step_avg:143.47ms
step:674/1375 train_time:95268ms step_avg:143.48ms
step:675/1375 train_time:95418ms step_avg:143.49ms
step:676/1375 train_time:95568ms step_avg:143.50ms
step:677/1375 train_time:95718ms step_avg:143.50ms
step:678/1375 train_time:95865ms step_avg:143.51ms
step:679/1375 train_time:96016ms step_avg:143.52ms
step:680/1375 train_time:96164ms step_avg:143.53ms
step:681/1375 train_time:96311ms step_avg:143.53ms
step:682/1375 train_time:96461ms step_avg:143.54ms
step:683/1375 train_time:96609ms step_avg:143.55ms
step:684/1375 train_time:96758ms step_avg:143.56ms
step:685/1375 train_time:96909ms step_avg:143.57ms
step:686/1375 train_time:97057ms step_avg:143.58ms
step:687/1375 train_time:97206ms step_avg:143.58ms
step:688/1375 train_time:97358ms step_avg:143.60ms
step:689/1375 train_time:97507ms step_avg:143.60ms
step:690/1375 train_time:97658ms step_avg:143.61ms
step:691/1375 train_time:97806ms step_avg:143.62ms
step:692/1375 train_time:97957ms step_avg:143.63ms
step:693/1375 train_time:98104ms step_avg:143.64ms
step:694/1375 train_time:98256ms step_avg:143.65ms
step:695/1375 train_time:98403ms step_avg:143.65ms
step:696/1375 train_time:98553ms step_avg:143.66ms
step:697/1375 train_time:98701ms step_avg:143.67ms
step:698/1375 train_time:98850ms step_avg:143.68ms
step:699/1375 train_time:99000ms step_avg:143.69ms
step:700/1375 train_time:99149ms step_avg:143.69ms
step:701/1375 train_time:99298ms step_avg:143.70ms
step:702/1375 train_time:99446ms step_avg:143.71ms
step:703/1375 train_time:99597ms step_avg:143.72ms
step:704/1375 train_time:99744ms step_avg:143.72ms
step:705/1375 train_time:99895ms step_avg:143.73ms
step:706/1375 train_time:100045ms step_avg:143.74ms
step:707/1375 train_time:100194ms step_avg:143.75ms
step:708/1375 train_time:100342ms step_avg:143.76ms
step:709/1375 train_time:100493ms step_avg:143.77ms
step:710/1375 train_time:100641ms step_avg:143.77ms
step:711/1375 train_time:100792ms step_avg:143.78ms
step:712/1375 train_time:100942ms step_avg:143.79ms
step:713/1375 train_time:101092ms step_avg:143.80ms
step:714/1375 train_time:101240ms step_avg:143.81ms
step:715/1375 train_time:101389ms step_avg:143.81ms
step:716/1375 train_time:101541ms step_avg:143.83ms
step:717/1375 train_time:101690ms step_avg:143.83ms
step:718/1375 train_time:101839ms step_avg:143.84ms
step:719/1375 train_time:101988ms step_avg:143.85ms
step:720/1375 train_time:102141ms step_avg:143.86ms
step:721/1375 train_time:102292ms step_avg:143.87ms
step:722/1375 train_time:102443ms step_avg:143.88ms
step:723/1375 train_time:102594ms step_avg:143.89ms
step:724/1375 train_time:102743ms step_avg:143.90ms
step:725/1375 train_time:102893ms step_avg:143.91ms
step:726/1375 train_time:103042ms step_avg:143.91ms
step:727/1375 train_time:103196ms step_avg:143.93ms
step:728/1375 train_time:103344ms step_avg:143.93ms
step:729/1375 train_time:103494ms step_avg:143.94ms
step:730/1375 train_time:103645ms step_avg:143.95ms
step:731/1375 train_time:103798ms step_avg:143.96ms
step:732/1375 train_time:103945ms step_avg:143.97ms
step:733/1375 train_time:104098ms step_avg:143.98ms
step:734/1375 train_time:104246ms step_avg:143.99ms
step:735/1375 train_time:104399ms step_avg:144.00ms
step:736/1375 train_time:104549ms step_avg:144.01ms
step:737/1375 train_time:104700ms step_avg:144.02ms
step:738/1375 train_time:104849ms step_avg:144.02ms
step:739/1375 train_time:105001ms step_avg:144.03ms
step:740/1375 train_time:105153ms step_avg:144.05ms
step:741/1375 train_time:105305ms step_avg:144.06ms
step:742/1375 train_time:105455ms step_avg:144.06ms
step:743/1375 train_time:105605ms step_avg:144.07ms
step:744/1375 train_time:105756ms step_avg:144.08ms
step:745/1375 train_time:105908ms step_avg:144.09ms
step:746/1375 train_time:106058ms step_avg:144.10ms
step:747/1375 train_time:106208ms step_avg:144.11ms
step:748/1375 train_time:106359ms step_avg:144.12ms
step:749/1375 train_time:106509ms step_avg:144.13ms
step:750/1375 train_time:106660ms step_avg:144.14ms
step:750/1375 val_loss:3.5238 train_time:106736ms step_avg:144.24ms
step:751/1375 train_time:106813ms step_avg:144.15ms
step:752/1375 train_time:106967ms step_avg:144.16ms
step:753/1375 train_time:107115ms step_avg:144.17ms
step:754/1375 train_time:107266ms step_avg:144.17ms
step:755/1375 train_time:107414ms step_avg:144.18ms
step:756/1375 train_time:107565ms step_avg:144.19ms
step:757/1375 train_time:107716ms step_avg:144.20ms
step:758/1375 train_time:107868ms step_avg:144.21ms
step:759/1375 train_time:108017ms step_avg:144.22ms
step:760/1375 train_time:108167ms step_avg:144.22ms
step:761/1375 train_time:108354ms step_avg:144.28ms
step:762/1375 train_time:108502ms step_avg:144.28ms
step:763/1375 train_time:108650ms step_avg:144.29ms
step:764/1375 train_time:108800ms step_avg:144.30ms
step:765/1375 train_time:108949ms step_avg:144.30ms
step:766/1375 train_time:109102ms step_avg:144.31ms
step:767/1375 train_time:109252ms step_avg:144.32ms
step:768/1375 train_time:109407ms step_avg:144.34ms
step:769/1375 train_time:109556ms step_avg:144.34ms
step:770/1375 train_time:109708ms step_avg:144.35ms
step:771/1375 train_time:109858ms step_avg:144.36ms
step:772/1375 train_time:110007ms step_avg:144.37ms
step:773/1375 train_time:110156ms step_avg:144.37ms
step:774/1375 train_time:110309ms step_avg:144.38ms
step:775/1375 train_time:110458ms step_avg:144.39ms
step:776/1375 train_time:110612ms step_avg:144.40ms
step:777/1375 train_time:110766ms step_avg:144.41ms
step:778/1375 train_time:110912ms step_avg:144.42ms
step:779/1375 train_time:111061ms step_avg:144.42ms
step:780/1375 train_time:111213ms step_avg:144.43ms
step:781/1375 train_time:111366ms step_avg:144.44ms
step:782/1375 train_time:111515ms step_avg:144.45ms
step:783/1375 train_time:111666ms step_avg:144.46ms
step:784/1375 train_time:111815ms step_avg:144.46ms
step:785/1375 train_time:111966ms step_avg:144.47ms
step:786/1375 train_time:112114ms step_avg:144.48ms
step:787/1375 train_time:112266ms step_avg:144.49ms
step:788/1375 train_time:112414ms step_avg:144.49ms
step:789/1375 train_time:112566ms step_avg:144.50ms
step:790/1375 train_time:112713ms step_avg:144.50ms
step:791/1375 train_time:112866ms step_avg:144.51ms
step:792/1375 train_time:113015ms step_avg:144.52ms
step:793/1375 train_time:113164ms step_avg:144.53ms
step:794/1375 train_time:113314ms step_avg:144.53ms
step:795/1375 train_time:113467ms step_avg:144.54ms
step:796/1375 train_time:113616ms step_avg:144.55ms
step:797/1375 train_time:113768ms step_avg:144.56ms
step:798/1375 train_time:113918ms step_avg:144.57ms
step:799/1375 train_time:114073ms step_avg:144.58ms
step:800/1375 train_time:114224ms step_avg:144.59ms
step:801/1375 train_time:114372ms step_avg:144.59ms
step:802/1375 train_time:114525ms step_avg:144.60ms
step:803/1375 train_time:114673ms step_avg:144.61ms
step:804/1375 train_time:114823ms step_avg:144.61ms
step:805/1375 train_time:114975ms step_avg:144.62ms
step:806/1375 train_time:115126ms step_avg:144.63ms
step:807/1375 train_time:115274ms step_avg:144.64ms
step:808/1375 train_time:115426ms step_avg:144.64ms
step:809/1375 train_time:115573ms step_avg:144.65ms
step:810/1375 train_time:115726ms step_avg:144.66ms
step:811/1375 train_time:115876ms step_avg:144.66ms
step:812/1375 train_time:116027ms step_avg:144.67ms
step:813/1375 train_time:116176ms step_avg:144.68ms
step:814/1375 train_time:116328ms step_avg:144.69ms
step:815/1375 train_time:116478ms step_avg:144.69ms
step:816/1375 train_time:116631ms step_avg:144.70ms
step:817/1375 train_time:116782ms step_avg:144.71ms
step:818/1375 train_time:116932ms step_avg:144.72ms
step:819/1375 train_time:117087ms step_avg:144.73ms
step:820/1375 train_time:117240ms step_avg:144.74ms
step:821/1375 train_time:117391ms step_avg:144.75ms
step:822/1375 train_time:117542ms step_avg:144.76ms
step:823/1375 train_time:117693ms step_avg:144.76ms
step:824/1375 train_time:117845ms step_avg:144.77ms
step:825/1375 train_time:117997ms step_avg:144.78ms
step:826/1375 train_time:118152ms step_avg:144.79ms
step:827/1375 train_time:118302ms step_avg:144.80ms
step:828/1375 train_time:118454ms step_avg:144.81ms
step:829/1375 train_time:118607ms step_avg:144.82ms
step:830/1375 train_time:118759ms step_avg:144.83ms
step:831/1375 train_time:118910ms step_avg:144.84ms
step:832/1375 train_time:119063ms step_avg:144.85ms
step:833/1375 train_time:119214ms step_avg:144.85ms
step:834/1375 train_time:119366ms step_avg:144.86ms
step:835/1375 train_time:119516ms step_avg:144.87ms
step:836/1375 train_time:119670ms step_avg:144.88ms
step:837/1375 train_time:119822ms step_avg:144.89ms
step:838/1375 train_time:119974ms step_avg:144.90ms
step:839/1375 train_time:120126ms step_avg:144.91ms
step:840/1375 train_time:120276ms step_avg:144.91ms
step:841/1375 train_time:120427ms step_avg:144.92ms
step:842/1375 train_time:120579ms step_avg:144.93ms
step:843/1375 train_time:120730ms step_avg:144.93ms
step:844/1375 train_time:120882ms step_avg:144.94ms
step:845/1375 train_time:121033ms step_avg:144.95ms
step:846/1375 train_time:121187ms step_avg:144.96ms
step:847/1375 train_time:121340ms step_avg:144.97ms
step:848/1375 train_time:121492ms step_avg:144.98ms
step:849/1375 train_time:121645ms step_avg:144.99ms
step:850/1375 train_time:121797ms step_avg:145.00ms
step:851/1375 train_time:121950ms step_avg:145.01ms
step:852/1375 train_time:122103ms step_avg:145.02ms
step:853/1375 train_time:122252ms step_avg:145.02ms
step:854/1375 train_time:122407ms step_avg:145.03ms
step:855/1375 train_time:122556ms step_avg:145.04ms
step:856/1375 train_time:122707ms step_avg:145.04ms
step:857/1375 train_time:122857ms step_avg:145.05ms
step:858/1375 train_time:123016ms step_avg:145.07ms
step:859/1375 train_time:123167ms step_avg:145.07ms
step:860/1375 train_time:123319ms step_avg:145.08ms
step:861/1375 train_time:123470ms step_avg:145.09ms
step:862/1375 train_time:123621ms step_avg:145.10ms
step:863/1375 train_time:123773ms step_avg:145.10ms
step:864/1375 train_time:123928ms step_avg:145.11ms
step:865/1375 train_time:124077ms step_avg:145.12ms
step:866/1375 train_time:124232ms step_avg:145.13ms
step:867/1375 train_time:124383ms step_avg:145.14ms
step:868/1375 train_time:124533ms step_avg:145.14ms
step:869/1375 train_time:124685ms step_avg:145.15ms
step:870/1375 train_time:124838ms step_avg:145.16ms
step:871/1375 train_time:124989ms step_avg:145.17ms
step:872/1375 train_time:125140ms step_avg:145.17ms
step:873/1375 train_time:125290ms step_avg:145.18ms
step:874/1375 train_time:125442ms step_avg:145.19ms
step:875/1375 train_time:125594ms step_avg:145.20ms
step:875/1375 val_loss:3.4729 train_time:125670ms step_avg:145.28ms
step:876/1375 train_time:125745ms step_avg:145.20ms
step:877/1375 train_time:125900ms step_avg:145.21ms
step:878/1375 train_time:126049ms step_avg:145.22ms
step:879/1375 train_time:126201ms step_avg:145.23ms
step:880/1375 train_time:126351ms step_avg:145.23ms
step:881/1375 train_time:126501ms step_avg:145.24ms
step:882/1375 train_time:126654ms step_avg:145.25ms
step:883/1375 train_time:126805ms step_avg:145.25ms
step:884/1375 train_time:126959ms step_avg:145.26ms
step:885/1375 train_time:127110ms step_avg:145.27ms
step:886/1375 train_time:127267ms step_avg:145.28ms
step:887/1375 train_time:127418ms step_avg:145.29ms
step:888/1375 train_time:127569ms step_avg:145.29ms
step:889/1375 train_time:127723ms step_avg:145.31ms
step:890/1375 train_time:127873ms step_avg:145.31ms
step:891/1375 train_time:128025ms step_avg:145.32ms
step:892/1375 train_time:128177ms step_avg:145.33ms
step:893/1375 train_time:128329ms step_avg:145.33ms
step:894/1375 train_time:128482ms step_avg:145.34ms
step:895/1375 train_time:128637ms step_avg:145.35ms
step:896/1375 train_time:128787ms step_avg:145.36ms
step:897/1375 train_time:128939ms step_avg:145.37ms
step:898/1375 train_time:129093ms step_avg:145.37ms
step:899/1375 train_time:129244ms step_avg:145.38ms
step:900/1375 train_time:129394ms step_avg:145.39ms
step:901/1375 train_time:129548ms step_avg:145.40ms
step:902/1375 train_time:129699ms step_avg:145.40ms
step:903/1375 train_time:129852ms step_avg:145.41ms
step:904/1375 train_time:130004ms step_avg:145.42ms
step:905/1375 train_time:130156ms step_avg:145.43ms
step:906/1375 train_time:130306ms step_avg:145.43ms
step:907/1375 train_time:130461ms step_avg:145.44ms
step:908/1375 train_time:130613ms step_avg:145.45ms
step:909/1375 train_time:130765ms step_avg:145.46ms
step:910/1375 train_time:130924ms step_avg:145.47ms
step:911/1375 train_time:131074ms step_avg:145.48ms
step:912/1375 train_time:131224ms step_avg:145.48ms
step:913/1375 train_time:131377ms step_avg:145.49ms
step:914/1375 train_time:131527ms step_avg:145.49ms
step:915/1375 train_time:131680ms step_avg:145.50ms
step:916/1375 train_time:131831ms step_avg:145.51ms
step:917/1375 train_time:131983ms step_avg:145.52ms
step:918/1375 train_time:132136ms step_avg:145.52ms
step:919/1375 train_time:132291ms step_avg:145.53ms
step:920/1375 train_time:132443ms step_avg:145.54ms
step:921/1375 train_time:132598ms step_avg:145.55ms
step:922/1375 train_time:132755ms step_avg:145.57ms
step:923/1375 train_time:132906ms step_avg:145.57ms
step:924/1375 train_time:133061ms step_avg:145.58ms
step:925/1375 train_time:133214ms step_avg:145.59ms
step:926/1375 train_time:133367ms step_avg:145.60ms
step:927/1375 train_time:133520ms step_avg:145.61ms
step:928/1375 train_time:133673ms step_avg:145.61ms
step:929/1375 train_time:133829ms step_avg:145.62ms
step:930/1375 train_time:133984ms step_avg:145.63ms
step:931/1375 train_time:134137ms step_avg:145.64ms
step:932/1375 train_time:134288ms step_avg:145.65ms
step:933/1375 train_time:134441ms step_avg:145.66ms
step:934/1375 train_time:134594ms step_avg:145.66ms
step:935/1375 train_time:134746ms step_avg:145.67ms
step:936/1375 train_time:134899ms step_avg:145.68ms
step:937/1375 train_time:135056ms step_avg:145.69ms
step:938/1375 train_time:135208ms step_avg:145.70ms
step:939/1375 train_time:135365ms step_avg:145.71ms
step:940/1375 train_time:135518ms step_avg:145.72ms
step:941/1375 train_time:135669ms step_avg:145.72ms
step:942/1375 train_time:135822ms step_avg:145.73ms
step:943/1375 train_time:135975ms step_avg:145.74ms
step:944/1375 train_time:136133ms step_avg:145.75ms
step:945/1375 train_time:136287ms step_avg:145.76ms
step:946/1375 train_time:136443ms step_avg:145.77ms
step:947/1375 train_time:136598ms step_avg:145.78ms
step:948/1375 train_time:136750ms step_avg:145.79ms
step:949/1375 train_time:136906ms step_avg:145.80ms
step:950/1375 train_time:137060ms step_avg:145.81ms
step:951/1375 train_time:137252ms step_avg:145.86ms
step:952/1375 train_time:137403ms step_avg:145.86ms
step:953/1375 train_time:137556ms step_avg:145.87ms
step:954/1375 train_time:137707ms step_avg:145.88ms
step:955/1375 train_time:137860ms step_avg:145.88ms
step:956/1375 train_time:138011ms step_avg:145.89ms
step:957/1375 train_time:138164ms step_avg:145.90ms
step:958/1375 train_time:138323ms step_avg:145.91ms
step:959/1375 train_time:138479ms step_avg:145.92ms
step:960/1375 train_time:138632ms step_avg:145.93ms
step:961/1375 train_time:138784ms step_avg:145.93ms
step:962/1375 train_time:138937ms step_avg:145.94ms
step:963/1375 train_time:139092ms step_avg:145.95ms
step:964/1375 train_time:139246ms step_avg:145.96ms
step:965/1375 train_time:139400ms step_avg:145.97ms
step:966/1375 train_time:139551ms step_avg:145.97ms
step:967/1375 train_time:139703ms step_avg:145.98ms
step:968/1375 train_time:139853ms step_avg:145.98ms
step:969/1375 train_time:140006ms step_avg:145.99ms
step:970/1375 train_time:140161ms step_avg:146.00ms
step:971/1375 train_time:140314ms step_avg:146.01ms
step:972/1375 train_time:140467ms step_avg:146.02ms
step:973/1375 train_time:140621ms step_avg:146.02ms
step:974/1375 train_time:140773ms step_avg:146.03ms
step:975/1375 train_time:140926ms step_avg:146.04ms
step:976/1375 train_time:141079ms step_avg:146.04ms
step:977/1375 train_time:141230ms step_avg:146.05ms
step:978/1375 train_time:141383ms step_avg:146.06ms
step:979/1375 train_time:141535ms step_avg:146.06ms
step:980/1375 train_time:141686ms step_avg:146.07ms
step:981/1375 train_time:141837ms step_avg:146.07ms
step:982/1375 train_time:141989ms step_avg:146.08ms
step:983/1375 train_time:142141ms step_avg:146.09ms
step:984/1375 train_time:142293ms step_avg:146.09ms
step:985/1375 train_time:142446ms step_avg:146.10ms
step:986/1375 train_time:142603ms step_avg:146.11ms
step:987/1375 train_time:142754ms step_avg:146.11ms
step:988/1375 train_time:142905ms step_avg:146.12ms
step:989/1375 train_time:143056ms step_avg:146.12ms
step:990/1375 train_time:143211ms step_avg:146.13ms
step:991/1375 train_time:143364ms step_avg:146.14ms
step:992/1375 train_time:143522ms step_avg:146.15ms
step:993/1375 train_time:143683ms step_avg:146.17ms
step:994/1375 train_time:143834ms step_avg:146.17ms
step:995/1375 train_time:143985ms step_avg:146.18ms
step:996/1375 train_time:144136ms step_avg:146.18ms
step:997/1375 train_time:144288ms step_avg:146.19ms
step:998/1375 train_time:144442ms step_avg:146.20ms
step:999/1375 train_time:144595ms step_avg:146.20ms
step:1000/1375 train_time:144749ms step_avg:146.21ms
step:1000/1375 val_loss:3.4068 train_time:144826ms step_avg:146.29ms
step:1001/1375 train_time:144902ms step_avg:146.22ms
step:1002/1375 train_time:145060ms step_avg:146.23ms
step:1003/1375 train_time:145215ms step_avg:146.24ms
step:1004/1375 train_time:145369ms step_avg:146.25ms
step:1005/1375 train_time:145520ms step_avg:146.25ms
step:1006/1375 train_time:145671ms step_avg:146.26ms
step:1007/1375 train_time:145826ms step_avg:146.26ms
step:1008/1375 train_time:145978ms step_avg:146.27ms
step:1009/1375 train_time:146138ms step_avg:146.28ms
step:1010/1375 train_time:146291ms step_avg:146.29ms
step:1011/1375 train_time:146442ms step_avg:146.30ms
step:1012/1375 train_time:146594ms step_avg:146.30ms
step:1013/1375 train_time:146750ms step_avg:146.31ms
step:1014/1375 train_time:146901ms step_avg:146.32ms
step:1015/1375 train_time:147055ms step_avg:146.32ms
step:1016/1375 train_time:147208ms step_avg:146.33ms
step:1017/1375 train_time:147361ms step_avg:146.34ms
step:1018/1375 train_time:147512ms step_avg:146.34ms
step:1019/1375 train_time:147666ms step_avg:146.35ms
step:1020/1375 train_time:147821ms step_avg:146.36ms
step:1021/1375 train_time:147975ms step_avg:146.37ms
step:1022/1375 train_time:148127ms step_avg:146.37ms
step:1023/1375 train_time:148280ms step_avg:146.38ms
step:1024/1375 train_time:148434ms step_avg:146.38ms
step:1025/1375 train_time:148588ms step_avg:146.39ms
step:1026/1375 train_time:148740ms step_avg:146.40ms
step:1027/1375 train_time:148894ms step_avg:146.41ms
step:1028/1375 train_time:149049ms step_avg:146.41ms
step:1029/1375 train_time:149207ms step_avg:146.43ms
step:1030/1375 train_time:149362ms step_avg:146.43ms
step:1031/1375 train_time:149515ms step_avg:146.44ms
step:1032/1375 train_time:149667ms step_avg:146.44ms
step:1033/1375 train_time:149821ms step_avg:146.45ms
step:1034/1375 train_time:149976ms step_avg:146.46ms
step:1035/1375 train_time:150131ms step_avg:146.47ms
step:1036/1375 train_time:150285ms step_avg:146.48ms
step:1037/1375 train_time:150440ms step_avg:146.49ms
step:1038/1375 train_time:150594ms step_avg:146.49ms
step:1039/1375 train_time:150747ms step_avg:146.50ms
step:1040/1375 train_time:150898ms step_avg:146.50ms
step:1041/1375 train_time:151054ms step_avg:146.51ms
step:1042/1375 train_time:151205ms step_avg:146.52ms
step:1043/1375 train_time:151357ms step_avg:146.52ms
step:1044/1375 train_time:151512ms step_avg:146.53ms
step:1045/1375 train_time:151668ms step_avg:146.54ms
step:1046/1375 train_time:151819ms step_avg:146.54ms
step:1047/1375 train_time:151974ms step_avg:146.55ms
step:1048/1375 train_time:152127ms step_avg:146.56ms
step:1049/1375 train_time:152282ms step_avg:146.57ms
step:1050/1375 train_time:152438ms step_avg:146.57ms
step:1051/1375 train_time:152597ms step_avg:146.59ms
step:1052/1375 train_time:152752ms step_avg:146.59ms
step:1053/1375 train_time:152903ms step_avg:146.60ms
step:1054/1375 train_time:153059ms step_avg:146.61ms
step:1055/1375 train_time:153213ms step_avg:146.62ms
step:1056/1375 train_time:153366ms step_avg:146.62ms
step:1057/1375 train_time:153522ms step_avg:146.63ms
step:1058/1375 train_time:153679ms step_avg:146.64ms
step:1059/1375 train_time:153835ms step_avg:146.65ms
step:1060/1375 train_time:153988ms step_avg:146.65ms
step:1061/1375 train_time:154138ms step_avg:146.66ms
step:1062/1375 train_time:154295ms step_avg:146.67ms
step:1063/1375 train_time:154449ms step_avg:146.68ms
step:1064/1375 train_time:154601ms step_avg:146.68ms
step:1065/1375 train_time:154755ms step_avg:146.69ms
step:1066/1375 train_time:154913ms step_avg:146.70ms
step:1067/1375 train_time:155066ms step_avg:146.70ms
step:1068/1375 train_time:155218ms step_avg:146.71ms
step:1069/1375 train_time:155377ms step_avg:146.72ms
step:1070/1375 train_time:155527ms step_avg:146.72ms
step:1071/1375 train_time:155682ms step_avg:146.73ms
step:1072/1375 train_time:155836ms step_avg:146.74ms
step:1073/1375 train_time:155990ms step_avg:146.75ms
step:1074/1375 train_time:156143ms step_avg:146.75ms
step:1075/1375 train_time:156298ms step_avg:146.76ms
step:1076/1375 train_time:156450ms step_avg:146.76ms
step:1077/1375 train_time:156601ms step_avg:146.77ms
step:1078/1375 train_time:156759ms step_avg:146.78ms
step:1079/1375 train_time:156916ms step_avg:146.79ms
step:1080/1375 train_time:157070ms step_avg:146.79ms
step:1081/1375 train_time:157222ms step_avg:146.80ms
step:1082/1375 train_time:157376ms step_avg:146.81ms
step:1083/1375 train_time:157528ms step_avg:146.81ms
step:1084/1375 train_time:157683ms step_avg:146.82ms
step:1085/1375 train_time:157836ms step_avg:146.82ms
step:1086/1375 train_time:157993ms step_avg:146.83ms
step:1087/1375 train_time:158148ms step_avg:146.84ms
step:1088/1375 train_time:158301ms step_avg:146.85ms
step:1089/1375 train_time:158460ms step_avg:146.86ms
step:1090/1375 train_time:158619ms step_avg:146.87ms
step:1091/1375 train_time:158773ms step_avg:146.88ms
step:1092/1375 train_time:158925ms step_avg:146.88ms
step:1093/1375 train_time:159079ms step_avg:146.89ms
step:1094/1375 train_time:159232ms step_avg:146.89ms
step:1095/1375 train_time:159387ms step_avg:146.90ms
step:1096/1375 train_time:159545ms step_avg:146.91ms
step:1097/1375 train_time:159697ms step_avg:146.92ms
step:1098/1375 train_time:159852ms step_avg:146.92ms
step:1099/1375 train_time:160004ms step_avg:146.93ms
step:1100/1375 train_time:160157ms step_avg:146.93ms
step:1101/1375 train_time:160310ms step_avg:146.94ms
step:1102/1375 train_time:160466ms step_avg:146.95ms
step:1103/1375 train_time:160619ms step_avg:146.95ms
step:1104/1375 train_time:160773ms step_avg:146.96ms
step:1105/1375 train_time:160928ms step_avg:146.97ms
step:1106/1375 train_time:161080ms step_avg:146.97ms
step:1107/1375 train_time:161235ms step_avg:146.98ms
step:1108/1375 train_time:161392ms step_avg:146.99ms
step:1109/1375 train_time:161545ms step_avg:146.99ms
step:1110/1375 train_time:161699ms step_avg:147.00ms
step:1111/1375 train_time:161854ms step_avg:147.01ms
step:1112/1375 train_time:162008ms step_avg:147.01ms
step:1113/1375 train_time:162160ms step_avg:147.02ms
step:1114/1375 train_time:162316ms step_avg:147.03ms
step:1115/1375 train_time:162470ms step_avg:147.03ms
step:1116/1375 train_time:162622ms step_avg:147.04ms
step:1117/1375 train_time:162779ms step_avg:147.05ms
step:1118/1375 train_time:162939ms step_avg:147.06ms
step:1119/1375 train_time:163094ms step_avg:147.06ms
step:1120/1375 train_time:163248ms step_avg:147.07ms
step:1121/1375 train_time:163400ms step_avg:147.07ms
step:1122/1375 train_time:163553ms step_avg:147.08ms
step:1123/1375 train_time:163706ms step_avg:147.09ms
step:1124/1375 train_time:163865ms step_avg:147.10ms
step:1125/1375 train_time:164019ms step_avg:147.10ms
step:1125/1375 val_loss:3.3532 train_time:164099ms step_avg:147.17ms
step:1126/1375 train_time:164178ms step_avg:147.11ms
step:1127/1375 train_time:164335ms step_avg:147.12ms
step:1128/1375 train_time:164490ms step_avg:147.13ms
step:1129/1375 train_time:164646ms step_avg:147.14ms
step:1130/1375 train_time:164799ms step_avg:147.14ms
step:1131/1375 train_time:164958ms step_avg:147.15ms
step:1132/1375 train_time:165113ms step_avg:147.16ms
step:1133/1375 train_time:165269ms step_avg:147.17ms
step:1134/1375 train_time:165426ms step_avg:147.18ms
step:1135/1375 train_time:165578ms step_avg:147.18ms
step:1136/1375 train_time:165737ms step_avg:147.19ms
step:1137/1375 train_time:165891ms step_avg:147.20ms
step:1138/1375 train_time:166045ms step_avg:147.20ms
step:1139/1375 train_time:166202ms step_avg:147.21ms
step:1140/1375 train_time:166358ms step_avg:147.22ms
step:1141/1375 train_time:166550ms step_avg:147.26ms
step:1142/1375 train_time:166703ms step_avg:147.26ms
step:1143/1375 train_time:166863ms step_avg:147.28ms
step:1144/1375 train_time:167020ms step_avg:147.28ms
step:1145/1375 train_time:167172ms step_avg:147.29ms
step:1146/1375 train_time:167330ms step_avg:147.30ms
step:1147/1375 train_time:167484ms step_avg:147.30ms
step:1148/1375 train_time:167639ms step_avg:147.31ms
step:1149/1375 train_time:167794ms step_avg:147.32ms
step:1150/1375 train_time:167946ms step_avg:147.32ms
step:1151/1375 train_time:168104ms step_avg:147.33ms
step:1152/1375 train_time:168259ms step_avg:147.34ms
step:1153/1375 train_time:168415ms step_avg:147.34ms
step:1154/1375 train_time:168571ms step_avg:147.35ms
step:1155/1375 train_time:168727ms step_avg:147.36ms
step:1156/1375 train_time:168885ms step_avg:147.37ms
step:1157/1375 train_time:169044ms step_avg:147.38ms
step:1158/1375 train_time:169198ms step_avg:147.39ms
step:1159/1375 train_time:169354ms step_avg:147.39ms
step:1160/1375 train_time:169506ms step_avg:147.40ms
step:1161/1375 train_time:169661ms step_avg:147.40ms
step:1162/1375 train_time:169817ms step_avg:147.41ms
step:1163/1375 train_time:169972ms step_avg:147.42ms
step:1164/1375 train_time:170126ms step_avg:147.42ms
step:1165/1375 train_time:170279ms step_avg:147.43ms
step:1166/1375 train_time:170437ms step_avg:147.44ms
step:1167/1375 train_time:170589ms step_avg:147.44ms
step:1168/1375 train_time:170742ms step_avg:147.45ms
step:1169/1375 train_time:170898ms step_avg:147.45ms
step:1170/1375 train_time:171055ms step_avg:147.46ms
step:1171/1375 train_time:171210ms step_avg:147.47ms
step:1172/1375 train_time:171365ms step_avg:147.47ms
step:1173/1375 train_time:171521ms step_avg:147.48ms
step:1174/1375 train_time:171686ms step_avg:147.50ms
step:1175/1375 train_time:171843ms step_avg:147.51ms
step:1176/1375 train_time:172004ms step_avg:147.52ms
step:1177/1375 train_time:172168ms step_avg:147.53ms
step:1178/1375 train_time:172321ms step_avg:147.53ms
step:1179/1375 train_time:172475ms step_avg:147.54ms
step:1180/1375 train_time:172638ms step_avg:147.55ms
step:1181/1375 train_time:172795ms step_avg:147.56ms
step:1182/1375 train_time:172946ms step_avg:147.57ms
step:1183/1375 train_time:173102ms step_avg:147.57ms
step:1184/1375 train_time:173256ms step_avg:147.58ms
step:1185/1375 train_time:173414ms step_avg:147.59ms
step:1186/1375 train_time:173568ms step_avg:147.59ms
step:1187/1375 train_time:173732ms step_avg:147.61ms
step:1188/1375 train_time:173884ms step_avg:147.61ms
step:1189/1375 train_time:174039ms step_avg:147.62ms
step:1190/1375 train_time:174197ms step_avg:147.62ms
step:1191/1375 train_time:174355ms step_avg:147.63ms
step:1192/1375 train_time:174508ms step_avg:147.64ms
step:1193/1375 train_time:174661ms step_avg:147.64ms
step:1194/1375 train_time:174816ms step_avg:147.65ms
step:1195/1375 train_time:174971ms step_avg:147.65ms
step:1196/1375 train_time:175125ms step_avg:147.66ms
step:1197/1375 train_time:175280ms step_avg:147.67ms
step:1198/1375 train_time:175440ms step_avg:147.68ms
step:1199/1375 train_time:175595ms step_avg:147.68ms
step:1200/1375 train_time:175748ms step_avg:147.69ms
step:1201/1375 train_time:175903ms step_avg:147.69ms
step:1202/1375 train_time:176073ms step_avg:147.71ms
step:1203/1375 train_time:176231ms step_avg:147.72ms
step:1204/1375 train_time:176387ms step_avg:147.73ms
step:1205/1375 train_time:176542ms step_avg:147.73ms
step:1206/1375 train_time:176699ms step_avg:147.74ms
step:1207/1375 train_time:176855ms step_avg:147.75ms
step:1208/1375 train_time:177010ms step_avg:147.75ms
step:1209/1375 train_time:177166ms step_avg:147.76ms
step:1210/1375 train_time:177327ms step_avg:147.77ms
step:1211/1375 train_time:177483ms step_avg:147.78ms
step:1212/1375 train_time:177638ms step_avg:147.79ms
step:1213/1375 train_time:177793ms step_avg:147.79ms
step:1214/1375 train_time:177951ms step_avg:147.80ms
step:1215/1375 train_time:178105ms step_avg:147.81ms
step:1216/1375 train_time:178259ms step_avg:147.81ms
step:1217/1375 train_time:178415ms step_avg:147.82ms
step:1218/1375 train_time:178568ms step_avg:147.82ms
step:1219/1375 train_time:178721ms step_avg:147.83ms
step:1220/1375 train_time:178877ms step_avg:147.83ms
step:1221/1375 train_time:179032ms step_avg:147.84ms
step:1222/1375 train_time:179186ms step_avg:147.84ms
step:1223/1375 train_time:179343ms step_avg:147.85ms
step:1224/1375 train_time:179501ms step_avg:147.86ms
step:1225/1375 train_time:179656ms step_avg:147.86ms
step:1226/1375 train_time:179813ms step_avg:147.87ms
step:1227/1375 train_time:179972ms step_avg:147.88ms
step:1228/1375 train_time:180127ms step_avg:147.89ms
step:1229/1375 train_time:180280ms step_avg:147.89ms
step:1230/1375 train_time:180441ms step_avg:147.90ms
step:1231/1375 train_time:180599ms step_avg:147.91ms
step:1232/1375 train_time:180759ms step_avg:147.92ms
step:1233/1375 train_time:180915ms step_avg:147.93ms
step:1234/1375 train_time:181070ms step_avg:147.93ms
step:1235/1375 train_time:181225ms step_avg:147.94ms
step:1236/1375 train_time:181379ms step_avg:147.94ms
step:1237/1375 train_time:181534ms step_avg:147.95ms
step:1238/1375 train_time:181698ms step_avg:147.96ms
step:1239/1375 train_time:181855ms step_avg:147.97ms
step:1240/1375 train_time:182013ms step_avg:147.98ms
step:1241/1375 train_time:182174ms step_avg:147.99ms
step:1242/1375 train_time:182330ms step_avg:147.99ms
step:1243/1375 train_time:182490ms step_avg:148.00ms
step:1244/1375 train_time:182643ms step_avg:148.01ms
step:1245/1375 train_time:182801ms step_avg:148.02ms
step:1246/1375 train_time:182956ms step_avg:148.02ms
step:1247/1375 train_time:183114ms step_avg:148.03ms
step:1248/1375 train_time:183267ms step_avg:148.03ms
step:1249/1375 train_time:183419ms step_avg:148.04ms
step:1250/1375 train_time:183573ms step_avg:148.04ms
step:1250/1375 val_loss:3.3075 train_time:183655ms step_avg:148.11ms
step:1251/1375 train_time:183734ms step_avg:148.05ms
step:1252/1375 train_time:183888ms step_avg:148.06ms
step:1253/1375 train_time:184042ms step_avg:148.06ms
step:1254/1375 train_time:184193ms step_avg:148.07ms
step:1255/1375 train_time:184360ms step_avg:148.08ms
step:1256/1375 train_time:184515ms step_avg:148.09ms
step:1257/1375 train_time:184671ms step_avg:148.09ms
step:1258/1375 train_time:184830ms step_avg:148.10ms
step:1259/1375 train_time:184987ms step_avg:148.11ms
step:1260/1375 train_time:185139ms step_avg:148.11ms
step:1261/1375 train_time:185297ms step_avg:148.12ms
step:1262/1375 train_time:185456ms step_avg:148.13ms
step:1263/1375 train_time:185611ms step_avg:148.13ms
step:1264/1375 train_time:185765ms step_avg:148.14ms
step:1265/1375 train_time:185918ms step_avg:148.14ms
step:1266/1375 train_time:186075ms step_avg:148.15ms
step:1267/1375 train_time:186232ms step_avg:148.16ms
step:1268/1375 train_time:186388ms step_avg:148.16ms
step:1269/1375 train_time:186549ms step_avg:148.17ms
step:1270/1375 train_time:186704ms step_avg:148.18ms
step:1271/1375 train_time:186859ms step_avg:148.18ms
step:1272/1375 train_time:187012ms step_avg:148.19ms
step:1273/1375 train_time:187165ms step_avg:148.19ms
step:1274/1375 train_time:187322ms step_avg:148.20ms
step:1275/1375 train_time:187477ms step_avg:148.20ms
step:1276/1375 train_time:187629ms step_avg:148.21ms
step:1277/1375 train_time:187787ms step_avg:148.21ms
step:1278/1375 train_time:187941ms step_avg:148.22ms
step:1279/1375 train_time:188100ms step_avg:148.23ms
step:1280/1375 train_time:188261ms step_avg:148.24ms
step:1281/1375 train_time:188417ms step_avg:148.24ms
step:1282/1375 train_time:188572ms step_avg:148.25ms
step:1283/1375 train_time:188731ms step_avg:148.26ms
step:1284/1375 train_time:188889ms step_avg:148.26ms
step:1285/1375 train_time:189046ms step_avg:148.27ms
step:1286/1375 train_time:189204ms step_avg:148.28ms
step:1287/1375 train_time:189359ms step_avg:148.28ms
step:1288/1375 train_time:189513ms step_avg:148.29ms
step:1289/1375 train_time:189676ms step_avg:148.30ms
step:1290/1375 train_time:189836ms step_avg:148.31ms
step:1291/1375 train_time:189999ms step_avg:148.32ms
step:1292/1375 train_time:190155ms step_avg:148.33ms
step:1293/1375 train_time:190315ms step_avg:148.34ms
step:1294/1375 train_time:190470ms step_avg:148.34ms
step:1295/1375 train_time:190627ms step_avg:148.35ms
step:1296/1375 train_time:190785ms step_avg:148.36ms
step:1297/1375 train_time:190946ms step_avg:148.37ms
step:1298/1375 train_time:191102ms step_avg:148.37ms
step:1299/1375 train_time:191256ms step_avg:148.38ms
step:1300/1375 train_time:191411ms step_avg:148.38ms
step:1301/1375 train_time:191565ms step_avg:148.38ms
step:1302/1375 train_time:191724ms step_avg:148.39ms
step:1303/1375 train_time:191882ms step_avg:148.40ms
step:1304/1375 train_time:192040ms step_avg:148.41ms
step:1305/1375 train_time:192196ms step_avg:148.41ms
step:1306/1375 train_time:192354ms step_avg:148.42ms
step:1307/1375 train_time:192508ms step_avg:148.43ms
step:1308/1375 train_time:192666ms step_avg:148.43ms
step:1309/1375 train_time:192821ms step_avg:148.44ms
step:1310/1375 train_time:192975ms step_avg:148.44ms
step:1311/1375 train_time:193131ms step_avg:148.45ms
step:1312/1375 train_time:193285ms step_avg:148.45ms
step:1313/1375 train_time:193438ms step_avg:148.46ms
step:1314/1375 train_time:193595ms step_avg:148.46ms
step:1315/1375 train_time:193753ms step_avg:148.47ms
step:1316/1375 train_time:193906ms step_avg:148.47ms
step:1317/1375 train_time:194061ms step_avg:148.48ms
step:1318/1375 train_time:194220ms step_avg:148.49ms
step:1319/1375 train_time:194377ms step_avg:148.49ms
step:1320/1375 train_time:194535ms step_avg:148.50ms
step:1321/1375 train_time:194693ms step_avg:148.51ms
step:1322/1375 train_time:194854ms step_avg:148.52ms
step:1323/1375 train_time:195008ms step_avg:148.52ms
step:1324/1375 train_time:195162ms step_avg:148.53ms
step:1325/1375 train_time:195318ms step_avg:148.53ms
step:1326/1375 train_time:195476ms step_avg:148.54ms
step:1327/1375 train_time:195631ms step_avg:148.54ms
step:1328/1375 train_time:195788ms step_avg:148.55ms
step:1329/1375 train_time:195962ms step_avg:148.57ms
step:1330/1375 train_time:196122ms step_avg:148.58ms
step:1331/1375 train_time:196315ms step_avg:148.61ms
step:1332/1375 train_time:196479ms step_avg:148.62ms
step:1333/1375 train_time:196635ms step_avg:148.63ms
step:1334/1375 train_time:196790ms step_avg:148.63ms
step:1335/1375 train_time:196943ms step_avg:148.64ms
step:1336/1375 train_time:197109ms step_avg:148.65ms
step:1337/1375 train_time:197267ms step_avg:148.66ms
step:1338/1375 train_time:197424ms step_avg:148.66ms
step:1339/1375 train_time:197582ms step_avg:148.67ms
step:1340/1375 train_time:197738ms step_avg:148.68ms
step:1341/1375 train_time:197893ms step_avg:148.68ms
step:1342/1375 train_time:198053ms step_avg:148.69ms
step:1343/1375 train_time:198209ms step_avg:148.69ms
step:1344/1375 train_time:198363ms step_avg:148.70ms
step:1345/1375 train_time:198519ms step_avg:148.70ms
step:1346/1375 train_time:198674ms step_avg:148.71ms
step:1347/1375 train_time:198832ms step_avg:148.72ms
step:1348/1375 train_time:198988ms step_avg:148.72ms
step:1349/1375 train_time:199147ms step_avg:148.73ms
step:1350/1375 train_time:199300ms step_avg:148.73ms
step:1351/1375 train_time:199454ms step_avg:148.74ms
step:1352/1375 train_time:199618ms step_avg:148.75ms
step:1353/1375 train_time:199780ms step_avg:148.76ms
step:1354/1375 train_time:199938ms step_avg:148.76ms
step:1355/1375 train_time:200096ms step_avg:148.77ms
step:1356/1375 train_time:200251ms step_avg:148.77ms
step:1357/1375 train_time:200411ms step_avg:148.78ms
step:1358/1375 train_time:200570ms step_avg:148.79ms
step:1359/1375 train_time:200727ms step_avg:148.80ms
step:1360/1375 train_time:200887ms step_avg:148.81ms
step:1361/1375 train_time:201048ms step_avg:148.81ms
step:1362/1375 train_time:201207ms step_avg:148.82ms
step:1363/1375 train_time:201370ms step_avg:148.83ms
step:1364/1375 train_time:201524ms step_avg:148.84ms
step:1365/1375 train_time:201677ms step_avg:148.84ms
step:1366/1375 train_time:201833ms step_avg:148.84ms
step:1367/1375 train_time:201991ms step_avg:148.85ms
step:1368/1375 train_time:202151ms step_avg:148.86ms
step:1369/1375 train_time:202317ms step_avg:148.87ms
step:1370/1375 train_time:202476ms step_avg:148.88ms
step:1371/1375 train_time:202631ms step_avg:148.88ms
step:1372/1375 train_time:202794ms step_avg:148.89ms
step:1373/1375 train_time:202949ms step_avg:148.90ms
step:1374/1375 train_time:203108ms step_avg:148.91ms
step:1375/1375 train_time:203262ms step_avg:148.91ms
step:1375/1375 val_loss:3.2821 train_time:203337ms step_avg:148.96ms
peak memory consumption: 31565 MiB

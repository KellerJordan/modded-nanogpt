import os
import sys
with open(sys.argv[0]) as f:
    code = f.read() # read the code of this file ASAP, for logging
import uuid
import time
import glob
import subprocess
import contextlib
from dataclasses import dataclass

import torch
torch.empty(1, device='cuda', requires_grad=True).backward()
from torch import nn
import torch.nn.functional as F
import torch.distributed as dist
from torch.nn.parallel import DistributedDataParallel as DDP
# use of FlexAttention contributed by @KoszarskyB
from torch.nn.attention.flex_attention import BlockMask, flex_attention

# -----------------------------------------------------------------------------
# Muon optimizer

@torch.compile
def zeropower_via_newtonschulz5(G, steps):
    """
    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a
    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose
    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at
    zero even beyond the point where the iteration no longer converges all the way to one everywhere
    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T
    where S' is diagonal with S_{ii}' ~ Uniform(0.5, 1.5), which turns out not to hurt model
    performance at all relative to UV^T, where USV^T = G is the SVD.
    """
    assert len(G.shape) == 2
    a, b, c = (3.4445, -4.7750,  2.0315)
    X = G.bfloat16()
    if G.size(0) > G.size(1):
        X = X.T

    # Ensure spectral norm is at most 1
    X = X / (X.norm() + 1e-7)
    # Perform the NS iterations
    for _ in range(steps):
        A = X @ X.T
        B = b * A + c * A @ A # adapted from suggestion by @jxbz, @leloykun, and @YouJiacheng
        X = a * X + B @ X
    
    if G.size(0) > G.size(1):
        X = X.T
    return X

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Some warnings:
    - This optimizer assumes that all parameters passed in are 2D.
    - It should not be used for the embedding layer, the final fully connected layer, or any {0,1}-D
    parameters; those should all be optimized by a standard method (e.g., AdamW).
    - To use it with 4D convolutional filters, it works well to just flatten their last 3 dimensions.
    - We believe it is unlikely to work well for training with small batch size.
    - We believe it may not work well for finetuning pretrained models, but we haven't tested this.
    - We have not yet tried this optimizer for training scenarios larger than NanoGPT (124M).

    Arguments:
        lr: The learning rate used by the internal SGD.
        momentum: The momentum used by the internal SGD.
        nesterov: Whether to use Nesterov-style momentum in the internal SGD. (recommended)
        ns_steps: The number of Newton-Schulz iteration steps to use.
    """
    def __init__(self, params, lr=0.02, momentum=0.95, nesterov=True, ns_steps=5):
        self.world_size = int(os.environ['WORLD_SIZE'])
        self.rank = int(os.environ['RANK'])
        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, ns_steps=ns_steps)
        assert all(isinstance(p, torch.Tensor) for p in params)
        sizes = {p.numel() for p in params}
        param_groups = [dict(params=[p for p in params if p.numel() == size],
                             update_buffer=[torch.empty(size, device='cuda', dtype=torch.bfloat16) for _ in range(self.world_size)])
                        for size in sizes]
        super().__init__(param_groups, defaults)

    def step(self):

        for group in self.param_groups:

            lr = group['lr']
            momentum = group['momentum']
            nesterov = group['nesterov']
            ns_steps = group['ns_steps']
            update_buffers = group['update_buffer']
            # generate weight updates in distributed fashion
            params = group['params']
            handle = None
            params_world = None
            def update_prev():
                if params_world is None:
                    return
                assert handle is not None
                handle.wait()
                for p_world, g_world in zip(params_world, update_buffers):
                    p_world.data.add_(
                        g_world.view_as(p_world),
                        alpha=-lr * max(1, p_world.size(0) / p_world.size(1)) ** 0.5,
                    )
            for base_i in range(len(params))[::self.world_size]:
                if base_i + rank < len(params):
                    p = params[base_i + self.rank]
                    g = p.grad
                    assert g is not None
                    state = self.state[p]
                    if 'momentum_buffer' not in state:
                        state['momentum_buffer'] = torch.zeros_like(g)
                    buf = state['momentum_buffer']
                    buf.lerp_(g, 1 - momentum)
                    g = g.lerp_(buf, momentum) if nesterov else buf
                    g = zeropower_via_newtonschulz5(g, steps=ns_steps).flatten()
                else:
                    g = update_buffers[rank]
                update_prev() # async all_gather instead of sync all_reduce by @YouJiacheng
                handle = dist.all_gather(update_buffers, g, async_op=True)
                params_world = params[base_i : base_i + self.world_size]
            update_prev()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the GPT-2 model

def norm(x):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):

    def __init__(self, in_features, out_features):
        super().__init__(in_features, out_features, bias=False)

    def forward(self, x):
        return F.linear(x, self.weight.type_as(x))

class Rotary(nn.Module):

    def __init__(self, dim, max_seq_len=65536):
        super().__init__()
        # half-truncate RoPE by @YouJiacheng
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=dim//4, dtype=torch.float32)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(dim//4)])
        t = torch.arange(max_seq_len, dtype=torch.float32)
        theta = torch.einsum('i,j -> ij', t, angular_freq)
        self.cos = nn.Buffer(theta.cos(), persistent=False)
        self.sin = nn.Buffer(theta.sin(), persistent=False)

    def forward(self, x):
        cos, sin = self.cos[None, :x.size(-3), None, :], self.sin[None, :x.size(-3), None, :]
        x1, x2 = x.float().chunk(2, dim=-1)
        y1 = x1 * cos + x2 * sin
        y2 = x1 * (-sin) + x2 * cos
        return torch.cat((y1, y2), 3).type_as(x)

class CausalSelfAttention(nn.Module):

    def __init__(self, dim, num_heads, layer_id):
        super().__init__()
        assert dim % num_heads == 0
        self.num_heads = num_heads
        self.layer_id = layer_id
        self.c_q = CastedLinear(dim, dim)
        self.c_k = CastedLinear(dim, dim)
        self.c_v = CastedLinear(dim, dim)
        self.lambdas = nn.Parameter(torch.tensor([0.5, 0.5]))
        self.rotary = Rotary(dim // num_heads) # dim // num_heads = head_dim
        self.c_proj = CastedLinear(dim, dim)
        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977
        # self.attn_scale = nn.Parameter(torch.tensor(1.0 / (dim // num_heads) ** 0.5))
        # self.attn_scale = nn.Parameter(torch.tensor(0.20))
        self.attn_scale = 0.13 + 0.01 * min(layer_id, 11 - layer_id)  # unet pattern attention scale by @leloykun

    def forward(self, x, ve, block_mask):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, 'Must use batch size = 1 for FlexAttention'
        q = self.c_q(x).view(B, T, self.num_heads, -1)
        k = self.c_k(x).view(B, T, self.num_heads, -1)
        v = self.c_v(x).view(B, T, self.num_heads, -1)
        if ve is not None:
            v = self.lambdas[0] * v + self.lambdas[1] * ve.view_as(v) # @KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = self.lambdas[0] * v
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = self.rotary(q), self.rotary(k)
        # y = flex_attention(q.transpose(1, 2) * self.attn_scale, k.transpose(1, 2), v.transpose(1, 2), block_mask=block_mask, scale=1.)
        y = flex_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), block_mask=block_mask, scale=self.attn_scale)
        y = y.transpose(1, 2).contiguous().view_as(x) # re-assemble all head outputs side by side
        y = self.c_proj(y)
        return y

class MLP(nn.Module):

    def __init__(self, dim):
        super().__init__()
        self.c_fc = CastedLinear(dim, 4 * dim)
        self.c_proj = CastedLinear(4 * dim, dim)
        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977

    def forward(self, x):
        x = self.c_fc(x)
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = self.c_proj(x)
        return x

class Block(nn.Module):

    def __init__(self, model_dim, num_heads, layer_id, use_attn=True):
        super().__init__()
        self.attn = CausalSelfAttention(model_dim, num_heads, layer_id) if use_attn else None
        self.mlp = MLP(model_dim)
        self.lambdas = nn.Parameter(torch.tensor([1., 0.]))

    def forward(self, x, ve, x0, block_mask):
        x = self.lambdas[0] * x + self.lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), ve, block_mask)
        x = x + self.mlp(norm(x))
        return x

class ValueEmbedding(nn.Module):
    def __init__(self, vocab_size, model_dim):
        super().__init__()
        self.embed = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])

    def forward(self, inputs):
        ve = [emb(inputs).bfloat16() for emb in self.embed]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        ve = [ve[0], ve[1], ve[2], None, None, None, None, None, None, ve[0], ve[1], ve[2]]
        return ve

# -----------------------------------------------------------------------------
# The main GPT-2 model

class GPT(nn.Module):

    def __init__(self, vocab_size, num_layers, num_heads, model_dim):
        super().__init__()
        self.embed = nn.Embedding(vocab_size, model_dim)
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.blocks = nn.ModuleList([Block(model_dim, num_heads, layer_id=layer_id, use_attn=(layer_id != 7))
                                     for layer_id in range(num_layers)])
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual learning
        # U-net structure on token value embeddings by @leloykun
        self.value_embeds = ValueEmbedding(vocab_size, model_dim)
        self.lm_head = CastedLinear(model_dim, vocab_size)
        self.lm_head.weight.data.zero_() # @Grad62304977
        # U-net design by @brendanh0gan
        self.num_encoder_layers = num_layers // 2 # Half of the layers for encoder
        self.num_decoder_layers = num_layers - self.num_encoder_layers # Remaining for decoder
        # Add learnable skip connection weights for decoder layers
        self.skip_weights = nn.Parameter(torch.ones(self.num_decoder_layers))

    def forward(self, inputs, targets, sliding_window_num_blocks):
        BLOCK_SIZE = 128
        seq_len = len(inputs)
        assert seq_len % BLOCK_SIZE == 0
        total_num_blocks = seq_len // BLOCK_SIZE
        assert inputs.ndim == 1
        docs = (inputs == 50256).cumsum(0)
        docs_low = docs.view(-1, BLOCK_SIZE)[:, 0].contiguous()
        docs_high = docs.view(-1, BLOCK_SIZE)[:, -1].contiguous()

        def document_causal(b, h, q_idx, kv_idx):
            causal_mask = q_idx >= kv_idx
            document_mask = docs[q_idx] == docs[kv_idx]
            return causal_mask & document_mask

        def dense_to_ordered(dense_mask):
            num_blocks = dense_mask.sum(dim=-1, dtype=torch.int32)
            indices = dense_mask.argsort(dim=-1, descending=True, stable=True).to(torch.int32)
            return num_blocks[None, None].contiguous(), indices[None, None].contiguous()

        def create_doc_swc_block_mask(sliding_window_num_blocks):
            kv_idx = block_idx = torch.arange(total_num_blocks, dtype=torch.int32, device='cuda')
            q_idx = block_idx[:, None]
            causal_bm = q_idx >= kv_idx
            causal_full_bm = q_idx > kv_idx
            window_bm = q_idx - kv_idx < sliding_window_num_blocks
            window_full_bm = window_bm # block-wise sliding window by @YouJiacheng
            # document_bm = (docs_low[q_idx] <= docs_high[kv_idx]) & (docs_low[kv_idx] <= docs_high[q_idx])
            document_bm = (docs_low[:, None] <= docs_high) & (docs_low <= docs_high[:, None])
            document_full_bm = (docs_low[:, None] == docs_high) & (docs_low == docs_high[:, None])
            nonzero_bm = causal_bm & window_bm & document_bm
            full_bm  = causal_full_bm & window_full_bm & document_full_bm
            kv_num_blocks, kv_indices = dense_to_ordered(nonzero_bm & ~full_bm)
            full_kv_num_blocks, full_kv_indices = dense_to_ordered(full_bm)
            return BlockMask.from_kv_blocks(
                kv_num_blocks,
                kv_indices,
                full_kv_num_blocks,
                full_kv_indices,
                BLOCK_SIZE=BLOCK_SIZE,
                mask_mod=document_causal,
            )

        block_mask = create_doc_swc_block_mask(sliding_window_num_blocks)

        x0 = norm(self.embed(inputs[None]).bfloat16()) # use of norm here by @Grad62304977
        x = x0
        ve = self.value_embeds(inputs)
        assert len(ve) == len(self.blocks)
        ve_enc, ve_dec = ve[:self.num_encoder_layers], ve[self.num_encoder_layers:]

        # Store outputs for U-Net skip connections
        skip_connections = []
        # Encoder pass - process only the first half of the blocks
        for i in range(self.num_encoder_layers):
            x = self.blocks[i](x, ve_enc[i], x0, block_mask)
            skip_connections.append(x)
        # Decoder pass - process the remaining blocks with weighted skip connections
        for i in range(self.num_decoder_layers):
            x = x + self.skip_weights[i] * skip_connections.pop()
            # U-net structure on token value embeddings by @leloykun
            x = self.blocks[self.num_encoder_layers + i](x, ve_dec[i], x0, block_mask)

        x = norm(x)
        logits = self.lm_head(x)
        logits = 15 * torch.tanh(logits / 15) # @Grad62304977 added tanh softcapping, @KoszarskyB reduced it from 30 to 15
        logits = logits.float()
        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets)
        return loss

# -----------------------------------------------------------------------------
# Our own simple Distributed Data Loader

def _load_data_shard(path):
    # only reads the header, returns header data
    # header is 256 int32
    header = torch.from_file(path, False, 256, dtype=torch.int32)
    assert header[0] == 20240520, 'magic number mismatch in the data .bin file'
    assert header[1] == 1, 'unsupported version'
    num_tokens = int(header[2]) # number of tokens (claimed)
    with open(path, 'rb', buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, 'number of tokens read does not match header'
    return tokens

class DistributedDataLoader:

    def __init__(self, filename_pattern):
        self.rank = int(os.environ['RANK'])
        self.world_size = int(os.environ['WORLD_SIZE'])
        self.files = sorted(glob.glob(filename_pattern))
        self.reset()

    def reset(self):
        self.current_shard = -1
        self.advance()

    def advance(self):
        self.current_shard = (self.current_shard + 1) % len(self.files)
        self.current_position = 0
        self.tokens = _load_data_shard(self.files[self.current_shard])

    def next_batch(self, batch_size):
        assert batch_size % self.world_size == 0
        device_batch_size = batch_size // self.world_size
        # load next shard if necessary
        if self.current_position + batch_size + 1 >= len(self.tokens):
            self.advance()
        pos = self.current_position + self.rank * device_batch_size
        device_batch_tokens = self.tokens[pos:pos+device_batch_size+1]
        # advance current position
        self.current_position += batch_size
        inputs = device_batch_tokens[:-1].to(device='cuda', dtype=torch.int32, non_blocking=True)
        targets = device_batch_tokens[1:].to(device='cuda', dtype=torch.int64, non_blocking=True)
        return inputs, targets

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files = 'data/fineweb10B/fineweb_train_*.bin' # input .bin to train on
    val_files = 'data/fineweb10B/fineweb_val_*.bin' # input .bin to eval validation loss on
    val_tokens = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # optimization
    batch_size = 8*64*1024 # batch size in tokens
    num_iterations = 1375 # number of iterations to run
    cooldown_frac = 0.4 # fraction of training spent cooling down the learning rate
    bf16_embeds = True
    # evaluation and logging
    val_loss_every = 125 # every how many steps to evaluate val loss? 0 for only at the end
    # implementation
    max_device_batch_size = 64*1024 # batch size per device in tokens
    save_checkpoint = False
args = Hyperparameters()

micro_bs = args.max_device_batch_size

# set up DDP (distributed data parallel). torchrun sets this env variable
rank = int(os.environ['RANK'])
local_rank = int(os.environ['LOCAL_RANK'])
world_size = int(os.environ['WORLD_SIZE'])
assert torch.cuda.is_available()
torch.cuda.set_device(local_rank)
dist.init_process_group(backend='nccl', device_id=torch.device(local_rank))
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = uuid.uuid4()
    os.makedirs('logs', exist_ok=True)
    logfile = f'logs/{run_id}.txt'
    print(logfile)

def print0(s, console=False):
    if master_process:
        with open(logfile, 'a') as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0('='*100)
# log information about the hardware/software environment this is running on
print0(f'Running Python {sys.version}')
print0(f'Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}')
print0(subprocess.run(['nvidia-smi'], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout)
print0('='*100)

# load data
train_loader = DistributedDataLoader(args.train_files)
val_loader = DistributedDataLoader(args.val_files)
print0(f'Training dataloader files: {train_loader.files}')
print0(f'Validation dataloader files: {val_loader.files}')
print0('='*100)

# there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency. suggested to me by @Grad62304977.
# this originates from Karpathy's experiments.
model = GPT(vocab_size=50304, num_layers=12, num_heads=6, model_dim=768)
model = model.cuda()
if args.bf16_embeds:
    for m in model.modules():
        if isinstance(m, nn.Embedding):
            m.bfloat16()
model: GPT = torch.compile(model)
ddp_model = DDP(model, device_ids=[local_rank], broadcast_buffers=False, gradient_as_bucket_view=True)

# collect the parameters to optimize
hidden_matrix_params = [p for p in model.blocks.parameters() if p.ndim == 2]
embed_params = [model.embed.weight, *model.value_embeds.parameters()]
scalar_params = [p for n, p in model.named_parameters() if p.ndim < 2 and "attn_scale" not in n]
attn_scale_params = [p for n, p in model.named_parameters() if p.ndim < 2 and "attn_scale" in n]
head_params = [model.lm_head.weight]

# init the optimizer(s)
optimizer1 = torch.optim.Adam([dict(params=embed_params, lr=0.6),
                               dict(params=head_params, lr=0.008),
                               dict(params=scalar_params, lr=0.04),
                               dict(params=attn_scale_params, lr=0.01)],
                              betas=(0.8, 0.95), fused=True)
optimizer2 = Muon(hidden_matrix_params, lr=0.05, momentum=0.95)
optimizers = [optimizer1, optimizer2]

# learning rate schedule: stable then decay
def get_lr(it):
    t = 1 - it / args.num_iterations # time remaining in training
    assert 1 >= t > 0
    # 1) constant lr for first part of training
    if t >= args.cooldown_frac:
        return 1.0
    # 2) then linear cooldown
    else:
        return t / args.cooldown_frac
schedulers = [torch.optim.lr_scheduler.LambdaLR(opt, get_lr) for opt in optimizers]

# sliding window size schedule: linear increase over training in chunks of 128 from 128 -> 1792. By @fernbear.bsky.social
def get_sliding_window_blocks(it):
    x = it / args.num_iterations # training progress
    assert 0 <= x <= 1
    return int(((1 - x) * 128 + x * 1856) // 128)
sliding_window_num_blocks = torch.tensor(1, dtype=torch.int32, device='cuda')

# Start training loop
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    # This effectively ignores timing first 10 steps, which are slower for weird reasons.
    # Alternately, and slightly more correctly in terms of benchmarking, we could do 10
    # steps with dummy data first, and then re-initialize the model and reset the loader.
    if step == 10:
        training_time_ms = 0
        t0 = time.perf_counter()
    timed_steps = float('nan') if step <= 11 else (step - 10) + 1 # <= 11 to avoid bug in val

    sliding_window_num_blocks.copy_(get_sliding_window_blocks(step))

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        # run validation batches
        model.eval()
        val_loader.reset()
        val_loss = 0.0
        # calculate the number of steps to take in the val loop.
        val_batch_size = world_size * micro_bs
        assert args.val_tokens % val_batch_size == 0
        val_steps = args.val_tokens // val_batch_size
        for _ in range(val_steps):
            with torch.no_grad():
                inputs_val, targets_val = val_loader.next_batch(val_batch_size)
                val_loss += ddp_model(inputs_val, targets_val, sliding_window_num_blocks)
        # Print attention scales
        for n, p in model.named_parameters():
            if p.ndim < 2 and "attn_scale" in n:
                print0(f'{n}: {p.item()}', console=True)
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        val_loss /= val_steps
        # logging
        print0(f'step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/(timed_steps-1):.2f}ms', console=True)
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f'logs/{run_id}', exist_ok=True)
            torch.save(log, f'logs/{run_id}/state_step{step:06d}.pt')
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    model.train()
    batch_size = args.batch_size
    assert batch_size % world_size == 0
    inputs_train, targets_train = train_loader.next_batch(batch_size)
    assert len(inputs_train) <= micro_bs or len(inputs_train) % micro_bs == 0
    for micro_inputs_train, micro_targets_train in zip(inputs_train.split(micro_bs), targets_train.split(micro_bs)):
        ddp_model(micro_inputs_train, micro_targets_train, sliding_window_num_blocks).backward()
    # momentum warmup for Muon
    frac = min(step/300, 1)
    for group in optimizer2.param_groups:
        group['momentum'] = (1 - frac) * 0.85 + frac * 0.95
    # step the optimizers and schedulers
    for opt, sched in zip(optimizers, schedulers):
        opt.step()
        if step != train_steps-1:
            sched.step()
    # null the gradients
    model.zero_grad(set_to_none=True)
    # logging
    approx_time = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f'step:{step+1}/{train_steps} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms', console=True)

print0(f'peak memory consumption: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB')
dist.destroy_process_group()

====================================================================================================
Running Python 3.10.12 (main, Sep 11 2024, 15:47:36) [GCC 11.4.0]
Running PyTorch 2.6.0.dev20241231+cu126 compiled for CUDA 12.6
Tue Jan 14 16:20:14 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.183.06             Driver Version: 535.183.06   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:19:00.0 Off |                    0 |
| N/A   34C    P0             123W / 700W |   7713MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:3B:00.0 Off |                    0 |
| N/A   30C    P0             113W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:4C:00.0 Off |                    0 |
| N/A   27C    P0             116W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   32C    P0             124W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:9B:00.0 Off |                    0 |
| N/A   33C    P0             120W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:BB:00.0 Off |                    0 |
| N/A   28C    P0             113W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:CB:00.0 Off |                    0 |
| N/A   32C    P0             113W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:DB:00.0 Off |                    0 |
| N/A   28C    P0             115W / 700W |   3211MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
Training dataloader files: ['data/fineweb10B/fineweb_train_000001.bin', 'data/fineweb10B/fineweb_train_000002.bin', 'data/fineweb10B/fineweb_train_000003.bin', 'data/fineweb10B/fineweb_train_000004.bin', 'data/fineweb10B/fineweb_train_000005.bin', 'data/fineweb10B/fineweb_train_000006.bin', 'data/fineweb10B/fineweb_train_000007.bin', 'data/fineweb10B/fineweb_train_000008.bin', 'data/fineweb10B/fineweb_train_000009.bin']
Validation dataloader files: ['data/fineweb10B/fineweb_val_000000.bin']
====================================================================================================
step:0/1375 val_loss:10.8258 train_time:0ms step_avg:nanms
step:1/1375 train_time:27417ms step_avg:nanms
step:2/1375 train_time:27516ms step_avg:nanms
step:3/1375 train_time:27699ms step_avg:nanms
step:4/1375 train_time:27833ms step_avg:nanms
step:5/1375 train_time:27968ms step_avg:nanms
step:6/1375 train_time:28100ms step_avg:nanms
step:7/1375 train_time:28233ms step_avg:nanms
step:8/1375 train_time:28366ms step_avg:nanms
step:9/1375 train_time:28499ms step_avg:nanms
step:10/1375 train_time:28638ms step_avg:nanms
step:11/1375 train_time:138ms step_avg:nanms
step:12/1375 train_time:273ms step_avg:nanms
step:13/1375 train_time:408ms step_avg:136.00ms
step:14/1375 train_time:542ms step_avg:135.53ms
step:15/1375 train_time:676ms step_avg:135.25ms
step:16/1375 train_time:808ms step_avg:134.68ms
step:17/1375 train_time:944ms step_avg:134.88ms
step:18/1375 train_time:1079ms step_avg:134.86ms
step:19/1375 train_time:1216ms step_avg:135.09ms
step:20/1375 train_time:1352ms step_avg:135.19ms
step:21/1375 train_time:1486ms step_avg:135.12ms
step:22/1375 train_time:1621ms step_avg:135.10ms
step:23/1375 train_time:1756ms step_avg:135.06ms
step:24/1375 train_time:1890ms step_avg:134.99ms
step:25/1375 train_time:2025ms step_avg:135.02ms
step:26/1375 train_time:2163ms step_avg:135.18ms
step:27/1375 train_time:2302ms step_avg:135.39ms
step:28/1375 train_time:2437ms step_avg:135.41ms
step:29/1375 train_time:2572ms step_avg:135.37ms
step:30/1375 train_time:2705ms step_avg:135.26ms
step:31/1375 train_time:2840ms step_avg:135.26ms
step:32/1375 train_time:2973ms step_avg:135.16ms
step:33/1375 train_time:3108ms step_avg:135.13ms
step:34/1375 train_time:3245ms step_avg:135.20ms
step:35/1375 train_time:3381ms step_avg:135.25ms
step:36/1375 train_time:3517ms step_avg:135.27ms
step:37/1375 train_time:3653ms step_avg:135.29ms
step:38/1375 train_time:3789ms step_avg:135.32ms
step:39/1375 train_time:3923ms step_avg:135.27ms
step:40/1375 train_time:4058ms step_avg:135.26ms
step:41/1375 train_time:4192ms step_avg:135.24ms
step:42/1375 train_time:4329ms step_avg:135.27ms
step:43/1375 train_time:4463ms step_avg:135.24ms
step:44/1375 train_time:4599ms step_avg:135.26ms
step:45/1375 train_time:4735ms step_avg:135.29ms
step:46/1375 train_time:4870ms step_avg:135.28ms
step:47/1375 train_time:5004ms step_avg:135.24ms
step:48/1375 train_time:5139ms step_avg:135.23ms
step:49/1375 train_time:5275ms step_avg:135.25ms
step:50/1375 train_time:5409ms step_avg:135.22ms
step:51/1375 train_time:5545ms step_avg:135.24ms
step:52/1375 train_time:5679ms step_avg:135.23ms
step:53/1375 train_time:5814ms step_avg:135.22ms
step:54/1375 train_time:5950ms step_avg:135.22ms
step:55/1375 train_time:6083ms step_avg:135.18ms
step:56/1375 train_time:6218ms step_avg:135.17ms
step:57/1375 train_time:6355ms step_avg:135.22ms
step:58/1375 train_time:6491ms step_avg:135.22ms
step:59/1375 train_time:6624ms step_avg:135.19ms
step:60/1375 train_time:6760ms step_avg:135.20ms
step:61/1375 train_time:6895ms step_avg:135.20ms
step:62/1375 train_time:7030ms step_avg:135.20ms
step:63/1375 train_time:7165ms step_avg:135.19ms
step:64/1375 train_time:7301ms step_avg:135.21ms
step:65/1375 train_time:7436ms step_avg:135.21ms
step:66/1375 train_time:7572ms step_avg:135.22ms
step:67/1375 train_time:7707ms step_avg:135.20ms
step:68/1375 train_time:7843ms step_avg:135.22ms
step:69/1375 train_time:7977ms step_avg:135.21ms
step:70/1375 train_time:8112ms step_avg:135.20ms
step:71/1375 train_time:8248ms step_avg:135.21ms
step:72/1375 train_time:8384ms step_avg:135.22ms
step:73/1375 train_time:8520ms step_avg:135.23ms
step:74/1375 train_time:8655ms step_avg:135.24ms
step:75/1375 train_time:8791ms step_avg:135.24ms
step:76/1375 train_time:8927ms step_avg:135.26ms
step:77/1375 train_time:9062ms step_avg:135.26ms
step:78/1375 train_time:9197ms step_avg:135.25ms
step:79/1375 train_time:9334ms step_avg:135.27ms
step:80/1375 train_time:9468ms step_avg:135.25ms
step:81/1375 train_time:9603ms step_avg:135.25ms
step:82/1375 train_time:9739ms step_avg:135.27ms
step:83/1375 train_time:9873ms step_avg:135.25ms
step:84/1375 train_time:10007ms step_avg:135.23ms
step:85/1375 train_time:10144ms step_avg:135.25ms
step:86/1375 train_time:10280ms step_avg:135.26ms
step:87/1375 train_time:10416ms step_avg:135.27ms
step:88/1375 train_time:10552ms step_avg:135.28ms
step:89/1375 train_time:10687ms step_avg:135.28ms
step:90/1375 train_time:10822ms step_avg:135.27ms
step:91/1375 train_time:10958ms step_avg:135.28ms
step:92/1375 train_time:11093ms step_avg:135.28ms
step:93/1375 train_time:11228ms step_avg:135.28ms
step:94/1375 train_time:11364ms step_avg:135.29ms
step:95/1375 train_time:11499ms step_avg:135.28ms
step:96/1375 train_time:11635ms step_avg:135.29ms
step:97/1375 train_time:11770ms step_avg:135.29ms
step:98/1375 train_time:11904ms step_avg:135.28ms
step:99/1375 train_time:12041ms step_avg:135.29ms
step:100/1375 train_time:12177ms step_avg:135.30ms
step:101/1375 train_time:12311ms step_avg:135.29ms
step:102/1375 train_time:12449ms step_avg:135.32ms
step:103/1375 train_time:12585ms step_avg:135.33ms
step:104/1375 train_time:12725ms step_avg:135.37ms
step:105/1375 train_time:12863ms step_avg:135.40ms
step:106/1375 train_time:13001ms step_avg:135.43ms
step:107/1375 train_time:13139ms step_avg:135.46ms
step:108/1375 train_time:13276ms step_avg:135.47ms
step:109/1375 train_time:13415ms step_avg:135.51ms
step:110/1375 train_time:13555ms step_avg:135.55ms
step:111/1375 train_time:13694ms step_avg:135.58ms
step:112/1375 train_time:13832ms step_avg:135.61ms
step:113/1375 train_time:13971ms step_avg:135.64ms
step:114/1375 train_time:14108ms step_avg:135.66ms
step:115/1375 train_time:14248ms step_avg:135.69ms
step:116/1375 train_time:14385ms step_avg:135.71ms
step:117/1375 train_time:14526ms step_avg:135.75ms
step:118/1375 train_time:14666ms step_avg:135.80ms
step:119/1375 train_time:14805ms step_avg:135.83ms
step:120/1375 train_time:14945ms step_avg:135.86ms
step:121/1375 train_time:15082ms step_avg:135.88ms
step:122/1375 train_time:15221ms step_avg:135.90ms
step:123/1375 train_time:15359ms step_avg:135.92ms
step:124/1375 train_time:15497ms step_avg:135.94ms
step:125/1375 train_time:15637ms step_avg:135.98ms
step:125/1375 val_loss:4.3701 train_time:15704ms step_avg:136.55ms
step:126/1375 train_time:15780ms step_avg:136.03ms
step:127/1375 train_time:15922ms step_avg:136.08ms
step:128/1375 train_time:16062ms step_avg:136.12ms
step:129/1375 train_time:16200ms step_avg:136.13ms
step:130/1375 train_time:16336ms step_avg:136.13ms
step:131/1375 train_time:16474ms step_avg:136.15ms
step:132/1375 train_time:16612ms step_avg:136.16ms
step:133/1375 train_time:16751ms step_avg:136.19ms
step:134/1375 train_time:16891ms step_avg:136.22ms
step:135/1375 train_time:17029ms step_avg:136.24ms
step:136/1375 train_time:17167ms step_avg:136.25ms
step:137/1375 train_time:17306ms step_avg:136.27ms
step:138/1375 train_time:17445ms step_avg:136.29ms
step:139/1375 train_time:17583ms step_avg:136.31ms
step:140/1375 train_time:17722ms step_avg:136.32ms
step:141/1375 train_time:17862ms step_avg:136.35ms
step:142/1375 train_time:18002ms step_avg:136.38ms
step:143/1375 train_time:18139ms step_avg:136.38ms
step:144/1375 train_time:18278ms step_avg:136.40ms
step:145/1375 train_time:18417ms step_avg:136.42ms
step:146/1375 train_time:18555ms step_avg:136.44ms
step:147/1375 train_time:18693ms step_avg:136.45ms
step:148/1375 train_time:18832ms step_avg:136.47ms
step:149/1375 train_time:18971ms step_avg:136.49ms
step:150/1375 train_time:19111ms step_avg:136.50ms
step:151/1375 train_time:19250ms step_avg:136.53ms
step:152/1375 train_time:19388ms step_avg:136.53ms
step:153/1375 train_time:19526ms step_avg:136.55ms
step:154/1375 train_time:19664ms step_avg:136.56ms
step:155/1375 train_time:19804ms step_avg:136.58ms
step:156/1375 train_time:19942ms step_avg:136.59ms
step:157/1375 train_time:20081ms step_avg:136.60ms
step:158/1375 train_time:20220ms step_avg:136.62ms
step:159/1375 train_time:20359ms step_avg:136.63ms
step:160/1375 train_time:20498ms step_avg:136.65ms
step:161/1375 train_time:20636ms step_avg:136.66ms
step:162/1375 train_time:20775ms step_avg:136.68ms
step:163/1375 train_time:20914ms step_avg:136.69ms
step:164/1375 train_time:21052ms step_avg:136.70ms
step:165/1375 train_time:21193ms step_avg:136.73ms
step:166/1375 train_time:21331ms step_avg:136.74ms
step:167/1375 train_time:21469ms step_avg:136.75ms
step:168/1375 train_time:21609ms step_avg:136.76ms
step:169/1375 train_time:21748ms step_avg:136.78ms
step:170/1375 train_time:21887ms step_avg:136.79ms
step:171/1375 train_time:22025ms step_avg:136.80ms
step:172/1375 train_time:22163ms step_avg:136.81ms
step:173/1375 train_time:22303ms step_avg:136.83ms
step:174/1375 train_time:22440ms step_avg:136.83ms
step:175/1375 train_time:22579ms step_avg:136.84ms
step:176/1375 train_time:22717ms step_avg:136.85ms
step:177/1375 train_time:22856ms step_avg:136.86ms
step:178/1375 train_time:22996ms step_avg:136.88ms
step:179/1375 train_time:23135ms step_avg:136.89ms
step:180/1375 train_time:23273ms step_avg:136.90ms
step:181/1375 train_time:23413ms step_avg:136.92ms
step:182/1375 train_time:23551ms step_avg:136.93ms
step:183/1375 train_time:23691ms step_avg:136.94ms
step:184/1375 train_time:23830ms step_avg:136.95ms
step:185/1375 train_time:23968ms step_avg:136.96ms
step:186/1375 train_time:24108ms step_avg:136.98ms
step:187/1375 train_time:24247ms step_avg:136.99ms
step:188/1375 train_time:24386ms step_avg:137.00ms
step:189/1375 train_time:24524ms step_avg:137.00ms
step:190/1375 train_time:24662ms step_avg:137.01ms
step:191/1375 train_time:24837ms step_avg:137.22ms
step:192/1375 train_time:24976ms step_avg:137.23ms
step:193/1375 train_time:25113ms step_avg:137.23ms
step:194/1375 train_time:25251ms step_avg:137.23ms
step:195/1375 train_time:25389ms step_avg:137.24ms
step:196/1375 train_time:25526ms step_avg:137.23ms
step:197/1375 train_time:25664ms step_avg:137.24ms
step:198/1375 train_time:25808ms step_avg:137.27ms
step:199/1375 train_time:25947ms step_avg:137.29ms
step:200/1375 train_time:26086ms step_avg:137.30ms
step:201/1375 train_time:26223ms step_avg:137.29ms
step:202/1375 train_time:26359ms step_avg:137.29ms
step:203/1375 train_time:26498ms step_avg:137.29ms
step:204/1375 train_time:26636ms step_avg:137.30ms
step:205/1375 train_time:26777ms step_avg:137.32ms
step:206/1375 train_time:26919ms step_avg:137.34ms
step:207/1375 train_time:27061ms step_avg:137.36ms
step:208/1375 train_time:27204ms step_avg:137.39ms
step:209/1375 train_time:27345ms step_avg:137.41ms
step:210/1375 train_time:27486ms step_avg:137.43ms
step:211/1375 train_time:27626ms step_avg:137.44ms
step:212/1375 train_time:27767ms step_avg:137.46ms
step:213/1375 train_time:27910ms step_avg:137.49ms
step:214/1375 train_time:28053ms step_avg:137.51ms
step:215/1375 train_time:28195ms step_avg:137.54ms
step:216/1375 train_time:28336ms step_avg:137.55ms
step:217/1375 train_time:28479ms step_avg:137.58ms
step:218/1375 train_time:28621ms step_avg:137.60ms
step:219/1375 train_time:28763ms step_avg:137.62ms
step:220/1375 train_time:28905ms step_avg:137.64ms
step:221/1375 train_time:29045ms step_avg:137.66ms
step:222/1375 train_time:29189ms step_avg:137.68ms
step:223/1375 train_time:29328ms step_avg:137.69ms
step:224/1375 train_time:29470ms step_avg:137.71ms
step:225/1375 train_time:29611ms step_avg:137.73ms
step:226/1375 train_time:29752ms step_avg:137.74ms
step:227/1375 train_time:29894ms step_avg:137.76ms
step:228/1375 train_time:30035ms step_avg:137.78ms
step:229/1375 train_time:30178ms step_avg:137.80ms
step:230/1375 train_time:30318ms step_avg:137.81ms
step:231/1375 train_time:30459ms step_avg:137.83ms
step:232/1375 train_time:30602ms step_avg:137.85ms
step:233/1375 train_time:30743ms step_avg:137.86ms
step:234/1375 train_time:30886ms step_avg:137.88ms
step:235/1375 train_time:31027ms step_avg:137.90ms
step:236/1375 train_time:31168ms step_avg:137.91ms
step:237/1375 train_time:31310ms step_avg:137.93ms
step:238/1375 train_time:31453ms step_avg:137.95ms
step:239/1375 train_time:31595ms step_avg:137.97ms
step:240/1375 train_time:31735ms step_avg:137.98ms
step:241/1375 train_time:31879ms step_avg:138.00ms
step:242/1375 train_time:32023ms step_avg:138.03ms
step:243/1375 train_time:32164ms step_avg:138.04ms
step:244/1375 train_time:32305ms step_avg:138.05ms
step:245/1375 train_time:32447ms step_avg:138.07ms
step:246/1375 train_time:32589ms step_avg:138.09ms
step:247/1375 train_time:32731ms step_avg:138.11ms
step:248/1375 train_time:32871ms step_avg:138.11ms
step:249/1375 train_time:33014ms step_avg:138.13ms
step:250/1375 train_time:33155ms step_avg:138.14ms
step:250/1375 val_loss:3.9517 train_time:33224ms step_avg:138.43ms
step:251/1375 train_time:33299ms step_avg:138.17ms
step:252/1375 train_time:33446ms step_avg:138.21ms
step:253/1375 train_time:33587ms step_avg:138.22ms
step:254/1375 train_time:33728ms step_avg:138.23ms
step:255/1375 train_time:33868ms step_avg:138.24ms
step:256/1375 train_time:34009ms step_avg:138.25ms
step:257/1375 train_time:34151ms step_avg:138.26ms
step:258/1375 train_time:34293ms step_avg:138.28ms
step:259/1375 train_time:34437ms step_avg:138.30ms
step:260/1375 train_time:34579ms step_avg:138.31ms
step:261/1375 train_time:34720ms step_avg:138.33ms
step:262/1375 train_time:34861ms step_avg:138.34ms
step:263/1375 train_time:35002ms step_avg:138.35ms
step:264/1375 train_time:35143ms step_avg:138.36ms
step:265/1375 train_time:35284ms step_avg:138.37ms
step:266/1375 train_time:35427ms step_avg:138.39ms
step:267/1375 train_time:35569ms step_avg:138.40ms
step:268/1375 train_time:35712ms step_avg:138.42ms
step:269/1375 train_time:35854ms step_avg:138.43ms
step:270/1375 train_time:35994ms step_avg:138.44ms
step:271/1375 train_time:36137ms step_avg:138.46ms
step:272/1375 train_time:36278ms step_avg:138.47ms
step:273/1375 train_time:36421ms step_avg:138.48ms
step:274/1375 train_time:36562ms step_avg:138.49ms
step:275/1375 train_time:36704ms step_avg:138.51ms
step:276/1375 train_time:36845ms step_avg:138.51ms
step:277/1375 train_time:36986ms step_avg:138.52ms
step:278/1375 train_time:37127ms step_avg:138.53ms
step:279/1375 train_time:37269ms step_avg:138.55ms
step:280/1375 train_time:37412ms step_avg:138.56ms
step:281/1375 train_time:37553ms step_avg:138.57ms
step:282/1375 train_time:37695ms step_avg:138.58ms
step:283/1375 train_time:37838ms step_avg:138.60ms
step:284/1375 train_time:37980ms step_avg:138.61ms
step:285/1375 train_time:38121ms step_avg:138.62ms
step:286/1375 train_time:38261ms step_avg:138.63ms
step:287/1375 train_time:38403ms step_avg:138.64ms
step:288/1375 train_time:38545ms step_avg:138.65ms
step:289/1375 train_time:38686ms step_avg:138.66ms
step:290/1375 train_time:38829ms step_avg:138.68ms
step:291/1375 train_time:38969ms step_avg:138.68ms
step:292/1375 train_time:39113ms step_avg:138.70ms
step:293/1375 train_time:39254ms step_avg:138.71ms
step:294/1375 train_time:39395ms step_avg:138.72ms
step:295/1375 train_time:39539ms step_avg:138.73ms
step:296/1375 train_time:39681ms step_avg:138.75ms
step:297/1375 train_time:39823ms step_avg:138.76ms
step:298/1375 train_time:39964ms step_avg:138.76ms
step:299/1375 train_time:40106ms step_avg:138.77ms
step:300/1375 train_time:40247ms step_avg:138.78ms
step:301/1375 train_time:40389ms step_avg:138.79ms
step:302/1375 train_time:40533ms step_avg:138.81ms
step:303/1375 train_time:40675ms step_avg:138.82ms
step:304/1375 train_time:40817ms step_avg:138.83ms
step:305/1375 train_time:40958ms step_avg:138.84ms
step:306/1375 train_time:41100ms step_avg:138.85ms
step:307/1375 train_time:41243ms step_avg:138.87ms
step:308/1375 train_time:41388ms step_avg:138.89ms
step:309/1375 train_time:41534ms step_avg:138.91ms
step:310/1375 train_time:41676ms step_avg:138.92ms
step:311/1375 train_time:41821ms step_avg:138.94ms
step:312/1375 train_time:41964ms step_avg:138.95ms
step:313/1375 train_time:42109ms step_avg:138.98ms
step:314/1375 train_time:42255ms step_avg:139.00ms
step:315/1375 train_time:42398ms step_avg:139.01ms
step:316/1375 train_time:42543ms step_avg:139.03ms
step:317/1375 train_time:42686ms step_avg:139.04ms
step:318/1375 train_time:42829ms step_avg:139.06ms
step:319/1375 train_time:42972ms step_avg:139.07ms
step:320/1375 train_time:43116ms step_avg:139.08ms
step:321/1375 train_time:43260ms step_avg:139.10ms
step:322/1375 train_time:43403ms step_avg:139.11ms
step:323/1375 train_time:43548ms step_avg:139.13ms
step:324/1375 train_time:43694ms step_avg:139.15ms
step:325/1375 train_time:43838ms step_avg:139.17ms
step:326/1375 train_time:43981ms step_avg:139.18ms
step:327/1375 train_time:44125ms step_avg:139.19ms
step:328/1375 train_time:44268ms step_avg:139.21ms
step:329/1375 train_time:44413ms step_avg:139.23ms
step:330/1375 train_time:44557ms step_avg:139.24ms
step:331/1375 train_time:44701ms step_avg:139.26ms
step:332/1375 train_time:44847ms step_avg:139.28ms
step:333/1375 train_time:44991ms step_avg:139.29ms
step:334/1375 train_time:45135ms step_avg:139.31ms
step:335/1375 train_time:45278ms step_avg:139.32ms
step:336/1375 train_time:45421ms step_avg:139.33ms
step:337/1375 train_time:45565ms step_avg:139.34ms
step:338/1375 train_time:45708ms step_avg:139.35ms
step:339/1375 train_time:45853ms step_avg:139.37ms
step:340/1375 train_time:45997ms step_avg:139.38ms
step:341/1375 train_time:46141ms step_avg:139.40ms
step:342/1375 train_time:46285ms step_avg:139.41ms
step:343/1375 train_time:46429ms step_avg:139.43ms
step:344/1375 train_time:46574ms step_avg:139.44ms
step:345/1375 train_time:46717ms step_avg:139.45ms
step:346/1375 train_time:46860ms step_avg:139.46ms
step:347/1375 train_time:47005ms step_avg:139.48ms
step:348/1375 train_time:47149ms step_avg:139.49ms
step:349/1375 train_time:47293ms step_avg:139.51ms
step:350/1375 train_time:47439ms step_avg:139.53ms
step:351/1375 train_time:47583ms step_avg:139.54ms
step:352/1375 train_time:47725ms step_avg:139.55ms
step:353/1375 train_time:47867ms step_avg:139.55ms
step:354/1375 train_time:48011ms step_avg:139.57ms
step:355/1375 train_time:48155ms step_avg:139.58ms
step:356/1375 train_time:48297ms step_avg:139.59ms
step:357/1375 train_time:48441ms step_avg:139.60ms
step:358/1375 train_time:48586ms step_avg:139.61ms
step:359/1375 train_time:48730ms step_avg:139.63ms
step:360/1375 train_time:48873ms step_avg:139.64ms
step:361/1375 train_time:49016ms step_avg:139.65ms
step:362/1375 train_time:49160ms step_avg:139.66ms
step:363/1375 train_time:49303ms step_avg:139.67ms
step:364/1375 train_time:49449ms step_avg:139.69ms
step:365/1375 train_time:49593ms step_avg:139.70ms
step:366/1375 train_time:49737ms step_avg:139.71ms
step:367/1375 train_time:49880ms step_avg:139.72ms
step:368/1375 train_time:50024ms step_avg:139.73ms
step:369/1375 train_time:50167ms step_avg:139.74ms
step:370/1375 train_time:50312ms step_avg:139.75ms
step:371/1375 train_time:50457ms step_avg:139.77ms
step:372/1375 train_time:50599ms step_avg:139.78ms
step:373/1375 train_time:50744ms step_avg:139.79ms
step:374/1375 train_time:50888ms step_avg:139.80ms
step:375/1375 train_time:51031ms step_avg:139.81ms
step:375/1375 val_loss:3.7677 train_time:51100ms step_avg:140.00ms
step:376/1375 train_time:51174ms step_avg:139.82ms
step:377/1375 train_time:51323ms step_avg:139.84ms
step:378/1375 train_time:51468ms step_avg:139.86ms
step:379/1375 train_time:51609ms step_avg:139.86ms
step:380/1375 train_time:51755ms step_avg:139.88ms
step:381/1375 train_time:51939ms step_avg:140.00ms
step:382/1375 train_time:52081ms step_avg:140.00ms
step:383/1375 train_time:52224ms step_avg:140.01ms
step:384/1375 train_time:52366ms step_avg:140.02ms
step:385/1375 train_time:52507ms step_avg:140.02ms
step:386/1375 train_time:52650ms step_avg:140.03ms
step:387/1375 train_time:52794ms step_avg:140.04ms
step:388/1375 train_time:52942ms step_avg:140.06ms
step:389/1375 train_time:53087ms step_avg:140.07ms
step:390/1375 train_time:53230ms step_avg:140.08ms
step:391/1375 train_time:53372ms step_avg:140.08ms
step:392/1375 train_time:53515ms step_avg:140.09ms
step:393/1375 train_time:53658ms step_avg:140.10ms
step:394/1375 train_time:53803ms step_avg:140.11ms
step:395/1375 train_time:53950ms step_avg:140.13ms
step:396/1375 train_time:54094ms step_avg:140.14ms
step:397/1375 train_time:54239ms step_avg:140.15ms
step:398/1375 train_time:54383ms step_avg:140.16ms
step:399/1375 train_time:54526ms step_avg:140.17ms
step:400/1375 train_time:54669ms step_avg:140.18ms
step:401/1375 train_time:54812ms step_avg:140.18ms
step:402/1375 train_time:54959ms step_avg:140.20ms
step:403/1375 train_time:55104ms step_avg:140.21ms
step:404/1375 train_time:55248ms step_avg:140.22ms
step:405/1375 train_time:55390ms step_avg:140.23ms
step:406/1375 train_time:55534ms step_avg:140.24ms
step:407/1375 train_time:55677ms step_avg:140.25ms
step:408/1375 train_time:55822ms step_avg:140.26ms
step:409/1375 train_time:55969ms step_avg:140.27ms
step:410/1375 train_time:56115ms step_avg:140.29ms
step:411/1375 train_time:56263ms step_avg:140.31ms
step:412/1375 train_time:56407ms step_avg:140.32ms
step:413/1375 train_time:56554ms step_avg:140.33ms
step:414/1375 train_time:56699ms step_avg:140.35ms
step:415/1375 train_time:56847ms step_avg:140.36ms
step:416/1375 train_time:56991ms step_avg:140.37ms
step:417/1375 train_time:57138ms step_avg:140.39ms
step:418/1375 train_time:57284ms step_avg:140.40ms
step:419/1375 train_time:57430ms step_avg:140.42ms
step:420/1375 train_time:57575ms step_avg:140.43ms
step:421/1375 train_time:57719ms step_avg:140.43ms
step:422/1375 train_time:57865ms step_avg:140.45ms
step:423/1375 train_time:58009ms step_avg:140.46ms
step:424/1375 train_time:58156ms step_avg:140.47ms
step:425/1375 train_time:58304ms step_avg:140.49ms
step:426/1375 train_time:58450ms step_avg:140.50ms
step:427/1375 train_time:58595ms step_avg:140.51ms
step:428/1375 train_time:58741ms step_avg:140.53ms
step:429/1375 train_time:58885ms step_avg:140.54ms
step:430/1375 train_time:59029ms step_avg:140.55ms
step:431/1375 train_time:59175ms step_avg:140.56ms
step:432/1375 train_time:59322ms step_avg:140.57ms
step:433/1375 train_time:59469ms step_avg:140.59ms
step:434/1375 train_time:59614ms step_avg:140.60ms
step:435/1375 train_time:59759ms step_avg:140.61ms
step:436/1375 train_time:59904ms step_avg:140.62ms
step:437/1375 train_time:60050ms step_avg:140.63ms
step:438/1375 train_time:60195ms step_avg:140.64ms
step:439/1375 train_time:60341ms step_avg:140.66ms
step:440/1375 train_time:60488ms step_avg:140.67ms
step:441/1375 train_time:60634ms step_avg:140.68ms
step:442/1375 train_time:60778ms step_avg:140.69ms
step:443/1375 train_time:60926ms step_avg:140.71ms
step:444/1375 train_time:61070ms step_avg:140.71ms
step:445/1375 train_time:61214ms step_avg:140.72ms
step:446/1375 train_time:61360ms step_avg:140.73ms
step:447/1375 train_time:61506ms step_avg:140.75ms
step:448/1375 train_time:61652ms step_avg:140.76ms
step:449/1375 train_time:61798ms step_avg:140.77ms
step:450/1375 train_time:61946ms step_avg:140.79ms
step:451/1375 train_time:62090ms step_avg:140.79ms
step:452/1375 train_time:62235ms step_avg:140.80ms
step:453/1375 train_time:62380ms step_avg:140.81ms
step:454/1375 train_time:62527ms step_avg:140.83ms
step:455/1375 train_time:62672ms step_avg:140.84ms
step:456/1375 train_time:62818ms step_avg:140.85ms
step:457/1375 train_time:62965ms step_avg:140.86ms
step:458/1375 train_time:63109ms step_avg:140.87ms
step:459/1375 train_time:63256ms step_avg:140.88ms
step:460/1375 train_time:63400ms step_avg:140.89ms
step:461/1375 train_time:63547ms step_avg:140.90ms
step:462/1375 train_time:63693ms step_avg:140.91ms
step:463/1375 train_time:63841ms step_avg:140.93ms
step:464/1375 train_time:63986ms step_avg:140.94ms
step:465/1375 train_time:64129ms step_avg:140.94ms
step:466/1375 train_time:64274ms step_avg:140.95ms
step:467/1375 train_time:64420ms step_avg:140.96ms
step:468/1375 train_time:64568ms step_avg:140.98ms
step:469/1375 train_time:64711ms step_avg:140.98ms
step:470/1375 train_time:64858ms step_avg:141.00ms
step:471/1375 train_time:65005ms step_avg:141.01ms
step:472/1375 train_time:65152ms step_avg:141.02ms
step:473/1375 train_time:65298ms step_avg:141.03ms
step:474/1375 train_time:65444ms step_avg:141.04ms
step:475/1375 train_time:65588ms step_avg:141.05ms
step:476/1375 train_time:65733ms step_avg:141.06ms
step:477/1375 train_time:65878ms step_avg:141.07ms
step:478/1375 train_time:66026ms step_avg:141.08ms
step:479/1375 train_time:66170ms step_avg:141.09ms
step:480/1375 train_time:66316ms step_avg:141.10ms
step:481/1375 train_time:66463ms step_avg:141.11ms
step:482/1375 train_time:66607ms step_avg:141.12ms
step:483/1375 train_time:66753ms step_avg:141.13ms
step:484/1375 train_time:66898ms step_avg:141.14ms
step:485/1375 train_time:67044ms step_avg:141.15ms
step:486/1375 train_time:67189ms step_avg:141.15ms
step:487/1375 train_time:67334ms step_avg:141.16ms
step:488/1375 train_time:67479ms step_avg:141.17ms
step:489/1375 train_time:67626ms step_avg:141.18ms
step:490/1375 train_time:67771ms step_avg:141.19ms
step:491/1375 train_time:67917ms step_avg:141.20ms
step:492/1375 train_time:68064ms step_avg:141.21ms
step:493/1375 train_time:68208ms step_avg:141.22ms
step:494/1375 train_time:68354ms step_avg:141.23ms
step:495/1375 train_time:68500ms step_avg:141.24ms
step:496/1375 train_time:68647ms step_avg:141.25ms
step:497/1375 train_time:68791ms step_avg:141.25ms
step:498/1375 train_time:68936ms step_avg:141.26ms
step:499/1375 train_time:69081ms step_avg:141.27ms
step:500/1375 train_time:69228ms step_avg:141.28ms
step:500/1375 val_loss:3.6532 train_time:69298ms step_avg:141.42ms
step:501/1375 train_time:69373ms step_avg:141.29ms
step:502/1375 train_time:69521ms step_avg:141.30ms
step:503/1375 train_time:69667ms step_avg:141.31ms
step:504/1375 train_time:69811ms step_avg:141.32ms
step:505/1375 train_time:69955ms step_avg:141.32ms
step:506/1375 train_time:70101ms step_avg:141.33ms
step:507/1375 train_time:70248ms step_avg:141.34ms
step:508/1375 train_time:70395ms step_avg:141.36ms
step:509/1375 train_time:70543ms step_avg:141.37ms
step:510/1375 train_time:70687ms step_avg:141.37ms
step:511/1375 train_time:70833ms step_avg:141.38ms
step:512/1375 train_time:70982ms step_avg:141.40ms
step:513/1375 train_time:71129ms step_avg:141.41ms
step:514/1375 train_time:71277ms step_avg:141.42ms
step:515/1375 train_time:71426ms step_avg:141.44ms
step:516/1375 train_time:71573ms step_avg:141.45ms
step:517/1375 train_time:71720ms step_avg:141.46ms
step:518/1375 train_time:71867ms step_avg:141.47ms
step:519/1375 train_time:72013ms step_avg:141.48ms
step:520/1375 train_time:72161ms step_avg:141.49ms
step:521/1375 train_time:72308ms step_avg:141.50ms
step:522/1375 train_time:72454ms step_avg:141.51ms
step:523/1375 train_time:72602ms step_avg:141.53ms
step:524/1375 train_time:72749ms step_avg:141.54ms
step:525/1375 train_time:72895ms step_avg:141.54ms
step:526/1375 train_time:73045ms step_avg:141.56ms
step:527/1375 train_time:73190ms step_avg:141.57ms
step:528/1375 train_time:73339ms step_avg:141.58ms
step:529/1375 train_time:73486ms step_avg:141.59ms
step:530/1375 train_time:73634ms step_avg:141.60ms
step:531/1375 train_time:73782ms step_avg:141.62ms
step:532/1375 train_time:73929ms step_avg:141.63ms
step:533/1375 train_time:74075ms step_avg:141.64ms
step:534/1375 train_time:74223ms step_avg:141.65ms
step:535/1375 train_time:74370ms step_avg:141.66ms
step:536/1375 train_time:74517ms step_avg:141.67ms
step:537/1375 train_time:74666ms step_avg:141.68ms
step:538/1375 train_time:74813ms step_avg:141.69ms
step:539/1375 train_time:74961ms step_avg:141.70ms
step:540/1375 train_time:75108ms step_avg:141.71ms
step:541/1375 train_time:75254ms step_avg:141.72ms
step:542/1375 train_time:75403ms step_avg:141.73ms
step:543/1375 train_time:75550ms step_avg:141.75ms
step:544/1375 train_time:75697ms step_avg:141.76ms
step:545/1375 train_time:75844ms step_avg:141.77ms
step:546/1375 train_time:75990ms step_avg:141.77ms
step:547/1375 train_time:76139ms step_avg:141.79ms
step:548/1375 train_time:76287ms step_avg:141.80ms
step:549/1375 train_time:76434ms step_avg:141.81ms
step:550/1375 train_time:76584ms step_avg:141.82ms
step:551/1375 train_time:76731ms step_avg:141.83ms
step:552/1375 train_time:76878ms step_avg:141.84ms
step:553/1375 train_time:77028ms step_avg:141.86ms
step:554/1375 train_time:77173ms step_avg:141.86ms
step:555/1375 train_time:77323ms step_avg:141.88ms
step:556/1375 train_time:77469ms step_avg:141.88ms
step:557/1375 train_time:77618ms step_avg:141.90ms
step:558/1375 train_time:77767ms step_avg:141.91ms
step:559/1375 train_time:77914ms step_avg:141.92ms
step:560/1375 train_time:78062ms step_avg:141.93ms
step:561/1375 train_time:78207ms step_avg:141.94ms
step:562/1375 train_time:78356ms step_avg:141.95ms
step:563/1375 train_time:78503ms step_avg:141.96ms
step:564/1375 train_time:78650ms step_avg:141.97ms
step:565/1375 train_time:78797ms step_avg:141.98ms
step:566/1375 train_time:78945ms step_avg:141.99ms
step:567/1375 train_time:79091ms step_avg:141.99ms
step:568/1375 train_time:79240ms step_avg:142.01ms
step:569/1375 train_time:79388ms step_avg:142.02ms
step:570/1375 train_time:79534ms step_avg:142.02ms
step:571/1375 train_time:79720ms step_avg:142.10ms
step:572/1375 train_time:79866ms step_avg:142.11ms
step:573/1375 train_time:80011ms step_avg:142.12ms
step:574/1375 train_time:80159ms step_avg:142.13ms
step:575/1375 train_time:80305ms step_avg:142.13ms
step:576/1375 train_time:80450ms step_avg:142.14ms
step:577/1375 train_time:80598ms step_avg:142.15ms
step:578/1375 train_time:80748ms step_avg:142.16ms
step:579/1375 train_time:80895ms step_avg:142.17ms
step:580/1375 train_time:81045ms step_avg:142.18ms
step:581/1375 train_time:81190ms step_avg:142.19ms
step:582/1375 train_time:81338ms step_avg:142.20ms
step:583/1375 train_time:81484ms step_avg:142.21ms
step:584/1375 train_time:81632ms step_avg:142.22ms
step:585/1375 train_time:81779ms step_avg:142.23ms
step:586/1375 train_time:81928ms step_avg:142.24ms
step:587/1375 train_time:82075ms step_avg:142.24ms
step:588/1375 train_time:82225ms step_avg:142.26ms
step:589/1375 train_time:82371ms step_avg:142.26ms
step:590/1375 train_time:82518ms step_avg:142.27ms
step:591/1375 train_time:82665ms step_avg:142.28ms
step:592/1375 train_time:82814ms step_avg:142.29ms
step:593/1375 train_time:82963ms step_avg:142.30ms
step:594/1375 train_time:83109ms step_avg:142.31ms
step:595/1375 train_time:83256ms step_avg:142.32ms
step:596/1375 train_time:83403ms step_avg:142.33ms
step:597/1375 train_time:83550ms step_avg:142.33ms
step:598/1375 train_time:83697ms step_avg:142.34ms
step:599/1375 train_time:83846ms step_avg:142.35ms
step:600/1375 train_time:83992ms step_avg:142.36ms
step:601/1375 train_time:84140ms step_avg:142.37ms
step:602/1375 train_time:84287ms step_avg:142.38ms
step:603/1375 train_time:84435ms step_avg:142.39ms
step:604/1375 train_time:84582ms step_avg:142.39ms
step:605/1375 train_time:84729ms step_avg:142.40ms
step:606/1375 train_time:84876ms step_avg:142.41ms
step:607/1375 train_time:85025ms step_avg:142.42ms
step:608/1375 train_time:85170ms step_avg:142.42ms
step:609/1375 train_time:85317ms step_avg:142.43ms
step:610/1375 train_time:85465ms step_avg:142.44ms
step:611/1375 train_time:85610ms step_avg:142.45ms
step:612/1375 train_time:85759ms step_avg:142.46ms
step:613/1375 train_time:85907ms step_avg:142.47ms
step:614/1375 train_time:86057ms step_avg:142.48ms
step:615/1375 train_time:86205ms step_avg:142.49ms
step:616/1375 train_time:86353ms step_avg:142.50ms
step:617/1375 train_time:86503ms step_avg:142.51ms
step:618/1375 train_time:86651ms step_avg:142.52ms
step:619/1375 train_time:86801ms step_avg:142.53ms
step:620/1375 train_time:86950ms step_avg:142.54ms
step:621/1375 train_time:87100ms step_avg:142.55ms
step:622/1375 train_time:87249ms step_avg:142.56ms
step:623/1375 train_time:87398ms step_avg:142.57ms
step:624/1375 train_time:87547ms step_avg:142.59ms
step:625/1375 train_time:87694ms step_avg:142.59ms
step:625/1375 val_loss:3.5721 train_time:87772ms step_avg:142.72ms
step:626/1375 train_time:87847ms step_avg:142.61ms
step:627/1375 train_time:87995ms step_avg:142.62ms
step:628/1375 train_time:88144ms step_avg:142.63ms
step:629/1375 train_time:88290ms step_avg:142.63ms
step:630/1375 train_time:88438ms step_avg:142.64ms
step:631/1375 train_time:88585ms step_avg:142.65ms
step:632/1375 train_time:88734ms step_avg:142.66ms
step:633/1375 train_time:88884ms step_avg:142.67ms
step:634/1375 train_time:89033ms step_avg:142.68ms
step:635/1375 train_time:89182ms step_avg:142.69ms
step:636/1375 train_time:89331ms step_avg:142.70ms
step:637/1375 train_time:89480ms step_avg:142.71ms
step:638/1375 train_time:89628ms step_avg:142.72ms
step:639/1375 train_time:89776ms step_avg:142.73ms
step:640/1375 train_time:89926ms step_avg:142.74ms
step:641/1375 train_time:90075ms step_avg:142.75ms
step:642/1375 train_time:90226ms step_avg:142.76ms
step:643/1375 train_time:90374ms step_avg:142.77ms
step:644/1375 train_time:90524ms step_avg:142.78ms
step:645/1375 train_time:90671ms step_avg:142.79ms
step:646/1375 train_time:90822ms step_avg:142.80ms
step:647/1375 train_time:90970ms step_avg:142.81ms
step:648/1375 train_time:91122ms step_avg:142.82ms
step:649/1375 train_time:91270ms step_avg:142.83ms
step:650/1375 train_time:91420ms step_avg:142.84ms
step:651/1375 train_time:91569ms step_avg:142.85ms
step:652/1375 train_time:91720ms step_avg:142.87ms
step:653/1375 train_time:91867ms step_avg:142.87ms
step:654/1375 train_time:92016ms step_avg:142.88ms
step:655/1375 train_time:92165ms step_avg:142.89ms
step:656/1375 train_time:92312ms step_avg:142.90ms
step:657/1375 train_time:92460ms step_avg:142.91ms
step:658/1375 train_time:92611ms step_avg:142.92ms
step:659/1375 train_time:92761ms step_avg:142.93ms
step:660/1375 train_time:92909ms step_avg:142.94ms
step:661/1375 train_time:93059ms step_avg:142.95ms
step:662/1375 train_time:93208ms step_avg:142.96ms
step:663/1375 train_time:93354ms step_avg:142.96ms
step:664/1375 train_time:93505ms step_avg:142.97ms
step:665/1375 train_time:93653ms step_avg:142.98ms
step:666/1375 train_time:93802ms step_avg:142.99ms
step:667/1375 train_time:93950ms step_avg:143.00ms
step:668/1375 train_time:94100ms step_avg:143.01ms
step:669/1375 train_time:94250ms step_avg:143.02ms
step:670/1375 train_time:94398ms step_avg:143.03ms
step:671/1375 train_time:94548ms step_avg:143.04ms
step:672/1375 train_time:94696ms step_avg:143.05ms
step:673/1375 train_time:94844ms step_avg:143.05ms
step:674/1375 train_time:94992ms step_avg:143.06ms
step:675/1375 train_time:95143ms step_avg:143.07ms
step:676/1375 train_time:95291ms step_avg:143.08ms
step:677/1375 train_time:95442ms step_avg:143.09ms
step:678/1375 train_time:95589ms step_avg:143.10ms
step:679/1375 train_time:95740ms step_avg:143.11ms
step:680/1375 train_time:95888ms step_avg:143.12ms
step:681/1375 train_time:96036ms step_avg:143.12ms
step:682/1375 train_time:96184ms step_avg:143.13ms
step:683/1375 train_time:96332ms step_avg:143.14ms
step:684/1375 train_time:96481ms step_avg:143.15ms
step:685/1375 train_time:96630ms step_avg:143.16ms
step:686/1375 train_time:96778ms step_avg:143.16ms
step:687/1375 train_time:96927ms step_avg:143.17ms
step:688/1375 train_time:97075ms step_avg:143.18ms
step:689/1375 train_time:97226ms step_avg:143.19ms
step:690/1375 train_time:97375ms step_avg:143.20ms
step:691/1375 train_time:97525ms step_avg:143.21ms
step:692/1375 train_time:97672ms step_avg:143.21ms
step:693/1375 train_time:97821ms step_avg:143.22ms
step:694/1375 train_time:97969ms step_avg:143.23ms
step:695/1375 train_time:98117ms step_avg:143.24ms
step:696/1375 train_time:98267ms step_avg:143.25ms
step:697/1375 train_time:98415ms step_avg:143.25ms
step:698/1375 train_time:98564ms step_avg:143.26ms
step:699/1375 train_time:98712ms step_avg:143.27ms
step:700/1375 train_time:98860ms step_avg:143.28ms
step:701/1375 train_time:99011ms step_avg:143.29ms
step:702/1375 train_time:99160ms step_avg:143.29ms
step:703/1375 train_time:99309ms step_avg:143.30ms
step:704/1375 train_time:99456ms step_avg:143.31ms
step:705/1375 train_time:99605ms step_avg:143.32ms
step:706/1375 train_time:99754ms step_avg:143.32ms
step:707/1375 train_time:99906ms step_avg:143.34ms
step:708/1375 train_time:100052ms step_avg:143.34ms
step:709/1375 train_time:100202ms step_avg:143.35ms
step:710/1375 train_time:100351ms step_avg:143.36ms
step:711/1375 train_time:100501ms step_avg:143.37ms
step:712/1375 train_time:100650ms step_avg:143.38ms
step:713/1375 train_time:100800ms step_avg:143.39ms
step:714/1375 train_time:100950ms step_avg:143.39ms
step:715/1375 train_time:101099ms step_avg:143.40ms
step:716/1375 train_time:101250ms step_avg:143.41ms
step:717/1375 train_time:101401ms step_avg:143.42ms
step:718/1375 train_time:101551ms step_avg:143.43ms
step:719/1375 train_time:101700ms step_avg:143.44ms
step:720/1375 train_time:101851ms step_avg:143.45ms
step:721/1375 train_time:102001ms step_avg:143.46ms
step:722/1375 train_time:102152ms step_avg:143.47ms
step:723/1375 train_time:102302ms step_avg:143.48ms
step:724/1375 train_time:102450ms step_avg:143.49ms
step:725/1375 train_time:102600ms step_avg:143.50ms
step:726/1375 train_time:102750ms step_avg:143.51ms
step:727/1375 train_time:102904ms step_avg:143.52ms
step:728/1375 train_time:103052ms step_avg:143.53ms
step:729/1375 train_time:103201ms step_avg:143.53ms
step:730/1375 train_time:103354ms step_avg:143.55ms
step:731/1375 train_time:103503ms step_avg:143.55ms
step:732/1375 train_time:103652ms step_avg:143.56ms
step:733/1375 train_time:103804ms step_avg:143.57ms
step:734/1375 train_time:103953ms step_avg:143.58ms
step:735/1375 train_time:104105ms step_avg:143.59ms
step:736/1375 train_time:104254ms step_avg:143.60ms
step:737/1375 train_time:104405ms step_avg:143.61ms
step:738/1375 train_time:104554ms step_avg:143.62ms
step:739/1375 train_time:104705ms step_avg:143.63ms
step:740/1375 train_time:104854ms step_avg:143.64ms
step:741/1375 train_time:105005ms step_avg:143.65ms
step:742/1375 train_time:105153ms step_avg:143.65ms
step:743/1375 train_time:105304ms step_avg:143.66ms
step:744/1375 train_time:105453ms step_avg:143.67ms
step:745/1375 train_time:105607ms step_avg:143.68ms
step:746/1375 train_time:105755ms step_avg:143.69ms
step:747/1375 train_time:105904ms step_avg:143.70ms
step:748/1375 train_time:106054ms step_avg:143.70ms
step:749/1375 train_time:106206ms step_avg:143.72ms
step:750/1375 train_time:106355ms step_avg:143.72ms
step:750/1375 val_loss:3.5184 train_time:106433ms step_avg:143.83ms
step:751/1375 train_time:106509ms step_avg:143.74ms
step:752/1375 train_time:106663ms step_avg:143.75ms
step:753/1375 train_time:106814ms step_avg:143.76ms
step:754/1375 train_time:106960ms step_avg:143.76ms
step:755/1375 train_time:107111ms step_avg:143.77ms
step:756/1375 train_time:107260ms step_avg:143.78ms
step:757/1375 train_time:107413ms step_avg:143.79ms
step:758/1375 train_time:107563ms step_avg:143.80ms
step:759/1375 train_time:107715ms step_avg:143.81ms
step:760/1375 train_time:107862ms step_avg:143.82ms
step:761/1375 train_time:108051ms step_avg:143.88ms
step:762/1375 train_time:108200ms step_avg:143.88ms
step:763/1375 train_time:108350ms step_avg:143.89ms
step:764/1375 train_time:108500ms step_avg:143.90ms
step:765/1375 train_time:108649ms step_avg:143.91ms
step:766/1375 train_time:108803ms step_avg:143.92ms
step:767/1375 train_time:108954ms step_avg:143.93ms
step:768/1375 train_time:109105ms step_avg:143.94ms
step:769/1375 train_time:109256ms step_avg:143.95ms
step:770/1375 train_time:109406ms step_avg:143.96ms
step:771/1375 train_time:109556ms step_avg:143.96ms
step:772/1375 train_time:109705ms step_avg:143.97ms
step:773/1375 train_time:109857ms step_avg:143.98ms
step:774/1375 train_time:110007ms step_avg:143.99ms
step:775/1375 train_time:110158ms step_avg:144.00ms
step:776/1375 train_time:110310ms step_avg:144.01ms
step:777/1375 train_time:110460ms step_avg:144.02ms
step:778/1375 train_time:110608ms step_avg:144.02ms
step:779/1375 train_time:110757ms step_avg:144.03ms
step:780/1375 train_time:110908ms step_avg:144.04ms
step:781/1375 train_time:111059ms step_avg:144.05ms
step:782/1375 train_time:111210ms step_avg:144.05ms
step:783/1375 train_time:111360ms step_avg:144.06ms
step:784/1375 train_time:111511ms step_avg:144.07ms
step:785/1375 train_time:111660ms step_avg:144.08ms
step:786/1375 train_time:111811ms step_avg:144.09ms
step:787/1375 train_time:111960ms step_avg:144.09ms
step:788/1375 train_time:112111ms step_avg:144.10ms
step:789/1375 train_time:112260ms step_avg:144.11ms
step:790/1375 train_time:112412ms step_avg:144.12ms
step:791/1375 train_time:112561ms step_avg:144.12ms
step:792/1375 train_time:112712ms step_avg:144.13ms
step:793/1375 train_time:112859ms step_avg:144.14ms
step:794/1375 train_time:113011ms step_avg:144.15ms
step:795/1375 train_time:113163ms step_avg:144.16ms
step:796/1375 train_time:113315ms step_avg:144.17ms
step:797/1375 train_time:113463ms step_avg:144.17ms
step:798/1375 train_time:113615ms step_avg:144.18ms
step:799/1375 train_time:113769ms step_avg:144.19ms
step:800/1375 train_time:113920ms step_avg:144.20ms
step:801/1375 train_time:114071ms step_avg:144.21ms
step:802/1375 train_time:114222ms step_avg:144.22ms
step:803/1375 train_time:114373ms step_avg:144.23ms
step:804/1375 train_time:114522ms step_avg:144.23ms
step:805/1375 train_time:114676ms step_avg:144.25ms
step:806/1375 train_time:114825ms step_avg:144.25ms
step:807/1375 train_time:114975ms step_avg:144.26ms
step:808/1375 train_time:115125ms step_avg:144.27ms
step:809/1375 train_time:115275ms step_avg:144.27ms
step:810/1375 train_time:115423ms step_avg:144.28ms
step:811/1375 train_time:115573ms step_avg:144.29ms
step:812/1375 train_time:115724ms step_avg:144.29ms
step:813/1375 train_time:115874ms step_avg:144.30ms
step:814/1375 train_time:116023ms step_avg:144.31ms
step:815/1375 train_time:116175ms step_avg:144.32ms
step:816/1375 train_time:116325ms step_avg:144.32ms
step:817/1375 train_time:116476ms step_avg:144.33ms
step:818/1375 train_time:116625ms step_avg:144.34ms
step:819/1375 train_time:116778ms step_avg:144.35ms
step:820/1375 train_time:116930ms step_avg:144.36ms
step:821/1375 train_time:117081ms step_avg:144.37ms
step:822/1375 train_time:117234ms step_avg:144.38ms
step:823/1375 train_time:117385ms step_avg:144.39ms
step:824/1375 train_time:117536ms step_avg:144.39ms
step:825/1375 train_time:117690ms step_avg:144.40ms
step:826/1375 train_time:117841ms step_avg:144.41ms
step:827/1375 train_time:117993ms step_avg:144.42ms
step:828/1375 train_time:118143ms step_avg:144.43ms
step:829/1375 train_time:118296ms step_avg:144.44ms
step:830/1375 train_time:118445ms step_avg:144.45ms
step:831/1375 train_time:118598ms step_avg:144.46ms
step:832/1375 train_time:118749ms step_avg:144.46ms
step:833/1375 train_time:118899ms step_avg:144.47ms
step:834/1375 train_time:119050ms step_avg:144.48ms
step:835/1375 train_time:119204ms step_avg:144.49ms
step:836/1375 train_time:119356ms step_avg:144.50ms
step:837/1375 train_time:119505ms step_avg:144.50ms
step:838/1375 train_time:119657ms step_avg:144.51ms
step:839/1375 train_time:119809ms step_avg:144.52ms
step:840/1375 train_time:119959ms step_avg:144.53ms
step:841/1375 train_time:120113ms step_avg:144.54ms
step:842/1375 train_time:120265ms step_avg:144.55ms
step:843/1375 train_time:120416ms step_avg:144.56ms
step:844/1375 train_time:120568ms step_avg:144.57ms
step:845/1375 train_time:120718ms step_avg:144.57ms
step:846/1375 train_time:120868ms step_avg:144.58ms
step:847/1375 train_time:121020ms step_avg:144.59ms
step:848/1375 train_time:121171ms step_avg:144.60ms
step:849/1375 train_time:121325ms step_avg:144.61ms
step:850/1375 train_time:121479ms step_avg:144.62ms
step:851/1375 train_time:121631ms step_avg:144.63ms
step:852/1375 train_time:121782ms step_avg:144.63ms
step:853/1375 train_time:121932ms step_avg:144.64ms
step:854/1375 train_time:122082ms step_avg:144.65ms
step:855/1375 train_time:122234ms step_avg:144.66ms
step:856/1375 train_time:122384ms step_avg:144.66ms
step:857/1375 train_time:122536ms step_avg:144.67ms
step:858/1375 train_time:122691ms step_avg:144.68ms
step:859/1375 train_time:122842ms step_avg:144.69ms
step:860/1375 train_time:122994ms step_avg:144.70ms
step:861/1375 train_time:123143ms step_avg:144.70ms
step:862/1375 train_time:123297ms step_avg:144.71ms
step:863/1375 train_time:123449ms step_avg:144.72ms
step:864/1375 train_time:123600ms step_avg:144.73ms
step:865/1375 train_time:123749ms step_avg:144.74ms
step:866/1375 train_time:123906ms step_avg:144.75ms
step:867/1375 train_time:124059ms step_avg:144.76ms
step:868/1375 train_time:124210ms step_avg:144.77ms
step:869/1375 train_time:124360ms step_avg:144.77ms
step:870/1375 train_time:124516ms step_avg:144.79ms
step:871/1375 train_time:124665ms step_avg:144.79ms
step:872/1375 train_time:124817ms step_avg:144.80ms
step:873/1375 train_time:124967ms step_avg:144.81ms
step:874/1375 train_time:125118ms step_avg:144.81ms
step:875/1375 train_time:125271ms step_avg:144.82ms
step:875/1375 val_loss:3.4663 train_time:125348ms step_avg:144.91ms
step:876/1375 train_time:125424ms step_avg:144.83ms
step:877/1375 train_time:125578ms step_avg:144.84ms
step:878/1375 train_time:125728ms step_avg:144.85ms
step:879/1375 train_time:125879ms step_avg:144.86ms
step:880/1375 train_time:126028ms step_avg:144.86ms
step:881/1375 train_time:126178ms step_avg:144.87ms
step:882/1375 train_time:126331ms step_avg:144.88ms
step:883/1375 train_time:126484ms step_avg:144.88ms
step:884/1375 train_time:126636ms step_avg:144.89ms
step:885/1375 train_time:126787ms step_avg:144.90ms
step:886/1375 train_time:126940ms step_avg:144.91ms
step:887/1375 train_time:127089ms step_avg:144.91ms
step:888/1375 train_time:127243ms step_avg:144.92ms
step:889/1375 train_time:127394ms step_avg:144.93ms
step:890/1375 train_time:127545ms step_avg:144.94ms
step:891/1375 train_time:127697ms step_avg:144.95ms
step:892/1375 train_time:127849ms step_avg:144.95ms
step:893/1375 train_time:128001ms step_avg:144.96ms
step:894/1375 train_time:128154ms step_avg:144.97ms
step:895/1375 train_time:128309ms step_avg:144.98ms
step:896/1375 train_time:128462ms step_avg:144.99ms
step:897/1375 train_time:128610ms step_avg:144.99ms
step:898/1375 train_time:128764ms step_avg:145.00ms
step:899/1375 train_time:128914ms step_avg:145.01ms
step:900/1375 train_time:129065ms step_avg:145.02ms
step:901/1375 train_time:129217ms step_avg:145.02ms
step:902/1375 train_time:129367ms step_avg:145.03ms
step:903/1375 train_time:129518ms step_avg:145.04ms
step:904/1375 train_time:129671ms step_avg:145.05ms
step:905/1375 train_time:129824ms step_avg:145.05ms
step:906/1375 train_time:129976ms step_avg:145.06ms
step:907/1375 train_time:130130ms step_avg:145.07ms
step:908/1375 train_time:130283ms step_avg:145.08ms
step:909/1375 train_time:130434ms step_avg:145.09ms
step:910/1375 train_time:130592ms step_avg:145.10ms
step:911/1375 train_time:130744ms step_avg:145.11ms
step:912/1375 train_time:130895ms step_avg:145.12ms
step:913/1375 train_time:131049ms step_avg:145.13ms
step:914/1375 train_time:131203ms step_avg:145.14ms
step:915/1375 train_time:131354ms step_avg:145.14ms
step:916/1375 train_time:131504ms step_avg:145.15ms
step:917/1375 train_time:131657ms step_avg:145.16ms
step:918/1375 train_time:131809ms step_avg:145.16ms
step:919/1375 train_time:131965ms step_avg:145.18ms
step:920/1375 train_time:132117ms step_avg:145.18ms
step:921/1375 train_time:132270ms step_avg:145.19ms
step:922/1375 train_time:132426ms step_avg:145.20ms
step:923/1375 train_time:132578ms step_avg:145.21ms
step:924/1375 train_time:132730ms step_avg:145.22ms
step:925/1375 train_time:132885ms step_avg:145.23ms
step:926/1375 train_time:133037ms step_avg:145.24ms
step:927/1375 train_time:133188ms step_avg:145.24ms
step:928/1375 train_time:133341ms step_avg:145.25ms
step:929/1375 train_time:133495ms step_avg:145.26ms
step:930/1375 train_time:133647ms step_avg:145.27ms
step:931/1375 train_time:133801ms step_avg:145.28ms
step:932/1375 train_time:133952ms step_avg:145.28ms
step:933/1375 train_time:134106ms step_avg:145.29ms
step:934/1375 train_time:134261ms step_avg:145.30ms
step:935/1375 train_time:134415ms step_avg:145.31ms
step:936/1375 train_time:134569ms step_avg:145.32ms
step:937/1375 train_time:134725ms step_avg:145.33ms
step:938/1375 train_time:134878ms step_avg:145.34ms
step:939/1375 train_time:135029ms step_avg:145.35ms
step:940/1375 train_time:135183ms step_avg:145.36ms
step:941/1375 train_time:135336ms step_avg:145.37ms
step:942/1375 train_time:135489ms step_avg:145.37ms
step:943/1375 train_time:135644ms step_avg:145.39ms
step:944/1375 train_time:135804ms step_avg:145.40ms
step:945/1375 train_time:135956ms step_avg:145.41ms
step:946/1375 train_time:136109ms step_avg:145.42ms
step:947/1375 train_time:136262ms step_avg:145.42ms
step:948/1375 train_time:136415ms step_avg:145.43ms
step:949/1375 train_time:136570ms step_avg:145.44ms
step:950/1375 train_time:136722ms step_avg:145.45ms
step:951/1375 train_time:136917ms step_avg:145.50ms
step:952/1375 train_time:137067ms step_avg:145.51ms
step:953/1375 train_time:137219ms step_avg:145.51ms
step:954/1375 train_time:137370ms step_avg:145.52ms
step:955/1375 train_time:137520ms step_avg:145.52ms
step:956/1375 train_time:137674ms step_avg:145.53ms
step:957/1375 train_time:137830ms step_avg:145.54ms
step:958/1375 train_time:137987ms step_avg:145.56ms
step:959/1375 train_time:138142ms step_avg:145.57ms
step:960/1375 train_time:138297ms step_avg:145.58ms
step:961/1375 train_time:138449ms step_avg:145.58ms
step:962/1375 train_time:138603ms step_avg:145.59ms
step:963/1375 train_time:138762ms step_avg:145.61ms
step:964/1375 train_time:138914ms step_avg:145.61ms
step:965/1375 train_time:139067ms step_avg:145.62ms
step:966/1375 train_time:139219ms step_avg:145.63ms
step:967/1375 train_time:139371ms step_avg:145.63ms
step:968/1375 train_time:139520ms step_avg:145.64ms
step:969/1375 train_time:139674ms step_avg:145.65ms
step:970/1375 train_time:139826ms step_avg:145.65ms
step:971/1375 train_time:139982ms step_avg:145.66ms
step:972/1375 train_time:140132ms step_avg:145.67ms
step:973/1375 train_time:140284ms step_avg:145.67ms
step:974/1375 train_time:140437ms step_avg:145.68ms
step:975/1375 train_time:140588ms step_avg:145.69ms
step:976/1375 train_time:140741ms step_avg:145.69ms
step:977/1375 train_time:140893ms step_avg:145.70ms
step:978/1375 train_time:141046ms step_avg:145.71ms
step:979/1375 train_time:141199ms step_avg:145.72ms
step:980/1375 train_time:141351ms step_avg:145.72ms
step:981/1375 train_time:141502ms step_avg:145.73ms
step:982/1375 train_time:141653ms step_avg:145.73ms
step:983/1375 train_time:141804ms step_avg:145.74ms
step:984/1375 train_time:141956ms step_avg:145.75ms
step:985/1375 train_time:142108ms step_avg:145.75ms
step:986/1375 train_time:142264ms step_avg:145.76ms
step:987/1375 train_time:142414ms step_avg:145.77ms
step:988/1375 train_time:142567ms step_avg:145.77ms
step:989/1375 train_time:142718ms step_avg:145.78ms
step:990/1375 train_time:142873ms step_avg:145.79ms
step:991/1375 train_time:143025ms step_avg:145.79ms
step:992/1375 train_time:143182ms step_avg:145.81ms
step:993/1375 train_time:143345ms step_avg:145.82ms
step:994/1375 train_time:143497ms step_avg:145.83ms
step:995/1375 train_time:143648ms step_avg:145.84ms
step:996/1375 train_time:143799ms step_avg:145.84ms
step:997/1375 train_time:143949ms step_avg:145.84ms
step:998/1375 train_time:144100ms step_avg:145.85ms
step:999/1375 train_time:144252ms step_avg:145.86ms
step:1000/1375 train_time:144404ms step_avg:145.86ms
step:1000/1375 val_loss:3.4012 train_time:144481ms step_avg:145.94ms
step:1001/1375 train_time:144558ms step_avg:145.87ms
step:1002/1375 train_time:144713ms step_avg:145.88ms
step:1003/1375 train_time:144864ms step_avg:145.89ms
step:1004/1375 train_time:145020ms step_avg:145.90ms
step:1005/1375 train_time:145172ms step_avg:145.90ms
step:1006/1375 train_time:145323ms step_avg:145.91ms
step:1007/1375 train_time:145477ms step_avg:145.91ms
step:1008/1375 train_time:145632ms step_avg:145.92ms
step:1009/1375 train_time:145790ms step_avg:145.94ms
step:1010/1375 train_time:145943ms step_avg:145.94ms
step:1011/1375 train_time:146097ms step_avg:145.95ms
step:1012/1375 train_time:146248ms step_avg:145.96ms
step:1013/1375 train_time:146402ms step_avg:145.96ms
step:1014/1375 train_time:146554ms step_avg:145.97ms
step:1015/1375 train_time:146705ms step_avg:145.98ms
step:1016/1375 train_time:146858ms step_avg:145.98ms
step:1017/1375 train_time:147013ms step_avg:145.99ms
step:1018/1375 train_time:147164ms step_avg:146.00ms
step:1019/1375 train_time:147318ms step_avg:146.00ms
step:1020/1375 train_time:147474ms step_avg:146.01ms
step:1021/1375 train_time:147626ms step_avg:146.02ms
step:1022/1375 train_time:147779ms step_avg:146.03ms
step:1023/1375 train_time:147934ms step_avg:146.04ms
step:1024/1375 train_time:148088ms step_avg:146.04ms
step:1025/1375 train_time:148242ms step_avg:146.05ms
step:1026/1375 train_time:148395ms step_avg:146.06ms
step:1027/1375 train_time:148549ms step_avg:146.07ms
step:1028/1375 train_time:148702ms step_avg:146.07ms
step:1029/1375 train_time:148857ms step_avg:146.08ms
step:1030/1375 train_time:149014ms step_avg:146.09ms
step:1031/1375 train_time:149164ms step_avg:146.10ms
step:1032/1375 train_time:149319ms step_avg:146.10ms
step:1033/1375 train_time:149472ms step_avg:146.11ms
step:1034/1375 train_time:149624ms step_avg:146.12ms
step:1035/1375 train_time:149779ms step_avg:146.13ms
step:1036/1375 train_time:149935ms step_avg:146.14ms
step:1037/1375 train_time:150089ms step_avg:146.14ms
step:1038/1375 train_time:150244ms step_avg:146.15ms
step:1039/1375 train_time:150398ms step_avg:146.16ms
step:1040/1375 train_time:150550ms step_avg:146.16ms
step:1041/1375 train_time:150704ms step_avg:146.17ms
step:1042/1375 train_time:150857ms step_avg:146.18ms
step:1043/1375 train_time:151009ms step_avg:146.19ms
step:1044/1375 train_time:151163ms step_avg:146.19ms
step:1045/1375 train_time:151319ms step_avg:146.20ms
step:1046/1375 train_time:151471ms step_avg:146.21ms
step:1047/1375 train_time:151625ms step_avg:146.21ms
step:1048/1375 train_time:151781ms step_avg:146.22ms
step:1049/1375 train_time:151936ms step_avg:146.23ms
step:1050/1375 train_time:152093ms step_avg:146.24ms
step:1051/1375 train_time:152248ms step_avg:146.25ms
step:1052/1375 train_time:152405ms step_avg:146.26ms
step:1053/1375 train_time:152557ms step_avg:146.27ms
step:1054/1375 train_time:152713ms step_avg:146.28ms
step:1055/1375 train_time:152865ms step_avg:146.28ms
step:1056/1375 train_time:153020ms step_avg:146.29ms
step:1057/1375 train_time:153174ms step_avg:146.30ms
step:1058/1375 train_time:153333ms step_avg:146.31ms
step:1059/1375 train_time:153488ms step_avg:146.32ms
step:1060/1375 train_time:153643ms step_avg:146.33ms
step:1061/1375 train_time:153796ms step_avg:146.33ms
step:1062/1375 train_time:153952ms step_avg:146.34ms
step:1063/1375 train_time:154107ms step_avg:146.35ms
step:1064/1375 train_time:154259ms step_avg:146.36ms
step:1065/1375 train_time:154413ms step_avg:146.36ms
step:1066/1375 train_time:154570ms step_avg:146.37ms
step:1067/1375 train_time:154726ms step_avg:146.38ms
step:1068/1375 train_time:154879ms step_avg:146.39ms
step:1069/1375 train_time:155037ms step_avg:146.40ms
step:1070/1375 train_time:155187ms step_avg:146.40ms
step:1071/1375 train_time:155343ms step_avg:146.41ms
step:1072/1375 train_time:155496ms step_avg:146.42ms
step:1073/1375 train_time:155648ms step_avg:146.42ms
step:1074/1375 train_time:155802ms step_avg:146.43ms
step:1075/1375 train_time:155956ms step_avg:146.44ms
step:1076/1375 train_time:156108ms step_avg:146.44ms
step:1077/1375 train_time:156259ms step_avg:146.45ms
step:1078/1375 train_time:156418ms step_avg:146.46ms
step:1079/1375 train_time:156576ms step_avg:146.47ms
step:1080/1375 train_time:156731ms step_avg:146.48ms
step:1081/1375 train_time:156884ms step_avg:146.48ms
step:1082/1375 train_time:157037ms step_avg:146.49ms
step:1083/1375 train_time:157188ms step_avg:146.49ms
step:1084/1375 train_time:157345ms step_avg:146.50ms
step:1085/1375 train_time:157498ms step_avg:146.51ms
step:1086/1375 train_time:157653ms step_avg:146.52ms
step:1087/1375 train_time:157808ms step_avg:146.53ms
step:1088/1375 train_time:157960ms step_avg:146.53ms
step:1089/1375 train_time:158120ms step_avg:146.54ms
step:1090/1375 train_time:158278ms step_avg:146.55ms
step:1091/1375 train_time:158432ms step_avg:146.56ms
step:1092/1375 train_time:158583ms step_avg:146.56ms
step:1093/1375 train_time:158738ms step_avg:146.57ms
step:1094/1375 train_time:158891ms step_avg:146.58ms
step:1095/1375 train_time:159044ms step_avg:146.58ms
step:1096/1375 train_time:159203ms step_avg:146.60ms
step:1097/1375 train_time:159360ms step_avg:146.61ms
step:1098/1375 train_time:159514ms step_avg:146.61ms
step:1099/1375 train_time:159666ms step_avg:146.62ms
step:1100/1375 train_time:159819ms step_avg:146.62ms
step:1101/1375 train_time:159970ms step_avg:146.63ms
step:1102/1375 train_time:160124ms step_avg:146.63ms
step:1103/1375 train_time:160278ms step_avg:146.64ms
step:1104/1375 train_time:160431ms step_avg:146.65ms
step:1105/1375 train_time:160585ms step_avg:146.65ms
step:1106/1375 train_time:160739ms step_avg:146.66ms
step:1107/1375 train_time:160896ms step_avg:146.67ms
step:1108/1375 train_time:161052ms step_avg:146.68ms
step:1109/1375 train_time:161206ms step_avg:146.68ms
step:1110/1375 train_time:161359ms step_avg:146.69ms
step:1111/1375 train_time:161515ms step_avg:146.70ms
step:1112/1375 train_time:161670ms step_avg:146.71ms
step:1113/1375 train_time:161824ms step_avg:146.71ms
step:1114/1375 train_time:161980ms step_avg:146.72ms
step:1115/1375 train_time:162137ms step_avg:146.73ms
step:1116/1375 train_time:162288ms step_avg:146.73ms
step:1117/1375 train_time:162447ms step_avg:146.75ms
step:1118/1375 train_time:162604ms step_avg:146.75ms
step:1119/1375 train_time:162758ms step_avg:146.76ms
step:1120/1375 train_time:162913ms step_avg:146.77ms
step:1121/1375 train_time:163065ms step_avg:146.77ms
step:1122/1375 train_time:163218ms step_avg:146.78ms
step:1123/1375 train_time:163374ms step_avg:146.79ms
step:1124/1375 train_time:163536ms step_avg:146.80ms
step:1125/1375 train_time:163690ms step_avg:146.81ms
step:1125/1375 val_loss:3.3485 train_time:163767ms step_avg:146.88ms
step:1126/1375 train_time:163843ms step_avg:146.81ms
step:1127/1375 train_time:164001ms step_avg:146.82ms
step:1128/1375 train_time:164156ms step_avg:146.83ms
step:1129/1375 train_time:164312ms step_avg:146.84ms
step:1130/1375 train_time:164466ms step_avg:146.84ms
step:1131/1375 train_time:164623ms step_avg:146.85ms
step:1132/1375 train_time:164777ms step_avg:146.86ms
step:1133/1375 train_time:164933ms step_avg:146.87ms
step:1134/1375 train_time:165090ms step_avg:146.88ms
step:1135/1375 train_time:165243ms step_avg:146.88ms
step:1136/1375 train_time:165402ms step_avg:146.89ms
step:1137/1375 train_time:165555ms step_avg:146.90ms
step:1138/1375 train_time:165707ms step_avg:146.90ms
step:1139/1375 train_time:165862ms step_avg:146.91ms
step:1140/1375 train_time:166015ms step_avg:146.92ms
step:1141/1375 train_time:166210ms step_avg:146.96ms
step:1142/1375 train_time:166364ms step_avg:146.96ms
step:1143/1375 train_time:166521ms step_avg:146.97ms
step:1144/1375 train_time:166679ms step_avg:146.98ms
step:1145/1375 train_time:166832ms step_avg:146.99ms
step:1146/1375 train_time:166989ms step_avg:147.00ms
step:1147/1375 train_time:167145ms step_avg:147.01ms
step:1148/1375 train_time:167301ms step_avg:147.01ms
step:1149/1375 train_time:167455ms step_avg:147.02ms
step:1150/1375 train_time:167608ms step_avg:147.02ms
step:1151/1375 train_time:167764ms step_avg:147.03ms
step:1152/1375 train_time:167919ms step_avg:147.04ms
step:1153/1375 train_time:168075ms step_avg:147.05ms
step:1154/1375 train_time:168229ms step_avg:147.05ms
step:1155/1375 train_time:168384ms step_avg:147.06ms
step:1156/1375 train_time:168544ms step_avg:147.07ms
step:1157/1375 train_time:168701ms step_avg:147.08ms
step:1158/1375 train_time:168853ms step_avg:147.08ms
step:1159/1375 train_time:169007ms step_avg:147.09ms
step:1160/1375 train_time:169161ms step_avg:147.10ms
step:1161/1375 train_time:169315ms step_avg:147.10ms
step:1162/1375 train_time:169472ms step_avg:147.11ms
step:1163/1375 train_time:169626ms step_avg:147.12ms
step:1164/1375 train_time:169782ms step_avg:147.12ms
step:1165/1375 train_time:169933ms step_avg:147.13ms
step:1166/1375 train_time:170088ms step_avg:147.14ms
step:1167/1375 train_time:170241ms step_avg:147.14ms
step:1168/1375 train_time:170395ms step_avg:147.15ms
step:1169/1375 train_time:170552ms step_avg:147.15ms
step:1170/1375 train_time:170706ms step_avg:147.16ms
step:1171/1375 train_time:170862ms step_avg:147.17ms
step:1172/1375 train_time:171016ms step_avg:147.17ms
step:1173/1375 train_time:171171ms step_avg:147.18ms
step:1174/1375 train_time:171333ms step_avg:147.19ms
step:1175/1375 train_time:171491ms step_avg:147.20ms
step:1176/1375 train_time:171650ms step_avg:147.21ms
step:1177/1375 train_time:171813ms step_avg:147.23ms
step:1178/1375 train_time:171967ms step_avg:147.23ms
step:1179/1375 train_time:172120ms step_avg:147.24ms
step:1180/1375 train_time:172284ms step_avg:147.25ms
step:1181/1375 train_time:172438ms step_avg:147.26ms
step:1182/1375 train_time:172590ms step_avg:147.26ms
step:1183/1375 train_time:172745ms step_avg:147.27ms
step:1184/1375 train_time:172901ms step_avg:147.28ms
step:1185/1375 train_time:173060ms step_avg:147.29ms
step:1186/1375 train_time:173213ms step_avg:147.29ms
step:1187/1375 train_time:173377ms step_avg:147.30ms
step:1188/1375 train_time:173528ms step_avg:147.31ms
step:1189/1375 train_time:173685ms step_avg:147.32ms
step:1190/1375 train_time:173842ms step_avg:147.32ms
step:1191/1375 train_time:173996ms step_avg:147.33ms
step:1192/1375 train_time:174149ms step_avg:147.33ms
step:1193/1375 train_time:174303ms step_avg:147.34ms
step:1194/1375 train_time:174460ms step_avg:147.35ms
step:1195/1375 train_time:174615ms step_avg:147.35ms
step:1196/1375 train_time:174772ms step_avg:147.36ms
step:1197/1375 train_time:174926ms step_avg:147.37ms
step:1198/1375 train_time:175086ms step_avg:147.38ms
step:1199/1375 train_time:175241ms step_avg:147.39ms
step:1200/1375 train_time:175392ms step_avg:147.39ms
step:1201/1375 train_time:175547ms step_avg:147.39ms
step:1202/1375 train_time:175717ms step_avg:147.41ms
step:1203/1375 train_time:175877ms step_avg:147.42ms
step:1204/1375 train_time:176032ms step_avg:147.43ms
step:1205/1375 train_time:176188ms step_avg:147.44ms
step:1206/1375 train_time:176344ms step_avg:147.44ms
step:1207/1375 train_time:176500ms step_avg:147.45ms
step:1208/1375 train_time:176655ms step_avg:147.46ms
step:1209/1375 train_time:176808ms step_avg:147.46ms
step:1210/1375 train_time:176967ms step_avg:147.47ms
step:1211/1375 train_time:177121ms step_avg:147.48ms
step:1212/1375 train_time:177276ms step_avg:147.48ms
step:1213/1375 train_time:177430ms step_avg:147.49ms
step:1214/1375 train_time:177586ms step_avg:147.50ms
step:1215/1375 train_time:177743ms step_avg:147.50ms
step:1216/1375 train_time:177896ms step_avg:147.51ms
step:1217/1375 train_time:178051ms step_avg:147.52ms
step:1218/1375 train_time:178203ms step_avg:147.52ms
step:1219/1375 train_time:178357ms step_avg:147.52ms
step:1220/1375 train_time:178509ms step_avg:147.53ms
step:1221/1375 train_time:178663ms step_avg:147.53ms
step:1222/1375 train_time:178819ms step_avg:147.54ms
step:1223/1375 train_time:178976ms step_avg:147.55ms
step:1224/1375 train_time:179134ms step_avg:147.56ms
step:1225/1375 train_time:179291ms step_avg:147.56ms
step:1226/1375 train_time:179447ms step_avg:147.57ms
step:1227/1375 train_time:179605ms step_avg:147.58ms
step:1228/1375 train_time:179761ms step_avg:147.59ms
step:1229/1375 train_time:179915ms step_avg:147.59ms
step:1230/1375 train_time:180077ms step_avg:147.60ms
step:1231/1375 train_time:180235ms step_avg:147.61ms
step:1232/1375 train_time:180393ms step_avg:147.62ms
step:1233/1375 train_time:180549ms step_avg:147.63ms
step:1234/1375 train_time:180703ms step_avg:147.63ms
step:1235/1375 train_time:180860ms step_avg:147.64ms
step:1236/1375 train_time:181015ms step_avg:147.65ms
step:1237/1375 train_time:181171ms step_avg:147.65ms
step:1238/1375 train_time:181335ms step_avg:147.67ms
step:1239/1375 train_time:181492ms step_avg:147.67ms
step:1240/1375 train_time:181653ms step_avg:147.69ms
step:1241/1375 train_time:181814ms step_avg:147.70ms
step:1242/1375 train_time:181968ms step_avg:147.70ms
step:1243/1375 train_time:182126ms step_avg:147.71ms
step:1244/1375 train_time:182282ms step_avg:147.72ms
step:1245/1375 train_time:182438ms step_avg:147.72ms
step:1246/1375 train_time:182594ms step_avg:147.73ms
step:1247/1375 train_time:182753ms step_avg:147.74ms
step:1248/1375 train_time:182906ms step_avg:147.74ms
step:1249/1375 train_time:183059ms step_avg:147.75ms
step:1250/1375 train_time:183214ms step_avg:147.75ms
step:1250/1375 val_loss:3.3033 train_time:183293ms step_avg:147.82ms
step:1251/1375 train_time:183373ms step_avg:147.76ms
step:1252/1375 train_time:183529ms step_avg:147.77ms
step:1253/1375 train_time:183683ms step_avg:147.77ms
step:1254/1375 train_time:183835ms step_avg:147.78ms
step:1255/1375 train_time:184003ms step_avg:147.79ms
step:1256/1375 train_time:184158ms step_avg:147.80ms
step:1257/1375 train_time:184314ms step_avg:147.81ms
step:1258/1375 train_time:184471ms step_avg:147.81ms
step:1259/1375 train_time:184629ms step_avg:147.82ms
step:1260/1375 train_time:184782ms step_avg:147.83ms
step:1261/1375 train_time:184938ms step_avg:147.83ms
step:1262/1375 train_time:185097ms step_avg:147.84ms
step:1263/1375 train_time:185253ms step_avg:147.85ms
step:1264/1375 train_time:185406ms step_avg:147.85ms
step:1265/1375 train_time:185561ms step_avg:147.86ms
step:1266/1375 train_time:185718ms step_avg:147.86ms
step:1267/1375 train_time:185872ms step_avg:147.87ms
step:1268/1375 train_time:186030ms step_avg:147.88ms
step:1269/1375 train_time:186191ms step_avg:147.89ms
step:1270/1375 train_time:186346ms step_avg:147.89ms
step:1271/1375 train_time:186504ms step_avg:147.90ms
step:1272/1375 train_time:186658ms step_avg:147.91ms
step:1273/1375 train_time:186812ms step_avg:147.91ms
step:1274/1375 train_time:186967ms step_avg:147.92ms
step:1275/1375 train_time:187123ms step_avg:147.92ms
step:1276/1375 train_time:187277ms step_avg:147.93ms
step:1277/1375 train_time:187434ms step_avg:147.93ms
step:1278/1375 train_time:187588ms step_avg:147.94ms
step:1279/1375 train_time:187744ms step_avg:147.95ms
step:1280/1375 train_time:187907ms step_avg:147.96ms
step:1281/1375 train_time:188062ms step_avg:147.96ms
step:1282/1375 train_time:188216ms step_avg:147.97ms
step:1283/1375 train_time:188372ms step_avg:147.98ms
step:1284/1375 train_time:188530ms step_avg:147.98ms
step:1285/1375 train_time:188683ms step_avg:147.99ms
step:1286/1375 train_time:188842ms step_avg:148.00ms
step:1287/1375 train_time:188998ms step_avg:148.00ms
step:1288/1375 train_time:189152ms step_avg:148.01ms
step:1289/1375 train_time:189312ms step_avg:148.02ms
step:1290/1375 train_time:189471ms step_avg:148.02ms
step:1291/1375 train_time:189629ms step_avg:148.03ms
step:1292/1375 train_time:189785ms step_avg:148.04ms
step:1293/1375 train_time:189944ms step_avg:148.05ms
step:1294/1375 train_time:190102ms step_avg:148.05ms
step:1295/1375 train_time:190255ms step_avg:148.06ms
step:1296/1375 train_time:190411ms step_avg:148.06ms
step:1297/1375 train_time:190570ms step_avg:148.07ms
step:1298/1375 train_time:190727ms step_avg:148.08ms
step:1299/1375 train_time:190882ms step_avg:148.09ms
step:1300/1375 train_time:191037ms step_avg:148.09ms
step:1301/1375 train_time:191189ms step_avg:148.09ms
step:1302/1375 train_time:191347ms step_avg:148.10ms
step:1303/1375 train_time:191505ms step_avg:148.11ms
step:1304/1375 train_time:191665ms step_avg:148.12ms
step:1305/1375 train_time:191821ms step_avg:148.12ms
step:1306/1375 train_time:191981ms step_avg:148.13ms
step:1307/1375 train_time:192134ms step_avg:148.14ms
step:1308/1375 train_time:192290ms step_avg:148.14ms
step:1309/1375 train_time:192445ms step_avg:148.15ms
step:1310/1375 train_time:192600ms step_avg:148.15ms
step:1311/1375 train_time:192753ms step_avg:148.16ms
step:1312/1375 train_time:192907ms step_avg:148.16ms
step:1313/1375 train_time:193062ms step_avg:148.17ms
step:1314/1375 train_time:193220ms step_avg:148.17ms
step:1315/1375 train_time:193376ms step_avg:148.18ms
step:1316/1375 train_time:193529ms step_avg:148.18ms
step:1317/1375 train_time:193682ms step_avg:148.19ms
step:1318/1375 train_time:193844ms step_avg:148.20ms
step:1319/1375 train_time:194001ms step_avg:148.21ms
step:1320/1375 train_time:194157ms step_avg:148.21ms
step:1321/1375 train_time:194314ms step_avg:148.22ms
step:1322/1375 train_time:194476ms step_avg:148.23ms
step:1323/1375 train_time:194632ms step_avg:148.23ms
step:1324/1375 train_time:194787ms step_avg:148.24ms
step:1325/1375 train_time:194945ms step_avg:148.25ms
step:1326/1375 train_time:195106ms step_avg:148.26ms
step:1327/1375 train_time:195260ms step_avg:148.26ms
step:1328/1375 train_time:195417ms step_avg:148.27ms
step:1329/1375 train_time:195590ms step_avg:148.29ms
step:1330/1375 train_time:195750ms step_avg:148.30ms
step:1331/1375 train_time:195945ms step_avg:148.33ms
step:1332/1375 train_time:196107ms step_avg:148.34ms
step:1333/1375 train_time:196263ms step_avg:148.35ms
step:1334/1375 train_time:196420ms step_avg:148.35ms
step:1335/1375 train_time:196573ms step_avg:148.36ms
step:1336/1375 train_time:196737ms step_avg:148.37ms
step:1337/1375 train_time:196894ms step_avg:148.38ms
step:1338/1375 train_time:197051ms step_avg:148.38ms
step:1339/1375 train_time:197208ms step_avg:148.39ms
step:1340/1375 train_time:197367ms step_avg:148.40ms
step:1341/1375 train_time:197523ms step_avg:148.40ms
step:1342/1375 train_time:197683ms step_avg:148.41ms
step:1343/1375 train_time:197840ms step_avg:148.42ms
step:1344/1375 train_time:197993ms step_avg:148.42ms
step:1345/1375 train_time:198148ms step_avg:148.43ms
step:1346/1375 train_time:198304ms step_avg:148.43ms
step:1347/1375 train_time:198462ms step_avg:148.44ms
step:1348/1375 train_time:198619ms step_avg:148.44ms
step:1349/1375 train_time:198776ms step_avg:148.45ms
step:1350/1375 train_time:198931ms step_avg:148.46ms
step:1351/1375 train_time:199087ms step_avg:148.46ms
step:1352/1375 train_time:199252ms step_avg:148.47ms
step:1353/1375 train_time:199415ms step_avg:148.49ms
step:1354/1375 train_time:199575ms step_avg:148.49ms
step:1355/1375 train_time:199731ms step_avg:148.50ms
step:1356/1375 train_time:199885ms step_avg:148.50ms
step:1357/1375 train_time:200044ms step_avg:148.51ms
step:1358/1375 train_time:200204ms step_avg:148.52ms
step:1359/1375 train_time:200361ms step_avg:148.53ms
step:1360/1375 train_time:200524ms step_avg:148.54ms
step:1361/1375 train_time:200682ms step_avg:148.54ms
step:1362/1375 train_time:200840ms step_avg:148.55ms
step:1363/1375 train_time:201003ms step_avg:148.56ms
step:1364/1375 train_time:201158ms step_avg:148.57ms
step:1365/1375 train_time:201312ms step_avg:148.57ms
step:1366/1375 train_time:201469ms step_avg:148.58ms
step:1367/1375 train_time:201625ms step_avg:148.58ms
step:1368/1375 train_time:201785ms step_avg:148.59ms
step:1369/1375 train_time:201948ms step_avg:148.60ms
step:1370/1375 train_time:202105ms step_avg:148.61ms
step:1371/1375 train_time:202261ms step_avg:148.61ms
step:1372/1375 train_time:202425ms step_avg:148.62ms
step:1373/1375 train_time:202580ms step_avg:148.63ms
step:1374/1375 train_time:202736ms step_avg:148.63ms
step:1375/1375 train_time:202891ms step_avg:148.64ms
step:1375/1375 val_loss:3.2781 train_time:202969ms step_avg:148.70ms
peak memory consumption: 31565 MiB

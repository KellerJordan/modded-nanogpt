import os
import sys
with open(sys.argv[0]) as f:
    code = f.read() # read the code of this file ASAP, for logging
import uuid
import time
import glob
import subprocess
import contextlib
from dataclasses import dataclass

import torch
torch.empty(1, device='cuda', requires_grad=True).backward()
from torch import nn
import torch.nn.functional as F
import torch.distributed as dist
from torch.nn.parallel import DistributedDataParallel as DDP
# use of FlexAttention contributed by @KoszarskyB
from torch.nn.attention.flex_attention import BlockMask, flex_attention

# -----------------------------------------------------------------------------
# Muon optimizer

@torch.compile
def zeropower_via_newtonschulz5(G, steps):
    """
    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a
    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose
    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at
    zero even beyond the point where the iteration no longer converges all the way to one everywhere
    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T
    where S' is diagonal with S_{ii}' ~ Uniform(0.5, 1.5), which turns out not to hurt model
    performance at all relative to UV^T, where USV^T = G is the SVD.
    """
    assert len(G.shape) == 2
    a, b, c = (3.4445, -4.7750,  2.0315)
    X = G.bfloat16()
    if G.size(0) > G.size(1):
        X = X.T

    # Ensure spectral norm is at most 1
    X = X / (X.norm() + 1e-7)
    # Perform the NS iterations
    for _ in range(steps):
        A = X @ X.T
        B = b * A + c * A @ A # adapted from suggestion by @jxbz, @leloykun, and @YouJiacheng
        X = a * X + B @ X
    
    if G.size(0) > G.size(1):
        X = X.T
    return X

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Some warnings:
    - This optimizer assumes that all parameters passed in are 2D.
    - It should not be used for the embedding layer, the final fully connected layer, or any {0,1}-D
    parameters; those should all be optimized by a standard method (e.g., AdamW).
    - To use it with 4D convolutional filters, it works well to just flatten their last 3 dimensions.
    - We believe it is unlikely to work well for training with small batch size.
    - We believe it may not work well for finetuning pretrained models, but we haven't tested this.
    - We have not yet tried this optimizer for training scenarios larger than NanoGPT (124M).

    Arguments:
        lr: The learning rate used by the internal SGD.
        momentum: The momentum used by the internal SGD.
        nesterov: Whether to use Nesterov-style momentum in the internal SGD. (recommended)
        ns_steps: The number of Newton-Schulz iteration steps to use.
    """
    def __init__(self, params, lr=0.02, momentum=0.95, nesterov=True, ns_steps=5):
        self.world_size = int(os.environ['WORLD_SIZE'])
        self.rank = int(os.environ['RANK'])
        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, ns_steps=ns_steps)
        assert all(isinstance(p, torch.Tensor) for p in params)
        sizes = {p.numel() for p in params}
        param_groups = [dict(params=[p for p in params if p.numel() == size],
                             update_buffer=[torch.empty(size, device='cuda', dtype=torch.bfloat16) for _ in range(self.world_size)])
                        for size in sizes]
        super().__init__(param_groups, defaults)

    def step(self):

        for group in self.param_groups:

            lr = group['lr']
            momentum = group['momentum']
            nesterov = group['nesterov']
            ns_steps = group['ns_steps']
            update_buffers = group['update_buffer']
            # generate weight updates in distributed fashion
            params = group['params']
            handle = None
            params_world = None
            def update_prev():
                if params_world is None:
                    return
                assert handle is not None
                handle.wait()
                for p_world, g_world in zip(params_world, update_buffers):
                    p_world.data.add_(
                        g_world.view_as(p_world),
                        alpha=-lr * max(1, p_world.size(0) / p_world.size(1)) ** 0.5,
                    )
            for base_i in range(len(params))[::self.world_size]:
                if base_i + rank < len(params):
                    p = params[base_i + self.rank]
                    g = p.grad
                    assert g is not None
                    state = self.state[p]
                    if 'momentum_buffer' not in state:
                        state['momentum_buffer'] = torch.zeros_like(g)
                    buf = state['momentum_buffer']
                    buf.lerp_(g, 1 - momentum)
                    g = g.lerp_(buf, momentum) if nesterov else buf
                    g = zeropower_via_newtonschulz5(g, steps=ns_steps).flatten()
                else:
                    g = update_buffers[rank]
                update_prev() # async all_gather instead of sync all_reduce by @YouJiacheng
                handle = dist.all_gather(update_buffers, g, async_op=True)
                params_world = params[base_i : base_i + self.world_size]
            update_prev()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the GPT-2 model

def norm(x):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):

    def __init__(self, in_features, out_features):
        super().__init__(in_features, out_features, bias=False)

    def forward(self, x):
        return F.linear(x, self.weight.type_as(x))

class Rotary(nn.Module):

    def __init__(self, dim, max_seq_len=65536):
        super().__init__()
        # half-truncate RoPE by @YouJiacheng
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=dim//4, dtype=torch.float32)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(dim//4)])
        t = torch.arange(max_seq_len, dtype=torch.float32)
        theta = torch.einsum('i,j -> ij', t, angular_freq)
        self.cos = nn.Buffer(theta.cos(), persistent=False)
        self.sin = nn.Buffer(theta.sin(), persistent=False)

    def forward(self, x):
        cos, sin = self.cos[None, :x.size(-3), None, :], self.sin[None, :x.size(-3), None, :]
        x1, x2 = x.float().chunk(2, dim=-1)
        y1 = x1 * cos + x2 * sin
        y2 = x1 * (-sin) + x2 * cos
        return torch.cat((y1, y2), 3).type_as(x)

class CausalSelfAttention(nn.Module):

    def __init__(self, dim, num_heads, layer_id):
        super().__init__()
        assert dim % num_heads == 0
        self.num_heads = num_heads
        self.layer_id = layer_id
        self.c_q = CastedLinear(dim, dim)
        self.c_k = CastedLinear(dim, dim)
        self.c_v = CastedLinear(dim, dim)
        self.lambdas = nn.Parameter(torch.tensor([0.5, 0.5]))
        self.rotary = Rotary(dim // num_heads) # dim // num_heads = head_dim
        self.c_proj = CastedLinear(dim, dim)
        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977
        # self.attn_scale = nn.Parameter(torch.tensor(1.0 / (dim // num_heads) ** 0.5))
        # self.attn_scale = nn.Parameter(torch.tensor(0.20))
        self.attn_scale = 0.13 + 0.01 * min(layer_id, 11 - layer_id)

    def forward(self, x, ve, block_mask):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, 'Must use batch size = 1 for FlexAttention'
        q = self.c_q(x).view(B, T, self.num_heads, -1)
        k = self.c_k(x).view(B, T, self.num_heads, -1)
        v = self.c_v(x).view(B, T, self.num_heads, -1)
        if ve is not None:
            v = self.lambdas[0] * v + self.lambdas[1] * ve.view_as(v) # @KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = self.lambdas[0] * v
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = self.rotary(q), self.rotary(k)
        # y = flex_attention(q.transpose(1, 2) * self.attn_scale, k.transpose(1, 2), v.transpose(1, 2), block_mask=block_mask, scale=1.)
        y = flex_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), block_mask=block_mask, scale=self.attn_scale)
        y = y.transpose(1, 2).contiguous().view_as(x) # re-assemble all head outputs side by side
        y = self.c_proj(y)
        return y

class MLP(nn.Module):

    def __init__(self, dim):
        super().__init__()
        self.c_fc = CastedLinear(dim, 4 * dim)
        self.c_proj = CastedLinear(4 * dim, dim)
        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977

    def forward(self, x):
        x = self.c_fc(x)
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = self.c_proj(x)
        return x

class Block(nn.Module):

    def __init__(self, model_dim, num_heads, layer_id, use_attn=True):
        super().__init__()
        self.attn = CausalSelfAttention(model_dim, num_heads, layer_id) if use_attn else None
        self.mlp = MLP(model_dim)
        self.lambdas = nn.Parameter(torch.tensor([1., 0.]))

    def forward(self, x, ve, x0, block_mask):
        x = self.lambdas[0] * x + self.lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), ve, block_mask)
        x = x + self.mlp(norm(x))
        return x

class ValueEmbedding(nn.Module):
    def __init__(self, vocab_size, model_dim):
        super().__init__()
        self.embed = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])

    def forward(self, inputs):
        ve = [emb(inputs).bfloat16() for emb in self.embed]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        ve = [ve[0], ve[1], ve[2], None, None, None, None, None, None, ve[0], ve[1], ve[2]]
        return ve

# -----------------------------------------------------------------------------
# The main GPT-2 model

class GPT(nn.Module):

    def __init__(self, vocab_size, num_layers, num_heads, model_dim):
        super().__init__()
        self.embed = nn.Embedding(vocab_size, model_dim)
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.blocks = nn.ModuleList([Block(model_dim, num_heads, layer_id=layer_id, use_attn=(layer_id != 7))
                                     for layer_id in range(num_layers)])
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual learning
        # U-net structure on token value embeddings by @leloykun
        self.value_embeds = ValueEmbedding(vocab_size, model_dim)
        self.lm_head = CastedLinear(model_dim, vocab_size)
        self.lm_head.weight.data.zero_() # @Grad62304977
        # U-net design by @brendanh0gan
        self.num_encoder_layers = num_layers // 2 # Half of the layers for encoder
        self.num_decoder_layers = num_layers - self.num_encoder_layers # Remaining for decoder
        # Add learnable skip connection weights for decoder layers
        self.skip_weights = nn.Parameter(torch.ones(self.num_decoder_layers))

    def forward(self, inputs, targets, sliding_window_num_blocks):
        BLOCK_SIZE = 128
        seq_len = len(inputs)
        assert seq_len % BLOCK_SIZE == 0
        total_num_blocks = seq_len // BLOCK_SIZE
        assert inputs.ndim == 1
        docs = (inputs == 50256).cumsum(0)
        docs_low = docs.view(-1, BLOCK_SIZE)[:, 0].contiguous()
        docs_high = docs.view(-1, BLOCK_SIZE)[:, -1].contiguous()

        def document_causal(b, h, q_idx, kv_idx):
            causal_mask = q_idx >= kv_idx
            document_mask = docs[q_idx] == docs[kv_idx]
            return causal_mask & document_mask

        def dense_to_ordered(dense_mask):
            num_blocks = dense_mask.sum(dim=-1, dtype=torch.int32)
            indices = dense_mask.argsort(dim=-1, descending=True, stable=True).to(torch.int32)
            return num_blocks[None, None].contiguous(), indices[None, None].contiguous()

        def create_doc_swc_block_mask(sliding_window_num_blocks):
            kv_idx = block_idx = torch.arange(total_num_blocks, dtype=torch.int32, device='cuda')
            q_idx = block_idx[:, None]
            causal_bm = q_idx >= kv_idx
            causal_full_bm = q_idx > kv_idx
            window_bm = q_idx - kv_idx < sliding_window_num_blocks
            window_full_bm = window_bm # block-wise sliding window by @YouJiacheng
            # document_bm = (docs_low[q_idx] <= docs_high[kv_idx]) & (docs_low[kv_idx] <= docs_high[q_idx])
            document_bm = (docs_low[:, None] <= docs_high) & (docs_low <= docs_high[:, None])
            document_full_bm = (docs_low[:, None] == docs_high) & (docs_low == docs_high[:, None])
            nonzero_bm = causal_bm & window_bm & document_bm
            full_bm  = causal_full_bm & window_full_bm & document_full_bm
            kv_num_blocks, kv_indices = dense_to_ordered(nonzero_bm & ~full_bm)
            full_kv_num_blocks, full_kv_indices = dense_to_ordered(full_bm)
            return BlockMask.from_kv_blocks(
                kv_num_blocks,
                kv_indices,
                full_kv_num_blocks,
                full_kv_indices,
                BLOCK_SIZE=BLOCK_SIZE,
                mask_mod=document_causal,
            )

        block_mask = create_doc_swc_block_mask(sliding_window_num_blocks)

        x0 = norm(self.embed(inputs[None]).bfloat16()) # use of norm here by @Grad62304977
        x = x0
        ve = self.value_embeds(inputs)
        assert len(ve) == len(self.blocks)
        ve_enc, ve_dec = ve[:self.num_encoder_layers], ve[self.num_encoder_layers:]

        # Store outputs for U-Net skip connections
        skip_connections = []
        # Encoder pass - process only the first half of the blocks
        for i in range(self.num_encoder_layers):
            x = self.blocks[i](x, ve_enc[i], x0, block_mask)
            skip_connections.append(x)
        # Decoder pass - process the remaining blocks with weighted skip connections
        for i in range(self.num_decoder_layers):
            x = x + self.skip_weights[i] * skip_connections.pop()
            # U-net structure on token value embeddings by @leloykun
            x = self.blocks[self.num_encoder_layers + i](x, ve_dec[i], x0, block_mask)

        x = norm(x)
        logits = self.lm_head(x)
        logits = 15 * torch.tanh(logits / 15) # @Grad62304977 added tanh softcapping, @KoszarskyB reduced it from 30 to 15
        logits = logits.float()
        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets)
        return loss

# -----------------------------------------------------------------------------
# Our own simple Distributed Data Loader

def _load_data_shard(path):
    # only reads the header, returns header data
    # header is 256 int32
    header = torch.from_file(path, False, 256, dtype=torch.int32)
    assert header[0] == 20240520, 'magic number mismatch in the data .bin file'
    assert header[1] == 1, 'unsupported version'
    num_tokens = int(header[2]) # number of tokens (claimed)
    with open(path, 'rb', buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, 'number of tokens read does not match header'
    return tokens

class DistributedDataLoader:

    def __init__(self, filename_pattern):
        self.rank = int(os.environ['RANK'])
        self.world_size = int(os.environ['WORLD_SIZE'])
        self.files = sorted(glob.glob(filename_pattern))
        self.reset()

    def reset(self):
        self.current_shard = -1
        self.advance()

    def advance(self):
        self.current_shard = (self.current_shard + 1) % len(self.files)
        self.current_position = 0
        self.tokens = _load_data_shard(self.files[self.current_shard])

    def next_batch(self, batch_size):
        assert batch_size % self.world_size == 0
        device_batch_size = batch_size // self.world_size
        # load next shard if necessary
        if self.current_position + batch_size + 1 >= len(self.tokens):
            self.advance()
        pos = self.current_position + self.rank * device_batch_size
        device_batch_tokens = self.tokens[pos:pos+device_batch_size+1]
        # advance current position
        self.current_position += batch_size
        inputs = device_batch_tokens[:-1].to(device='cuda', dtype=torch.int32, non_blocking=True)
        targets = device_batch_tokens[1:].to(device='cuda', dtype=torch.int64, non_blocking=True)
        return inputs, targets

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files = 'data/fineweb10B/fineweb_train_*.bin' # input .bin to train on
    val_files = 'data/fineweb10B/fineweb_val_*.bin' # input .bin to eval validation loss on
    val_tokens = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # optimization
    batch_size = 8*64*1024 # batch size in tokens
    num_iterations = 1375 # number of iterations to run
    cooldown_frac = 0.4 # fraction of training spent cooling down the learning rate
    bf16_embeds = True
    # evaluation and logging
    val_loss_every = 125 # every how many steps to evaluate val loss? 0 for only at the end
    # implementation
    max_device_batch_size = 64*1024 # batch size per device in tokens
    save_checkpoint = False
args = Hyperparameters()

micro_bs = args.max_device_batch_size

# set up DDP (distributed data parallel). torchrun sets this env variable
rank = int(os.environ['RANK'])
local_rank = int(os.environ['LOCAL_RANK'])
world_size = int(os.environ['WORLD_SIZE'])
assert torch.cuda.is_available()
torch.cuda.set_device(local_rank)
dist.init_process_group(backend='nccl', device_id=torch.device(local_rank))
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = uuid.uuid4()
    os.makedirs('logs', exist_ok=True)
    logfile = f'logs/{run_id}.txt'
    print(logfile)

def print0(s, console=False):
    if master_process:
        with open(logfile, 'a') as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0('='*100)
# log information about the hardware/software environment this is running on
print0(f'Running Python {sys.version}')
print0(f'Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}')
print0(subprocess.run(['nvidia-smi'], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout)
print0('='*100)

# load data
train_loader = DistributedDataLoader(args.train_files)
val_loader = DistributedDataLoader(args.val_files)
print0(f'Training dataloader files: {train_loader.files}')
print0(f'Validation dataloader files: {val_loader.files}')
print0('='*100)

# there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency. suggested to me by @Grad62304977.
# this originates from Karpathy's experiments.
model = GPT(vocab_size=50304, num_layers=12, num_heads=6, model_dim=768)
model = model.cuda()
if args.bf16_embeds:
    for m in model.modules():
        if isinstance(m, nn.Embedding):
            m.bfloat16()
model: GPT = torch.compile(model)
ddp_model = DDP(model, device_ids=[local_rank], broadcast_buffers=False, gradient_as_bucket_view=True)

# collect the parameters to optimize
hidden_matrix_params = [p for p in model.blocks.parameters() if p.ndim == 2]
embed_params = [model.embed.weight, *model.value_embeds.parameters()]
scalar_params = [p for n, p in model.named_parameters() if p.ndim < 2 and "attn_scale" not in n]
attn_scale_params = [p for n, p in model.named_parameters() if p.ndim < 2 and "attn_scale" in n]
head_params = [model.lm_head.weight]

# init the optimizer(s)
optimizer1 = torch.optim.Adam([dict(params=embed_params, lr=0.6),
                               dict(params=head_params, lr=0.008),
                               dict(params=scalar_params, lr=0.04),
                               dict(params=attn_scale_params, lr=0.01)],
                              betas=(0.8, 0.95), fused=True)
optimizer2 = Muon(hidden_matrix_params, lr=0.05, momentum=0.95)
optimizers = [optimizer1, optimizer2]

# learning rate schedule: stable then decay
def get_lr(it):
    t = 1 - it / args.num_iterations # time remaining in training
    assert 1 >= t > 0
    # 1) constant lr for first part of training
    if t >= args.cooldown_frac:
        return 1.0
    # 2) then linear cooldown
    else:
        return t / args.cooldown_frac
schedulers = [torch.optim.lr_scheduler.LambdaLR(opt, get_lr) for opt in optimizers]

# sliding window size schedule: linear increase over training in chunks of 128 from 128 -> 1792. By @fernbear.bsky.social
def get_sliding_window_blocks(it):
    x = it / args.num_iterations # training progress
    assert 0 <= x <= 1
    return int(((1 - x) * 128 + x * 1856) // 128)
sliding_window_num_blocks = torch.tensor(1, dtype=torch.int32, device='cuda')

# Start training loop
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    # This effectively ignores timing first 10 steps, which are slower for weird reasons.
    # Alternately, and slightly more correctly in terms of benchmarking, we could do 10
    # steps with dummy data first, and then re-initialize the model and reset the loader.
    if step == 10:
        training_time_ms = 0
        t0 = time.perf_counter()
    timed_steps = float('nan') if step <= 11 else (step - 10) + 1 # <= 11 to avoid bug in val

    sliding_window_num_blocks.copy_(get_sliding_window_blocks(step))

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        # run validation batches
        model.eval()
        val_loader.reset()
        val_loss = 0.0
        # calculate the number of steps to take in the val loop.
        val_batch_size = world_size * micro_bs
        assert args.val_tokens % val_batch_size == 0
        val_steps = args.val_tokens // val_batch_size
        for _ in range(val_steps):
            with torch.no_grad():
                inputs_val, targets_val = val_loader.next_batch(val_batch_size)
                val_loss += ddp_model(inputs_val, targets_val, sliding_window_num_blocks)
        # Print attention scales
        for n, p in model.named_parameters():
            if p.ndim < 2 and "attn_scale" in n:
                print0(f'{n}: {p.item()}', console=True)
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        val_loss /= val_steps
        # logging
        print0(f'step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/(timed_steps-1):.2f}ms', console=True)
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f'logs/{run_id}', exist_ok=True)
            torch.save(log, f'logs/{run_id}/state_step{step:06d}.pt')
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    model.train()
    batch_size = args.batch_size
    assert batch_size % world_size == 0
    inputs_train, targets_train = train_loader.next_batch(batch_size)
    assert len(inputs_train) <= micro_bs or len(inputs_train) % micro_bs == 0
    for micro_inputs_train, micro_targets_train in zip(inputs_train.split(micro_bs), targets_train.split(micro_bs)):
        ddp_model(micro_inputs_train, micro_targets_train, sliding_window_num_blocks).backward()
    # momentum warmup for Muon
    frac = min(step/300, 1)
    for group in optimizer2.param_groups:
        group['momentum'] = (1 - frac) * 0.85 + frac * 0.95
    # step the optimizers and schedulers
    for opt, sched in zip(optimizers, schedulers):
        opt.step()
        if step != train_steps-1:
            sched.step()
    # null the gradients
    model.zero_grad(set_to_none=True)
    # logging
    approx_time = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f'step:{step+1}/{train_steps} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms', console=True)

print0(f'peak memory consumption: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB')
dist.destroy_process_group()

====================================================================================================
Running Python 3.10.12 (main, Sep 11 2024, 15:47:36) [GCC 11.4.0]
Running PyTorch 2.6.0.dev20241231+cu126 compiled for CUDA 12.6
Tue Jan 14 14:55:04 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.183.06             Driver Version: 535.183.06   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:19:00.0 Off |                    0 |
| N/A   39C    P0             127W / 700W |   7713MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:3B:00.0 Off |                    0 |
| N/A   32C    P0             114W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:4C:00.0 Off |                    0 |
| N/A   29C    P0             117W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   38C    P0             130W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:9B:00.0 Off |                    0 |
| N/A   39C    P0             123W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:BB:00.0 Off |                    0 |
| N/A   30C    P0             114W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:CB:00.0 Off |                    0 |
| N/A   37C    P0             116W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:DB:00.0 Off |                    0 |
| N/A   30C    P0             117W / 700W |   3211MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
Training dataloader files: ['data/fineweb10B/fineweb_train_000001.bin', 'data/fineweb10B/fineweb_train_000002.bin', 'data/fineweb10B/fineweb_train_000003.bin', 'data/fineweb10B/fineweb_train_000004.bin', 'data/fineweb10B/fineweb_train_000005.bin', 'data/fineweb10B/fineweb_train_000006.bin', 'data/fineweb10B/fineweb_train_000007.bin', 'data/fineweb10B/fineweb_train_000008.bin', 'data/fineweb10B/fineweb_train_000009.bin']
Validation dataloader files: ['data/fineweb10B/fineweb_val_000000.bin']
====================================================================================================
step:0/1375 val_loss:10.8258 train_time:0ms step_avg:nanms
step:1/1375 train_time:27434ms step_avg:nanms
step:2/1375 train_time:27508ms step_avg:nanms
step:3/1375 train_time:27690ms step_avg:nanms
step:4/1375 train_time:27822ms step_avg:nanms
step:5/1375 train_time:27956ms step_avg:nanms
step:6/1375 train_time:28090ms step_avg:nanms
step:7/1375 train_time:28222ms step_avg:nanms
step:8/1375 train_time:28356ms step_avg:nanms
step:9/1375 train_time:28489ms step_avg:nanms
step:10/1375 train_time:28631ms step_avg:nanms
step:11/1375 train_time:135ms step_avg:nanms
step:12/1375 train_time:271ms step_avg:nanms
step:13/1375 train_time:406ms step_avg:135.34ms
step:14/1375 train_time:541ms step_avg:135.17ms
step:15/1375 train_time:674ms step_avg:134.89ms
step:16/1375 train_time:809ms step_avg:134.84ms
step:17/1375 train_time:945ms step_avg:135.01ms
step:18/1375 train_time:1083ms step_avg:135.33ms
step:19/1375 train_time:1218ms step_avg:135.33ms
step:20/1375 train_time:1356ms step_avg:135.61ms
step:21/1375 train_time:1491ms step_avg:135.51ms
step:22/1375 train_time:1625ms step_avg:135.43ms
step:23/1375 train_time:1758ms step_avg:135.25ms
step:24/1375 train_time:1895ms step_avg:135.35ms
step:25/1375 train_time:2029ms step_avg:135.29ms
step:26/1375 train_time:2165ms step_avg:135.33ms
step:27/1375 train_time:2299ms step_avg:135.26ms
step:28/1375 train_time:2436ms step_avg:135.35ms
step:29/1375 train_time:2571ms step_avg:135.33ms
step:30/1375 train_time:2708ms step_avg:135.41ms
step:31/1375 train_time:2844ms step_avg:135.45ms
step:32/1375 train_time:2980ms step_avg:135.48ms
step:33/1375 train_time:3113ms step_avg:135.36ms
step:34/1375 train_time:3250ms step_avg:135.41ms
step:35/1375 train_time:3385ms step_avg:135.39ms
step:36/1375 train_time:3519ms step_avg:135.36ms
step:37/1375 train_time:3655ms step_avg:135.35ms
step:38/1375 train_time:3791ms step_avg:135.41ms
step:39/1375 train_time:3928ms step_avg:135.43ms
step:40/1375 train_time:4063ms step_avg:135.43ms
step:41/1375 train_time:4198ms step_avg:135.44ms
step:42/1375 train_time:4335ms step_avg:135.46ms
step:43/1375 train_time:4470ms step_avg:135.44ms
step:44/1375 train_time:4605ms step_avg:135.44ms
step:45/1375 train_time:4740ms step_avg:135.43ms
step:46/1375 train_time:4876ms step_avg:135.45ms
step:47/1375 train_time:5011ms step_avg:135.43ms
step:48/1375 train_time:5147ms step_avg:135.44ms
step:49/1375 train_time:5285ms step_avg:135.53ms
step:50/1375 train_time:5420ms step_avg:135.51ms
step:51/1375 train_time:5557ms step_avg:135.54ms
step:52/1375 train_time:5691ms step_avg:135.51ms
step:53/1375 train_time:5827ms step_avg:135.51ms
step:54/1375 train_time:5963ms step_avg:135.51ms
step:55/1375 train_time:6097ms step_avg:135.50ms
step:56/1375 train_time:6232ms step_avg:135.48ms
step:57/1375 train_time:6368ms step_avg:135.49ms
step:58/1375 train_time:6504ms step_avg:135.50ms
step:59/1375 train_time:6639ms step_avg:135.49ms
step:60/1375 train_time:6776ms step_avg:135.52ms
step:61/1375 train_time:6910ms step_avg:135.50ms
step:62/1375 train_time:7046ms step_avg:135.49ms
step:63/1375 train_time:7179ms step_avg:135.45ms
step:64/1375 train_time:7315ms step_avg:135.47ms
step:65/1375 train_time:7451ms step_avg:135.47ms
step:66/1375 train_time:7588ms step_avg:135.51ms
step:67/1375 train_time:7723ms step_avg:135.49ms
step:68/1375 train_time:7860ms step_avg:135.51ms
step:69/1375 train_time:7995ms step_avg:135.51ms
step:70/1375 train_time:8131ms step_avg:135.52ms
step:71/1375 train_time:8266ms step_avg:135.51ms
step:72/1375 train_time:8403ms step_avg:135.53ms
step:73/1375 train_time:8538ms step_avg:135.53ms
step:74/1375 train_time:8674ms step_avg:135.53ms
step:75/1375 train_time:8810ms step_avg:135.53ms
step:76/1375 train_time:8946ms step_avg:135.55ms
step:77/1375 train_time:9082ms step_avg:135.55ms
step:78/1375 train_time:9217ms step_avg:135.54ms
step:79/1375 train_time:9353ms step_avg:135.56ms
step:80/1375 train_time:9489ms step_avg:135.56ms
step:81/1375 train_time:9624ms step_avg:135.55ms
step:82/1375 train_time:9762ms step_avg:135.58ms
step:83/1375 train_time:9897ms step_avg:135.58ms
step:84/1375 train_time:10033ms step_avg:135.59ms
step:85/1375 train_time:10169ms step_avg:135.58ms
step:86/1375 train_time:10303ms step_avg:135.57ms
step:87/1375 train_time:10440ms step_avg:135.58ms
step:88/1375 train_time:10575ms step_avg:135.58ms
step:89/1375 train_time:10710ms step_avg:135.57ms
step:90/1375 train_time:10845ms step_avg:135.56ms
step:91/1375 train_time:10980ms step_avg:135.55ms
step:92/1375 train_time:11116ms step_avg:135.56ms
step:93/1375 train_time:11252ms step_avg:135.56ms
step:94/1375 train_time:11387ms step_avg:135.56ms
step:95/1375 train_time:11523ms step_avg:135.56ms
step:96/1375 train_time:11659ms step_avg:135.57ms
step:97/1375 train_time:11796ms step_avg:135.58ms
step:98/1375 train_time:11930ms step_avg:135.57ms
step:99/1375 train_time:12065ms step_avg:135.56ms
step:100/1375 train_time:12200ms step_avg:135.55ms
step:101/1375 train_time:12336ms step_avg:135.56ms
step:102/1375 train_time:12471ms step_avg:135.56ms
step:103/1375 train_time:12609ms step_avg:135.58ms
step:104/1375 train_time:12747ms step_avg:135.61ms
step:105/1375 train_time:12885ms step_avg:135.63ms
step:106/1375 train_time:13023ms step_avg:135.66ms
step:107/1375 train_time:13162ms step_avg:135.69ms
step:108/1375 train_time:13301ms step_avg:135.72ms
step:109/1375 train_time:13440ms step_avg:135.76ms
step:110/1375 train_time:13580ms step_avg:135.80ms
step:111/1375 train_time:13719ms step_avg:135.83ms
step:112/1375 train_time:13859ms step_avg:135.87ms
step:113/1375 train_time:13997ms step_avg:135.89ms
step:114/1375 train_time:14135ms step_avg:135.91ms
step:115/1375 train_time:14272ms step_avg:135.93ms
step:116/1375 train_time:14412ms step_avg:135.97ms
step:117/1375 train_time:14552ms step_avg:136.00ms
step:118/1375 train_time:14691ms step_avg:136.03ms
step:119/1375 train_time:14830ms step_avg:136.06ms
step:120/1375 train_time:14970ms step_avg:136.09ms
step:121/1375 train_time:15108ms step_avg:136.11ms
step:122/1375 train_time:15247ms step_avg:136.14ms
step:123/1375 train_time:15386ms step_avg:136.16ms
step:124/1375 train_time:15524ms step_avg:136.18ms
step:125/1375 train_time:15663ms step_avg:136.20ms
step:125/1375 val_loss:4.3619 train_time:15731ms step_avg:136.79ms
step:126/1375 train_time:15806ms step_avg:136.26ms
step:127/1375 train_time:15946ms step_avg:136.29ms
step:128/1375 train_time:16085ms step_avg:136.31ms
step:129/1375 train_time:16223ms step_avg:136.32ms
step:130/1375 train_time:16359ms step_avg:136.32ms
step:131/1375 train_time:16496ms step_avg:136.33ms
step:132/1375 train_time:16635ms step_avg:136.35ms
step:133/1375 train_time:16777ms step_avg:136.40ms
step:134/1375 train_time:16919ms step_avg:136.44ms
step:135/1375 train_time:17059ms step_avg:136.47ms
step:136/1375 train_time:17197ms step_avg:136.48ms
step:137/1375 train_time:17336ms step_avg:136.50ms
step:138/1375 train_time:17473ms step_avg:136.51ms
step:139/1375 train_time:17612ms step_avg:136.52ms
step:140/1375 train_time:17751ms step_avg:136.55ms
step:141/1375 train_time:17892ms step_avg:136.58ms
step:142/1375 train_time:18032ms step_avg:136.61ms
step:143/1375 train_time:18171ms step_avg:136.63ms
step:144/1375 train_time:18311ms step_avg:136.65ms
step:145/1375 train_time:18449ms step_avg:136.66ms
step:146/1375 train_time:18588ms step_avg:136.68ms
step:147/1375 train_time:18727ms step_avg:136.69ms
step:148/1375 train_time:18866ms step_avg:136.71ms
step:149/1375 train_time:19005ms step_avg:136.73ms
step:150/1375 train_time:19145ms step_avg:136.75ms
step:151/1375 train_time:19285ms step_avg:136.77ms
step:152/1375 train_time:19424ms step_avg:136.79ms
step:153/1375 train_time:19561ms step_avg:136.79ms
step:154/1375 train_time:19701ms step_avg:136.81ms
step:155/1375 train_time:19841ms step_avg:136.83ms
step:156/1375 train_time:19980ms step_avg:136.85ms
step:157/1375 train_time:20121ms step_avg:136.87ms
step:158/1375 train_time:20258ms step_avg:136.88ms
step:159/1375 train_time:20398ms step_avg:136.90ms
step:160/1375 train_time:20538ms step_avg:136.92ms
step:161/1375 train_time:20676ms step_avg:136.92ms
step:162/1375 train_time:20815ms step_avg:136.94ms
step:163/1375 train_time:20953ms step_avg:136.95ms
step:164/1375 train_time:21093ms step_avg:136.96ms
step:165/1375 train_time:21231ms step_avg:136.97ms
step:166/1375 train_time:21370ms step_avg:136.98ms
step:167/1375 train_time:21509ms step_avg:137.00ms
step:168/1375 train_time:21647ms step_avg:137.01ms
step:169/1375 train_time:21786ms step_avg:137.02ms
step:170/1375 train_time:21926ms step_avg:137.03ms
step:171/1375 train_time:22064ms step_avg:137.04ms
step:172/1375 train_time:22204ms step_avg:137.06ms
step:173/1375 train_time:22345ms step_avg:137.08ms
step:174/1375 train_time:22484ms step_avg:137.10ms
step:175/1375 train_time:22623ms step_avg:137.11ms
step:176/1375 train_time:22762ms step_avg:137.12ms
step:177/1375 train_time:22903ms step_avg:137.14ms
step:178/1375 train_time:23044ms step_avg:137.16ms
step:179/1375 train_time:23183ms step_avg:137.18ms
step:180/1375 train_time:23323ms step_avg:137.19ms
step:181/1375 train_time:23462ms step_avg:137.20ms
step:182/1375 train_time:23601ms step_avg:137.21ms
step:183/1375 train_time:23739ms step_avg:137.22ms
step:184/1375 train_time:23878ms step_avg:137.23ms
step:185/1375 train_time:24017ms step_avg:137.24ms
step:186/1375 train_time:24155ms step_avg:137.25ms
step:187/1375 train_time:24293ms step_avg:137.25ms
step:188/1375 train_time:24432ms step_avg:137.26ms
step:189/1375 train_time:24570ms step_avg:137.26ms
step:190/1375 train_time:24709ms step_avg:137.27ms
step:191/1375 train_time:24886ms step_avg:137.49ms
step:192/1375 train_time:25023ms step_avg:137.49ms
step:193/1375 train_time:25160ms step_avg:137.49ms
step:194/1375 train_time:25297ms step_avg:137.48ms
step:195/1375 train_time:25435ms step_avg:137.48ms
step:196/1375 train_time:25572ms step_avg:137.48ms
step:197/1375 train_time:25711ms step_avg:137.49ms
step:198/1375 train_time:25856ms step_avg:137.53ms
step:199/1375 train_time:25997ms step_avg:137.55ms
step:200/1375 train_time:26137ms step_avg:137.56ms
step:201/1375 train_time:26275ms step_avg:137.56ms
step:202/1375 train_time:26413ms step_avg:137.57ms
step:203/1375 train_time:26552ms step_avg:137.58ms
step:204/1375 train_time:26693ms step_avg:137.59ms
step:205/1375 train_time:26835ms step_avg:137.61ms
step:206/1375 train_time:26978ms step_avg:137.64ms
step:207/1375 train_time:27119ms step_avg:137.66ms
step:208/1375 train_time:27260ms step_avg:137.68ms
step:209/1375 train_time:27402ms step_avg:137.70ms
step:210/1375 train_time:27543ms step_avg:137.72ms
step:211/1375 train_time:27684ms step_avg:137.73ms
step:212/1375 train_time:27827ms step_avg:137.76ms
step:213/1375 train_time:27968ms step_avg:137.77ms
step:214/1375 train_time:28109ms step_avg:137.79ms
step:215/1375 train_time:28252ms step_avg:137.81ms
step:216/1375 train_time:28394ms step_avg:137.84ms
step:217/1375 train_time:28535ms step_avg:137.85ms
step:218/1375 train_time:28676ms step_avg:137.87ms
step:219/1375 train_time:28818ms step_avg:137.89ms
step:220/1375 train_time:28960ms step_avg:137.91ms
step:221/1375 train_time:29104ms step_avg:137.93ms
step:222/1375 train_time:29246ms step_avg:137.95ms
step:223/1375 train_time:29388ms step_avg:137.97ms
step:224/1375 train_time:29529ms step_avg:137.99ms
step:225/1375 train_time:29669ms step_avg:138.00ms
step:226/1375 train_time:29810ms step_avg:138.01ms
step:227/1375 train_time:29951ms step_avg:138.02ms
step:228/1375 train_time:30093ms step_avg:138.04ms
step:229/1375 train_time:30237ms step_avg:138.07ms
step:230/1375 train_time:30379ms step_avg:138.09ms
step:231/1375 train_time:30522ms step_avg:138.11ms
step:232/1375 train_time:30661ms step_avg:138.11ms
step:233/1375 train_time:30804ms step_avg:138.13ms
step:234/1375 train_time:30946ms step_avg:138.15ms
step:235/1375 train_time:31089ms step_avg:138.17ms
step:236/1375 train_time:31228ms step_avg:138.18ms
step:237/1375 train_time:31369ms step_avg:138.19ms
step:238/1375 train_time:31512ms step_avg:138.21ms
step:239/1375 train_time:31654ms step_avg:138.23ms
step:240/1375 train_time:31797ms step_avg:138.25ms
step:241/1375 train_time:31939ms step_avg:138.26ms
step:242/1375 train_time:32082ms step_avg:138.28ms
step:243/1375 train_time:32223ms step_avg:138.30ms
step:244/1375 train_time:32364ms step_avg:138.31ms
step:245/1375 train_time:32506ms step_avg:138.32ms
step:246/1375 train_time:32649ms step_avg:138.34ms
step:247/1375 train_time:32791ms step_avg:138.36ms
step:248/1375 train_time:32932ms step_avg:138.37ms
step:249/1375 train_time:33075ms step_avg:138.39ms
step:250/1375 train_time:33218ms step_avg:138.41ms
step:250/1375 val_loss:3.9519 train_time:33286ms step_avg:138.69ms
step:251/1375 train_time:33361ms step_avg:138.43ms
step:252/1375 train_time:33509ms step_avg:138.47ms
step:253/1375 train_time:33650ms step_avg:138.48ms
step:254/1375 train_time:33791ms step_avg:138.49ms
step:255/1375 train_time:33932ms step_avg:138.50ms
step:256/1375 train_time:34074ms step_avg:138.51ms
step:257/1375 train_time:34214ms step_avg:138.52ms
step:258/1375 train_time:34357ms step_avg:138.54ms
step:259/1375 train_time:34501ms step_avg:138.56ms
step:260/1375 train_time:34645ms step_avg:138.58ms
step:261/1375 train_time:34787ms step_avg:138.59ms
step:262/1375 train_time:34928ms step_avg:138.60ms
step:263/1375 train_time:35070ms step_avg:138.61ms
step:264/1375 train_time:35210ms step_avg:138.62ms
step:265/1375 train_time:35351ms step_avg:138.63ms
step:266/1375 train_time:35494ms step_avg:138.65ms
step:267/1375 train_time:35635ms step_avg:138.66ms
step:268/1375 train_time:35778ms step_avg:138.68ms
step:269/1375 train_time:35920ms step_avg:138.69ms
step:270/1375 train_time:36062ms step_avg:138.70ms
step:271/1375 train_time:36204ms step_avg:138.71ms
step:272/1375 train_time:36345ms step_avg:138.72ms
step:273/1375 train_time:36485ms step_avg:138.73ms
step:274/1375 train_time:36628ms step_avg:138.74ms
step:275/1375 train_time:36771ms step_avg:138.76ms
step:276/1375 train_time:36913ms step_avg:138.77ms
step:277/1375 train_time:37057ms step_avg:138.79ms
step:278/1375 train_time:37197ms step_avg:138.79ms
step:279/1375 train_time:37339ms step_avg:138.81ms
step:280/1375 train_time:37483ms step_avg:138.82ms
step:281/1375 train_time:37625ms step_avg:138.84ms
step:282/1375 train_time:37767ms step_avg:138.85ms
step:283/1375 train_time:37909ms step_avg:138.86ms
step:284/1375 train_time:38051ms step_avg:138.87ms
step:285/1375 train_time:38192ms step_avg:138.88ms
step:286/1375 train_time:38334ms step_avg:138.89ms
step:287/1375 train_time:38476ms step_avg:138.90ms
step:288/1375 train_time:38619ms step_avg:138.92ms
step:289/1375 train_time:38761ms step_avg:138.93ms
step:290/1375 train_time:38903ms step_avg:138.94ms
step:291/1375 train_time:39045ms step_avg:138.95ms
step:292/1375 train_time:39186ms step_avg:138.96ms
step:293/1375 train_time:39328ms step_avg:138.97ms
step:294/1375 train_time:39471ms step_avg:138.98ms
step:295/1375 train_time:39611ms step_avg:138.99ms
step:296/1375 train_time:39753ms step_avg:139.00ms
step:297/1375 train_time:39894ms step_avg:139.00ms
step:298/1375 train_time:40036ms step_avg:139.01ms
step:299/1375 train_time:40178ms step_avg:139.03ms
step:300/1375 train_time:40320ms step_avg:139.04ms
step:301/1375 train_time:40464ms step_avg:139.05ms
step:302/1375 train_time:40606ms step_avg:139.06ms
step:303/1375 train_time:40747ms step_avg:139.07ms
step:304/1375 train_time:40888ms step_avg:139.07ms
step:305/1375 train_time:41030ms step_avg:139.09ms
step:306/1375 train_time:41173ms step_avg:139.10ms
step:307/1375 train_time:41315ms step_avg:139.11ms
step:308/1375 train_time:41460ms step_avg:139.13ms
step:309/1375 train_time:41605ms step_avg:139.15ms
step:310/1375 train_time:41750ms step_avg:139.17ms
step:311/1375 train_time:41893ms step_avg:139.18ms
step:312/1375 train_time:42038ms step_avg:139.20ms
step:313/1375 train_time:42184ms step_avg:139.22ms
step:314/1375 train_time:42328ms step_avg:139.24ms
step:315/1375 train_time:42471ms step_avg:139.25ms
step:316/1375 train_time:42615ms step_avg:139.26ms
step:317/1375 train_time:42760ms step_avg:139.28ms
step:318/1375 train_time:42902ms step_avg:139.29ms
step:319/1375 train_time:43048ms step_avg:139.32ms
step:320/1375 train_time:43192ms step_avg:139.33ms
step:321/1375 train_time:43336ms step_avg:139.34ms
step:322/1375 train_time:43477ms step_avg:139.35ms
step:323/1375 train_time:43621ms step_avg:139.36ms
step:324/1375 train_time:43764ms step_avg:139.38ms
step:325/1375 train_time:43909ms step_avg:139.39ms
step:326/1375 train_time:44054ms step_avg:139.41ms
step:327/1375 train_time:44198ms step_avg:139.42ms
step:328/1375 train_time:44343ms step_avg:139.44ms
step:329/1375 train_time:44487ms step_avg:139.46ms
step:330/1375 train_time:44631ms step_avg:139.47ms
step:331/1375 train_time:44774ms step_avg:139.48ms
step:332/1375 train_time:44918ms step_avg:139.50ms
step:333/1375 train_time:45062ms step_avg:139.51ms
step:334/1375 train_time:45206ms step_avg:139.53ms
step:335/1375 train_time:45350ms step_avg:139.54ms
step:336/1375 train_time:45492ms step_avg:139.55ms
step:337/1375 train_time:45637ms step_avg:139.56ms
step:338/1375 train_time:45783ms step_avg:139.58ms
step:339/1375 train_time:45926ms step_avg:139.59ms
step:340/1375 train_time:46070ms step_avg:139.61ms
step:341/1375 train_time:46213ms step_avg:139.62ms
step:342/1375 train_time:46357ms step_avg:139.63ms
step:343/1375 train_time:46501ms step_avg:139.64ms
step:344/1375 train_time:46644ms step_avg:139.65ms
step:345/1375 train_time:46789ms step_avg:139.67ms
step:346/1375 train_time:46932ms step_avg:139.68ms
step:347/1375 train_time:47075ms step_avg:139.69ms
step:348/1375 train_time:47220ms step_avg:139.70ms
step:349/1375 train_time:47362ms step_avg:139.71ms
step:350/1375 train_time:47506ms step_avg:139.72ms
step:351/1375 train_time:47650ms step_avg:139.74ms
step:352/1375 train_time:47792ms step_avg:139.74ms
step:353/1375 train_time:47938ms step_avg:139.76ms
step:354/1375 train_time:48082ms step_avg:139.77ms
step:355/1375 train_time:48229ms step_avg:139.79ms
step:356/1375 train_time:48372ms step_avg:139.80ms
step:357/1375 train_time:48516ms step_avg:139.82ms
step:358/1375 train_time:48659ms step_avg:139.82ms
step:359/1375 train_time:48804ms step_avg:139.84ms
step:360/1375 train_time:48949ms step_avg:139.85ms
step:361/1375 train_time:49092ms step_avg:139.86ms
step:362/1375 train_time:49235ms step_avg:139.87ms
step:363/1375 train_time:49379ms step_avg:139.88ms
step:364/1375 train_time:49523ms step_avg:139.90ms
step:365/1375 train_time:49667ms step_avg:139.91ms
step:366/1375 train_time:49810ms step_avg:139.92ms
step:367/1375 train_time:49953ms step_avg:139.93ms
step:368/1375 train_time:50096ms step_avg:139.93ms
step:369/1375 train_time:50241ms step_avg:139.95ms
step:370/1375 train_time:50387ms step_avg:139.96ms
step:371/1375 train_time:50531ms step_avg:139.97ms
step:372/1375 train_time:50675ms step_avg:139.99ms
step:373/1375 train_time:50819ms step_avg:140.00ms
step:374/1375 train_time:50962ms step_avg:140.01ms
step:375/1375 train_time:51107ms step_avg:140.02ms
step:375/1375 val_loss:3.7725 train_time:51176ms step_avg:140.21ms
step:376/1375 train_time:51251ms step_avg:140.03ms
step:377/1375 train_time:51397ms step_avg:140.05ms
step:378/1375 train_time:51541ms step_avg:140.06ms
step:379/1375 train_time:51685ms step_avg:140.07ms
step:380/1375 train_time:51828ms step_avg:140.07ms
step:381/1375 train_time:52014ms step_avg:140.20ms
step:382/1375 train_time:52156ms step_avg:140.20ms
step:383/1375 train_time:52298ms step_avg:140.21ms
step:384/1375 train_time:52441ms step_avg:140.22ms
step:385/1375 train_time:52586ms step_avg:140.23ms
step:386/1375 train_time:52729ms step_avg:140.24ms
step:387/1375 train_time:52874ms step_avg:140.25ms
step:388/1375 train_time:53022ms step_avg:140.27ms
step:389/1375 train_time:53166ms step_avg:140.28ms
step:390/1375 train_time:53309ms step_avg:140.29ms
step:391/1375 train_time:53452ms step_avg:140.29ms
step:392/1375 train_time:53595ms step_avg:140.30ms
step:393/1375 train_time:53738ms step_avg:140.31ms
step:394/1375 train_time:53883ms step_avg:140.32ms
step:395/1375 train_time:54030ms step_avg:140.34ms
step:396/1375 train_time:54175ms step_avg:140.35ms
step:397/1375 train_time:54319ms step_avg:140.36ms
step:398/1375 train_time:54464ms step_avg:140.37ms
step:399/1375 train_time:54607ms step_avg:140.38ms
step:400/1375 train_time:54751ms step_avg:140.39ms
step:401/1375 train_time:54895ms step_avg:140.40ms
step:402/1375 train_time:55040ms step_avg:140.41ms
step:403/1375 train_time:55186ms step_avg:140.42ms
step:404/1375 train_time:55331ms step_avg:140.43ms
step:405/1375 train_time:55475ms step_avg:140.44ms
step:406/1375 train_time:55618ms step_avg:140.45ms
step:407/1375 train_time:55762ms step_avg:140.46ms
step:408/1375 train_time:55905ms step_avg:140.47ms
step:409/1375 train_time:56051ms step_avg:140.48ms
step:410/1375 train_time:56196ms step_avg:140.49ms
step:411/1375 train_time:56343ms step_avg:140.51ms
step:412/1375 train_time:56489ms step_avg:140.52ms
step:413/1375 train_time:56635ms step_avg:140.53ms
step:414/1375 train_time:56779ms step_avg:140.54ms
step:415/1375 train_time:56924ms step_avg:140.55ms
step:416/1375 train_time:57071ms step_avg:140.57ms
step:417/1375 train_time:57217ms step_avg:140.58ms
step:418/1375 train_time:57364ms step_avg:140.60ms
step:419/1375 train_time:57509ms step_avg:140.61ms
step:420/1375 train_time:57654ms step_avg:140.62ms
step:421/1375 train_time:57801ms step_avg:140.64ms
step:422/1375 train_time:57947ms step_avg:140.65ms
step:423/1375 train_time:58091ms step_avg:140.66ms
step:424/1375 train_time:58236ms step_avg:140.67ms
step:425/1375 train_time:58382ms step_avg:140.68ms
step:426/1375 train_time:58530ms step_avg:140.70ms
step:427/1375 train_time:58675ms step_avg:140.71ms
step:428/1375 train_time:58821ms step_avg:140.72ms
step:429/1375 train_time:58968ms step_avg:140.73ms
step:430/1375 train_time:59113ms step_avg:140.74ms
step:431/1375 train_time:59258ms step_avg:140.75ms
step:432/1375 train_time:59402ms step_avg:140.76ms
step:433/1375 train_time:59549ms step_avg:140.78ms
step:434/1375 train_time:59693ms step_avg:140.79ms
step:435/1375 train_time:59840ms step_avg:140.80ms
step:436/1375 train_time:59987ms step_avg:140.81ms
step:437/1375 train_time:60133ms step_avg:140.83ms
step:438/1375 train_time:60280ms step_avg:140.84ms
step:439/1375 train_time:60427ms step_avg:140.86ms
step:440/1375 train_time:60573ms step_avg:140.87ms
step:441/1375 train_time:60717ms step_avg:140.88ms
step:442/1375 train_time:60863ms step_avg:140.89ms
step:443/1375 train_time:61009ms step_avg:140.90ms
step:444/1375 train_time:61154ms step_avg:140.91ms
step:445/1375 train_time:61299ms step_avg:140.92ms
step:446/1375 train_time:61448ms step_avg:140.94ms
step:447/1375 train_time:61592ms step_avg:140.94ms
step:448/1375 train_time:61739ms step_avg:140.96ms
step:449/1375 train_time:61885ms step_avg:140.97ms
step:450/1375 train_time:62031ms step_avg:140.98ms
step:451/1375 train_time:62177ms step_avg:140.99ms
step:452/1375 train_time:62323ms step_avg:141.00ms
step:453/1375 train_time:62470ms step_avg:141.02ms
step:454/1375 train_time:62615ms step_avg:141.03ms
step:455/1375 train_time:62762ms step_avg:141.04ms
step:456/1375 train_time:62909ms step_avg:141.05ms
step:457/1375 train_time:63055ms step_avg:141.06ms
step:458/1375 train_time:63200ms step_avg:141.07ms
step:459/1375 train_time:63348ms step_avg:141.09ms
step:460/1375 train_time:63492ms step_avg:141.09ms
step:461/1375 train_time:63637ms step_avg:141.10ms
step:462/1375 train_time:63785ms step_avg:141.12ms
step:463/1375 train_time:63931ms step_avg:141.13ms
step:464/1375 train_time:64077ms step_avg:141.14ms
step:465/1375 train_time:64222ms step_avg:141.15ms
step:466/1375 train_time:64371ms step_avg:141.16ms
step:467/1375 train_time:64517ms step_avg:141.18ms
step:468/1375 train_time:64663ms step_avg:141.18ms
step:469/1375 train_time:64809ms step_avg:141.20ms
step:470/1375 train_time:64954ms step_avg:141.21ms
step:471/1375 train_time:65099ms step_avg:141.21ms
step:472/1375 train_time:65246ms step_avg:141.23ms
step:473/1375 train_time:65391ms step_avg:141.23ms
step:474/1375 train_time:65537ms step_avg:141.24ms
step:475/1375 train_time:65682ms step_avg:141.25ms
step:476/1375 train_time:65829ms step_avg:141.26ms
step:477/1375 train_time:65974ms step_avg:141.27ms
step:478/1375 train_time:66121ms step_avg:141.28ms
step:479/1375 train_time:66269ms step_avg:141.30ms
step:480/1375 train_time:66415ms step_avg:141.31ms
step:481/1375 train_time:66560ms step_avg:141.32ms
step:482/1375 train_time:66706ms step_avg:141.33ms
step:483/1375 train_time:66852ms step_avg:141.34ms
step:484/1375 train_time:66997ms step_avg:141.34ms
step:485/1375 train_time:67143ms step_avg:141.35ms
step:486/1375 train_time:67290ms step_avg:141.36ms
step:487/1375 train_time:67435ms step_avg:141.37ms
step:488/1375 train_time:67582ms step_avg:141.39ms
step:489/1375 train_time:67729ms step_avg:141.40ms
step:490/1375 train_time:67874ms step_avg:141.40ms
step:491/1375 train_time:68021ms step_avg:141.42ms
step:492/1375 train_time:68168ms step_avg:141.43ms
step:493/1375 train_time:68312ms step_avg:141.43ms
step:494/1375 train_time:68457ms step_avg:141.44ms
step:495/1375 train_time:68602ms step_avg:141.45ms
step:496/1375 train_time:68749ms step_avg:141.46ms
step:497/1375 train_time:68893ms step_avg:141.46ms
step:498/1375 train_time:69040ms step_avg:141.47ms
step:499/1375 train_time:69187ms step_avg:141.49ms
step:500/1375 train_time:69333ms step_avg:141.50ms
step:500/1375 val_loss:3.6551 train_time:69405ms step_avg:141.64ms
step:501/1375 train_time:69480ms step_avg:141.51ms
step:502/1375 train_time:69628ms step_avg:141.52ms
step:503/1375 train_time:69774ms step_avg:141.53ms
step:504/1375 train_time:69919ms step_avg:141.54ms
step:505/1375 train_time:70064ms step_avg:141.54ms
step:506/1375 train_time:70209ms step_avg:141.55ms
step:507/1375 train_time:70357ms step_avg:141.56ms
step:508/1375 train_time:70503ms step_avg:141.57ms
step:509/1375 train_time:70650ms step_avg:141.58ms
step:510/1375 train_time:70797ms step_avg:141.59ms
step:511/1375 train_time:70942ms step_avg:141.60ms
step:512/1375 train_time:71090ms step_avg:141.61ms
step:513/1375 train_time:71239ms step_avg:141.63ms
step:514/1375 train_time:71385ms step_avg:141.64ms
step:515/1375 train_time:71533ms step_avg:141.65ms
step:516/1375 train_time:71682ms step_avg:141.66ms
step:517/1375 train_time:71828ms step_avg:141.67ms
step:518/1375 train_time:71977ms step_avg:141.69ms
step:519/1375 train_time:72124ms step_avg:141.70ms
step:520/1375 train_time:72272ms step_avg:141.71ms
step:521/1375 train_time:72421ms step_avg:141.72ms
step:522/1375 train_time:72570ms step_avg:141.74ms
step:523/1375 train_time:72718ms step_avg:141.75ms
step:524/1375 train_time:72865ms step_avg:141.76ms
step:525/1375 train_time:73011ms step_avg:141.77ms
step:526/1375 train_time:73162ms step_avg:141.79ms
step:527/1375 train_time:73309ms step_avg:141.80ms
step:528/1375 train_time:73456ms step_avg:141.81ms
step:529/1375 train_time:73603ms step_avg:141.82ms
step:530/1375 train_time:73750ms step_avg:141.83ms
step:531/1375 train_time:73898ms step_avg:141.84ms
step:532/1375 train_time:74046ms step_avg:141.85ms
step:533/1375 train_time:74193ms step_avg:141.86ms
step:534/1375 train_time:74341ms step_avg:141.87ms
step:535/1375 train_time:74490ms step_avg:141.89ms
step:536/1375 train_time:74639ms step_avg:141.90ms
step:537/1375 train_time:74785ms step_avg:141.91ms
step:538/1375 train_time:74932ms step_avg:141.92ms
step:539/1375 train_time:75082ms step_avg:141.93ms
step:540/1375 train_time:75230ms step_avg:141.94ms
step:541/1375 train_time:75376ms step_avg:141.95ms
step:542/1375 train_time:75523ms step_avg:141.96ms
step:543/1375 train_time:75671ms step_avg:141.97ms
step:544/1375 train_time:75819ms step_avg:141.98ms
step:545/1375 train_time:75966ms step_avg:141.99ms
step:546/1375 train_time:76114ms step_avg:142.00ms
step:547/1375 train_time:76261ms step_avg:142.01ms
step:548/1375 train_time:76409ms step_avg:142.02ms
step:549/1375 train_time:76557ms step_avg:142.04ms
step:550/1375 train_time:76704ms step_avg:142.04ms
step:551/1375 train_time:76851ms step_avg:142.05ms
step:552/1375 train_time:77000ms step_avg:142.07ms
step:553/1375 train_time:77146ms step_avg:142.07ms
step:554/1375 train_time:77293ms step_avg:142.08ms
step:555/1375 train_time:77443ms step_avg:142.10ms
step:556/1375 train_time:77591ms step_avg:142.11ms
step:557/1375 train_time:77741ms step_avg:142.12ms
step:558/1375 train_time:77886ms step_avg:142.13ms
step:559/1375 train_time:78031ms step_avg:142.13ms
step:560/1375 train_time:78179ms step_avg:142.14ms
step:561/1375 train_time:78327ms step_avg:142.15ms
step:562/1375 train_time:78475ms step_avg:142.17ms
step:563/1375 train_time:78623ms step_avg:142.18ms
step:564/1375 train_time:78770ms step_avg:142.18ms
step:565/1375 train_time:78917ms step_avg:142.19ms
step:566/1375 train_time:79064ms step_avg:142.20ms
step:567/1375 train_time:79212ms step_avg:142.21ms
step:568/1375 train_time:79361ms step_avg:142.22ms
step:569/1375 train_time:79509ms step_avg:142.23ms
step:570/1375 train_time:79658ms step_avg:142.25ms
step:571/1375 train_time:79846ms step_avg:142.33ms
step:572/1375 train_time:79992ms step_avg:142.33ms
step:573/1375 train_time:80139ms step_avg:142.34ms
step:574/1375 train_time:80287ms step_avg:142.35ms
step:575/1375 train_time:80434ms step_avg:142.36ms
step:576/1375 train_time:80580ms step_avg:142.37ms
step:577/1375 train_time:80727ms step_avg:142.38ms
step:578/1375 train_time:80877ms step_avg:142.39ms
step:579/1375 train_time:81025ms step_avg:142.40ms
step:580/1375 train_time:81172ms step_avg:142.41ms
step:581/1375 train_time:81320ms step_avg:142.42ms
step:582/1375 train_time:81467ms step_avg:142.43ms
step:583/1375 train_time:81614ms step_avg:142.43ms
step:584/1375 train_time:81762ms step_avg:142.44ms
step:585/1375 train_time:81910ms step_avg:142.45ms
step:586/1375 train_time:82060ms step_avg:142.46ms
step:587/1375 train_time:82206ms step_avg:142.47ms
step:588/1375 train_time:82352ms step_avg:142.48ms
step:589/1375 train_time:82499ms step_avg:142.49ms
step:590/1375 train_time:82646ms step_avg:142.49ms
step:591/1375 train_time:82792ms step_avg:142.50ms
step:592/1375 train_time:82943ms step_avg:142.51ms
step:593/1375 train_time:83090ms step_avg:142.52ms
step:594/1375 train_time:83238ms step_avg:142.53ms
step:595/1375 train_time:83384ms step_avg:142.54ms
step:596/1375 train_time:83531ms step_avg:142.54ms
step:597/1375 train_time:83679ms step_avg:142.55ms
step:598/1375 train_time:83825ms step_avg:142.56ms
step:599/1375 train_time:83974ms step_avg:142.57ms
step:600/1375 train_time:84122ms step_avg:142.58ms
step:601/1375 train_time:84271ms step_avg:142.59ms
step:602/1375 train_time:84418ms step_avg:142.60ms
step:603/1375 train_time:84564ms step_avg:142.60ms
step:604/1375 train_time:84711ms step_avg:142.61ms
step:605/1375 train_time:84859ms step_avg:142.62ms
step:606/1375 train_time:85007ms step_avg:142.63ms
step:607/1375 train_time:85154ms step_avg:142.64ms
step:608/1375 train_time:85301ms step_avg:142.64ms
step:609/1375 train_time:85447ms step_avg:142.65ms
step:610/1375 train_time:85594ms step_avg:142.66ms
step:611/1375 train_time:85742ms step_avg:142.67ms
step:612/1375 train_time:85889ms step_avg:142.67ms
step:613/1375 train_time:86039ms step_avg:142.68ms
step:614/1375 train_time:86187ms step_avg:142.69ms
step:615/1375 train_time:86336ms step_avg:142.70ms
step:616/1375 train_time:86482ms step_avg:142.71ms
step:617/1375 train_time:86632ms step_avg:142.72ms
step:618/1375 train_time:86782ms step_avg:142.73ms
step:619/1375 train_time:86931ms step_avg:142.74ms
step:620/1375 train_time:87080ms step_avg:142.75ms
step:621/1375 train_time:87230ms step_avg:142.77ms
step:622/1375 train_time:87380ms step_avg:142.78ms
step:623/1375 train_time:87529ms step_avg:142.79ms
step:624/1375 train_time:87678ms step_avg:142.80ms
step:625/1375 train_time:87827ms step_avg:142.81ms
step:625/1375 val_loss:3.5744 train_time:87905ms step_avg:142.93ms
step:626/1375 train_time:87980ms step_avg:142.82ms
step:627/1375 train_time:88133ms step_avg:142.84ms
step:628/1375 train_time:88281ms step_avg:142.85ms
step:629/1375 train_time:88432ms step_avg:142.86ms
step:630/1375 train_time:88579ms step_avg:142.87ms
step:631/1375 train_time:88726ms step_avg:142.88ms
step:632/1375 train_time:88875ms step_avg:142.89ms
step:633/1375 train_time:89025ms step_avg:142.90ms
step:634/1375 train_time:89175ms step_avg:142.91ms
step:635/1375 train_time:89323ms step_avg:142.92ms
step:636/1375 train_time:89472ms step_avg:142.93ms
step:637/1375 train_time:89620ms step_avg:142.93ms
step:638/1375 train_time:89768ms step_avg:142.94ms
step:639/1375 train_time:89916ms step_avg:142.95ms
step:640/1375 train_time:90068ms step_avg:142.96ms
step:641/1375 train_time:90218ms step_avg:142.98ms
step:642/1375 train_time:90367ms step_avg:142.99ms
step:643/1375 train_time:90518ms step_avg:143.00ms
step:644/1375 train_time:90665ms step_avg:143.01ms
step:645/1375 train_time:90816ms step_avg:143.02ms
step:646/1375 train_time:90964ms step_avg:143.03ms
step:647/1375 train_time:91113ms step_avg:143.03ms
step:648/1375 train_time:91265ms step_avg:143.05ms
step:649/1375 train_time:91415ms step_avg:143.06ms
step:650/1375 train_time:91564ms step_avg:143.07ms
step:651/1375 train_time:91715ms step_avg:143.08ms
step:652/1375 train_time:91864ms step_avg:143.09ms
step:653/1375 train_time:92013ms step_avg:143.10ms
step:654/1375 train_time:92163ms step_avg:143.11ms
step:655/1375 train_time:92312ms step_avg:143.12ms
step:656/1375 train_time:92461ms step_avg:143.13ms
step:657/1375 train_time:92611ms step_avg:143.14ms
step:658/1375 train_time:92760ms step_avg:143.15ms
step:659/1375 train_time:92911ms step_avg:143.16ms
step:660/1375 train_time:93056ms step_avg:143.16ms
step:661/1375 train_time:93206ms step_avg:143.17ms
step:662/1375 train_time:93356ms step_avg:143.18ms
step:663/1375 train_time:93505ms step_avg:143.19ms
step:664/1375 train_time:93656ms step_avg:143.21ms
step:665/1375 train_time:93806ms step_avg:143.21ms
step:666/1375 train_time:93954ms step_avg:143.22ms
step:667/1375 train_time:94101ms step_avg:143.23ms
step:668/1375 train_time:94252ms step_avg:143.24ms
step:669/1375 train_time:94401ms step_avg:143.25ms
step:670/1375 train_time:94550ms step_avg:143.26ms
step:671/1375 train_time:94700ms step_avg:143.27ms
step:672/1375 train_time:94851ms step_avg:143.28ms
step:673/1375 train_time:94999ms step_avg:143.29ms
step:674/1375 train_time:95149ms step_avg:143.30ms
step:675/1375 train_time:95297ms step_avg:143.30ms
step:676/1375 train_time:95447ms step_avg:143.31ms
step:677/1375 train_time:95596ms step_avg:143.32ms
step:678/1375 train_time:95745ms step_avg:143.33ms
step:679/1375 train_time:95897ms step_avg:143.34ms
step:680/1375 train_time:96045ms step_avg:143.35ms
step:681/1375 train_time:96194ms step_avg:143.36ms
step:682/1375 train_time:96343ms step_avg:143.37ms
step:683/1375 train_time:96494ms step_avg:143.38ms
step:684/1375 train_time:96643ms step_avg:143.39ms
step:685/1375 train_time:96793ms step_avg:143.40ms
step:686/1375 train_time:96942ms step_avg:143.40ms
step:687/1375 train_time:97090ms step_avg:143.41ms
step:688/1375 train_time:97239ms step_avg:143.42ms
step:689/1375 train_time:97389ms step_avg:143.43ms
step:690/1375 train_time:97540ms step_avg:143.44ms
step:691/1375 train_time:97690ms step_avg:143.45ms
step:692/1375 train_time:97838ms step_avg:143.46ms
step:693/1375 train_time:97989ms step_avg:143.47ms
step:694/1375 train_time:98137ms step_avg:143.48ms
step:695/1375 train_time:98285ms step_avg:143.48ms
step:696/1375 train_time:98436ms step_avg:143.49ms
step:697/1375 train_time:98586ms step_avg:143.50ms
step:698/1375 train_time:98735ms step_avg:143.51ms
step:699/1375 train_time:98884ms step_avg:143.52ms
step:700/1375 train_time:99033ms step_avg:143.53ms
step:701/1375 train_time:99181ms step_avg:143.53ms
step:702/1375 train_time:99333ms step_avg:143.55ms
step:703/1375 train_time:99483ms step_avg:143.55ms
step:704/1375 train_time:99633ms step_avg:143.56ms
step:705/1375 train_time:99781ms step_avg:143.57ms
step:706/1375 train_time:99933ms step_avg:143.58ms
step:707/1375 train_time:100082ms step_avg:143.59ms
step:708/1375 train_time:100231ms step_avg:143.60ms
step:709/1375 train_time:100379ms step_avg:143.60ms
step:710/1375 train_time:100530ms step_avg:143.61ms
step:711/1375 train_time:100679ms step_avg:143.62ms
step:712/1375 train_time:100829ms step_avg:143.63ms
step:713/1375 train_time:100979ms step_avg:143.64ms
step:714/1375 train_time:101127ms step_avg:143.65ms
step:715/1375 train_time:101277ms step_avg:143.66ms
step:716/1375 train_time:101428ms step_avg:143.67ms
step:717/1375 train_time:101578ms step_avg:143.67ms
step:718/1375 train_time:101730ms step_avg:143.69ms
step:719/1375 train_time:101877ms step_avg:143.69ms
step:720/1375 train_time:102030ms step_avg:143.70ms
step:721/1375 train_time:102180ms step_avg:143.71ms
step:722/1375 train_time:102332ms step_avg:143.72ms
step:723/1375 train_time:102483ms step_avg:143.74ms
step:724/1375 train_time:102634ms step_avg:143.75ms
step:725/1375 train_time:102785ms step_avg:143.75ms
step:726/1375 train_time:102936ms step_avg:143.77ms
step:727/1375 train_time:103089ms step_avg:143.78ms
step:728/1375 train_time:103238ms step_avg:143.79ms
step:729/1375 train_time:103388ms step_avg:143.79ms
step:730/1375 train_time:103541ms step_avg:143.81ms
step:731/1375 train_time:103693ms step_avg:143.82ms
step:732/1375 train_time:103842ms step_avg:143.83ms
step:733/1375 train_time:103993ms step_avg:143.84ms
step:734/1375 train_time:104144ms step_avg:143.85ms
step:735/1375 train_time:104296ms step_avg:143.86ms
step:736/1375 train_time:104447ms step_avg:143.87ms
step:737/1375 train_time:104597ms step_avg:143.88ms
step:738/1375 train_time:104747ms step_avg:143.88ms
step:739/1375 train_time:104898ms step_avg:143.89ms
step:740/1375 train_time:105049ms step_avg:143.90ms
step:741/1375 train_time:105198ms step_avg:143.91ms
step:742/1375 train_time:105348ms step_avg:143.92ms
step:743/1375 train_time:105498ms step_avg:143.93ms
step:744/1375 train_time:105649ms step_avg:143.94ms
step:745/1375 train_time:105801ms step_avg:143.95ms
step:746/1375 train_time:105950ms step_avg:143.95ms
step:747/1375 train_time:106098ms step_avg:143.96ms
step:748/1375 train_time:106248ms step_avg:143.97ms
step:749/1375 train_time:106398ms step_avg:143.98ms
step:750/1375 train_time:106549ms step_avg:143.99ms
step:750/1375 val_loss:3.5195 train_time:106626ms step_avg:144.09ms
step:751/1375 train_time:106702ms step_avg:144.00ms
step:752/1375 train_time:106856ms step_avg:144.01ms
step:753/1375 train_time:107006ms step_avg:144.02ms
step:754/1375 train_time:107155ms step_avg:144.03ms
step:755/1375 train_time:107305ms step_avg:144.03ms
step:756/1375 train_time:107455ms step_avg:144.04ms
step:757/1375 train_time:107607ms step_avg:144.05ms
step:758/1375 train_time:107758ms step_avg:144.06ms
step:759/1375 train_time:107910ms step_avg:144.07ms
step:760/1375 train_time:108059ms step_avg:144.08ms
step:761/1375 train_time:108256ms step_avg:144.15ms
step:762/1375 train_time:108406ms step_avg:144.16ms
step:763/1375 train_time:108555ms step_avg:144.16ms
step:764/1375 train_time:108707ms step_avg:144.17ms
step:765/1375 train_time:108855ms step_avg:144.18ms
step:766/1375 train_time:109008ms step_avg:144.19ms
step:767/1375 train_time:109160ms step_avg:144.20ms
step:768/1375 train_time:109312ms step_avg:144.21ms
step:769/1375 train_time:109465ms step_avg:144.22ms
step:770/1375 train_time:109615ms step_avg:144.23ms
step:771/1375 train_time:109766ms step_avg:144.24ms
step:772/1375 train_time:109915ms step_avg:144.25ms
step:773/1375 train_time:110067ms step_avg:144.26ms
step:774/1375 train_time:110217ms step_avg:144.26ms
step:775/1375 train_time:110371ms step_avg:144.28ms
step:776/1375 train_time:110522ms step_avg:144.28ms
step:777/1375 train_time:110673ms step_avg:144.29ms
step:778/1375 train_time:110822ms step_avg:144.30ms
step:779/1375 train_time:110973ms step_avg:144.31ms
step:780/1375 train_time:111123ms step_avg:144.32ms
step:781/1375 train_time:111276ms step_avg:144.33ms
step:782/1375 train_time:111425ms step_avg:144.33ms
step:783/1375 train_time:111576ms step_avg:144.34ms
step:784/1375 train_time:111727ms step_avg:144.35ms
step:785/1375 train_time:111879ms step_avg:144.36ms
step:786/1375 train_time:112029ms step_avg:144.37ms
step:787/1375 train_time:112180ms step_avg:144.38ms
step:788/1375 train_time:112332ms step_avg:144.39ms
step:789/1375 train_time:112482ms step_avg:144.39ms
step:790/1375 train_time:112632ms step_avg:144.40ms
step:791/1375 train_time:112783ms step_avg:144.41ms
step:792/1375 train_time:112933ms step_avg:144.42ms
step:793/1375 train_time:113082ms step_avg:144.42ms
step:794/1375 train_time:113233ms step_avg:144.43ms
step:795/1375 train_time:113385ms step_avg:144.44ms
step:796/1375 train_time:113536ms step_avg:144.45ms
step:797/1375 train_time:113689ms step_avg:144.46ms
step:798/1375 train_time:113839ms step_avg:144.47ms
step:799/1375 train_time:113995ms step_avg:144.48ms
step:800/1375 train_time:114146ms step_avg:144.49ms
step:801/1375 train_time:114295ms step_avg:144.49ms
step:802/1375 train_time:114447ms step_avg:144.50ms
step:803/1375 train_time:114596ms step_avg:144.51ms
step:804/1375 train_time:114745ms step_avg:144.52ms
step:805/1375 train_time:114898ms step_avg:144.53ms
step:806/1375 train_time:115050ms step_avg:144.54ms
step:807/1375 train_time:115197ms step_avg:144.54ms
step:808/1375 train_time:115349ms step_avg:144.55ms
step:809/1375 train_time:115498ms step_avg:144.55ms
step:810/1375 train_time:115649ms step_avg:144.56ms
step:811/1375 train_time:115798ms step_avg:144.57ms
step:812/1375 train_time:115949ms step_avg:144.57ms
step:813/1375 train_time:116098ms step_avg:144.58ms
step:814/1375 train_time:116250ms step_avg:144.59ms
step:815/1375 train_time:116399ms step_avg:144.60ms
step:816/1375 train_time:116552ms step_avg:144.61ms
step:817/1375 train_time:116704ms step_avg:144.61ms
step:818/1375 train_time:116854ms step_avg:144.62ms
step:819/1375 train_time:117007ms step_avg:144.63ms
step:820/1375 train_time:117160ms step_avg:144.64ms
step:821/1375 train_time:117312ms step_avg:144.65ms
step:822/1375 train_time:117464ms step_avg:144.66ms
step:823/1375 train_time:117617ms step_avg:144.67ms
step:824/1375 train_time:117768ms step_avg:144.68ms
step:825/1375 train_time:117921ms step_avg:144.69ms
step:826/1375 train_time:118074ms step_avg:144.70ms
step:827/1375 train_time:118225ms step_avg:144.71ms
step:828/1375 train_time:118378ms step_avg:144.72ms
step:829/1375 train_time:118529ms step_avg:144.72ms
step:830/1375 train_time:118681ms step_avg:144.73ms
step:831/1375 train_time:118833ms step_avg:144.74ms
step:832/1375 train_time:118986ms step_avg:144.75ms
step:833/1375 train_time:119136ms step_avg:144.76ms
step:834/1375 train_time:119288ms step_avg:144.77ms
step:835/1375 train_time:119439ms step_avg:144.77ms
step:836/1375 train_time:119595ms step_avg:144.79ms
step:837/1375 train_time:119748ms step_avg:144.80ms
step:838/1375 train_time:119898ms step_avg:144.80ms
step:839/1375 train_time:120048ms step_avg:144.81ms
step:840/1375 train_time:120199ms step_avg:144.82ms
step:841/1375 train_time:120352ms step_avg:144.83ms
step:842/1375 train_time:120504ms step_avg:144.84ms
step:843/1375 train_time:120655ms step_avg:144.84ms
step:844/1375 train_time:120806ms step_avg:144.85ms
step:845/1375 train_time:120957ms step_avg:144.86ms
step:846/1375 train_time:121109ms step_avg:144.87ms
step:847/1375 train_time:121262ms step_avg:144.88ms
step:848/1375 train_time:121415ms step_avg:144.89ms
step:849/1375 train_time:121567ms step_avg:144.90ms
step:850/1375 train_time:121719ms step_avg:144.90ms
step:851/1375 train_time:121873ms step_avg:144.91ms
step:852/1375 train_time:122025ms step_avg:144.92ms
step:853/1375 train_time:122175ms step_avg:144.93ms
step:854/1375 train_time:122327ms step_avg:144.94ms
step:855/1375 train_time:122478ms step_avg:144.94ms
step:856/1375 train_time:122627ms step_avg:144.95ms
step:857/1375 train_time:122778ms step_avg:144.96ms
step:858/1375 train_time:122933ms step_avg:144.97ms
step:859/1375 train_time:123086ms step_avg:144.98ms
step:860/1375 train_time:123237ms step_avg:144.98ms
step:861/1375 train_time:123389ms step_avg:144.99ms
step:862/1375 train_time:123539ms step_avg:145.00ms
step:863/1375 train_time:123692ms step_avg:145.01ms
step:864/1375 train_time:123845ms step_avg:145.02ms
step:865/1375 train_time:123995ms step_avg:145.02ms
step:866/1375 train_time:124152ms step_avg:145.04ms
step:867/1375 train_time:124302ms step_avg:145.04ms
step:868/1375 train_time:124454ms step_avg:145.05ms
step:869/1375 train_time:124606ms step_avg:145.06ms
step:870/1375 train_time:124759ms step_avg:145.07ms
step:871/1375 train_time:124910ms step_avg:145.08ms
step:872/1375 train_time:125061ms step_avg:145.08ms
step:873/1375 train_time:125212ms step_avg:145.09ms
step:874/1375 train_time:125366ms step_avg:145.10ms
step:875/1375 train_time:125517ms step_avg:145.11ms
step:875/1375 val_loss:3.4671 train_time:125593ms step_avg:145.19ms
step:876/1375 train_time:125668ms step_avg:145.11ms
step:877/1375 train_time:125821ms step_avg:145.12ms
step:878/1375 train_time:125973ms step_avg:145.13ms
step:879/1375 train_time:126125ms step_avg:145.14ms
step:880/1375 train_time:126276ms step_avg:145.14ms
step:881/1375 train_time:126426ms step_avg:145.15ms
step:882/1375 train_time:126579ms step_avg:145.16ms
step:883/1375 train_time:126732ms step_avg:145.17ms
step:884/1375 train_time:126883ms step_avg:145.18ms
step:885/1375 train_time:127035ms step_avg:145.18ms
step:886/1375 train_time:127189ms step_avg:145.19ms
step:887/1375 train_time:127341ms step_avg:145.20ms
step:888/1375 train_time:127492ms step_avg:145.21ms
step:889/1375 train_time:127648ms step_avg:145.22ms
step:890/1375 train_time:127800ms step_avg:145.23ms
step:891/1375 train_time:127951ms step_avg:145.23ms
step:892/1375 train_time:128105ms step_avg:145.24ms
step:893/1375 train_time:128255ms step_avg:145.25ms
step:894/1375 train_time:128408ms step_avg:145.26ms
step:895/1375 train_time:128563ms step_avg:145.27ms
step:896/1375 train_time:128714ms step_avg:145.28ms
step:897/1375 train_time:128866ms step_avg:145.28ms
step:898/1375 train_time:129019ms step_avg:145.29ms
step:899/1375 train_time:129170ms step_avg:145.30ms
step:900/1375 train_time:129320ms step_avg:145.30ms
step:901/1375 train_time:129473ms step_avg:145.31ms
step:902/1375 train_time:129623ms step_avg:145.32ms
step:903/1375 train_time:129775ms step_avg:145.33ms
step:904/1375 train_time:129928ms step_avg:145.33ms
step:905/1375 train_time:130081ms step_avg:145.34ms
step:906/1375 train_time:130233ms step_avg:145.35ms
step:907/1375 train_time:130387ms step_avg:145.36ms
step:908/1375 train_time:130538ms step_avg:145.36ms
step:909/1375 train_time:130690ms step_avg:145.37ms
step:910/1375 train_time:130849ms step_avg:145.39ms
step:911/1375 train_time:131001ms step_avg:145.39ms
step:912/1375 train_time:131152ms step_avg:145.40ms
step:913/1375 train_time:131306ms step_avg:145.41ms
step:914/1375 train_time:131457ms step_avg:145.42ms
step:915/1375 train_time:131609ms step_avg:145.42ms
step:916/1375 train_time:131762ms step_avg:145.43ms
step:917/1375 train_time:131914ms step_avg:145.44ms
step:918/1375 train_time:132067ms step_avg:145.45ms
step:919/1375 train_time:132226ms step_avg:145.46ms
step:920/1375 train_time:132376ms step_avg:145.47ms
step:921/1375 train_time:132530ms step_avg:145.48ms
step:922/1375 train_time:132687ms step_avg:145.49ms
step:923/1375 train_time:132839ms step_avg:145.50ms
step:924/1375 train_time:132992ms step_avg:145.51ms
step:925/1375 train_time:133149ms step_avg:145.52ms
step:926/1375 train_time:133302ms step_avg:145.53ms
step:927/1375 train_time:133455ms step_avg:145.53ms
step:928/1375 train_time:133608ms step_avg:145.54ms
step:929/1375 train_time:133763ms step_avg:145.55ms
step:930/1375 train_time:133916ms step_avg:145.56ms
step:931/1375 train_time:134069ms step_avg:145.57ms
step:932/1375 train_time:134223ms step_avg:145.58ms
step:933/1375 train_time:134375ms step_avg:145.59ms
step:934/1375 train_time:134529ms step_avg:145.59ms
step:935/1375 train_time:134683ms step_avg:145.60ms
step:936/1375 train_time:134835ms step_avg:145.61ms
step:937/1375 train_time:134992ms step_avg:145.62ms
step:938/1375 train_time:135146ms step_avg:145.63ms
step:939/1375 train_time:135301ms step_avg:145.64ms
step:940/1375 train_time:135454ms step_avg:145.65ms
step:941/1375 train_time:135607ms step_avg:145.66ms
step:942/1375 train_time:135758ms step_avg:145.66ms
step:943/1375 train_time:135914ms step_avg:145.67ms
step:944/1375 train_time:136071ms step_avg:145.69ms
step:945/1375 train_time:136226ms step_avg:145.70ms
step:946/1375 train_time:136381ms step_avg:145.71ms
step:947/1375 train_time:136534ms step_avg:145.71ms
step:948/1375 train_time:136687ms step_avg:145.72ms
step:949/1375 train_time:136844ms step_avg:145.73ms
step:950/1375 train_time:136995ms step_avg:145.74ms
step:951/1375 train_time:137193ms step_avg:145.79ms
step:952/1375 train_time:137344ms step_avg:145.80ms
step:953/1375 train_time:137496ms step_avg:145.81ms
step:954/1375 train_time:137649ms step_avg:145.81ms
step:955/1375 train_time:137799ms step_avg:145.82ms
step:956/1375 train_time:137951ms step_avg:145.83ms
step:957/1375 train_time:138105ms step_avg:145.83ms
step:958/1375 train_time:138262ms step_avg:145.85ms
step:959/1375 train_time:138417ms step_avg:145.86ms
step:960/1375 train_time:138572ms step_avg:145.86ms
step:961/1375 train_time:138723ms step_avg:145.87ms
step:962/1375 train_time:138874ms step_avg:145.88ms
step:963/1375 train_time:139031ms step_avg:145.89ms
step:964/1375 train_time:139185ms step_avg:145.90ms
step:965/1375 train_time:139336ms step_avg:145.90ms
step:966/1375 train_time:139490ms step_avg:145.91ms
step:967/1375 train_time:139643ms step_avg:145.92ms
step:968/1375 train_time:139792ms step_avg:145.92ms
step:969/1375 train_time:139946ms step_avg:145.93ms
step:970/1375 train_time:140100ms step_avg:145.94ms
step:971/1375 train_time:140254ms step_avg:145.95ms
step:972/1375 train_time:140407ms step_avg:145.95ms
step:973/1375 train_time:140559ms step_avg:145.96ms
step:974/1375 train_time:140711ms step_avg:145.97ms
step:975/1375 train_time:140867ms step_avg:145.98ms
step:976/1375 train_time:141019ms step_avg:145.98ms
step:977/1375 train_time:141171ms step_avg:145.99ms
step:978/1375 train_time:141325ms step_avg:146.00ms
step:979/1375 train_time:141475ms step_avg:146.00ms
step:980/1375 train_time:141626ms step_avg:146.01ms
step:981/1375 train_time:141778ms step_avg:146.01ms
step:982/1375 train_time:141929ms step_avg:146.02ms
step:983/1375 train_time:142081ms step_avg:146.02ms
step:984/1375 train_time:142233ms step_avg:146.03ms
step:985/1375 train_time:142386ms step_avg:146.04ms
step:986/1375 train_time:142542ms step_avg:146.05ms
step:987/1375 train_time:142693ms step_avg:146.05ms
step:988/1375 train_time:142847ms step_avg:146.06ms
step:989/1375 train_time:142999ms step_avg:146.07ms
step:990/1375 train_time:143151ms step_avg:146.07ms
step:991/1375 train_time:143304ms step_avg:146.08ms
step:992/1375 train_time:143463ms step_avg:146.09ms
step:993/1375 train_time:143626ms step_avg:146.11ms
step:994/1375 train_time:143779ms step_avg:146.12ms
step:995/1375 train_time:143929ms step_avg:146.12ms
step:996/1375 train_time:144081ms step_avg:146.13ms
step:997/1375 train_time:144232ms step_avg:146.13ms
step:998/1375 train_time:144384ms step_avg:146.14ms
step:999/1375 train_time:144537ms step_avg:146.14ms
step:1000/1375 train_time:144689ms step_avg:146.15ms
step:1000/1375 val_loss:3.4016 train_time:144766ms step_avg:146.23ms
step:1001/1375 train_time:144842ms step_avg:146.16ms
step:1002/1375 train_time:144998ms step_avg:146.17ms
step:1003/1375 train_time:145153ms step_avg:146.18ms
step:1004/1375 train_time:145307ms step_avg:146.18ms
step:1005/1375 train_time:145459ms step_avg:146.19ms
step:1006/1375 train_time:145610ms step_avg:146.19ms
step:1007/1375 train_time:145764ms step_avg:146.20ms
step:1008/1375 train_time:145919ms step_avg:146.21ms
step:1009/1375 train_time:146079ms step_avg:146.23ms
step:1010/1375 train_time:146231ms step_avg:146.23ms
step:1011/1375 train_time:146384ms step_avg:146.24ms
step:1012/1375 train_time:146535ms step_avg:146.24ms
step:1013/1375 train_time:146690ms step_avg:146.25ms
step:1014/1375 train_time:146844ms step_avg:146.26ms
step:1015/1375 train_time:146997ms step_avg:146.27ms
step:1016/1375 train_time:147152ms step_avg:146.27ms
step:1017/1375 train_time:147304ms step_avg:146.28ms
step:1018/1375 train_time:147457ms step_avg:146.29ms
step:1019/1375 train_time:147611ms step_avg:146.29ms
step:1020/1375 train_time:147768ms step_avg:146.30ms
step:1021/1375 train_time:147922ms step_avg:146.31ms
step:1022/1375 train_time:148077ms step_avg:146.32ms
step:1023/1375 train_time:148231ms step_avg:146.33ms
step:1024/1375 train_time:148383ms step_avg:146.33ms
step:1025/1375 train_time:148538ms step_avg:146.34ms
step:1026/1375 train_time:148690ms step_avg:146.35ms
step:1027/1375 train_time:148842ms step_avg:146.35ms
step:1028/1375 train_time:148998ms step_avg:146.36ms
step:1029/1375 train_time:149154ms step_avg:146.37ms
step:1030/1375 train_time:149309ms step_avg:146.38ms
step:1031/1375 train_time:149459ms step_avg:146.39ms
step:1032/1375 train_time:149612ms step_avg:146.39ms
step:1033/1375 train_time:149765ms step_avg:146.40ms
step:1034/1375 train_time:149917ms step_avg:146.40ms
step:1035/1375 train_time:150074ms step_avg:146.41ms
step:1036/1375 train_time:150228ms step_avg:146.42ms
step:1037/1375 train_time:150385ms step_avg:146.43ms
step:1038/1375 train_time:150539ms step_avg:146.44ms
step:1039/1375 train_time:150692ms step_avg:146.45ms
step:1040/1375 train_time:150845ms step_avg:146.45ms
step:1041/1375 train_time:151000ms step_avg:146.46ms
step:1042/1375 train_time:151152ms step_avg:146.47ms
step:1043/1375 train_time:151303ms step_avg:146.47ms
step:1044/1375 train_time:151459ms step_avg:146.48ms
step:1045/1375 train_time:151615ms step_avg:146.49ms
step:1046/1375 train_time:151766ms step_avg:146.49ms
step:1047/1375 train_time:151919ms step_avg:146.50ms
step:1048/1375 train_time:152074ms step_avg:146.51ms
step:1049/1375 train_time:152229ms step_avg:146.51ms
step:1050/1375 train_time:152386ms step_avg:146.53ms
step:1051/1375 train_time:152544ms step_avg:146.54ms
step:1052/1375 train_time:152698ms step_avg:146.54ms
step:1053/1375 train_time:152851ms step_avg:146.55ms
step:1054/1375 train_time:153006ms step_avg:146.56ms
step:1055/1375 train_time:153160ms step_avg:146.56ms
step:1056/1375 train_time:153315ms step_avg:146.57ms
step:1057/1375 train_time:153469ms step_avg:146.58ms
step:1058/1375 train_time:153628ms step_avg:146.59ms
step:1059/1375 train_time:153783ms step_avg:146.60ms
step:1060/1375 train_time:153937ms step_avg:146.61ms
step:1061/1375 train_time:154090ms step_avg:146.61ms
step:1062/1375 train_time:154246ms step_avg:146.62ms
step:1063/1375 train_time:154399ms step_avg:146.63ms
step:1064/1375 train_time:154552ms step_avg:146.63ms
step:1065/1375 train_time:154704ms step_avg:146.64ms
step:1066/1375 train_time:154863ms step_avg:146.65ms
step:1067/1375 train_time:155020ms step_avg:146.66ms
step:1068/1375 train_time:155172ms step_avg:146.67ms
step:1069/1375 train_time:155332ms step_avg:146.68ms
step:1070/1375 train_time:155483ms step_avg:146.68ms
step:1071/1375 train_time:155639ms step_avg:146.69ms
step:1072/1375 train_time:155792ms step_avg:146.70ms
step:1073/1375 train_time:155945ms step_avg:146.70ms
step:1074/1375 train_time:156098ms step_avg:146.71ms
step:1075/1375 train_time:156253ms step_avg:146.72ms
step:1076/1375 train_time:156406ms step_avg:146.72ms
step:1077/1375 train_time:156559ms step_avg:146.73ms
step:1078/1375 train_time:156718ms step_avg:146.74ms
step:1079/1375 train_time:156876ms step_avg:146.75ms
step:1080/1375 train_time:157033ms step_avg:146.76ms
step:1081/1375 train_time:157185ms step_avg:146.76ms
step:1082/1375 train_time:157338ms step_avg:146.77ms
step:1083/1375 train_time:157493ms step_avg:146.78ms
step:1084/1375 train_time:157650ms step_avg:146.79ms
step:1085/1375 train_time:157803ms step_avg:146.79ms
step:1086/1375 train_time:157959ms step_avg:146.80ms
step:1087/1375 train_time:158115ms step_avg:146.81ms
step:1088/1375 train_time:158269ms step_avg:146.82ms
step:1089/1375 train_time:158426ms step_avg:146.83ms
step:1090/1375 train_time:158586ms step_avg:146.84ms
step:1091/1375 train_time:158740ms step_avg:146.85ms
step:1092/1375 train_time:158894ms step_avg:146.85ms
step:1093/1375 train_time:159051ms step_avg:146.86ms
step:1094/1375 train_time:159204ms step_avg:146.87ms
step:1095/1375 train_time:159359ms step_avg:146.87ms
step:1096/1375 train_time:159517ms step_avg:146.88ms
step:1097/1375 train_time:159672ms step_avg:146.89ms
step:1098/1375 train_time:159825ms step_avg:146.90ms
step:1099/1375 train_time:159978ms step_avg:146.90ms
step:1100/1375 train_time:160131ms step_avg:146.91ms
step:1101/1375 train_time:160283ms step_avg:146.91ms
step:1102/1375 train_time:160439ms step_avg:146.92ms
step:1103/1375 train_time:160594ms step_avg:146.93ms
step:1104/1375 train_time:160748ms step_avg:146.94ms
step:1105/1375 train_time:160901ms step_avg:146.94ms
step:1106/1375 train_time:161055ms step_avg:146.95ms
step:1107/1375 train_time:161209ms step_avg:146.95ms
step:1108/1375 train_time:161364ms step_avg:146.96ms
step:1109/1375 train_time:161516ms step_avg:146.97ms
step:1110/1375 train_time:161671ms step_avg:146.97ms
step:1111/1375 train_time:161827ms step_avg:146.98ms
step:1112/1375 train_time:161982ms step_avg:146.99ms
step:1113/1375 train_time:162136ms step_avg:147.00ms
step:1114/1375 train_time:162294ms step_avg:147.01ms
step:1115/1375 train_time:162451ms step_avg:147.01ms
step:1116/1375 train_time:162601ms step_avg:147.02ms
step:1117/1375 train_time:162758ms step_avg:147.03ms
step:1118/1375 train_time:162916ms step_avg:147.04ms
step:1119/1375 train_time:163071ms step_avg:147.04ms
step:1120/1375 train_time:163224ms step_avg:147.05ms
step:1121/1375 train_time:163377ms step_avg:147.05ms
step:1122/1375 train_time:163533ms step_avg:147.06ms
step:1123/1375 train_time:163689ms step_avg:147.07ms
step:1124/1375 train_time:163848ms step_avg:147.08ms
step:1125/1375 train_time:164005ms step_avg:147.09ms
step:1125/1375 val_loss:3.3482 train_time:164082ms step_avg:147.16ms
step:1126/1375 train_time:164159ms step_avg:147.10ms
step:1127/1375 train_time:164317ms step_avg:147.11ms
step:1128/1375 train_time:164473ms step_avg:147.11ms
step:1129/1375 train_time:164633ms step_avg:147.13ms
step:1130/1375 train_time:164787ms step_avg:147.13ms
step:1131/1375 train_time:164944ms step_avg:147.14ms
step:1132/1375 train_time:165097ms step_avg:147.15ms
step:1133/1375 train_time:165253ms step_avg:147.15ms
step:1134/1375 train_time:165408ms step_avg:147.16ms
step:1135/1375 train_time:165561ms step_avg:147.17ms
step:1136/1375 train_time:165720ms step_avg:147.18ms
step:1137/1375 train_time:165873ms step_avg:147.18ms
step:1138/1375 train_time:166028ms step_avg:147.19ms
step:1139/1375 train_time:166182ms step_avg:147.19ms
step:1140/1375 train_time:166336ms step_avg:147.20ms
step:1141/1375 train_time:166536ms step_avg:147.25ms
step:1142/1375 train_time:166691ms step_avg:147.25ms
step:1143/1375 train_time:166848ms step_avg:147.26ms
step:1144/1375 train_time:167004ms step_avg:147.27ms
step:1145/1375 train_time:167155ms step_avg:147.27ms
step:1146/1375 train_time:167311ms step_avg:147.28ms
step:1147/1375 train_time:167469ms step_avg:147.29ms
step:1148/1375 train_time:167624ms step_avg:147.30ms
step:1149/1375 train_time:167780ms step_avg:147.30ms
step:1150/1375 train_time:167933ms step_avg:147.31ms
step:1151/1375 train_time:168089ms step_avg:147.32ms
step:1152/1375 train_time:168245ms step_avg:147.32ms
step:1153/1375 train_time:168402ms step_avg:147.33ms
step:1154/1375 train_time:168557ms step_avg:147.34ms
step:1155/1375 train_time:168713ms step_avg:147.35ms
step:1156/1375 train_time:168873ms step_avg:147.36ms
step:1157/1375 train_time:169029ms step_avg:147.37ms
step:1158/1375 train_time:169185ms step_avg:147.37ms
step:1159/1375 train_time:169339ms step_avg:147.38ms
step:1160/1375 train_time:169493ms step_avg:147.38ms
step:1161/1375 train_time:169648ms step_avg:147.39ms
step:1162/1375 train_time:169805ms step_avg:147.40ms
step:1163/1375 train_time:169959ms step_avg:147.41ms
step:1164/1375 train_time:170114ms step_avg:147.41ms
step:1165/1375 train_time:170268ms step_avg:147.42ms
step:1166/1375 train_time:170423ms step_avg:147.42ms
step:1167/1375 train_time:170576ms step_avg:147.43ms
step:1168/1375 train_time:170731ms step_avg:147.44ms
step:1169/1375 train_time:170889ms step_avg:147.45ms
step:1170/1375 train_time:171044ms step_avg:147.45ms
step:1171/1375 train_time:171201ms step_avg:147.46ms
step:1172/1375 train_time:171354ms step_avg:147.47ms
step:1173/1375 train_time:171511ms step_avg:147.47ms
step:1174/1375 train_time:171672ms step_avg:147.48ms
step:1175/1375 train_time:171829ms step_avg:147.49ms
step:1176/1375 train_time:171986ms step_avg:147.50ms
step:1177/1375 train_time:172149ms step_avg:147.51ms
step:1178/1375 train_time:172304ms step_avg:147.52ms
step:1179/1375 train_time:172460ms step_avg:147.53ms
step:1180/1375 train_time:172623ms step_avg:147.54ms
step:1181/1375 train_time:172778ms step_avg:147.55ms
step:1182/1375 train_time:172933ms step_avg:147.55ms
step:1183/1375 train_time:173087ms step_avg:147.56ms
step:1184/1375 train_time:173244ms step_avg:147.57ms
step:1185/1375 train_time:173401ms step_avg:147.58ms
step:1186/1375 train_time:173557ms step_avg:147.58ms
step:1187/1375 train_time:173720ms step_avg:147.60ms
step:1188/1375 train_time:173873ms step_avg:147.60ms
step:1189/1375 train_time:174031ms step_avg:147.61ms
step:1190/1375 train_time:174188ms step_avg:147.62ms
step:1191/1375 train_time:174346ms step_avg:147.63ms
step:1192/1375 train_time:174499ms step_avg:147.63ms
step:1193/1375 train_time:174654ms step_avg:147.64ms
step:1194/1375 train_time:174809ms step_avg:147.64ms
step:1195/1375 train_time:174963ms step_avg:147.65ms
step:1196/1375 train_time:175119ms step_avg:147.66ms
step:1197/1375 train_time:175277ms step_avg:147.66ms
step:1198/1375 train_time:175436ms step_avg:147.67ms
step:1199/1375 train_time:175591ms step_avg:147.68ms
step:1200/1375 train_time:175744ms step_avg:147.68ms
step:1201/1375 train_time:175899ms step_avg:147.69ms
step:1202/1375 train_time:176068ms step_avg:147.71ms
step:1203/1375 train_time:176227ms step_avg:147.72ms
step:1204/1375 train_time:176384ms step_avg:147.73ms
step:1205/1375 train_time:176540ms step_avg:147.73ms
step:1206/1375 train_time:176696ms step_avg:147.74ms
step:1207/1375 train_time:176852ms step_avg:147.75ms
step:1208/1375 train_time:177010ms step_avg:147.75ms
step:1209/1375 train_time:177165ms step_avg:147.76ms
step:1210/1375 train_time:177326ms step_avg:147.77ms
step:1211/1375 train_time:177482ms step_avg:147.78ms
step:1212/1375 train_time:177637ms step_avg:147.78ms
step:1213/1375 train_time:177793ms step_avg:147.79ms
step:1214/1375 train_time:177948ms step_avg:147.80ms
step:1215/1375 train_time:178102ms step_avg:147.80ms
step:1216/1375 train_time:178254ms step_avg:147.81ms
step:1217/1375 train_time:178411ms step_avg:147.81ms
step:1218/1375 train_time:178563ms step_avg:147.82ms
step:1219/1375 train_time:178718ms step_avg:147.82ms
step:1220/1375 train_time:178871ms step_avg:147.83ms
step:1221/1375 train_time:179024ms step_avg:147.83ms
step:1222/1375 train_time:179177ms step_avg:147.84ms
step:1223/1375 train_time:179335ms step_avg:147.84ms
step:1224/1375 train_time:179494ms step_avg:147.85ms
step:1225/1375 train_time:179650ms step_avg:147.86ms
step:1226/1375 train_time:179806ms step_avg:147.87ms
step:1227/1375 train_time:179966ms step_avg:147.88ms
step:1228/1375 train_time:180121ms step_avg:147.88ms
step:1229/1375 train_time:180275ms step_avg:147.89ms
step:1230/1375 train_time:180436ms step_avg:147.90ms
step:1231/1375 train_time:180595ms step_avg:147.91ms
step:1232/1375 train_time:180756ms step_avg:147.92ms
step:1233/1375 train_time:180914ms step_avg:147.93ms
step:1234/1375 train_time:181069ms step_avg:147.93ms
step:1235/1375 train_time:181225ms step_avg:147.94ms
step:1236/1375 train_time:181377ms step_avg:147.94ms
step:1237/1375 train_time:181533ms step_avg:147.95ms
step:1238/1375 train_time:181695ms step_avg:147.96ms
step:1239/1375 train_time:181851ms step_avg:147.97ms
step:1240/1375 train_time:182009ms step_avg:147.97ms
step:1241/1375 train_time:182171ms step_avg:147.99ms
step:1242/1375 train_time:182328ms step_avg:147.99ms
step:1243/1375 train_time:182487ms step_avg:148.00ms
step:1244/1375 train_time:182640ms step_avg:148.01ms
step:1245/1375 train_time:182796ms step_avg:148.01ms
step:1246/1375 train_time:182950ms step_avg:148.02ms
step:1247/1375 train_time:183106ms step_avg:148.02ms
step:1248/1375 train_time:183260ms step_avg:148.03ms
step:1249/1375 train_time:183416ms step_avg:148.04ms
step:1250/1375 train_time:183571ms step_avg:148.04ms
step:1250/1375 val_loss:3.3029 train_time:183651ms step_avg:148.11ms
step:1251/1375 train_time:183731ms step_avg:148.05ms
step:1252/1375 train_time:183886ms step_avg:148.06ms
step:1253/1375 train_time:184041ms step_avg:148.06ms
step:1254/1375 train_time:184193ms step_avg:148.07ms
step:1255/1375 train_time:184362ms step_avg:148.08ms
step:1256/1375 train_time:184518ms step_avg:148.09ms
step:1257/1375 train_time:184675ms step_avg:148.10ms
step:1258/1375 train_time:184834ms step_avg:148.10ms
step:1259/1375 train_time:184991ms step_avg:148.11ms
step:1260/1375 train_time:185143ms step_avg:148.11ms
step:1261/1375 train_time:185300ms step_avg:148.12ms
step:1262/1375 train_time:185458ms step_avg:148.13ms
step:1263/1375 train_time:185616ms step_avg:148.14ms
step:1264/1375 train_time:185770ms step_avg:148.14ms
step:1265/1375 train_time:185925ms step_avg:148.15ms
step:1266/1375 train_time:186083ms step_avg:148.15ms
step:1267/1375 train_time:186240ms step_avg:148.16ms
step:1268/1375 train_time:186395ms step_avg:148.17ms
step:1269/1375 train_time:186555ms step_avg:148.18ms
step:1270/1375 train_time:186711ms step_avg:148.18ms
step:1271/1375 train_time:186869ms step_avg:148.19ms
step:1272/1375 train_time:187024ms step_avg:148.20ms
step:1273/1375 train_time:187177ms step_avg:148.20ms
step:1274/1375 train_time:187332ms step_avg:148.21ms
step:1275/1375 train_time:187487ms step_avg:148.21ms
step:1276/1375 train_time:187640ms step_avg:148.21ms
step:1277/1375 train_time:187797ms step_avg:148.22ms
step:1278/1375 train_time:187953ms step_avg:148.23ms
step:1279/1375 train_time:188108ms step_avg:148.23ms
step:1280/1375 train_time:188270ms step_avg:148.24ms
step:1281/1375 train_time:188425ms step_avg:148.25ms
step:1282/1375 train_time:188578ms step_avg:148.25ms
step:1283/1375 train_time:188736ms step_avg:148.26ms
step:1284/1375 train_time:188895ms step_avg:148.27ms
step:1285/1375 train_time:189049ms step_avg:148.27ms
step:1286/1375 train_time:189206ms step_avg:148.28ms
step:1287/1375 train_time:189363ms step_avg:148.29ms
step:1288/1375 train_time:189519ms step_avg:148.29ms
step:1289/1375 train_time:189680ms step_avg:148.30ms
step:1290/1375 train_time:189839ms step_avg:148.31ms
step:1291/1375 train_time:190001ms step_avg:148.32ms
step:1292/1375 train_time:190160ms step_avg:148.33ms
step:1293/1375 train_time:190319ms step_avg:148.34ms
step:1294/1375 train_time:190475ms step_avg:148.35ms
step:1295/1375 train_time:190633ms step_avg:148.35ms
step:1296/1375 train_time:190789ms step_avg:148.36ms
step:1297/1375 train_time:190949ms step_avg:148.37ms
step:1298/1375 train_time:191104ms step_avg:148.37ms
step:1299/1375 train_time:191258ms step_avg:148.38ms
step:1300/1375 train_time:191412ms step_avg:148.38ms
step:1301/1375 train_time:191566ms step_avg:148.39ms
step:1302/1375 train_time:191724ms step_avg:148.39ms
step:1303/1375 train_time:191882ms step_avg:148.40ms
step:1304/1375 train_time:192041ms step_avg:148.41ms
step:1305/1375 train_time:192196ms step_avg:148.41ms
step:1306/1375 train_time:192354ms step_avg:148.42ms
step:1307/1375 train_time:192509ms step_avg:148.43ms
step:1308/1375 train_time:192666ms step_avg:148.43ms
step:1309/1375 train_time:192824ms step_avg:148.44ms
step:1310/1375 train_time:192977ms step_avg:148.44ms
step:1311/1375 train_time:193132ms step_avg:148.45ms
step:1312/1375 train_time:193285ms step_avg:148.45ms
step:1313/1375 train_time:193443ms step_avg:148.46ms
step:1314/1375 train_time:193600ms step_avg:148.47ms
step:1315/1375 train_time:193756ms step_avg:148.47ms
step:1316/1375 train_time:193908ms step_avg:148.48ms
step:1317/1375 train_time:194064ms step_avg:148.48ms
step:1318/1375 train_time:194224ms step_avg:148.49ms
step:1319/1375 train_time:194380ms step_avg:148.49ms
step:1320/1375 train_time:194535ms step_avg:148.50ms
step:1321/1375 train_time:194692ms step_avg:148.51ms
step:1322/1375 train_time:194853ms step_avg:148.52ms
step:1323/1375 train_time:195008ms step_avg:148.52ms
step:1324/1375 train_time:195164ms step_avg:148.53ms
step:1325/1375 train_time:195319ms step_avg:148.53ms
step:1326/1375 train_time:195478ms step_avg:148.54ms
step:1327/1375 train_time:195633ms step_avg:148.54ms
step:1328/1375 train_time:195789ms step_avg:148.55ms
step:1329/1375 train_time:195965ms step_avg:148.57ms
step:1330/1375 train_time:196124ms step_avg:148.58ms
step:1331/1375 train_time:196324ms step_avg:148.62ms
step:1332/1375 train_time:196484ms step_avg:148.63ms
step:1333/1375 train_time:196643ms step_avg:148.63ms
step:1334/1375 train_time:196799ms step_avg:148.64ms
step:1335/1375 train_time:196952ms step_avg:148.64ms
step:1336/1375 train_time:197114ms step_avg:148.65ms
step:1337/1375 train_time:197273ms step_avg:148.66ms
step:1338/1375 train_time:197429ms step_avg:148.67ms
step:1339/1375 train_time:197588ms step_avg:148.67ms
step:1340/1375 train_time:197749ms step_avg:148.68ms
step:1341/1375 train_time:197903ms step_avg:148.69ms
step:1342/1375 train_time:198064ms step_avg:148.70ms
step:1343/1375 train_time:198219ms step_avg:148.70ms
step:1344/1375 train_time:198375ms step_avg:148.71ms
step:1345/1375 train_time:198532ms step_avg:148.71ms
step:1346/1375 train_time:198688ms step_avg:148.72ms
step:1347/1375 train_time:198845ms step_avg:148.72ms
step:1348/1375 train_time:199000ms step_avg:148.73ms
step:1349/1375 train_time:199158ms step_avg:148.74ms
step:1350/1375 train_time:199310ms step_avg:148.74ms
step:1351/1375 train_time:199466ms step_avg:148.74ms
step:1352/1375 train_time:199629ms step_avg:148.75ms
step:1353/1375 train_time:199790ms step_avg:148.76ms
step:1354/1375 train_time:199948ms step_avg:148.77ms
step:1355/1375 train_time:200104ms step_avg:148.78ms
step:1356/1375 train_time:200259ms step_avg:148.78ms
step:1357/1375 train_time:200419ms step_avg:148.79ms
step:1358/1375 train_time:200578ms step_avg:148.80ms
step:1359/1375 train_time:200735ms step_avg:148.80ms
step:1360/1375 train_time:200895ms step_avg:148.81ms
step:1361/1375 train_time:201054ms step_avg:148.82ms
step:1362/1375 train_time:201213ms step_avg:148.83ms
step:1363/1375 train_time:201375ms step_avg:148.84ms
step:1364/1375 train_time:201529ms step_avg:148.84ms
step:1365/1375 train_time:201682ms step_avg:148.84ms
step:1366/1375 train_time:201840ms step_avg:148.85ms
step:1367/1375 train_time:201998ms step_avg:148.86ms
step:1368/1375 train_time:202157ms step_avg:148.86ms
step:1369/1375 train_time:202322ms step_avg:148.88ms
step:1370/1375 train_time:202481ms step_avg:148.88ms
step:1371/1375 train_time:202638ms step_avg:148.89ms
step:1372/1375 train_time:202800ms step_avg:148.90ms
step:1373/1375 train_time:202955ms step_avg:148.90ms
step:1374/1375 train_time:203113ms step_avg:148.91ms
step:1375/1375 train_time:203269ms step_avg:148.92ms
step:1375/1375 val_loss:3.2775 train_time:203346ms step_avg:148.97ms
peak memory consumption: 31565 MiB

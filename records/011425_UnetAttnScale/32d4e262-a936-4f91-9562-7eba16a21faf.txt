import os
import sys
with open(sys.argv[0]) as f:
    code = f.read() # read the code of this file ASAP, for logging
import uuid
import time
import glob
import subprocess
import contextlib
from dataclasses import dataclass

import torch
torch.empty(1, device='cuda', requires_grad=True).backward()
from torch import nn
import torch.nn.functional as F
import torch.distributed as dist
from torch.nn.parallel import DistributedDataParallel as DDP
# use of FlexAttention contributed by @KoszarskyB
from torch.nn.attention.flex_attention import BlockMask, flex_attention

# -----------------------------------------------------------------------------
# Muon optimizer

@torch.compile
def zeropower_via_newtonschulz5(G, steps):
    """
    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a
    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose
    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at
    zero even beyond the point where the iteration no longer converges all the way to one everywhere
    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T
    where S' is diagonal with S_{ii}' ~ Uniform(0.5, 1.5), which turns out not to hurt model
    performance at all relative to UV^T, where USV^T = G is the SVD.
    """
    assert len(G.shape) == 2
    a, b, c = (3.4445, -4.7750,  2.0315)
    X = G.bfloat16()
    if G.size(0) > G.size(1):
        X = X.T

    # Ensure spectral norm is at most 1
    X = X / (X.norm() + 1e-7)
    # Perform the NS iterations
    for _ in range(steps):
        A = X @ X.T
        B = b * A + c * A @ A # adapted from suggestion by @jxbz, @leloykun, and @YouJiacheng
        X = a * X + B @ X
    
    if G.size(0) > G.size(1):
        X = X.T
    return X

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Some warnings:
    - This optimizer assumes that all parameters passed in are 2D.
    - It should not be used for the embedding layer, the final fully connected layer, or any {0,1}-D
    parameters; those should all be optimized by a standard method (e.g., AdamW).
    - To use it with 4D convolutional filters, it works well to just flatten their last 3 dimensions.
    - We believe it is unlikely to work well for training with small batch size.
    - We believe it may not work well for finetuning pretrained models, but we haven't tested this.
    - We have not yet tried this optimizer for training scenarios larger than NanoGPT (124M).

    Arguments:
        lr: The learning rate used by the internal SGD.
        momentum: The momentum used by the internal SGD.
        nesterov: Whether to use Nesterov-style momentum in the internal SGD. (recommended)
        ns_steps: The number of Newton-Schulz iteration steps to use.
    """
    def __init__(self, params, lr=0.02, momentum=0.95, nesterov=True, ns_steps=5):
        self.world_size = int(os.environ['WORLD_SIZE'])
        self.rank = int(os.environ['RANK'])
        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, ns_steps=ns_steps)
        assert all(isinstance(p, torch.Tensor) for p in params)
        sizes = {p.numel() for p in params}
        param_groups = [dict(params=[p for p in params if p.numel() == size],
                             update_buffer=[torch.empty(size, device='cuda', dtype=torch.bfloat16) for _ in range(self.world_size)])
                        for size in sizes]
        super().__init__(param_groups, defaults)

    def step(self):

        for group in self.param_groups:

            lr = group['lr']
            momentum = group['momentum']
            nesterov = group['nesterov']
            ns_steps = group['ns_steps']
            update_buffers = group['update_buffer']
            # generate weight updates in distributed fashion
            params = group['params']
            handle = None
            params_world = None
            def update_prev():
                if params_world is None:
                    return
                assert handle is not None
                handle.wait()
                for p_world, g_world in zip(params_world, update_buffers):
                    p_world.data.add_(
                        g_world.view_as(p_world),
                        alpha=-lr * max(1, p_world.size(0) / p_world.size(1)) ** 0.5,
                    )
            for base_i in range(len(params))[::self.world_size]:
                if base_i + rank < len(params):
                    p = params[base_i + self.rank]
                    g = p.grad
                    assert g is not None
                    state = self.state[p]
                    if 'momentum_buffer' not in state:
                        state['momentum_buffer'] = torch.zeros_like(g)
                    buf = state['momentum_buffer']
                    buf.lerp_(g, 1 - momentum)
                    g = g.lerp_(buf, momentum) if nesterov else buf
                    g = zeropower_via_newtonschulz5(g, steps=ns_steps).flatten()
                else:
                    g = update_buffers[rank]
                update_prev() # async all_gather instead of sync all_reduce by @YouJiacheng
                handle = dist.all_gather(update_buffers, g, async_op=True)
                params_world = params[base_i : base_i + self.world_size]
            update_prev()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the GPT-2 model

def norm(x):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):

    def __init__(self, in_features, out_features):
        super().__init__(in_features, out_features, bias=False)

    def forward(self, x):
        return F.linear(x, self.weight.type_as(x))

class Rotary(nn.Module):

    def __init__(self, dim, max_seq_len=65536):
        super().__init__()
        # half-truncate RoPE by @YouJiacheng
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=dim//4, dtype=torch.float32)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(dim//4)])
        t = torch.arange(max_seq_len, dtype=torch.float32)
        theta = torch.einsum('i,j -> ij', t, angular_freq)
        self.cos = nn.Buffer(theta.cos(), persistent=False)
        self.sin = nn.Buffer(theta.sin(), persistent=False)

    def forward(self, x):
        cos, sin = self.cos[None, :x.size(-3), None, :], self.sin[None, :x.size(-3), None, :]
        x1, x2 = x.float().chunk(2, dim=-1)
        y1 = x1 * cos + x2 * sin
        y2 = x1 * (-sin) + x2 * cos
        return torch.cat((y1, y2), 3).type_as(x)

class CausalSelfAttention(nn.Module):

    def __init__(self, dim, num_heads, layer_id):
        super().__init__()
        assert dim % num_heads == 0
        self.num_heads = num_heads
        self.layer_id = layer_id
        self.c_q = CastedLinear(dim, dim)
        self.c_k = CastedLinear(dim, dim)
        self.c_v = CastedLinear(dim, dim)
        self.lambdas = nn.Parameter(torch.tensor([0.5, 0.5]))
        self.rotary = Rotary(dim // num_heads) # dim // num_heads = head_dim
        self.c_proj = CastedLinear(dim, dim)
        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977
        # self.attn_scale = nn.Parameter(torch.tensor(1.0 / (dim // num_heads) ** 0.5))
        # self.attn_scale = nn.Parameter(torch.tensor(0.20))
        self.attn_scale = 0.13 + 0.01 * min(layer_id, 11 - layer_id)

    def forward(self, x, ve, block_mask):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, 'Must use batch size = 1 for FlexAttention'
        q = self.c_q(x).view(B, T, self.num_heads, -1)
        k = self.c_k(x).view(B, T, self.num_heads, -1)
        v = self.c_v(x).view(B, T, self.num_heads, -1)
        if ve is not None:
            v = self.lambdas[0] * v + self.lambdas[1] * ve.view_as(v) # @KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = self.lambdas[0] * v
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = self.rotary(q), self.rotary(k)
        # y = flex_attention(q.transpose(1, 2) * self.attn_scale, k.transpose(1, 2), v.transpose(1, 2), block_mask=block_mask, scale=1.)
        y = flex_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), block_mask=block_mask, scale=self.attn_scale)
        y = y.transpose(1, 2).contiguous().view_as(x) # re-assemble all head outputs side by side
        y = self.c_proj(y)
        return y

class MLP(nn.Module):

    def __init__(self, dim):
        super().__init__()
        self.c_fc = CastedLinear(dim, 4 * dim)
        self.c_proj = CastedLinear(4 * dim, dim)
        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977

    def forward(self, x):
        x = self.c_fc(x)
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = self.c_proj(x)
        return x

class Block(nn.Module):

    def __init__(self, model_dim, num_heads, layer_id, use_attn=True):
        super().__init__()
        self.attn = CausalSelfAttention(model_dim, num_heads, layer_id) if use_attn else None
        self.mlp = MLP(model_dim)
        self.lambdas = nn.Parameter(torch.tensor([1., 0.]))

    def forward(self, x, ve, x0, block_mask):
        x = self.lambdas[0] * x + self.lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), ve, block_mask)
        x = x + self.mlp(norm(x))
        return x

class ValueEmbedding(nn.Module):
    def __init__(self, vocab_size, model_dim):
        super().__init__()
        self.embed = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])

    def forward(self, inputs):
        ve = [emb(inputs).bfloat16() for emb in self.embed]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        ve = [ve[0], ve[1], ve[2], None, None, None, None, None, None, ve[0], ve[1], ve[2]]
        return ve

# -----------------------------------------------------------------------------
# The main GPT-2 model

class GPT(nn.Module):

    def __init__(self, vocab_size, num_layers, num_heads, model_dim):
        super().__init__()
        self.embed = nn.Embedding(vocab_size, model_dim)
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.blocks = nn.ModuleList([Block(model_dim, num_heads, layer_id=layer_id, use_attn=(layer_id != 7))
                                     for layer_id in range(num_layers)])
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual learning
        # U-net structure on token value embeddings by @leloykun
        self.value_embeds = ValueEmbedding(vocab_size, model_dim)
        self.lm_head = CastedLinear(model_dim, vocab_size)
        self.lm_head.weight.data.zero_() # @Grad62304977
        # U-net design by @brendanh0gan
        self.num_encoder_layers = num_layers // 2 # Half of the layers for encoder
        self.num_decoder_layers = num_layers - self.num_encoder_layers # Remaining for decoder
        # Add learnable skip connection weights for decoder layers
        self.skip_weights = nn.Parameter(torch.ones(self.num_decoder_layers))

    def forward(self, inputs, targets, sliding_window_num_blocks):
        BLOCK_SIZE = 128
        seq_len = len(inputs)
        assert seq_len % BLOCK_SIZE == 0
        total_num_blocks = seq_len // BLOCK_SIZE
        assert inputs.ndim == 1
        docs = (inputs == 50256).cumsum(0)
        docs_low = docs.view(-1, BLOCK_SIZE)[:, 0].contiguous()
        docs_high = docs.view(-1, BLOCK_SIZE)[:, -1].contiguous()

        def document_causal(b, h, q_idx, kv_idx):
            causal_mask = q_idx >= kv_idx
            document_mask = docs[q_idx] == docs[kv_idx]
            return causal_mask & document_mask

        def dense_to_ordered(dense_mask):
            num_blocks = dense_mask.sum(dim=-1, dtype=torch.int32)
            indices = dense_mask.argsort(dim=-1, descending=True, stable=True).to(torch.int32)
            return num_blocks[None, None].contiguous(), indices[None, None].contiguous()

        def create_doc_swc_block_mask(sliding_window_num_blocks):
            kv_idx = block_idx = torch.arange(total_num_blocks, dtype=torch.int32, device='cuda')
            q_idx = block_idx[:, None]
            causal_bm = q_idx >= kv_idx
            causal_full_bm = q_idx > kv_idx
            window_bm = q_idx - kv_idx < sliding_window_num_blocks
            window_full_bm = window_bm # block-wise sliding window by @YouJiacheng
            # document_bm = (docs_low[q_idx] <= docs_high[kv_idx]) & (docs_low[kv_idx] <= docs_high[q_idx])
            document_bm = (docs_low[:, None] <= docs_high) & (docs_low <= docs_high[:, None])
            document_full_bm = (docs_low[:, None] == docs_high) & (docs_low == docs_high[:, None])
            nonzero_bm = causal_bm & window_bm & document_bm
            full_bm  = causal_full_bm & window_full_bm & document_full_bm
            kv_num_blocks, kv_indices = dense_to_ordered(nonzero_bm & ~full_bm)
            full_kv_num_blocks, full_kv_indices = dense_to_ordered(full_bm)
            return BlockMask.from_kv_blocks(
                kv_num_blocks,
                kv_indices,
                full_kv_num_blocks,
                full_kv_indices,
                BLOCK_SIZE=BLOCK_SIZE,
                mask_mod=document_causal,
            )

        block_mask = create_doc_swc_block_mask(sliding_window_num_blocks)

        x0 = norm(self.embed(inputs[None]).bfloat16()) # use of norm here by @Grad62304977
        x = x0
        ve = self.value_embeds(inputs)
        assert len(ve) == len(self.blocks)
        ve_enc, ve_dec = ve[:self.num_encoder_layers], ve[self.num_encoder_layers:]

        # Store outputs for U-Net skip connections
        skip_connections = []
        # Encoder pass - process only the first half of the blocks
        for i in range(self.num_encoder_layers):
            x = self.blocks[i](x, ve_enc[i], x0, block_mask)
            skip_connections.append(x)
        # Decoder pass - process the remaining blocks with weighted skip connections
        for i in range(self.num_decoder_layers):
            x = x + self.skip_weights[i] * skip_connections.pop()
            # U-net structure on token value embeddings by @leloykun
            x = self.blocks[self.num_encoder_layers + i](x, ve_dec[i], x0, block_mask)

        x = norm(x)
        logits = self.lm_head(x)
        logits = 15 * torch.tanh(logits / 15) # @Grad62304977 added tanh softcapping, @KoszarskyB reduced it from 30 to 15
        logits = logits.float()
        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets)
        return loss

# -----------------------------------------------------------------------------
# Our own simple Distributed Data Loader

def _load_data_shard(path):
    # only reads the header, returns header data
    # header is 256 int32
    header = torch.from_file(path, False, 256, dtype=torch.int32)
    assert header[0] == 20240520, 'magic number mismatch in the data .bin file'
    assert header[1] == 1, 'unsupported version'
    num_tokens = int(header[2]) # number of tokens (claimed)
    with open(path, 'rb', buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, 'number of tokens read does not match header'
    return tokens

class DistributedDataLoader:

    def __init__(self, filename_pattern):
        self.rank = int(os.environ['RANK'])
        self.world_size = int(os.environ['WORLD_SIZE'])
        self.files = sorted(glob.glob(filename_pattern))
        self.reset()

    def reset(self):
        self.current_shard = -1
        self.advance()

    def advance(self):
        self.current_shard = (self.current_shard + 1) % len(self.files)
        self.current_position = 0
        self.tokens = _load_data_shard(self.files[self.current_shard])

    def next_batch(self, batch_size):
        assert batch_size % self.world_size == 0
        device_batch_size = batch_size // self.world_size
        # load next shard if necessary
        if self.current_position + batch_size + 1 >= len(self.tokens):
            self.advance()
        pos = self.current_position + self.rank * device_batch_size
        device_batch_tokens = self.tokens[pos:pos+device_batch_size+1]
        # advance current position
        self.current_position += batch_size
        inputs = device_batch_tokens[:-1].to(device='cuda', dtype=torch.int32, non_blocking=True)
        targets = device_batch_tokens[1:].to(device='cuda', dtype=torch.int64, non_blocking=True)
        return inputs, targets

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files = 'data/fineweb10B/fineweb_train_*.bin' # input .bin to train on
    val_files = 'data/fineweb10B/fineweb_val_*.bin' # input .bin to eval validation loss on
    val_tokens = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # optimization
    batch_size = 8*64*1024 # batch size in tokens
    num_iterations = 1375 # number of iterations to run
    cooldown_frac = 0.4 # fraction of training spent cooling down the learning rate
    bf16_embeds = True
    # evaluation and logging
    val_loss_every = 125 # every how many steps to evaluate val loss? 0 for only at the end
    # implementation
    max_device_batch_size = 64*1024 # batch size per device in tokens
    save_checkpoint = False
args = Hyperparameters()

micro_bs = args.max_device_batch_size

# set up DDP (distributed data parallel). torchrun sets this env variable
rank = int(os.environ['RANK'])
local_rank = int(os.environ['LOCAL_RANK'])
world_size = int(os.environ['WORLD_SIZE'])
assert torch.cuda.is_available()
torch.cuda.set_device(local_rank)
dist.init_process_group(backend='nccl', device_id=torch.device(local_rank))
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = uuid.uuid4()
    os.makedirs('logs', exist_ok=True)
    logfile = f'logs/{run_id}.txt'
    print(logfile)

def print0(s, console=False):
    if master_process:
        with open(logfile, 'a') as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0('='*100)
# log information about the hardware/software environment this is running on
print0(f'Running Python {sys.version}')
print0(f'Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}')
print0(subprocess.run(['nvidia-smi'], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout)
print0('='*100)

# load data
train_loader = DistributedDataLoader(args.train_files)
val_loader = DistributedDataLoader(args.val_files)
print0(f'Training dataloader files: {train_loader.files}')
print0(f'Validation dataloader files: {val_loader.files}')
print0('='*100)

# there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency. suggested to me by @Grad62304977.
# this originates from Karpathy's experiments.
model = GPT(vocab_size=50304, num_layers=12, num_heads=6, model_dim=768)
model = model.cuda()
if args.bf16_embeds:
    for m in model.modules():
        if isinstance(m, nn.Embedding):
            m.bfloat16()
model: GPT = torch.compile(model)
ddp_model = DDP(model, device_ids=[local_rank], broadcast_buffers=False, gradient_as_bucket_view=True)

# collect the parameters to optimize
hidden_matrix_params = [p for p in model.blocks.parameters() if p.ndim == 2]
embed_params = [model.embed.weight, *model.value_embeds.parameters()]
scalar_params = [p for n, p in model.named_parameters() if p.ndim < 2 and "attn_scale" not in n]
attn_scale_params = [p for n, p in model.named_parameters() if p.ndim < 2 and "attn_scale" in n]
head_params = [model.lm_head.weight]

# init the optimizer(s)
optimizer1 = torch.optim.Adam([dict(params=embed_params, lr=0.6),
                               dict(params=head_params, lr=0.008),
                               dict(params=scalar_params, lr=0.04),
                               dict(params=attn_scale_params, lr=0.01)],
                              betas=(0.8, 0.95), fused=True)
optimizer2 = Muon(hidden_matrix_params, lr=0.05, momentum=0.95)
optimizers = [optimizer1, optimizer2]

# learning rate schedule: stable then decay
def get_lr(it):
    t = 1 - it / args.num_iterations # time remaining in training
    assert 1 >= t > 0
    # 1) constant lr for first part of training
    if t >= args.cooldown_frac:
        return 1.0
    # 2) then linear cooldown
    else:
        return t / args.cooldown_frac
schedulers = [torch.optim.lr_scheduler.LambdaLR(opt, get_lr) for opt in optimizers]

# sliding window size schedule: linear increase over training in chunks of 128 from 128 -> 1792. By @fernbear.bsky.social
def get_sliding_window_blocks(it):
    x = it / args.num_iterations # training progress
    assert 0 <= x <= 1
    return int(((1 - x) * 128 + x * 1856) // 128)
sliding_window_num_blocks = torch.tensor(1, dtype=torch.int32, device='cuda')

# Start training loop
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    # This effectively ignores timing first 10 steps, which are slower for weird reasons.
    # Alternately, and slightly more correctly in terms of benchmarking, we could do 10
    # steps with dummy data first, and then re-initialize the model and reset the loader.
    if step == 10:
        training_time_ms = 0
        t0 = time.perf_counter()
    timed_steps = float('nan') if step <= 11 else (step - 10) + 1 # <= 11 to avoid bug in val

    sliding_window_num_blocks.copy_(get_sliding_window_blocks(step))

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        # run validation batches
        model.eval()
        val_loader.reset()
        val_loss = 0.0
        # calculate the number of steps to take in the val loop.
        val_batch_size = world_size * micro_bs
        assert args.val_tokens % val_batch_size == 0
        val_steps = args.val_tokens // val_batch_size
        for _ in range(val_steps):
            with torch.no_grad():
                inputs_val, targets_val = val_loader.next_batch(val_batch_size)
                val_loss += ddp_model(inputs_val, targets_val, sliding_window_num_blocks)
        # Print attention scales
        for n, p in model.named_parameters():
            if p.ndim < 2 and "attn_scale" in n:
                print0(f'{n}: {p.item()}', console=True)
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        val_loss /= val_steps
        # logging
        print0(f'step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/(timed_steps-1):.2f}ms', console=True)
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f'logs/{run_id}', exist_ok=True)
            torch.save(log, f'logs/{run_id}/state_step{step:06d}.pt')
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    model.train()
    batch_size = args.batch_size
    assert batch_size % world_size == 0
    inputs_train, targets_train = train_loader.next_batch(batch_size)
    assert len(inputs_train) <= micro_bs or len(inputs_train) % micro_bs == 0
    for micro_inputs_train, micro_targets_train in zip(inputs_train.split(micro_bs), targets_train.split(micro_bs)):
        ddp_model(micro_inputs_train, micro_targets_train, sliding_window_num_blocks).backward()
    # momentum warmup for Muon
    frac = min(step/300, 1)
    for group in optimizer2.param_groups:
        group['momentum'] = (1 - frac) * 0.85 + frac * 0.95
    # step the optimizers and schedulers
    for opt, sched in zip(optimizers, schedulers):
        opt.step()
        if step != train_steps-1:
            sched.step()
    # null the gradients
    model.zero_grad(set_to_none=True)
    # logging
    approx_time = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f'step:{step+1}/{train_steps} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms', console=True)

print0(f'peak memory consumption: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB')
dist.destroy_process_group()

====================================================================================================
Running Python 3.10.12 (main, Sep 11 2024, 15:47:36) [GCC 11.4.0]
Running PyTorch 2.6.0.dev20241231+cu126 compiled for CUDA 12.6
Tue Jan 14 14:35:42 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.183.06             Driver Version: 535.183.06   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:19:00.0 Off |                    0 |
| N/A   30C    P0             120W / 700W |   7713MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:3B:00.0 Off |                    0 |
| N/A   29C    P0             112W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:4C:00.0 Off |                    0 |
| N/A   26C    P0             116W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   29C    P0             122W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:9B:00.0 Off |                    0 |
| N/A   31C    P0             119W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:BB:00.0 Off |                    0 |
| N/A   27C    P0             112W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:CB:00.0 Off |                    0 |
| N/A   29C    P0             111W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:DB:00.0 Off |                    0 |
| N/A   27C    P0             115W / 700W |   3211MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
Training dataloader files: ['data/fineweb10B/fineweb_train_000001.bin', 'data/fineweb10B/fineweb_train_000002.bin', 'data/fineweb10B/fineweb_train_000003.bin', 'data/fineweb10B/fineweb_train_000004.bin', 'data/fineweb10B/fineweb_train_000005.bin', 'data/fineweb10B/fineweb_train_000006.bin', 'data/fineweb10B/fineweb_train_000007.bin', 'data/fineweb10B/fineweb_train_000008.bin', 'data/fineweb10B/fineweb_train_000009.bin']
Validation dataloader files: ['data/fineweb10B/fineweb_val_000000.bin']
====================================================================================================
step:0/1375 val_loss:10.8258 train_time:0ms step_avg:nanms
step:1/1375 train_time:27463ms step_avg:nanms
step:2/1375 train_time:27536ms step_avg:nanms
step:3/1375 train_time:27721ms step_avg:nanms
step:4/1375 train_time:27853ms step_avg:nanms
step:5/1375 train_time:27988ms step_avg:nanms
step:6/1375 train_time:28121ms step_avg:nanms
step:7/1375 train_time:28253ms step_avg:nanms
step:8/1375 train_time:28388ms step_avg:nanms
step:9/1375 train_time:28520ms step_avg:nanms
step:10/1375 train_time:28661ms step_avg:nanms
step:11/1375 train_time:137ms step_avg:nanms
step:12/1375 train_time:273ms step_avg:nanms
step:13/1375 train_time:406ms step_avg:135.30ms
step:14/1375 train_time:541ms step_avg:135.36ms
step:15/1375 train_time:674ms step_avg:134.86ms
step:16/1375 train_time:809ms step_avg:134.79ms
step:17/1375 train_time:942ms step_avg:134.64ms
step:18/1375 train_time:1080ms step_avg:134.94ms
step:19/1375 train_time:1215ms step_avg:135.01ms
step:20/1375 train_time:1351ms step_avg:135.07ms
step:21/1375 train_time:1484ms step_avg:134.91ms
step:22/1375 train_time:1619ms step_avg:134.89ms
step:23/1375 train_time:1754ms step_avg:134.90ms
step:24/1375 train_time:1889ms step_avg:134.95ms
step:25/1375 train_time:2024ms step_avg:134.91ms
step:26/1375 train_time:2158ms step_avg:134.90ms
step:27/1375 train_time:2293ms step_avg:134.85ms
step:28/1375 train_time:2428ms step_avg:134.89ms
step:29/1375 train_time:2562ms step_avg:134.85ms
step:30/1375 train_time:2697ms step_avg:134.85ms
step:31/1375 train_time:2832ms step_avg:134.86ms
step:32/1375 train_time:2968ms step_avg:134.92ms
step:33/1375 train_time:3104ms step_avg:134.94ms
step:34/1375 train_time:3238ms step_avg:134.94ms
step:35/1375 train_time:3374ms step_avg:134.95ms
step:36/1375 train_time:3508ms step_avg:134.93ms
step:37/1375 train_time:3643ms step_avg:134.93ms
step:38/1375 train_time:3778ms step_avg:134.92ms
step:39/1375 train_time:3913ms step_avg:134.94ms
step:40/1375 train_time:4048ms step_avg:134.94ms
step:41/1375 train_time:4182ms step_avg:134.91ms
step:42/1375 train_time:4317ms step_avg:134.89ms
step:43/1375 train_time:4455ms step_avg:134.99ms
step:44/1375 train_time:4590ms step_avg:134.99ms
step:45/1375 train_time:4724ms step_avg:134.97ms
step:46/1375 train_time:4859ms step_avg:134.96ms
step:47/1375 train_time:4994ms step_avg:134.98ms
step:48/1375 train_time:5131ms step_avg:135.02ms
step:49/1375 train_time:5264ms step_avg:134.99ms
step:50/1375 train_time:5400ms step_avg:135.00ms
step:51/1375 train_time:5537ms step_avg:135.04ms
step:52/1375 train_time:5673ms step_avg:135.07ms
step:53/1375 train_time:5807ms step_avg:135.05ms
step:54/1375 train_time:5942ms step_avg:135.04ms
step:55/1375 train_time:6076ms step_avg:135.02ms
step:56/1375 train_time:6212ms step_avg:135.05ms
step:57/1375 train_time:6346ms step_avg:135.03ms
step:58/1375 train_time:6481ms step_avg:135.01ms
step:59/1375 train_time:6616ms step_avg:135.01ms
step:60/1375 train_time:6752ms step_avg:135.04ms
step:61/1375 train_time:6886ms step_avg:135.01ms
step:62/1375 train_time:7021ms step_avg:135.01ms
step:63/1375 train_time:7156ms step_avg:135.02ms
step:64/1375 train_time:7290ms step_avg:135.00ms
step:65/1375 train_time:7425ms step_avg:135.00ms
step:66/1375 train_time:7560ms step_avg:134.99ms
step:67/1375 train_time:7694ms step_avg:134.99ms
step:68/1375 train_time:7830ms step_avg:135.01ms
step:69/1375 train_time:7965ms step_avg:134.99ms
step:70/1375 train_time:8100ms step_avg:135.01ms
step:71/1375 train_time:8235ms step_avg:135.00ms
step:72/1375 train_time:8371ms step_avg:135.02ms
step:73/1375 train_time:8507ms step_avg:135.02ms
step:74/1375 train_time:8640ms step_avg:135.00ms
step:75/1375 train_time:8775ms step_avg:135.00ms
step:76/1375 train_time:8913ms step_avg:135.05ms
step:77/1375 train_time:9047ms step_avg:135.02ms
step:78/1375 train_time:9180ms step_avg:135.00ms
step:79/1375 train_time:9316ms step_avg:135.02ms
step:80/1375 train_time:9454ms step_avg:135.05ms
step:81/1375 train_time:9589ms step_avg:135.05ms
step:82/1375 train_time:9724ms step_avg:135.05ms
step:83/1375 train_time:9859ms step_avg:135.06ms
step:84/1375 train_time:9995ms step_avg:135.06ms
step:85/1375 train_time:10131ms step_avg:135.08ms
step:86/1375 train_time:10265ms step_avg:135.07ms
step:87/1375 train_time:10401ms step_avg:135.08ms
step:88/1375 train_time:10535ms step_avg:135.07ms
step:89/1375 train_time:10672ms step_avg:135.09ms
step:90/1375 train_time:10807ms step_avg:135.09ms
step:91/1375 train_time:10942ms step_avg:135.09ms
step:92/1375 train_time:11078ms step_avg:135.09ms
step:93/1375 train_time:11214ms step_avg:135.11ms
step:94/1375 train_time:11350ms step_avg:135.12ms
step:95/1375 train_time:11484ms step_avg:135.11ms
step:96/1375 train_time:11619ms step_avg:135.11ms
step:97/1375 train_time:11756ms step_avg:135.12ms
step:98/1375 train_time:11891ms step_avg:135.13ms
step:99/1375 train_time:12027ms step_avg:135.13ms
step:100/1375 train_time:12161ms step_avg:135.13ms
step:101/1375 train_time:12297ms step_avg:135.13ms
step:102/1375 train_time:12433ms step_avg:135.14ms
step:103/1375 train_time:12568ms step_avg:135.14ms
step:104/1375 train_time:12707ms step_avg:135.18ms
step:105/1375 train_time:12845ms step_avg:135.21ms
step:106/1375 train_time:12985ms step_avg:135.26ms
step:107/1375 train_time:13123ms step_avg:135.28ms
step:108/1375 train_time:13264ms step_avg:135.34ms
step:109/1375 train_time:13402ms step_avg:135.37ms
step:110/1375 train_time:13541ms step_avg:135.41ms
step:111/1375 train_time:13678ms step_avg:135.43ms
step:112/1375 train_time:13817ms step_avg:135.46ms
step:113/1375 train_time:13957ms step_avg:135.50ms
step:114/1375 train_time:14093ms step_avg:135.51ms
step:115/1375 train_time:14233ms step_avg:135.55ms
step:116/1375 train_time:14372ms step_avg:135.58ms
step:117/1375 train_time:14510ms step_avg:135.60ms
step:118/1375 train_time:14649ms step_avg:135.64ms
step:119/1375 train_time:14786ms step_avg:135.65ms
step:120/1375 train_time:14924ms step_avg:135.68ms
step:121/1375 train_time:15061ms step_avg:135.69ms
step:122/1375 train_time:15199ms step_avg:135.71ms
step:123/1375 train_time:15339ms step_avg:135.74ms
step:124/1375 train_time:15478ms step_avg:135.77ms
step:125/1375 train_time:15617ms step_avg:135.80ms
step:125/1375 val_loss:4.3664 train_time:15685ms step_avg:136.39ms
step:126/1375 train_time:15758ms step_avg:135.84ms
step:127/1375 train_time:15901ms step_avg:135.91ms
step:128/1375 train_time:16040ms step_avg:135.93ms
step:129/1375 train_time:16178ms step_avg:135.95ms
step:130/1375 train_time:16315ms step_avg:135.95ms
step:131/1375 train_time:16451ms step_avg:135.96ms
step:132/1375 train_time:16590ms step_avg:135.98ms
step:133/1375 train_time:16730ms step_avg:136.02ms
step:134/1375 train_time:16872ms step_avg:136.07ms
step:135/1375 train_time:17012ms step_avg:136.09ms
step:136/1375 train_time:17151ms step_avg:136.12ms
step:137/1375 train_time:17289ms step_avg:136.13ms
step:138/1375 train_time:17425ms step_avg:136.13ms
step:139/1375 train_time:17563ms step_avg:136.15ms
step:140/1375 train_time:17703ms step_avg:136.18ms
step:141/1375 train_time:17843ms step_avg:136.21ms
step:142/1375 train_time:17983ms step_avg:136.24ms
step:143/1375 train_time:18123ms step_avg:136.26ms
step:144/1375 train_time:18260ms step_avg:136.27ms
step:145/1375 train_time:18398ms step_avg:136.28ms
step:146/1375 train_time:18536ms step_avg:136.29ms
step:147/1375 train_time:18674ms step_avg:136.31ms
step:148/1375 train_time:18815ms step_avg:136.34ms
step:149/1375 train_time:18952ms step_avg:136.35ms
step:150/1375 train_time:19092ms step_avg:136.37ms
step:151/1375 train_time:19230ms step_avg:136.38ms
step:152/1375 train_time:19370ms step_avg:136.41ms
step:153/1375 train_time:19508ms step_avg:136.42ms
step:154/1375 train_time:19647ms step_avg:136.44ms
step:155/1375 train_time:19785ms step_avg:136.45ms
step:156/1375 train_time:19923ms step_avg:136.46ms
step:157/1375 train_time:20062ms step_avg:136.48ms
step:158/1375 train_time:20202ms step_avg:136.50ms
step:159/1375 train_time:20341ms step_avg:136.52ms
step:160/1375 train_time:20481ms step_avg:136.54ms
step:161/1375 train_time:20620ms step_avg:136.56ms
step:162/1375 train_time:20758ms step_avg:136.57ms
step:163/1375 train_time:20898ms step_avg:136.59ms
step:164/1375 train_time:21037ms step_avg:136.60ms
step:165/1375 train_time:21175ms step_avg:136.61ms
step:166/1375 train_time:21313ms step_avg:136.62ms
step:167/1375 train_time:21453ms step_avg:136.64ms
step:168/1375 train_time:21592ms step_avg:136.66ms
step:169/1375 train_time:21730ms step_avg:136.67ms
step:170/1375 train_time:21869ms step_avg:136.68ms
step:171/1375 train_time:22010ms step_avg:136.71ms
step:172/1375 train_time:22149ms step_avg:136.72ms
step:173/1375 train_time:22288ms step_avg:136.74ms
step:174/1375 train_time:22425ms step_avg:136.74ms
step:175/1375 train_time:22564ms step_avg:136.75ms
step:176/1375 train_time:22703ms step_avg:136.76ms
step:177/1375 train_time:22841ms step_avg:136.77ms
step:178/1375 train_time:22980ms step_avg:136.78ms
step:179/1375 train_time:23119ms step_avg:136.80ms
step:180/1375 train_time:23257ms step_avg:136.81ms
step:181/1375 train_time:23396ms step_avg:136.82ms
step:182/1375 train_time:23535ms step_avg:136.83ms
step:183/1375 train_time:23676ms step_avg:136.85ms
step:184/1375 train_time:23816ms step_avg:136.87ms
step:185/1375 train_time:23955ms step_avg:136.89ms
step:186/1375 train_time:24094ms step_avg:136.90ms
step:187/1375 train_time:24233ms step_avg:136.91ms
step:188/1375 train_time:24372ms step_avg:136.92ms
step:189/1375 train_time:24511ms step_avg:136.93ms
step:190/1375 train_time:24650ms step_avg:136.94ms
step:191/1375 train_time:24838ms step_avg:137.23ms
step:192/1375 train_time:24975ms step_avg:137.23ms
step:193/1375 train_time:25112ms step_avg:137.23ms
step:194/1375 train_time:25251ms step_avg:137.23ms
step:195/1375 train_time:25388ms step_avg:137.23ms
step:196/1375 train_time:25525ms step_avg:137.23ms
step:197/1375 train_time:25664ms step_avg:137.24ms
step:198/1375 train_time:25809ms step_avg:137.28ms
step:199/1375 train_time:25950ms step_avg:137.30ms
step:200/1375 train_time:26092ms step_avg:137.33ms
step:201/1375 train_time:26230ms step_avg:137.33ms
step:202/1375 train_time:26368ms step_avg:137.33ms
step:203/1375 train_time:26508ms step_avg:137.35ms
step:204/1375 train_time:26647ms step_avg:137.36ms
step:205/1375 train_time:26788ms step_avg:137.37ms
step:206/1375 train_time:26929ms step_avg:137.39ms
step:207/1375 train_time:27071ms step_avg:137.42ms
step:208/1375 train_time:27213ms step_avg:137.44ms
step:209/1375 train_time:27353ms step_avg:137.45ms
step:210/1375 train_time:27494ms step_avg:137.47ms
step:211/1375 train_time:27635ms step_avg:137.49ms
step:212/1375 train_time:27778ms step_avg:137.51ms
step:213/1375 train_time:27921ms step_avg:137.54ms
step:214/1375 train_time:28063ms step_avg:137.56ms
step:215/1375 train_time:28205ms step_avg:137.59ms
step:216/1375 train_time:28348ms step_avg:137.61ms
step:217/1375 train_time:28489ms step_avg:137.63ms
step:218/1375 train_time:28630ms step_avg:137.64ms
step:219/1375 train_time:28772ms step_avg:137.67ms
step:220/1375 train_time:28914ms step_avg:137.68ms
step:221/1375 train_time:29055ms step_avg:137.70ms
step:222/1375 train_time:29197ms step_avg:137.72ms
step:223/1375 train_time:29340ms step_avg:137.75ms
step:224/1375 train_time:29482ms step_avg:137.77ms
step:225/1375 train_time:29624ms step_avg:137.79ms
step:226/1375 train_time:29766ms step_avg:137.81ms
step:227/1375 train_time:29909ms step_avg:137.83ms
step:228/1375 train_time:30050ms step_avg:137.84ms
step:229/1375 train_time:30192ms step_avg:137.86ms
step:230/1375 train_time:30333ms step_avg:137.88ms
step:231/1375 train_time:30475ms step_avg:137.90ms
step:232/1375 train_time:30617ms step_avg:137.92ms
step:233/1375 train_time:30759ms step_avg:137.93ms
step:234/1375 train_time:30900ms step_avg:137.95ms
step:235/1375 train_time:31044ms step_avg:137.97ms
step:236/1375 train_time:31185ms step_avg:137.99ms
step:237/1375 train_time:31327ms step_avg:138.00ms
step:238/1375 train_time:31469ms step_avg:138.02ms
step:239/1375 train_time:31611ms step_avg:138.04ms
step:240/1375 train_time:31753ms step_avg:138.06ms
step:241/1375 train_time:31895ms step_avg:138.07ms
step:242/1375 train_time:32038ms step_avg:138.09ms
step:243/1375 train_time:32182ms step_avg:138.12ms
step:244/1375 train_time:32323ms step_avg:138.13ms
step:245/1375 train_time:32464ms step_avg:138.15ms
step:246/1375 train_time:32606ms step_avg:138.16ms
step:247/1375 train_time:32748ms step_avg:138.18ms
step:248/1375 train_time:32890ms step_avg:138.19ms
step:249/1375 train_time:33032ms step_avg:138.21ms
step:250/1375 train_time:33175ms step_avg:138.23ms
step:250/1375 val_loss:3.9542 train_time:33243ms step_avg:138.51ms
step:251/1375 train_time:33317ms step_avg:138.24ms
step:252/1375 train_time:33462ms step_avg:138.27ms
step:253/1375 train_time:33604ms step_avg:138.29ms
step:254/1375 train_time:33744ms step_avg:138.30ms
step:255/1375 train_time:33884ms step_avg:138.30ms
step:256/1375 train_time:34024ms step_avg:138.31ms
step:257/1375 train_time:34166ms step_avg:138.32ms
step:258/1375 train_time:34310ms step_avg:138.35ms
step:259/1375 train_time:34454ms step_avg:138.37ms
step:260/1375 train_time:34598ms step_avg:138.39ms
step:261/1375 train_time:34739ms step_avg:138.40ms
step:262/1375 train_time:34880ms step_avg:138.41ms
step:263/1375 train_time:35021ms step_avg:138.42ms
step:264/1375 train_time:35161ms step_avg:138.43ms
step:265/1375 train_time:35303ms step_avg:138.44ms
step:266/1375 train_time:35447ms step_avg:138.47ms
step:267/1375 train_time:35590ms step_avg:138.48ms
step:268/1375 train_time:35732ms step_avg:138.49ms
step:269/1375 train_time:35875ms step_avg:138.51ms
step:270/1375 train_time:36016ms step_avg:138.52ms
step:271/1375 train_time:36157ms step_avg:138.53ms
step:272/1375 train_time:36299ms step_avg:138.54ms
step:273/1375 train_time:36443ms step_avg:138.57ms
step:274/1375 train_time:36587ms step_avg:138.59ms
step:275/1375 train_time:36729ms step_avg:138.60ms
step:276/1375 train_time:36871ms step_avg:138.61ms
step:277/1375 train_time:37013ms step_avg:138.63ms
step:278/1375 train_time:37154ms step_avg:138.64ms
step:279/1375 train_time:37296ms step_avg:138.65ms
step:280/1375 train_time:37438ms step_avg:138.66ms
step:281/1375 train_time:37580ms step_avg:138.67ms
step:282/1375 train_time:37723ms step_avg:138.69ms
step:283/1375 train_time:37866ms step_avg:138.70ms
step:284/1375 train_time:38009ms step_avg:138.72ms
step:285/1375 train_time:38151ms step_avg:138.73ms
step:286/1375 train_time:38292ms step_avg:138.74ms
step:287/1375 train_time:38434ms step_avg:138.75ms
step:288/1375 train_time:38576ms step_avg:138.76ms
step:289/1375 train_time:38718ms step_avg:138.77ms
step:290/1375 train_time:38860ms step_avg:138.79ms
step:291/1375 train_time:39002ms step_avg:138.80ms
step:292/1375 train_time:39144ms step_avg:138.81ms
step:293/1375 train_time:39286ms step_avg:138.82ms
step:294/1375 train_time:39427ms step_avg:138.83ms
step:295/1375 train_time:39569ms step_avg:138.84ms
step:296/1375 train_time:39711ms step_avg:138.85ms
step:297/1375 train_time:39854ms step_avg:138.86ms
step:298/1375 train_time:39998ms step_avg:138.88ms
step:299/1375 train_time:40137ms step_avg:138.88ms
step:300/1375 train_time:40281ms step_avg:138.90ms
step:301/1375 train_time:40422ms step_avg:138.91ms
step:302/1375 train_time:40564ms step_avg:138.92ms
step:303/1375 train_time:40706ms step_avg:138.93ms
step:304/1375 train_time:40847ms step_avg:138.93ms
step:305/1375 train_time:40991ms step_avg:138.95ms
step:306/1375 train_time:41133ms step_avg:138.96ms
step:307/1375 train_time:41277ms step_avg:138.98ms
step:308/1375 train_time:41422ms step_avg:139.00ms
step:309/1375 train_time:41564ms step_avg:139.01ms
step:310/1375 train_time:41708ms step_avg:139.03ms
step:311/1375 train_time:41854ms step_avg:139.05ms
step:312/1375 train_time:41998ms step_avg:139.07ms
step:313/1375 train_time:42140ms step_avg:139.07ms
step:314/1375 train_time:42285ms step_avg:139.09ms
step:315/1375 train_time:42427ms step_avg:139.11ms
step:316/1375 train_time:42572ms step_avg:139.12ms
step:317/1375 train_time:42717ms step_avg:139.14ms
step:318/1375 train_time:42860ms step_avg:139.16ms
step:319/1375 train_time:43005ms step_avg:139.17ms
step:320/1375 train_time:43150ms step_avg:139.19ms
step:321/1375 train_time:43294ms step_avg:139.21ms
step:322/1375 train_time:43436ms step_avg:139.22ms
step:323/1375 train_time:43579ms step_avg:139.23ms
step:324/1375 train_time:43722ms step_avg:139.24ms
step:325/1375 train_time:43868ms step_avg:139.26ms
step:326/1375 train_time:44014ms step_avg:139.28ms
step:327/1375 train_time:44157ms step_avg:139.30ms
step:328/1375 train_time:44302ms step_avg:139.32ms
step:329/1375 train_time:44445ms step_avg:139.33ms
step:330/1375 train_time:44588ms step_avg:139.34ms
step:331/1375 train_time:44732ms step_avg:139.35ms
step:332/1375 train_time:44876ms step_avg:139.37ms
step:333/1375 train_time:45020ms step_avg:139.38ms
step:334/1375 train_time:45163ms step_avg:139.39ms
step:335/1375 train_time:45308ms step_avg:139.41ms
step:336/1375 train_time:45453ms step_avg:139.43ms
step:337/1375 train_time:45597ms step_avg:139.44ms
step:338/1375 train_time:45739ms step_avg:139.45ms
step:339/1375 train_time:45883ms step_avg:139.46ms
step:340/1375 train_time:46025ms step_avg:139.47ms
step:341/1375 train_time:46170ms step_avg:139.49ms
step:342/1375 train_time:46315ms step_avg:139.50ms
step:343/1375 train_time:46460ms step_avg:139.52ms
step:344/1375 train_time:46603ms step_avg:139.53ms
step:345/1375 train_time:46748ms step_avg:139.55ms
step:346/1375 train_time:46892ms step_avg:139.56ms
step:347/1375 train_time:47035ms step_avg:139.57ms
step:348/1375 train_time:47179ms step_avg:139.58ms
step:349/1375 train_time:47323ms step_avg:139.60ms
step:350/1375 train_time:47468ms step_avg:139.61ms
step:351/1375 train_time:47612ms step_avg:139.62ms
step:352/1375 train_time:47756ms step_avg:139.64ms
step:353/1375 train_time:47901ms step_avg:139.65ms
step:354/1375 train_time:48043ms step_avg:139.66ms
step:355/1375 train_time:48189ms step_avg:139.68ms
step:356/1375 train_time:48332ms step_avg:139.69ms
step:357/1375 train_time:48476ms step_avg:139.70ms
step:358/1375 train_time:48618ms step_avg:139.71ms
step:359/1375 train_time:48761ms step_avg:139.72ms
step:360/1375 train_time:48908ms step_avg:139.74ms
step:361/1375 train_time:49053ms step_avg:139.75ms
step:362/1375 train_time:49197ms step_avg:139.76ms
step:363/1375 train_time:49339ms step_avg:139.77ms
step:364/1375 train_time:49485ms step_avg:139.79ms
step:365/1375 train_time:49630ms step_avg:139.80ms
step:366/1375 train_time:49774ms step_avg:139.82ms
step:367/1375 train_time:49919ms step_avg:139.83ms
step:368/1375 train_time:50062ms step_avg:139.84ms
step:369/1375 train_time:50206ms step_avg:139.85ms
step:370/1375 train_time:50350ms step_avg:139.86ms
step:371/1375 train_time:50495ms step_avg:139.88ms
step:372/1375 train_time:50637ms step_avg:139.88ms
step:373/1375 train_time:50782ms step_avg:139.90ms
step:374/1375 train_time:50926ms step_avg:139.91ms
step:375/1375 train_time:51070ms step_avg:139.92ms
step:375/1375 val_loss:3.7771 train_time:51141ms step_avg:140.11ms
step:376/1375 train_time:51215ms step_avg:139.93ms
step:377/1375 train_time:51363ms step_avg:139.95ms
step:378/1375 train_time:51508ms step_avg:139.97ms
step:379/1375 train_time:51652ms step_avg:139.98ms
step:380/1375 train_time:51795ms step_avg:139.99ms
step:381/1375 train_time:51987ms step_avg:140.13ms
step:382/1375 train_time:52128ms step_avg:140.13ms
step:383/1375 train_time:52273ms step_avg:140.14ms
step:384/1375 train_time:52416ms step_avg:140.15ms
step:385/1375 train_time:52558ms step_avg:140.15ms
step:386/1375 train_time:52701ms step_avg:140.16ms
step:387/1375 train_time:52846ms step_avg:140.18ms
step:388/1375 train_time:52996ms step_avg:140.20ms
step:389/1375 train_time:53139ms step_avg:140.21ms
step:390/1375 train_time:53284ms step_avg:140.22ms
step:391/1375 train_time:53427ms step_avg:140.23ms
step:392/1375 train_time:53572ms step_avg:140.24ms
step:393/1375 train_time:53715ms step_avg:140.25ms
step:394/1375 train_time:53859ms step_avg:140.26ms
step:395/1375 train_time:54006ms step_avg:140.27ms
step:396/1375 train_time:54150ms step_avg:140.28ms
step:397/1375 train_time:54295ms step_avg:140.30ms
step:398/1375 train_time:54438ms step_avg:140.30ms
step:399/1375 train_time:54582ms step_avg:140.31ms
step:400/1375 train_time:54725ms step_avg:140.32ms
step:401/1375 train_time:54870ms step_avg:140.33ms
step:402/1375 train_time:55014ms step_avg:140.34ms
step:403/1375 train_time:55159ms step_avg:140.35ms
step:404/1375 train_time:55304ms step_avg:140.37ms
step:405/1375 train_time:55449ms step_avg:140.38ms
step:406/1375 train_time:55592ms step_avg:140.38ms
step:407/1375 train_time:55735ms step_avg:140.39ms
step:408/1375 train_time:55879ms step_avg:140.40ms
step:409/1375 train_time:56024ms step_avg:140.41ms
step:410/1375 train_time:56170ms step_avg:140.43ms
step:411/1375 train_time:56316ms step_avg:140.44ms
step:412/1375 train_time:56460ms step_avg:140.45ms
step:413/1375 train_time:56605ms step_avg:140.46ms
step:414/1375 train_time:56750ms step_avg:140.47ms
step:415/1375 train_time:56896ms step_avg:140.48ms
step:416/1375 train_time:57043ms step_avg:140.50ms
step:417/1375 train_time:57189ms step_avg:140.51ms
step:418/1375 train_time:57333ms step_avg:140.52ms
step:419/1375 train_time:57479ms step_avg:140.54ms
step:420/1375 train_time:57625ms step_avg:140.55ms
step:421/1375 train_time:57770ms step_avg:140.56ms
step:422/1375 train_time:57915ms step_avg:140.57ms
step:423/1375 train_time:58061ms step_avg:140.58ms
step:424/1375 train_time:58209ms step_avg:140.60ms
step:425/1375 train_time:58355ms step_avg:140.61ms
step:426/1375 train_time:58500ms step_avg:140.63ms
step:427/1375 train_time:58645ms step_avg:140.64ms
step:428/1375 train_time:58793ms step_avg:140.65ms
step:429/1375 train_time:58938ms step_avg:140.66ms
step:430/1375 train_time:59084ms step_avg:140.68ms
step:431/1375 train_time:59230ms step_avg:140.69ms
step:432/1375 train_time:59376ms step_avg:140.70ms
step:433/1375 train_time:59521ms step_avg:140.71ms
step:434/1375 train_time:59667ms step_avg:140.72ms
step:435/1375 train_time:59813ms step_avg:140.74ms
step:436/1375 train_time:59959ms step_avg:140.75ms
step:437/1375 train_time:60105ms step_avg:140.76ms
step:438/1375 train_time:60252ms step_avg:140.77ms
step:439/1375 train_time:60398ms step_avg:140.79ms
step:440/1375 train_time:60542ms step_avg:140.80ms
step:441/1375 train_time:60688ms step_avg:140.81ms
step:442/1375 train_time:60833ms step_avg:140.82ms
step:443/1375 train_time:60981ms step_avg:140.83ms
step:444/1375 train_time:61126ms step_avg:140.84ms
step:445/1375 train_time:61272ms step_avg:140.86ms
step:446/1375 train_time:61419ms step_avg:140.87ms
step:447/1375 train_time:61563ms step_avg:140.88ms
step:448/1375 train_time:61709ms step_avg:140.89ms
step:449/1375 train_time:61854ms step_avg:140.90ms
step:450/1375 train_time:62001ms step_avg:140.91ms
step:451/1375 train_time:62148ms step_avg:140.93ms
step:452/1375 train_time:62294ms step_avg:140.94ms
step:453/1375 train_time:62439ms step_avg:140.95ms
step:454/1375 train_time:62585ms step_avg:140.96ms
step:455/1375 train_time:62730ms step_avg:140.97ms
step:456/1375 train_time:62876ms step_avg:140.98ms
step:457/1375 train_time:63021ms step_avg:140.99ms
step:458/1375 train_time:63168ms step_avg:141.00ms
step:459/1375 train_time:63313ms step_avg:141.01ms
step:460/1375 train_time:63458ms step_avg:141.02ms
step:461/1375 train_time:63604ms step_avg:141.03ms
step:462/1375 train_time:63752ms step_avg:141.04ms
step:463/1375 train_time:63898ms step_avg:141.05ms
step:464/1375 train_time:64043ms step_avg:141.06ms
step:465/1375 train_time:64191ms step_avg:141.08ms
step:466/1375 train_time:64337ms step_avg:141.09ms
step:467/1375 train_time:64484ms step_avg:141.10ms
step:468/1375 train_time:64628ms step_avg:141.11ms
step:469/1375 train_time:64775ms step_avg:141.12ms
step:470/1375 train_time:64920ms step_avg:141.13ms
step:471/1375 train_time:65066ms step_avg:141.14ms
step:472/1375 train_time:65213ms step_avg:141.15ms
step:473/1375 train_time:65359ms step_avg:141.16ms
step:474/1375 train_time:65505ms step_avg:141.17ms
step:475/1375 train_time:65650ms step_avg:141.18ms
step:476/1375 train_time:65797ms step_avg:141.20ms
step:477/1375 train_time:65943ms step_avg:141.21ms
step:478/1375 train_time:66089ms step_avg:141.22ms
step:479/1375 train_time:66234ms step_avg:141.22ms
step:480/1375 train_time:66381ms step_avg:141.24ms
step:481/1375 train_time:66527ms step_avg:141.25ms
step:482/1375 train_time:66673ms step_avg:141.26ms
step:483/1375 train_time:66818ms step_avg:141.26ms
step:484/1375 train_time:66964ms step_avg:141.28ms
step:485/1375 train_time:67111ms step_avg:141.29ms
step:486/1375 train_time:67258ms step_avg:141.30ms
step:487/1375 train_time:67405ms step_avg:141.31ms
step:488/1375 train_time:67550ms step_avg:141.32ms
step:489/1375 train_time:67695ms step_avg:141.33ms
step:490/1375 train_time:67840ms step_avg:141.33ms
step:491/1375 train_time:67987ms step_avg:141.35ms
step:492/1375 train_time:68132ms step_avg:141.35ms
step:493/1375 train_time:68278ms step_avg:141.36ms
step:494/1375 train_time:68425ms step_avg:141.37ms
step:495/1375 train_time:68571ms step_avg:141.38ms
step:496/1375 train_time:68716ms step_avg:141.39ms
step:497/1375 train_time:68861ms step_avg:141.40ms
step:498/1375 train_time:69006ms step_avg:141.41ms
step:499/1375 train_time:69152ms step_avg:141.42ms
step:500/1375 train_time:69299ms step_avg:141.43ms
step:500/1375 val_loss:3.6583 train_time:69371ms step_avg:141.57ms
step:501/1375 train_time:69446ms step_avg:141.44ms
step:502/1375 train_time:69596ms step_avg:141.45ms
step:503/1375 train_time:69741ms step_avg:141.46ms
step:504/1375 train_time:69887ms step_avg:141.47ms
step:505/1375 train_time:70033ms step_avg:141.48ms
step:506/1375 train_time:70177ms step_avg:141.49ms
step:507/1375 train_time:70322ms step_avg:141.49ms
step:508/1375 train_time:70471ms step_avg:141.51ms
step:509/1375 train_time:70617ms step_avg:141.52ms
step:510/1375 train_time:70764ms step_avg:141.53ms
step:511/1375 train_time:70910ms step_avg:141.54ms
step:512/1375 train_time:71059ms step_avg:141.55ms
step:513/1375 train_time:71206ms step_avg:141.56ms
step:514/1375 train_time:71355ms step_avg:141.58ms
step:515/1375 train_time:71503ms step_avg:141.59ms
step:516/1375 train_time:71653ms step_avg:141.61ms
step:517/1375 train_time:71799ms step_avg:141.62ms
step:518/1375 train_time:71946ms step_avg:141.63ms
step:519/1375 train_time:72096ms step_avg:141.64ms
step:520/1375 train_time:72243ms step_avg:141.65ms
step:521/1375 train_time:72389ms step_avg:141.66ms
step:522/1375 train_time:72537ms step_avg:141.67ms
step:523/1375 train_time:72685ms step_avg:141.69ms
step:524/1375 train_time:72835ms step_avg:141.70ms
step:525/1375 train_time:72981ms step_avg:141.71ms
step:526/1375 train_time:73130ms step_avg:141.72ms
step:527/1375 train_time:73278ms step_avg:141.74ms
step:528/1375 train_time:73424ms step_avg:141.75ms
step:529/1375 train_time:73572ms step_avg:141.76ms
step:530/1375 train_time:73718ms step_avg:141.77ms
step:531/1375 train_time:73867ms step_avg:141.78ms
step:532/1375 train_time:74014ms step_avg:141.79ms
step:533/1375 train_time:74162ms step_avg:141.80ms
step:534/1375 train_time:74308ms step_avg:141.81ms
step:535/1375 train_time:74457ms step_avg:141.82ms
step:536/1375 train_time:74604ms step_avg:141.83ms
step:537/1375 train_time:74751ms step_avg:141.84ms
step:538/1375 train_time:74898ms step_avg:141.85ms
step:539/1375 train_time:75046ms step_avg:141.86ms
step:540/1375 train_time:75193ms step_avg:141.87ms
step:541/1375 train_time:75340ms step_avg:141.88ms
step:542/1375 train_time:75487ms step_avg:141.89ms
step:543/1375 train_time:75635ms step_avg:141.90ms
step:544/1375 train_time:75781ms step_avg:141.91ms
step:545/1375 train_time:75928ms step_avg:141.92ms
step:546/1375 train_time:76078ms step_avg:141.94ms
step:547/1375 train_time:76224ms step_avg:141.94ms
step:548/1375 train_time:76373ms step_avg:141.96ms
step:549/1375 train_time:76519ms step_avg:141.97ms
step:550/1375 train_time:76668ms step_avg:141.98ms
step:551/1375 train_time:76815ms step_avg:141.99ms
step:552/1375 train_time:76964ms step_avg:142.00ms
step:553/1375 train_time:77113ms step_avg:142.01ms
step:554/1375 train_time:77260ms step_avg:142.02ms
step:555/1375 train_time:77408ms step_avg:142.03ms
step:556/1375 train_time:77555ms step_avg:142.04ms
step:557/1375 train_time:77702ms step_avg:142.05ms
step:558/1375 train_time:77848ms step_avg:142.06ms
step:559/1375 train_time:77997ms step_avg:142.07ms
step:560/1375 train_time:78144ms step_avg:142.08ms
step:561/1375 train_time:78292ms step_avg:142.09ms
step:562/1375 train_time:78439ms step_avg:142.10ms
step:563/1375 train_time:78588ms step_avg:142.11ms
step:564/1375 train_time:78736ms step_avg:142.12ms
step:565/1375 train_time:78884ms step_avg:142.13ms
step:566/1375 train_time:79032ms step_avg:142.14ms
step:567/1375 train_time:79180ms step_avg:142.15ms
step:568/1375 train_time:79326ms step_avg:142.16ms
step:569/1375 train_time:79474ms step_avg:142.17ms
step:570/1375 train_time:79619ms step_avg:142.18ms
step:571/1375 train_time:79812ms step_avg:142.27ms
step:572/1375 train_time:79959ms step_avg:142.28ms
step:573/1375 train_time:80105ms step_avg:142.28ms
step:574/1375 train_time:80254ms step_avg:142.29ms
step:575/1375 train_time:80400ms step_avg:142.30ms
step:576/1375 train_time:80546ms step_avg:142.31ms
step:577/1375 train_time:80694ms step_avg:142.32ms
step:578/1375 train_time:80844ms step_avg:142.33ms
step:579/1375 train_time:80992ms step_avg:142.34ms
step:580/1375 train_time:81140ms step_avg:142.35ms
step:581/1375 train_time:81286ms step_avg:142.36ms
step:582/1375 train_time:81434ms step_avg:142.37ms
step:583/1375 train_time:81580ms step_avg:142.37ms
step:584/1375 train_time:81727ms step_avg:142.38ms
step:585/1375 train_time:81876ms step_avg:142.39ms
step:586/1375 train_time:82024ms step_avg:142.40ms
step:587/1375 train_time:82172ms step_avg:142.41ms
step:588/1375 train_time:82319ms step_avg:142.42ms
step:589/1375 train_time:82468ms step_avg:142.43ms
step:590/1375 train_time:82614ms step_avg:142.44ms
step:591/1375 train_time:82761ms step_avg:142.45ms
step:592/1375 train_time:82911ms step_avg:142.46ms
step:593/1375 train_time:83059ms step_avg:142.47ms
step:594/1375 train_time:83206ms step_avg:142.48ms
step:595/1375 train_time:83355ms step_avg:142.49ms
step:596/1375 train_time:83502ms step_avg:142.49ms
step:597/1375 train_time:83648ms step_avg:142.50ms
step:598/1375 train_time:83797ms step_avg:142.51ms
step:599/1375 train_time:83944ms step_avg:142.52ms
step:600/1375 train_time:84092ms step_avg:142.53ms
step:601/1375 train_time:84238ms step_avg:142.54ms
step:602/1375 train_time:84386ms step_avg:142.54ms
step:603/1375 train_time:84534ms step_avg:142.55ms
step:604/1375 train_time:84680ms step_avg:142.56ms
step:605/1375 train_time:84827ms step_avg:142.57ms
step:606/1375 train_time:84975ms step_avg:142.58ms
step:607/1375 train_time:85121ms step_avg:142.58ms
step:608/1375 train_time:85269ms step_avg:142.59ms
step:609/1375 train_time:85416ms step_avg:142.60ms
step:610/1375 train_time:85565ms step_avg:142.61ms
step:611/1375 train_time:85712ms step_avg:142.62ms
step:612/1375 train_time:85859ms step_avg:142.62ms
step:613/1375 train_time:86008ms step_avg:142.63ms
step:614/1375 train_time:86155ms step_avg:142.64ms
step:615/1375 train_time:86304ms step_avg:142.65ms
step:616/1375 train_time:86453ms step_avg:142.66ms
step:617/1375 train_time:86602ms step_avg:142.67ms
step:618/1375 train_time:86750ms step_avg:142.68ms
step:619/1375 train_time:86900ms step_avg:142.69ms
step:620/1375 train_time:87049ms step_avg:142.70ms
step:621/1375 train_time:87198ms step_avg:142.71ms
step:622/1375 train_time:87347ms step_avg:142.72ms
step:623/1375 train_time:87498ms step_avg:142.74ms
step:624/1375 train_time:87647ms step_avg:142.75ms
step:625/1375 train_time:87797ms step_avg:142.76ms
step:625/1375 val_loss:3.5756 train_time:87872ms step_avg:142.88ms
step:626/1375 train_time:87947ms step_avg:142.77ms
step:627/1375 train_time:88098ms step_avg:142.78ms
step:628/1375 train_time:88245ms step_avg:142.79ms
step:629/1375 train_time:88394ms step_avg:142.80ms
step:630/1375 train_time:88541ms step_avg:142.81ms
step:631/1375 train_time:88689ms step_avg:142.82ms
step:632/1375 train_time:88838ms step_avg:142.83ms
step:633/1375 train_time:88986ms step_avg:142.83ms
step:634/1375 train_time:89136ms step_avg:142.85ms
step:635/1375 train_time:89284ms step_avg:142.85ms
step:636/1375 train_time:89434ms step_avg:142.87ms
step:637/1375 train_time:89583ms step_avg:142.87ms
step:638/1375 train_time:89731ms step_avg:142.88ms
step:639/1375 train_time:89880ms step_avg:142.89ms
step:640/1375 train_time:90031ms step_avg:142.91ms
step:641/1375 train_time:90180ms step_avg:142.92ms
step:642/1375 train_time:90331ms step_avg:142.93ms
step:643/1375 train_time:90479ms step_avg:142.94ms
step:644/1375 train_time:90629ms step_avg:142.95ms
step:645/1375 train_time:90777ms step_avg:142.96ms
step:646/1375 train_time:90926ms step_avg:142.97ms
step:647/1375 train_time:91074ms step_avg:142.97ms
step:648/1375 train_time:91226ms step_avg:142.99ms
step:649/1375 train_time:91376ms step_avg:143.00ms
step:650/1375 train_time:91527ms step_avg:143.01ms
step:651/1375 train_time:91677ms step_avg:143.02ms
step:652/1375 train_time:91824ms step_avg:143.03ms
step:653/1375 train_time:91973ms step_avg:143.04ms
step:654/1375 train_time:92123ms step_avg:143.05ms
step:655/1375 train_time:92271ms step_avg:143.06ms
step:656/1375 train_time:92420ms step_avg:143.07ms
step:657/1375 train_time:92570ms step_avg:143.08ms
step:658/1375 train_time:92719ms step_avg:143.09ms
step:659/1375 train_time:92867ms step_avg:143.09ms
step:660/1375 train_time:93015ms step_avg:143.10ms
step:661/1375 train_time:93164ms step_avg:143.11ms
step:662/1375 train_time:93312ms step_avg:143.12ms
step:663/1375 train_time:93461ms step_avg:143.13ms
step:664/1375 train_time:93612ms step_avg:143.14ms
step:665/1375 train_time:93762ms step_avg:143.15ms
step:666/1375 train_time:93909ms step_avg:143.15ms
step:667/1375 train_time:94058ms step_avg:143.16ms
step:668/1375 train_time:94207ms step_avg:143.17ms
step:669/1375 train_time:94358ms step_avg:143.18ms
step:670/1375 train_time:94506ms step_avg:143.19ms
step:671/1375 train_time:94656ms step_avg:143.20ms
step:672/1375 train_time:94805ms step_avg:143.21ms
step:673/1375 train_time:94953ms step_avg:143.22ms
step:674/1375 train_time:95103ms step_avg:143.23ms
step:675/1375 train_time:95252ms step_avg:143.24ms
step:676/1375 train_time:95401ms step_avg:143.24ms
step:677/1375 train_time:95550ms step_avg:143.25ms
step:678/1375 train_time:95700ms step_avg:143.26ms
step:679/1375 train_time:95850ms step_avg:143.27ms
step:680/1375 train_time:95999ms step_avg:143.28ms
step:681/1375 train_time:96147ms step_avg:143.29ms
step:682/1375 train_time:96297ms step_avg:143.30ms
step:683/1375 train_time:96446ms step_avg:143.31ms
step:684/1375 train_time:96595ms step_avg:143.32ms
step:685/1375 train_time:96744ms step_avg:143.32ms
step:686/1375 train_time:96892ms step_avg:143.33ms
step:687/1375 train_time:97041ms step_avg:143.34ms
step:688/1375 train_time:97192ms step_avg:143.35ms
step:689/1375 train_time:97341ms step_avg:143.36ms
step:690/1375 train_time:97491ms step_avg:143.37ms
step:691/1375 train_time:97640ms step_avg:143.38ms
step:692/1375 train_time:97790ms step_avg:143.39ms
step:693/1375 train_time:97940ms step_avg:143.40ms
step:694/1375 train_time:98088ms step_avg:143.40ms
step:695/1375 train_time:98238ms step_avg:143.41ms
step:696/1375 train_time:98385ms step_avg:143.42ms
step:697/1375 train_time:98536ms step_avg:143.43ms
step:698/1375 train_time:98683ms step_avg:143.43ms
step:699/1375 train_time:98832ms step_avg:143.44ms
step:700/1375 train_time:98980ms step_avg:143.45ms
step:701/1375 train_time:99131ms step_avg:143.46ms
step:702/1375 train_time:99280ms step_avg:143.47ms
step:703/1375 train_time:99430ms step_avg:143.48ms
step:704/1375 train_time:99579ms step_avg:143.49ms
step:705/1375 train_time:99729ms step_avg:143.50ms
step:706/1375 train_time:99882ms step_avg:143.51ms
step:707/1375 train_time:100031ms step_avg:143.52ms
step:708/1375 train_time:100179ms step_avg:143.52ms
step:709/1375 train_time:100332ms step_avg:143.54ms
step:710/1375 train_time:100480ms step_avg:143.54ms
step:711/1375 train_time:100633ms step_avg:143.56ms
step:712/1375 train_time:100782ms step_avg:143.56ms
step:713/1375 train_time:100933ms step_avg:143.58ms
step:714/1375 train_time:101082ms step_avg:143.58ms
step:715/1375 train_time:101233ms step_avg:143.59ms
step:716/1375 train_time:101382ms step_avg:143.60ms
step:717/1375 train_time:101533ms step_avg:143.61ms
step:718/1375 train_time:101682ms step_avg:143.62ms
step:719/1375 train_time:101832ms step_avg:143.63ms
step:720/1375 train_time:101981ms step_avg:143.64ms
step:721/1375 train_time:102133ms step_avg:143.65ms
step:722/1375 train_time:102283ms step_avg:143.66ms
step:723/1375 train_time:102433ms step_avg:143.66ms
step:724/1375 train_time:102584ms step_avg:143.67ms
step:725/1375 train_time:102735ms step_avg:143.68ms
step:726/1375 train_time:102883ms step_avg:143.69ms
step:727/1375 train_time:103036ms step_avg:143.70ms
step:728/1375 train_time:103184ms step_avg:143.71ms
step:729/1375 train_time:103336ms step_avg:143.72ms
step:730/1375 train_time:103487ms step_avg:143.73ms
step:731/1375 train_time:103638ms step_avg:143.74ms
step:732/1375 train_time:103787ms step_avg:143.75ms
step:733/1375 train_time:103939ms step_avg:143.76ms
step:734/1375 train_time:104088ms step_avg:143.77ms
step:735/1375 train_time:104240ms step_avg:143.78ms
step:736/1375 train_time:104390ms step_avg:143.79ms
step:737/1375 train_time:104541ms step_avg:143.80ms
step:738/1375 train_time:104690ms step_avg:143.81ms
step:739/1375 train_time:104843ms step_avg:143.82ms
step:740/1375 train_time:104994ms step_avg:143.83ms
step:741/1375 train_time:105145ms step_avg:143.84ms
step:742/1375 train_time:105295ms step_avg:143.85ms
step:743/1375 train_time:105443ms step_avg:143.85ms
step:744/1375 train_time:105593ms step_avg:143.86ms
step:745/1375 train_time:105746ms step_avg:143.87ms
step:746/1375 train_time:105897ms step_avg:143.88ms
step:747/1375 train_time:106046ms step_avg:143.89ms
step:748/1375 train_time:106198ms step_avg:143.90ms
step:749/1375 train_time:106347ms step_avg:143.91ms
step:750/1375 train_time:106499ms step_avg:143.92ms
step:750/1375 val_loss:3.5214 train_time:106574ms step_avg:144.02ms
step:751/1375 train_time:106651ms step_avg:143.93ms
step:752/1375 train_time:106803ms step_avg:143.94ms
step:753/1375 train_time:106953ms step_avg:143.95ms
step:754/1375 train_time:107101ms step_avg:143.95ms
step:755/1375 train_time:107251ms step_avg:143.96ms
step:756/1375 train_time:107399ms step_avg:143.97ms
step:757/1375 train_time:107552ms step_avg:143.98ms
step:758/1375 train_time:107703ms step_avg:143.99ms
step:759/1375 train_time:107853ms step_avg:144.00ms
step:760/1375 train_time:108002ms step_avg:144.00ms
step:761/1375 train_time:108196ms step_avg:144.07ms
step:762/1375 train_time:108344ms step_avg:144.07ms
step:763/1375 train_time:108493ms step_avg:144.08ms
step:764/1375 train_time:108641ms step_avg:144.09ms
step:765/1375 train_time:108791ms step_avg:144.09ms
step:766/1375 train_time:108941ms step_avg:144.10ms
step:767/1375 train_time:109095ms step_avg:144.11ms
step:768/1375 train_time:109245ms step_avg:144.12ms
step:769/1375 train_time:109397ms step_avg:144.13ms
step:770/1375 train_time:109546ms step_avg:144.14ms
step:771/1375 train_time:109697ms step_avg:144.15ms
step:772/1375 train_time:109846ms step_avg:144.15ms
step:773/1375 train_time:109997ms step_avg:144.16ms
step:774/1375 train_time:110147ms step_avg:144.17ms
step:775/1375 train_time:110297ms step_avg:144.18ms
step:776/1375 train_time:110448ms step_avg:144.19ms
step:777/1375 train_time:110599ms step_avg:144.20ms
step:778/1375 train_time:110747ms step_avg:144.20ms
step:779/1375 train_time:110896ms step_avg:144.21ms
step:780/1375 train_time:111047ms step_avg:144.22ms
step:781/1375 train_time:111198ms step_avg:144.23ms
step:782/1375 train_time:111349ms step_avg:144.23ms
step:783/1375 train_time:111498ms step_avg:144.24ms
step:784/1375 train_time:111648ms step_avg:144.25ms
step:785/1375 train_time:111797ms step_avg:144.25ms
step:786/1375 train_time:111947ms step_avg:144.26ms
step:787/1375 train_time:112097ms step_avg:144.27ms
step:788/1375 train_time:112247ms step_avg:144.28ms
step:789/1375 train_time:112396ms step_avg:144.28ms
step:790/1375 train_time:112545ms step_avg:144.29ms
step:791/1375 train_time:112696ms step_avg:144.30ms
step:792/1375 train_time:112846ms step_avg:144.30ms
step:793/1375 train_time:112998ms step_avg:144.31ms
step:794/1375 train_time:113149ms step_avg:144.32ms
step:795/1375 train_time:113301ms step_avg:144.33ms
step:796/1375 train_time:113452ms step_avg:144.34ms
step:797/1375 train_time:113602ms step_avg:144.35ms
step:798/1375 train_time:113754ms step_avg:144.36ms
step:799/1375 train_time:113904ms step_avg:144.36ms
step:800/1375 train_time:114054ms step_avg:144.37ms
step:801/1375 train_time:114202ms step_avg:144.38ms
step:802/1375 train_time:114354ms step_avg:144.39ms
step:803/1375 train_time:114503ms step_avg:144.39ms
step:804/1375 train_time:114652ms step_avg:144.40ms
step:805/1375 train_time:114806ms step_avg:144.41ms
step:806/1375 train_time:114956ms step_avg:144.42ms
step:807/1375 train_time:115103ms step_avg:144.42ms
step:808/1375 train_time:115255ms step_avg:144.43ms
step:809/1375 train_time:115405ms step_avg:144.44ms
step:810/1375 train_time:115556ms step_avg:144.44ms
step:811/1375 train_time:115707ms step_avg:144.45ms
step:812/1375 train_time:115860ms step_avg:144.46ms
step:813/1375 train_time:116008ms step_avg:144.47ms
step:814/1375 train_time:116157ms step_avg:144.47ms
step:815/1375 train_time:116306ms step_avg:144.48ms
step:816/1375 train_time:116459ms step_avg:144.49ms
step:817/1375 train_time:116609ms step_avg:144.50ms
step:818/1375 train_time:116759ms step_avg:144.50ms
step:819/1375 train_time:116913ms step_avg:144.52ms
step:820/1375 train_time:117066ms step_avg:144.53ms
step:821/1375 train_time:117216ms step_avg:144.53ms
step:822/1375 train_time:117365ms step_avg:144.54ms
step:823/1375 train_time:117517ms step_avg:144.55ms
step:824/1375 train_time:117668ms step_avg:144.56ms
step:825/1375 train_time:117820ms step_avg:144.56ms
step:826/1375 train_time:117974ms step_avg:144.58ms
step:827/1375 train_time:118124ms step_avg:144.58ms
step:828/1375 train_time:118274ms step_avg:144.59ms
step:829/1375 train_time:118425ms step_avg:144.60ms
step:830/1375 train_time:118577ms step_avg:144.61ms
step:831/1375 train_time:118729ms step_avg:144.62ms
step:832/1375 train_time:118880ms step_avg:144.62ms
step:833/1375 train_time:119030ms step_avg:144.63ms
step:834/1375 train_time:119181ms step_avg:144.64ms
step:835/1375 train_time:119334ms step_avg:144.65ms
step:836/1375 train_time:119488ms step_avg:144.66ms
step:837/1375 train_time:119638ms step_avg:144.67ms
step:838/1375 train_time:119791ms step_avg:144.68ms
step:839/1375 train_time:119940ms step_avg:144.68ms
step:840/1375 train_time:120092ms step_avg:144.69ms
step:841/1375 train_time:120242ms step_avg:144.70ms
step:842/1375 train_time:120395ms step_avg:144.71ms
step:843/1375 train_time:120544ms step_avg:144.71ms
step:844/1375 train_time:120697ms step_avg:144.72ms
step:845/1375 train_time:120847ms step_avg:144.73ms
step:846/1375 train_time:120999ms step_avg:144.74ms
step:847/1375 train_time:121152ms step_avg:144.75ms
step:848/1375 train_time:121303ms step_avg:144.75ms
step:849/1375 train_time:121457ms step_avg:144.76ms
step:850/1375 train_time:121612ms step_avg:144.78ms
step:851/1375 train_time:121763ms step_avg:144.78ms
step:852/1375 train_time:121916ms step_avg:144.79ms
step:853/1375 train_time:122067ms step_avg:144.80ms
step:854/1375 train_time:122217ms step_avg:144.81ms
step:855/1375 train_time:122368ms step_avg:144.81ms
step:856/1375 train_time:122517ms step_avg:144.82ms
step:857/1375 train_time:122670ms step_avg:144.83ms
step:858/1375 train_time:122825ms step_avg:144.84ms
step:859/1375 train_time:122977ms step_avg:144.85ms
step:860/1375 train_time:123129ms step_avg:144.86ms
step:861/1375 train_time:123280ms step_avg:144.87ms
step:862/1375 train_time:123432ms step_avg:144.87ms
step:863/1375 train_time:123583ms step_avg:144.88ms
step:864/1375 train_time:123736ms step_avg:144.89ms
step:865/1375 train_time:123886ms step_avg:144.90ms
step:866/1375 train_time:124045ms step_avg:144.91ms
step:867/1375 train_time:124196ms step_avg:144.92ms
step:868/1375 train_time:124345ms step_avg:144.92ms
step:869/1375 train_time:124496ms step_avg:144.93ms
step:870/1375 train_time:124649ms step_avg:144.94ms
step:871/1375 train_time:124799ms step_avg:144.95ms
step:872/1375 train_time:124952ms step_avg:144.96ms
step:873/1375 train_time:125103ms step_avg:144.96ms
step:874/1375 train_time:125256ms step_avg:144.97ms
step:875/1375 train_time:125408ms step_avg:144.98ms
step:875/1375 val_loss:3.4690 train_time:125483ms step_avg:145.07ms
step:876/1375 train_time:125559ms step_avg:144.99ms
step:877/1375 train_time:125715ms step_avg:145.00ms
step:878/1375 train_time:125865ms step_avg:145.01ms
step:879/1375 train_time:126017ms step_avg:145.01ms
step:880/1375 train_time:126167ms step_avg:145.02ms
step:881/1375 train_time:126316ms step_avg:145.02ms
step:882/1375 train_time:126470ms step_avg:145.03ms
step:883/1375 train_time:126622ms step_avg:145.04ms
step:884/1375 train_time:126777ms step_avg:145.05ms
step:885/1375 train_time:126928ms step_avg:145.06ms
step:886/1375 train_time:127081ms step_avg:145.07ms
step:887/1375 train_time:127231ms step_avg:145.08ms
step:888/1375 train_time:127384ms step_avg:145.08ms
step:889/1375 train_time:127537ms step_avg:145.09ms
step:890/1375 train_time:127687ms step_avg:145.10ms
step:891/1375 train_time:127842ms step_avg:145.11ms
step:892/1375 train_time:127994ms step_avg:145.12ms
step:893/1375 train_time:128143ms step_avg:145.12ms
step:894/1375 train_time:128296ms step_avg:145.13ms
step:895/1375 train_time:128448ms step_avg:145.14ms
step:896/1375 train_time:128600ms step_avg:145.15ms
step:897/1375 train_time:128751ms step_avg:145.15ms
step:898/1375 train_time:128905ms step_avg:145.16ms
step:899/1375 train_time:129055ms step_avg:145.17ms
step:900/1375 train_time:129205ms step_avg:145.17ms
step:901/1375 train_time:129361ms step_avg:145.19ms
step:902/1375 train_time:129509ms step_avg:145.19ms
step:903/1375 train_time:129661ms step_avg:145.20ms
step:904/1375 train_time:129814ms step_avg:145.21ms
step:905/1375 train_time:129966ms step_avg:145.21ms
step:906/1375 train_time:130117ms step_avg:145.22ms
step:907/1375 train_time:130269ms step_avg:145.23ms
step:908/1375 train_time:130422ms step_avg:145.24ms
step:909/1375 train_time:130576ms step_avg:145.25ms
step:910/1375 train_time:130736ms step_avg:145.26ms
step:911/1375 train_time:130885ms step_avg:145.27ms
step:912/1375 train_time:131035ms step_avg:145.27ms
step:913/1375 train_time:131185ms step_avg:145.28ms
step:914/1375 train_time:131339ms step_avg:145.29ms
step:915/1375 train_time:131489ms step_avg:145.29ms
step:916/1375 train_time:131643ms step_avg:145.30ms
step:917/1375 train_time:131795ms step_avg:145.31ms
step:918/1375 train_time:131949ms step_avg:145.32ms
step:919/1375 train_time:132104ms step_avg:145.33ms
step:920/1375 train_time:132256ms step_avg:145.34ms
step:921/1375 train_time:132409ms step_avg:145.34ms
step:922/1375 train_time:132566ms step_avg:145.36ms
step:923/1375 train_time:132717ms step_avg:145.36ms
step:924/1375 train_time:132868ms step_avg:145.37ms
step:925/1375 train_time:133024ms step_avg:145.38ms
step:926/1375 train_time:133180ms step_avg:145.39ms
step:927/1375 train_time:133334ms step_avg:145.40ms
step:928/1375 train_time:133488ms step_avg:145.41ms
step:929/1375 train_time:133644ms step_avg:145.42ms
step:930/1375 train_time:133798ms step_avg:145.43ms
step:931/1375 train_time:133950ms step_avg:145.44ms
step:932/1375 train_time:134103ms step_avg:145.45ms
step:933/1375 train_time:134255ms step_avg:145.46ms
step:934/1375 train_time:134409ms step_avg:145.46ms
step:935/1375 train_time:134563ms step_avg:145.47ms
step:936/1375 train_time:134716ms step_avg:145.48ms
step:937/1375 train_time:134876ms step_avg:145.50ms
step:938/1375 train_time:135029ms step_avg:145.51ms
step:939/1375 train_time:135182ms step_avg:145.51ms
step:940/1375 train_time:135335ms step_avg:145.52ms
step:941/1375 train_time:135486ms step_avg:145.53ms
step:942/1375 train_time:135637ms step_avg:145.53ms
step:943/1375 train_time:135791ms step_avg:145.54ms
step:944/1375 train_time:135952ms step_avg:145.56ms
step:945/1375 train_time:136105ms step_avg:145.57ms
step:946/1375 train_time:136259ms step_avg:145.58ms
step:947/1375 train_time:136413ms step_avg:145.58ms
step:948/1375 train_time:136565ms step_avg:145.59ms
step:949/1375 train_time:136719ms step_avg:145.60ms
step:950/1375 train_time:136870ms step_avg:145.61ms
step:951/1375 train_time:137066ms step_avg:145.66ms
step:952/1375 train_time:137217ms step_avg:145.67ms
step:953/1375 train_time:137369ms step_avg:145.67ms
step:954/1375 train_time:137522ms step_avg:145.68ms
step:955/1375 train_time:137672ms step_avg:145.68ms
step:956/1375 train_time:137825ms step_avg:145.69ms
step:957/1375 train_time:137978ms step_avg:145.70ms
step:958/1375 train_time:138138ms step_avg:145.71ms
step:959/1375 train_time:138292ms step_avg:145.72ms
step:960/1375 train_time:138446ms step_avg:145.73ms
step:961/1375 train_time:138599ms step_avg:145.74ms
step:962/1375 train_time:138749ms step_avg:145.75ms
step:963/1375 train_time:138909ms step_avg:145.76ms
step:964/1375 train_time:139061ms step_avg:145.77ms
step:965/1375 train_time:139212ms step_avg:145.77ms
step:966/1375 train_time:139367ms step_avg:145.78ms
step:967/1375 train_time:139519ms step_avg:145.79ms
step:968/1375 train_time:139668ms step_avg:145.79ms
step:969/1375 train_time:139823ms step_avg:145.80ms
step:970/1375 train_time:139976ms step_avg:145.81ms
step:971/1375 train_time:140129ms step_avg:145.82ms
step:972/1375 train_time:140281ms step_avg:145.82ms
step:973/1375 train_time:140432ms step_avg:145.83ms
step:974/1375 train_time:140585ms step_avg:145.84ms
step:975/1375 train_time:140738ms step_avg:145.84ms
step:976/1375 train_time:140891ms step_avg:145.85ms
step:977/1375 train_time:141043ms step_avg:145.86ms
step:978/1375 train_time:141196ms step_avg:145.86ms
step:979/1375 train_time:141347ms step_avg:145.87ms
step:980/1375 train_time:141500ms step_avg:145.88ms
step:981/1375 train_time:141649ms step_avg:145.88ms
step:982/1375 train_time:141802ms step_avg:145.89ms
step:983/1375 train_time:141955ms step_avg:145.89ms
step:984/1375 train_time:142106ms step_avg:145.90ms
step:985/1375 train_time:142260ms step_avg:145.91ms
step:986/1375 train_time:142416ms step_avg:145.92ms
step:987/1375 train_time:142568ms step_avg:145.92ms
step:988/1375 train_time:142721ms step_avg:145.93ms
step:989/1375 train_time:142871ms step_avg:145.94ms
step:990/1375 train_time:143026ms step_avg:145.94ms
step:991/1375 train_time:143178ms step_avg:145.95ms
step:992/1375 train_time:143335ms step_avg:145.96ms
step:993/1375 train_time:143499ms step_avg:145.98ms
step:994/1375 train_time:143650ms step_avg:145.99ms
step:995/1375 train_time:143801ms step_avg:145.99ms
step:996/1375 train_time:143950ms step_avg:145.99ms
step:997/1375 train_time:144103ms step_avg:146.00ms
step:998/1375 train_time:144254ms step_avg:146.01ms
step:999/1375 train_time:144408ms step_avg:146.01ms
step:1000/1375 train_time:144561ms step_avg:146.02ms
step:1000/1375 val_loss:3.4030 train_time:144636ms step_avg:146.10ms
step:1001/1375 train_time:144712ms step_avg:146.03ms
step:1002/1375 train_time:144869ms step_avg:146.04ms
step:1003/1375 train_time:145022ms step_avg:146.04ms
step:1004/1375 train_time:145178ms step_avg:146.05ms
step:1005/1375 train_time:145329ms step_avg:146.06ms
step:1006/1375 train_time:145480ms step_avg:146.06ms
step:1007/1375 train_time:145634ms step_avg:146.07ms
step:1008/1375 train_time:145788ms step_avg:146.08ms
step:1009/1375 train_time:145947ms step_avg:146.09ms
step:1010/1375 train_time:146101ms step_avg:146.10ms
step:1011/1375 train_time:146253ms step_avg:146.11ms
step:1012/1375 train_time:146405ms step_avg:146.11ms
step:1013/1375 train_time:146558ms step_avg:146.12ms
step:1014/1375 train_time:146710ms step_avg:146.13ms
step:1015/1375 train_time:146863ms step_avg:146.13ms
step:1016/1375 train_time:147017ms step_avg:146.14ms
step:1017/1375 train_time:147170ms step_avg:146.15ms
step:1018/1375 train_time:147322ms step_avg:146.15ms
step:1019/1375 train_time:147477ms step_avg:146.16ms
step:1020/1375 train_time:147631ms step_avg:146.17ms
step:1021/1375 train_time:147783ms step_avg:146.18ms
step:1022/1375 train_time:147938ms step_avg:146.18ms
step:1023/1375 train_time:148092ms step_avg:146.19ms
step:1024/1375 train_time:148245ms step_avg:146.20ms
step:1025/1375 train_time:148401ms step_avg:146.21ms
step:1026/1375 train_time:148554ms step_avg:146.21ms
step:1027/1375 train_time:148706ms step_avg:146.22ms
step:1028/1375 train_time:148862ms step_avg:146.23ms
step:1029/1375 train_time:149017ms step_avg:146.24ms
step:1030/1375 train_time:149171ms step_avg:146.25ms
step:1031/1375 train_time:149322ms step_avg:146.25ms
step:1032/1375 train_time:149476ms step_avg:146.26ms
step:1033/1375 train_time:149629ms step_avg:146.26ms
step:1034/1375 train_time:149783ms step_avg:146.27ms
step:1035/1375 train_time:149939ms step_avg:146.28ms
step:1036/1375 train_time:150096ms step_avg:146.29ms
step:1037/1375 train_time:150252ms step_avg:146.30ms
step:1038/1375 train_time:150407ms step_avg:146.31ms
step:1039/1375 train_time:150559ms step_avg:146.32ms
step:1040/1375 train_time:150712ms step_avg:146.32ms
step:1041/1375 train_time:150865ms step_avg:146.33ms
step:1042/1375 train_time:151017ms step_avg:146.33ms
step:1043/1375 train_time:151170ms step_avg:146.34ms
step:1044/1375 train_time:151325ms step_avg:146.35ms
step:1045/1375 train_time:151481ms step_avg:146.36ms
step:1046/1375 train_time:151631ms step_avg:146.36ms
step:1047/1375 train_time:151786ms step_avg:146.37ms
step:1048/1375 train_time:151941ms step_avg:146.38ms
step:1049/1375 train_time:152098ms step_avg:146.39ms
step:1050/1375 train_time:152256ms step_avg:146.40ms
step:1051/1375 train_time:152412ms step_avg:146.41ms
step:1052/1375 train_time:152566ms step_avg:146.42ms
step:1053/1375 train_time:152718ms step_avg:146.42ms
step:1054/1375 train_time:152874ms step_avg:146.43ms
step:1055/1375 train_time:153026ms step_avg:146.44ms
step:1056/1375 train_time:153183ms step_avg:146.45ms
step:1057/1375 train_time:153337ms step_avg:146.45ms
step:1058/1375 train_time:153496ms step_avg:146.47ms
step:1059/1375 train_time:153651ms step_avg:146.47ms
step:1060/1375 train_time:153806ms step_avg:146.48ms
step:1061/1375 train_time:153958ms step_avg:146.49ms
step:1062/1375 train_time:154113ms step_avg:146.50ms
step:1063/1375 train_time:154266ms step_avg:146.50ms
step:1064/1375 train_time:154418ms step_avg:146.51ms
step:1065/1375 train_time:154573ms step_avg:146.51ms
step:1066/1375 train_time:154731ms step_avg:146.53ms
step:1067/1375 train_time:154886ms step_avg:146.53ms
step:1068/1375 train_time:155039ms step_avg:146.54ms
step:1069/1375 train_time:155200ms step_avg:146.55ms
step:1070/1375 train_time:155352ms step_avg:146.56ms
step:1071/1375 train_time:155507ms step_avg:146.57ms
step:1072/1375 train_time:155659ms step_avg:146.57ms
step:1073/1375 train_time:155813ms step_avg:146.58ms
step:1074/1375 train_time:155966ms step_avg:146.58ms
step:1075/1375 train_time:156120ms step_avg:146.59ms
step:1076/1375 train_time:156274ms step_avg:146.60ms
step:1077/1375 train_time:156426ms step_avg:146.60ms
step:1078/1375 train_time:156585ms step_avg:146.62ms
step:1079/1375 train_time:156742ms step_avg:146.62ms
step:1080/1375 train_time:156897ms step_avg:146.63ms
step:1081/1375 train_time:157051ms step_avg:146.64ms
step:1082/1375 train_time:157203ms step_avg:146.64ms
step:1083/1375 train_time:157356ms step_avg:146.65ms
step:1084/1375 train_time:157516ms step_avg:146.66ms
step:1085/1375 train_time:157667ms step_avg:146.67ms
step:1086/1375 train_time:157823ms step_avg:146.68ms
step:1087/1375 train_time:157982ms step_avg:146.69ms
step:1088/1375 train_time:158135ms step_avg:146.69ms
step:1089/1375 train_time:158294ms step_avg:146.70ms
step:1090/1375 train_time:158454ms step_avg:146.72ms
step:1091/1375 train_time:158607ms step_avg:146.72ms
step:1092/1375 train_time:158759ms step_avg:146.73ms
step:1093/1375 train_time:158915ms step_avg:146.74ms
step:1094/1375 train_time:159068ms step_avg:146.74ms
step:1095/1375 train_time:159223ms step_avg:146.75ms
step:1096/1375 train_time:159386ms step_avg:146.76ms
step:1097/1375 train_time:159541ms step_avg:146.77ms
step:1098/1375 train_time:159692ms step_avg:146.78ms
step:1099/1375 train_time:159845ms step_avg:146.78ms
step:1100/1375 train_time:159997ms step_avg:146.79ms
step:1101/1375 train_time:160149ms step_avg:146.79ms
step:1102/1375 train_time:160306ms step_avg:146.80ms
step:1103/1375 train_time:160462ms step_avg:146.81ms
step:1104/1375 train_time:160614ms step_avg:146.81ms
step:1105/1375 train_time:160769ms step_avg:146.82ms
step:1106/1375 train_time:160922ms step_avg:146.83ms
step:1107/1375 train_time:161075ms step_avg:146.83ms
step:1108/1375 train_time:161230ms step_avg:146.84ms
step:1109/1375 train_time:161383ms step_avg:146.84ms
step:1110/1375 train_time:161536ms step_avg:146.85ms
step:1111/1375 train_time:161694ms step_avg:146.86ms
step:1112/1375 train_time:161847ms step_avg:146.87ms
step:1113/1375 train_time:162000ms step_avg:146.87ms
step:1114/1375 train_time:162156ms step_avg:146.88ms
step:1115/1375 train_time:162312ms step_avg:146.89ms
step:1116/1375 train_time:162464ms step_avg:146.89ms
step:1117/1375 train_time:162621ms step_avg:146.90ms
step:1118/1375 train_time:162779ms step_avg:146.91ms
step:1119/1375 train_time:162933ms step_avg:146.92ms
step:1120/1375 train_time:163086ms step_avg:146.92ms
step:1121/1375 train_time:163240ms step_avg:146.93ms
step:1122/1375 train_time:163393ms step_avg:146.94ms
step:1123/1375 train_time:163546ms step_avg:146.94ms
step:1124/1375 train_time:163705ms step_avg:146.95ms
step:1125/1375 train_time:163862ms step_avg:146.96ms
step:1125/1375 val_loss:3.3501 train_time:163939ms step_avg:147.03ms
step:1126/1375 train_time:164015ms step_avg:146.97ms
step:1127/1375 train_time:164171ms step_avg:146.97ms
step:1128/1375 train_time:164326ms step_avg:146.98ms
step:1129/1375 train_time:164484ms step_avg:146.99ms
step:1130/1375 train_time:164637ms step_avg:147.00ms
step:1131/1375 train_time:164794ms step_avg:147.01ms
step:1132/1375 train_time:164947ms step_avg:147.01ms
step:1133/1375 train_time:165103ms step_avg:147.02ms
step:1134/1375 train_time:165259ms step_avg:147.03ms
step:1135/1375 train_time:165412ms step_avg:147.03ms
step:1136/1375 train_time:165571ms step_avg:147.04ms
step:1137/1375 train_time:165724ms step_avg:147.05ms
step:1138/1375 train_time:165879ms step_avg:147.06ms
step:1139/1375 train_time:166033ms step_avg:147.06ms
step:1140/1375 train_time:166187ms step_avg:147.07ms
step:1141/1375 train_time:166382ms step_avg:147.11ms
step:1142/1375 train_time:166534ms step_avg:147.12ms
step:1143/1375 train_time:166693ms step_avg:147.13ms
step:1144/1375 train_time:166848ms step_avg:147.13ms
step:1145/1375 train_time:166999ms step_avg:147.14ms
step:1146/1375 train_time:167157ms step_avg:147.14ms
step:1147/1375 train_time:167314ms step_avg:147.15ms
step:1148/1375 train_time:167469ms step_avg:147.16ms
step:1149/1375 train_time:167623ms step_avg:147.17ms
step:1150/1375 train_time:167775ms step_avg:147.17ms
step:1151/1375 train_time:167933ms step_avg:147.18ms
step:1152/1375 train_time:168091ms step_avg:147.19ms
step:1153/1375 train_time:168248ms step_avg:147.20ms
step:1154/1375 train_time:168400ms step_avg:147.20ms
step:1155/1375 train_time:168554ms step_avg:147.21ms
step:1156/1375 train_time:168713ms step_avg:147.22ms
step:1157/1375 train_time:168870ms step_avg:147.23ms
step:1158/1375 train_time:169024ms step_avg:147.23ms
step:1159/1375 train_time:169179ms step_avg:147.24ms
step:1160/1375 train_time:169332ms step_avg:147.25ms
step:1161/1375 train_time:169488ms step_avg:147.25ms
step:1162/1375 train_time:169645ms step_avg:147.26ms
step:1163/1375 train_time:169798ms step_avg:147.27ms
step:1164/1375 train_time:169954ms step_avg:147.27ms
step:1165/1375 train_time:170106ms step_avg:147.28ms
step:1166/1375 train_time:170261ms step_avg:147.28ms
step:1167/1375 train_time:170415ms step_avg:147.29ms
step:1168/1375 train_time:170571ms step_avg:147.30ms
step:1169/1375 train_time:170728ms step_avg:147.31ms
step:1170/1375 train_time:170884ms step_avg:147.31ms
step:1171/1375 train_time:171040ms step_avg:147.32ms
step:1172/1375 train_time:171194ms step_avg:147.33ms
step:1173/1375 train_time:171351ms step_avg:147.34ms
step:1174/1375 train_time:171515ms step_avg:147.35ms
step:1175/1375 train_time:171674ms step_avg:147.36ms
step:1176/1375 train_time:171838ms step_avg:147.37ms
step:1177/1375 train_time:171997ms step_avg:147.38ms
step:1178/1375 train_time:172152ms step_avg:147.39ms
step:1179/1375 train_time:172307ms step_avg:147.40ms
step:1180/1375 train_time:172469ms step_avg:147.41ms
step:1181/1375 train_time:172624ms step_avg:147.42ms
step:1182/1375 train_time:172775ms step_avg:147.42ms
step:1183/1375 train_time:172931ms step_avg:147.43ms
step:1184/1375 train_time:173085ms step_avg:147.43ms
step:1185/1375 train_time:173245ms step_avg:147.44ms
step:1186/1375 train_time:173399ms step_avg:147.45ms
step:1187/1375 train_time:173562ms step_avg:147.46ms
step:1188/1375 train_time:173714ms step_avg:147.47ms
step:1189/1375 train_time:173871ms step_avg:147.47ms
step:1190/1375 train_time:174028ms step_avg:147.48ms
step:1191/1375 train_time:174182ms step_avg:147.49ms
step:1192/1375 train_time:174335ms step_avg:147.49ms
step:1193/1375 train_time:174491ms step_avg:147.50ms
step:1194/1375 train_time:174646ms step_avg:147.51ms
step:1195/1375 train_time:174801ms step_avg:147.51ms
step:1196/1375 train_time:174955ms step_avg:147.52ms
step:1197/1375 train_time:175112ms step_avg:147.52ms
step:1198/1375 train_time:175272ms step_avg:147.53ms
step:1199/1375 train_time:175425ms step_avg:147.54ms
step:1200/1375 train_time:175578ms step_avg:147.54ms
step:1201/1375 train_time:175734ms step_avg:147.55ms
step:1202/1375 train_time:175901ms step_avg:147.57ms
step:1203/1375 train_time:176062ms step_avg:147.58ms
step:1204/1375 train_time:176216ms step_avg:147.58ms
step:1205/1375 train_time:176371ms step_avg:147.59ms
step:1206/1375 train_time:176524ms step_avg:147.60ms
step:1207/1375 train_time:176680ms step_avg:147.60ms
step:1208/1375 train_time:176837ms step_avg:147.61ms
step:1209/1375 train_time:176991ms step_avg:147.62ms
step:1210/1375 train_time:177153ms step_avg:147.63ms
step:1211/1375 train_time:177309ms step_avg:147.63ms
step:1212/1375 train_time:177462ms step_avg:147.64ms
step:1213/1375 train_time:177617ms step_avg:147.64ms
step:1214/1375 train_time:177774ms step_avg:147.65ms
step:1215/1375 train_time:177930ms step_avg:147.66ms
step:1216/1375 train_time:178083ms step_avg:147.66ms
step:1217/1375 train_time:178237ms step_avg:147.67ms
step:1218/1375 train_time:178390ms step_avg:147.67ms
step:1219/1375 train_time:178545ms step_avg:147.68ms
step:1220/1375 train_time:178699ms step_avg:147.69ms
step:1221/1375 train_time:178852ms step_avg:147.69ms
step:1222/1375 train_time:179007ms step_avg:147.70ms
step:1223/1375 train_time:179163ms step_avg:147.70ms
step:1224/1375 train_time:179320ms step_avg:147.71ms
step:1225/1375 train_time:179475ms step_avg:147.72ms
step:1226/1375 train_time:179632ms step_avg:147.72ms
step:1227/1375 train_time:179791ms step_avg:147.73ms
step:1228/1375 train_time:179944ms step_avg:147.74ms
step:1229/1375 train_time:180097ms step_avg:147.74ms
step:1230/1375 train_time:180257ms step_avg:147.75ms
step:1231/1375 train_time:180415ms step_avg:147.76ms
step:1232/1375 train_time:180575ms step_avg:147.77ms
step:1233/1375 train_time:180732ms step_avg:147.78ms
step:1234/1375 train_time:180888ms step_avg:147.78ms
step:1235/1375 train_time:181044ms step_avg:147.79ms
step:1236/1375 train_time:181199ms step_avg:147.80ms
step:1237/1375 train_time:181353ms step_avg:147.80ms
step:1238/1375 train_time:181518ms step_avg:147.82ms
step:1239/1375 train_time:181674ms step_avg:147.82ms
step:1240/1375 train_time:181833ms step_avg:147.83ms
step:1241/1375 train_time:181994ms step_avg:147.84ms
step:1242/1375 train_time:182150ms step_avg:147.85ms
step:1243/1375 train_time:182310ms step_avg:147.86ms
step:1244/1375 train_time:182465ms step_avg:147.86ms
step:1245/1375 train_time:182620ms step_avg:147.87ms
step:1246/1375 train_time:182776ms step_avg:147.88ms
step:1247/1375 train_time:182933ms step_avg:147.88ms
step:1248/1375 train_time:183087ms step_avg:147.89ms
step:1249/1375 train_time:183242ms step_avg:147.89ms
step:1250/1375 train_time:183396ms step_avg:147.90ms
step:1250/1375 val_loss:3.3044 train_time:183477ms step_avg:147.97ms
step:1251/1375 train_time:183556ms step_avg:147.91ms
step:1252/1375 train_time:183711ms step_avg:147.92ms
step:1253/1375 train_time:183865ms step_avg:147.92ms
step:1254/1375 train_time:184016ms step_avg:147.92ms
step:1255/1375 train_time:184184ms step_avg:147.94ms
step:1256/1375 train_time:184338ms step_avg:147.94ms
step:1257/1375 train_time:184493ms step_avg:147.95ms
step:1258/1375 train_time:184652ms step_avg:147.96ms
step:1259/1375 train_time:184810ms step_avg:147.97ms
step:1260/1375 train_time:184963ms step_avg:147.97ms
step:1261/1375 train_time:185119ms step_avg:147.98ms
step:1262/1375 train_time:185278ms step_avg:147.99ms
step:1263/1375 train_time:185435ms step_avg:147.99ms
step:1264/1375 train_time:185590ms step_avg:148.00ms
step:1265/1375 train_time:185745ms step_avg:148.00ms
step:1266/1375 train_time:185901ms step_avg:148.01ms
step:1267/1375 train_time:186057ms step_avg:148.02ms
step:1268/1375 train_time:186214ms step_avg:148.02ms
step:1269/1375 train_time:186373ms step_avg:148.03ms
step:1270/1375 train_time:186529ms step_avg:148.04ms
step:1271/1375 train_time:186685ms step_avg:148.05ms
step:1272/1375 train_time:186840ms step_avg:148.05ms
step:1273/1375 train_time:186995ms step_avg:148.06ms
step:1274/1375 train_time:187150ms step_avg:148.06ms
step:1275/1375 train_time:187309ms step_avg:148.07ms
step:1276/1375 train_time:187460ms step_avg:148.07ms
step:1277/1375 train_time:187615ms step_avg:148.08ms
step:1278/1375 train_time:187769ms step_avg:148.08ms
step:1279/1375 train_time:187928ms step_avg:148.09ms
step:1280/1375 train_time:188090ms step_avg:148.10ms
step:1281/1375 train_time:188245ms step_avg:148.11ms
step:1282/1375 train_time:188398ms step_avg:148.11ms
step:1283/1375 train_time:188554ms step_avg:148.12ms
step:1284/1375 train_time:188712ms step_avg:148.13ms
step:1285/1375 train_time:188866ms step_avg:148.13ms
step:1286/1375 train_time:189023ms step_avg:148.14ms
step:1287/1375 train_time:189177ms step_avg:148.14ms
step:1288/1375 train_time:189334ms step_avg:148.15ms
step:1289/1375 train_time:189494ms step_avg:148.16ms
step:1290/1375 train_time:189655ms step_avg:148.17ms
step:1291/1375 train_time:189814ms step_avg:148.18ms
step:1292/1375 train_time:189970ms step_avg:148.18ms
step:1293/1375 train_time:190130ms step_avg:148.19ms
step:1294/1375 train_time:190286ms step_avg:148.20ms
step:1295/1375 train_time:190441ms step_avg:148.20ms
step:1296/1375 train_time:190599ms step_avg:148.21ms
step:1297/1375 train_time:190757ms step_avg:148.22ms
step:1298/1375 train_time:190913ms step_avg:148.22ms
step:1299/1375 train_time:191067ms step_avg:148.23ms
step:1300/1375 train_time:191220ms step_avg:148.23ms
step:1301/1375 train_time:191373ms step_avg:148.24ms
step:1302/1375 train_time:191531ms step_avg:148.24ms
step:1303/1375 train_time:191688ms step_avg:148.25ms
step:1304/1375 train_time:191846ms step_avg:148.26ms
step:1305/1375 train_time:192002ms step_avg:148.26ms
step:1306/1375 train_time:192158ms step_avg:148.27ms
step:1307/1375 train_time:192312ms step_avg:148.27ms
step:1308/1375 train_time:192468ms step_avg:148.28ms
step:1309/1375 train_time:192623ms step_avg:148.29ms
step:1310/1375 train_time:192777ms step_avg:148.29ms
step:1311/1375 train_time:192932ms step_avg:148.29ms
step:1312/1375 train_time:193085ms step_avg:148.30ms
step:1313/1375 train_time:193240ms step_avg:148.30ms
step:1314/1375 train_time:193396ms step_avg:148.31ms
step:1315/1375 train_time:193552ms step_avg:148.32ms
step:1316/1375 train_time:193706ms step_avg:148.32ms
step:1317/1375 train_time:193860ms step_avg:148.32ms
step:1318/1375 train_time:194021ms step_avg:148.33ms
step:1319/1375 train_time:194179ms step_avg:148.34ms
step:1320/1375 train_time:194334ms step_avg:148.35ms
step:1321/1375 train_time:194491ms step_avg:148.35ms
step:1322/1375 train_time:194651ms step_avg:148.36ms
step:1323/1375 train_time:194807ms step_avg:148.37ms
step:1324/1375 train_time:194962ms step_avg:148.37ms
step:1325/1375 train_time:195119ms step_avg:148.38ms
step:1326/1375 train_time:195279ms step_avg:148.39ms
step:1327/1375 train_time:195434ms step_avg:148.39ms
step:1328/1375 train_time:195589ms step_avg:148.40ms
step:1329/1375 train_time:195766ms step_avg:148.42ms
step:1330/1375 train_time:195925ms step_avg:148.43ms
step:1331/1375 train_time:196122ms step_avg:148.46ms
step:1332/1375 train_time:196283ms step_avg:148.47ms
step:1333/1375 train_time:196440ms step_avg:148.48ms
step:1334/1375 train_time:196596ms step_avg:148.49ms
step:1335/1375 train_time:196750ms step_avg:148.49ms
step:1336/1375 train_time:196912ms step_avg:148.50ms
step:1337/1375 train_time:197071ms step_avg:148.51ms
step:1338/1375 train_time:197228ms step_avg:148.52ms
step:1339/1375 train_time:197389ms step_avg:148.52ms
step:1340/1375 train_time:197547ms step_avg:148.53ms
step:1341/1375 train_time:197702ms step_avg:148.54ms
step:1342/1375 train_time:197863ms step_avg:148.55ms
step:1343/1375 train_time:198017ms step_avg:148.55ms
step:1344/1375 train_time:198171ms step_avg:148.55ms
step:1345/1375 train_time:198327ms step_avg:148.56ms
step:1346/1375 train_time:198484ms step_avg:148.57ms
step:1347/1375 train_time:198642ms step_avg:148.57ms
step:1348/1375 train_time:198799ms step_avg:148.58ms
step:1349/1375 train_time:198957ms step_avg:148.59ms
step:1350/1375 train_time:199111ms step_avg:148.59ms
step:1351/1375 train_time:199266ms step_avg:148.60ms
step:1352/1375 train_time:199429ms step_avg:148.61ms
step:1353/1375 train_time:199592ms step_avg:148.62ms
step:1354/1375 train_time:199753ms step_avg:148.63ms
step:1355/1375 train_time:199910ms step_avg:148.63ms
step:1356/1375 train_time:200066ms step_avg:148.64ms
step:1357/1375 train_time:200224ms step_avg:148.64ms
step:1358/1375 train_time:200383ms step_avg:148.65ms
step:1359/1375 train_time:200540ms step_avg:148.66ms
step:1360/1375 train_time:200701ms step_avg:148.67ms
step:1361/1375 train_time:200862ms step_avg:148.68ms
step:1362/1375 train_time:201023ms step_avg:148.69ms
step:1363/1375 train_time:201184ms step_avg:148.69ms
step:1364/1375 train_time:201338ms step_avg:148.70ms
step:1365/1375 train_time:201492ms step_avg:148.70ms
step:1366/1375 train_time:201650ms step_avg:148.71ms
step:1367/1375 train_time:201806ms step_avg:148.71ms
step:1368/1375 train_time:201964ms step_avg:148.72ms
step:1369/1375 train_time:202131ms step_avg:148.74ms
step:1370/1375 train_time:202290ms step_avg:148.74ms
step:1371/1375 train_time:202445ms step_avg:148.75ms
step:1372/1375 train_time:202608ms step_avg:148.76ms
step:1373/1375 train_time:202761ms step_avg:148.76ms
step:1374/1375 train_time:202920ms step_avg:148.77ms
step:1375/1375 train_time:203075ms step_avg:148.77ms
step:1375/1375 val_loss:3.2790 train_time:203152ms step_avg:148.83ms
peak memory consumption: 31565 MiB

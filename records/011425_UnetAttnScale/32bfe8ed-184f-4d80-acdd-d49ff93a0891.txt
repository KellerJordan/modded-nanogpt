import os
import sys
with open(sys.argv[0]) as f:
    code = f.read() # read the code of this file ASAP, for logging
import uuid
import time
import glob
import subprocess
import contextlib
from dataclasses import dataclass

import torch
torch.empty(1, device='cuda', requires_grad=True).backward()
from torch import nn
import torch.nn.functional as F
import torch.distributed as dist
from torch.nn.parallel import DistributedDataParallel as DDP
# use of FlexAttention contributed by @KoszarskyB
from torch.nn.attention.flex_attention import BlockMask, flex_attention

# -----------------------------------------------------------------------------
# Muon optimizer

@torch.compile
def zeropower_via_newtonschulz5(G, steps):
    """
    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a
    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose
    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at
    zero even beyond the point where the iteration no longer converges all the way to one everywhere
    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T
    where S' is diagonal with S_{ii}' ~ Uniform(0.5, 1.5), which turns out not to hurt model
    performance at all relative to UV^T, where USV^T = G is the SVD.
    """
    assert len(G.shape) == 2
    a, b, c = (3.4445, -4.7750,  2.0315)
    X = G.bfloat16()
    if G.size(0) > G.size(1):
        X = X.T

    # Ensure spectral norm is at most 1
    X = X / (X.norm() + 1e-7)
    # Perform the NS iterations
    for _ in range(steps):
        A = X @ X.T
        B = b * A + c * A @ A # adapted from suggestion by @jxbz, @leloykun, and @YouJiacheng
        X = a * X + B @ X
    
    if G.size(0) > G.size(1):
        X = X.T
    return X

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Some warnings:
    - This optimizer assumes that all parameters passed in are 2D.
    - It should not be used for the embedding layer, the final fully connected layer, or any {0,1}-D
    parameters; those should all be optimized by a standard method (e.g., AdamW).
    - To use it with 4D convolutional filters, it works well to just flatten their last 3 dimensions.
    - We believe it is unlikely to work well for training with small batch size.
    - We believe it may not work well for finetuning pretrained models, but we haven't tested this.
    - We have not yet tried this optimizer for training scenarios larger than NanoGPT (124M).

    Arguments:
        lr: The learning rate used by the internal SGD.
        momentum: The momentum used by the internal SGD.
        nesterov: Whether to use Nesterov-style momentum in the internal SGD. (recommended)
        ns_steps: The number of Newton-Schulz iteration steps to use.
    """
    def __init__(self, params, lr=0.02, momentum=0.95, nesterov=True, ns_steps=5):
        self.world_size = int(os.environ['WORLD_SIZE'])
        self.rank = int(os.environ['RANK'])
        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, ns_steps=ns_steps)
        assert all(isinstance(p, torch.Tensor) for p in params)
        sizes = {p.numel() for p in params}
        param_groups = [dict(params=[p for p in params if p.numel() == size],
                             update_buffer=[torch.empty(size, device='cuda', dtype=torch.bfloat16) for _ in range(self.world_size)])
                        for size in sizes]
        super().__init__(param_groups, defaults)

    def step(self):

        for group in self.param_groups:

            lr = group['lr']
            momentum = group['momentum']
            nesterov = group['nesterov']
            ns_steps = group['ns_steps']
            update_buffers = group['update_buffer']
            # generate weight updates in distributed fashion
            params = group['params']
            handle = None
            params_world = None
            def update_prev():
                if params_world is None:
                    return
                assert handle is not None
                handle.wait()
                for p_world, g_world in zip(params_world, update_buffers):
                    p_world.data.add_(
                        g_world.view_as(p_world),
                        alpha=-lr * max(1, p_world.size(0) / p_world.size(1)) ** 0.5,
                    )
            for base_i in range(len(params))[::self.world_size]:
                if base_i + rank < len(params):
                    p = params[base_i + self.rank]
                    g = p.grad
                    assert g is not None
                    state = self.state[p]
                    if 'momentum_buffer' not in state:
                        state['momentum_buffer'] = torch.zeros_like(g)
                    buf = state['momentum_buffer']
                    buf.lerp_(g, 1 - momentum)
                    g = g.lerp_(buf, momentum) if nesterov else buf
                    g = zeropower_via_newtonschulz5(g, steps=ns_steps).flatten()
                else:
                    g = update_buffers[rank]
                update_prev() # async all_gather instead of sync all_reduce by @YouJiacheng
                handle = dist.all_gather(update_buffers, g, async_op=True)
                params_world = params[base_i : base_i + self.world_size]
            update_prev()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the GPT-2 model

def norm(x):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):

    def __init__(self, in_features, out_features):
        super().__init__(in_features, out_features, bias=False)

    def forward(self, x):
        return F.linear(x, self.weight.type_as(x))

class Rotary(nn.Module):

    def __init__(self, dim, max_seq_len=65536):
        super().__init__()
        # half-truncate RoPE by @YouJiacheng
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=dim//4, dtype=torch.float32)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(dim//4)])
        t = torch.arange(max_seq_len, dtype=torch.float32)
        theta = torch.einsum('i,j -> ij', t, angular_freq)
        self.cos = nn.Buffer(theta.cos(), persistent=False)
        self.sin = nn.Buffer(theta.sin(), persistent=False)

    def forward(self, x):
        cos, sin = self.cos[None, :x.size(-3), None, :], self.sin[None, :x.size(-3), None, :]
        x1, x2 = x.float().chunk(2, dim=-1)
        y1 = x1 * cos + x2 * sin
        y2 = x1 * (-sin) + x2 * cos
        return torch.cat((y1, y2), 3).type_as(x)

class CausalSelfAttention(nn.Module):

    def __init__(self, dim, num_heads, layer_id):
        super().__init__()
        assert dim % num_heads == 0
        self.num_heads = num_heads
        self.layer_id = layer_id
        self.c_q = CastedLinear(dim, dim)
        self.c_k = CastedLinear(dim, dim)
        self.c_v = CastedLinear(dim, dim)
        self.lambdas = nn.Parameter(torch.tensor([0.5, 0.5]))
        self.rotary = Rotary(dim // num_heads) # dim // num_heads = head_dim
        self.c_proj = CastedLinear(dim, dim)
        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977
        # self.attn_scale = nn.Parameter(torch.tensor(1.0 / (dim // num_heads) ** 0.5))
        # self.attn_scale = nn.Parameter(torch.tensor(0.20))
        self.attn_scale = 0.13 + 0.01 * min(layer_id, 11 - layer_id)  # unet pattern attention scale by @leloykun

    def forward(self, x, ve, block_mask):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, 'Must use batch size = 1 for FlexAttention'
        q = self.c_q(x).view(B, T, self.num_heads, -1)
        k = self.c_k(x).view(B, T, self.num_heads, -1)
        v = self.c_v(x).view(B, T, self.num_heads, -1)
        if ve is not None:
            v = self.lambdas[0] * v + self.lambdas[1] * ve.view_as(v) # @KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = self.lambdas[0] * v
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = self.rotary(q), self.rotary(k)
        # y = flex_attention(q.transpose(1, 2) * self.attn_scale, k.transpose(1, 2), v.transpose(1, 2), block_mask=block_mask, scale=1.)
        y = flex_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), block_mask=block_mask, scale=self.attn_scale)
        y = y.transpose(1, 2).contiguous().view_as(x) # re-assemble all head outputs side by side
        y = self.c_proj(y)
        return y

class MLP(nn.Module):

    def __init__(self, dim):
        super().__init__()
        self.c_fc = CastedLinear(dim, 4 * dim)
        self.c_proj = CastedLinear(4 * dim, dim)
        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977

    def forward(self, x):
        x = self.c_fc(x)
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = self.c_proj(x)
        return x

class Block(nn.Module):

    def __init__(self, model_dim, num_heads, layer_id, use_attn=True):
        super().__init__()
        self.attn = CausalSelfAttention(model_dim, num_heads, layer_id) if use_attn else None
        self.mlp = MLP(model_dim)
        self.lambdas = nn.Parameter(torch.tensor([1., 0.]))

    def forward(self, x, ve, x0, block_mask):
        x = self.lambdas[0] * x + self.lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), ve, block_mask)
        x = x + self.mlp(norm(x))
        return x

class ValueEmbedding(nn.Module):
    def __init__(self, vocab_size, model_dim):
        super().__init__()
        self.embed = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])

    def forward(self, inputs):
        ve = [emb(inputs).bfloat16() for emb in self.embed]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        ve = [ve[0], ve[1], ve[2], None, None, None, None, None, None, ve[0], ve[1], ve[2]]
        return ve

# -----------------------------------------------------------------------------
# The main GPT-2 model

class GPT(nn.Module):

    def __init__(self, vocab_size, num_layers, num_heads, model_dim):
        super().__init__()
        self.embed = nn.Embedding(vocab_size, model_dim)
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.blocks = nn.ModuleList([Block(model_dim, num_heads, layer_id=layer_id, use_attn=(layer_id != 7))
                                     for layer_id in range(num_layers)])
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual learning
        # U-net structure on token value embeddings by @leloykun
        self.value_embeds = ValueEmbedding(vocab_size, model_dim)
        self.lm_head = CastedLinear(model_dim, vocab_size)
        self.lm_head.weight.data.zero_() # @Grad62304977
        # U-net design by @brendanh0gan
        self.num_encoder_layers = num_layers // 2 # Half of the layers for encoder
        self.num_decoder_layers = num_layers - self.num_encoder_layers # Remaining for decoder
        # Add learnable skip connection weights for decoder layers
        self.skip_weights = nn.Parameter(torch.ones(self.num_decoder_layers))

    def forward(self, inputs, targets, sliding_window_num_blocks):
        BLOCK_SIZE = 128
        seq_len = len(inputs)
        assert seq_len % BLOCK_SIZE == 0
        total_num_blocks = seq_len // BLOCK_SIZE
        assert inputs.ndim == 1
        docs = (inputs == 50256).cumsum(0)
        docs_low = docs.view(-1, BLOCK_SIZE)[:, 0].contiguous()
        docs_high = docs.view(-1, BLOCK_SIZE)[:, -1].contiguous()

        def document_causal(b, h, q_idx, kv_idx):
            causal_mask = q_idx >= kv_idx
            document_mask = docs[q_idx] == docs[kv_idx]
            return causal_mask & document_mask

        def dense_to_ordered(dense_mask):
            num_blocks = dense_mask.sum(dim=-1, dtype=torch.int32)
            indices = dense_mask.argsort(dim=-1, descending=True, stable=True).to(torch.int32)
            return num_blocks[None, None].contiguous(), indices[None, None].contiguous()

        def create_doc_swc_block_mask(sliding_window_num_blocks):
            kv_idx = block_idx = torch.arange(total_num_blocks, dtype=torch.int32, device='cuda')
            q_idx = block_idx[:, None]
            causal_bm = q_idx >= kv_idx
            causal_full_bm = q_idx > kv_idx
            window_bm = q_idx - kv_idx < sliding_window_num_blocks
            window_full_bm = window_bm # block-wise sliding window by @YouJiacheng
            # document_bm = (docs_low[q_idx] <= docs_high[kv_idx]) & (docs_low[kv_idx] <= docs_high[q_idx])
            document_bm = (docs_low[:, None] <= docs_high) & (docs_low <= docs_high[:, None])
            document_full_bm = (docs_low[:, None] == docs_high) & (docs_low == docs_high[:, None])
            nonzero_bm = causal_bm & window_bm & document_bm
            full_bm  = causal_full_bm & window_full_bm & document_full_bm
            kv_num_blocks, kv_indices = dense_to_ordered(nonzero_bm & ~full_bm)
            full_kv_num_blocks, full_kv_indices = dense_to_ordered(full_bm)
            return BlockMask.from_kv_blocks(
                kv_num_blocks,
                kv_indices,
                full_kv_num_blocks,
                full_kv_indices,
                BLOCK_SIZE=BLOCK_SIZE,
                mask_mod=document_causal,
            )

        block_mask = create_doc_swc_block_mask(sliding_window_num_blocks)

        x0 = norm(self.embed(inputs[None]).bfloat16()) # use of norm here by @Grad62304977
        x = x0
        ve = self.value_embeds(inputs)
        assert len(ve) == len(self.blocks)
        ve_enc, ve_dec = ve[:self.num_encoder_layers], ve[self.num_encoder_layers:]

        # Store outputs for U-Net skip connections
        skip_connections = []
        # Encoder pass - process only the first half of the blocks
        for i in range(self.num_encoder_layers):
            x = self.blocks[i](x, ve_enc[i], x0, block_mask)
            skip_connections.append(x)
        # Decoder pass - process the remaining blocks with weighted skip connections
        for i in range(self.num_decoder_layers):
            x = x + self.skip_weights[i] * skip_connections.pop()
            # U-net structure on token value embeddings by @leloykun
            x = self.blocks[self.num_encoder_layers + i](x, ve_dec[i], x0, block_mask)

        x = norm(x)
        logits = self.lm_head(x)
        logits = 15 * torch.tanh(logits / 15) # @Grad62304977 added tanh softcapping, @KoszarskyB reduced it from 30 to 15
        logits = logits.float()
        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets)
        return loss

# -----------------------------------------------------------------------------
# Our own simple Distributed Data Loader

def _load_data_shard(path):
    # only reads the header, returns header data
    # header is 256 int32
    header = torch.from_file(path, False, 256, dtype=torch.int32)
    assert header[0] == 20240520, 'magic number mismatch in the data .bin file'
    assert header[1] == 1, 'unsupported version'
    num_tokens = int(header[2]) # number of tokens (claimed)
    with open(path, 'rb', buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, 'number of tokens read does not match header'
    return tokens

class DistributedDataLoader:

    def __init__(self, filename_pattern):
        self.rank = int(os.environ['RANK'])
        self.world_size = int(os.environ['WORLD_SIZE'])
        self.files = sorted(glob.glob(filename_pattern))
        self.reset()

    def reset(self):
        self.current_shard = -1
        self.advance()

    def advance(self):
        self.current_shard = (self.current_shard + 1) % len(self.files)
        self.current_position = 0
        self.tokens = _load_data_shard(self.files[self.current_shard])

    def next_batch(self, batch_size):
        assert batch_size % self.world_size == 0
        device_batch_size = batch_size // self.world_size
        # load next shard if necessary
        if self.current_position + batch_size + 1 >= len(self.tokens):
            self.advance()
        pos = self.current_position + self.rank * device_batch_size
        device_batch_tokens = self.tokens[pos:pos+device_batch_size+1]
        # advance current position
        self.current_position += batch_size
        inputs = device_batch_tokens[:-1].to(device='cuda', dtype=torch.int32, non_blocking=True)
        targets = device_batch_tokens[1:].to(device='cuda', dtype=torch.int64, non_blocking=True)
        return inputs, targets

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files = 'data/fineweb10B/fineweb_train_*.bin' # input .bin to train on
    val_files = 'data/fineweb10B/fineweb_val_*.bin' # input .bin to eval validation loss on
    val_tokens = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # optimization
    batch_size = 8*64*1024 # batch size in tokens
    num_iterations = 1375 # number of iterations to run
    cooldown_frac = 0.4 # fraction of training spent cooling down the learning rate
    bf16_embeds = True
    # evaluation and logging
    val_loss_every = 125 # every how many steps to evaluate val loss? 0 for only at the end
    # implementation
    max_device_batch_size = 64*1024 # batch size per device in tokens
    save_checkpoint = False
args = Hyperparameters()

micro_bs = args.max_device_batch_size

# set up DDP (distributed data parallel). torchrun sets this env variable
rank = int(os.environ['RANK'])
local_rank = int(os.environ['LOCAL_RANK'])
world_size = int(os.environ['WORLD_SIZE'])
assert torch.cuda.is_available()
torch.cuda.set_device(local_rank)
dist.init_process_group(backend='nccl', device_id=torch.device(local_rank))
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = uuid.uuid4()
    os.makedirs('logs', exist_ok=True)
    logfile = f'logs/{run_id}.txt'
    print(logfile)

def print0(s, console=False):
    if master_process:
        with open(logfile, 'a') as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0('='*100)
# log information about the hardware/software environment this is running on
print0(f'Running Python {sys.version}')
print0(f'Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}')
print0(subprocess.run(['nvidia-smi'], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout)
print0('='*100)

# load data
train_loader = DistributedDataLoader(args.train_files)
val_loader = DistributedDataLoader(args.val_files)
print0(f'Training dataloader files: {train_loader.files}')
print0(f'Validation dataloader files: {val_loader.files}')
print0('='*100)

# there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency. suggested to me by @Grad62304977.
# this originates from Karpathy's experiments.
model = GPT(vocab_size=50304, num_layers=12, num_heads=6, model_dim=768)
model = model.cuda()
if args.bf16_embeds:
    for m in model.modules():
        if isinstance(m, nn.Embedding):
            m.bfloat16()
model: GPT = torch.compile(model)
ddp_model = DDP(model, device_ids=[local_rank], broadcast_buffers=False, gradient_as_bucket_view=True)

# collect the parameters to optimize
hidden_matrix_params = [p for p in model.blocks.parameters() if p.ndim == 2]
embed_params = [model.embed.weight, *model.value_embeds.parameters()]
scalar_params = [p for n, p in model.named_parameters() if p.ndim < 2 and "attn_scale" not in n]
attn_scale_params = [p for n, p in model.named_parameters() if p.ndim < 2 and "attn_scale" in n]
head_params = [model.lm_head.weight]

# init the optimizer(s)
optimizer1 = torch.optim.Adam([dict(params=embed_params, lr=0.6),
                               dict(params=head_params, lr=0.008),
                               dict(params=scalar_params, lr=0.04),
                               dict(params=attn_scale_params, lr=0.01)],
                              betas=(0.8, 0.95), fused=True)
optimizer2 = Muon(hidden_matrix_params, lr=0.05, momentum=0.95)
optimizers = [optimizer1, optimizer2]

# learning rate schedule: stable then decay
def get_lr(it):
    t = 1 - it / args.num_iterations # time remaining in training
    assert 1 >= t > 0
    # 1) constant lr for first part of training
    if t >= args.cooldown_frac:
        return 1.0
    # 2) then linear cooldown
    else:
        return t / args.cooldown_frac
schedulers = [torch.optim.lr_scheduler.LambdaLR(opt, get_lr) for opt in optimizers]

# sliding window size schedule: linear increase over training in chunks of 128 from 128 -> 1792. By @fernbear.bsky.social
def get_sliding_window_blocks(it):
    x = it / args.num_iterations # training progress
    assert 0 <= x <= 1
    return int(((1 - x) * 128 + x * 1856) // 128)
sliding_window_num_blocks = torch.tensor(1, dtype=torch.int32, device='cuda')

# Start training loop
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    # This effectively ignores timing first 10 steps, which are slower for weird reasons.
    # Alternately, and slightly more correctly in terms of benchmarking, we could do 10
    # steps with dummy data first, and then re-initialize the model and reset the loader.
    if step == 10:
        training_time_ms = 0
        t0 = time.perf_counter()
    timed_steps = float('nan') if step <= 11 else (step - 10) + 1 # <= 11 to avoid bug in val

    sliding_window_num_blocks.copy_(get_sliding_window_blocks(step))

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        # run validation batches
        model.eval()
        val_loader.reset()
        val_loss = 0.0
        # calculate the number of steps to take in the val loop.
        val_batch_size = world_size * micro_bs
        assert args.val_tokens % val_batch_size == 0
        val_steps = args.val_tokens // val_batch_size
        for _ in range(val_steps):
            with torch.no_grad():
                inputs_val, targets_val = val_loader.next_batch(val_batch_size)
                val_loss += ddp_model(inputs_val, targets_val, sliding_window_num_blocks)
        # Print attention scales
        for n, p in model.named_parameters():
            if p.ndim < 2 and "attn_scale" in n:
                print0(f'{n}: {p.item()}', console=True)
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        val_loss /= val_steps
        # logging
        print0(f'step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/(timed_steps-1):.2f}ms', console=True)
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f'logs/{run_id}', exist_ok=True)
            torch.save(log, f'logs/{run_id}/state_step{step:06d}.pt')
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    model.train()
    batch_size = args.batch_size
    assert batch_size % world_size == 0
    inputs_train, targets_train = train_loader.next_batch(batch_size)
    assert len(inputs_train) <= micro_bs or len(inputs_train) % micro_bs == 0
    for micro_inputs_train, micro_targets_train in zip(inputs_train.split(micro_bs), targets_train.split(micro_bs)):
        ddp_model(micro_inputs_train, micro_targets_train, sliding_window_num_blocks).backward()
    # momentum warmup for Muon
    frac = min(step/300, 1)
    for group in optimizer2.param_groups:
        group['momentum'] = (1 - frac) * 0.85 + frac * 0.95
    # step the optimizers and schedulers
    for opt, sched in zip(optimizers, schedulers):
        opt.step()
        if step != train_steps-1:
            sched.step()
    # null the gradients
    model.zero_grad(set_to_none=True)
    # logging
    approx_time = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f'step:{step+1}/{train_steps} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms', console=True)

print0(f'peak memory consumption: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB')
dist.destroy_process_group()

====================================================================================================
Running Python 3.10.12 (main, Sep 11 2024, 15:47:36) [GCC 11.4.0]
Running PyTorch 2.6.0.dev20241231+cu126 compiled for CUDA 12.6
Tue Jan 14 16:29:55 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.183.06             Driver Version: 535.183.06   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:19:00.0 Off |                    0 |
| N/A   39C    P0             126W / 700W |   7713MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:3B:00.0 Off |                    0 |
| N/A   32C    P0             114W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:4C:00.0 Off |                    0 |
| N/A   29C    P0             118W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   37C    P0             129W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:9B:00.0 Off |                    0 |
| N/A   38C    P0             123W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:BB:00.0 Off |                    0 |
| N/A   30C    P0             115W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:CB:00.0 Off |                    0 |
| N/A   37C    P0             116W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:DB:00.0 Off |                    0 |
| N/A   30C    P0             117W / 700W |   3211MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
Training dataloader files: ['data/fineweb10B/fineweb_train_000001.bin', 'data/fineweb10B/fineweb_train_000002.bin', 'data/fineweb10B/fineweb_train_000003.bin', 'data/fineweb10B/fineweb_train_000004.bin', 'data/fineweb10B/fineweb_train_000005.bin', 'data/fineweb10B/fineweb_train_000006.bin', 'data/fineweb10B/fineweb_train_000007.bin', 'data/fineweb10B/fineweb_train_000008.bin', 'data/fineweb10B/fineweb_train_000009.bin']
Validation dataloader files: ['data/fineweb10B/fineweb_val_000000.bin']
====================================================================================================
step:0/1375 val_loss:10.8258 train_time:0ms step_avg:nanms
step:1/1375 train_time:27569ms step_avg:nanms
step:2/1375 train_time:27636ms step_avg:nanms
step:3/1375 train_time:27820ms step_avg:nanms
step:4/1375 train_time:27951ms step_avg:nanms
step:5/1375 train_time:28086ms step_avg:nanms
step:6/1375 train_time:28219ms step_avg:nanms
step:7/1375 train_time:28352ms step_avg:nanms
step:8/1375 train_time:28486ms step_avg:nanms
step:9/1375 train_time:28620ms step_avg:nanms
step:10/1375 train_time:28762ms step_avg:nanms
step:11/1375 train_time:136ms step_avg:nanms
step:12/1375 train_time:271ms step_avg:nanms
step:13/1375 train_time:406ms step_avg:135.43ms
step:14/1375 train_time:541ms step_avg:135.13ms
step:15/1375 train_time:675ms step_avg:135.08ms
step:16/1375 train_time:810ms step_avg:134.92ms
step:17/1375 train_time:946ms step_avg:135.09ms
step:18/1375 train_time:1084ms step_avg:135.54ms
step:19/1375 train_time:1221ms step_avg:135.68ms
step:20/1375 train_time:1356ms step_avg:135.60ms
step:21/1375 train_time:1490ms step_avg:135.48ms
step:22/1375 train_time:1626ms step_avg:135.52ms
step:23/1375 train_time:1760ms step_avg:135.41ms
step:24/1375 train_time:1897ms step_avg:135.47ms
step:25/1375 train_time:2032ms step_avg:135.45ms
step:26/1375 train_time:2169ms step_avg:135.55ms
step:27/1375 train_time:2305ms step_avg:135.61ms
step:28/1375 train_time:2442ms step_avg:135.64ms
step:29/1375 train_time:2578ms step_avg:135.69ms
step:30/1375 train_time:2713ms step_avg:135.64ms
step:31/1375 train_time:2847ms step_avg:135.56ms
step:32/1375 train_time:2984ms step_avg:135.62ms
step:33/1375 train_time:3117ms step_avg:135.54ms
step:34/1375 train_time:3254ms step_avg:135.60ms
step:35/1375 train_time:3390ms step_avg:135.62ms
step:36/1375 train_time:3527ms step_avg:135.64ms
step:37/1375 train_time:3662ms step_avg:135.61ms
step:38/1375 train_time:3797ms step_avg:135.61ms
step:39/1375 train_time:3931ms step_avg:135.54ms
step:40/1375 train_time:4065ms step_avg:135.51ms
step:41/1375 train_time:4202ms step_avg:135.55ms
step:42/1375 train_time:4337ms step_avg:135.54ms
step:43/1375 train_time:4475ms step_avg:135.60ms
step:44/1375 train_time:4610ms step_avg:135.60ms
step:45/1375 train_time:4746ms step_avg:135.61ms
step:46/1375 train_time:4882ms step_avg:135.62ms
step:47/1375 train_time:5016ms step_avg:135.56ms
step:48/1375 train_time:5151ms step_avg:135.55ms
step:49/1375 train_time:5286ms step_avg:135.53ms
step:50/1375 train_time:5423ms step_avg:135.58ms
step:51/1375 train_time:5560ms step_avg:135.60ms
step:52/1375 train_time:5696ms step_avg:135.61ms
step:53/1375 train_time:5830ms step_avg:135.59ms
step:54/1375 train_time:5964ms step_avg:135.55ms
step:55/1375 train_time:6102ms step_avg:135.60ms
step:56/1375 train_time:6235ms step_avg:135.55ms
step:57/1375 train_time:6371ms step_avg:135.56ms
step:58/1375 train_time:6507ms step_avg:135.55ms
step:59/1375 train_time:6643ms step_avg:135.57ms
step:60/1375 train_time:6777ms step_avg:135.54ms
step:61/1375 train_time:6912ms step_avg:135.53ms
step:62/1375 train_time:7047ms step_avg:135.52ms
step:63/1375 train_time:7182ms step_avg:135.51ms
step:64/1375 train_time:7316ms step_avg:135.49ms
step:65/1375 train_time:7452ms step_avg:135.49ms
step:66/1375 train_time:7588ms step_avg:135.51ms
step:67/1375 train_time:7724ms step_avg:135.50ms
step:68/1375 train_time:7858ms step_avg:135.48ms
step:69/1375 train_time:7992ms step_avg:135.46ms
step:70/1375 train_time:8127ms step_avg:135.45ms
step:71/1375 train_time:8262ms step_avg:135.45ms
step:72/1375 train_time:8399ms step_avg:135.46ms
step:73/1375 train_time:8533ms step_avg:135.45ms
step:74/1375 train_time:8668ms step_avg:135.44ms
step:75/1375 train_time:8805ms step_avg:135.45ms
step:76/1375 train_time:8940ms step_avg:135.46ms
step:77/1375 train_time:9074ms step_avg:135.44ms
step:78/1375 train_time:9210ms step_avg:135.45ms
step:79/1375 train_time:9345ms step_avg:135.44ms
step:80/1375 train_time:9482ms step_avg:135.46ms
step:81/1375 train_time:9616ms step_avg:135.44ms
step:82/1375 train_time:9752ms step_avg:135.44ms
step:83/1375 train_time:9887ms step_avg:135.44ms
step:84/1375 train_time:10024ms step_avg:135.46ms
step:85/1375 train_time:10158ms step_avg:135.44ms
step:86/1375 train_time:10293ms step_avg:135.44ms
step:87/1375 train_time:10428ms step_avg:135.43ms
step:88/1375 train_time:10564ms step_avg:135.44ms
step:89/1375 train_time:10703ms step_avg:135.48ms
step:90/1375 train_time:10838ms step_avg:135.47ms
step:91/1375 train_time:10973ms step_avg:135.47ms
step:92/1375 train_time:11108ms step_avg:135.46ms
step:93/1375 train_time:11244ms step_avg:135.47ms
step:94/1375 train_time:11380ms step_avg:135.48ms
step:95/1375 train_time:11517ms step_avg:135.49ms
step:96/1375 train_time:11652ms step_avg:135.49ms
step:97/1375 train_time:11788ms step_avg:135.49ms
step:98/1375 train_time:11924ms step_avg:135.50ms
step:99/1375 train_time:12059ms step_avg:135.49ms
step:100/1375 train_time:12194ms step_avg:135.49ms
step:101/1375 train_time:12329ms step_avg:135.48ms
step:102/1375 train_time:12465ms step_avg:135.48ms
step:103/1375 train_time:12604ms step_avg:135.52ms
step:104/1375 train_time:12742ms step_avg:135.56ms
step:105/1375 train_time:12880ms step_avg:135.58ms
step:106/1375 train_time:13018ms step_avg:135.61ms
step:107/1375 train_time:13156ms step_avg:135.63ms
step:108/1375 train_time:13295ms step_avg:135.66ms
step:109/1375 train_time:13433ms step_avg:135.69ms
step:110/1375 train_time:13571ms step_avg:135.71ms
step:111/1375 train_time:13711ms step_avg:135.75ms
step:112/1375 train_time:13851ms step_avg:135.80ms
step:113/1375 train_time:13990ms step_avg:135.83ms
step:114/1375 train_time:14129ms step_avg:135.85ms
step:115/1375 train_time:14268ms step_avg:135.89ms
step:116/1375 train_time:14407ms step_avg:135.92ms
step:117/1375 train_time:14546ms step_avg:135.94ms
step:118/1375 train_time:14685ms step_avg:135.97ms
step:119/1375 train_time:14824ms step_avg:136.00ms
step:120/1375 train_time:14962ms step_avg:136.02ms
step:121/1375 train_time:15100ms step_avg:136.04ms
step:122/1375 train_time:15237ms step_avg:136.05ms
step:123/1375 train_time:15376ms step_avg:136.07ms
step:124/1375 train_time:15515ms step_avg:136.10ms
step:125/1375 train_time:15654ms step_avg:136.12ms
step:125/1375 val_loss:4.3693 train_time:15722ms step_avg:136.71ms
step:126/1375 train_time:15795ms step_avg:136.16ms
step:127/1375 train_time:15941ms step_avg:136.25ms
step:128/1375 train_time:16080ms step_avg:136.27ms
step:129/1375 train_time:16217ms step_avg:136.27ms
step:130/1375 train_time:16354ms step_avg:136.28ms
step:131/1375 train_time:16491ms step_avg:136.29ms
step:132/1375 train_time:16629ms step_avg:136.31ms
step:133/1375 train_time:16770ms step_avg:136.34ms
step:134/1375 train_time:16910ms step_avg:136.37ms
step:135/1375 train_time:17052ms step_avg:136.41ms
step:136/1375 train_time:17190ms step_avg:136.43ms
step:137/1375 train_time:17328ms step_avg:136.44ms
step:138/1375 train_time:17468ms step_avg:136.47ms
step:139/1375 train_time:17607ms step_avg:136.49ms
step:140/1375 train_time:17748ms step_avg:136.53ms
step:141/1375 train_time:17889ms step_avg:136.56ms
step:142/1375 train_time:18029ms step_avg:136.59ms
step:143/1375 train_time:18169ms step_avg:136.61ms
step:144/1375 train_time:18308ms step_avg:136.63ms
step:145/1375 train_time:18450ms step_avg:136.67ms
step:146/1375 train_time:18589ms step_avg:136.68ms
step:147/1375 train_time:18729ms step_avg:136.70ms
step:148/1375 train_time:18868ms step_avg:136.72ms
step:149/1375 train_time:19006ms step_avg:136.73ms
step:150/1375 train_time:19148ms step_avg:136.77ms
step:151/1375 train_time:19288ms step_avg:136.79ms
step:152/1375 train_time:19427ms step_avg:136.81ms
step:153/1375 train_time:19566ms step_avg:136.83ms
step:154/1375 train_time:19707ms step_avg:136.86ms
step:155/1375 train_time:19848ms step_avg:136.88ms
step:156/1375 train_time:19988ms step_avg:136.91ms
step:157/1375 train_time:20128ms step_avg:136.93ms
step:158/1375 train_time:20268ms step_avg:136.95ms
step:159/1375 train_time:20407ms step_avg:136.96ms
step:160/1375 train_time:20548ms step_avg:136.98ms
step:161/1375 train_time:20687ms step_avg:137.00ms
step:162/1375 train_time:20825ms step_avg:137.01ms
step:163/1375 train_time:20965ms step_avg:137.02ms
step:164/1375 train_time:21104ms step_avg:137.04ms
step:165/1375 train_time:21244ms step_avg:137.06ms
step:166/1375 train_time:21382ms step_avg:137.07ms
step:167/1375 train_time:21521ms step_avg:137.08ms
step:168/1375 train_time:21661ms step_avg:137.10ms
step:169/1375 train_time:21800ms step_avg:137.11ms
step:170/1375 train_time:21939ms step_avg:137.12ms
step:171/1375 train_time:22078ms step_avg:137.13ms
step:172/1375 train_time:22219ms step_avg:137.15ms
step:173/1375 train_time:22357ms step_avg:137.16ms
step:174/1375 train_time:22497ms step_avg:137.18ms
step:175/1375 train_time:22636ms step_avg:137.19ms
step:176/1375 train_time:22773ms step_avg:137.19ms
step:177/1375 train_time:22912ms step_avg:137.20ms
step:178/1375 train_time:23052ms step_avg:137.21ms
step:179/1375 train_time:23191ms step_avg:137.22ms
step:180/1375 train_time:23331ms step_avg:137.24ms
step:181/1375 train_time:23469ms step_avg:137.25ms
step:182/1375 train_time:23607ms step_avg:137.25ms
step:183/1375 train_time:23748ms step_avg:137.27ms
step:184/1375 train_time:23887ms step_avg:137.28ms
step:185/1375 train_time:24027ms step_avg:137.30ms
step:186/1375 train_time:24167ms step_avg:137.31ms
step:187/1375 train_time:24306ms step_avg:137.32ms
step:188/1375 train_time:24446ms step_avg:137.34ms
step:189/1375 train_time:24585ms step_avg:137.34ms
step:190/1375 train_time:24724ms step_avg:137.35ms
step:191/1375 train_time:24902ms step_avg:137.58ms
step:192/1375 train_time:25046ms step_avg:137.61ms
step:193/1375 train_time:25182ms step_avg:137.61ms
step:194/1375 train_time:25319ms step_avg:137.60ms
step:195/1375 train_time:25457ms step_avg:137.61ms
step:196/1375 train_time:25594ms step_avg:137.60ms
step:197/1375 train_time:25733ms step_avg:137.61ms
step:198/1375 train_time:25876ms step_avg:137.64ms
step:199/1375 train_time:26017ms step_avg:137.65ms
step:200/1375 train_time:26156ms step_avg:137.66ms
step:201/1375 train_time:26294ms step_avg:137.67ms
step:202/1375 train_time:26432ms step_avg:137.67ms
step:203/1375 train_time:26570ms step_avg:137.67ms
step:204/1375 train_time:26709ms step_avg:137.68ms
step:205/1375 train_time:26851ms step_avg:137.70ms
step:206/1375 train_time:26996ms step_avg:137.73ms
step:207/1375 train_time:27139ms step_avg:137.76ms
step:208/1375 train_time:27280ms step_avg:137.78ms
step:209/1375 train_time:27421ms step_avg:137.79ms
step:210/1375 train_time:27562ms step_avg:137.81ms
step:211/1375 train_time:27704ms step_avg:137.83ms
step:212/1375 train_time:27846ms step_avg:137.85ms
step:213/1375 train_time:27989ms step_avg:137.87ms
step:214/1375 train_time:28132ms step_avg:137.90ms
step:215/1375 train_time:28272ms step_avg:137.91ms
step:216/1375 train_time:28412ms step_avg:137.92ms
step:217/1375 train_time:28554ms step_avg:137.94ms
step:218/1375 train_time:28695ms step_avg:137.95ms
step:219/1375 train_time:28836ms step_avg:137.97ms
step:220/1375 train_time:28978ms step_avg:137.99ms
step:221/1375 train_time:29120ms step_avg:138.01ms
step:222/1375 train_time:29262ms step_avg:138.03ms
step:223/1375 train_time:29403ms step_avg:138.04ms
step:224/1375 train_time:29545ms step_avg:138.06ms
step:225/1375 train_time:29686ms step_avg:138.08ms
step:226/1375 train_time:29829ms step_avg:138.10ms
step:227/1375 train_time:29971ms step_avg:138.11ms
step:228/1375 train_time:30111ms step_avg:138.12ms
step:229/1375 train_time:30253ms step_avg:138.14ms
step:230/1375 train_time:30395ms step_avg:138.16ms
step:231/1375 train_time:30537ms step_avg:138.18ms
step:232/1375 train_time:30677ms step_avg:138.18ms
step:233/1375 train_time:30819ms step_avg:138.20ms
step:234/1375 train_time:30960ms step_avg:138.22ms
step:235/1375 train_time:31102ms step_avg:138.23ms
step:236/1375 train_time:31243ms step_avg:138.25ms
step:237/1375 train_time:31387ms step_avg:138.27ms
step:238/1375 train_time:31529ms step_avg:138.28ms
step:239/1375 train_time:31671ms step_avg:138.30ms
step:240/1375 train_time:31812ms step_avg:138.31ms
step:241/1375 train_time:31954ms step_avg:138.33ms
step:242/1375 train_time:32095ms step_avg:138.34ms
step:243/1375 train_time:32237ms step_avg:138.36ms
step:244/1375 train_time:32377ms step_avg:138.36ms
step:245/1375 train_time:32518ms step_avg:138.37ms
step:246/1375 train_time:32660ms step_avg:138.39ms
step:247/1375 train_time:32803ms step_avg:138.41ms
step:248/1375 train_time:32947ms step_avg:138.43ms
step:249/1375 train_time:33087ms step_avg:138.44ms
step:250/1375 train_time:33230ms step_avg:138.46ms
step:250/1375 val_loss:3.9548 train_time:33298ms step_avg:138.74ms
step:251/1375 train_time:33372ms step_avg:138.47ms
step:252/1375 train_time:33517ms step_avg:138.50ms
step:253/1375 train_time:33660ms step_avg:138.52ms
step:254/1375 train_time:33800ms step_avg:138.52ms
step:255/1375 train_time:33940ms step_avg:138.53ms
step:256/1375 train_time:34081ms step_avg:138.54ms
step:257/1375 train_time:34221ms step_avg:138.55ms
step:258/1375 train_time:34368ms step_avg:138.58ms
step:259/1375 train_time:34509ms step_avg:138.59ms
step:260/1375 train_time:34652ms step_avg:138.61ms
step:261/1375 train_time:34793ms step_avg:138.62ms
step:262/1375 train_time:34936ms step_avg:138.64ms
step:263/1375 train_time:35079ms step_avg:138.65ms
step:264/1375 train_time:35221ms step_avg:138.66ms
step:265/1375 train_time:35363ms step_avg:138.68ms
step:266/1375 train_time:35505ms step_avg:138.69ms
step:267/1375 train_time:35647ms step_avg:138.70ms
step:268/1375 train_time:35788ms step_avg:138.71ms
step:269/1375 train_time:35929ms step_avg:138.72ms
step:270/1375 train_time:36073ms step_avg:138.74ms
step:271/1375 train_time:36215ms step_avg:138.75ms
step:272/1375 train_time:36356ms step_avg:138.76ms
step:273/1375 train_time:36497ms step_avg:138.77ms
step:274/1375 train_time:36638ms step_avg:138.78ms
step:275/1375 train_time:36779ms step_avg:138.79ms
step:276/1375 train_time:36919ms step_avg:138.79ms
step:277/1375 train_time:37061ms step_avg:138.81ms
step:278/1375 train_time:37202ms step_avg:138.81ms
step:279/1375 train_time:37343ms step_avg:138.82ms
step:280/1375 train_time:37485ms step_avg:138.83ms
step:281/1375 train_time:37627ms step_avg:138.85ms
step:282/1375 train_time:37770ms step_avg:138.86ms
step:283/1375 train_time:37911ms step_avg:138.87ms
step:284/1375 train_time:38052ms step_avg:138.88ms
step:285/1375 train_time:38194ms step_avg:138.89ms
step:286/1375 train_time:38336ms step_avg:138.90ms
step:287/1375 train_time:38480ms step_avg:138.92ms
step:288/1375 train_time:38621ms step_avg:138.93ms
step:289/1375 train_time:38762ms step_avg:138.93ms
step:290/1375 train_time:38903ms step_avg:138.94ms
step:291/1375 train_time:39045ms step_avg:138.95ms
step:292/1375 train_time:39187ms step_avg:138.96ms
step:293/1375 train_time:39328ms step_avg:138.97ms
step:294/1375 train_time:39471ms step_avg:138.98ms
step:295/1375 train_time:39613ms step_avg:138.99ms
step:296/1375 train_time:39756ms step_avg:139.01ms
step:297/1375 train_time:39899ms step_avg:139.02ms
step:298/1375 train_time:40041ms step_avg:139.03ms
step:299/1375 train_time:40182ms step_avg:139.04ms
step:300/1375 train_time:40323ms step_avg:139.04ms
step:301/1375 train_time:40467ms step_avg:139.06ms
step:302/1375 train_time:40608ms step_avg:139.07ms
step:303/1375 train_time:40750ms step_avg:139.08ms
step:304/1375 train_time:40892ms step_avg:139.09ms
step:305/1375 train_time:41032ms step_avg:139.09ms
step:306/1375 train_time:41176ms step_avg:139.11ms
step:307/1375 train_time:41320ms step_avg:139.12ms
step:308/1375 train_time:41464ms step_avg:139.14ms
step:309/1375 train_time:41607ms step_avg:139.15ms
step:310/1375 train_time:41751ms step_avg:139.17ms
step:311/1375 train_time:41893ms step_avg:139.18ms
step:312/1375 train_time:42036ms step_avg:139.19ms
step:313/1375 train_time:42183ms step_avg:139.22ms
step:314/1375 train_time:42324ms step_avg:139.23ms
step:315/1375 train_time:42469ms step_avg:139.24ms
step:316/1375 train_time:42613ms step_avg:139.26ms
step:317/1375 train_time:42758ms step_avg:139.28ms
step:318/1375 train_time:42900ms step_avg:139.29ms
step:319/1375 train_time:43045ms step_avg:139.30ms
step:320/1375 train_time:43189ms step_avg:139.32ms
step:321/1375 train_time:43332ms step_avg:139.33ms
step:322/1375 train_time:43476ms step_avg:139.35ms
step:323/1375 train_time:43620ms step_avg:139.36ms
step:324/1375 train_time:43764ms step_avg:139.38ms
step:325/1375 train_time:43907ms step_avg:139.39ms
step:326/1375 train_time:44052ms step_avg:139.40ms
step:327/1375 train_time:44195ms step_avg:139.42ms
step:328/1375 train_time:44339ms step_avg:139.43ms
step:329/1375 train_time:44485ms step_avg:139.45ms
step:330/1375 train_time:44628ms step_avg:139.46ms
step:331/1375 train_time:44772ms step_avg:139.48ms
step:332/1375 train_time:44915ms step_avg:139.49ms
step:333/1375 train_time:45062ms step_avg:139.51ms
step:334/1375 train_time:45204ms step_avg:139.52ms
step:335/1375 train_time:45349ms step_avg:139.54ms
step:336/1375 train_time:45493ms step_avg:139.55ms
step:337/1375 train_time:45636ms step_avg:139.56ms
step:338/1375 train_time:45782ms step_avg:139.58ms
step:339/1375 train_time:45926ms step_avg:139.59ms
step:340/1375 train_time:46070ms step_avg:139.61ms
step:341/1375 train_time:46214ms step_avg:139.62ms
step:342/1375 train_time:46360ms step_avg:139.64ms
step:343/1375 train_time:46503ms step_avg:139.65ms
step:344/1375 train_time:46647ms step_avg:139.66ms
step:345/1375 train_time:46792ms step_avg:139.68ms
step:346/1375 train_time:46936ms step_avg:139.69ms
step:347/1375 train_time:47081ms step_avg:139.71ms
step:348/1375 train_time:47224ms step_avg:139.72ms
step:349/1375 train_time:47370ms step_avg:139.74ms
step:350/1375 train_time:47512ms step_avg:139.74ms
step:351/1375 train_time:47656ms step_avg:139.75ms
step:352/1375 train_time:47800ms step_avg:139.77ms
step:353/1375 train_time:47946ms step_avg:139.78ms
step:354/1375 train_time:48089ms step_avg:139.79ms
step:355/1375 train_time:48232ms step_avg:139.80ms
step:356/1375 train_time:48377ms step_avg:139.82ms
step:357/1375 train_time:48522ms step_avg:139.83ms
step:358/1375 train_time:48666ms step_avg:139.85ms
step:359/1375 train_time:48808ms step_avg:139.85ms
step:360/1375 train_time:48953ms step_avg:139.87ms
step:361/1375 train_time:49096ms step_avg:139.88ms
step:362/1375 train_time:49241ms step_avg:139.89ms
step:363/1375 train_time:49386ms step_avg:139.90ms
step:364/1375 train_time:49530ms step_avg:139.91ms
step:365/1375 train_time:49674ms step_avg:139.93ms
step:366/1375 train_time:49819ms step_avg:139.94ms
step:367/1375 train_time:49963ms step_avg:139.95ms
step:368/1375 train_time:50106ms step_avg:139.96ms
step:369/1375 train_time:50251ms step_avg:139.98ms
step:370/1375 train_time:50395ms step_avg:139.99ms
step:371/1375 train_time:50540ms step_avg:140.00ms
step:372/1375 train_time:50685ms step_avg:140.01ms
step:373/1375 train_time:50828ms step_avg:140.02ms
step:374/1375 train_time:50973ms step_avg:140.03ms
step:375/1375 train_time:51116ms step_avg:140.04ms
step:375/1375 val_loss:3.7742 train_time:51186ms step_avg:140.24ms
step:376/1375 train_time:51267ms step_avg:140.07ms
step:377/1375 train_time:51411ms step_avg:140.08ms
step:378/1375 train_time:51555ms step_avg:140.10ms
step:379/1375 train_time:51698ms step_avg:140.10ms
step:380/1375 train_time:51842ms step_avg:140.11ms
step:381/1375 train_time:52029ms step_avg:140.24ms
step:382/1375 train_time:52177ms step_avg:140.26ms
step:383/1375 train_time:52321ms step_avg:140.27ms
step:384/1375 train_time:52463ms step_avg:140.27ms
step:385/1375 train_time:52606ms step_avg:140.28ms
step:386/1375 train_time:52748ms step_avg:140.29ms
step:387/1375 train_time:52893ms step_avg:140.30ms
step:388/1375 train_time:53038ms step_avg:140.31ms
step:389/1375 train_time:53187ms step_avg:140.33ms
step:390/1375 train_time:53330ms step_avg:140.34ms
step:391/1375 train_time:53474ms step_avg:140.35ms
step:392/1375 train_time:53616ms step_avg:140.36ms
step:393/1375 train_time:53760ms step_avg:140.36ms
step:394/1375 train_time:53902ms step_avg:140.37ms
step:395/1375 train_time:54047ms step_avg:140.38ms
step:396/1375 train_time:54192ms step_avg:140.39ms
step:397/1375 train_time:54337ms step_avg:140.41ms
step:398/1375 train_time:54482ms step_avg:140.42ms
step:399/1375 train_time:54625ms step_avg:140.42ms
step:400/1375 train_time:54768ms step_avg:140.43ms
step:401/1375 train_time:54912ms step_avg:140.44ms
step:402/1375 train_time:55057ms step_avg:140.45ms
step:403/1375 train_time:55200ms step_avg:140.46ms
step:404/1375 train_time:55344ms step_avg:140.47ms
step:405/1375 train_time:55489ms step_avg:140.48ms
step:406/1375 train_time:55632ms step_avg:140.49ms
step:407/1375 train_time:55777ms step_avg:140.50ms
step:408/1375 train_time:55921ms step_avg:140.50ms
step:409/1375 train_time:56065ms step_avg:140.51ms
step:410/1375 train_time:56211ms step_avg:140.53ms
step:411/1375 train_time:56357ms step_avg:140.54ms
step:412/1375 train_time:56504ms step_avg:140.56ms
step:413/1375 train_time:56649ms step_avg:140.57ms
step:414/1375 train_time:56794ms step_avg:140.58ms
step:415/1375 train_time:56939ms step_avg:140.59ms
step:416/1375 train_time:57086ms step_avg:140.60ms
step:417/1375 train_time:57232ms step_avg:140.62ms
step:418/1375 train_time:57379ms step_avg:140.63ms
step:419/1375 train_time:57525ms step_avg:140.65ms
step:420/1375 train_time:57672ms step_avg:140.66ms
step:421/1375 train_time:57816ms step_avg:140.67ms
step:422/1375 train_time:57963ms step_avg:140.69ms
step:423/1375 train_time:58109ms step_avg:140.70ms
step:424/1375 train_time:58254ms step_avg:140.71ms
step:425/1375 train_time:58401ms step_avg:140.72ms
step:426/1375 train_time:58547ms step_avg:140.74ms
step:427/1375 train_time:58692ms step_avg:140.75ms
step:428/1375 train_time:58839ms step_avg:140.76ms
step:429/1375 train_time:58985ms step_avg:140.77ms
step:430/1375 train_time:59129ms step_avg:140.78ms
step:431/1375 train_time:59275ms step_avg:140.80ms
step:432/1375 train_time:59420ms step_avg:140.81ms
step:433/1375 train_time:59565ms step_avg:140.82ms
step:434/1375 train_time:59710ms step_avg:140.83ms
step:435/1375 train_time:59858ms step_avg:140.84ms
step:436/1375 train_time:60002ms step_avg:140.85ms
step:437/1375 train_time:60147ms step_avg:140.86ms
step:438/1375 train_time:60294ms step_avg:140.87ms
step:439/1375 train_time:60440ms step_avg:140.89ms
step:440/1375 train_time:60587ms step_avg:140.90ms
step:441/1375 train_time:60730ms step_avg:140.91ms
step:442/1375 train_time:60877ms step_avg:140.92ms
step:443/1375 train_time:61023ms step_avg:140.93ms
step:444/1375 train_time:61168ms step_avg:140.94ms
step:445/1375 train_time:61314ms step_avg:140.95ms
step:446/1375 train_time:61459ms step_avg:140.96ms
step:447/1375 train_time:61605ms step_avg:140.97ms
step:448/1375 train_time:61751ms step_avg:140.98ms
step:449/1375 train_time:61896ms step_avg:140.99ms
step:450/1375 train_time:62042ms step_avg:141.01ms
step:451/1375 train_time:62190ms step_avg:141.02ms
step:452/1375 train_time:62335ms step_avg:141.03ms
step:453/1375 train_time:62483ms step_avg:141.04ms
step:454/1375 train_time:62626ms step_avg:141.05ms
step:455/1375 train_time:62773ms step_avg:141.06ms
step:456/1375 train_time:62918ms step_avg:141.07ms
step:457/1375 train_time:63065ms step_avg:141.08ms
step:458/1375 train_time:63210ms step_avg:141.09ms
step:459/1375 train_time:63358ms step_avg:141.11ms
step:460/1375 train_time:63504ms step_avg:141.12ms
step:461/1375 train_time:63649ms step_avg:141.13ms
step:462/1375 train_time:63794ms step_avg:141.14ms
step:463/1375 train_time:63940ms step_avg:141.15ms
step:464/1375 train_time:64087ms step_avg:141.16ms
step:465/1375 train_time:64232ms step_avg:141.17ms
step:466/1375 train_time:64379ms step_avg:141.18ms
step:467/1375 train_time:64526ms step_avg:141.19ms
step:468/1375 train_time:64673ms step_avg:141.21ms
step:469/1375 train_time:64817ms step_avg:141.21ms
step:470/1375 train_time:64963ms step_avg:141.22ms
step:471/1375 train_time:65108ms step_avg:141.23ms
step:472/1375 train_time:65254ms step_avg:141.24ms
step:473/1375 train_time:65400ms step_avg:141.25ms
step:474/1375 train_time:65545ms step_avg:141.26ms
step:475/1375 train_time:65691ms step_avg:141.27ms
step:476/1375 train_time:65837ms step_avg:141.28ms
step:477/1375 train_time:65984ms step_avg:141.29ms
step:478/1375 train_time:66129ms step_avg:141.30ms
step:479/1375 train_time:66277ms step_avg:141.32ms
step:480/1375 train_time:66423ms step_avg:141.32ms
step:481/1375 train_time:66568ms step_avg:141.33ms
step:482/1375 train_time:66713ms step_avg:141.34ms
step:483/1375 train_time:66859ms step_avg:141.35ms
step:484/1375 train_time:67004ms step_avg:141.36ms
step:485/1375 train_time:67151ms step_avg:141.37ms
step:486/1375 train_time:67298ms step_avg:141.38ms
step:487/1375 train_time:67443ms step_avg:141.39ms
step:488/1375 train_time:67590ms step_avg:141.40ms
step:489/1375 train_time:67736ms step_avg:141.41ms
step:490/1375 train_time:67882ms step_avg:141.42ms
step:491/1375 train_time:68028ms step_avg:141.43ms
step:492/1375 train_time:68174ms step_avg:141.44ms
step:493/1375 train_time:68320ms step_avg:141.45ms
step:494/1375 train_time:68465ms step_avg:141.46ms
step:495/1375 train_time:68611ms step_avg:141.47ms
step:496/1375 train_time:68758ms step_avg:141.48ms
step:497/1375 train_time:68902ms step_avg:141.48ms
step:498/1375 train_time:69049ms step_avg:141.49ms
step:499/1375 train_time:69196ms step_avg:141.51ms
step:500/1375 train_time:69341ms step_avg:141.51ms
step:500/1375 val_loss:3.6561 train_time:69412ms step_avg:141.66ms
step:501/1375 train_time:69487ms step_avg:141.52ms
step:502/1375 train_time:69634ms step_avg:141.53ms
step:503/1375 train_time:69781ms step_avg:141.54ms
step:504/1375 train_time:69926ms step_avg:141.55ms
step:505/1375 train_time:70071ms step_avg:141.56ms
step:506/1375 train_time:70215ms step_avg:141.56ms
step:507/1375 train_time:70364ms step_avg:141.58ms
step:508/1375 train_time:70511ms step_avg:141.59ms
step:509/1375 train_time:70658ms step_avg:141.60ms
step:510/1375 train_time:70802ms step_avg:141.60ms
step:511/1375 train_time:70949ms step_avg:141.61ms
step:512/1375 train_time:71098ms step_avg:141.63ms
step:513/1375 train_time:71247ms step_avg:141.64ms
step:514/1375 train_time:71395ms step_avg:141.66ms
step:515/1375 train_time:71543ms step_avg:141.67ms
step:516/1375 train_time:71691ms step_avg:141.68ms
step:517/1375 train_time:71838ms step_avg:141.69ms
step:518/1375 train_time:71985ms step_avg:141.70ms
step:519/1375 train_time:72133ms step_avg:141.71ms
step:520/1375 train_time:72280ms step_avg:141.73ms
step:521/1375 train_time:72427ms step_avg:141.74ms
step:522/1375 train_time:72576ms step_avg:141.75ms
step:523/1375 train_time:72722ms step_avg:141.76ms
step:524/1375 train_time:72871ms step_avg:141.77ms
step:525/1375 train_time:73018ms step_avg:141.78ms
step:526/1375 train_time:73166ms step_avg:141.79ms
step:527/1375 train_time:73312ms step_avg:141.80ms
step:528/1375 train_time:73460ms step_avg:141.82ms
step:529/1375 train_time:73606ms step_avg:141.82ms
step:530/1375 train_time:73754ms step_avg:141.84ms
step:531/1375 train_time:73902ms step_avg:141.85ms
step:532/1375 train_time:74050ms step_avg:141.86ms
step:533/1375 train_time:74198ms step_avg:141.87ms
step:534/1375 train_time:74345ms step_avg:141.88ms
step:535/1375 train_time:74492ms step_avg:141.89ms
step:536/1375 train_time:74641ms step_avg:141.90ms
step:537/1375 train_time:74788ms step_avg:141.91ms
step:538/1375 train_time:74936ms step_avg:141.92ms
step:539/1375 train_time:75084ms step_avg:141.94ms
step:540/1375 train_time:75231ms step_avg:141.94ms
step:541/1375 train_time:75379ms step_avg:141.96ms
step:542/1375 train_time:75526ms step_avg:141.97ms
step:543/1375 train_time:75676ms step_avg:141.98ms
step:544/1375 train_time:75822ms step_avg:141.99ms
step:545/1375 train_time:75970ms step_avg:142.00ms
step:546/1375 train_time:76118ms step_avg:142.01ms
step:547/1375 train_time:76266ms step_avg:142.02ms
step:548/1375 train_time:76413ms step_avg:142.03ms
step:549/1375 train_time:76562ms step_avg:142.04ms
step:550/1375 train_time:76709ms step_avg:142.05ms
step:551/1375 train_time:76858ms step_avg:142.07ms
step:552/1375 train_time:77005ms step_avg:142.08ms
step:553/1375 train_time:77152ms step_avg:142.09ms
step:554/1375 train_time:77301ms step_avg:142.10ms
step:555/1375 train_time:77447ms step_avg:142.10ms
step:556/1375 train_time:77593ms step_avg:142.11ms
step:557/1375 train_time:77742ms step_avg:142.12ms
step:558/1375 train_time:77888ms step_avg:142.13ms
step:559/1375 train_time:78035ms step_avg:142.14ms
step:560/1375 train_time:78184ms step_avg:142.15ms
step:561/1375 train_time:78331ms step_avg:142.16ms
step:562/1375 train_time:78479ms step_avg:142.17ms
step:563/1375 train_time:78627ms step_avg:142.18ms
step:564/1375 train_time:78774ms step_avg:142.19ms
step:565/1375 train_time:78921ms step_avg:142.20ms
step:566/1375 train_time:79069ms step_avg:142.21ms
step:567/1375 train_time:79217ms step_avg:142.22ms
step:568/1375 train_time:79364ms step_avg:142.23ms
step:569/1375 train_time:79510ms step_avg:142.24ms
step:570/1375 train_time:79657ms step_avg:142.25ms
step:571/1375 train_time:79846ms step_avg:142.33ms
step:572/1375 train_time:80000ms step_avg:142.35ms
step:573/1375 train_time:80146ms step_avg:142.36ms
step:574/1375 train_time:80294ms step_avg:142.37ms
step:575/1375 train_time:80441ms step_avg:142.37ms
step:576/1375 train_time:80586ms step_avg:142.38ms
step:577/1375 train_time:80732ms step_avg:142.38ms
step:578/1375 train_time:80883ms step_avg:142.40ms
step:579/1375 train_time:81032ms step_avg:142.41ms
step:580/1375 train_time:81181ms step_avg:142.42ms
step:581/1375 train_time:81327ms step_avg:142.43ms
step:582/1375 train_time:81474ms step_avg:142.44ms
step:583/1375 train_time:81620ms step_avg:142.44ms
step:584/1375 train_time:81767ms step_avg:142.45ms
step:585/1375 train_time:81914ms step_avg:142.46ms
step:586/1375 train_time:82063ms step_avg:142.47ms
step:587/1375 train_time:82209ms step_avg:142.48ms
step:588/1375 train_time:82357ms step_avg:142.49ms
step:589/1375 train_time:82503ms step_avg:142.49ms
step:590/1375 train_time:82649ms step_avg:142.50ms
step:591/1375 train_time:82796ms step_avg:142.51ms
step:592/1375 train_time:82948ms step_avg:142.52ms
step:593/1375 train_time:83095ms step_avg:142.53ms
step:594/1375 train_time:83243ms step_avg:142.54ms
step:595/1375 train_time:83390ms step_avg:142.55ms
step:596/1375 train_time:83538ms step_avg:142.56ms
step:597/1375 train_time:83684ms step_avg:142.56ms
step:598/1375 train_time:83831ms step_avg:142.57ms
step:599/1375 train_time:83980ms step_avg:142.58ms
step:600/1375 train_time:84127ms step_avg:142.59ms
step:601/1375 train_time:84276ms step_avg:142.60ms
step:602/1375 train_time:84423ms step_avg:142.61ms
step:603/1375 train_time:84571ms step_avg:142.61ms
step:604/1375 train_time:84720ms step_avg:142.63ms
step:605/1375 train_time:84867ms step_avg:142.63ms
step:606/1375 train_time:85013ms step_avg:142.64ms
step:607/1375 train_time:85162ms step_avg:142.65ms
step:608/1375 train_time:85309ms step_avg:142.66ms
step:609/1375 train_time:85457ms step_avg:142.67ms
step:610/1375 train_time:85603ms step_avg:142.67ms
step:611/1375 train_time:85749ms step_avg:142.68ms
step:612/1375 train_time:85897ms step_avg:142.69ms
step:613/1375 train_time:86045ms step_avg:142.69ms
step:614/1375 train_time:86193ms step_avg:142.70ms
step:615/1375 train_time:86341ms step_avg:142.71ms
step:616/1375 train_time:86491ms step_avg:142.72ms
step:617/1375 train_time:86640ms step_avg:142.74ms
step:618/1375 train_time:86789ms step_avg:142.75ms
step:619/1375 train_time:86939ms step_avg:142.76ms
step:620/1375 train_time:87086ms step_avg:142.76ms
step:621/1375 train_time:87236ms step_avg:142.78ms
step:622/1375 train_time:87386ms step_avg:142.79ms
step:623/1375 train_time:87536ms step_avg:142.80ms
step:624/1375 train_time:87685ms step_avg:142.81ms
step:625/1375 train_time:87832ms step_avg:142.82ms
step:625/1375 val_loss:3.5741 train_time:87907ms step_avg:142.94ms
step:626/1375 train_time:87982ms step_avg:142.83ms
step:627/1375 train_time:88132ms step_avg:142.84ms
step:628/1375 train_time:88278ms step_avg:142.85ms
step:629/1375 train_time:88428ms step_avg:142.86ms
step:630/1375 train_time:88575ms step_avg:142.86ms
step:631/1375 train_time:88720ms step_avg:142.87ms
step:632/1375 train_time:88868ms step_avg:142.88ms
step:633/1375 train_time:89019ms step_avg:142.89ms
step:634/1375 train_time:89168ms step_avg:142.90ms
step:635/1375 train_time:89316ms step_avg:142.91ms
step:636/1375 train_time:89465ms step_avg:142.92ms
step:637/1375 train_time:89614ms step_avg:142.92ms
step:638/1375 train_time:89762ms step_avg:142.93ms
step:639/1375 train_time:89911ms step_avg:142.94ms
step:640/1375 train_time:90060ms step_avg:142.95ms
step:641/1375 train_time:90210ms step_avg:142.96ms
step:642/1375 train_time:90358ms step_avg:142.97ms
step:643/1375 train_time:90507ms step_avg:142.98ms
step:644/1375 train_time:90656ms step_avg:142.99ms
step:645/1375 train_time:90805ms step_avg:143.00ms
step:646/1375 train_time:90955ms step_avg:143.01ms
step:647/1375 train_time:91102ms step_avg:143.02ms
step:648/1375 train_time:91255ms step_avg:143.03ms
step:649/1375 train_time:91404ms step_avg:143.04ms
step:650/1375 train_time:91554ms step_avg:143.05ms
step:651/1375 train_time:91701ms step_avg:143.06ms
step:652/1375 train_time:91853ms step_avg:143.07ms
step:653/1375 train_time:92000ms step_avg:143.08ms
step:654/1375 train_time:92151ms step_avg:143.09ms
step:655/1375 train_time:92298ms step_avg:143.10ms
step:656/1375 train_time:92449ms step_avg:143.11ms
step:657/1375 train_time:92597ms step_avg:143.12ms
step:658/1375 train_time:92748ms step_avg:143.13ms
step:659/1375 train_time:92897ms step_avg:143.14ms
step:660/1375 train_time:93045ms step_avg:143.15ms
step:661/1375 train_time:93194ms step_avg:143.16ms
step:662/1375 train_time:93342ms step_avg:143.16ms
step:663/1375 train_time:93491ms step_avg:143.17ms
step:664/1375 train_time:93642ms step_avg:143.18ms
step:665/1375 train_time:93792ms step_avg:143.19ms
step:666/1375 train_time:93940ms step_avg:143.20ms
step:667/1375 train_time:94088ms step_avg:143.21ms
step:668/1375 train_time:94238ms step_avg:143.22ms
step:669/1375 train_time:94387ms step_avg:143.23ms
step:670/1375 train_time:94537ms step_avg:143.24ms
step:671/1375 train_time:94684ms step_avg:143.24ms
step:672/1375 train_time:94834ms step_avg:143.25ms
step:673/1375 train_time:94982ms step_avg:143.26ms
step:674/1375 train_time:95133ms step_avg:143.27ms
step:675/1375 train_time:95281ms step_avg:143.28ms
step:676/1375 train_time:95431ms step_avg:143.29ms
step:677/1375 train_time:95578ms step_avg:143.30ms
step:678/1375 train_time:95728ms step_avg:143.31ms
step:679/1375 train_time:95878ms step_avg:143.32ms
step:680/1375 train_time:96029ms step_avg:143.33ms
step:681/1375 train_time:96177ms step_avg:143.33ms
step:682/1375 train_time:96326ms step_avg:143.34ms
step:683/1375 train_time:96475ms step_avg:143.35ms
step:684/1375 train_time:96622ms step_avg:143.36ms
step:685/1375 train_time:96772ms step_avg:143.37ms
step:686/1375 train_time:96920ms step_avg:143.37ms
step:687/1375 train_time:97067ms step_avg:143.38ms
step:688/1375 train_time:97219ms step_avg:143.39ms
step:689/1375 train_time:97369ms step_avg:143.40ms
step:690/1375 train_time:97519ms step_avg:143.41ms
step:691/1375 train_time:97667ms step_avg:143.42ms
step:692/1375 train_time:97816ms step_avg:143.43ms
step:693/1375 train_time:97966ms step_avg:143.43ms
step:694/1375 train_time:98116ms step_avg:143.44ms
step:695/1375 train_time:98263ms step_avg:143.45ms
step:696/1375 train_time:98412ms step_avg:143.46ms
step:697/1375 train_time:98561ms step_avg:143.47ms
step:698/1375 train_time:98710ms step_avg:143.47ms
step:699/1375 train_time:98859ms step_avg:143.48ms
step:700/1375 train_time:99010ms step_avg:143.49ms
step:701/1375 train_time:99159ms step_avg:143.50ms
step:702/1375 train_time:99310ms step_avg:143.51ms
step:703/1375 train_time:99458ms step_avg:143.52ms
step:704/1375 train_time:99606ms step_avg:143.52ms
step:705/1375 train_time:99756ms step_avg:143.53ms
step:706/1375 train_time:99906ms step_avg:143.54ms
step:707/1375 train_time:100055ms step_avg:143.55ms
step:708/1375 train_time:100201ms step_avg:143.55ms
step:709/1375 train_time:100352ms step_avg:143.57ms
step:710/1375 train_time:100500ms step_avg:143.57ms
step:711/1375 train_time:100652ms step_avg:143.58ms
step:712/1375 train_time:100800ms step_avg:143.59ms
step:713/1375 train_time:100950ms step_avg:143.60ms
step:714/1375 train_time:101098ms step_avg:143.61ms
step:715/1375 train_time:101250ms step_avg:143.62ms
step:716/1375 train_time:101399ms step_avg:143.62ms
step:717/1375 train_time:101550ms step_avg:143.64ms
step:718/1375 train_time:101698ms step_avg:143.64ms
step:719/1375 train_time:101848ms step_avg:143.65ms
step:720/1375 train_time:101998ms step_avg:143.66ms
step:721/1375 train_time:102150ms step_avg:143.67ms
step:722/1375 train_time:102299ms step_avg:143.68ms
step:723/1375 train_time:102450ms step_avg:143.69ms
step:724/1375 train_time:102600ms step_avg:143.70ms
step:725/1375 train_time:102749ms step_avg:143.71ms
step:726/1375 train_time:102899ms step_avg:143.71ms
step:727/1375 train_time:103051ms step_avg:143.73ms
step:728/1375 train_time:103200ms step_avg:143.73ms
step:729/1375 train_time:103351ms step_avg:143.74ms
step:730/1375 train_time:103502ms step_avg:143.75ms
step:731/1375 train_time:103653ms step_avg:143.76ms
step:732/1375 train_time:103800ms step_avg:143.77ms
step:733/1375 train_time:103953ms step_avg:143.78ms
step:734/1375 train_time:104101ms step_avg:143.79ms
step:735/1375 train_time:104253ms step_avg:143.80ms
step:736/1375 train_time:104401ms step_avg:143.80ms
step:737/1375 train_time:104553ms step_avg:143.81ms
step:738/1375 train_time:104704ms step_avg:143.82ms
step:739/1375 train_time:104857ms step_avg:143.84ms
step:740/1375 train_time:105006ms step_avg:143.84ms
step:741/1375 train_time:105157ms step_avg:143.85ms
step:742/1375 train_time:105306ms step_avg:143.86ms
step:743/1375 train_time:105456ms step_avg:143.87ms
step:744/1375 train_time:105604ms step_avg:143.87ms
step:745/1375 train_time:105758ms step_avg:143.89ms
step:746/1375 train_time:105908ms step_avg:143.90ms
step:747/1375 train_time:106059ms step_avg:143.91ms
step:748/1375 train_time:106211ms step_avg:143.92ms
step:749/1375 train_time:106360ms step_avg:143.92ms
step:750/1375 train_time:106510ms step_avg:143.93ms
step:750/1375 val_loss:3.5207 train_time:106587ms step_avg:144.04ms
step:751/1375 train_time:106663ms step_avg:143.94ms
step:752/1375 train_time:106815ms step_avg:143.96ms
step:753/1375 train_time:106964ms step_avg:143.96ms
step:754/1375 train_time:107114ms step_avg:143.97ms
step:755/1375 train_time:107262ms step_avg:143.98ms
step:756/1375 train_time:107413ms step_avg:143.99ms
step:757/1375 train_time:107565ms step_avg:144.00ms
step:758/1375 train_time:107716ms step_avg:144.01ms
step:759/1375 train_time:107865ms step_avg:144.01ms
step:760/1375 train_time:108015ms step_avg:144.02ms
step:761/1375 train_time:108208ms step_avg:144.08ms
step:762/1375 train_time:108361ms step_avg:144.10ms
step:763/1375 train_time:108511ms step_avg:144.10ms
step:764/1375 train_time:108660ms step_avg:144.11ms
step:765/1375 train_time:108810ms step_avg:144.12ms
step:766/1375 train_time:108961ms step_avg:144.13ms
step:767/1375 train_time:109115ms step_avg:144.14ms
step:768/1375 train_time:109267ms step_avg:144.15ms
step:769/1375 train_time:109418ms step_avg:144.16ms
step:770/1375 train_time:109567ms step_avg:144.17ms
step:771/1375 train_time:109717ms step_avg:144.18ms
step:772/1375 train_time:109866ms step_avg:144.18ms
step:773/1375 train_time:110016ms step_avg:144.19ms
step:774/1375 train_time:110168ms step_avg:144.20ms
step:775/1375 train_time:110319ms step_avg:144.21ms
step:776/1375 train_time:110472ms step_avg:144.22ms
step:777/1375 train_time:110623ms step_avg:144.23ms
step:778/1375 train_time:110773ms step_avg:144.24ms
step:779/1375 train_time:110922ms step_avg:144.24ms
step:780/1375 train_time:111073ms step_avg:144.25ms
step:781/1375 train_time:111223ms step_avg:144.26ms
step:782/1375 train_time:111374ms step_avg:144.27ms
step:783/1375 train_time:111525ms step_avg:144.27ms
step:784/1375 train_time:111676ms step_avg:144.28ms
step:785/1375 train_time:111826ms step_avg:144.29ms
step:786/1375 train_time:111977ms step_avg:144.30ms
step:787/1375 train_time:112126ms step_avg:144.31ms
step:788/1375 train_time:112276ms step_avg:144.31ms
step:789/1375 train_time:112426ms step_avg:144.32ms
step:790/1375 train_time:112576ms step_avg:144.33ms
step:791/1375 train_time:112726ms step_avg:144.34ms
step:792/1375 train_time:112878ms step_avg:144.35ms
step:793/1375 train_time:113028ms step_avg:144.35ms
step:794/1375 train_time:113178ms step_avg:144.36ms
step:795/1375 train_time:113330ms step_avg:144.37ms
step:796/1375 train_time:113479ms step_avg:144.38ms
step:797/1375 train_time:113631ms step_avg:144.38ms
step:798/1375 train_time:113781ms step_avg:144.39ms
step:799/1375 train_time:113937ms step_avg:144.41ms
step:800/1375 train_time:114087ms step_avg:144.41ms
step:801/1375 train_time:114236ms step_avg:144.42ms
step:802/1375 train_time:114388ms step_avg:144.43ms
step:803/1375 train_time:114536ms step_avg:144.43ms
step:804/1375 train_time:114685ms step_avg:144.44ms
step:805/1375 train_time:114837ms step_avg:144.45ms
step:806/1375 train_time:114990ms step_avg:144.46ms
step:807/1375 train_time:115138ms step_avg:144.46ms
step:808/1375 train_time:115289ms step_avg:144.47ms
step:809/1375 train_time:115439ms step_avg:144.48ms
step:810/1375 train_time:115588ms step_avg:144.48ms
step:811/1375 train_time:115738ms step_avg:144.49ms
step:812/1375 train_time:115888ms step_avg:144.50ms
step:813/1375 train_time:116038ms step_avg:144.51ms
step:814/1375 train_time:116189ms step_avg:144.51ms
step:815/1375 train_time:116339ms step_avg:144.52ms
step:816/1375 train_time:116491ms step_avg:144.53ms
step:817/1375 train_time:116641ms step_avg:144.54ms
step:818/1375 train_time:116791ms step_avg:144.54ms
step:819/1375 train_time:116942ms step_avg:144.55ms
step:820/1375 train_time:117098ms step_avg:144.57ms
step:821/1375 train_time:117248ms step_avg:144.57ms
step:822/1375 train_time:117399ms step_avg:144.58ms
step:823/1375 train_time:117550ms step_avg:144.59ms
step:824/1375 train_time:117700ms step_avg:144.59ms
step:825/1375 train_time:117853ms step_avg:144.61ms
step:826/1375 train_time:118006ms step_avg:144.62ms
step:827/1375 train_time:118160ms step_avg:144.63ms
step:828/1375 train_time:118313ms step_avg:144.64ms
step:829/1375 train_time:118463ms step_avg:144.64ms
step:830/1375 train_time:118615ms step_avg:144.65ms
step:831/1375 train_time:118766ms step_avg:144.66ms
step:832/1375 train_time:118918ms step_avg:144.67ms
step:833/1375 train_time:119069ms step_avg:144.68ms
step:834/1375 train_time:119221ms step_avg:144.69ms
step:835/1375 train_time:119375ms step_avg:144.70ms
step:836/1375 train_time:119529ms step_avg:144.71ms
step:837/1375 train_time:119679ms step_avg:144.71ms
step:838/1375 train_time:119830ms step_avg:144.72ms
step:839/1375 train_time:119979ms step_avg:144.73ms
step:840/1375 train_time:120132ms step_avg:144.74ms
step:841/1375 train_time:120283ms step_avg:144.75ms
step:842/1375 train_time:120436ms step_avg:144.76ms
step:843/1375 train_time:120589ms step_avg:144.76ms
step:844/1375 train_time:120740ms step_avg:144.77ms
step:845/1375 train_time:120891ms step_avg:144.78ms
step:846/1375 train_time:121041ms step_avg:144.79ms
step:847/1375 train_time:121196ms step_avg:144.80ms
step:848/1375 train_time:121348ms step_avg:144.81ms
step:849/1375 train_time:121501ms step_avg:144.82ms
step:850/1375 train_time:121655ms step_avg:144.83ms
step:851/1375 train_time:121808ms step_avg:144.84ms
step:852/1375 train_time:121960ms step_avg:144.85ms
step:853/1375 train_time:122110ms step_avg:144.85ms
step:854/1375 train_time:122259ms step_avg:144.86ms
step:855/1375 train_time:122411ms step_avg:144.87ms
step:856/1375 train_time:122561ms step_avg:144.87ms
step:857/1375 train_time:122716ms step_avg:144.88ms
step:858/1375 train_time:122873ms step_avg:144.90ms
step:859/1375 train_time:123024ms step_avg:144.90ms
step:860/1375 train_time:123175ms step_avg:144.91ms
step:861/1375 train_time:123326ms step_avg:144.92ms
step:862/1375 train_time:123478ms step_avg:144.93ms
step:863/1375 train_time:123630ms step_avg:144.94ms
step:864/1375 train_time:123783ms step_avg:144.94ms
step:865/1375 train_time:123935ms step_avg:144.95ms
step:866/1375 train_time:124090ms step_avg:144.97ms
step:867/1375 train_time:124240ms step_avg:144.97ms
step:868/1375 train_time:124390ms step_avg:144.98ms
step:869/1375 train_time:124540ms step_avg:144.98ms
step:870/1375 train_time:124695ms step_avg:144.99ms
step:871/1375 train_time:124846ms step_avg:145.00ms
step:872/1375 train_time:124996ms step_avg:145.01ms
step:873/1375 train_time:125147ms step_avg:145.01ms
step:874/1375 train_time:125298ms step_avg:145.02ms
step:875/1375 train_time:125450ms step_avg:145.03ms
step:875/1375 val_loss:3.4681 train_time:125524ms step_avg:145.11ms
step:876/1375 train_time:125600ms step_avg:145.03ms
step:877/1375 train_time:125754ms step_avg:145.04ms
step:878/1375 train_time:125905ms step_avg:145.05ms
step:879/1375 train_time:126056ms step_avg:145.06ms
step:880/1375 train_time:126207ms step_avg:145.07ms
step:881/1375 train_time:126357ms step_avg:145.07ms
step:882/1375 train_time:126513ms step_avg:145.08ms
step:883/1375 train_time:126667ms step_avg:145.09ms
step:884/1375 train_time:126820ms step_avg:145.10ms
step:885/1375 train_time:126971ms step_avg:145.11ms
step:886/1375 train_time:127126ms step_avg:145.12ms
step:887/1375 train_time:127276ms step_avg:145.13ms
step:888/1375 train_time:127429ms step_avg:145.14ms
step:889/1375 train_time:127581ms step_avg:145.14ms
step:890/1375 train_time:127732ms step_avg:145.15ms
step:891/1375 train_time:127883ms step_avg:145.16ms
step:892/1375 train_time:128035ms step_avg:145.16ms
step:893/1375 train_time:128186ms step_avg:145.17ms
step:894/1375 train_time:128339ms step_avg:145.18ms
step:895/1375 train_time:128494ms step_avg:145.19ms
step:896/1375 train_time:128645ms step_avg:145.20ms
step:897/1375 train_time:128795ms step_avg:145.20ms
step:898/1375 train_time:128948ms step_avg:145.21ms
step:899/1375 train_time:129098ms step_avg:145.22ms
step:900/1375 train_time:129251ms step_avg:145.23ms
step:901/1375 train_time:129403ms step_avg:145.23ms
step:902/1375 train_time:129552ms step_avg:145.24ms
step:903/1375 train_time:129703ms step_avg:145.24ms
step:904/1375 train_time:129855ms step_avg:145.25ms
step:905/1375 train_time:130005ms step_avg:145.26ms
step:906/1375 train_time:130157ms step_avg:145.26ms
step:907/1375 train_time:130312ms step_avg:145.28ms
step:908/1375 train_time:130465ms step_avg:145.28ms
step:909/1375 train_time:130617ms step_avg:145.29ms
step:910/1375 train_time:130775ms step_avg:145.31ms
step:911/1375 train_time:130927ms step_avg:145.31ms
step:912/1375 train_time:131078ms step_avg:145.32ms
step:913/1375 train_time:131231ms step_avg:145.33ms
step:914/1375 train_time:131382ms step_avg:145.33ms
step:915/1375 train_time:131534ms step_avg:145.34ms
step:916/1375 train_time:131686ms step_avg:145.35ms
step:917/1375 train_time:131837ms step_avg:145.35ms
step:918/1375 train_time:131990ms step_avg:145.36ms
step:919/1375 train_time:132146ms step_avg:145.37ms
step:920/1375 train_time:132299ms step_avg:145.38ms
step:921/1375 train_time:132452ms step_avg:145.39ms
step:922/1375 train_time:132609ms step_avg:145.40ms
step:923/1375 train_time:132762ms step_avg:145.41ms
step:924/1375 train_time:132914ms step_avg:145.42ms
step:925/1375 train_time:133071ms step_avg:145.43ms
step:926/1375 train_time:133225ms step_avg:145.44ms
step:927/1375 train_time:133376ms step_avg:145.45ms
step:928/1375 train_time:133530ms step_avg:145.46ms
step:929/1375 train_time:133684ms step_avg:145.47ms
step:930/1375 train_time:133837ms step_avg:145.47ms
step:931/1375 train_time:133990ms step_avg:145.48ms
step:932/1375 train_time:134142ms step_avg:145.49ms
step:933/1375 train_time:134294ms step_avg:145.50ms
step:934/1375 train_time:134447ms step_avg:145.51ms
step:935/1375 train_time:134600ms step_avg:145.51ms
step:936/1375 train_time:134754ms step_avg:145.52ms
step:937/1375 train_time:134913ms step_avg:145.54ms
step:938/1375 train_time:135067ms step_avg:145.55ms
step:939/1375 train_time:135219ms step_avg:145.55ms
step:940/1375 train_time:135373ms step_avg:145.56ms
step:941/1375 train_time:135526ms step_avg:145.57ms
step:942/1375 train_time:135676ms step_avg:145.57ms
step:943/1375 train_time:135833ms step_avg:145.59ms
step:944/1375 train_time:135992ms step_avg:145.60ms
step:945/1375 train_time:136147ms step_avg:145.61ms
step:946/1375 train_time:136301ms step_avg:145.62ms
step:947/1375 train_time:136455ms step_avg:145.63ms
step:948/1375 train_time:136609ms step_avg:145.64ms
step:949/1375 train_time:136767ms step_avg:145.65ms
step:950/1375 train_time:136917ms step_avg:145.66ms
step:951/1375 train_time:137117ms step_avg:145.71ms
step:952/1375 train_time:137276ms step_avg:145.73ms
step:953/1375 train_time:137431ms step_avg:145.74ms
step:954/1375 train_time:137584ms step_avg:145.75ms
step:955/1375 train_time:137734ms step_avg:145.75ms
step:956/1375 train_time:137888ms step_avg:145.76ms
step:957/1375 train_time:138040ms step_avg:145.77ms
step:958/1375 train_time:138198ms step_avg:145.78ms
step:959/1375 train_time:138354ms step_avg:145.79ms
step:960/1375 train_time:138508ms step_avg:145.80ms
step:961/1375 train_time:138661ms step_avg:145.81ms
step:962/1375 train_time:138812ms step_avg:145.81ms
step:963/1375 train_time:138971ms step_avg:145.82ms
step:964/1375 train_time:139123ms step_avg:145.83ms
step:965/1375 train_time:139276ms step_avg:145.84ms
step:966/1375 train_time:139428ms step_avg:145.85ms
step:967/1375 train_time:139578ms step_avg:145.85ms
step:968/1375 train_time:139730ms step_avg:145.86ms
step:969/1375 train_time:139882ms step_avg:145.86ms
step:970/1375 train_time:140035ms step_avg:145.87ms
step:971/1375 train_time:140191ms step_avg:145.88ms
step:972/1375 train_time:140344ms step_avg:145.89ms
step:973/1375 train_time:140494ms step_avg:145.89ms
step:974/1375 train_time:140647ms step_avg:145.90ms
step:975/1375 train_time:140800ms step_avg:145.91ms
step:976/1375 train_time:140952ms step_avg:145.91ms
step:977/1375 train_time:141104ms step_avg:145.92ms
step:978/1375 train_time:141256ms step_avg:145.93ms
step:979/1375 train_time:141408ms step_avg:145.93ms
step:980/1375 train_time:141560ms step_avg:145.94ms
step:981/1375 train_time:141711ms step_avg:145.94ms
step:982/1375 train_time:141865ms step_avg:145.95ms
step:983/1375 train_time:142016ms step_avg:145.96ms
step:984/1375 train_time:142168ms step_avg:145.96ms
step:985/1375 train_time:142321ms step_avg:145.97ms
step:986/1375 train_time:142475ms step_avg:145.98ms
step:987/1375 train_time:142627ms step_avg:145.98ms
step:988/1375 train_time:142779ms step_avg:145.99ms
step:989/1375 train_time:142930ms step_avg:146.00ms
step:990/1375 train_time:143083ms step_avg:146.00ms
step:991/1375 train_time:143235ms step_avg:146.01ms
step:992/1375 train_time:143394ms step_avg:146.02ms
step:993/1375 train_time:143555ms step_avg:146.04ms
step:994/1375 train_time:143708ms step_avg:146.04ms
step:995/1375 train_time:143858ms step_avg:146.05ms
step:996/1375 train_time:144008ms step_avg:146.05ms
step:997/1375 train_time:144159ms step_avg:146.06ms
step:998/1375 train_time:144311ms step_avg:146.06ms
step:999/1375 train_time:144466ms step_avg:146.07ms
step:1000/1375 train_time:144617ms step_avg:146.08ms
step:1000/1375 val_loss:3.4035 train_time:144695ms step_avg:146.16ms
step:1001/1375 train_time:144772ms step_avg:146.09ms
step:1002/1375 train_time:144926ms step_avg:146.09ms
step:1003/1375 train_time:145080ms step_avg:146.10ms
step:1004/1375 train_time:145236ms step_avg:146.11ms
step:1005/1375 train_time:145387ms step_avg:146.12ms
step:1006/1375 train_time:145539ms step_avg:146.12ms
step:1007/1375 train_time:145694ms step_avg:146.13ms
step:1008/1375 train_time:145847ms step_avg:146.14ms
step:1009/1375 train_time:146006ms step_avg:146.15ms
step:1010/1375 train_time:146158ms step_avg:146.16ms
step:1011/1375 train_time:146310ms step_avg:146.16ms
step:1012/1375 train_time:146462ms step_avg:146.17ms
step:1013/1375 train_time:146615ms step_avg:146.18ms
step:1014/1375 train_time:146767ms step_avg:146.18ms
step:1015/1375 train_time:146922ms step_avg:146.19ms
step:1016/1375 train_time:147076ms step_avg:146.20ms
step:1017/1375 train_time:147228ms step_avg:146.20ms
step:1018/1375 train_time:147381ms step_avg:146.21ms
step:1019/1375 train_time:147533ms step_avg:146.22ms
step:1020/1375 train_time:147689ms step_avg:146.23ms
step:1021/1375 train_time:147843ms step_avg:146.23ms
step:1022/1375 train_time:147998ms step_avg:146.24ms
step:1023/1375 train_time:148151ms step_avg:146.25ms
step:1024/1375 train_time:148305ms step_avg:146.26ms
step:1025/1375 train_time:148459ms step_avg:146.26ms
step:1026/1375 train_time:148610ms step_avg:146.27ms
step:1027/1375 train_time:148762ms step_avg:146.28ms
step:1028/1375 train_time:148918ms step_avg:146.28ms
step:1029/1375 train_time:149074ms step_avg:146.29ms
step:1030/1375 train_time:149227ms step_avg:146.30ms
step:1031/1375 train_time:149379ms step_avg:146.31ms
step:1032/1375 train_time:149532ms step_avg:146.31ms
step:1033/1375 train_time:149684ms step_avg:146.32ms
step:1034/1375 train_time:149839ms step_avg:146.33ms
step:1035/1375 train_time:149995ms step_avg:146.34ms
step:1036/1375 train_time:150151ms step_avg:146.35ms
step:1037/1375 train_time:150306ms step_avg:146.35ms
step:1038/1375 train_time:150461ms step_avg:146.36ms
step:1039/1375 train_time:150615ms step_avg:146.37ms
step:1040/1375 train_time:150765ms step_avg:146.37ms
step:1041/1375 train_time:150920ms step_avg:146.38ms
step:1042/1375 train_time:151070ms step_avg:146.39ms
step:1043/1375 train_time:151224ms step_avg:146.39ms
step:1044/1375 train_time:151380ms step_avg:146.40ms
step:1045/1375 train_time:151535ms step_avg:146.41ms
step:1046/1375 train_time:151687ms step_avg:146.42ms
step:1047/1375 train_time:151841ms step_avg:146.42ms
step:1048/1375 train_time:151996ms step_avg:146.43ms
step:1049/1375 train_time:152149ms step_avg:146.44ms
step:1050/1375 train_time:152305ms step_avg:146.45ms
step:1051/1375 train_time:152462ms step_avg:146.46ms
step:1052/1375 train_time:152617ms step_avg:146.47ms
step:1053/1375 train_time:152767ms step_avg:146.47ms
step:1054/1375 train_time:152924ms step_avg:146.48ms
step:1055/1375 train_time:153076ms step_avg:146.48ms
step:1056/1375 train_time:153229ms step_avg:146.49ms
step:1057/1375 train_time:153384ms step_avg:146.50ms
step:1058/1375 train_time:153541ms step_avg:146.51ms
step:1059/1375 train_time:153697ms step_avg:146.52ms
step:1060/1375 train_time:153850ms step_avg:146.52ms
step:1061/1375 train_time:154003ms step_avg:146.53ms
step:1062/1375 train_time:154158ms step_avg:146.54ms
step:1063/1375 train_time:154314ms step_avg:146.55ms
step:1064/1375 train_time:154465ms step_avg:146.55ms
step:1065/1375 train_time:154621ms step_avg:146.56ms
step:1066/1375 train_time:154780ms step_avg:146.57ms
step:1067/1375 train_time:154936ms step_avg:146.58ms
step:1068/1375 train_time:155087ms step_avg:146.59ms
step:1069/1375 train_time:155245ms step_avg:146.60ms
step:1070/1375 train_time:155398ms step_avg:146.60ms
step:1071/1375 train_time:155553ms step_avg:146.61ms
step:1072/1375 train_time:155704ms step_avg:146.61ms
step:1073/1375 train_time:155856ms step_avg:146.62ms
step:1074/1375 train_time:156009ms step_avg:146.62ms
step:1075/1375 train_time:156163ms step_avg:146.63ms
step:1076/1375 train_time:156318ms step_avg:146.64ms
step:1077/1375 train_time:156469ms step_avg:146.64ms
step:1078/1375 train_time:156627ms step_avg:146.65ms
step:1079/1375 train_time:156784ms step_avg:146.66ms
step:1080/1375 train_time:156939ms step_avg:146.67ms
step:1081/1375 train_time:157092ms step_avg:146.68ms
step:1082/1375 train_time:157245ms step_avg:146.68ms
step:1083/1375 train_time:157399ms step_avg:146.69ms
step:1084/1375 train_time:157556ms step_avg:146.70ms
step:1085/1375 train_time:157708ms step_avg:146.70ms
step:1086/1375 train_time:157862ms step_avg:146.71ms
step:1087/1375 train_time:158020ms step_avg:146.72ms
step:1088/1375 train_time:158173ms step_avg:146.73ms
step:1089/1375 train_time:158330ms step_avg:146.74ms
step:1090/1375 train_time:158488ms step_avg:146.75ms
step:1091/1375 train_time:158643ms step_avg:146.76ms
step:1092/1375 train_time:158797ms step_avg:146.76ms
step:1093/1375 train_time:158951ms step_avg:146.77ms
step:1094/1375 train_time:159104ms step_avg:146.78ms
step:1095/1375 train_time:159258ms step_avg:146.78ms
step:1096/1375 train_time:159419ms step_avg:146.79ms
step:1097/1375 train_time:159572ms step_avg:146.80ms
step:1098/1375 train_time:159725ms step_avg:146.81ms
step:1099/1375 train_time:159878ms step_avg:146.81ms
step:1100/1375 train_time:160029ms step_avg:146.82ms
step:1101/1375 train_time:160185ms step_avg:146.82ms
step:1102/1375 train_time:160341ms step_avg:146.83ms
step:1103/1375 train_time:160496ms step_avg:146.84ms
step:1104/1375 train_time:160648ms step_avg:146.84ms
step:1105/1375 train_time:160804ms step_avg:146.85ms
step:1106/1375 train_time:160957ms step_avg:146.86ms
step:1107/1375 train_time:161108ms step_avg:146.86ms
step:1108/1375 train_time:161266ms step_avg:146.87ms
step:1109/1375 train_time:161421ms step_avg:146.88ms
step:1110/1375 train_time:161576ms step_avg:146.89ms
step:1111/1375 train_time:161731ms step_avg:146.89ms
step:1112/1375 train_time:161885ms step_avg:146.90ms
step:1113/1375 train_time:162038ms step_avg:146.91ms
step:1114/1375 train_time:162194ms step_avg:146.91ms
step:1115/1375 train_time:162349ms step_avg:146.92ms
step:1116/1375 train_time:162503ms step_avg:146.93ms
step:1117/1375 train_time:162662ms step_avg:146.94ms
step:1118/1375 train_time:162820ms step_avg:146.95ms
step:1119/1375 train_time:162975ms step_avg:146.96ms
step:1120/1375 train_time:163128ms step_avg:146.96ms
step:1121/1375 train_time:163281ms step_avg:146.97ms
step:1122/1375 train_time:163434ms step_avg:146.97ms
step:1123/1375 train_time:163588ms step_avg:146.98ms
step:1124/1375 train_time:163747ms step_avg:146.99ms
step:1125/1375 train_time:163902ms step_avg:147.00ms
step:1125/1375 val_loss:3.3499 train_time:163980ms step_avg:147.07ms
step:1126/1375 train_time:164057ms step_avg:147.00ms
step:1127/1375 train_time:164214ms step_avg:147.01ms
step:1128/1375 train_time:164369ms step_avg:147.02ms
step:1129/1375 train_time:164527ms step_avg:147.03ms
step:1130/1375 train_time:164681ms step_avg:147.04ms
step:1131/1375 train_time:164838ms step_avg:147.05ms
step:1132/1375 train_time:164993ms step_avg:147.05ms
step:1133/1375 train_time:165147ms step_avg:147.06ms
step:1134/1375 train_time:165301ms step_avg:147.07ms
step:1135/1375 train_time:165456ms step_avg:147.07ms
step:1136/1375 train_time:165615ms step_avg:147.08ms
step:1137/1375 train_time:165766ms step_avg:147.09ms
step:1138/1375 train_time:165922ms step_avg:147.09ms
step:1139/1375 train_time:166077ms step_avg:147.10ms
step:1140/1375 train_time:166230ms step_avg:147.11ms
step:1141/1375 train_time:166426ms step_avg:147.15ms
step:1142/1375 train_time:166582ms step_avg:147.16ms
step:1143/1375 train_time:166742ms step_avg:147.17ms
step:1144/1375 train_time:166897ms step_avg:147.18ms
step:1145/1375 train_time:167048ms step_avg:147.18ms
step:1146/1375 train_time:167204ms step_avg:147.19ms
step:1147/1375 train_time:167360ms step_avg:147.19ms
step:1148/1375 train_time:167515ms step_avg:147.20ms
step:1149/1375 train_time:167670ms step_avg:147.21ms
step:1150/1375 train_time:167824ms step_avg:147.21ms
step:1151/1375 train_time:167980ms step_avg:147.22ms
step:1152/1375 train_time:168136ms step_avg:147.23ms
step:1153/1375 train_time:168291ms step_avg:147.24ms
step:1154/1375 train_time:168444ms step_avg:147.24ms
step:1155/1375 train_time:168599ms step_avg:147.25ms
step:1156/1375 train_time:168759ms step_avg:147.26ms
step:1157/1375 train_time:168919ms step_avg:147.27ms
step:1158/1375 train_time:169075ms step_avg:147.28ms
step:1159/1375 train_time:169230ms step_avg:147.28ms
step:1160/1375 train_time:169381ms step_avg:147.29ms
step:1161/1375 train_time:169538ms step_avg:147.30ms
step:1162/1375 train_time:169692ms step_avg:147.30ms
step:1163/1375 train_time:169845ms step_avg:147.31ms
step:1164/1375 train_time:170002ms step_avg:147.32ms
step:1165/1375 train_time:170155ms step_avg:147.32ms
step:1166/1375 train_time:170308ms step_avg:147.33ms
step:1167/1375 train_time:170461ms step_avg:147.33ms
step:1168/1375 train_time:170617ms step_avg:147.34ms
step:1169/1375 train_time:170772ms step_avg:147.34ms
step:1170/1375 train_time:170926ms step_avg:147.35ms
step:1171/1375 train_time:171081ms step_avg:147.36ms
step:1172/1375 train_time:171237ms step_avg:147.36ms
step:1173/1375 train_time:171391ms step_avg:147.37ms
step:1174/1375 train_time:171554ms step_avg:147.38ms
step:1175/1375 train_time:171710ms step_avg:147.39ms
step:1176/1375 train_time:171869ms step_avg:147.40ms
step:1177/1375 train_time:172032ms step_avg:147.41ms
step:1178/1375 train_time:172185ms step_avg:147.42ms
step:1179/1375 train_time:172339ms step_avg:147.42ms
step:1180/1375 train_time:172502ms step_avg:147.44ms
step:1181/1375 train_time:172659ms step_avg:147.45ms
step:1182/1375 train_time:172813ms step_avg:147.45ms
step:1183/1375 train_time:172966ms step_avg:147.46ms
step:1184/1375 train_time:173119ms step_avg:147.46ms
step:1185/1375 train_time:173277ms step_avg:147.47ms
step:1186/1375 train_time:173432ms step_avg:147.48ms
step:1187/1375 train_time:173594ms step_avg:147.49ms
step:1188/1375 train_time:173746ms step_avg:147.49ms
step:1189/1375 train_time:173903ms step_avg:147.50ms
step:1190/1375 train_time:174059ms step_avg:147.51ms
step:1191/1375 train_time:174216ms step_avg:147.52ms
step:1192/1375 train_time:174368ms step_avg:147.52ms
step:1193/1375 train_time:174523ms step_avg:147.53ms
step:1194/1375 train_time:174681ms step_avg:147.53ms
step:1195/1375 train_time:174837ms step_avg:147.54ms
step:1196/1375 train_time:174992ms step_avg:147.55ms
step:1197/1375 train_time:175149ms step_avg:147.56ms
step:1198/1375 train_time:175308ms step_avg:147.57ms
step:1199/1375 train_time:175463ms step_avg:147.57ms
step:1200/1375 train_time:175618ms step_avg:147.58ms
step:1201/1375 train_time:175773ms step_avg:147.58ms
step:1202/1375 train_time:175941ms step_avg:147.60ms
step:1203/1375 train_time:176098ms step_avg:147.61ms
step:1204/1375 train_time:176255ms step_avg:147.62ms
step:1205/1375 train_time:176408ms step_avg:147.62ms
step:1206/1375 train_time:176563ms step_avg:147.63ms
step:1207/1375 train_time:176719ms step_avg:147.63ms
step:1208/1375 train_time:176875ms step_avg:147.64ms
step:1209/1375 train_time:177030ms step_avg:147.65ms
step:1210/1375 train_time:177188ms step_avg:147.66ms
step:1211/1375 train_time:177343ms step_avg:147.66ms
step:1212/1375 train_time:177497ms step_avg:147.67ms
step:1213/1375 train_time:177652ms step_avg:147.67ms
step:1214/1375 train_time:177810ms step_avg:147.68ms
step:1215/1375 train_time:177966ms step_avg:147.69ms
step:1216/1375 train_time:178119ms step_avg:147.69ms
step:1217/1375 train_time:178274ms step_avg:147.70ms
step:1218/1375 train_time:178427ms step_avg:147.70ms
step:1219/1375 train_time:178580ms step_avg:147.71ms
step:1220/1375 train_time:178734ms step_avg:147.71ms
step:1221/1375 train_time:178887ms step_avg:147.72ms
step:1222/1375 train_time:179042ms step_avg:147.72ms
step:1223/1375 train_time:179200ms step_avg:147.73ms
step:1224/1375 train_time:179358ms step_avg:147.74ms
step:1225/1375 train_time:179515ms step_avg:147.75ms
step:1226/1375 train_time:179669ms step_avg:147.75ms
step:1227/1375 train_time:179824ms step_avg:147.76ms
step:1228/1375 train_time:179980ms step_avg:147.77ms
step:1229/1375 train_time:180136ms step_avg:147.77ms
step:1230/1375 train_time:180294ms step_avg:147.78ms
step:1231/1375 train_time:180453ms step_avg:147.79ms
step:1232/1375 train_time:180613ms step_avg:147.80ms
step:1233/1375 train_time:180768ms step_avg:147.81ms
step:1234/1375 train_time:180920ms step_avg:147.81ms
step:1235/1375 train_time:181075ms step_avg:147.82ms
step:1236/1375 train_time:181228ms step_avg:147.82ms
step:1237/1375 train_time:181383ms step_avg:147.83ms
step:1238/1375 train_time:181548ms step_avg:147.84ms
step:1239/1375 train_time:181704ms step_avg:147.85ms
step:1240/1375 train_time:181862ms step_avg:147.86ms
step:1241/1375 train_time:182023ms step_avg:147.87ms
step:1242/1375 train_time:182178ms step_avg:147.87ms
step:1243/1375 train_time:182337ms step_avg:147.88ms
step:1244/1375 train_time:182489ms step_avg:147.88ms
step:1245/1375 train_time:182646ms step_avg:147.89ms
step:1246/1375 train_time:182800ms step_avg:147.90ms
step:1247/1375 train_time:182959ms step_avg:147.91ms
step:1248/1375 train_time:183113ms step_avg:147.91ms
step:1249/1375 train_time:183266ms step_avg:147.91ms
step:1250/1375 train_time:183420ms step_avg:147.92ms
step:1250/1375 val_loss:3.3047 train_time:183499ms step_avg:147.98ms
step:1251/1375 train_time:183578ms step_avg:147.93ms
step:1252/1375 train_time:183734ms step_avg:147.93ms
step:1253/1375 train_time:183889ms step_avg:147.94ms
step:1254/1375 train_time:184042ms step_avg:147.94ms
step:1255/1375 train_time:184208ms step_avg:147.96ms
step:1256/1375 train_time:184363ms step_avg:147.96ms
step:1257/1375 train_time:184516ms step_avg:147.97ms
step:1258/1375 train_time:184675ms step_avg:147.98ms
step:1259/1375 train_time:184833ms step_avg:147.99ms
step:1260/1375 train_time:184987ms step_avg:147.99ms
step:1261/1375 train_time:185144ms step_avg:148.00ms
step:1262/1375 train_time:185300ms step_avg:148.00ms
step:1263/1375 train_time:185456ms step_avg:148.01ms
step:1264/1375 train_time:185610ms step_avg:148.01ms
step:1265/1375 train_time:185770ms step_avg:148.02ms
step:1266/1375 train_time:185928ms step_avg:148.03ms
step:1267/1375 train_time:186085ms step_avg:148.04ms
step:1268/1375 train_time:186242ms step_avg:148.05ms
step:1269/1375 train_time:186401ms step_avg:148.05ms
step:1270/1375 train_time:186556ms step_avg:148.06ms
step:1271/1375 train_time:186712ms step_avg:148.07ms
step:1272/1375 train_time:186869ms step_avg:148.07ms
step:1273/1375 train_time:187024ms step_avg:148.08ms
step:1274/1375 train_time:187178ms step_avg:148.08ms
step:1275/1375 train_time:187333ms step_avg:148.09ms
step:1276/1375 train_time:187487ms step_avg:148.09ms
step:1277/1375 train_time:187642ms step_avg:148.10ms
step:1278/1375 train_time:187796ms step_avg:148.10ms
step:1279/1375 train_time:187953ms step_avg:148.11ms
step:1280/1375 train_time:188115ms step_avg:148.12ms
step:1281/1375 train_time:188271ms step_avg:148.13ms
step:1282/1375 train_time:188425ms step_avg:148.13ms
step:1283/1375 train_time:188582ms step_avg:148.14ms
step:1284/1375 train_time:188741ms step_avg:148.15ms
step:1285/1375 train_time:188895ms step_avg:148.15ms
step:1286/1375 train_time:189050ms step_avg:148.16ms
step:1287/1375 train_time:189205ms step_avg:148.16ms
step:1288/1375 train_time:189361ms step_avg:148.17ms
step:1289/1375 train_time:189522ms step_avg:148.18ms
step:1290/1375 train_time:189682ms step_avg:148.19ms
step:1291/1375 train_time:189841ms step_avg:148.20ms
step:1292/1375 train_time:189996ms step_avg:148.20ms
step:1293/1375 train_time:190154ms step_avg:148.21ms
step:1294/1375 train_time:190309ms step_avg:148.22ms
step:1295/1375 train_time:190465ms step_avg:148.22ms
step:1296/1375 train_time:190622ms step_avg:148.23ms
step:1297/1375 train_time:190780ms step_avg:148.24ms
step:1298/1375 train_time:190933ms step_avg:148.24ms
step:1299/1375 train_time:191089ms step_avg:148.25ms
step:1300/1375 train_time:191245ms step_avg:148.25ms
step:1301/1375 train_time:191398ms step_avg:148.26ms
step:1302/1375 train_time:191558ms step_avg:148.26ms
step:1303/1375 train_time:191716ms step_avg:148.27ms
step:1304/1375 train_time:191875ms step_avg:148.28ms
step:1305/1375 train_time:192029ms step_avg:148.28ms
step:1306/1375 train_time:192189ms step_avg:148.29ms
step:1307/1375 train_time:192342ms step_avg:148.30ms
step:1308/1375 train_time:192497ms step_avg:148.30ms
step:1309/1375 train_time:192651ms step_avg:148.31ms
step:1310/1375 train_time:192807ms step_avg:148.31ms
step:1311/1375 train_time:192962ms step_avg:148.32ms
step:1312/1375 train_time:193114ms step_avg:148.32ms
step:1313/1375 train_time:193269ms step_avg:148.33ms
step:1314/1375 train_time:193426ms step_avg:148.33ms
step:1315/1375 train_time:193583ms step_avg:148.34ms
step:1316/1375 train_time:193735ms step_avg:148.34ms
step:1317/1375 train_time:193889ms step_avg:148.35ms
step:1318/1375 train_time:194049ms step_avg:148.36ms
step:1319/1375 train_time:194207ms step_avg:148.36ms
step:1320/1375 train_time:194363ms step_avg:148.37ms
step:1321/1375 train_time:194520ms step_avg:148.38ms
step:1322/1375 train_time:194683ms step_avg:148.39ms
step:1323/1375 train_time:194835ms step_avg:148.39ms
step:1324/1375 train_time:194989ms step_avg:148.39ms
step:1325/1375 train_time:195145ms step_avg:148.40ms
step:1326/1375 train_time:195304ms step_avg:148.41ms
step:1327/1375 train_time:195458ms step_avg:148.41ms
step:1328/1375 train_time:195612ms step_avg:148.42ms
step:1329/1375 train_time:195790ms step_avg:148.44ms
step:1330/1375 train_time:195948ms step_avg:148.45ms
step:1331/1375 train_time:196148ms step_avg:148.48ms
step:1332/1375 train_time:196317ms step_avg:148.50ms
step:1333/1375 train_time:196473ms step_avg:148.51ms
step:1334/1375 train_time:196628ms step_avg:148.51ms
step:1335/1375 train_time:196782ms step_avg:148.51ms
step:1336/1375 train_time:196946ms step_avg:148.53ms
step:1337/1375 train_time:197105ms step_avg:148.53ms
step:1338/1375 train_time:197261ms step_avg:148.54ms
step:1339/1375 train_time:197417ms step_avg:148.55ms
step:1340/1375 train_time:197576ms step_avg:148.55ms
step:1341/1375 train_time:197733ms step_avg:148.56ms
step:1342/1375 train_time:197893ms step_avg:148.57ms
step:1343/1375 train_time:198048ms step_avg:148.57ms
step:1344/1375 train_time:198206ms step_avg:148.58ms
step:1345/1375 train_time:198365ms step_avg:148.59ms
step:1346/1375 train_time:198520ms step_avg:148.59ms
step:1347/1375 train_time:198678ms step_avg:148.60ms
step:1348/1375 train_time:198833ms step_avg:148.60ms
step:1349/1375 train_time:198990ms step_avg:148.61ms
step:1350/1375 train_time:199146ms step_avg:148.62ms
step:1351/1375 train_time:199301ms step_avg:148.62ms
step:1352/1375 train_time:199464ms step_avg:148.63ms
step:1353/1375 train_time:199622ms step_avg:148.64ms
step:1354/1375 train_time:199783ms step_avg:148.65ms
step:1355/1375 train_time:199939ms step_avg:148.65ms
step:1356/1375 train_time:200093ms step_avg:148.66ms
step:1357/1375 train_time:200253ms step_avg:148.67ms
step:1358/1375 train_time:200413ms step_avg:148.67ms
step:1359/1375 train_time:200570ms step_avg:148.68ms
step:1360/1375 train_time:200732ms step_avg:148.69ms
step:1361/1375 train_time:200892ms step_avg:148.70ms
step:1362/1375 train_time:201049ms step_avg:148.70ms
step:1363/1375 train_time:201213ms step_avg:148.72ms
step:1364/1375 train_time:201368ms step_avg:148.72ms
step:1365/1375 train_time:201521ms step_avg:148.72ms
step:1366/1375 train_time:201678ms step_avg:148.73ms
step:1367/1375 train_time:201833ms step_avg:148.73ms
step:1368/1375 train_time:201991ms step_avg:148.74ms
step:1369/1375 train_time:202155ms step_avg:148.75ms
step:1370/1375 train_time:202314ms step_avg:148.76ms
step:1371/1375 train_time:202471ms step_avg:148.77ms
step:1372/1375 train_time:202634ms step_avg:148.78ms
step:1373/1375 train_time:202788ms step_avg:148.78ms
step:1374/1375 train_time:202946ms step_avg:148.79ms
step:1375/1375 train_time:203104ms step_avg:148.79ms
step:1375/1375 val_loss:3.2792 train_time:203179ms step_avg:148.85ms
peak memory consumption: 31563 MiB

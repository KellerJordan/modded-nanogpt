import os
import sys
with open(sys.argv[0]) as f:
    code = f.read() # read the code of this file ASAP, for logging
import uuid
import time
import glob
import subprocess
import contextlib
from dataclasses import dataclass

import torch
torch.empty(1, device='cuda', requires_grad=True).backward()
from torch import nn
import torch.nn.functional as F
import torch.distributed as dist
from torch.nn.parallel import DistributedDataParallel as DDP
# use of FlexAttention contributed by @KoszarskyB
from torch.nn.attention.flex_attention import BlockMask, flex_attention

# -----------------------------------------------------------------------------
# Muon optimizer

@torch.compile
def zeropower_via_newtonschulz5(G, steps):
    """
    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a
    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose
    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at
    zero even beyond the point where the iteration no longer converges all the way to one everywhere
    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T
    where S' is diagonal with S_{ii}' ~ Uniform(0.5, 1.5), which turns out not to hurt model
    performance at all relative to UV^T, where USV^T = G is the SVD.
    """
    assert len(G.shape) == 2
    a, b, c = (3.4445, -4.7750,  2.0315)
    X = G.bfloat16()
    if G.size(0) > G.size(1):
        X = X.T

    # Ensure spectral norm is at most 1
    X = X / (X.norm() + 1e-7)
    # Perform the NS iterations
    for _ in range(steps):
        A = X @ X.T
        B = b * A + c * A @ A # adapted from suggestion by @jxbz, @leloykun, and @YouJiacheng
        X = a * X + B @ X
    
    if G.size(0) > G.size(1):
        X = X.T
    return X

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Some warnings:
    - This optimizer assumes that all parameters passed in are 2D.
    - It should not be used for the embedding layer, the final fully connected layer, or any {0,1}-D
    parameters; those should all be optimized by a standard method (e.g., AdamW).
    - To use it with 4D convolutional filters, it works well to just flatten their last 3 dimensions.
    - We believe it is unlikely to work well for training with small batch size.
    - We believe it may not work well for finetuning pretrained models, but we haven't tested this.
    - We have not yet tried this optimizer for training scenarios larger than NanoGPT (124M).

    Arguments:
        lr: The learning rate used by the internal SGD.
        momentum: The momentum used by the internal SGD.
        nesterov: Whether to use Nesterov-style momentum in the internal SGD. (recommended)
        ns_steps: The number of Newton-Schulz iteration steps to use.
    """
    def __init__(self, params, lr=0.02, momentum=0.95, nesterov=True, ns_steps=5):
        self.world_size = int(os.environ['WORLD_SIZE'])
        self.rank = int(os.environ['RANK'])
        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, ns_steps=ns_steps)
        assert all(isinstance(p, torch.Tensor) for p in params)
        sizes = {p.numel() for p in params}
        param_groups = [dict(params=[p for p in params if p.numel() == size],
                             update_buffer=[torch.empty(size, device='cuda', dtype=torch.bfloat16) for _ in range(self.world_size)])
                        for size in sizes]
        super().__init__(param_groups, defaults)

    def step(self):

        for group in self.param_groups:

            lr = group['lr']
            momentum = group['momentum']
            nesterov = group['nesterov']
            ns_steps = group['ns_steps']
            update_buffers = group['update_buffer']
            # generate weight updates in distributed fashion
            params = group['params']
            handle = None
            params_world = None
            def update_prev():
                if params_world is None:
                    return
                assert handle is not None
                handle.wait()
                for p_world, g_world in zip(params_world, update_buffers):
                    p_world.data.add_(
                        g_world.view_as(p_world),
                        alpha=-lr * max(1, p_world.size(0) / p_world.size(1)) ** 0.5,
                    )
            for base_i in range(len(params))[::self.world_size]:
                if base_i + rank < len(params):
                    p = params[base_i + self.rank]
                    g = p.grad
                    assert g is not None
                    state = self.state[p]
                    if 'momentum_buffer' not in state:
                        state['momentum_buffer'] = torch.zeros_like(g)
                    buf = state['momentum_buffer']
                    buf.lerp_(g, 1 - momentum)
                    g = g.lerp_(buf, momentum) if nesterov else buf
                    g = zeropower_via_newtonschulz5(g, steps=ns_steps).flatten()
                else:
                    g = update_buffers[rank]
                update_prev() # async all_gather instead of sync all_reduce by @YouJiacheng
                handle = dist.all_gather(update_buffers, g, async_op=True)
                params_world = params[base_i : base_i + self.world_size]
            update_prev()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the GPT-2 model

def norm(x):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):

    def __init__(self, in_features, out_features):
        super().__init__(in_features, out_features, bias=False)

    def forward(self, x):
        return F.linear(x, self.weight.type_as(x))

class Rotary(nn.Module):

    def __init__(self, dim, max_seq_len=65536):
        super().__init__()
        # half-truncate RoPE by @YouJiacheng
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=dim//4, dtype=torch.float32)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(dim//4)])
        t = torch.arange(max_seq_len, dtype=torch.float32)
        theta = torch.einsum('i,j -> ij', t, angular_freq)
        self.cos = nn.Buffer(theta.cos(), persistent=False)
        self.sin = nn.Buffer(theta.sin(), persistent=False)

    def forward(self, x):
        cos, sin = self.cos[None, :x.size(-3), None, :], self.sin[None, :x.size(-3), None, :]
        x1, x2 = x.float().chunk(2, dim=-1)
        y1 = x1 * cos + x2 * sin
        y2 = x1 * (-sin) + x2 * cos
        return torch.cat((y1, y2), 3).type_as(x)

class CausalSelfAttention(nn.Module):

    def __init__(self, dim, num_heads, layer_id):
        super().__init__()
        assert dim % num_heads == 0
        self.num_heads = num_heads
        self.layer_id = layer_id
        self.c_q = CastedLinear(dim, dim)
        self.c_k = CastedLinear(dim, dim)
        self.c_v = CastedLinear(dim, dim)
        self.lambdas = nn.Parameter(torch.tensor([0.5, 0.5]))
        self.rotary = Rotary(dim // num_heads) # dim // num_heads = head_dim
        self.c_proj = CastedLinear(dim, dim)
        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977
        # self.attn_scale = nn.Parameter(torch.tensor(1.0 / (dim // num_heads) ** 0.5))
        # self.attn_scale = nn.Parameter(torch.tensor(0.20))
        self.attn_scale = 0.13 + 0.01 * min(layer_id, 11 - layer_id)

    def forward(self, x, ve, block_mask):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, 'Must use batch size = 1 for FlexAttention'
        q = self.c_q(x).view(B, T, self.num_heads, -1)
        k = self.c_k(x).view(B, T, self.num_heads, -1)
        v = self.c_v(x).view(B, T, self.num_heads, -1)
        if ve is not None:
            v = self.lambdas[0] * v + self.lambdas[1] * ve.view_as(v) # @KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = self.lambdas[0] * v
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = self.rotary(q), self.rotary(k)
        # y = flex_attention(q.transpose(1, 2) * self.attn_scale, k.transpose(1, 2), v.transpose(1, 2), block_mask=block_mask, scale=1.)
        y = flex_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), block_mask=block_mask, scale=self.attn_scale)
        y = y.transpose(1, 2).contiguous().view_as(x) # re-assemble all head outputs side by side
        y = self.c_proj(y)
        return y

class MLP(nn.Module):

    def __init__(self, dim):
        super().__init__()
        self.c_fc = CastedLinear(dim, 4 * dim)
        self.c_proj = CastedLinear(4 * dim, dim)
        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977

    def forward(self, x):
        x = self.c_fc(x)
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = self.c_proj(x)
        return x

class Block(nn.Module):

    def __init__(self, model_dim, num_heads, layer_id, use_attn=True):
        super().__init__()
        self.attn = CausalSelfAttention(model_dim, num_heads, layer_id) if use_attn else None
        self.mlp = MLP(model_dim)
        self.lambdas = nn.Parameter(torch.tensor([1., 0.]))

    def forward(self, x, ve, x0, block_mask):
        x = self.lambdas[0] * x + self.lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), ve, block_mask)
        x = x + self.mlp(norm(x))
        return x

class ValueEmbedding(nn.Module):
    def __init__(self, vocab_size, model_dim):
        super().__init__()
        self.embed = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])

    def forward(self, inputs):
        ve = [emb(inputs).bfloat16() for emb in self.embed]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        ve = [ve[0], ve[1], ve[2], None, None, None, None, None, None, ve[0], ve[1], ve[2]]
        return ve

# -----------------------------------------------------------------------------
# The main GPT-2 model

class GPT(nn.Module):

    def __init__(self, vocab_size, num_layers, num_heads, model_dim):
        super().__init__()
        self.embed = nn.Embedding(vocab_size, model_dim)
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.blocks = nn.ModuleList([Block(model_dim, num_heads, layer_id=layer_id, use_attn=(layer_id != 7))
                                     for layer_id in range(num_layers)])
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual learning
        # U-net structure on token value embeddings by @leloykun
        self.value_embeds = ValueEmbedding(vocab_size, model_dim)
        self.lm_head = CastedLinear(model_dim, vocab_size)
        self.lm_head.weight.data.zero_() # @Grad62304977
        # U-net design by @brendanh0gan
        self.num_encoder_layers = num_layers // 2 # Half of the layers for encoder
        self.num_decoder_layers = num_layers - self.num_encoder_layers # Remaining for decoder
        # Add learnable skip connection weights for decoder layers
        self.skip_weights = nn.Parameter(torch.ones(self.num_decoder_layers))

    def forward(self, inputs, targets, sliding_window_num_blocks):
        BLOCK_SIZE = 128
        seq_len = len(inputs)
        assert seq_len % BLOCK_SIZE == 0
        total_num_blocks = seq_len // BLOCK_SIZE
        assert inputs.ndim == 1
        docs = (inputs == 50256).cumsum(0)
        docs_low = docs.view(-1, BLOCK_SIZE)[:, 0].contiguous()
        docs_high = docs.view(-1, BLOCK_SIZE)[:, -1].contiguous()

        def document_causal(b, h, q_idx, kv_idx):
            causal_mask = q_idx >= kv_idx
            document_mask = docs[q_idx] == docs[kv_idx]
            return causal_mask & document_mask

        def dense_to_ordered(dense_mask):
            num_blocks = dense_mask.sum(dim=-1, dtype=torch.int32)
            indices = dense_mask.argsort(dim=-1, descending=True, stable=True).to(torch.int32)
            return num_blocks[None, None].contiguous(), indices[None, None].contiguous()

        def create_doc_swc_block_mask(sliding_window_num_blocks):
            kv_idx = block_idx = torch.arange(total_num_blocks, dtype=torch.int32, device='cuda')
            q_idx = block_idx[:, None]
            causal_bm = q_idx >= kv_idx
            causal_full_bm = q_idx > kv_idx
            window_bm = q_idx - kv_idx < sliding_window_num_blocks
            window_full_bm = window_bm # block-wise sliding window by @YouJiacheng
            # document_bm = (docs_low[q_idx] <= docs_high[kv_idx]) & (docs_low[kv_idx] <= docs_high[q_idx])
            document_bm = (docs_low[:, None] <= docs_high) & (docs_low <= docs_high[:, None])
            document_full_bm = (docs_low[:, None] == docs_high) & (docs_low == docs_high[:, None])
            nonzero_bm = causal_bm & window_bm & document_bm
            full_bm  = causal_full_bm & window_full_bm & document_full_bm
            kv_num_blocks, kv_indices = dense_to_ordered(nonzero_bm & ~full_bm)
            full_kv_num_blocks, full_kv_indices = dense_to_ordered(full_bm)
            return BlockMask.from_kv_blocks(
                kv_num_blocks,
                kv_indices,
                full_kv_num_blocks,
                full_kv_indices,
                BLOCK_SIZE=BLOCK_SIZE,
                mask_mod=document_causal,
            )

        block_mask = create_doc_swc_block_mask(sliding_window_num_blocks)

        x0 = norm(self.embed(inputs[None]).bfloat16()) # use of norm here by @Grad62304977
        x = x0
        ve = self.value_embeds(inputs)
        assert len(ve) == len(self.blocks)
        ve_enc, ve_dec = ve[:self.num_encoder_layers], ve[self.num_encoder_layers:]

        # Store outputs for U-Net skip connections
        skip_connections = []
        # Encoder pass - process only the first half of the blocks
        for i in range(self.num_encoder_layers):
            x = self.blocks[i](x, ve_enc[i], x0, block_mask)
            skip_connections.append(x)
        # Decoder pass - process the remaining blocks with weighted skip connections
        for i in range(self.num_decoder_layers):
            x = x + self.skip_weights[i] * skip_connections.pop()
            # U-net structure on token value embeddings by @leloykun
            x = self.blocks[self.num_encoder_layers + i](x, ve_dec[i], x0, block_mask)

        x = norm(x)
        logits = self.lm_head(x)
        logits = 15 * torch.tanh(logits / 15) # @Grad62304977 added tanh softcapping, @KoszarskyB reduced it from 30 to 15
        logits = logits.float()
        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets)
        return loss

# -----------------------------------------------------------------------------
# Our own simple Distributed Data Loader

def _load_data_shard(path):
    # only reads the header, returns header data
    # header is 256 int32
    header = torch.from_file(path, False, 256, dtype=torch.int32)
    assert header[0] == 20240520, 'magic number mismatch in the data .bin file'
    assert header[1] == 1, 'unsupported version'
    num_tokens = int(header[2]) # number of tokens (claimed)
    with open(path, 'rb', buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, 'number of tokens read does not match header'
    return tokens

class DistributedDataLoader:

    def __init__(self, filename_pattern):
        self.rank = int(os.environ['RANK'])
        self.world_size = int(os.environ['WORLD_SIZE'])
        self.files = sorted(glob.glob(filename_pattern))
        self.reset()

    def reset(self):
        self.current_shard = -1
        self.advance()

    def advance(self):
        self.current_shard = (self.current_shard + 1) % len(self.files)
        self.current_position = 0
        self.tokens = _load_data_shard(self.files[self.current_shard])

    def next_batch(self, batch_size):
        assert batch_size % self.world_size == 0
        device_batch_size = batch_size // self.world_size
        # load next shard if necessary
        if self.current_position + batch_size + 1 >= len(self.tokens):
            self.advance()
        pos = self.current_position + self.rank * device_batch_size
        device_batch_tokens = self.tokens[pos:pos+device_batch_size+1]
        # advance current position
        self.current_position += batch_size
        inputs = device_batch_tokens[:-1].to(device='cuda', dtype=torch.int32, non_blocking=True)
        targets = device_batch_tokens[1:].to(device='cuda', dtype=torch.int64, non_blocking=True)
        return inputs, targets

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files = 'data/fineweb10B/fineweb_train_*.bin' # input .bin to train on
    val_files = 'data/fineweb10B/fineweb_val_*.bin' # input .bin to eval validation loss on
    val_tokens = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # optimization
    batch_size = 8*64*1024 # batch size in tokens
    num_iterations = 1375 # number of iterations to run
    cooldown_frac = 0.4 # fraction of training spent cooling down the learning rate
    bf16_embeds = True
    # evaluation and logging
    val_loss_every = 125 # every how many steps to evaluate val loss? 0 for only at the end
    # implementation
    max_device_batch_size = 64*1024 # batch size per device in tokens
    save_checkpoint = False
args = Hyperparameters()

micro_bs = args.max_device_batch_size

# set up DDP (distributed data parallel). torchrun sets this env variable
rank = int(os.environ['RANK'])
local_rank = int(os.environ['LOCAL_RANK'])
world_size = int(os.environ['WORLD_SIZE'])
assert torch.cuda.is_available()
torch.cuda.set_device(local_rank)
dist.init_process_group(backend='nccl', device_id=torch.device(local_rank))
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = uuid.uuid4()
    os.makedirs('logs', exist_ok=True)
    logfile = f'logs/{run_id}.txt'
    print(logfile)

def print0(s, console=False):
    if master_process:
        with open(logfile, 'a') as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0('='*100)
# log information about the hardware/software environment this is running on
print0(f'Running Python {sys.version}')
print0(f'Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}')
print0(subprocess.run(['nvidia-smi'], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout)
print0('='*100)

# load data
train_loader = DistributedDataLoader(args.train_files)
val_loader = DistributedDataLoader(args.val_files)
print0(f'Training dataloader files: {train_loader.files}')
print0(f'Validation dataloader files: {val_loader.files}')
print0('='*100)

# there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency. suggested to me by @Grad62304977.
# this originates from Karpathy's experiments.
model = GPT(vocab_size=50304, num_layers=12, num_heads=6, model_dim=768)
model = model.cuda()
if args.bf16_embeds:
    for m in model.modules():
        if isinstance(m, nn.Embedding):
            m.bfloat16()
model: GPT = torch.compile(model)
ddp_model = DDP(model, device_ids=[local_rank], broadcast_buffers=False, gradient_as_bucket_view=True)

# collect the parameters to optimize
hidden_matrix_params = [p for p in model.blocks.parameters() if p.ndim == 2]
embed_params = [model.embed.weight, *model.value_embeds.parameters()]
scalar_params = [p for n, p in model.named_parameters() if p.ndim < 2 and "attn_scale" not in n]
attn_scale_params = [p for n, p in model.named_parameters() if p.ndim < 2 and "attn_scale" in n]
head_params = [model.lm_head.weight]

# init the optimizer(s)
optimizer1 = torch.optim.Adam([dict(params=embed_params, lr=0.6),
                               dict(params=head_params, lr=0.008),
                               dict(params=scalar_params, lr=0.04),
                               dict(params=attn_scale_params, lr=0.01)],
                              betas=(0.8, 0.95), fused=True)
optimizer2 = Muon(hidden_matrix_params, lr=0.05, momentum=0.95)
optimizers = [optimizer1, optimizer2]

# learning rate schedule: stable then decay
def get_lr(it):
    t = 1 - it / args.num_iterations # time remaining in training
    assert 1 >= t > 0
    # 1) constant lr for first part of training
    if t >= args.cooldown_frac:
        return 1.0
    # 2) then linear cooldown
    else:
        return t / args.cooldown_frac
schedulers = [torch.optim.lr_scheduler.LambdaLR(opt, get_lr) for opt in optimizers]

# sliding window size schedule: linear increase over training in chunks of 128 from 128 -> 1792. By @fernbear.bsky.social
def get_sliding_window_blocks(it):
    x = it / args.num_iterations # training progress
    assert 0 <= x <= 1
    return int(((1 - x) * 128 + x * 1856) // 128)
sliding_window_num_blocks = torch.tensor(1, dtype=torch.int32, device='cuda')

# Start training loop
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    # This effectively ignores timing first 10 steps, which are slower for weird reasons.
    # Alternately, and slightly more correctly in terms of benchmarking, we could do 10
    # steps with dummy data first, and then re-initialize the model and reset the loader.
    if step == 10:
        training_time_ms = 0
        t0 = time.perf_counter()
    timed_steps = float('nan') if step <= 11 else (step - 10) + 1 # <= 11 to avoid bug in val

    sliding_window_num_blocks.copy_(get_sliding_window_blocks(step))

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        # run validation batches
        model.eval()
        val_loader.reset()
        val_loss = 0.0
        # calculate the number of steps to take in the val loop.
        val_batch_size = world_size * micro_bs
        assert args.val_tokens % val_batch_size == 0
        val_steps = args.val_tokens // val_batch_size
        for _ in range(val_steps):
            with torch.no_grad():
                inputs_val, targets_val = val_loader.next_batch(val_batch_size)
                val_loss += ddp_model(inputs_val, targets_val, sliding_window_num_blocks)
        # Print attention scales
        for n, p in model.named_parameters():
            if p.ndim < 2 and "attn_scale" in n:
                print0(f'{n}: {p.item()}', console=True)
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        val_loss /= val_steps
        # logging
        print0(f'step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/(timed_steps-1):.2f}ms', console=True)
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f'logs/{run_id}', exist_ok=True)
            torch.save(log, f'logs/{run_id}/state_step{step:06d}.pt')
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    model.train()
    batch_size = args.batch_size
    assert batch_size % world_size == 0
    inputs_train, targets_train = train_loader.next_batch(batch_size)
    assert len(inputs_train) <= micro_bs or len(inputs_train) % micro_bs == 0
    for micro_inputs_train, micro_targets_train in zip(inputs_train.split(micro_bs), targets_train.split(micro_bs)):
        ddp_model(micro_inputs_train, micro_targets_train, sliding_window_num_blocks).backward()
    # momentum warmup for Muon
    frac = min(step/300, 1)
    for group in optimizer2.param_groups:
        group['momentum'] = (1 - frac) * 0.85 + frac * 0.95
    # step the optimizers and schedulers
    for opt, sched in zip(optimizers, schedulers):
        opt.step()
        if step != train_steps-1:
            sched.step()
    # null the gradients
    model.zero_grad(set_to_none=True)
    # logging
    approx_time = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f'step:{step+1}/{train_steps} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms', console=True)

print0(f'peak memory consumption: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB')
dist.destroy_process_group()

====================================================================================================
Running Python 3.10.12 (main, Sep 11 2024, 15:47:36) [GCC 11.4.0]
Running PyTorch 2.6.0.dev20241231+cu126 compiled for CUDA 12.6
Tue Jan 14 14:40:32 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.183.06             Driver Version: 535.183.06   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:19:00.0 Off |                    0 |
| N/A   39C    P0             126W / 700W |   7713MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:3B:00.0 Off |                    0 |
| N/A   32C    P0             114W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:4C:00.0 Off |                    0 |
| N/A   29C    P0             117W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   38C    P0             129W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:9B:00.0 Off |                    0 |
| N/A   39C    P0             124W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:BB:00.0 Off |                    0 |
| N/A   30C    P0             115W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:CB:00.0 Off |                    0 |
| N/A   37C    P0             116W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:DB:00.0 Off |                    0 |
| N/A   30C    P0             117W / 700W |   3211MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
Training dataloader files: ['data/fineweb10B/fineweb_train_000001.bin', 'data/fineweb10B/fineweb_train_000002.bin', 'data/fineweb10B/fineweb_train_000003.bin', 'data/fineweb10B/fineweb_train_000004.bin', 'data/fineweb10B/fineweb_train_000005.bin', 'data/fineweb10B/fineweb_train_000006.bin', 'data/fineweb10B/fineweb_train_000007.bin', 'data/fineweb10B/fineweb_train_000008.bin', 'data/fineweb10B/fineweb_train_000009.bin']
Validation dataloader files: ['data/fineweb10B/fineweb_val_000000.bin']
====================================================================================================
step:0/1375 val_loss:10.8258 train_time:0ms step_avg:nanms
step:1/1375 train_time:27376ms step_avg:nanms
step:2/1375 train_time:27460ms step_avg:nanms
step:3/1375 train_time:27643ms step_avg:nanms
step:4/1375 train_time:27775ms step_avg:nanms
step:5/1375 train_time:27908ms step_avg:nanms
step:6/1375 train_time:28042ms step_avg:nanms
step:7/1375 train_time:28173ms step_avg:nanms
step:8/1375 train_time:28308ms step_avg:nanms
step:9/1375 train_time:28442ms step_avg:nanms
step:10/1375 train_time:28583ms step_avg:nanms
step:11/1375 train_time:136ms step_avg:nanms
step:12/1375 train_time:270ms step_avg:nanms
step:13/1375 train_time:405ms step_avg:135.08ms
step:14/1375 train_time:538ms step_avg:134.56ms
step:15/1375 train_time:672ms step_avg:134.39ms
step:16/1375 train_time:806ms step_avg:134.40ms
step:17/1375 train_time:943ms step_avg:134.78ms
step:18/1375 train_time:1081ms step_avg:135.18ms
step:19/1375 train_time:1219ms step_avg:135.41ms
step:20/1375 train_time:1353ms step_avg:135.30ms
step:21/1375 train_time:1488ms step_avg:135.27ms
step:22/1375 train_time:1621ms step_avg:135.12ms
step:23/1375 train_time:1758ms step_avg:135.20ms
step:24/1375 train_time:1892ms step_avg:135.17ms
step:25/1375 train_time:2029ms step_avg:135.26ms
step:26/1375 train_time:2165ms step_avg:135.33ms
step:27/1375 train_time:2302ms step_avg:135.42ms
step:28/1375 train_time:2437ms step_avg:135.39ms
step:29/1375 train_time:2573ms step_avg:135.41ms
step:30/1375 train_time:2706ms step_avg:135.29ms
step:31/1375 train_time:2841ms step_avg:135.29ms
step:32/1375 train_time:2977ms step_avg:135.30ms
step:33/1375 train_time:3111ms step_avg:135.27ms
step:34/1375 train_time:3247ms step_avg:135.28ms
step:35/1375 train_time:3383ms step_avg:135.33ms
step:36/1375 train_time:3519ms step_avg:135.35ms
step:37/1375 train_time:3654ms step_avg:135.33ms
step:38/1375 train_time:3789ms step_avg:135.30ms
step:39/1375 train_time:3923ms step_avg:135.27ms
step:40/1375 train_time:4059ms step_avg:135.30ms
step:41/1375 train_time:4194ms step_avg:135.30ms
step:42/1375 train_time:4329ms step_avg:135.28ms
step:43/1375 train_time:4465ms step_avg:135.29ms
step:44/1375 train_time:4600ms step_avg:135.30ms
step:45/1375 train_time:4734ms step_avg:135.27ms
step:46/1375 train_time:4869ms step_avg:135.26ms
step:47/1375 train_time:5005ms step_avg:135.28ms
step:48/1375 train_time:5141ms step_avg:135.28ms
step:49/1375 train_time:5278ms step_avg:135.34ms
step:50/1375 train_time:5414ms step_avg:135.35ms
step:51/1375 train_time:5547ms step_avg:135.30ms
step:52/1375 train_time:5683ms step_avg:135.31ms
step:53/1375 train_time:5819ms step_avg:135.32ms
step:54/1375 train_time:5956ms step_avg:135.36ms
step:55/1375 train_time:6092ms step_avg:135.37ms
step:56/1375 train_time:6227ms step_avg:135.38ms
step:57/1375 train_time:6363ms step_avg:135.38ms
step:58/1375 train_time:6500ms step_avg:135.42ms
step:59/1375 train_time:6635ms step_avg:135.42ms
step:60/1375 train_time:6771ms step_avg:135.42ms
step:61/1375 train_time:6907ms step_avg:135.42ms
step:62/1375 train_time:7041ms step_avg:135.40ms
step:63/1375 train_time:7178ms step_avg:135.43ms
step:64/1375 train_time:7312ms step_avg:135.41ms
step:65/1375 train_time:7448ms step_avg:135.42ms
step:66/1375 train_time:7584ms step_avg:135.42ms
step:67/1375 train_time:7719ms step_avg:135.43ms
step:68/1375 train_time:7855ms step_avg:135.43ms
step:69/1375 train_time:7990ms step_avg:135.43ms
step:70/1375 train_time:8124ms step_avg:135.40ms
step:71/1375 train_time:8260ms step_avg:135.41ms
step:72/1375 train_time:8397ms step_avg:135.44ms
step:73/1375 train_time:8531ms step_avg:135.41ms
step:74/1375 train_time:8667ms step_avg:135.42ms
step:75/1375 train_time:8802ms step_avg:135.42ms
step:76/1375 train_time:8938ms step_avg:135.43ms
step:77/1375 train_time:9074ms step_avg:135.44ms
step:78/1375 train_time:9210ms step_avg:135.44ms
step:79/1375 train_time:9345ms step_avg:135.43ms
step:80/1375 train_time:9480ms step_avg:135.43ms
step:81/1375 train_time:9616ms step_avg:135.43ms
step:82/1375 train_time:9753ms step_avg:135.45ms
step:83/1375 train_time:9887ms step_avg:135.44ms
step:84/1375 train_time:10022ms step_avg:135.44ms
step:85/1375 train_time:10159ms step_avg:135.45ms
step:86/1375 train_time:10295ms step_avg:135.46ms
step:87/1375 train_time:10429ms step_avg:135.45ms
step:88/1375 train_time:10565ms step_avg:135.44ms
step:89/1375 train_time:10702ms step_avg:135.47ms
step:90/1375 train_time:10838ms step_avg:135.47ms
step:91/1375 train_time:10975ms step_avg:135.49ms
step:92/1375 train_time:11110ms step_avg:135.48ms
step:93/1375 train_time:11245ms step_avg:135.48ms
step:94/1375 train_time:11380ms step_avg:135.48ms
step:95/1375 train_time:11517ms step_avg:135.50ms
step:96/1375 train_time:11653ms step_avg:135.49ms
step:97/1375 train_time:11787ms step_avg:135.49ms
step:98/1375 train_time:11924ms step_avg:135.50ms
step:99/1375 train_time:12060ms step_avg:135.51ms
step:100/1375 train_time:12199ms step_avg:135.54ms
step:101/1375 train_time:12332ms step_avg:135.52ms
step:102/1375 train_time:12468ms step_avg:135.52ms
step:103/1375 train_time:12605ms step_avg:135.54ms
step:104/1375 train_time:12744ms step_avg:135.58ms
step:105/1375 train_time:12884ms step_avg:135.62ms
step:106/1375 train_time:13023ms step_avg:135.66ms
step:107/1375 train_time:13162ms step_avg:135.69ms
step:108/1375 train_time:13299ms step_avg:135.70ms
step:109/1375 train_time:13436ms step_avg:135.72ms
step:110/1375 train_time:13577ms step_avg:135.77ms
step:111/1375 train_time:13716ms step_avg:135.80ms
step:112/1375 train_time:13855ms step_avg:135.84ms
step:113/1375 train_time:13993ms step_avg:135.86ms
step:114/1375 train_time:14131ms step_avg:135.87ms
step:115/1375 train_time:14269ms step_avg:135.89ms
step:116/1375 train_time:14406ms step_avg:135.91ms
step:117/1375 train_time:14544ms step_avg:135.93ms
step:118/1375 train_time:14684ms step_avg:135.97ms
step:119/1375 train_time:14826ms step_avg:136.02ms
step:120/1375 train_time:14964ms step_avg:136.04ms
step:121/1375 train_time:15103ms step_avg:136.06ms
step:122/1375 train_time:15241ms step_avg:136.08ms
step:123/1375 train_time:15381ms step_avg:136.11ms
step:124/1375 train_time:15520ms step_avg:136.14ms
step:125/1375 train_time:15659ms step_avg:136.17ms
step:125/1375 val_loss:4.3702 train_time:15729ms step_avg:136.77ms
step:126/1375 train_time:15801ms step_avg:136.22ms
step:127/1375 train_time:15946ms step_avg:136.29ms
step:128/1375 train_time:16085ms step_avg:136.31ms
step:129/1375 train_time:16222ms step_avg:136.32ms
step:130/1375 train_time:16360ms step_avg:136.33ms
step:131/1375 train_time:16498ms step_avg:136.35ms
step:132/1375 train_time:16638ms step_avg:136.38ms
step:133/1375 train_time:16777ms step_avg:136.40ms
step:134/1375 train_time:16920ms step_avg:136.45ms
step:135/1375 train_time:17061ms step_avg:136.49ms
step:136/1375 train_time:17200ms step_avg:136.51ms
step:137/1375 train_time:17339ms step_avg:136.53ms
step:138/1375 train_time:17476ms step_avg:136.53ms
step:139/1375 train_time:17616ms step_avg:136.56ms
step:140/1375 train_time:17755ms step_avg:136.58ms
step:141/1375 train_time:17896ms step_avg:136.61ms
step:142/1375 train_time:18036ms step_avg:136.63ms
step:143/1375 train_time:18173ms step_avg:136.64ms
step:144/1375 train_time:18313ms step_avg:136.66ms
step:145/1375 train_time:18450ms step_avg:136.67ms
step:146/1375 train_time:18589ms step_avg:136.68ms
step:147/1375 train_time:18727ms step_avg:136.69ms
step:148/1375 train_time:18865ms step_avg:136.70ms
step:149/1375 train_time:19004ms step_avg:136.72ms
step:150/1375 train_time:19144ms step_avg:136.74ms
step:151/1375 train_time:19283ms step_avg:136.76ms
step:152/1375 train_time:19422ms step_avg:136.78ms
step:153/1375 train_time:19561ms step_avg:136.79ms
step:154/1375 train_time:19700ms step_avg:136.81ms
step:155/1375 train_time:19840ms step_avg:136.83ms
step:156/1375 train_time:19979ms step_avg:136.84ms
step:157/1375 train_time:20119ms step_avg:136.86ms
step:158/1375 train_time:20260ms step_avg:136.89ms
step:159/1375 train_time:20400ms step_avg:136.91ms
step:160/1375 train_time:20540ms step_avg:136.94ms
step:161/1375 train_time:20679ms step_avg:136.95ms
step:162/1375 train_time:20819ms step_avg:136.97ms
step:163/1375 train_time:20959ms step_avg:136.98ms
step:164/1375 train_time:21098ms step_avg:137.00ms
step:165/1375 train_time:21238ms step_avg:137.02ms
step:166/1375 train_time:21377ms step_avg:137.03ms
step:167/1375 train_time:21518ms step_avg:137.06ms
step:168/1375 train_time:21658ms step_avg:137.07ms
step:169/1375 train_time:21797ms step_avg:137.09ms
step:170/1375 train_time:21936ms step_avg:137.10ms
step:171/1375 train_time:22075ms step_avg:137.11ms
step:172/1375 train_time:22214ms step_avg:137.13ms
step:173/1375 train_time:22355ms step_avg:137.15ms
step:174/1375 train_time:22494ms step_avg:137.16ms
step:175/1375 train_time:22634ms step_avg:137.18ms
step:176/1375 train_time:22771ms step_avg:137.18ms
step:177/1375 train_time:22910ms step_avg:137.19ms
step:178/1375 train_time:23048ms step_avg:137.19ms
step:179/1375 train_time:23188ms step_avg:137.20ms
step:180/1375 train_time:23326ms step_avg:137.21ms
step:181/1375 train_time:23466ms step_avg:137.23ms
step:182/1375 train_time:23606ms step_avg:137.24ms
step:183/1375 train_time:23746ms step_avg:137.26ms
step:184/1375 train_time:23886ms step_avg:137.28ms
step:185/1375 train_time:24027ms step_avg:137.30ms
step:186/1375 train_time:24166ms step_avg:137.30ms
step:187/1375 train_time:24304ms step_avg:137.31ms
step:188/1375 train_time:24443ms step_avg:137.32ms
step:189/1375 train_time:24583ms step_avg:137.34ms
step:190/1375 train_time:24723ms step_avg:137.35ms
step:191/1375 train_time:24910ms step_avg:137.63ms
step:192/1375 train_time:25047ms step_avg:137.62ms
step:193/1375 train_time:25185ms step_avg:137.62ms
step:194/1375 train_time:25323ms step_avg:137.63ms
step:195/1375 train_time:25460ms step_avg:137.62ms
step:196/1375 train_time:25600ms step_avg:137.63ms
step:197/1375 train_time:25741ms step_avg:137.65ms
step:198/1375 train_time:25883ms step_avg:137.68ms
step:199/1375 train_time:26024ms step_avg:137.69ms
step:200/1375 train_time:26163ms step_avg:137.70ms
step:201/1375 train_time:26302ms step_avg:137.71ms
step:202/1375 train_time:26441ms step_avg:137.71ms
step:203/1375 train_time:26579ms step_avg:137.72ms
step:204/1375 train_time:26720ms step_avg:137.73ms
step:205/1375 train_time:26863ms step_avg:137.76ms
step:206/1375 train_time:27006ms step_avg:137.78ms
step:207/1375 train_time:27148ms step_avg:137.80ms
step:208/1375 train_time:27290ms step_avg:137.83ms
step:209/1375 train_time:27433ms step_avg:137.85ms
step:210/1375 train_time:27572ms step_avg:137.86ms
step:211/1375 train_time:27713ms step_avg:137.87ms
step:212/1375 train_time:27857ms step_avg:137.90ms
step:213/1375 train_time:28001ms step_avg:137.93ms
step:214/1375 train_time:28144ms step_avg:137.96ms
step:215/1375 train_time:28285ms step_avg:137.97ms
step:216/1375 train_time:28427ms step_avg:137.99ms
step:217/1375 train_time:28568ms step_avg:138.01ms
step:218/1375 train_time:28711ms step_avg:138.03ms
step:219/1375 train_time:28854ms step_avg:138.06ms
step:220/1375 train_time:28997ms step_avg:138.08ms
step:221/1375 train_time:29138ms step_avg:138.10ms
step:222/1375 train_time:29280ms step_avg:138.11ms
step:223/1375 train_time:29423ms step_avg:138.14ms
step:224/1375 train_time:29564ms step_avg:138.15ms
step:225/1375 train_time:29706ms step_avg:138.17ms
step:226/1375 train_time:29847ms step_avg:138.18ms
step:227/1375 train_time:29990ms step_avg:138.20ms
step:228/1375 train_time:30133ms step_avg:138.22ms
step:229/1375 train_time:30274ms step_avg:138.24ms
step:230/1375 train_time:30417ms step_avg:138.26ms
step:231/1375 train_time:30559ms step_avg:138.28ms
step:232/1375 train_time:30700ms step_avg:138.29ms
step:233/1375 train_time:30842ms step_avg:138.31ms
step:234/1375 train_time:30985ms step_avg:138.33ms
step:235/1375 train_time:31126ms step_avg:138.34ms
step:236/1375 train_time:31267ms step_avg:138.35ms
step:237/1375 train_time:31408ms step_avg:138.36ms
step:238/1375 train_time:31553ms step_avg:138.39ms
step:239/1375 train_time:31695ms step_avg:138.41ms
step:240/1375 train_time:31835ms step_avg:138.41ms
step:241/1375 train_time:31977ms step_avg:138.43ms
step:242/1375 train_time:32119ms step_avg:138.44ms
step:243/1375 train_time:32261ms step_avg:138.46ms
step:244/1375 train_time:32402ms step_avg:138.47ms
step:245/1375 train_time:32543ms step_avg:138.48ms
step:246/1375 train_time:32685ms step_avg:138.49ms
step:247/1375 train_time:32828ms step_avg:138.51ms
step:248/1375 train_time:32970ms step_avg:138.53ms
step:249/1375 train_time:33113ms step_avg:138.55ms
step:250/1375 train_time:33256ms step_avg:138.57ms
step:250/1375 val_loss:3.9522 train_time:33324ms step_avg:138.85ms
step:251/1375 train_time:33400ms step_avg:138.59ms
step:252/1375 train_time:33544ms step_avg:138.61ms
step:253/1375 train_time:33687ms step_avg:138.63ms
step:254/1375 train_time:33829ms step_avg:138.64ms
step:255/1375 train_time:33969ms step_avg:138.65ms
step:256/1375 train_time:34110ms step_avg:138.66ms
step:257/1375 train_time:34252ms step_avg:138.67ms
step:258/1375 train_time:34395ms step_avg:138.69ms
step:259/1375 train_time:34541ms step_avg:138.72ms
step:260/1375 train_time:34683ms step_avg:138.73ms
step:261/1375 train_time:34824ms step_avg:138.74ms
step:262/1375 train_time:34966ms step_avg:138.76ms
step:263/1375 train_time:35107ms step_avg:138.76ms
step:264/1375 train_time:35248ms step_avg:138.77ms
step:265/1375 train_time:35389ms step_avg:138.78ms
step:266/1375 train_time:35534ms step_avg:138.81ms
step:267/1375 train_time:35677ms step_avg:138.82ms
step:268/1375 train_time:35819ms step_avg:138.83ms
step:269/1375 train_time:35960ms step_avg:138.84ms
step:270/1375 train_time:36102ms step_avg:138.85ms
step:271/1375 train_time:36244ms step_avg:138.86ms
step:272/1375 train_time:36386ms step_avg:138.88ms
step:273/1375 train_time:36528ms step_avg:138.89ms
step:274/1375 train_time:36670ms step_avg:138.90ms
step:275/1375 train_time:36812ms step_avg:138.91ms
step:276/1375 train_time:36952ms step_avg:138.92ms
step:277/1375 train_time:37093ms step_avg:138.92ms
step:278/1375 train_time:37235ms step_avg:138.94ms
step:279/1375 train_time:37377ms step_avg:138.95ms
step:280/1375 train_time:37519ms step_avg:138.96ms
step:281/1375 train_time:37661ms step_avg:138.97ms
step:282/1375 train_time:37804ms step_avg:138.98ms
step:283/1375 train_time:37946ms step_avg:139.00ms
step:284/1375 train_time:38087ms step_avg:139.00ms
step:285/1375 train_time:38228ms step_avg:139.01ms
step:286/1375 train_time:38371ms step_avg:139.02ms
step:287/1375 train_time:38512ms step_avg:139.03ms
step:288/1375 train_time:38654ms step_avg:139.04ms
step:289/1375 train_time:38796ms step_avg:139.05ms
step:290/1375 train_time:38939ms step_avg:139.07ms
step:291/1375 train_time:39079ms step_avg:139.07ms
step:292/1375 train_time:39220ms step_avg:139.08ms
step:293/1375 train_time:39364ms step_avg:139.10ms
step:294/1375 train_time:39506ms step_avg:139.11ms
step:295/1375 train_time:39649ms step_avg:139.12ms
step:296/1375 train_time:39791ms step_avg:139.13ms
step:297/1375 train_time:39934ms step_avg:139.14ms
step:298/1375 train_time:40075ms step_avg:139.15ms
step:299/1375 train_time:40217ms step_avg:139.16ms
step:300/1375 train_time:40358ms step_avg:139.17ms
step:301/1375 train_time:40500ms step_avg:139.17ms
step:302/1375 train_time:40643ms step_avg:139.19ms
step:303/1375 train_time:40786ms step_avg:139.20ms
step:304/1375 train_time:40927ms step_avg:139.21ms
step:305/1375 train_time:41069ms step_avg:139.22ms
step:306/1375 train_time:41211ms step_avg:139.23ms
step:307/1375 train_time:41353ms step_avg:139.24ms
step:308/1375 train_time:41496ms step_avg:139.25ms
step:309/1375 train_time:41641ms step_avg:139.27ms
step:310/1375 train_time:41784ms step_avg:139.28ms
step:311/1375 train_time:41929ms step_avg:139.30ms
step:312/1375 train_time:42073ms step_avg:139.31ms
step:313/1375 train_time:42218ms step_avg:139.33ms
step:314/1375 train_time:42361ms step_avg:139.35ms
step:315/1375 train_time:42506ms step_avg:139.36ms
step:316/1375 train_time:42651ms step_avg:139.38ms
step:317/1375 train_time:42793ms step_avg:139.39ms
step:318/1375 train_time:42937ms step_avg:139.41ms
step:319/1375 train_time:43080ms step_avg:139.42ms
step:320/1375 train_time:43226ms step_avg:139.44ms
step:321/1375 train_time:43372ms step_avg:139.46ms
step:322/1375 train_time:43516ms step_avg:139.47ms
step:323/1375 train_time:43659ms step_avg:139.49ms
step:324/1375 train_time:43802ms step_avg:139.50ms
step:325/1375 train_time:43946ms step_avg:139.51ms
step:326/1375 train_time:44090ms step_avg:139.52ms
step:327/1375 train_time:44235ms step_avg:139.54ms
step:328/1375 train_time:44380ms step_avg:139.56ms
step:329/1375 train_time:44523ms step_avg:139.57ms
step:330/1375 train_time:44668ms step_avg:139.59ms
step:331/1375 train_time:44811ms step_avg:139.60ms
step:332/1375 train_time:44955ms step_avg:139.61ms
step:333/1375 train_time:45099ms step_avg:139.63ms
step:334/1375 train_time:45243ms step_avg:139.64ms
step:335/1375 train_time:45388ms step_avg:139.66ms
step:336/1375 train_time:45533ms step_avg:139.67ms
step:337/1375 train_time:45677ms step_avg:139.69ms
step:338/1375 train_time:45821ms step_avg:139.70ms
step:339/1375 train_time:45966ms step_avg:139.71ms
step:340/1375 train_time:46111ms step_avg:139.73ms
step:341/1375 train_time:46254ms step_avg:139.74ms
step:342/1375 train_time:46396ms step_avg:139.75ms
step:343/1375 train_time:46541ms step_avg:139.76ms
step:344/1375 train_time:46685ms step_avg:139.77ms
step:345/1375 train_time:46830ms step_avg:139.79ms
step:346/1375 train_time:46973ms step_avg:139.80ms
step:347/1375 train_time:47115ms step_avg:139.81ms
step:348/1375 train_time:47260ms step_avg:139.82ms
step:349/1375 train_time:47404ms step_avg:139.84ms
step:350/1375 train_time:47549ms step_avg:139.85ms
step:351/1375 train_time:47693ms step_avg:139.86ms
step:352/1375 train_time:47838ms step_avg:139.88ms
step:353/1375 train_time:47983ms step_avg:139.89ms
step:354/1375 train_time:48127ms step_avg:139.91ms
step:355/1375 train_time:48272ms step_avg:139.92ms
step:356/1375 train_time:48416ms step_avg:139.93ms
step:357/1375 train_time:48560ms step_avg:139.94ms
step:358/1375 train_time:48704ms step_avg:139.95ms
step:359/1375 train_time:48847ms step_avg:139.96ms
step:360/1375 train_time:48992ms step_avg:139.98ms
step:361/1375 train_time:49136ms step_avg:139.99ms
step:362/1375 train_time:49280ms step_avg:140.00ms
step:363/1375 train_time:49426ms step_avg:140.02ms
step:364/1375 train_time:49569ms step_avg:140.03ms
step:365/1375 train_time:49714ms step_avg:140.04ms
step:366/1375 train_time:49859ms step_avg:140.05ms
step:367/1375 train_time:50002ms step_avg:140.06ms
step:368/1375 train_time:50145ms step_avg:140.07ms
step:369/1375 train_time:50290ms step_avg:140.08ms
step:370/1375 train_time:50433ms step_avg:140.09ms
step:371/1375 train_time:50576ms step_avg:140.10ms
step:372/1375 train_time:50721ms step_avg:140.11ms
step:373/1375 train_time:50864ms step_avg:140.12ms
step:374/1375 train_time:51008ms step_avg:140.13ms
step:375/1375 train_time:51154ms step_avg:140.15ms
step:375/1375 val_loss:3.7690 train_time:51223ms step_avg:140.34ms
step:376/1375 train_time:51298ms step_avg:140.16ms
step:377/1375 train_time:51446ms step_avg:140.18ms
step:378/1375 train_time:51590ms step_avg:140.19ms
step:379/1375 train_time:51733ms step_avg:140.20ms
step:380/1375 train_time:51876ms step_avg:140.20ms
step:381/1375 train_time:52059ms step_avg:140.32ms
step:382/1375 train_time:52201ms step_avg:140.33ms
step:383/1375 train_time:52345ms step_avg:140.33ms
step:384/1375 train_time:52487ms step_avg:140.34ms
step:385/1375 train_time:52628ms step_avg:140.34ms
step:386/1375 train_time:52771ms step_avg:140.35ms
step:387/1375 train_time:52918ms step_avg:140.37ms
step:388/1375 train_time:53066ms step_avg:140.39ms
step:389/1375 train_time:53210ms step_avg:140.40ms
step:390/1375 train_time:53356ms step_avg:140.41ms
step:391/1375 train_time:53499ms step_avg:140.42ms
step:392/1375 train_time:53643ms step_avg:140.43ms
step:393/1375 train_time:53786ms step_avg:140.43ms
step:394/1375 train_time:53930ms step_avg:140.44ms
step:395/1375 train_time:54076ms step_avg:140.46ms
step:396/1375 train_time:54219ms step_avg:140.46ms
step:397/1375 train_time:54363ms step_avg:140.47ms
step:398/1375 train_time:54507ms step_avg:140.48ms
step:399/1375 train_time:54651ms step_avg:140.49ms
step:400/1375 train_time:54793ms step_avg:140.50ms
step:401/1375 train_time:54938ms step_avg:140.51ms
step:402/1375 train_time:55082ms step_avg:140.51ms
step:403/1375 train_time:55227ms step_avg:140.53ms
step:404/1375 train_time:55371ms step_avg:140.54ms
step:405/1375 train_time:55516ms step_avg:140.55ms
step:406/1375 train_time:55659ms step_avg:140.55ms
step:407/1375 train_time:55802ms step_avg:140.56ms
step:408/1375 train_time:55947ms step_avg:140.57ms
step:409/1375 train_time:56092ms step_avg:140.58ms
step:410/1375 train_time:56238ms step_avg:140.60ms
step:411/1375 train_time:56383ms step_avg:140.61ms
step:412/1375 train_time:56530ms step_avg:140.62ms
step:413/1375 train_time:56677ms step_avg:140.64ms
step:414/1375 train_time:56823ms step_avg:140.65ms
step:415/1375 train_time:56969ms step_avg:140.66ms
step:416/1375 train_time:57114ms step_avg:140.67ms
step:417/1375 train_time:57261ms step_avg:140.69ms
step:418/1375 train_time:57405ms step_avg:140.70ms
step:419/1375 train_time:57552ms step_avg:140.71ms
step:420/1375 train_time:57697ms step_avg:140.72ms
step:421/1375 train_time:57843ms step_avg:140.74ms
step:422/1375 train_time:57989ms step_avg:140.75ms
step:423/1375 train_time:58136ms step_avg:140.77ms
step:424/1375 train_time:58281ms step_avg:140.78ms
step:425/1375 train_time:58429ms step_avg:140.79ms
step:426/1375 train_time:58575ms step_avg:140.81ms
step:427/1375 train_time:58720ms step_avg:140.82ms
step:428/1375 train_time:58866ms step_avg:140.83ms
step:429/1375 train_time:59011ms step_avg:140.84ms
step:430/1375 train_time:59158ms step_avg:140.85ms
step:431/1375 train_time:59302ms step_avg:140.86ms
step:432/1375 train_time:59450ms step_avg:140.88ms
step:433/1375 train_time:59595ms step_avg:140.89ms
step:434/1375 train_time:59741ms step_avg:140.90ms
step:435/1375 train_time:59888ms step_avg:140.91ms
step:436/1375 train_time:60033ms step_avg:140.92ms
step:437/1375 train_time:60178ms step_avg:140.93ms
step:438/1375 train_time:60323ms step_avg:140.94ms
step:439/1375 train_time:60469ms step_avg:140.95ms
step:440/1375 train_time:60615ms step_avg:140.97ms
step:441/1375 train_time:60761ms step_avg:140.98ms
step:442/1375 train_time:60906ms step_avg:140.99ms
step:443/1375 train_time:61052ms step_avg:141.00ms
step:444/1375 train_time:61198ms step_avg:141.01ms
step:445/1375 train_time:61346ms step_avg:141.02ms
step:446/1375 train_time:61492ms step_avg:141.04ms
step:447/1375 train_time:61638ms step_avg:141.05ms
step:448/1375 train_time:61785ms step_avg:141.06ms
step:449/1375 train_time:61931ms step_avg:141.07ms
step:450/1375 train_time:62078ms step_avg:141.09ms
step:451/1375 train_time:62223ms step_avg:141.10ms
step:452/1375 train_time:62369ms step_avg:141.11ms
step:453/1375 train_time:62516ms step_avg:141.12ms
step:454/1375 train_time:62660ms step_avg:141.13ms
step:455/1375 train_time:62806ms step_avg:141.14ms
step:456/1375 train_time:62953ms step_avg:141.15ms
step:457/1375 train_time:63098ms step_avg:141.16ms
step:458/1375 train_time:63245ms step_avg:141.17ms
step:459/1375 train_time:63391ms step_avg:141.18ms
step:460/1375 train_time:63537ms step_avg:141.19ms
step:461/1375 train_time:63682ms step_avg:141.20ms
step:462/1375 train_time:63830ms step_avg:141.22ms
step:463/1375 train_time:63977ms step_avg:141.23ms
step:464/1375 train_time:64122ms step_avg:141.24ms
step:465/1375 train_time:64268ms step_avg:141.25ms
step:466/1375 train_time:64415ms step_avg:141.26ms
step:467/1375 train_time:64560ms step_avg:141.27ms
step:468/1375 train_time:64706ms step_avg:141.28ms
step:469/1375 train_time:64852ms step_avg:141.29ms
step:470/1375 train_time:64998ms step_avg:141.30ms
step:471/1375 train_time:65145ms step_avg:141.31ms
step:472/1375 train_time:65291ms step_avg:141.32ms
step:473/1375 train_time:65438ms step_avg:141.33ms
step:474/1375 train_time:65583ms step_avg:141.34ms
step:475/1375 train_time:65729ms step_avg:141.35ms
step:476/1375 train_time:65876ms step_avg:141.36ms
step:477/1375 train_time:66021ms step_avg:141.37ms
step:478/1375 train_time:66168ms step_avg:141.38ms
step:479/1375 train_time:66314ms step_avg:141.39ms
step:480/1375 train_time:66459ms step_avg:141.40ms
step:481/1375 train_time:66603ms step_avg:141.41ms
step:482/1375 train_time:66750ms step_avg:141.42ms
step:483/1375 train_time:66895ms step_avg:141.43ms
step:484/1375 train_time:67040ms step_avg:141.44ms
step:485/1375 train_time:67186ms step_avg:141.44ms
step:486/1375 train_time:67333ms step_avg:141.46ms
step:487/1375 train_time:67478ms step_avg:141.46ms
step:488/1375 train_time:67623ms step_avg:141.47ms
step:489/1375 train_time:67768ms step_avg:141.48ms
step:490/1375 train_time:67914ms step_avg:141.49ms
step:491/1375 train_time:68060ms step_avg:141.50ms
step:492/1375 train_time:68205ms step_avg:141.51ms
step:493/1375 train_time:68351ms step_avg:141.51ms
step:494/1375 train_time:68497ms step_avg:141.52ms
step:495/1375 train_time:68645ms step_avg:141.54ms
step:496/1375 train_time:68792ms step_avg:141.55ms
step:497/1375 train_time:68937ms step_avg:141.56ms
step:498/1375 train_time:69083ms step_avg:141.56ms
step:499/1375 train_time:69230ms step_avg:141.57ms
step:500/1375 train_time:69376ms step_avg:141.58ms
step:500/1375 val_loss:3.6542 train_time:69447ms step_avg:141.73ms
step:501/1375 train_time:69523ms step_avg:141.60ms
step:502/1375 train_time:69670ms step_avg:141.61ms
step:503/1375 train_time:69817ms step_avg:141.62ms
step:504/1375 train_time:69962ms step_avg:141.62ms
step:505/1375 train_time:70106ms step_avg:141.63ms
step:506/1375 train_time:70250ms step_avg:141.63ms
step:507/1375 train_time:70395ms step_avg:141.64ms
step:508/1375 train_time:70544ms step_avg:141.65ms
step:509/1375 train_time:70689ms step_avg:141.66ms
step:510/1375 train_time:70836ms step_avg:141.67ms
step:511/1375 train_time:70982ms step_avg:141.68ms
step:512/1375 train_time:71130ms step_avg:141.69ms
step:513/1375 train_time:71279ms step_avg:141.71ms
step:514/1375 train_time:71427ms step_avg:141.72ms
step:515/1375 train_time:71574ms step_avg:141.73ms
step:516/1375 train_time:71723ms step_avg:141.74ms
step:517/1375 train_time:71870ms step_avg:141.75ms
step:518/1375 train_time:72018ms step_avg:141.77ms
step:519/1375 train_time:72167ms step_avg:141.78ms
step:520/1375 train_time:72315ms step_avg:141.79ms
step:521/1375 train_time:72463ms step_avg:141.81ms
step:522/1375 train_time:72609ms step_avg:141.82ms
step:523/1375 train_time:72756ms step_avg:141.83ms
step:524/1375 train_time:72904ms step_avg:141.84ms
step:525/1375 train_time:73050ms step_avg:141.84ms
step:526/1375 train_time:73198ms step_avg:141.86ms
step:527/1375 train_time:73347ms step_avg:141.87ms
step:528/1375 train_time:73492ms step_avg:141.88ms
step:529/1375 train_time:73642ms step_avg:141.89ms
step:530/1375 train_time:73787ms step_avg:141.90ms
step:531/1375 train_time:73935ms step_avg:141.91ms
step:532/1375 train_time:74081ms step_avg:141.92ms
step:533/1375 train_time:74230ms step_avg:141.93ms
step:534/1375 train_time:74377ms step_avg:141.94ms
step:535/1375 train_time:74524ms step_avg:141.95ms
step:536/1375 train_time:74671ms step_avg:141.96ms
step:537/1375 train_time:74819ms step_avg:141.97ms
step:538/1375 train_time:74967ms step_avg:141.98ms
step:539/1375 train_time:75116ms step_avg:142.00ms
step:540/1375 train_time:75264ms step_avg:142.01ms
step:541/1375 train_time:75410ms step_avg:142.01ms
step:542/1375 train_time:75558ms step_avg:142.03ms
step:543/1375 train_time:75705ms step_avg:142.04ms
step:544/1375 train_time:75851ms step_avg:142.04ms
step:545/1375 train_time:76000ms step_avg:142.06ms
step:546/1375 train_time:76147ms step_avg:142.07ms
step:547/1375 train_time:76294ms step_avg:142.07ms
step:548/1375 train_time:76443ms step_avg:142.09ms
step:549/1375 train_time:76590ms step_avg:142.10ms
step:550/1375 train_time:76740ms step_avg:142.11ms
step:551/1375 train_time:76887ms step_avg:142.12ms
step:552/1375 train_time:77035ms step_avg:142.13ms
step:553/1375 train_time:77183ms step_avg:142.14ms
step:554/1375 train_time:77330ms step_avg:142.15ms
step:555/1375 train_time:77478ms step_avg:142.16ms
step:556/1375 train_time:77624ms step_avg:142.17ms
step:557/1375 train_time:77772ms step_avg:142.18ms
step:558/1375 train_time:77920ms step_avg:142.19ms
step:559/1375 train_time:78067ms step_avg:142.20ms
step:560/1375 train_time:78215ms step_avg:142.21ms
step:561/1375 train_time:78361ms step_avg:142.22ms
step:562/1375 train_time:78509ms step_avg:142.23ms
step:563/1375 train_time:78656ms step_avg:142.24ms
step:564/1375 train_time:78804ms step_avg:142.25ms
step:565/1375 train_time:78951ms step_avg:142.25ms
step:566/1375 train_time:79098ms step_avg:142.26ms
step:567/1375 train_time:79246ms step_avg:142.27ms
step:568/1375 train_time:79393ms step_avg:142.28ms
step:569/1375 train_time:79541ms step_avg:142.29ms
step:570/1375 train_time:79687ms step_avg:142.30ms
step:571/1375 train_time:79869ms step_avg:142.37ms
step:572/1375 train_time:80024ms step_avg:142.39ms
step:573/1375 train_time:80171ms step_avg:142.40ms
step:574/1375 train_time:80321ms step_avg:142.41ms
step:575/1375 train_time:80468ms step_avg:142.42ms
step:576/1375 train_time:80614ms step_avg:142.43ms
step:577/1375 train_time:80762ms step_avg:142.44ms
step:578/1375 train_time:80911ms step_avg:142.45ms
step:579/1375 train_time:81059ms step_avg:142.46ms
step:580/1375 train_time:81207ms step_avg:142.47ms
step:581/1375 train_time:81353ms step_avg:142.47ms
step:582/1375 train_time:81500ms step_avg:142.48ms
step:583/1375 train_time:81647ms step_avg:142.49ms
step:584/1375 train_time:81795ms step_avg:142.50ms
step:585/1375 train_time:81944ms step_avg:142.51ms
step:586/1375 train_time:82091ms step_avg:142.52ms
step:587/1375 train_time:82241ms step_avg:142.53ms
step:588/1375 train_time:82386ms step_avg:142.54ms
step:589/1375 train_time:82535ms step_avg:142.55ms
step:590/1375 train_time:82682ms step_avg:142.55ms
step:591/1375 train_time:82830ms step_avg:142.56ms
step:592/1375 train_time:82980ms step_avg:142.58ms
step:593/1375 train_time:83127ms step_avg:142.59ms
step:594/1375 train_time:83275ms step_avg:142.59ms
step:595/1375 train_time:83422ms step_avg:142.60ms
step:596/1375 train_time:83568ms step_avg:142.61ms
step:597/1375 train_time:83714ms step_avg:142.61ms
step:598/1375 train_time:83863ms step_avg:142.62ms
step:599/1375 train_time:84010ms step_avg:142.63ms
step:600/1375 train_time:84158ms step_avg:142.64ms
step:601/1375 train_time:84307ms step_avg:142.65ms
step:602/1375 train_time:84452ms step_avg:142.66ms
step:603/1375 train_time:84602ms step_avg:142.67ms
step:604/1375 train_time:84749ms step_avg:142.68ms
step:605/1375 train_time:84896ms step_avg:142.68ms
step:606/1375 train_time:85044ms step_avg:142.69ms
step:607/1375 train_time:85189ms step_avg:142.70ms
step:608/1375 train_time:85338ms step_avg:142.71ms
step:609/1375 train_time:85485ms step_avg:142.71ms
step:610/1375 train_time:85632ms step_avg:142.72ms
step:611/1375 train_time:85779ms step_avg:142.73ms
step:612/1375 train_time:85927ms step_avg:142.74ms
step:613/1375 train_time:86074ms step_avg:142.74ms
step:614/1375 train_time:86223ms step_avg:142.75ms
step:615/1375 train_time:86372ms step_avg:142.76ms
step:616/1375 train_time:86520ms step_avg:142.77ms
step:617/1375 train_time:86668ms step_avg:142.78ms
step:618/1375 train_time:86818ms step_avg:142.79ms
step:619/1375 train_time:86967ms step_avg:142.80ms
step:620/1375 train_time:87115ms step_avg:142.81ms
step:621/1375 train_time:87263ms step_avg:142.82ms
step:622/1375 train_time:87412ms step_avg:142.83ms
step:623/1375 train_time:87563ms step_avg:142.84ms
step:624/1375 train_time:87711ms step_avg:142.85ms
step:625/1375 train_time:87860ms step_avg:142.86ms
step:625/1375 val_loss:3.5718 train_time:87935ms step_avg:142.98ms
step:626/1375 train_time:88012ms step_avg:142.88ms
step:627/1375 train_time:88159ms step_avg:142.88ms
step:628/1375 train_time:88310ms step_avg:142.90ms
step:629/1375 train_time:88457ms step_avg:142.90ms
step:630/1375 train_time:88606ms step_avg:142.91ms
step:631/1375 train_time:88752ms step_avg:142.92ms
step:632/1375 train_time:88900ms step_avg:142.93ms
step:633/1375 train_time:89052ms step_avg:142.94ms
step:634/1375 train_time:89201ms step_avg:142.95ms
step:635/1375 train_time:89349ms step_avg:142.96ms
step:636/1375 train_time:89498ms step_avg:142.97ms
step:637/1375 train_time:89647ms step_avg:142.98ms
step:638/1375 train_time:89795ms step_avg:142.99ms
step:639/1375 train_time:89943ms step_avg:142.99ms
step:640/1375 train_time:90092ms step_avg:143.00ms
step:641/1375 train_time:90240ms step_avg:143.01ms
step:642/1375 train_time:90391ms step_avg:143.02ms
step:643/1375 train_time:90538ms step_avg:143.03ms
step:644/1375 train_time:90688ms step_avg:143.04ms
step:645/1375 train_time:90838ms step_avg:143.05ms
step:646/1375 train_time:90989ms step_avg:143.06ms
step:647/1375 train_time:91138ms step_avg:143.07ms
step:648/1375 train_time:91291ms step_avg:143.09ms
step:649/1375 train_time:91440ms step_avg:143.10ms
step:650/1375 train_time:91590ms step_avg:143.11ms
step:651/1375 train_time:91740ms step_avg:143.12ms
step:652/1375 train_time:91887ms step_avg:143.13ms
step:653/1375 train_time:92036ms step_avg:143.14ms
step:654/1375 train_time:92189ms step_avg:143.15ms
step:655/1375 train_time:92336ms step_avg:143.16ms
step:656/1375 train_time:92486ms step_avg:143.17ms
step:657/1375 train_time:92635ms step_avg:143.18ms
step:658/1375 train_time:92783ms step_avg:143.18ms
step:659/1375 train_time:92932ms step_avg:143.19ms
step:660/1375 train_time:93081ms step_avg:143.20ms
step:661/1375 train_time:93231ms step_avg:143.21ms
step:662/1375 train_time:93379ms step_avg:143.22ms
step:663/1375 train_time:93529ms step_avg:143.23ms
step:664/1375 train_time:93678ms step_avg:143.24ms
step:665/1375 train_time:93829ms step_avg:143.25ms
step:666/1375 train_time:93976ms step_avg:143.26ms
step:667/1375 train_time:94125ms step_avg:143.27ms
step:668/1375 train_time:94276ms step_avg:143.28ms
step:669/1375 train_time:94424ms step_avg:143.28ms
step:670/1375 train_time:94573ms step_avg:143.29ms
step:671/1375 train_time:94721ms step_avg:143.30ms
step:672/1375 train_time:94871ms step_avg:143.31ms
step:673/1375 train_time:95019ms step_avg:143.32ms
step:674/1375 train_time:95168ms step_avg:143.33ms
step:675/1375 train_time:95317ms step_avg:143.33ms
step:676/1375 train_time:95467ms step_avg:143.34ms
step:677/1375 train_time:95616ms step_avg:143.35ms
step:678/1375 train_time:95763ms step_avg:143.36ms
step:679/1375 train_time:95913ms step_avg:143.37ms
step:680/1375 train_time:96061ms step_avg:143.37ms
step:681/1375 train_time:96210ms step_avg:143.38ms
step:682/1375 train_time:96358ms step_avg:143.39ms
step:683/1375 train_time:96507ms step_avg:143.40ms
step:684/1375 train_time:96655ms step_avg:143.41ms
step:685/1375 train_time:96806ms step_avg:143.42ms
step:686/1375 train_time:96954ms step_avg:143.42ms
step:687/1375 train_time:97103ms step_avg:143.43ms
step:688/1375 train_time:97252ms step_avg:143.44ms
step:689/1375 train_time:97402ms step_avg:143.45ms
step:690/1375 train_time:97552ms step_avg:143.46ms
step:691/1375 train_time:97700ms step_avg:143.47ms
step:692/1375 train_time:97850ms step_avg:143.47ms
step:693/1375 train_time:97998ms step_avg:143.48ms
step:694/1375 train_time:98148ms step_avg:143.49ms
step:695/1375 train_time:98296ms step_avg:143.50ms
step:696/1375 train_time:98444ms step_avg:143.50ms
step:697/1375 train_time:98593ms step_avg:143.51ms
step:698/1375 train_time:98741ms step_avg:143.52ms
step:699/1375 train_time:98891ms step_avg:143.53ms
step:700/1375 train_time:99038ms step_avg:143.53ms
step:701/1375 train_time:99187ms step_avg:143.54ms
step:702/1375 train_time:99338ms step_avg:143.55ms
step:703/1375 train_time:99487ms step_avg:143.56ms
step:704/1375 train_time:99636ms step_avg:143.57ms
step:705/1375 train_time:99784ms step_avg:143.57ms
step:706/1375 train_time:99936ms step_avg:143.59ms
step:707/1375 train_time:100085ms step_avg:143.59ms
step:708/1375 train_time:100236ms step_avg:143.60ms
step:709/1375 train_time:100386ms step_avg:143.61ms
step:710/1375 train_time:100536ms step_avg:143.62ms
step:711/1375 train_time:100685ms step_avg:143.63ms
step:712/1375 train_time:100836ms step_avg:143.64ms
step:713/1375 train_time:100987ms step_avg:143.65ms
step:714/1375 train_time:101136ms step_avg:143.66ms
step:715/1375 train_time:101287ms step_avg:143.67ms
step:716/1375 train_time:101436ms step_avg:143.68ms
step:717/1375 train_time:101588ms step_avg:143.69ms
step:718/1375 train_time:101737ms step_avg:143.70ms
step:719/1375 train_time:101888ms step_avg:143.71ms
step:720/1375 train_time:102038ms step_avg:143.72ms
step:721/1375 train_time:102190ms step_avg:143.73ms
step:722/1375 train_time:102342ms step_avg:143.74ms
step:723/1375 train_time:102492ms step_avg:143.75ms
step:724/1375 train_time:102642ms step_avg:143.76ms
step:725/1375 train_time:102793ms step_avg:143.77ms
step:726/1375 train_time:102942ms step_avg:143.77ms
step:727/1375 train_time:103094ms step_avg:143.79ms
step:728/1375 train_time:103244ms step_avg:143.79ms
step:729/1375 train_time:103395ms step_avg:143.80ms
step:730/1375 train_time:103546ms step_avg:143.81ms
step:731/1375 train_time:103695ms step_avg:143.82ms
step:732/1375 train_time:103843ms step_avg:143.83ms
step:733/1375 train_time:103994ms step_avg:143.84ms
step:734/1375 train_time:104144ms step_avg:143.84ms
step:735/1375 train_time:104296ms step_avg:143.86ms
step:736/1375 train_time:104445ms step_avg:143.86ms
step:737/1375 train_time:104596ms step_avg:143.87ms
step:738/1375 train_time:104747ms step_avg:143.88ms
step:739/1375 train_time:104898ms step_avg:143.89ms
step:740/1375 train_time:105049ms step_avg:143.90ms
step:741/1375 train_time:105200ms step_avg:143.91ms
step:742/1375 train_time:105350ms step_avg:143.92ms
step:743/1375 train_time:105499ms step_avg:143.93ms
step:744/1375 train_time:105650ms step_avg:143.94ms
step:745/1375 train_time:105801ms step_avg:143.95ms
step:746/1375 train_time:105950ms step_avg:143.95ms
step:747/1375 train_time:106101ms step_avg:143.96ms
step:748/1375 train_time:106251ms step_avg:143.97ms
step:749/1375 train_time:106401ms step_avg:143.98ms
step:750/1375 train_time:106551ms step_avg:143.99ms
step:750/1375 val_loss:3.5185 train_time:106627ms step_avg:144.09ms
step:751/1375 train_time:106705ms step_avg:144.00ms
step:752/1375 train_time:106859ms step_avg:144.01ms
step:753/1375 train_time:107008ms step_avg:144.02ms
step:754/1375 train_time:107157ms step_avg:144.03ms
step:755/1375 train_time:107307ms step_avg:144.04ms
step:756/1375 train_time:107458ms step_avg:144.05ms
step:757/1375 train_time:107609ms step_avg:144.05ms
step:758/1375 train_time:107760ms step_avg:144.06ms
step:759/1375 train_time:107912ms step_avg:144.07ms
step:760/1375 train_time:108062ms step_avg:144.08ms
step:761/1375 train_time:108247ms step_avg:144.14ms
step:762/1375 train_time:108404ms step_avg:144.15ms
step:763/1375 train_time:108554ms step_avg:144.16ms
step:764/1375 train_time:108703ms step_avg:144.17ms
step:765/1375 train_time:108852ms step_avg:144.17ms
step:766/1375 train_time:109003ms step_avg:144.18ms
step:767/1375 train_time:109154ms step_avg:144.19ms
step:768/1375 train_time:109305ms step_avg:144.20ms
step:769/1375 train_time:109456ms step_avg:144.21ms
step:770/1375 train_time:109604ms step_avg:144.22ms
step:771/1375 train_time:109755ms step_avg:144.22ms
step:772/1375 train_time:109905ms step_avg:144.23ms
step:773/1375 train_time:110057ms step_avg:144.24ms
step:774/1375 train_time:110207ms step_avg:144.25ms
step:775/1375 train_time:110359ms step_avg:144.26ms
step:776/1375 train_time:110509ms step_avg:144.27ms
step:777/1375 train_time:110661ms step_avg:144.28ms
step:778/1375 train_time:110809ms step_avg:144.28ms
step:779/1375 train_time:110959ms step_avg:144.29ms
step:780/1375 train_time:111110ms step_avg:144.30ms
step:781/1375 train_time:111260ms step_avg:144.31ms
step:782/1375 train_time:111409ms step_avg:144.31ms
step:783/1375 train_time:111559ms step_avg:144.32ms
step:784/1375 train_time:111709ms step_avg:144.33ms
step:785/1375 train_time:111857ms step_avg:144.33ms
step:786/1375 train_time:112008ms step_avg:144.34ms
step:787/1375 train_time:112159ms step_avg:144.35ms
step:788/1375 train_time:112310ms step_avg:144.36ms
step:789/1375 train_time:112460ms step_avg:144.36ms
step:790/1375 train_time:112609ms step_avg:144.37ms
step:791/1375 train_time:112759ms step_avg:144.38ms
step:792/1375 train_time:112908ms step_avg:144.38ms
step:793/1375 train_time:113059ms step_avg:144.39ms
step:794/1375 train_time:113208ms step_avg:144.40ms
step:795/1375 train_time:113360ms step_avg:144.41ms
step:796/1375 train_time:113509ms step_avg:144.41ms
step:797/1375 train_time:113659ms step_avg:144.42ms
step:798/1375 train_time:113811ms step_avg:144.43ms
step:799/1375 train_time:113966ms step_avg:144.44ms
step:800/1375 train_time:114117ms step_avg:144.45ms
step:801/1375 train_time:114267ms step_avg:144.46ms
step:802/1375 train_time:114417ms step_avg:144.47ms
step:803/1375 train_time:114565ms step_avg:144.47ms
step:804/1375 train_time:114715ms step_avg:144.48ms
step:805/1375 train_time:114868ms step_avg:144.49ms
step:806/1375 train_time:115018ms step_avg:144.50ms
step:807/1375 train_time:115165ms step_avg:144.50ms
step:808/1375 train_time:115318ms step_avg:144.51ms
step:809/1375 train_time:115466ms step_avg:144.51ms
step:810/1375 train_time:115616ms step_avg:144.52ms
step:811/1375 train_time:115765ms step_avg:144.53ms
step:812/1375 train_time:115916ms step_avg:144.53ms
step:813/1375 train_time:116065ms step_avg:144.54ms
step:814/1375 train_time:116216ms step_avg:144.55ms
step:815/1375 train_time:116365ms step_avg:144.55ms
step:816/1375 train_time:116518ms step_avg:144.56ms
step:817/1375 train_time:116668ms step_avg:144.57ms
step:818/1375 train_time:116819ms step_avg:144.58ms
step:819/1375 train_time:116971ms step_avg:144.59ms
step:820/1375 train_time:117126ms step_avg:144.60ms
step:821/1375 train_time:117277ms step_avg:144.61ms
step:822/1375 train_time:117430ms step_avg:144.62ms
step:823/1375 train_time:117581ms step_avg:144.63ms
step:824/1375 train_time:117731ms step_avg:144.63ms
step:825/1375 train_time:117884ms step_avg:144.64ms
step:826/1375 train_time:118037ms step_avg:144.65ms
step:827/1375 train_time:118187ms step_avg:144.66ms
step:828/1375 train_time:118340ms step_avg:144.67ms
step:829/1375 train_time:118489ms step_avg:144.68ms
step:830/1375 train_time:118641ms step_avg:144.68ms
step:831/1375 train_time:118792ms step_avg:144.69ms
step:832/1375 train_time:118943ms step_avg:144.70ms
step:833/1375 train_time:119094ms step_avg:144.71ms
step:834/1375 train_time:119245ms step_avg:144.71ms
step:835/1375 train_time:119398ms step_avg:144.72ms
step:836/1375 train_time:119552ms step_avg:144.74ms
step:837/1375 train_time:119701ms step_avg:144.74ms
step:838/1375 train_time:119854ms step_avg:144.75ms
step:839/1375 train_time:120004ms step_avg:144.76ms
step:840/1375 train_time:120154ms step_avg:144.76ms
step:841/1375 train_time:120305ms step_avg:144.77ms
step:842/1375 train_time:120459ms step_avg:144.78ms
step:843/1375 train_time:120608ms step_avg:144.79ms
step:844/1375 train_time:120759ms step_avg:144.79ms
step:845/1375 train_time:120909ms step_avg:144.80ms
step:846/1375 train_time:121063ms step_avg:144.81ms
step:847/1375 train_time:121215ms step_avg:144.82ms
step:848/1375 train_time:121365ms step_avg:144.83ms
step:849/1375 train_time:121518ms step_avg:144.84ms
step:850/1375 train_time:121672ms step_avg:144.85ms
step:851/1375 train_time:121823ms step_avg:144.85ms
step:852/1375 train_time:121975ms step_avg:144.86ms
step:853/1375 train_time:122125ms step_avg:144.87ms
step:854/1375 train_time:122279ms step_avg:144.88ms
step:855/1375 train_time:122429ms step_avg:144.89ms
step:856/1375 train_time:122581ms step_avg:144.89ms
step:857/1375 train_time:122733ms step_avg:144.90ms
step:858/1375 train_time:122886ms step_avg:144.91ms
step:859/1375 train_time:123037ms step_avg:144.92ms
step:860/1375 train_time:123187ms step_avg:144.93ms
step:861/1375 train_time:123339ms step_avg:144.93ms
step:862/1375 train_time:123489ms step_avg:144.94ms
step:863/1375 train_time:123640ms step_avg:144.95ms
step:864/1375 train_time:123793ms step_avg:144.96ms
step:865/1375 train_time:123942ms step_avg:144.96ms
step:866/1375 train_time:124098ms step_avg:144.97ms
step:867/1375 train_time:124248ms step_avg:144.98ms
step:868/1375 train_time:124399ms step_avg:144.99ms
step:869/1375 train_time:124549ms step_avg:144.99ms
step:870/1375 train_time:124700ms step_avg:145.00ms
step:871/1375 train_time:124850ms step_avg:145.01ms
step:872/1375 train_time:125001ms step_avg:145.01ms
step:873/1375 train_time:125150ms step_avg:145.02ms
step:874/1375 train_time:125302ms step_avg:145.03ms
step:875/1375 train_time:125455ms step_avg:145.03ms
step:875/1375 val_loss:3.4664 train_time:125529ms step_avg:145.12ms
step:876/1375 train_time:125605ms step_avg:145.04ms
step:877/1375 train_time:125759ms step_avg:145.05ms
step:878/1375 train_time:125911ms step_avg:145.06ms
step:879/1375 train_time:126061ms step_avg:145.06ms
step:880/1375 train_time:126213ms step_avg:145.07ms
step:881/1375 train_time:126362ms step_avg:145.08ms
step:882/1375 train_time:126517ms step_avg:145.09ms
step:883/1375 train_time:126669ms step_avg:145.10ms
step:884/1375 train_time:126822ms step_avg:145.11ms
step:885/1375 train_time:126973ms step_avg:145.11ms
step:886/1375 train_time:127126ms step_avg:145.12ms
step:887/1375 train_time:127276ms step_avg:145.13ms
step:888/1375 train_time:127431ms step_avg:145.14ms
step:889/1375 train_time:127585ms step_avg:145.15ms
step:890/1375 train_time:127734ms step_avg:145.15ms
step:891/1375 train_time:127885ms step_avg:145.16ms
step:892/1375 train_time:128036ms step_avg:145.17ms
step:893/1375 train_time:128187ms step_avg:145.17ms
step:894/1375 train_time:128340ms step_avg:145.18ms
step:895/1375 train_time:128497ms step_avg:145.19ms
step:896/1375 train_time:128648ms step_avg:145.20ms
step:897/1375 train_time:128799ms step_avg:145.21ms
step:898/1375 train_time:128955ms step_avg:145.22ms
step:899/1375 train_time:129105ms step_avg:145.22ms
step:900/1375 train_time:129254ms step_avg:145.23ms
step:901/1375 train_time:129407ms step_avg:145.24ms
step:902/1375 train_time:129556ms step_avg:145.24ms
step:903/1375 train_time:129709ms step_avg:145.25ms
step:904/1375 train_time:129861ms step_avg:145.26ms
step:905/1375 train_time:130015ms step_avg:145.27ms
step:906/1375 train_time:130167ms step_avg:145.28ms
step:907/1375 train_time:130323ms step_avg:145.29ms
step:908/1375 train_time:130473ms step_avg:145.29ms
step:909/1375 train_time:130624ms step_avg:145.30ms
step:910/1375 train_time:130781ms step_avg:145.31ms
step:911/1375 train_time:130933ms step_avg:145.32ms
step:912/1375 train_time:131083ms step_avg:145.32ms
step:913/1375 train_time:131238ms step_avg:145.34ms
step:914/1375 train_time:131387ms step_avg:145.34ms
step:915/1375 train_time:131539ms step_avg:145.35ms
step:916/1375 train_time:131692ms step_avg:145.36ms
step:917/1375 train_time:131842ms step_avg:145.36ms
step:918/1375 train_time:131996ms step_avg:145.37ms
step:919/1375 train_time:132153ms step_avg:145.38ms
step:920/1375 train_time:132302ms step_avg:145.39ms
step:921/1375 train_time:132456ms step_avg:145.40ms
step:922/1375 train_time:132612ms step_avg:145.41ms
step:923/1375 train_time:132762ms step_avg:145.41ms
step:924/1375 train_time:132914ms step_avg:145.42ms
step:925/1375 train_time:133069ms step_avg:145.43ms
step:926/1375 train_time:133220ms step_avg:145.44ms
step:927/1375 train_time:133372ms step_avg:145.44ms
step:928/1375 train_time:133526ms step_avg:145.45ms
step:929/1375 train_time:133679ms step_avg:145.46ms
step:930/1375 train_time:133834ms step_avg:145.47ms
step:931/1375 train_time:133984ms step_avg:145.48ms
step:932/1375 train_time:134136ms step_avg:145.48ms
step:933/1375 train_time:134289ms step_avg:145.49ms
step:934/1375 train_time:134440ms step_avg:145.50ms
step:935/1375 train_time:134595ms step_avg:145.51ms
step:936/1375 train_time:134749ms step_avg:145.52ms
step:937/1375 train_time:134905ms step_avg:145.53ms
step:938/1375 train_time:135056ms step_avg:145.53ms
step:939/1375 train_time:135211ms step_avg:145.54ms
step:940/1375 train_time:135364ms step_avg:145.55ms
step:941/1375 train_time:135515ms step_avg:145.56ms
step:942/1375 train_time:135666ms step_avg:145.56ms
step:943/1375 train_time:135820ms step_avg:145.57ms
step:944/1375 train_time:135978ms step_avg:145.59ms
step:945/1375 train_time:136133ms step_avg:145.60ms
step:946/1375 train_time:136285ms step_avg:145.60ms
step:947/1375 train_time:136439ms step_avg:145.61ms
step:948/1375 train_time:136592ms step_avg:145.62ms
step:949/1375 train_time:136745ms step_avg:145.63ms
step:950/1375 train_time:136898ms step_avg:145.64ms
step:951/1375 train_time:137090ms step_avg:145.69ms
step:952/1375 train_time:137239ms step_avg:145.69ms
step:953/1375 train_time:137394ms step_avg:145.70ms
step:954/1375 train_time:137544ms step_avg:145.70ms
step:955/1375 train_time:137696ms step_avg:145.71ms
step:956/1375 train_time:137848ms step_avg:145.72ms
step:957/1375 train_time:138002ms step_avg:145.73ms
step:958/1375 train_time:138160ms step_avg:145.74ms
step:959/1375 train_time:138315ms step_avg:145.75ms
step:960/1375 train_time:138470ms step_avg:145.76ms
step:961/1375 train_time:138620ms step_avg:145.76ms
step:962/1375 train_time:138773ms step_avg:145.77ms
step:963/1375 train_time:138933ms step_avg:145.78ms
step:964/1375 train_time:139086ms step_avg:145.79ms
step:965/1375 train_time:139241ms step_avg:145.80ms
step:966/1375 train_time:139392ms step_avg:145.81ms
step:967/1375 train_time:139544ms step_avg:145.81ms
step:968/1375 train_time:139694ms step_avg:145.82ms
step:969/1375 train_time:139846ms step_avg:145.83ms
step:970/1375 train_time:139999ms step_avg:145.83ms
step:971/1375 train_time:140155ms step_avg:145.84ms
step:972/1375 train_time:140308ms step_avg:145.85ms
step:973/1375 train_time:140459ms step_avg:145.86ms
step:974/1375 train_time:140613ms step_avg:145.86ms
step:975/1375 train_time:140764ms step_avg:145.87ms
step:976/1375 train_time:140917ms step_avg:145.88ms
step:977/1375 train_time:141069ms step_avg:145.88ms
step:978/1375 train_time:141222ms step_avg:145.89ms
step:979/1375 train_time:141374ms step_avg:145.90ms
step:980/1375 train_time:141525ms step_avg:145.90ms
step:981/1375 train_time:141675ms step_avg:145.91ms
step:982/1375 train_time:141827ms step_avg:145.91ms
step:983/1375 train_time:141978ms step_avg:145.92ms
step:984/1375 train_time:142131ms step_avg:145.93ms
step:985/1375 train_time:142283ms step_avg:145.93ms
step:986/1375 train_time:142440ms step_avg:145.94ms
step:987/1375 train_time:142591ms step_avg:145.95ms
step:988/1375 train_time:142742ms step_avg:145.95ms
step:989/1375 train_time:142893ms step_avg:145.96ms
step:990/1375 train_time:143047ms step_avg:145.97ms
step:991/1375 train_time:143199ms step_avg:145.97ms
step:992/1375 train_time:143355ms step_avg:145.98ms
step:993/1375 train_time:143517ms step_avg:146.00ms
step:994/1375 train_time:143668ms step_avg:146.00ms
step:995/1375 train_time:143819ms step_avg:146.01ms
step:996/1375 train_time:143970ms step_avg:146.01ms
step:997/1375 train_time:144122ms step_avg:146.02ms
step:998/1375 train_time:144275ms step_avg:146.03ms
step:999/1375 train_time:144429ms step_avg:146.04ms
step:1000/1375 train_time:144581ms step_avg:146.04ms
step:1000/1375 val_loss:3.4012 train_time:144658ms step_avg:146.12ms
step:1001/1375 train_time:144734ms step_avg:146.05ms
step:1002/1375 train_time:144888ms step_avg:146.06ms
step:1003/1375 train_time:145043ms step_avg:146.07ms
step:1004/1375 train_time:145198ms step_avg:146.07ms
step:1005/1375 train_time:145348ms step_avg:146.08ms
step:1006/1375 train_time:145500ms step_avg:146.08ms
step:1007/1375 train_time:145656ms step_avg:146.09ms
step:1008/1375 train_time:145809ms step_avg:146.10ms
step:1009/1375 train_time:145967ms step_avg:146.11ms
step:1010/1375 train_time:146120ms step_avg:146.12ms
step:1011/1375 train_time:146272ms step_avg:146.13ms
step:1012/1375 train_time:146423ms step_avg:146.13ms
step:1013/1375 train_time:146577ms step_avg:146.14ms
step:1014/1375 train_time:146729ms step_avg:146.14ms
step:1015/1375 train_time:146883ms step_avg:146.15ms
step:1016/1375 train_time:147037ms step_avg:146.16ms
step:1017/1375 train_time:147189ms step_avg:146.17ms
step:1018/1375 train_time:147340ms step_avg:146.17ms
step:1019/1375 train_time:147493ms step_avg:146.18ms
step:1020/1375 train_time:147647ms step_avg:146.18ms
step:1021/1375 train_time:147801ms step_avg:146.19ms
step:1022/1375 train_time:147955ms step_avg:146.20ms
step:1023/1375 train_time:148110ms step_avg:146.21ms
step:1024/1375 train_time:148263ms step_avg:146.22ms
step:1025/1375 train_time:148417ms step_avg:146.22ms
step:1026/1375 train_time:148569ms step_avg:146.23ms
step:1027/1375 train_time:148722ms step_avg:146.24ms
step:1028/1375 train_time:148877ms step_avg:146.24ms
step:1029/1375 train_time:149032ms step_avg:146.25ms
step:1030/1375 train_time:149187ms step_avg:146.26ms
step:1031/1375 train_time:149338ms step_avg:146.27ms
step:1032/1375 train_time:149489ms step_avg:146.27ms
step:1033/1375 train_time:149642ms step_avg:146.28ms
step:1034/1375 train_time:149795ms step_avg:146.28ms
step:1035/1375 train_time:149953ms step_avg:146.30ms
step:1036/1375 train_time:150106ms step_avg:146.30ms
step:1037/1375 train_time:150261ms step_avg:146.31ms
step:1038/1375 train_time:150416ms step_avg:146.32ms
step:1039/1375 train_time:150568ms step_avg:146.32ms
step:1040/1375 train_time:150719ms step_avg:146.33ms
step:1041/1375 train_time:150875ms step_avg:146.34ms
step:1042/1375 train_time:151028ms step_avg:146.34ms
step:1043/1375 train_time:151180ms step_avg:146.35ms
step:1044/1375 train_time:151335ms step_avg:146.36ms
step:1045/1375 train_time:151492ms step_avg:146.37ms
step:1046/1375 train_time:151644ms step_avg:146.37ms
step:1047/1375 train_time:151798ms step_avg:146.38ms
step:1048/1375 train_time:151952ms step_avg:146.39ms
step:1049/1375 train_time:152107ms step_avg:146.40ms
step:1050/1375 train_time:152263ms step_avg:146.41ms
step:1051/1375 train_time:152418ms step_avg:146.42ms
step:1052/1375 train_time:152571ms step_avg:146.42ms
step:1053/1375 train_time:152724ms step_avg:146.43ms
step:1054/1375 train_time:152879ms step_avg:146.44ms
step:1055/1375 train_time:153032ms step_avg:146.44ms
step:1056/1375 train_time:153185ms step_avg:146.45ms
step:1057/1375 train_time:153339ms step_avg:146.46ms
step:1058/1375 train_time:153495ms step_avg:146.46ms
step:1059/1375 train_time:153651ms step_avg:146.47ms
step:1060/1375 train_time:153809ms step_avg:146.48ms
step:1061/1375 train_time:153963ms step_avg:146.49ms
step:1062/1375 train_time:154118ms step_avg:146.50ms
step:1063/1375 train_time:154271ms step_avg:146.51ms
step:1064/1375 train_time:154423ms step_avg:146.51ms
step:1065/1375 train_time:154579ms step_avg:146.52ms
step:1066/1375 train_time:154734ms step_avg:146.53ms
step:1067/1375 train_time:154888ms step_avg:146.54ms
step:1068/1375 train_time:155041ms step_avg:146.54ms
step:1069/1375 train_time:155200ms step_avg:146.55ms
step:1070/1375 train_time:155351ms step_avg:146.56ms
step:1071/1375 train_time:155506ms step_avg:146.57ms
step:1072/1375 train_time:155659ms step_avg:146.57ms
step:1073/1375 train_time:155810ms step_avg:146.58ms
step:1074/1375 train_time:155962ms step_avg:146.58ms
step:1075/1375 train_time:156116ms step_avg:146.59ms
step:1076/1375 train_time:156268ms step_avg:146.59ms
step:1077/1375 train_time:156423ms step_avg:146.60ms
step:1078/1375 train_time:156581ms step_avg:146.61ms
step:1079/1375 train_time:156737ms step_avg:146.62ms
step:1080/1375 train_time:156889ms step_avg:146.63ms
step:1081/1375 train_time:157042ms step_avg:146.63ms
step:1082/1375 train_time:157195ms step_avg:146.64ms
step:1083/1375 train_time:157348ms step_avg:146.64ms
step:1084/1375 train_time:157504ms step_avg:146.65ms
step:1085/1375 train_time:157657ms step_avg:146.66ms
step:1086/1375 train_time:157810ms step_avg:146.66ms
step:1087/1375 train_time:157964ms step_avg:146.67ms
step:1088/1375 train_time:158117ms step_avg:146.68ms
step:1089/1375 train_time:158274ms step_avg:146.69ms
step:1090/1375 train_time:158431ms step_avg:146.70ms
step:1091/1375 train_time:158587ms step_avg:146.70ms
step:1092/1375 train_time:158740ms step_avg:146.71ms
step:1093/1375 train_time:158895ms step_avg:146.72ms
step:1094/1375 train_time:159047ms step_avg:146.72ms
step:1095/1375 train_time:159204ms step_avg:146.73ms
step:1096/1375 train_time:159362ms step_avg:146.74ms
step:1097/1375 train_time:159517ms step_avg:146.75ms
step:1098/1375 train_time:159669ms step_avg:146.75ms
step:1099/1375 train_time:159822ms step_avg:146.76ms
step:1100/1375 train_time:159976ms step_avg:146.77ms
step:1101/1375 train_time:160128ms step_avg:146.77ms
step:1102/1375 train_time:160284ms step_avg:146.78ms
step:1103/1375 train_time:160438ms step_avg:146.79ms
step:1104/1375 train_time:160590ms step_avg:146.79ms
step:1105/1375 train_time:160744ms step_avg:146.80ms
step:1106/1375 train_time:160897ms step_avg:146.80ms
step:1107/1375 train_time:161052ms step_avg:146.81ms
step:1108/1375 train_time:161205ms step_avg:146.82ms
step:1109/1375 train_time:161358ms step_avg:146.82ms
step:1110/1375 train_time:161510ms step_avg:146.83ms
step:1111/1375 train_time:161665ms step_avg:146.83ms
step:1112/1375 train_time:161820ms step_avg:146.84ms
step:1113/1375 train_time:161973ms step_avg:146.85ms
step:1114/1375 train_time:162129ms step_avg:146.86ms
step:1115/1375 train_time:162285ms step_avg:146.86ms
step:1116/1375 train_time:162437ms step_avg:146.87ms
step:1117/1375 train_time:162593ms step_avg:146.88ms
step:1118/1375 train_time:162752ms step_avg:146.89ms
step:1119/1375 train_time:162906ms step_avg:146.89ms
step:1120/1375 train_time:163060ms step_avg:146.90ms
step:1121/1375 train_time:163213ms step_avg:146.91ms
step:1122/1375 train_time:163364ms step_avg:146.91ms
step:1123/1375 train_time:163516ms step_avg:146.92ms
step:1124/1375 train_time:163673ms step_avg:146.92ms
step:1125/1375 train_time:163827ms step_avg:146.93ms
step:1125/1375 val_loss:3.3485 train_time:163905ms step_avg:147.00ms
step:1126/1375 train_time:163983ms step_avg:146.94ms
step:1127/1375 train_time:164138ms step_avg:146.95ms
step:1128/1375 train_time:164294ms step_avg:146.95ms
step:1129/1375 train_time:164453ms step_avg:146.96ms
step:1130/1375 train_time:164607ms step_avg:146.97ms
step:1131/1375 train_time:164764ms step_avg:146.98ms
step:1132/1375 train_time:164917ms step_avg:146.99ms
step:1133/1375 train_time:165072ms step_avg:146.99ms
step:1134/1375 train_time:165228ms step_avg:147.00ms
step:1135/1375 train_time:165382ms step_avg:147.01ms
step:1136/1375 train_time:165541ms step_avg:147.02ms
step:1137/1375 train_time:165693ms step_avg:147.02ms
step:1138/1375 train_time:165846ms step_avg:147.03ms
step:1139/1375 train_time:166002ms step_avg:147.03ms
step:1140/1375 train_time:166159ms step_avg:147.04ms
step:1141/1375 train_time:166349ms step_avg:147.08ms
step:1142/1375 train_time:166507ms step_avg:147.09ms
step:1143/1375 train_time:166667ms step_avg:147.10ms
step:1144/1375 train_time:166824ms step_avg:147.11ms
step:1145/1375 train_time:166973ms step_avg:147.11ms
step:1146/1375 train_time:167129ms step_avg:147.12ms
step:1147/1375 train_time:167284ms step_avg:147.13ms
step:1148/1375 train_time:167443ms step_avg:147.14ms
step:1149/1375 train_time:167597ms step_avg:147.14ms
step:1150/1375 train_time:167749ms step_avg:147.15ms
step:1151/1375 train_time:167905ms step_avg:147.16ms
step:1152/1375 train_time:168060ms step_avg:147.16ms
step:1153/1375 train_time:168214ms step_avg:147.17ms
step:1154/1375 train_time:168367ms step_avg:147.17ms
step:1155/1375 train_time:168524ms step_avg:147.18ms
step:1156/1375 train_time:168683ms step_avg:147.19ms
step:1157/1375 train_time:168843ms step_avg:147.20ms
step:1158/1375 train_time:168997ms step_avg:147.21ms
step:1159/1375 train_time:169151ms step_avg:147.22ms
step:1160/1375 train_time:169303ms step_avg:147.22ms
step:1161/1375 train_time:169458ms step_avg:147.23ms
step:1162/1375 train_time:169614ms step_avg:147.23ms
step:1163/1375 train_time:169770ms step_avg:147.24ms
step:1164/1375 train_time:169924ms step_avg:147.25ms
step:1165/1375 train_time:170076ms step_avg:147.25ms
step:1166/1375 train_time:170231ms step_avg:147.26ms
step:1167/1375 train_time:170384ms step_avg:147.26ms
step:1168/1375 train_time:170538ms step_avg:147.27ms
step:1169/1375 train_time:170694ms step_avg:147.28ms
step:1170/1375 train_time:170849ms step_avg:147.28ms
step:1171/1375 train_time:171005ms step_avg:147.29ms
step:1172/1375 train_time:171160ms step_avg:147.30ms
step:1173/1375 train_time:171314ms step_avg:147.30ms
step:1174/1375 train_time:171477ms step_avg:147.32ms
step:1175/1375 train_time:171632ms step_avg:147.32ms
step:1176/1375 train_time:171790ms step_avg:147.33ms
step:1177/1375 train_time:171951ms step_avg:147.34ms
step:1178/1375 train_time:172105ms step_avg:147.35ms
step:1179/1375 train_time:172261ms step_avg:147.36ms
step:1180/1375 train_time:172423ms step_avg:147.37ms
step:1181/1375 train_time:172577ms step_avg:147.38ms
step:1182/1375 train_time:172728ms step_avg:147.38ms
step:1183/1375 train_time:172883ms step_avg:147.39ms
step:1184/1375 train_time:173037ms step_avg:147.39ms
step:1185/1375 train_time:173197ms step_avg:147.40ms
step:1186/1375 train_time:173353ms step_avg:147.41ms
step:1187/1375 train_time:173516ms step_avg:147.42ms
step:1188/1375 train_time:173667ms step_avg:147.43ms
step:1189/1375 train_time:173824ms step_avg:147.43ms
step:1190/1375 train_time:173979ms step_avg:147.44ms
step:1191/1375 train_time:174134ms step_avg:147.45ms
step:1192/1375 train_time:174288ms step_avg:147.45ms
step:1193/1375 train_time:174444ms step_avg:147.46ms
step:1194/1375 train_time:174599ms step_avg:147.47ms
step:1195/1375 train_time:174751ms step_avg:147.47ms
step:1196/1375 train_time:174906ms step_avg:147.48ms
step:1197/1375 train_time:175063ms step_avg:147.48ms
step:1198/1375 train_time:175224ms step_avg:147.50ms
step:1199/1375 train_time:175378ms step_avg:147.50ms
step:1200/1375 train_time:175530ms step_avg:147.50ms
step:1201/1375 train_time:175685ms step_avg:147.51ms
step:1202/1375 train_time:175854ms step_avg:147.53ms
step:1203/1375 train_time:176014ms step_avg:147.54ms
step:1204/1375 train_time:176169ms step_avg:147.55ms
step:1205/1375 train_time:176324ms step_avg:147.55ms
step:1206/1375 train_time:176479ms step_avg:147.56ms
step:1207/1375 train_time:176633ms step_avg:147.56ms
step:1208/1375 train_time:176789ms step_avg:147.57ms
step:1209/1375 train_time:176944ms step_avg:147.58ms
step:1210/1375 train_time:177102ms step_avg:147.59ms
step:1211/1375 train_time:177257ms step_avg:147.59ms
step:1212/1375 train_time:177413ms step_avg:147.60ms
step:1213/1375 train_time:177567ms step_avg:147.60ms
step:1214/1375 train_time:177724ms step_avg:147.61ms
step:1215/1375 train_time:177878ms step_avg:147.62ms
step:1216/1375 train_time:178030ms step_avg:147.62ms
step:1217/1375 train_time:178186ms step_avg:147.63ms
step:1218/1375 train_time:178341ms step_avg:147.63ms
step:1219/1375 train_time:178494ms step_avg:147.64ms
step:1220/1375 train_time:178646ms step_avg:147.64ms
step:1221/1375 train_time:178800ms step_avg:147.65ms
step:1222/1375 train_time:178956ms step_avg:147.65ms
step:1223/1375 train_time:179113ms step_avg:147.66ms
step:1224/1375 train_time:179270ms step_avg:147.67ms
step:1225/1375 train_time:179428ms step_avg:147.68ms
step:1226/1375 train_time:179584ms step_avg:147.68ms
step:1227/1375 train_time:179742ms step_avg:147.69ms
step:1228/1375 train_time:179896ms step_avg:147.70ms
step:1229/1375 train_time:180049ms step_avg:147.70ms
step:1230/1375 train_time:180209ms step_avg:147.71ms
step:1231/1375 train_time:180368ms step_avg:147.72ms
step:1232/1375 train_time:180527ms step_avg:147.73ms
step:1233/1375 train_time:180683ms step_avg:147.74ms
step:1234/1375 train_time:180837ms step_avg:147.74ms
step:1235/1375 train_time:180993ms step_avg:147.75ms
step:1236/1375 train_time:181146ms step_avg:147.75ms
step:1237/1375 train_time:181301ms step_avg:147.76ms
step:1238/1375 train_time:181464ms step_avg:147.77ms
step:1239/1375 train_time:181620ms step_avg:147.78ms
step:1240/1375 train_time:181778ms step_avg:147.79ms
step:1241/1375 train_time:181939ms step_avg:147.80ms
step:1242/1375 train_time:182094ms step_avg:147.80ms
step:1243/1375 train_time:182253ms step_avg:147.81ms
step:1244/1375 train_time:182407ms step_avg:147.82ms
step:1245/1375 train_time:182563ms step_avg:147.82ms
step:1246/1375 train_time:182718ms step_avg:147.83ms
step:1247/1375 train_time:182878ms step_avg:147.84ms
step:1248/1375 train_time:183031ms step_avg:147.84ms
step:1249/1375 train_time:183184ms step_avg:147.85ms
step:1250/1375 train_time:183341ms step_avg:147.86ms
step:1250/1375 val_loss:3.3030 train_time:183420ms step_avg:147.92ms
step:1251/1375 train_time:183499ms step_avg:147.86ms
step:1252/1375 train_time:183654ms step_avg:147.87ms
step:1253/1375 train_time:183806ms step_avg:147.87ms
step:1254/1375 train_time:183961ms step_avg:147.88ms
step:1255/1375 train_time:184127ms step_avg:147.89ms
step:1256/1375 train_time:184282ms step_avg:147.90ms
step:1257/1375 train_time:184438ms step_avg:147.91ms
step:1258/1375 train_time:184596ms step_avg:147.91ms
step:1259/1375 train_time:184752ms step_avg:147.92ms
step:1260/1375 train_time:184904ms step_avg:147.92ms
step:1261/1375 train_time:185060ms step_avg:147.93ms
step:1262/1375 train_time:185221ms step_avg:147.94ms
step:1263/1375 train_time:185377ms step_avg:147.95ms
step:1264/1375 train_time:185530ms step_avg:147.95ms
step:1265/1375 train_time:185684ms step_avg:147.96ms
step:1266/1375 train_time:185841ms step_avg:147.96ms
step:1267/1375 train_time:185995ms step_avg:147.97ms
step:1268/1375 train_time:186152ms step_avg:147.97ms
step:1269/1375 train_time:186315ms step_avg:147.99ms
step:1270/1375 train_time:186470ms step_avg:147.99ms
step:1271/1375 train_time:186628ms step_avg:148.00ms
step:1272/1375 train_time:186783ms step_avg:148.01ms
step:1273/1375 train_time:186938ms step_avg:148.01ms
step:1274/1375 train_time:187092ms step_avg:148.02ms
step:1275/1375 train_time:187248ms step_avg:148.02ms
step:1276/1375 train_time:187400ms step_avg:148.03ms
step:1277/1375 train_time:187559ms step_avg:148.03ms
step:1278/1375 train_time:187711ms step_avg:148.04ms
step:1279/1375 train_time:187867ms step_avg:148.04ms
step:1280/1375 train_time:188031ms step_avg:148.06ms
step:1281/1375 train_time:188185ms step_avg:148.06ms
step:1282/1375 train_time:188337ms step_avg:148.06ms
step:1283/1375 train_time:188493ms step_avg:148.07ms
step:1284/1375 train_time:188652ms step_avg:148.08ms
step:1285/1375 train_time:188807ms step_avg:148.08ms
step:1286/1375 train_time:188963ms step_avg:148.09ms
step:1287/1375 train_time:189118ms step_avg:148.10ms
step:1288/1375 train_time:189275ms step_avg:148.10ms
step:1289/1375 train_time:189436ms step_avg:148.11ms
step:1290/1375 train_time:189594ms step_avg:148.12ms
step:1291/1375 train_time:189756ms step_avg:148.13ms
step:1292/1375 train_time:189912ms step_avg:148.14ms
step:1293/1375 train_time:190069ms step_avg:148.14ms
step:1294/1375 train_time:190224ms step_avg:148.15ms
step:1295/1375 train_time:190380ms step_avg:148.16ms
step:1296/1375 train_time:190538ms step_avg:148.16ms
step:1297/1375 train_time:190697ms step_avg:148.17ms
step:1298/1375 train_time:190853ms step_avg:148.18ms
step:1299/1375 train_time:191007ms step_avg:148.18ms
step:1300/1375 train_time:191161ms step_avg:148.19ms
step:1301/1375 train_time:191317ms step_avg:148.19ms
step:1302/1375 train_time:191474ms step_avg:148.20ms
step:1303/1375 train_time:191632ms step_avg:148.21ms
step:1304/1375 train_time:191790ms step_avg:148.21ms
step:1305/1375 train_time:191945ms step_avg:148.22ms
step:1306/1375 train_time:192102ms step_avg:148.23ms
step:1307/1375 train_time:192257ms step_avg:148.23ms
step:1308/1375 train_time:192414ms step_avg:148.24ms
step:1309/1375 train_time:192568ms step_avg:148.24ms
step:1310/1375 train_time:192723ms step_avg:148.25ms
step:1311/1375 train_time:192878ms step_avg:148.25ms
step:1312/1375 train_time:193032ms step_avg:148.26ms
step:1313/1375 train_time:193186ms step_avg:148.26ms
step:1314/1375 train_time:193342ms step_avg:148.27ms
step:1315/1375 train_time:193498ms step_avg:148.27ms
step:1316/1375 train_time:193653ms step_avg:148.28ms
step:1317/1375 train_time:193806ms step_avg:148.28ms
step:1318/1375 train_time:193968ms step_avg:148.29ms
step:1319/1375 train_time:194125ms step_avg:148.30ms
step:1320/1375 train_time:194281ms step_avg:148.31ms
step:1321/1375 train_time:194439ms step_avg:148.31ms
step:1322/1375 train_time:194599ms step_avg:148.32ms
step:1323/1375 train_time:194754ms step_avg:148.33ms
step:1324/1375 train_time:194908ms step_avg:148.33ms
step:1325/1375 train_time:195064ms step_avg:148.34ms
step:1326/1375 train_time:195224ms step_avg:148.35ms
step:1327/1375 train_time:195381ms step_avg:148.35ms
step:1328/1375 train_time:195537ms step_avg:148.36ms
step:1329/1375 train_time:195714ms step_avg:148.38ms
step:1330/1375 train_time:195873ms step_avg:148.39ms
step:1331/1375 train_time:196070ms step_avg:148.43ms
step:1332/1375 train_time:196236ms step_avg:148.44ms
step:1333/1375 train_time:196391ms step_avg:148.44ms
step:1334/1375 train_time:196547ms step_avg:148.45ms
step:1335/1375 train_time:196699ms step_avg:148.45ms
step:1336/1375 train_time:196862ms step_avg:148.46ms
step:1337/1375 train_time:197021ms step_avg:148.47ms
step:1338/1375 train_time:197176ms step_avg:148.48ms
step:1339/1375 train_time:197335ms step_avg:148.48ms
step:1340/1375 train_time:197492ms step_avg:148.49ms
step:1341/1375 train_time:197646ms step_avg:148.49ms
step:1342/1375 train_time:197805ms step_avg:148.50ms
step:1343/1375 train_time:197959ms step_avg:148.51ms
step:1344/1375 train_time:198116ms step_avg:148.51ms
step:1345/1375 train_time:198272ms step_avg:148.52ms
step:1346/1375 train_time:198428ms step_avg:148.52ms
step:1347/1375 train_time:198588ms step_avg:148.53ms
step:1348/1375 train_time:198742ms step_avg:148.54ms
step:1349/1375 train_time:198899ms step_avg:148.54ms
step:1350/1375 train_time:199054ms step_avg:148.55ms
step:1351/1375 train_time:199208ms step_avg:148.55ms
step:1352/1375 train_time:199374ms step_avg:148.57ms
step:1353/1375 train_time:199536ms step_avg:148.57ms
step:1354/1375 train_time:199692ms step_avg:148.58ms
step:1355/1375 train_time:199849ms step_avg:148.59ms
step:1356/1375 train_time:200003ms step_avg:148.59ms
step:1357/1375 train_time:200159ms step_avg:148.60ms
step:1358/1375 train_time:200318ms step_avg:148.60ms
step:1359/1375 train_time:200475ms step_avg:148.61ms
step:1360/1375 train_time:200637ms step_avg:148.62ms
step:1361/1375 train_time:200794ms step_avg:148.63ms
step:1362/1375 train_time:200951ms step_avg:148.63ms
step:1363/1375 train_time:201116ms step_avg:148.64ms
step:1364/1375 train_time:201271ms step_avg:148.65ms
step:1365/1375 train_time:201424ms step_avg:148.65ms
step:1366/1375 train_time:201581ms step_avg:148.66ms
step:1367/1375 train_time:201737ms step_avg:148.66ms
step:1368/1375 train_time:201892ms step_avg:148.67ms
step:1369/1375 train_time:202055ms step_avg:148.68ms
step:1370/1375 train_time:202213ms step_avg:148.69ms
step:1371/1375 train_time:202369ms step_avg:148.69ms
step:1372/1375 train_time:202533ms step_avg:148.70ms
step:1373/1375 train_time:202687ms step_avg:148.71ms
step:1374/1375 train_time:202846ms step_avg:148.71ms
step:1375/1375 train_time:202999ms step_avg:148.72ms
step:1375/1375 val_loss:3.2775 train_time:203074ms step_avg:148.77ms
peak memory consumption: 31565 MiB

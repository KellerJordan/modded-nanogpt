import os
import sys
with open(sys.argv[0]) as f:
    code = f.read() # read the code of this file ASAP, for logging
import uuid
import time
import glob
import subprocess
import contextlib
from dataclasses import dataclass

import torch
torch.empty(1, device='cuda', requires_grad=True).backward()
from torch import nn
import torch.nn.functional as F
import torch.distributed as dist
from torch.nn.parallel import DistributedDataParallel as DDP
# use of FlexAttention contributed by @KoszarskyB
from torch.nn.attention.flex_attention import BlockMask, flex_attention

# -----------------------------------------------------------------------------
# Muon optimizer

@torch.compile
def zeropower_via_newtonschulz5(G, steps):
    """
    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a
    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose
    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at
    zero even beyond the point where the iteration no longer converges all the way to one everywhere
    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T
    where S' is diagonal with S_{ii}' ~ Uniform(0.5, 1.5), which turns out not to hurt model
    performance at all relative to UV^T, where USV^T = G is the SVD.
    """
    assert len(G.shape) == 2
    a, b, c = (3.4445, -4.7750,  2.0315)
    X = G.bfloat16()
    if G.size(0) > G.size(1):
        X = X.T

    # Ensure spectral norm is at most 1
    X = X / (X.norm() + 1e-7)
    # Perform the NS iterations
    for _ in range(steps):
        A = X @ X.T
        B = b * A + c * A @ A # adapted from suggestion by @jxbz, @leloykun, and @YouJiacheng
        X = a * X + B @ X
    
    if G.size(0) > G.size(1):
        X = X.T
    return X

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Some warnings:
    - This optimizer assumes that all parameters passed in are 2D.
    - It should not be used for the embedding layer, the final fully connected layer, or any {0,1}-D
    parameters; those should all be optimized by a standard method (e.g., AdamW).
    - To use it with 4D convolutional filters, it works well to just flatten their last 3 dimensions.
    - We believe it is unlikely to work well for training with small batch size.
    - We believe it may not work well for finetuning pretrained models, but we haven't tested this.
    - We have not yet tried this optimizer for training scenarios larger than NanoGPT (124M).

    Arguments:
        lr: The learning rate used by the internal SGD.
        momentum: The momentum used by the internal SGD.
        nesterov: Whether to use Nesterov-style momentum in the internal SGD. (recommended)
        ns_steps: The number of Newton-Schulz iteration steps to use.
    """
    def __init__(self, params, lr=0.02, momentum=0.95, nesterov=True, ns_steps=5):
        self.world_size = int(os.environ['WORLD_SIZE'])
        self.rank = int(os.environ['RANK'])
        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, ns_steps=ns_steps)
        assert all(isinstance(p, torch.Tensor) for p in params)
        sizes = {p.numel() for p in params}
        param_groups = [dict(params=[p for p in params if p.numel() == size],
                             update_buffer=[torch.empty(size, device='cuda', dtype=torch.bfloat16) for _ in range(self.world_size)])
                        for size in sizes]
        super().__init__(param_groups, defaults)

    def step(self):

        for group in self.param_groups:

            lr = group['lr']
            momentum = group['momentum']
            nesterov = group['nesterov']
            ns_steps = group['ns_steps']
            update_buffers = group['update_buffer']
            # generate weight updates in distributed fashion
            params = group['params']
            handle = None
            params_world = None
            def update_prev():
                if params_world is None:
                    return
                assert handle is not None
                handle.wait()
                for p_world, g_world in zip(params_world, update_buffers):
                    p_world.data.add_(
                        g_world.view_as(p_world),
                        alpha=-lr * max(1, p_world.size(0) / p_world.size(1)) ** 0.5,
                    )
            for base_i in range(len(params))[::self.world_size]:
                if base_i + rank < len(params):
                    p = params[base_i + self.rank]
                    g = p.grad
                    assert g is not None
                    state = self.state[p]
                    if 'momentum_buffer' not in state:
                        state['momentum_buffer'] = torch.zeros_like(g)
                    buf = state['momentum_buffer']
                    buf.lerp_(g, 1 - momentum)
                    g = g.lerp_(buf, momentum) if nesterov else buf
                    g = zeropower_via_newtonschulz5(g, steps=ns_steps).flatten()
                else:
                    g = update_buffers[rank]
                update_prev() # async all_gather instead of sync all_reduce by @YouJiacheng
                handle = dist.all_gather(update_buffers, g, async_op=True)
                params_world = params[base_i : base_i + self.world_size]
            update_prev()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the GPT-2 model

def norm(x):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):

    def __init__(self, in_features, out_features):
        super().__init__(in_features, out_features, bias=False)

    def forward(self, x):
        return F.linear(x, self.weight.type_as(x))

class Rotary(nn.Module):

    def __init__(self, dim, max_seq_len=65536):
        super().__init__()
        # half-truncate RoPE by @YouJiacheng
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=dim//4, dtype=torch.float32)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(dim//4)])
        t = torch.arange(max_seq_len, dtype=torch.float32)
        theta = torch.einsum('i,j -> ij', t, angular_freq)
        self.cos = nn.Buffer(theta.cos(), persistent=False)
        self.sin = nn.Buffer(theta.sin(), persistent=False)

    def forward(self, x):
        cos, sin = self.cos[None, :x.size(-3), None, :], self.sin[None, :x.size(-3), None, :]
        x1, x2 = x.float().chunk(2, dim=-1)
        y1 = x1 * cos + x2 * sin
        y2 = x1 * (-sin) + x2 * cos
        return torch.cat((y1, y2), 3).type_as(x)

class CausalSelfAttention(nn.Module):

    def __init__(self, dim, num_heads, layer_id):
        super().__init__()
        assert dim % num_heads == 0
        self.num_heads = num_heads
        self.layer_id = layer_id
        self.c_q = CastedLinear(dim, dim)
        self.c_k = CastedLinear(dim, dim)
        self.c_v = CastedLinear(dim, dim)
        self.lambdas = nn.Parameter(torch.tensor([0.5, 0.5]))
        self.rotary = Rotary(dim // num_heads) # dim // num_heads = head_dim
        self.c_proj = CastedLinear(dim, dim)
        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977
        # self.attn_scale = nn.Parameter(torch.tensor(1.0 / (dim // num_heads) ** 0.5))
        # self.attn_scale = nn.Parameter(torch.tensor(0.20))
        self.attn_scale = 0.13 + 0.01 * min(layer_id, 11 - layer_id)  # unet pattern attention scale by @leloykun

    def forward(self, x, ve, block_mask):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, 'Must use batch size = 1 for FlexAttention'
        q = self.c_q(x).view(B, T, self.num_heads, -1)
        k = self.c_k(x).view(B, T, self.num_heads, -1)
        v = self.c_v(x).view(B, T, self.num_heads, -1)
        if ve is not None:
            v = self.lambdas[0] * v + self.lambdas[1] * ve.view_as(v) # @KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = self.lambdas[0] * v
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = self.rotary(q), self.rotary(k)
        # y = flex_attention(q.transpose(1, 2) * self.attn_scale, k.transpose(1, 2), v.transpose(1, 2), block_mask=block_mask, scale=1.)
        y = flex_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), block_mask=block_mask, scale=self.attn_scale)
        y = y.transpose(1, 2).contiguous().view_as(x) # re-assemble all head outputs side by side
        y = self.c_proj(y)
        return y

class MLP(nn.Module):

    def __init__(self, dim):
        super().__init__()
        self.c_fc = CastedLinear(dim, 4 * dim)
        self.c_proj = CastedLinear(4 * dim, dim)
        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977

    def forward(self, x):
        x = self.c_fc(x)
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = self.c_proj(x)
        return x

class Block(nn.Module):

    def __init__(self, model_dim, num_heads, layer_id, use_attn=True):
        super().__init__()
        self.attn = CausalSelfAttention(model_dim, num_heads, layer_id) if use_attn else None
        self.mlp = MLP(model_dim)
        self.lambdas = nn.Parameter(torch.tensor([1., 0.]))

    def forward(self, x, ve, x0, block_mask):
        x = self.lambdas[0] * x + self.lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), ve, block_mask)
        x = x + self.mlp(norm(x))
        return x

class ValueEmbedding(nn.Module):
    def __init__(self, vocab_size, model_dim):
        super().__init__()
        self.embed = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])

    def forward(self, inputs):
        ve = [emb(inputs).bfloat16() for emb in self.embed]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        ve = [ve[0], ve[1], ve[2], None, None, None, None, None, None, ve[0], ve[1], ve[2]]
        return ve

# -----------------------------------------------------------------------------
# The main GPT-2 model

class GPT(nn.Module):

    def __init__(self, vocab_size, num_layers, num_heads, model_dim):
        super().__init__()
        self.embed = nn.Embedding(vocab_size, model_dim)
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.blocks = nn.ModuleList([Block(model_dim, num_heads, layer_id=layer_id, use_attn=(layer_id != 7))
                                     for layer_id in range(num_layers)])
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual learning
        # U-net structure on token value embeddings by @leloykun
        self.value_embeds = ValueEmbedding(vocab_size, model_dim)
        self.lm_head = CastedLinear(model_dim, vocab_size)
        self.lm_head.weight.data.zero_() # @Grad62304977
        # U-net design by @brendanh0gan
        self.num_encoder_layers = num_layers // 2 # Half of the layers for encoder
        self.num_decoder_layers = num_layers - self.num_encoder_layers # Remaining for decoder
        # Add learnable skip connection weights for decoder layers
        self.skip_weights = nn.Parameter(torch.ones(self.num_decoder_layers))

    def forward(self, inputs, targets, sliding_window_num_blocks):
        BLOCK_SIZE = 128
        seq_len = len(inputs)
        assert seq_len % BLOCK_SIZE == 0
        total_num_blocks = seq_len // BLOCK_SIZE
        assert inputs.ndim == 1
        docs = (inputs == 50256).cumsum(0)
        docs_low = docs.view(-1, BLOCK_SIZE)[:, 0].contiguous()
        docs_high = docs.view(-1, BLOCK_SIZE)[:, -1].contiguous()

        def document_causal(b, h, q_idx, kv_idx):
            causal_mask = q_idx >= kv_idx
            document_mask = docs[q_idx] == docs[kv_idx]
            return causal_mask & document_mask

        def dense_to_ordered(dense_mask):
            num_blocks = dense_mask.sum(dim=-1, dtype=torch.int32)
            indices = dense_mask.argsort(dim=-1, descending=True, stable=True).to(torch.int32)
            return num_blocks[None, None].contiguous(), indices[None, None].contiguous()

        def create_doc_swc_block_mask(sliding_window_num_blocks):
            kv_idx = block_idx = torch.arange(total_num_blocks, dtype=torch.int32, device='cuda')
            q_idx = block_idx[:, None]
            causal_bm = q_idx >= kv_idx
            causal_full_bm = q_idx > kv_idx
            window_bm = q_idx - kv_idx < sliding_window_num_blocks
            window_full_bm = window_bm # block-wise sliding window by @YouJiacheng
            # document_bm = (docs_low[q_idx] <= docs_high[kv_idx]) & (docs_low[kv_idx] <= docs_high[q_idx])
            document_bm = (docs_low[:, None] <= docs_high) & (docs_low <= docs_high[:, None])
            document_full_bm = (docs_low[:, None] == docs_high) & (docs_low == docs_high[:, None])
            nonzero_bm = causal_bm & window_bm & document_bm
            full_bm  = causal_full_bm & window_full_bm & document_full_bm
            kv_num_blocks, kv_indices = dense_to_ordered(nonzero_bm & ~full_bm)
            full_kv_num_blocks, full_kv_indices = dense_to_ordered(full_bm)
            return BlockMask.from_kv_blocks(
                kv_num_blocks,
                kv_indices,
                full_kv_num_blocks,
                full_kv_indices,
                BLOCK_SIZE=BLOCK_SIZE,
                mask_mod=document_causal,
            )

        block_mask = create_doc_swc_block_mask(sliding_window_num_blocks)

        x0 = norm(self.embed(inputs[None]).bfloat16()) # use of norm here by @Grad62304977
        x = x0
        ve = self.value_embeds(inputs)
        assert len(ve) == len(self.blocks)
        ve_enc, ve_dec = ve[:self.num_encoder_layers], ve[self.num_encoder_layers:]

        # Store outputs for U-Net skip connections
        skip_connections = []
        # Encoder pass - process only the first half of the blocks
        for i in range(self.num_encoder_layers):
            x = self.blocks[i](x, ve_enc[i], x0, block_mask)
            skip_connections.append(x)
        # Decoder pass - process the remaining blocks with weighted skip connections
        for i in range(self.num_decoder_layers):
            x = x + self.skip_weights[i] * skip_connections.pop()
            # U-net structure on token value embeddings by @leloykun
            x = self.blocks[self.num_encoder_layers + i](x, ve_dec[i], x0, block_mask)

        x = norm(x)
        logits = self.lm_head(x)
        logits = 15 * torch.tanh(logits / 15) # @Grad62304977 added tanh softcapping, @KoszarskyB reduced it from 30 to 15
        logits = logits.float()
        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets)
        return loss

# -----------------------------------------------------------------------------
# Our own simple Distributed Data Loader

def _load_data_shard(path):
    # only reads the header, returns header data
    # header is 256 int32
    header = torch.from_file(path, False, 256, dtype=torch.int32)
    assert header[0] == 20240520, 'magic number mismatch in the data .bin file'
    assert header[1] == 1, 'unsupported version'
    num_tokens = int(header[2]) # number of tokens (claimed)
    with open(path, 'rb', buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, 'number of tokens read does not match header'
    return tokens

class DistributedDataLoader:

    def __init__(self, filename_pattern):
        self.rank = int(os.environ['RANK'])
        self.world_size = int(os.environ['WORLD_SIZE'])
        self.files = sorted(glob.glob(filename_pattern))
        self.reset()

    def reset(self):
        self.current_shard = -1
        self.advance()

    def advance(self):
        self.current_shard = (self.current_shard + 1) % len(self.files)
        self.current_position = 0
        self.tokens = _load_data_shard(self.files[self.current_shard])

    def next_batch(self, batch_size):
        assert batch_size % self.world_size == 0
        device_batch_size = batch_size // self.world_size
        # load next shard if necessary
        if self.current_position + batch_size + 1 >= len(self.tokens):
            self.advance()
        pos = self.current_position + self.rank * device_batch_size
        device_batch_tokens = self.tokens[pos:pos+device_batch_size+1]
        # advance current position
        self.current_position += batch_size
        inputs = device_batch_tokens[:-1].to(device='cuda', dtype=torch.int32, non_blocking=True)
        targets = device_batch_tokens[1:].to(device='cuda', dtype=torch.int64, non_blocking=True)
        return inputs, targets

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files = 'data/fineweb10B/fineweb_train_*.bin' # input .bin to train on
    val_files = 'data/fineweb10B/fineweb_val_*.bin' # input .bin to eval validation loss on
    val_tokens = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # optimization
    batch_size = 8*64*1024 # batch size in tokens
    num_iterations = 1375 # number of iterations to run
    cooldown_frac = 0.4 # fraction of training spent cooling down the learning rate
    bf16_embeds = True
    # evaluation and logging
    val_loss_every = 125 # every how many steps to evaluate val loss? 0 for only at the end
    # implementation
    max_device_batch_size = 64*1024 # batch size per device in tokens
    save_checkpoint = False
args = Hyperparameters()

micro_bs = args.max_device_batch_size

# set up DDP (distributed data parallel). torchrun sets this env variable
rank = int(os.environ['RANK'])
local_rank = int(os.environ['LOCAL_RANK'])
world_size = int(os.environ['WORLD_SIZE'])
assert torch.cuda.is_available()
torch.cuda.set_device(local_rank)
dist.init_process_group(backend='nccl', device_id=torch.device(local_rank))
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = uuid.uuid4()
    os.makedirs('logs', exist_ok=True)
    logfile = f'logs/{run_id}.txt'
    print(logfile)

def print0(s, console=False):
    if master_process:
        with open(logfile, 'a') as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0('='*100)
# log information about the hardware/software environment this is running on
print0(f'Running Python {sys.version}')
print0(f'Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}')
print0(subprocess.run(['nvidia-smi'], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout)
print0('='*100)

# load data
train_loader = DistributedDataLoader(args.train_files)
val_loader = DistributedDataLoader(args.val_files)
print0(f'Training dataloader files: {train_loader.files}')
print0(f'Validation dataloader files: {val_loader.files}')
print0('='*100)

# there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency. suggested to me by @Grad62304977.
# this originates from Karpathy's experiments.
model = GPT(vocab_size=50304, num_layers=12, num_heads=6, model_dim=768)
model = model.cuda()
if args.bf16_embeds:
    for m in model.modules():
        if isinstance(m, nn.Embedding):
            m.bfloat16()
model: GPT = torch.compile(model)
ddp_model = DDP(model, device_ids=[local_rank], broadcast_buffers=False, gradient_as_bucket_view=True)

# collect the parameters to optimize
hidden_matrix_params = [p for p in model.blocks.parameters() if p.ndim == 2]
embed_params = [model.embed.weight, *model.value_embeds.parameters()]
scalar_params = [p for n, p in model.named_parameters() if p.ndim < 2 and "attn_scale" not in n]
attn_scale_params = [p for n, p in model.named_parameters() if p.ndim < 2 and "attn_scale" in n]
head_params = [model.lm_head.weight]

# init the optimizer(s)
optimizer1 = torch.optim.Adam([dict(params=embed_params, lr=0.6),
                               dict(params=head_params, lr=0.008),
                               dict(params=scalar_params, lr=0.04),
                               dict(params=attn_scale_params, lr=0.01)],
                              betas=(0.8, 0.95), fused=True)
optimizer2 = Muon(hidden_matrix_params, lr=0.05, momentum=0.95)
optimizers = [optimizer1, optimizer2]

# learning rate schedule: stable then decay
def get_lr(it):
    t = 1 - it / args.num_iterations # time remaining in training
    assert 1 >= t > 0
    # 1) constant lr for first part of training
    if t >= args.cooldown_frac:
        return 1.0
    # 2) then linear cooldown
    else:
        return t / args.cooldown_frac
schedulers = [torch.optim.lr_scheduler.LambdaLR(opt, get_lr) for opt in optimizers]

# sliding window size schedule: linear increase over training in chunks of 128 from 128 -> 1792. By @fernbear.bsky.social
def get_sliding_window_blocks(it):
    x = it / args.num_iterations # training progress
    assert 0 <= x <= 1
    return int(((1 - x) * 128 + x * 1856) // 128)
sliding_window_num_blocks = torch.tensor(1, dtype=torch.int32, device='cuda')

# Start training loop
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    # This effectively ignores timing first 10 steps, which are slower for weird reasons.
    # Alternately, and slightly more correctly in terms of benchmarking, we could do 10
    # steps with dummy data first, and then re-initialize the model and reset the loader.
    if step == 10:
        training_time_ms = 0
        t0 = time.perf_counter()
    timed_steps = float('nan') if step <= 11 else (step - 10) + 1 # <= 11 to avoid bug in val

    sliding_window_num_blocks.copy_(get_sliding_window_blocks(step))

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        # run validation batches
        model.eval()
        val_loader.reset()
        val_loss = 0.0
        # calculate the number of steps to take in the val loop.
        val_batch_size = world_size * micro_bs
        assert args.val_tokens % val_batch_size == 0
        val_steps = args.val_tokens // val_batch_size
        for _ in range(val_steps):
            with torch.no_grad():
                inputs_val, targets_val = val_loader.next_batch(val_batch_size)
                val_loss += ddp_model(inputs_val, targets_val, sliding_window_num_blocks)
        # Print attention scales
        for n, p in model.named_parameters():
            if p.ndim < 2 and "attn_scale" in n:
                print0(f'{n}: {p.item()}', console=True)
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        val_loss /= val_steps
        # logging
        print0(f'step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/(timed_steps-1):.2f}ms', console=True)
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f'logs/{run_id}', exist_ok=True)
            torch.save(log, f'logs/{run_id}/state_step{step:06d}.pt')
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    model.train()
    batch_size = args.batch_size
    assert batch_size % world_size == 0
    inputs_train, targets_train = train_loader.next_batch(batch_size)
    assert len(inputs_train) <= micro_bs or len(inputs_train) % micro_bs == 0
    for micro_inputs_train, micro_targets_train in zip(inputs_train.split(micro_bs), targets_train.split(micro_bs)):
        ddp_model(micro_inputs_train, micro_targets_train, sliding_window_num_blocks).backward()
    # momentum warmup for Muon
    frac = min(step/300, 1)
    for group in optimizer2.param_groups:
        group['momentum'] = (1 - frac) * 0.85 + frac * 0.95
    # step the optimizers and schedulers
    for opt, sched in zip(optimizers, schedulers):
        opt.step()
        if step != train_steps-1:
            sched.step()
    # null the gradients
    model.zero_grad(set_to_none=True)
    # logging
    approx_time = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f'step:{step+1}/{train_steps} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms', console=True)

print0(f'peak memory consumption: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB')
dist.destroy_process_group()

====================================================================================================
Running Python 3.10.12 (main, Sep 11 2024, 15:47:36) [GCC 11.4.0]
Running PyTorch 2.6.0.dev20241231+cu126 compiled for CUDA 12.6
Tue Jan 14 16:34:46 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.183.06             Driver Version: 535.183.06   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:19:00.0 Off |                    0 |
| N/A   39C    P0             127W / 700W |   7713MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:3B:00.0 Off |                    0 |
| N/A   32C    P0             114W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:4C:00.0 Off |                    0 |
| N/A   29C    P0             117W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   38C    P0             130W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:9B:00.0 Off |                    0 |
| N/A   38C    P0             123W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:BB:00.0 Off |                    0 |
| N/A   30C    P0             114W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:CB:00.0 Off |                    0 |
| N/A   37C    P0             116W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:DB:00.0 Off |                    0 |
| N/A   30C    P0             117W / 700W |   3211MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
Training dataloader files: ['data/fineweb10B/fineweb_train_000001.bin', 'data/fineweb10B/fineweb_train_000002.bin', 'data/fineweb10B/fineweb_train_000003.bin', 'data/fineweb10B/fineweb_train_000004.bin', 'data/fineweb10B/fineweb_train_000005.bin', 'data/fineweb10B/fineweb_train_000006.bin', 'data/fineweb10B/fineweb_train_000007.bin', 'data/fineweb10B/fineweb_train_000008.bin', 'data/fineweb10B/fineweb_train_000009.bin']
Validation dataloader files: ['data/fineweb10B/fineweb_val_000000.bin']
====================================================================================================
step:0/1375 val_loss:10.8258 train_time:0ms step_avg:nanms
step:1/1375 train_time:27431ms step_avg:nanms
step:2/1375 train_time:27514ms step_avg:nanms
step:3/1375 train_time:27711ms step_avg:nanms
step:4/1375 train_time:27844ms step_avg:nanms
step:5/1375 train_time:27977ms step_avg:nanms
step:6/1375 train_time:28114ms step_avg:nanms
step:7/1375 train_time:28246ms step_avg:nanms
step:8/1375 train_time:28379ms step_avg:nanms
step:9/1375 train_time:28513ms step_avg:nanms
step:10/1375 train_time:28653ms step_avg:nanms
step:11/1375 train_time:138ms step_avg:nanms
step:12/1375 train_time:273ms step_avg:nanms
step:13/1375 train_time:406ms step_avg:135.46ms
step:14/1375 train_time:541ms step_avg:135.14ms
step:15/1375 train_time:675ms step_avg:135.08ms
step:16/1375 train_time:809ms step_avg:134.80ms
step:17/1375 train_time:944ms step_avg:134.88ms
step:18/1375 train_time:1081ms step_avg:135.14ms
step:19/1375 train_time:1217ms step_avg:135.22ms
step:20/1375 train_time:1356ms step_avg:135.63ms
step:21/1375 train_time:1495ms step_avg:135.93ms
step:22/1375 train_time:1631ms step_avg:135.89ms
step:23/1375 train_time:1765ms step_avg:135.78ms
step:24/1375 train_time:1900ms step_avg:135.70ms
step:25/1375 train_time:2038ms step_avg:135.86ms
step:26/1375 train_time:2173ms step_avg:135.83ms
step:27/1375 train_time:2307ms step_avg:135.71ms
step:28/1375 train_time:2444ms step_avg:135.79ms
step:29/1375 train_time:2579ms step_avg:135.74ms
step:30/1375 train_time:2715ms step_avg:135.74ms
step:31/1375 train_time:2848ms step_avg:135.63ms
step:32/1375 train_time:2983ms step_avg:135.59ms
step:33/1375 train_time:3118ms step_avg:135.57ms
step:34/1375 train_time:3256ms step_avg:135.66ms
step:35/1375 train_time:3390ms step_avg:135.60ms
step:36/1375 train_time:3526ms step_avg:135.60ms
step:37/1375 train_time:3661ms step_avg:135.59ms
step:38/1375 train_time:3797ms step_avg:135.61ms
step:39/1375 train_time:3931ms step_avg:135.54ms
step:40/1375 train_time:4066ms step_avg:135.53ms
step:41/1375 train_time:4200ms step_avg:135.49ms
step:42/1375 train_time:4337ms step_avg:135.55ms
step:43/1375 train_time:4473ms step_avg:135.55ms
step:44/1375 train_time:4607ms step_avg:135.51ms
step:45/1375 train_time:4742ms step_avg:135.49ms
step:46/1375 train_time:4877ms step_avg:135.49ms
step:47/1375 train_time:5012ms step_avg:135.47ms
step:48/1375 train_time:5148ms step_avg:135.46ms
step:49/1375 train_time:5283ms step_avg:135.46ms
step:50/1375 train_time:5418ms step_avg:135.45ms
step:51/1375 train_time:5556ms step_avg:135.51ms
step:52/1375 train_time:5690ms step_avg:135.48ms
step:53/1375 train_time:5825ms step_avg:135.47ms
step:54/1375 train_time:5962ms step_avg:135.49ms
step:55/1375 train_time:6096ms step_avg:135.47ms
step:56/1375 train_time:6231ms step_avg:135.45ms
step:57/1375 train_time:6366ms step_avg:135.45ms
step:58/1375 train_time:6502ms step_avg:135.46ms
step:59/1375 train_time:6638ms step_avg:135.48ms
step:60/1375 train_time:6773ms step_avg:135.46ms
step:61/1375 train_time:6907ms step_avg:135.43ms
step:62/1375 train_time:7042ms step_avg:135.42ms
step:63/1375 train_time:7178ms step_avg:135.43ms
step:64/1375 train_time:7311ms step_avg:135.39ms
step:65/1375 train_time:7447ms step_avg:135.41ms
step:66/1375 train_time:7583ms step_avg:135.40ms
step:67/1375 train_time:7718ms step_avg:135.40ms
step:68/1375 train_time:7853ms step_avg:135.39ms
step:69/1375 train_time:7987ms step_avg:135.37ms
step:70/1375 train_time:8122ms step_avg:135.37ms
step:71/1375 train_time:8259ms step_avg:135.39ms
step:72/1375 train_time:8393ms step_avg:135.38ms
step:73/1375 train_time:8531ms step_avg:135.41ms
step:74/1375 train_time:8665ms step_avg:135.39ms
step:75/1375 train_time:8800ms step_avg:135.39ms
step:76/1375 train_time:8938ms step_avg:135.42ms
step:77/1375 train_time:9073ms step_avg:135.42ms
step:78/1375 train_time:9206ms step_avg:135.39ms
step:79/1375 train_time:9344ms step_avg:135.41ms
step:80/1375 train_time:9479ms step_avg:135.42ms
step:81/1375 train_time:9614ms step_avg:135.41ms
step:82/1375 train_time:9754ms step_avg:135.47ms
step:83/1375 train_time:9889ms step_avg:135.47ms
step:84/1375 train_time:10024ms step_avg:135.46ms
step:85/1375 train_time:10160ms step_avg:135.47ms
step:86/1375 train_time:10296ms step_avg:135.47ms
step:87/1375 train_time:10430ms step_avg:135.46ms
step:88/1375 train_time:10565ms step_avg:135.45ms
step:89/1375 train_time:10703ms step_avg:135.48ms
step:90/1375 train_time:10838ms step_avg:135.48ms
step:91/1375 train_time:10975ms step_avg:135.50ms
step:92/1375 train_time:11109ms step_avg:135.47ms
step:93/1375 train_time:11245ms step_avg:135.48ms
step:94/1375 train_time:11379ms step_avg:135.47ms
step:95/1375 train_time:11516ms step_avg:135.49ms
step:96/1375 train_time:11654ms step_avg:135.51ms
step:97/1375 train_time:11788ms step_avg:135.49ms
step:98/1375 train_time:11923ms step_avg:135.49ms
step:99/1375 train_time:12059ms step_avg:135.49ms
step:100/1375 train_time:12194ms step_avg:135.49ms
step:101/1375 train_time:12328ms step_avg:135.47ms
step:102/1375 train_time:12463ms step_avg:135.47ms
step:103/1375 train_time:12600ms step_avg:135.48ms
step:104/1375 train_time:12740ms step_avg:135.54ms
step:105/1375 train_time:12880ms step_avg:135.58ms
step:106/1375 train_time:13018ms step_avg:135.61ms
step:107/1375 train_time:13159ms step_avg:135.66ms
step:108/1375 train_time:13296ms step_avg:135.67ms
step:109/1375 train_time:13433ms step_avg:135.69ms
step:110/1375 train_time:13573ms step_avg:135.73ms
step:111/1375 train_time:13713ms step_avg:135.77ms
step:112/1375 train_time:13852ms step_avg:135.81ms
step:113/1375 train_time:13990ms step_avg:135.83ms
step:114/1375 train_time:14130ms step_avg:135.87ms
step:115/1375 train_time:14268ms step_avg:135.89ms
step:116/1375 train_time:14407ms step_avg:135.92ms
step:117/1375 train_time:14548ms step_avg:135.96ms
step:118/1375 train_time:14687ms step_avg:135.99ms
step:119/1375 train_time:14825ms step_avg:136.01ms
step:120/1375 train_time:14965ms step_avg:136.05ms
step:121/1375 train_time:15106ms step_avg:136.09ms
step:122/1375 train_time:15245ms step_avg:136.12ms
step:123/1375 train_time:15384ms step_avg:136.14ms
step:124/1375 train_time:15522ms step_avg:136.16ms
step:125/1375 train_time:15662ms step_avg:136.19ms
step:125/1375 val_loss:4.3737 train_time:15729ms step_avg:136.77ms
step:126/1375 train_time:15805ms step_avg:136.25ms
step:127/1375 train_time:15946ms step_avg:136.29ms
step:128/1375 train_time:16086ms step_avg:136.32ms
step:129/1375 train_time:16221ms step_avg:136.31ms
step:130/1375 train_time:16359ms step_avg:136.32ms
step:131/1375 train_time:16496ms step_avg:136.33ms
step:132/1375 train_time:16633ms step_avg:136.34ms
step:133/1375 train_time:16775ms step_avg:136.38ms
step:134/1375 train_time:16918ms step_avg:136.43ms
step:135/1375 train_time:17057ms step_avg:136.46ms
step:136/1375 train_time:17196ms step_avg:136.48ms
step:137/1375 train_time:17335ms step_avg:136.49ms
step:138/1375 train_time:17473ms step_avg:136.51ms
step:139/1375 train_time:17613ms step_avg:136.53ms
step:140/1375 train_time:17752ms step_avg:136.55ms
step:141/1375 train_time:17894ms step_avg:136.59ms
step:142/1375 train_time:18033ms step_avg:136.61ms
step:143/1375 train_time:18171ms step_avg:136.62ms
step:144/1375 train_time:18311ms step_avg:136.65ms
step:145/1375 train_time:18450ms step_avg:136.67ms
step:146/1375 train_time:18589ms step_avg:136.68ms
step:147/1375 train_time:18729ms step_avg:136.71ms
step:148/1375 train_time:18868ms step_avg:136.72ms
step:149/1375 train_time:19009ms step_avg:136.75ms
step:150/1375 train_time:19147ms step_avg:136.76ms
step:151/1375 train_time:19286ms step_avg:136.78ms
step:152/1375 train_time:19426ms step_avg:136.80ms
step:153/1375 train_time:19563ms step_avg:136.80ms
step:154/1375 train_time:19701ms step_avg:136.81ms
step:155/1375 train_time:19840ms step_avg:136.83ms
step:156/1375 train_time:19981ms step_avg:136.85ms
step:157/1375 train_time:20120ms step_avg:136.87ms
step:158/1375 train_time:20260ms step_avg:136.89ms
step:159/1375 train_time:20398ms step_avg:136.90ms
step:160/1375 train_time:20537ms step_avg:136.91ms
step:161/1375 train_time:20677ms step_avg:136.93ms
step:162/1375 train_time:20816ms step_avg:136.94ms
step:163/1375 train_time:20956ms step_avg:136.97ms
step:164/1375 train_time:21097ms step_avg:136.99ms
step:165/1375 train_time:21237ms step_avg:137.01ms
step:166/1375 train_time:21376ms step_avg:137.03ms
step:167/1375 train_time:21515ms step_avg:137.04ms
step:168/1375 train_time:21655ms step_avg:137.06ms
step:169/1375 train_time:21796ms step_avg:137.08ms
step:170/1375 train_time:21935ms step_avg:137.10ms
step:171/1375 train_time:22073ms step_avg:137.10ms
step:172/1375 train_time:22215ms step_avg:137.13ms
step:173/1375 train_time:22353ms step_avg:137.14ms
step:174/1375 train_time:22492ms step_avg:137.15ms
step:175/1375 train_time:22631ms step_avg:137.16ms
step:176/1375 train_time:22769ms step_avg:137.16ms
step:177/1375 train_time:22909ms step_avg:137.18ms
step:178/1375 train_time:23049ms step_avg:137.19ms
step:179/1375 train_time:23188ms step_avg:137.21ms
step:180/1375 train_time:23328ms step_avg:137.22ms
step:181/1375 train_time:23468ms step_avg:137.24ms
step:182/1375 train_time:23607ms step_avg:137.25ms
step:183/1375 train_time:23746ms step_avg:137.26ms
step:184/1375 train_time:23885ms step_avg:137.27ms
step:185/1375 train_time:24023ms step_avg:137.27ms
step:186/1375 train_time:24161ms step_avg:137.28ms
step:187/1375 train_time:24300ms step_avg:137.29ms
step:188/1375 train_time:24439ms step_avg:137.30ms
step:189/1375 train_time:24579ms step_avg:137.31ms
step:190/1375 train_time:24718ms step_avg:137.32ms
step:191/1375 train_time:24892ms step_avg:137.53ms
step:192/1375 train_time:25030ms step_avg:137.53ms
step:193/1375 train_time:25167ms step_avg:137.52ms
step:194/1375 train_time:25306ms step_avg:137.53ms
step:195/1375 train_time:25442ms step_avg:137.53ms
step:196/1375 train_time:25580ms step_avg:137.53ms
step:197/1375 train_time:25719ms step_avg:137.53ms
step:198/1375 train_time:25864ms step_avg:137.57ms
step:199/1375 train_time:26004ms step_avg:137.59ms
step:200/1375 train_time:26141ms step_avg:137.58ms
step:201/1375 train_time:26279ms step_avg:137.59ms
step:202/1375 train_time:26418ms step_avg:137.59ms
step:203/1375 train_time:26556ms step_avg:137.60ms
step:204/1375 train_time:26695ms step_avg:137.60ms
step:205/1375 train_time:26836ms step_avg:137.62ms
step:206/1375 train_time:26981ms step_avg:137.66ms
step:207/1375 train_time:27122ms step_avg:137.68ms
step:208/1375 train_time:27263ms step_avg:137.69ms
step:209/1375 train_time:27402ms step_avg:137.70ms
step:210/1375 train_time:27542ms step_avg:137.71ms
step:211/1375 train_time:27683ms step_avg:137.73ms
step:212/1375 train_time:27825ms step_avg:137.75ms
step:213/1375 train_time:27967ms step_avg:137.77ms
step:214/1375 train_time:28111ms step_avg:137.80ms
step:215/1375 train_time:28252ms step_avg:137.82ms
step:216/1375 train_time:28394ms step_avg:137.83ms
step:217/1375 train_time:28535ms step_avg:137.85ms
step:218/1375 train_time:28675ms step_avg:137.86ms
step:219/1375 train_time:28816ms step_avg:137.88ms
step:220/1375 train_time:28959ms step_avg:137.90ms
step:221/1375 train_time:29102ms step_avg:137.93ms
step:222/1375 train_time:29245ms step_avg:137.95ms
step:223/1375 train_time:29388ms step_avg:137.97ms
step:224/1375 train_time:29529ms step_avg:137.98ms
step:225/1375 train_time:29669ms step_avg:138.00ms
step:226/1375 train_time:29812ms step_avg:138.02ms
step:227/1375 train_time:29954ms step_avg:138.04ms
step:228/1375 train_time:30096ms step_avg:138.06ms
step:229/1375 train_time:30237ms step_avg:138.07ms
step:230/1375 train_time:30380ms step_avg:138.09ms
step:231/1375 train_time:30521ms step_avg:138.11ms
step:232/1375 train_time:30662ms step_avg:138.12ms
step:233/1375 train_time:30803ms step_avg:138.13ms
step:234/1375 train_time:30945ms step_avg:138.15ms
step:235/1375 train_time:31087ms step_avg:138.16ms
step:236/1375 train_time:31228ms step_avg:138.18ms
step:237/1375 train_time:31369ms step_avg:138.19ms
step:238/1375 train_time:31514ms step_avg:138.22ms
step:239/1375 train_time:31655ms step_avg:138.23ms
step:240/1375 train_time:31798ms step_avg:138.25ms
step:241/1375 train_time:31939ms step_avg:138.26ms
step:242/1375 train_time:32082ms step_avg:138.28ms
step:243/1375 train_time:32225ms step_avg:138.31ms
step:244/1375 train_time:32366ms step_avg:138.32ms
step:245/1375 train_time:32509ms step_avg:138.34ms
step:246/1375 train_time:32650ms step_avg:138.35ms
step:247/1375 train_time:32793ms step_avg:138.37ms
step:248/1375 train_time:32934ms step_avg:138.38ms
step:249/1375 train_time:33075ms step_avg:138.39ms
step:250/1375 train_time:33218ms step_avg:138.41ms
step:250/1375 val_loss:3.9551 train_time:33287ms step_avg:138.70ms
step:251/1375 train_time:33360ms step_avg:138.42ms
step:252/1375 train_time:33504ms step_avg:138.45ms
step:253/1375 train_time:33645ms step_avg:138.46ms
step:254/1375 train_time:33784ms step_avg:138.46ms
step:255/1375 train_time:33924ms step_avg:138.47ms
step:256/1375 train_time:34065ms step_avg:138.47ms
step:257/1375 train_time:34207ms step_avg:138.49ms
step:258/1375 train_time:34351ms step_avg:138.51ms
step:259/1375 train_time:34493ms step_avg:138.53ms
step:260/1375 train_time:34635ms step_avg:138.54ms
step:261/1375 train_time:34776ms step_avg:138.55ms
step:262/1375 train_time:34917ms step_avg:138.56ms
step:263/1375 train_time:35056ms step_avg:138.56ms
step:264/1375 train_time:35197ms step_avg:138.57ms
step:265/1375 train_time:35340ms step_avg:138.59ms
step:266/1375 train_time:35485ms step_avg:138.61ms
step:267/1375 train_time:35629ms step_avg:138.64ms
step:268/1375 train_time:35771ms step_avg:138.65ms
step:269/1375 train_time:35912ms step_avg:138.66ms
step:270/1375 train_time:36053ms step_avg:138.66ms
step:271/1375 train_time:36193ms step_avg:138.67ms
step:272/1375 train_time:36336ms step_avg:138.69ms
step:273/1375 train_time:36480ms step_avg:138.71ms
step:274/1375 train_time:36623ms step_avg:138.72ms
step:275/1375 train_time:36763ms step_avg:138.73ms
step:276/1375 train_time:36905ms step_avg:138.74ms
step:277/1375 train_time:37046ms step_avg:138.75ms
step:278/1375 train_time:37186ms step_avg:138.75ms
step:279/1375 train_time:37330ms step_avg:138.77ms
step:280/1375 train_time:37471ms step_avg:138.78ms
step:281/1375 train_time:37613ms step_avg:138.79ms
step:282/1375 train_time:37754ms step_avg:138.80ms
step:283/1375 train_time:37896ms step_avg:138.81ms
step:284/1375 train_time:38038ms step_avg:138.83ms
step:285/1375 train_time:38181ms step_avg:138.84ms
step:286/1375 train_time:38323ms step_avg:138.85ms
step:287/1375 train_time:38465ms step_avg:138.86ms
step:288/1375 train_time:38606ms step_avg:138.87ms
step:289/1375 train_time:38749ms step_avg:138.88ms
step:290/1375 train_time:38890ms step_avg:138.89ms
step:291/1375 train_time:39033ms step_avg:138.91ms
step:292/1375 train_time:39176ms step_avg:138.92ms
step:293/1375 train_time:39320ms step_avg:138.94ms
step:294/1375 train_time:39461ms step_avg:138.95ms
step:295/1375 train_time:39603ms step_avg:138.96ms
step:296/1375 train_time:39745ms step_avg:138.97ms
step:297/1375 train_time:39887ms step_avg:138.98ms
step:298/1375 train_time:40030ms step_avg:138.99ms
step:299/1375 train_time:40171ms step_avg:139.00ms
step:300/1375 train_time:40314ms step_avg:139.01ms
step:301/1375 train_time:40455ms step_avg:139.02ms
step:302/1375 train_time:40597ms step_avg:139.03ms
step:303/1375 train_time:40739ms step_avg:139.04ms
step:304/1375 train_time:40882ms step_avg:139.05ms
step:305/1375 train_time:41023ms step_avg:139.06ms
step:306/1375 train_time:41166ms step_avg:139.07ms
step:307/1375 train_time:41307ms step_avg:139.08ms
step:308/1375 train_time:41451ms step_avg:139.10ms
step:309/1375 train_time:41595ms step_avg:139.11ms
step:310/1375 train_time:41740ms step_avg:139.13ms
step:311/1375 train_time:41885ms step_avg:139.15ms
step:312/1375 train_time:42028ms step_avg:139.17ms
step:313/1375 train_time:42172ms step_avg:139.18ms
step:314/1375 train_time:42316ms step_avg:139.20ms
step:315/1375 train_time:42460ms step_avg:139.21ms
step:316/1375 train_time:42605ms step_avg:139.23ms
step:317/1375 train_time:42749ms step_avg:139.25ms
step:318/1375 train_time:42893ms step_avg:139.26ms
step:319/1375 train_time:43037ms step_avg:139.28ms
step:320/1375 train_time:43181ms step_avg:139.29ms
step:321/1375 train_time:43325ms step_avg:139.31ms
step:322/1375 train_time:43467ms step_avg:139.32ms
step:323/1375 train_time:43610ms step_avg:139.33ms
step:324/1375 train_time:43754ms step_avg:139.35ms
step:325/1375 train_time:43898ms step_avg:139.36ms
step:326/1375 train_time:44042ms step_avg:139.37ms
step:327/1375 train_time:44185ms step_avg:139.38ms
step:328/1375 train_time:44329ms step_avg:139.40ms
step:329/1375 train_time:44473ms step_avg:139.41ms
step:330/1375 train_time:44617ms step_avg:139.43ms
step:331/1375 train_time:44761ms step_avg:139.44ms
step:332/1375 train_time:44905ms step_avg:139.46ms
step:333/1375 train_time:45048ms step_avg:139.47ms
step:334/1375 train_time:45193ms step_avg:139.48ms
step:335/1375 train_time:45337ms step_avg:139.50ms
step:336/1375 train_time:45482ms step_avg:139.51ms
step:337/1375 train_time:45627ms step_avg:139.53ms
step:338/1375 train_time:45771ms step_avg:139.54ms
step:339/1375 train_time:45914ms step_avg:139.56ms
step:340/1375 train_time:46057ms step_avg:139.57ms
step:341/1375 train_time:46201ms step_avg:139.58ms
step:342/1375 train_time:46346ms step_avg:139.60ms
step:343/1375 train_time:46488ms step_avg:139.60ms
step:344/1375 train_time:46631ms step_avg:139.61ms
step:345/1375 train_time:46775ms step_avg:139.63ms
step:346/1375 train_time:46919ms step_avg:139.64ms
step:347/1375 train_time:47062ms step_avg:139.65ms
step:348/1375 train_time:47206ms step_avg:139.66ms
step:349/1375 train_time:47351ms step_avg:139.68ms
step:350/1375 train_time:47496ms step_avg:139.69ms
step:351/1375 train_time:47640ms step_avg:139.71ms
step:352/1375 train_time:47785ms step_avg:139.72ms
step:353/1375 train_time:47931ms step_avg:139.74ms
step:354/1375 train_time:48074ms step_avg:139.75ms
step:355/1375 train_time:48218ms step_avg:139.76ms
step:356/1375 train_time:48362ms step_avg:139.78ms
step:357/1375 train_time:48506ms step_avg:139.79ms
step:358/1375 train_time:48651ms step_avg:139.80ms
step:359/1375 train_time:48793ms step_avg:139.81ms
step:360/1375 train_time:48939ms step_avg:139.82ms
step:361/1375 train_time:49082ms step_avg:139.83ms
step:362/1375 train_time:49227ms step_avg:139.85ms
step:363/1375 train_time:49370ms step_avg:139.86ms
step:364/1375 train_time:49515ms step_avg:139.87ms
step:365/1375 train_time:49659ms step_avg:139.89ms
step:366/1375 train_time:49806ms step_avg:139.90ms
step:367/1375 train_time:49950ms step_avg:139.92ms
step:368/1375 train_time:50093ms step_avg:139.92ms
step:369/1375 train_time:50238ms step_avg:139.94ms
step:370/1375 train_time:50381ms step_avg:139.95ms
step:371/1375 train_time:50527ms step_avg:139.96ms
step:372/1375 train_time:50670ms step_avg:139.97ms
step:373/1375 train_time:50815ms step_avg:139.99ms
step:374/1375 train_time:50960ms step_avg:140.00ms
step:375/1375 train_time:51103ms step_avg:140.01ms
step:375/1375 val_loss:3.7705 train_time:51172ms step_avg:140.20ms
step:376/1375 train_time:51246ms step_avg:140.02ms
step:377/1375 train_time:51393ms step_avg:140.04ms
step:378/1375 train_time:51538ms step_avg:140.05ms
step:379/1375 train_time:51681ms step_avg:140.06ms
step:380/1375 train_time:51824ms step_avg:140.07ms
step:381/1375 train_time:52013ms step_avg:140.20ms
step:382/1375 train_time:52154ms step_avg:140.20ms
step:383/1375 train_time:52296ms step_avg:140.20ms
step:384/1375 train_time:52438ms step_avg:140.21ms
step:385/1375 train_time:52582ms step_avg:140.22ms
step:386/1375 train_time:52724ms step_avg:140.22ms
step:387/1375 train_time:52867ms step_avg:140.23ms
step:388/1375 train_time:53015ms step_avg:140.25ms
step:389/1375 train_time:53160ms step_avg:140.26ms
step:390/1375 train_time:53305ms step_avg:140.28ms
step:391/1375 train_time:53449ms step_avg:140.29ms
step:392/1375 train_time:53593ms step_avg:140.29ms
step:393/1375 train_time:53737ms step_avg:140.30ms
step:394/1375 train_time:53879ms step_avg:140.31ms
step:395/1375 train_time:54025ms step_avg:140.32ms
step:396/1375 train_time:54170ms step_avg:140.34ms
step:397/1375 train_time:54315ms step_avg:140.35ms
step:398/1375 train_time:54459ms step_avg:140.36ms
step:399/1375 train_time:54602ms step_avg:140.37ms
step:400/1375 train_time:54747ms step_avg:140.38ms
step:401/1375 train_time:54890ms step_avg:140.38ms
step:402/1375 train_time:55035ms step_avg:140.40ms
step:403/1375 train_time:55178ms step_avg:140.40ms
step:404/1375 train_time:55324ms step_avg:140.42ms
step:405/1375 train_time:55467ms step_avg:140.42ms
step:406/1375 train_time:55611ms step_avg:140.43ms
step:407/1375 train_time:55755ms step_avg:140.44ms
step:408/1375 train_time:55898ms step_avg:140.45ms
step:409/1375 train_time:56043ms step_avg:140.46ms
step:410/1375 train_time:56189ms step_avg:140.47ms
step:411/1375 train_time:56335ms step_avg:140.49ms
step:412/1375 train_time:56480ms step_avg:140.50ms
step:413/1375 train_time:56628ms step_avg:140.52ms
step:414/1375 train_time:56774ms step_avg:140.53ms
step:415/1375 train_time:56921ms step_avg:140.54ms
step:416/1375 train_time:57066ms step_avg:140.56ms
step:417/1375 train_time:57213ms step_avg:140.57ms
step:418/1375 train_time:57358ms step_avg:140.58ms
step:419/1375 train_time:57504ms step_avg:140.60ms
step:420/1375 train_time:57650ms step_avg:140.61ms
step:421/1375 train_time:57794ms step_avg:140.62ms
step:422/1375 train_time:57942ms step_avg:140.63ms
step:423/1375 train_time:58087ms step_avg:140.65ms
step:424/1375 train_time:58232ms step_avg:140.66ms
step:425/1375 train_time:58375ms step_avg:140.66ms
step:426/1375 train_time:58522ms step_avg:140.68ms
step:427/1375 train_time:58666ms step_avg:140.68ms
step:428/1375 train_time:58813ms step_avg:140.70ms
step:429/1375 train_time:58958ms step_avg:140.71ms
step:430/1375 train_time:59104ms step_avg:140.72ms
step:431/1375 train_time:59251ms step_avg:140.74ms
step:432/1375 train_time:59394ms step_avg:140.74ms
step:433/1375 train_time:59540ms step_avg:140.76ms
step:434/1375 train_time:59685ms step_avg:140.77ms
step:435/1375 train_time:59832ms step_avg:140.78ms
step:436/1375 train_time:59976ms step_avg:140.79ms
step:437/1375 train_time:60122ms step_avg:140.80ms
step:438/1375 train_time:60267ms step_avg:140.81ms
step:439/1375 train_time:60412ms step_avg:140.82ms
step:440/1375 train_time:60559ms step_avg:140.83ms
step:441/1375 train_time:60705ms step_avg:140.85ms
step:442/1375 train_time:60852ms step_avg:140.86ms
step:443/1375 train_time:60995ms step_avg:140.87ms
step:444/1375 train_time:61144ms step_avg:140.88ms
step:445/1375 train_time:61289ms step_avg:140.89ms
step:446/1375 train_time:61434ms step_avg:140.90ms
step:447/1375 train_time:61578ms step_avg:140.91ms
step:448/1375 train_time:61725ms step_avg:140.93ms
step:449/1375 train_time:61871ms step_avg:140.94ms
step:450/1375 train_time:62015ms step_avg:140.94ms
step:451/1375 train_time:62162ms step_avg:140.96ms
step:452/1375 train_time:62308ms step_avg:140.97ms
step:453/1375 train_time:62454ms step_avg:140.98ms
step:454/1375 train_time:62600ms step_avg:140.99ms
step:455/1375 train_time:62747ms step_avg:141.00ms
step:456/1375 train_time:62893ms step_avg:141.02ms
step:457/1375 train_time:63039ms step_avg:141.03ms
step:458/1375 train_time:63185ms step_avg:141.04ms
step:459/1375 train_time:63332ms step_avg:141.05ms
step:460/1375 train_time:63476ms step_avg:141.06ms
step:461/1375 train_time:63623ms step_avg:141.07ms
step:462/1375 train_time:63769ms step_avg:141.08ms
step:463/1375 train_time:63915ms step_avg:141.09ms
step:464/1375 train_time:64059ms step_avg:141.10ms
step:465/1375 train_time:64205ms step_avg:141.11ms
step:466/1375 train_time:64352ms step_avg:141.12ms
step:467/1375 train_time:64499ms step_avg:141.14ms
step:468/1375 train_time:64644ms step_avg:141.14ms
step:469/1375 train_time:64790ms step_avg:141.15ms
step:470/1375 train_time:64935ms step_avg:141.16ms
step:471/1375 train_time:65079ms step_avg:141.17ms
step:472/1375 train_time:65226ms step_avg:141.18ms
step:473/1375 train_time:65372ms step_avg:141.19ms
step:474/1375 train_time:65518ms step_avg:141.20ms
step:475/1375 train_time:65664ms step_avg:141.21ms
step:476/1375 train_time:65810ms step_avg:141.22ms
step:477/1375 train_time:65955ms step_avg:141.23ms
step:478/1375 train_time:66099ms step_avg:141.24ms
step:479/1375 train_time:66246ms step_avg:141.25ms
step:480/1375 train_time:66391ms step_avg:141.26ms
step:481/1375 train_time:66536ms step_avg:141.27ms
step:482/1375 train_time:66680ms step_avg:141.27ms
step:483/1375 train_time:66829ms step_avg:141.29ms
step:484/1375 train_time:66974ms step_avg:141.30ms
step:485/1375 train_time:67118ms step_avg:141.30ms
step:486/1375 train_time:67264ms step_avg:141.31ms
step:487/1375 train_time:67410ms step_avg:141.32ms
step:488/1375 train_time:67555ms step_avg:141.33ms
step:489/1375 train_time:67699ms step_avg:141.33ms
step:490/1375 train_time:67846ms step_avg:141.34ms
step:491/1375 train_time:67992ms step_avg:141.36ms
step:492/1375 train_time:68138ms step_avg:141.37ms
step:493/1375 train_time:68284ms step_avg:141.37ms
step:494/1375 train_time:68430ms step_avg:141.38ms
step:495/1375 train_time:68575ms step_avg:141.39ms
step:496/1375 train_time:68720ms step_avg:141.40ms
step:497/1375 train_time:68865ms step_avg:141.41ms
step:498/1375 train_time:69011ms step_avg:141.42ms
step:499/1375 train_time:69156ms step_avg:141.42ms
step:500/1375 train_time:69303ms step_avg:141.43ms
step:500/1375 val_loss:3.6557 train_time:69375ms step_avg:141.58ms
step:501/1375 train_time:69451ms step_avg:141.45ms
step:502/1375 train_time:69595ms step_avg:141.45ms
step:503/1375 train_time:69742ms step_avg:141.46ms
step:504/1375 train_time:69886ms step_avg:141.47ms
step:505/1375 train_time:70033ms step_avg:141.48ms
step:506/1375 train_time:70176ms step_avg:141.48ms
step:507/1375 train_time:70323ms step_avg:141.49ms
step:508/1375 train_time:70472ms step_avg:141.51ms
step:509/1375 train_time:70617ms step_avg:141.52ms
step:510/1375 train_time:70764ms step_avg:141.53ms
step:511/1375 train_time:70909ms step_avg:141.54ms
step:512/1375 train_time:71057ms step_avg:141.55ms
step:513/1375 train_time:71205ms step_avg:141.56ms
step:514/1375 train_time:71354ms step_avg:141.58ms
step:515/1375 train_time:71503ms step_avg:141.59ms
step:516/1375 train_time:71652ms step_avg:141.60ms
step:517/1375 train_time:71797ms step_avg:141.61ms
step:518/1375 train_time:71945ms step_avg:141.62ms
step:519/1375 train_time:72091ms step_avg:141.63ms
step:520/1375 train_time:72239ms step_avg:141.65ms
step:521/1375 train_time:72387ms step_avg:141.66ms
step:522/1375 train_time:72535ms step_avg:141.67ms
step:523/1375 train_time:72682ms step_avg:141.68ms
step:524/1375 train_time:72830ms step_avg:141.69ms
step:525/1375 train_time:72976ms step_avg:141.70ms
step:526/1375 train_time:73125ms step_avg:141.71ms
step:527/1375 train_time:73272ms step_avg:141.73ms
step:528/1375 train_time:73420ms step_avg:141.74ms
step:529/1375 train_time:73567ms step_avg:141.75ms
step:530/1375 train_time:73714ms step_avg:141.76ms
step:531/1375 train_time:73861ms step_avg:141.77ms
step:532/1375 train_time:74009ms step_avg:141.78ms
step:533/1375 train_time:74157ms step_avg:141.79ms
step:534/1375 train_time:74306ms step_avg:141.81ms
step:535/1375 train_time:74454ms step_avg:141.82ms
step:536/1375 train_time:74602ms step_avg:141.83ms
step:537/1375 train_time:74751ms step_avg:141.84ms
step:538/1375 train_time:74896ms step_avg:141.85ms
step:539/1375 train_time:75047ms step_avg:141.87ms
step:540/1375 train_time:75194ms step_avg:141.88ms
step:541/1375 train_time:75342ms step_avg:141.89ms
step:542/1375 train_time:75489ms step_avg:141.90ms
step:543/1375 train_time:75636ms step_avg:141.91ms
step:544/1375 train_time:75783ms step_avg:141.91ms
step:545/1375 train_time:75931ms step_avg:141.93ms
step:546/1375 train_time:76077ms step_avg:141.93ms
step:547/1375 train_time:76225ms step_avg:141.95ms
step:548/1375 train_time:76373ms step_avg:141.96ms
step:549/1375 train_time:76521ms step_avg:141.97ms
step:550/1375 train_time:76670ms step_avg:141.98ms
step:551/1375 train_time:76815ms step_avg:141.99ms
step:552/1375 train_time:76964ms step_avg:142.00ms
step:553/1375 train_time:77111ms step_avg:142.01ms
step:554/1375 train_time:77256ms step_avg:142.01ms
step:555/1375 train_time:77404ms step_avg:142.03ms
step:556/1375 train_time:77552ms step_avg:142.04ms
step:557/1375 train_time:77700ms step_avg:142.05ms
step:558/1375 train_time:77848ms step_avg:142.06ms
step:559/1375 train_time:77993ms step_avg:142.06ms
step:560/1375 train_time:78141ms step_avg:142.08ms
step:561/1375 train_time:78288ms step_avg:142.08ms
step:562/1375 train_time:78435ms step_avg:142.09ms
step:563/1375 train_time:78582ms step_avg:142.10ms
step:564/1375 train_time:78731ms step_avg:142.11ms
step:565/1375 train_time:78875ms step_avg:142.12ms
step:566/1375 train_time:79022ms step_avg:142.13ms
step:567/1375 train_time:79170ms step_avg:142.14ms
step:568/1375 train_time:79316ms step_avg:142.14ms
step:569/1375 train_time:79465ms step_avg:142.16ms
step:570/1375 train_time:79612ms step_avg:142.16ms
step:571/1375 train_time:79812ms step_avg:142.27ms
step:572/1375 train_time:79958ms step_avg:142.27ms
step:573/1375 train_time:80104ms step_avg:142.28ms
step:574/1375 train_time:80254ms step_avg:142.30ms
step:575/1375 train_time:80401ms step_avg:142.30ms
step:576/1375 train_time:80548ms step_avg:142.31ms
step:577/1375 train_time:80695ms step_avg:142.32ms
step:578/1375 train_time:80844ms step_avg:142.33ms
step:579/1375 train_time:80991ms step_avg:142.34ms
step:580/1375 train_time:81137ms step_avg:142.35ms
step:581/1375 train_time:81284ms step_avg:142.35ms
step:582/1375 train_time:81431ms step_avg:142.36ms
step:583/1375 train_time:81576ms step_avg:142.37ms
step:584/1375 train_time:81727ms step_avg:142.38ms
step:585/1375 train_time:81875ms step_avg:142.39ms
step:586/1375 train_time:82024ms step_avg:142.40ms
step:587/1375 train_time:82172ms step_avg:142.41ms
step:588/1375 train_time:82318ms step_avg:142.42ms
step:589/1375 train_time:82465ms step_avg:142.43ms
step:590/1375 train_time:82612ms step_avg:142.44ms
step:591/1375 train_time:82761ms step_avg:142.45ms
step:592/1375 train_time:82912ms step_avg:142.46ms
step:593/1375 train_time:83060ms step_avg:142.47ms
step:594/1375 train_time:83208ms step_avg:142.48ms
step:595/1375 train_time:83354ms step_avg:142.48ms
step:596/1375 train_time:83500ms step_avg:142.49ms
step:597/1375 train_time:83647ms step_avg:142.50ms
step:598/1375 train_time:83793ms step_avg:142.51ms
step:599/1375 train_time:83943ms step_avg:142.52ms
step:600/1375 train_time:84090ms step_avg:142.52ms
step:601/1375 train_time:84236ms step_avg:142.53ms
step:602/1375 train_time:84384ms step_avg:142.54ms
step:603/1375 train_time:84533ms step_avg:142.55ms
step:604/1375 train_time:84679ms step_avg:142.56ms
step:605/1375 train_time:84827ms step_avg:142.57ms
step:606/1375 train_time:84974ms step_avg:142.57ms
step:607/1375 train_time:85121ms step_avg:142.58ms
step:608/1375 train_time:85268ms step_avg:142.59ms
step:609/1375 train_time:85414ms step_avg:142.59ms
step:610/1375 train_time:85561ms step_avg:142.60ms
step:611/1375 train_time:85709ms step_avg:142.61ms
step:612/1375 train_time:85855ms step_avg:142.62ms
step:613/1375 train_time:86007ms step_avg:142.63ms
step:614/1375 train_time:86155ms step_avg:142.64ms
step:615/1375 train_time:86302ms step_avg:142.65ms
step:616/1375 train_time:86451ms step_avg:142.66ms
step:617/1375 train_time:86598ms step_avg:142.67ms
step:618/1375 train_time:86747ms step_avg:142.68ms
step:619/1375 train_time:86897ms step_avg:142.69ms
step:620/1375 train_time:87049ms step_avg:142.70ms
step:621/1375 train_time:87196ms step_avg:142.71ms
step:622/1375 train_time:87347ms step_avg:142.72ms
step:623/1375 train_time:87494ms step_avg:142.73ms
step:624/1375 train_time:87643ms step_avg:142.74ms
step:625/1375 train_time:87792ms step_avg:142.75ms
step:625/1375 val_loss:3.5735 train_time:87867ms step_avg:142.87ms
step:626/1375 train_time:87942ms step_avg:142.76ms
step:627/1375 train_time:88092ms step_avg:142.77ms
step:628/1375 train_time:88239ms step_avg:142.78ms
step:629/1375 train_time:88387ms step_avg:142.79ms
step:630/1375 train_time:88535ms step_avg:142.80ms
step:631/1375 train_time:88681ms step_avg:142.80ms
step:632/1375 train_time:88832ms step_avg:142.82ms
step:633/1375 train_time:88980ms step_avg:142.83ms
step:634/1375 train_time:89132ms step_avg:142.84ms
step:635/1375 train_time:89279ms step_avg:142.85ms
step:636/1375 train_time:89429ms step_avg:142.86ms
step:637/1375 train_time:89576ms step_avg:142.86ms
step:638/1375 train_time:89726ms step_avg:142.88ms
step:639/1375 train_time:89874ms step_avg:142.88ms
step:640/1375 train_time:90026ms step_avg:142.90ms
step:641/1375 train_time:90174ms step_avg:142.91ms
step:642/1375 train_time:90326ms step_avg:142.92ms
step:643/1375 train_time:90474ms step_avg:142.93ms
step:644/1375 train_time:90624ms step_avg:142.94ms
step:645/1375 train_time:90772ms step_avg:142.95ms
step:646/1375 train_time:90920ms step_avg:142.96ms
step:647/1375 train_time:91069ms step_avg:142.96ms
step:648/1375 train_time:91220ms step_avg:142.98ms
step:649/1375 train_time:91370ms step_avg:142.99ms
step:650/1375 train_time:91520ms step_avg:143.00ms
step:651/1375 train_time:91669ms step_avg:143.01ms
step:652/1375 train_time:91818ms step_avg:143.02ms
step:653/1375 train_time:91968ms step_avg:143.03ms
step:654/1375 train_time:92117ms step_avg:143.04ms
step:655/1375 train_time:92266ms step_avg:143.05ms
step:656/1375 train_time:92415ms step_avg:143.06ms
step:657/1375 train_time:92563ms step_avg:143.07ms
step:658/1375 train_time:92712ms step_avg:143.07ms
step:659/1375 train_time:92858ms step_avg:143.08ms
step:660/1375 train_time:93008ms step_avg:143.09ms
step:661/1375 train_time:93155ms step_avg:143.10ms
step:662/1375 train_time:93306ms step_avg:143.11ms
step:663/1375 train_time:93453ms step_avg:143.11ms
step:664/1375 train_time:93606ms step_avg:143.13ms
step:665/1375 train_time:93755ms step_avg:143.14ms
step:666/1375 train_time:93904ms step_avg:143.15ms
step:667/1375 train_time:94052ms step_avg:143.15ms
step:668/1375 train_time:94201ms step_avg:143.16ms
step:669/1375 train_time:94349ms step_avg:143.17ms
step:670/1375 train_time:94497ms step_avg:143.18ms
step:671/1375 train_time:94645ms step_avg:143.18ms
step:672/1375 train_time:94796ms step_avg:143.20ms
step:673/1375 train_time:94944ms step_avg:143.20ms
step:674/1375 train_time:95093ms step_avg:143.21ms
step:675/1375 train_time:95241ms step_avg:143.22ms
step:676/1375 train_time:95391ms step_avg:143.23ms
step:677/1375 train_time:95541ms step_avg:143.24ms
step:678/1375 train_time:95690ms step_avg:143.25ms
step:679/1375 train_time:95840ms step_avg:143.26ms
step:680/1375 train_time:95989ms step_avg:143.27ms
step:681/1375 train_time:96138ms step_avg:143.28ms
step:682/1375 train_time:96288ms step_avg:143.29ms
step:683/1375 train_time:96436ms step_avg:143.29ms
step:684/1375 train_time:96586ms step_avg:143.30ms
step:685/1375 train_time:96735ms step_avg:143.31ms
step:686/1375 train_time:96882ms step_avg:143.32ms
step:687/1375 train_time:97030ms step_avg:143.32ms
step:688/1375 train_time:97179ms step_avg:143.33ms
step:689/1375 train_time:97329ms step_avg:143.34ms
step:690/1375 train_time:97480ms step_avg:143.35ms
step:691/1375 train_time:97629ms step_avg:143.36ms
step:692/1375 train_time:97777ms step_avg:143.37ms
step:693/1375 train_time:97927ms step_avg:143.38ms
step:694/1375 train_time:98074ms step_avg:143.38ms
step:695/1375 train_time:98225ms step_avg:143.39ms
step:696/1375 train_time:98372ms step_avg:143.40ms
step:697/1375 train_time:98522ms step_avg:143.41ms
step:698/1375 train_time:98670ms step_avg:143.42ms
step:699/1375 train_time:98818ms step_avg:143.42ms
step:700/1375 train_time:98968ms step_avg:143.43ms
step:701/1375 train_time:99116ms step_avg:143.44ms
step:702/1375 train_time:99267ms step_avg:143.45ms
step:703/1375 train_time:99415ms step_avg:143.46ms
step:704/1375 train_time:99565ms step_avg:143.46ms
step:705/1375 train_time:99714ms step_avg:143.47ms
step:706/1375 train_time:99865ms step_avg:143.48ms
step:707/1375 train_time:100013ms step_avg:143.49ms
step:708/1375 train_time:100161ms step_avg:143.50ms
step:709/1375 train_time:100312ms step_avg:143.51ms
step:710/1375 train_time:100461ms step_avg:143.52ms
step:711/1375 train_time:100612ms step_avg:143.53ms
step:712/1375 train_time:100758ms step_avg:143.53ms
step:713/1375 train_time:100911ms step_avg:143.54ms
step:714/1375 train_time:101057ms step_avg:143.55ms
step:715/1375 train_time:101209ms step_avg:143.56ms
step:716/1375 train_time:101358ms step_avg:143.57ms
step:717/1375 train_time:101510ms step_avg:143.58ms
step:718/1375 train_time:101656ms step_avg:143.58ms
step:719/1375 train_time:101807ms step_avg:143.59ms
step:720/1375 train_time:101957ms step_avg:143.60ms
step:721/1375 train_time:102109ms step_avg:143.61ms
step:722/1375 train_time:102258ms step_avg:143.62ms
step:723/1375 train_time:102410ms step_avg:143.63ms
step:724/1375 train_time:102558ms step_avg:143.64ms
step:725/1375 train_time:102710ms step_avg:143.65ms
step:726/1375 train_time:102858ms step_avg:143.66ms
step:727/1375 train_time:103012ms step_avg:143.67ms
step:728/1375 train_time:103161ms step_avg:143.68ms
step:729/1375 train_time:103310ms step_avg:143.69ms
step:730/1375 train_time:103459ms step_avg:143.69ms
step:731/1375 train_time:103610ms step_avg:143.70ms
step:732/1375 train_time:103758ms step_avg:143.71ms
step:733/1375 train_time:103908ms step_avg:143.72ms
step:734/1375 train_time:104055ms step_avg:143.72ms
step:735/1375 train_time:104209ms step_avg:143.74ms
step:736/1375 train_time:104358ms step_avg:143.74ms
step:737/1375 train_time:104510ms step_avg:143.76ms
step:738/1375 train_time:104659ms step_avg:143.76ms
step:739/1375 train_time:104812ms step_avg:143.77ms
step:740/1375 train_time:104960ms step_avg:143.78ms
step:741/1375 train_time:105113ms step_avg:143.79ms
step:742/1375 train_time:105262ms step_avg:143.80ms
step:743/1375 train_time:105413ms step_avg:143.81ms
step:744/1375 train_time:105561ms step_avg:143.82ms
step:745/1375 train_time:105715ms step_avg:143.83ms
step:746/1375 train_time:105862ms step_avg:143.83ms
step:747/1375 train_time:106012ms step_avg:143.84ms
step:748/1375 train_time:106162ms step_avg:143.85ms
step:749/1375 train_time:106313ms step_avg:143.86ms
step:750/1375 train_time:106461ms step_avg:143.87ms
step:750/1375 val_loss:3.5192 train_time:106539ms step_avg:143.97ms
step:751/1375 train_time:106615ms step_avg:143.88ms
step:752/1375 train_time:106766ms step_avg:143.89ms
step:753/1375 train_time:106915ms step_avg:143.90ms
step:754/1375 train_time:107064ms step_avg:143.90ms
step:755/1375 train_time:107213ms step_avg:143.91ms
step:756/1375 train_time:107363ms step_avg:143.92ms
step:757/1375 train_time:107515ms step_avg:143.93ms
step:758/1375 train_time:107667ms step_avg:143.94ms
step:759/1375 train_time:107818ms step_avg:143.95ms
step:760/1375 train_time:107966ms step_avg:143.95ms
step:761/1375 train_time:108169ms step_avg:144.03ms
step:762/1375 train_time:108317ms step_avg:144.04ms
step:763/1375 train_time:108465ms step_avg:144.04ms
step:764/1375 train_time:108615ms step_avg:144.05ms
step:765/1375 train_time:108764ms step_avg:144.06ms
step:766/1375 train_time:108915ms step_avg:144.07ms
step:767/1375 train_time:109068ms step_avg:144.08ms
step:768/1375 train_time:109222ms step_avg:144.09ms
step:769/1375 train_time:109372ms step_avg:144.10ms
step:770/1375 train_time:109523ms step_avg:144.11ms
step:771/1375 train_time:109672ms step_avg:144.12ms
step:772/1375 train_time:109823ms step_avg:144.12ms
step:773/1375 train_time:109973ms step_avg:144.13ms
step:774/1375 train_time:110125ms step_avg:144.14ms
step:775/1375 train_time:110278ms step_avg:144.15ms
step:776/1375 train_time:110428ms step_avg:144.16ms
step:777/1375 train_time:110581ms step_avg:144.17ms
step:778/1375 train_time:110728ms step_avg:144.18ms
step:779/1375 train_time:110879ms step_avg:144.19ms
step:780/1375 train_time:111028ms step_avg:144.19ms
step:781/1375 train_time:111180ms step_avg:144.20ms
step:782/1375 train_time:111327ms step_avg:144.21ms
step:783/1375 train_time:111479ms step_avg:144.22ms
step:784/1375 train_time:111629ms step_avg:144.22ms
step:785/1375 train_time:111780ms step_avg:144.23ms
step:786/1375 train_time:111929ms step_avg:144.24ms
step:787/1375 train_time:112082ms step_avg:144.25ms
step:788/1375 train_time:112230ms step_avg:144.25ms
step:789/1375 train_time:112382ms step_avg:144.26ms
step:790/1375 train_time:112528ms step_avg:144.27ms
step:791/1375 train_time:112680ms step_avg:144.28ms
step:792/1375 train_time:112829ms step_avg:144.28ms
step:793/1375 train_time:112980ms step_avg:144.29ms
step:794/1375 train_time:113128ms step_avg:144.30ms
step:795/1375 train_time:113283ms step_avg:144.31ms
step:796/1375 train_time:113432ms step_avg:144.32ms
step:797/1375 train_time:113584ms step_avg:144.32ms
step:798/1375 train_time:113734ms step_avg:144.33ms
step:799/1375 train_time:113888ms step_avg:144.35ms
step:800/1375 train_time:114038ms step_avg:144.35ms
step:801/1375 train_time:114187ms step_avg:144.36ms
step:802/1375 train_time:114338ms step_avg:144.37ms
step:803/1375 train_time:114486ms step_avg:144.37ms
step:804/1375 train_time:114636ms step_avg:144.38ms
step:805/1375 train_time:114788ms step_avg:144.39ms
step:806/1375 train_time:114938ms step_avg:144.39ms
step:807/1375 train_time:115086ms step_avg:144.40ms
step:808/1375 train_time:115237ms step_avg:144.41ms
step:809/1375 train_time:115387ms step_avg:144.41ms
step:810/1375 train_time:115536ms step_avg:144.42ms
step:811/1375 train_time:115688ms step_avg:144.43ms
step:812/1375 train_time:115837ms step_avg:144.44ms
step:813/1375 train_time:115987ms step_avg:144.44ms
step:814/1375 train_time:116137ms step_avg:144.45ms
step:815/1375 train_time:116286ms step_avg:144.45ms
step:816/1375 train_time:116437ms step_avg:144.46ms
step:817/1375 train_time:116588ms step_avg:144.47ms
step:818/1375 train_time:116739ms step_avg:144.48ms
step:819/1375 train_time:116892ms step_avg:144.49ms
step:820/1375 train_time:117046ms step_avg:144.50ms
step:821/1375 train_time:117197ms step_avg:144.51ms
step:822/1375 train_time:117346ms step_avg:144.52ms
step:823/1375 train_time:117499ms step_avg:144.53ms
step:824/1375 train_time:117649ms step_avg:144.53ms
step:825/1375 train_time:117804ms step_avg:144.54ms
step:826/1375 train_time:117954ms step_avg:144.55ms
step:827/1375 train_time:118105ms step_avg:144.56ms
step:828/1375 train_time:118257ms step_avg:144.57ms
step:829/1375 train_time:118407ms step_avg:144.58ms
step:830/1375 train_time:118558ms step_avg:144.58ms
step:831/1375 train_time:118708ms step_avg:144.59ms
step:832/1375 train_time:118861ms step_avg:144.60ms
step:833/1375 train_time:119009ms step_avg:144.60ms
step:834/1375 train_time:119163ms step_avg:144.61ms
step:835/1375 train_time:119315ms step_avg:144.62ms
step:836/1375 train_time:119469ms step_avg:144.64ms
step:837/1375 train_time:119621ms step_avg:144.64ms
step:838/1375 train_time:119771ms step_avg:144.65ms
step:839/1375 train_time:119924ms step_avg:144.66ms
step:840/1375 train_time:120076ms step_avg:144.67ms
step:841/1375 train_time:120227ms step_avg:144.68ms
step:842/1375 train_time:120381ms step_avg:144.69ms
step:843/1375 train_time:120528ms step_avg:144.69ms
step:844/1375 train_time:120681ms step_avg:144.70ms
step:845/1375 train_time:120829ms step_avg:144.71ms
step:846/1375 train_time:120982ms step_avg:144.72ms
step:847/1375 train_time:121132ms step_avg:144.72ms
step:848/1375 train_time:121283ms step_avg:144.73ms
step:849/1375 train_time:121434ms step_avg:144.74ms
step:850/1375 train_time:121587ms step_avg:144.75ms
step:851/1375 train_time:121741ms step_avg:144.76ms
step:852/1375 train_time:121893ms step_avg:144.77ms
step:853/1375 train_time:122043ms step_avg:144.77ms
step:854/1375 train_time:122196ms step_avg:144.78ms
step:855/1375 train_time:122346ms step_avg:144.79ms
step:856/1375 train_time:122498ms step_avg:144.80ms
step:857/1375 train_time:122648ms step_avg:144.80ms
step:858/1375 train_time:122805ms step_avg:144.82ms
step:859/1375 train_time:122958ms step_avg:144.83ms
step:860/1375 train_time:123108ms step_avg:144.83ms
step:861/1375 train_time:123261ms step_avg:144.84ms
step:862/1375 train_time:123412ms step_avg:144.85ms
step:863/1375 train_time:123565ms step_avg:144.86ms
step:864/1375 train_time:123715ms step_avg:144.87ms
step:865/1375 train_time:123866ms step_avg:144.87ms
step:866/1375 train_time:124022ms step_avg:144.89ms
step:867/1375 train_time:124173ms step_avg:144.89ms
step:868/1375 train_time:124324ms step_avg:144.90ms
step:869/1375 train_time:124473ms step_avg:144.91ms
step:870/1375 train_time:124627ms step_avg:144.92ms
step:871/1375 train_time:124778ms step_avg:144.92ms
step:872/1375 train_time:124927ms step_avg:144.93ms
step:873/1375 train_time:125079ms step_avg:144.93ms
step:874/1375 train_time:125229ms step_avg:144.94ms
step:875/1375 train_time:125381ms step_avg:144.95ms
step:875/1375 val_loss:3.4690 train_time:125455ms step_avg:145.03ms
step:876/1375 train_time:125533ms step_avg:144.96ms
step:877/1375 train_time:125685ms step_avg:144.97ms
step:878/1375 train_time:125836ms step_avg:144.97ms
step:879/1375 train_time:125987ms step_avg:144.98ms
step:880/1375 train_time:126136ms step_avg:144.98ms
step:881/1375 train_time:126287ms step_avg:144.99ms
step:882/1375 train_time:126443ms step_avg:145.00ms
step:883/1375 train_time:126594ms step_avg:145.01ms
step:884/1375 train_time:126748ms step_avg:145.02ms
step:885/1375 train_time:126897ms step_avg:145.03ms
step:886/1375 train_time:127051ms step_avg:145.04ms
step:887/1375 train_time:127200ms step_avg:145.04ms
step:888/1375 train_time:127353ms step_avg:145.05ms
step:889/1375 train_time:127507ms step_avg:145.06ms
step:890/1375 train_time:127656ms step_avg:145.06ms
step:891/1375 train_time:127808ms step_avg:145.07ms
step:892/1375 train_time:127961ms step_avg:145.08ms
step:893/1375 train_time:128111ms step_avg:145.09ms
step:894/1375 train_time:128264ms step_avg:145.09ms
step:895/1375 train_time:128417ms step_avg:145.10ms
step:896/1375 train_time:128569ms step_avg:145.11ms
step:897/1375 train_time:128721ms step_avg:145.12ms
step:898/1375 train_time:128873ms step_avg:145.13ms
step:899/1375 train_time:129025ms step_avg:145.13ms
step:900/1375 train_time:129173ms step_avg:145.14ms
step:901/1375 train_time:129327ms step_avg:145.15ms
step:902/1375 train_time:129474ms step_avg:145.15ms
step:903/1375 train_time:129628ms step_avg:145.16ms
step:904/1375 train_time:129778ms step_avg:145.17ms
step:905/1375 train_time:129930ms step_avg:145.17ms
step:906/1375 train_time:130081ms step_avg:145.18ms
step:907/1375 train_time:130237ms step_avg:145.19ms
step:908/1375 train_time:130388ms step_avg:145.20ms
step:909/1375 train_time:130539ms step_avg:145.21ms
step:910/1375 train_time:130695ms step_avg:145.22ms
step:911/1375 train_time:130847ms step_avg:145.22ms
step:912/1375 train_time:130995ms step_avg:145.23ms
step:913/1375 train_time:131150ms step_avg:145.24ms
step:914/1375 train_time:131301ms step_avg:145.24ms
step:915/1375 train_time:131453ms step_avg:145.25ms
step:916/1375 train_time:131605ms step_avg:145.26ms
step:917/1375 train_time:131755ms step_avg:145.26ms
step:918/1375 train_time:131907ms step_avg:145.27ms
step:919/1375 train_time:132063ms step_avg:145.28ms
step:920/1375 train_time:132215ms step_avg:145.29ms
step:921/1375 train_time:132369ms step_avg:145.30ms
step:922/1375 train_time:132526ms step_avg:145.31ms
step:923/1375 train_time:132675ms step_avg:145.32ms
step:924/1375 train_time:132828ms step_avg:145.33ms
step:925/1375 train_time:132982ms step_avg:145.34ms
step:926/1375 train_time:133134ms step_avg:145.34ms
step:927/1375 train_time:133288ms step_avg:145.35ms
step:928/1375 train_time:133438ms step_avg:145.36ms
step:929/1375 train_time:133592ms step_avg:145.37ms
step:930/1375 train_time:133748ms step_avg:145.38ms
step:931/1375 train_time:133898ms step_avg:145.38ms
step:932/1375 train_time:134050ms step_avg:145.39ms
step:933/1375 train_time:134203ms step_avg:145.40ms
step:934/1375 train_time:134354ms step_avg:145.40ms
step:935/1375 train_time:134507ms step_avg:145.41ms
step:936/1375 train_time:134661ms step_avg:145.42ms
step:937/1375 train_time:134817ms step_avg:145.43ms
step:938/1375 train_time:134969ms step_avg:145.44ms
step:939/1375 train_time:135123ms step_avg:145.45ms
step:940/1375 train_time:135276ms step_avg:145.46ms
step:941/1375 train_time:135428ms step_avg:145.46ms
step:942/1375 train_time:135579ms step_avg:145.47ms
step:943/1375 train_time:135734ms step_avg:145.48ms
step:944/1375 train_time:135892ms step_avg:145.49ms
step:945/1375 train_time:136048ms step_avg:145.51ms
step:946/1375 train_time:136201ms step_avg:145.51ms
step:947/1375 train_time:136354ms step_avg:145.52ms
step:948/1375 train_time:136507ms step_avg:145.53ms
step:949/1375 train_time:136663ms step_avg:145.54ms
step:950/1375 train_time:136814ms step_avg:145.55ms
step:951/1375 train_time:137014ms step_avg:145.60ms
step:952/1375 train_time:137165ms step_avg:145.61ms
step:953/1375 train_time:137317ms step_avg:145.62ms
step:954/1375 train_time:137470ms step_avg:145.62ms
step:955/1375 train_time:137620ms step_avg:145.63ms
step:956/1375 train_time:137773ms step_avg:145.64ms
step:957/1375 train_time:137927ms step_avg:145.65ms
step:958/1375 train_time:138086ms step_avg:145.66ms
step:959/1375 train_time:138241ms step_avg:145.67ms
step:960/1375 train_time:138395ms step_avg:145.68ms
step:961/1375 train_time:138548ms step_avg:145.69ms
step:962/1375 train_time:138698ms step_avg:145.69ms
step:963/1375 train_time:138857ms step_avg:145.70ms
step:964/1375 train_time:139010ms step_avg:145.71ms
step:965/1375 train_time:139162ms step_avg:145.72ms
step:966/1375 train_time:139313ms step_avg:145.72ms
step:967/1375 train_time:139466ms step_avg:145.73ms
step:968/1375 train_time:139615ms step_avg:145.74ms
step:969/1375 train_time:139770ms step_avg:145.75ms
step:970/1375 train_time:139921ms step_avg:145.75ms
step:971/1375 train_time:140075ms step_avg:145.76ms
step:972/1375 train_time:140227ms step_avg:145.77ms
step:973/1375 train_time:140377ms step_avg:145.77ms
step:974/1375 train_time:140530ms step_avg:145.78ms
step:975/1375 train_time:140683ms step_avg:145.79ms
step:976/1375 train_time:140836ms step_avg:145.79ms
step:977/1375 train_time:140989ms step_avg:145.80ms
step:978/1375 train_time:141143ms step_avg:145.81ms
step:979/1375 train_time:141293ms step_avg:145.81ms
step:980/1375 train_time:141447ms step_avg:145.82ms
step:981/1375 train_time:141596ms step_avg:145.83ms
step:982/1375 train_time:141749ms step_avg:145.83ms
step:983/1375 train_time:141900ms step_avg:145.84ms
step:984/1375 train_time:142053ms step_avg:145.84ms
step:985/1375 train_time:142207ms step_avg:145.85ms
step:986/1375 train_time:142362ms step_avg:145.86ms
step:987/1375 train_time:142512ms step_avg:145.87ms
step:988/1375 train_time:142666ms step_avg:145.88ms
step:989/1375 train_time:142816ms step_avg:145.88ms
step:990/1375 train_time:142970ms step_avg:145.89ms
step:991/1375 train_time:143122ms step_avg:145.89ms
step:992/1375 train_time:143278ms step_avg:145.90ms
step:993/1375 train_time:143438ms step_avg:145.92ms
step:994/1375 train_time:143590ms step_avg:145.92ms
step:995/1375 train_time:143740ms step_avg:145.93ms
step:996/1375 train_time:143890ms step_avg:145.93ms
step:997/1375 train_time:144043ms step_avg:145.94ms
step:998/1375 train_time:144195ms step_avg:145.95ms
step:999/1375 train_time:144349ms step_avg:145.95ms
step:1000/1375 train_time:144498ms step_avg:145.96ms
step:1000/1375 val_loss:3.4028 train_time:144575ms step_avg:146.04ms
step:1001/1375 train_time:144651ms step_avg:145.96ms
step:1002/1375 train_time:144809ms step_avg:145.98ms
step:1003/1375 train_time:144962ms step_avg:145.98ms
step:1004/1375 train_time:145118ms step_avg:145.99ms
step:1005/1375 train_time:145268ms step_avg:146.00ms
step:1006/1375 train_time:145419ms step_avg:146.00ms
step:1007/1375 train_time:145575ms step_avg:146.01ms
step:1008/1375 train_time:145726ms step_avg:146.02ms
step:1009/1375 train_time:145885ms step_avg:146.03ms
step:1010/1375 train_time:146039ms step_avg:146.04ms
step:1011/1375 train_time:146190ms step_avg:146.04ms
step:1012/1375 train_time:146342ms step_avg:146.05ms
step:1013/1375 train_time:146496ms step_avg:146.06ms
step:1014/1375 train_time:146646ms step_avg:146.06ms
step:1015/1375 train_time:146801ms step_avg:146.07ms
step:1016/1375 train_time:146954ms step_avg:146.08ms
step:1017/1375 train_time:147107ms step_avg:146.08ms
step:1018/1375 train_time:147260ms step_avg:146.09ms
step:1019/1375 train_time:147414ms step_avg:146.10ms
step:1020/1375 train_time:147569ms step_avg:146.11ms
step:1021/1375 train_time:147723ms step_avg:146.12ms
step:1022/1375 train_time:147878ms step_avg:146.12ms
step:1023/1375 train_time:148031ms step_avg:146.13ms
step:1024/1375 train_time:148186ms step_avg:146.14ms
step:1025/1375 train_time:148342ms step_avg:146.15ms
step:1026/1375 train_time:148493ms step_avg:146.15ms
step:1027/1375 train_time:148645ms step_avg:146.16ms
step:1028/1375 train_time:148801ms step_avg:146.17ms
step:1029/1375 train_time:148957ms step_avg:146.18ms
step:1030/1375 train_time:149112ms step_avg:146.19ms
step:1031/1375 train_time:149262ms step_avg:146.19ms
step:1032/1375 train_time:149416ms step_avg:146.20ms
step:1033/1375 train_time:149568ms step_avg:146.20ms
step:1034/1375 train_time:149722ms step_avg:146.21ms
step:1035/1375 train_time:149877ms step_avg:146.22ms
step:1036/1375 train_time:150030ms step_avg:146.23ms
step:1037/1375 train_time:150189ms step_avg:146.24ms
step:1038/1375 train_time:150343ms step_avg:146.25ms
step:1039/1375 train_time:150497ms step_avg:146.26ms
step:1040/1375 train_time:150650ms step_avg:146.26ms
step:1041/1375 train_time:150804ms step_avg:146.27ms
step:1042/1375 train_time:150955ms step_avg:146.27ms
step:1043/1375 train_time:151106ms step_avg:146.28ms
step:1044/1375 train_time:151262ms step_avg:146.29ms
step:1045/1375 train_time:151418ms step_avg:146.30ms
step:1046/1375 train_time:151569ms step_avg:146.30ms
step:1047/1375 train_time:151722ms step_avg:146.31ms
step:1048/1375 train_time:151877ms step_avg:146.32ms
step:1049/1375 train_time:152032ms step_avg:146.33ms
step:1050/1375 train_time:152188ms step_avg:146.33ms
step:1051/1375 train_time:152345ms step_avg:146.34ms
step:1052/1375 train_time:152500ms step_avg:146.35ms
step:1053/1375 train_time:152650ms step_avg:146.36ms
step:1054/1375 train_time:152805ms step_avg:146.36ms
step:1055/1375 train_time:152957ms step_avg:146.37ms
step:1056/1375 train_time:153111ms step_avg:146.38ms
step:1057/1375 train_time:153265ms step_avg:146.38ms
step:1058/1375 train_time:153423ms step_avg:146.40ms
step:1059/1375 train_time:153579ms step_avg:146.40ms
step:1060/1375 train_time:153732ms step_avg:146.41ms
step:1061/1375 train_time:153884ms step_avg:146.42ms
step:1062/1375 train_time:154041ms step_avg:146.43ms
step:1063/1375 train_time:154197ms step_avg:146.44ms
step:1064/1375 train_time:154347ms step_avg:146.44ms
step:1065/1375 train_time:154503ms step_avg:146.45ms
step:1066/1375 train_time:154660ms step_avg:146.46ms
step:1067/1375 train_time:154816ms step_avg:146.47ms
step:1068/1375 train_time:154967ms step_avg:146.47ms
step:1069/1375 train_time:155127ms step_avg:146.48ms
step:1070/1375 train_time:155280ms step_avg:146.49ms
step:1071/1375 train_time:155433ms step_avg:146.50ms
step:1072/1375 train_time:155585ms step_avg:146.50ms
step:1073/1375 train_time:155736ms step_avg:146.51ms
step:1074/1375 train_time:155888ms step_avg:146.51ms
step:1075/1375 train_time:156043ms step_avg:146.52ms
step:1076/1375 train_time:156195ms step_avg:146.52ms
step:1077/1375 train_time:156348ms step_avg:146.53ms
step:1078/1375 train_time:156506ms step_avg:146.54ms
step:1079/1375 train_time:156662ms step_avg:146.55ms
step:1080/1375 train_time:156819ms step_avg:146.56ms
step:1081/1375 train_time:156971ms step_avg:146.56ms
step:1082/1375 train_time:157123ms step_avg:146.57ms
step:1083/1375 train_time:157276ms step_avg:146.58ms
step:1084/1375 train_time:157434ms step_avg:146.59ms
step:1085/1375 train_time:157586ms step_avg:146.59ms
step:1086/1375 train_time:157742ms step_avg:146.60ms
step:1087/1375 train_time:157898ms step_avg:146.61ms
step:1088/1375 train_time:158050ms step_avg:146.61ms
step:1089/1375 train_time:158208ms step_avg:146.62ms
step:1090/1375 train_time:158365ms step_avg:146.63ms
step:1091/1375 train_time:158522ms step_avg:146.64ms
step:1092/1375 train_time:158674ms step_avg:146.65ms
step:1093/1375 train_time:158830ms step_avg:146.66ms
step:1094/1375 train_time:158982ms step_avg:146.66ms
step:1095/1375 train_time:159136ms step_avg:146.67ms
step:1096/1375 train_time:159293ms step_avg:146.68ms
step:1097/1375 train_time:159448ms step_avg:146.69ms
step:1098/1375 train_time:159602ms step_avg:146.69ms
step:1099/1375 train_time:159755ms step_avg:146.70ms
step:1100/1375 train_time:159906ms step_avg:146.70ms
step:1101/1375 train_time:160058ms step_avg:146.71ms
step:1102/1375 train_time:160214ms step_avg:146.72ms
step:1103/1375 train_time:160368ms step_avg:146.72ms
step:1104/1375 train_time:160523ms step_avg:146.73ms
step:1105/1375 train_time:160679ms step_avg:146.74ms
step:1106/1375 train_time:160830ms step_avg:146.74ms
step:1107/1375 train_time:160985ms step_avg:146.75ms
step:1108/1375 train_time:161141ms step_avg:146.76ms
step:1109/1375 train_time:161293ms step_avg:146.76ms
step:1110/1375 train_time:161446ms step_avg:146.77ms
step:1111/1375 train_time:161603ms step_avg:146.78ms
step:1112/1375 train_time:161757ms step_avg:146.79ms
step:1113/1375 train_time:161910ms step_avg:146.79ms
step:1114/1375 train_time:162068ms step_avg:146.80ms
step:1115/1375 train_time:162225ms step_avg:146.81ms
step:1116/1375 train_time:162376ms step_avg:146.81ms
step:1117/1375 train_time:162531ms step_avg:146.82ms
step:1118/1375 train_time:162690ms step_avg:146.83ms
step:1119/1375 train_time:162845ms step_avg:146.84ms
step:1120/1375 train_time:163000ms step_avg:146.85ms
step:1121/1375 train_time:163153ms step_avg:146.85ms
step:1122/1375 train_time:163306ms step_avg:146.86ms
step:1123/1375 train_time:163459ms step_avg:146.86ms
step:1124/1375 train_time:163618ms step_avg:146.87ms
step:1125/1375 train_time:163772ms step_avg:146.88ms
step:1125/1375 val_loss:3.3490 train_time:163851ms step_avg:146.95ms
step:1126/1375 train_time:163928ms step_avg:146.89ms
step:1127/1375 train_time:164083ms step_avg:146.90ms
step:1128/1375 train_time:164238ms step_avg:146.90ms
step:1129/1375 train_time:164397ms step_avg:146.91ms
step:1130/1375 train_time:164550ms step_avg:146.92ms
step:1131/1375 train_time:164708ms step_avg:146.93ms
step:1132/1375 train_time:164861ms step_avg:146.94ms
step:1133/1375 train_time:165015ms step_avg:146.94ms
step:1134/1375 train_time:165172ms step_avg:146.95ms
step:1135/1375 train_time:165326ms step_avg:146.96ms
step:1136/1375 train_time:165484ms step_avg:146.97ms
step:1137/1375 train_time:165635ms step_avg:146.97ms
step:1138/1375 train_time:165791ms step_avg:146.98ms
step:1139/1375 train_time:165946ms step_avg:146.99ms
step:1140/1375 train_time:166099ms step_avg:146.99ms
step:1141/1375 train_time:166299ms step_avg:147.04ms
step:1142/1375 train_time:166452ms step_avg:147.04ms
step:1143/1375 train_time:166610ms step_avg:147.05ms
step:1144/1375 train_time:166767ms step_avg:147.06ms
step:1145/1375 train_time:166917ms step_avg:147.06ms
step:1146/1375 train_time:167073ms step_avg:147.07ms
step:1147/1375 train_time:167229ms step_avg:147.08ms
step:1148/1375 train_time:167385ms step_avg:147.09ms
step:1149/1375 train_time:167536ms step_avg:147.09ms
step:1150/1375 train_time:167688ms step_avg:147.10ms
step:1151/1375 train_time:167844ms step_avg:147.10ms
step:1152/1375 train_time:167999ms step_avg:147.11ms
step:1153/1375 train_time:168156ms step_avg:147.12ms
step:1154/1375 train_time:168310ms step_avg:147.12ms
step:1155/1375 train_time:168468ms step_avg:147.13ms
step:1156/1375 train_time:168627ms step_avg:147.14ms
step:1157/1375 train_time:168784ms step_avg:147.15ms
step:1158/1375 train_time:168936ms step_avg:147.16ms
step:1159/1375 train_time:169091ms step_avg:147.16ms
step:1160/1375 train_time:169245ms step_avg:147.17ms
step:1161/1375 train_time:169399ms step_avg:147.18ms
step:1162/1375 train_time:169555ms step_avg:147.18ms
step:1163/1375 train_time:169710ms step_avg:147.19ms
step:1164/1375 train_time:169864ms step_avg:147.20ms
step:1165/1375 train_time:170015ms step_avg:147.20ms
step:1166/1375 train_time:170171ms step_avg:147.21ms
step:1167/1375 train_time:170325ms step_avg:147.21ms
step:1168/1375 train_time:170478ms step_avg:147.22ms
step:1169/1375 train_time:170633ms step_avg:147.22ms
step:1170/1375 train_time:170790ms step_avg:147.23ms
step:1171/1375 train_time:170949ms step_avg:147.24ms
step:1172/1375 train_time:171103ms step_avg:147.25ms
step:1173/1375 train_time:171256ms step_avg:147.25ms
step:1174/1375 train_time:171417ms step_avg:147.27ms
step:1175/1375 train_time:171578ms step_avg:147.28ms
step:1176/1375 train_time:171736ms step_avg:147.29ms
step:1177/1375 train_time:171899ms step_avg:147.30ms
step:1178/1375 train_time:172055ms step_avg:147.31ms
step:1179/1375 train_time:172209ms step_avg:147.31ms
step:1180/1375 train_time:172373ms step_avg:147.33ms
step:1181/1375 train_time:172528ms step_avg:147.33ms
step:1182/1375 train_time:172680ms step_avg:147.34ms
step:1183/1375 train_time:172835ms step_avg:147.34ms
step:1184/1375 train_time:172989ms step_avg:147.35ms
step:1185/1375 train_time:173148ms step_avg:147.36ms
step:1186/1375 train_time:173302ms step_avg:147.37ms
step:1187/1375 train_time:173467ms step_avg:147.38ms
step:1188/1375 train_time:173618ms step_avg:147.38ms
step:1189/1375 train_time:173775ms step_avg:147.39ms
step:1190/1375 train_time:173931ms step_avg:147.40ms
step:1191/1375 train_time:174088ms step_avg:147.41ms
step:1192/1375 train_time:174241ms step_avg:147.41ms
step:1193/1375 train_time:174397ms step_avg:147.42ms
step:1194/1375 train_time:174553ms step_avg:147.43ms
step:1195/1375 train_time:174707ms step_avg:147.43ms
step:1196/1375 train_time:174863ms step_avg:147.44ms
step:1197/1375 train_time:175018ms step_avg:147.45ms
step:1198/1375 train_time:175178ms step_avg:147.46ms
step:1199/1375 train_time:175332ms step_avg:147.46ms
step:1200/1375 train_time:175486ms step_avg:147.47ms
step:1201/1375 train_time:175640ms step_avg:147.47ms
step:1202/1375 train_time:175808ms step_avg:147.49ms
step:1203/1375 train_time:175967ms step_avg:147.50ms
step:1204/1375 train_time:176122ms step_avg:147.51ms
step:1205/1375 train_time:176275ms step_avg:147.51ms
step:1206/1375 train_time:176430ms step_avg:147.52ms
step:1207/1375 train_time:176586ms step_avg:147.52ms
step:1208/1375 train_time:176743ms step_avg:147.53ms
step:1209/1375 train_time:176896ms step_avg:147.54ms
step:1210/1375 train_time:177057ms step_avg:147.55ms
step:1211/1375 train_time:177211ms step_avg:147.55ms
step:1212/1375 train_time:177368ms step_avg:147.56ms
step:1213/1375 train_time:177521ms step_avg:147.57ms
step:1214/1375 train_time:177678ms step_avg:147.57ms
step:1215/1375 train_time:177834ms step_avg:147.58ms
step:1216/1375 train_time:177987ms step_avg:147.58ms
step:1217/1375 train_time:178141ms step_avg:147.59ms
step:1218/1375 train_time:178293ms step_avg:147.59ms
step:1219/1375 train_time:178447ms step_avg:147.60ms
step:1220/1375 train_time:178600ms step_avg:147.60ms
step:1221/1375 train_time:178753ms step_avg:147.61ms
step:1222/1375 train_time:178908ms step_avg:147.61ms
step:1223/1375 train_time:179065ms step_avg:147.62ms
step:1224/1375 train_time:179223ms step_avg:147.63ms
step:1225/1375 train_time:179378ms step_avg:147.64ms
step:1226/1375 train_time:179534ms step_avg:147.64ms
step:1227/1375 train_time:179692ms step_avg:147.65ms
step:1228/1375 train_time:179847ms step_avg:147.66ms
step:1229/1375 train_time:180001ms step_avg:147.66ms
step:1230/1375 train_time:180164ms step_avg:147.68ms
step:1231/1375 train_time:180321ms step_avg:147.68ms
step:1232/1375 train_time:180481ms step_avg:147.69ms
step:1233/1375 train_time:180636ms step_avg:147.70ms
step:1234/1375 train_time:180790ms step_avg:147.70ms
step:1235/1375 train_time:180948ms step_avg:147.71ms
step:1236/1375 train_time:181102ms step_avg:147.72ms
step:1237/1375 train_time:181258ms step_avg:147.72ms
step:1238/1375 train_time:181422ms step_avg:147.74ms
step:1239/1375 train_time:181577ms step_avg:147.74ms
step:1240/1375 train_time:181734ms step_avg:147.75ms
step:1241/1375 train_time:181894ms step_avg:147.76ms
step:1242/1375 train_time:182051ms step_avg:147.77ms
step:1243/1375 train_time:182210ms step_avg:147.78ms
step:1244/1375 train_time:182366ms step_avg:147.78ms
step:1245/1375 train_time:182521ms step_avg:147.79ms
step:1246/1375 train_time:182675ms step_avg:147.79ms
step:1247/1375 train_time:182831ms step_avg:147.80ms
step:1248/1375 train_time:182987ms step_avg:147.81ms
step:1249/1375 train_time:183141ms step_avg:147.81ms
step:1250/1375 train_time:183296ms step_avg:147.82ms
step:1250/1375 val_loss:3.3037 train_time:183376ms step_avg:147.88ms
step:1251/1375 train_time:183455ms step_avg:147.83ms
step:1252/1375 train_time:183609ms step_avg:147.83ms
step:1253/1375 train_time:183764ms step_avg:147.84ms
step:1254/1375 train_time:183917ms step_avg:147.84ms
step:1255/1375 train_time:184084ms step_avg:147.86ms
step:1256/1375 train_time:184237ms step_avg:147.86ms
step:1257/1375 train_time:184393ms step_avg:147.87ms
step:1258/1375 train_time:184550ms step_avg:147.88ms
step:1259/1375 train_time:184709ms step_avg:147.89ms
step:1260/1375 train_time:184863ms step_avg:147.89ms
step:1261/1375 train_time:185020ms step_avg:147.90ms
step:1262/1375 train_time:185177ms step_avg:147.90ms
step:1263/1375 train_time:185332ms step_avg:147.91ms
step:1264/1375 train_time:185486ms step_avg:147.92ms
step:1265/1375 train_time:185640ms step_avg:147.92ms
step:1266/1375 train_time:185796ms step_avg:147.93ms
step:1267/1375 train_time:185953ms step_avg:147.93ms
step:1268/1375 train_time:186111ms step_avg:147.94ms
step:1269/1375 train_time:186272ms step_avg:147.95ms
step:1270/1375 train_time:186428ms step_avg:147.96ms
step:1271/1375 train_time:186586ms step_avg:147.97ms
step:1272/1375 train_time:186740ms step_avg:147.97ms
step:1273/1375 train_time:186893ms step_avg:147.98ms
step:1274/1375 train_time:187047ms step_avg:147.98ms
step:1275/1375 train_time:187203ms step_avg:147.99ms
step:1276/1375 train_time:187354ms step_avg:147.99ms
step:1277/1375 train_time:187510ms step_avg:148.00ms
step:1278/1375 train_time:187665ms step_avg:148.00ms
step:1279/1375 train_time:187819ms step_avg:148.01ms
step:1280/1375 train_time:187982ms step_avg:148.02ms
step:1281/1375 train_time:188139ms step_avg:148.02ms
step:1282/1375 train_time:188290ms step_avg:148.03ms
step:1283/1375 train_time:188447ms step_avg:148.03ms
step:1284/1375 train_time:188608ms step_avg:148.04ms
step:1285/1375 train_time:188762ms step_avg:148.05ms
step:1286/1375 train_time:188916ms step_avg:148.05ms
step:1287/1375 train_time:189070ms step_avg:148.06ms
step:1288/1375 train_time:189226ms step_avg:148.06ms
step:1289/1375 train_time:189387ms step_avg:148.07ms
step:1290/1375 train_time:189546ms step_avg:148.08ms
step:1291/1375 train_time:189705ms step_avg:148.09ms
step:1292/1375 train_time:189861ms step_avg:148.10ms
step:1293/1375 train_time:190018ms step_avg:148.10ms
step:1294/1375 train_time:190172ms step_avg:148.11ms
step:1295/1375 train_time:190328ms step_avg:148.12ms
step:1296/1375 train_time:190487ms step_avg:148.12ms
step:1297/1375 train_time:190645ms step_avg:148.13ms
step:1298/1375 train_time:190803ms step_avg:148.14ms
step:1299/1375 train_time:190955ms step_avg:148.14ms
step:1300/1375 train_time:191109ms step_avg:148.15ms
step:1301/1375 train_time:191264ms step_avg:148.15ms
step:1302/1375 train_time:191420ms step_avg:148.16ms
step:1303/1375 train_time:191576ms step_avg:148.16ms
step:1304/1375 train_time:191734ms step_avg:148.17ms
step:1305/1375 train_time:191889ms step_avg:148.18ms
step:1306/1375 train_time:192048ms step_avg:148.18ms
step:1307/1375 train_time:192202ms step_avg:148.19ms
step:1308/1375 train_time:192357ms step_avg:148.20ms
step:1309/1375 train_time:192512ms step_avg:148.20ms
step:1310/1375 train_time:192667ms step_avg:148.21ms
step:1311/1375 train_time:192824ms step_avg:148.21ms
step:1312/1375 train_time:192976ms step_avg:148.21ms
step:1313/1375 train_time:193128ms step_avg:148.22ms
step:1314/1375 train_time:193285ms step_avg:148.22ms
step:1315/1375 train_time:193441ms step_avg:148.23ms
step:1316/1375 train_time:193594ms step_avg:148.23ms
step:1317/1375 train_time:193747ms step_avg:148.24ms
step:1318/1375 train_time:193907ms step_avg:148.25ms
step:1319/1375 train_time:194062ms step_avg:148.25ms
step:1320/1375 train_time:194216ms step_avg:148.26ms
step:1321/1375 train_time:194375ms step_avg:148.26ms
step:1322/1375 train_time:194533ms step_avg:148.27ms
step:1323/1375 train_time:194688ms step_avg:148.28ms
step:1324/1375 train_time:194842ms step_avg:148.28ms
step:1325/1375 train_time:194998ms step_avg:148.29ms
step:1326/1375 train_time:195158ms step_avg:148.30ms
step:1327/1375 train_time:195313ms step_avg:148.30ms
step:1328/1375 train_time:195468ms step_avg:148.31ms
step:1329/1375 train_time:195644ms step_avg:148.33ms
step:1330/1375 train_time:195804ms step_avg:148.34ms
step:1331/1375 train_time:196016ms step_avg:148.38ms
step:1332/1375 train_time:196179ms step_avg:148.40ms
step:1333/1375 train_time:196335ms step_avg:148.40ms
step:1334/1375 train_time:196489ms step_avg:148.41ms
step:1335/1375 train_time:196641ms step_avg:148.41ms
step:1336/1375 train_time:196806ms step_avg:148.42ms
step:1337/1375 train_time:196965ms step_avg:148.43ms
step:1338/1375 train_time:197121ms step_avg:148.43ms
step:1339/1375 train_time:197281ms step_avg:148.44ms
step:1340/1375 train_time:197437ms step_avg:148.45ms
step:1341/1375 train_time:197590ms step_avg:148.45ms
step:1342/1375 train_time:197750ms step_avg:148.46ms
step:1343/1375 train_time:197906ms step_avg:148.47ms
step:1344/1375 train_time:198062ms step_avg:148.47ms
step:1345/1375 train_time:198218ms step_avg:148.48ms
step:1346/1375 train_time:198375ms step_avg:148.48ms
step:1347/1375 train_time:198532ms step_avg:148.49ms
step:1348/1375 train_time:198687ms step_avg:148.50ms
step:1349/1375 train_time:198844ms step_avg:148.50ms
step:1350/1375 train_time:198999ms step_avg:148.51ms
step:1351/1375 train_time:199153ms step_avg:148.51ms
step:1352/1375 train_time:199319ms step_avg:148.52ms
step:1353/1375 train_time:199478ms step_avg:148.53ms
step:1354/1375 train_time:199634ms step_avg:148.54ms
step:1355/1375 train_time:199790ms step_avg:148.54ms
step:1356/1375 train_time:199945ms step_avg:148.55ms
step:1357/1375 train_time:200103ms step_avg:148.55ms
step:1358/1375 train_time:200260ms step_avg:148.56ms
step:1359/1375 train_time:200415ms step_avg:148.57ms
step:1360/1375 train_time:200576ms step_avg:148.57ms
step:1361/1375 train_time:200734ms step_avg:148.58ms
step:1362/1375 train_time:200891ms step_avg:148.59ms
step:1363/1375 train_time:201055ms step_avg:148.60ms
step:1364/1375 train_time:201210ms step_avg:148.60ms
step:1365/1375 train_time:201365ms step_avg:148.61ms
step:1366/1375 train_time:201525ms step_avg:148.62ms
step:1367/1375 train_time:201681ms step_avg:148.62ms
step:1368/1375 train_time:201837ms step_avg:148.63ms
step:1369/1375 train_time:202000ms step_avg:148.64ms
step:1370/1375 train_time:202160ms step_avg:148.65ms
step:1371/1375 train_time:202313ms step_avg:148.65ms
step:1372/1375 train_time:202478ms step_avg:148.66ms
step:1373/1375 train_time:202633ms step_avg:148.67ms
step:1374/1375 train_time:202792ms step_avg:148.67ms
step:1375/1375 train_time:202948ms step_avg:148.68ms
step:1375/1375 val_loss:3.2782 train_time:203025ms step_avg:148.74ms
peak memory consumption: 31565 MiB

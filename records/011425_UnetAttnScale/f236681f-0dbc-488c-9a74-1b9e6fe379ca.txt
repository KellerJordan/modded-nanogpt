import os
import sys
with open(sys.argv[0]) as f:
    code = f.read() # read the code of this file ASAP, for logging
import uuid
import time
import glob
import subprocess
import contextlib
from dataclasses import dataclass

import torch
torch.empty(1, device='cuda', requires_grad=True).backward()
from torch import nn
import torch.nn.functional as F
import torch.distributed as dist
from torch.nn.parallel import DistributedDataParallel as DDP
# use of FlexAttention contributed by @KoszarskyB
from torch.nn.attention.flex_attention import BlockMask, flex_attention

# -----------------------------------------------------------------------------
# Muon optimizer

@torch.compile
def zeropower_via_newtonschulz5(G, steps):
    """
    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a
    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose
    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at
    zero even beyond the point where the iteration no longer converges all the way to one everywhere
    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T
    where S' is diagonal with S_{ii}' ~ Uniform(0.5, 1.5), which turns out not to hurt model
    performance at all relative to UV^T, where USV^T = G is the SVD.
    """
    assert len(G.shape) == 2
    a, b, c = (3.4445, -4.7750,  2.0315)
    X = G.bfloat16()
    if G.size(0) > G.size(1):
        X = X.T

    # Ensure spectral norm is at most 1
    X = X / (X.norm() + 1e-7)
    # Perform the NS iterations
    for _ in range(steps):
        A = X @ X.T
        B = b * A + c * A @ A # adapted from suggestion by @jxbz, @leloykun, and @YouJiacheng
        X = a * X + B @ X
    
    if G.size(0) > G.size(1):
        X = X.T
    return X

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Some warnings:
    - This optimizer assumes that all parameters passed in are 2D.
    - It should not be used for the embedding layer, the final fully connected layer, or any {0,1}-D
    parameters; those should all be optimized by a standard method (e.g., AdamW).
    - To use it with 4D convolutional filters, it works well to just flatten their last 3 dimensions.
    - We believe it is unlikely to work well for training with small batch size.
    - We believe it may not work well for finetuning pretrained models, but we haven't tested this.
    - We have not yet tried this optimizer for training scenarios larger than NanoGPT (124M).

    Arguments:
        lr: The learning rate used by the internal SGD.
        momentum: The momentum used by the internal SGD.
        nesterov: Whether to use Nesterov-style momentum in the internal SGD. (recommended)
        ns_steps: The number of Newton-Schulz iteration steps to use.
    """
    def __init__(self, params, lr=0.02, momentum=0.95, nesterov=True, ns_steps=5):
        self.world_size = int(os.environ['WORLD_SIZE'])
        self.rank = int(os.environ['RANK'])
        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, ns_steps=ns_steps)
        assert all(isinstance(p, torch.Tensor) for p in params)
        sizes = {p.numel() for p in params}
        param_groups = [dict(params=[p for p in params if p.numel() == size],
                             update_buffer=[torch.empty(size, device='cuda', dtype=torch.bfloat16) for _ in range(self.world_size)])
                        for size in sizes]
        super().__init__(param_groups, defaults)

    def step(self):

        for group in self.param_groups:

            lr = group['lr']
            momentum = group['momentum']
            nesterov = group['nesterov']
            ns_steps = group['ns_steps']
            update_buffers = group['update_buffer']
            # generate weight updates in distributed fashion
            params = group['params']
            handle = None
            params_world = None
            def update_prev():
                if params_world is None:
                    return
                assert handle is not None
                handle.wait()
                for p_world, g_world in zip(params_world, update_buffers):
                    p_world.data.add_(
                        g_world.view_as(p_world),
                        alpha=-lr * max(1, p_world.size(0) / p_world.size(1)) ** 0.5,
                    )
            for base_i in range(len(params))[::self.world_size]:
                if base_i + rank < len(params):
                    p = params[base_i + self.rank]
                    g = p.grad
                    assert g is not None
                    state = self.state[p]
                    if 'momentum_buffer' not in state:
                        state['momentum_buffer'] = torch.zeros_like(g)
                    buf = state['momentum_buffer']
                    buf.lerp_(g, 1 - momentum)
                    g = g.lerp_(buf, momentum) if nesterov else buf
                    g = zeropower_via_newtonschulz5(g, steps=ns_steps).flatten()
                else:
                    g = update_buffers[rank]
                update_prev() # async all_gather instead of sync all_reduce by @YouJiacheng
                handle = dist.all_gather(update_buffers, g, async_op=True)
                params_world = params[base_i : base_i + self.world_size]
            update_prev()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the GPT-2 model

def norm(x):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):

    def __init__(self, in_features, out_features):
        super().__init__(in_features, out_features, bias=False)

    def forward(self, x):
        return F.linear(x, self.weight.type_as(x))

class Rotary(nn.Module):

    def __init__(self, dim, max_seq_len=65536):
        super().__init__()
        # half-truncate RoPE by @YouJiacheng
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=dim//4, dtype=torch.float32)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(dim//4)])
        t = torch.arange(max_seq_len, dtype=torch.float32)
        theta = torch.einsum('i,j -> ij', t, angular_freq)
        self.cos = nn.Buffer(theta.cos(), persistent=False)
        self.sin = nn.Buffer(theta.sin(), persistent=False)

    def forward(self, x):
        cos, sin = self.cos[None, :x.size(-3), None, :], self.sin[None, :x.size(-3), None, :]
        x1, x2 = x.float().chunk(2, dim=-1)
        y1 = x1 * cos + x2 * sin
        y2 = x1 * (-sin) + x2 * cos
        return torch.cat((y1, y2), 3).type_as(x)

class CausalSelfAttention(nn.Module):

    def __init__(self, dim, num_heads, layer_id):
        super().__init__()
        assert dim % num_heads == 0
        self.num_heads = num_heads
        self.layer_id = layer_id
        self.c_q = CastedLinear(dim, dim)
        self.c_k = CastedLinear(dim, dim)
        self.c_v = CastedLinear(dim, dim)
        self.lambdas = nn.Parameter(torch.tensor([0.5, 0.5]))
        self.rotary = Rotary(dim // num_heads) # dim // num_heads = head_dim
        self.c_proj = CastedLinear(dim, dim)
        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977
        # self.attn_scale = nn.Parameter(torch.tensor(1.0 / (dim // num_heads) ** 0.5))
        # self.attn_scale = nn.Parameter(torch.tensor(0.20))
        self.attn_scale = 0.13 + 0.01 * min(layer_id, 11 - layer_id)  # unet pattern attention scale by @leloykun

    def forward(self, x, ve, block_mask):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, 'Must use batch size = 1 for FlexAttention'
        q = self.c_q(x).view(B, T, self.num_heads, -1)
        k = self.c_k(x).view(B, T, self.num_heads, -1)
        v = self.c_v(x).view(B, T, self.num_heads, -1)
        if ve is not None:
            v = self.lambdas[0] * v + self.lambdas[1] * ve.view_as(v) # @KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = self.lambdas[0] * v
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = self.rotary(q), self.rotary(k)
        # y = flex_attention(q.transpose(1, 2) * self.attn_scale, k.transpose(1, 2), v.transpose(1, 2), block_mask=block_mask, scale=1.)
        y = flex_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), block_mask=block_mask, scale=self.attn_scale)
        y = y.transpose(1, 2).contiguous().view_as(x) # re-assemble all head outputs side by side
        y = self.c_proj(y)
        return y

class MLP(nn.Module):

    def __init__(self, dim):
        super().__init__()
        self.c_fc = CastedLinear(dim, 4 * dim)
        self.c_proj = CastedLinear(4 * dim, dim)
        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977

    def forward(self, x):
        x = self.c_fc(x)
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = self.c_proj(x)
        return x

class Block(nn.Module):

    def __init__(self, model_dim, num_heads, layer_id, use_attn=True):
        super().__init__()
        self.attn = CausalSelfAttention(model_dim, num_heads, layer_id) if use_attn else None
        self.mlp = MLP(model_dim)
        self.lambdas = nn.Parameter(torch.tensor([1., 0.]))

    def forward(self, x, ve, x0, block_mask):
        x = self.lambdas[0] * x + self.lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), ve, block_mask)
        x = x + self.mlp(norm(x))
        return x

class ValueEmbedding(nn.Module):
    def __init__(self, vocab_size, model_dim):
        super().__init__()
        self.embed = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])

    def forward(self, inputs):
        ve = [emb(inputs).bfloat16() for emb in self.embed]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        ve = [ve[0], ve[1], ve[2], None, None, None, None, None, None, ve[0], ve[1], ve[2]]
        return ve

# -----------------------------------------------------------------------------
# The main GPT-2 model

class GPT(nn.Module):

    def __init__(self, vocab_size, num_layers, num_heads, model_dim):
        super().__init__()
        self.embed = nn.Embedding(vocab_size, model_dim)
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.blocks = nn.ModuleList([Block(model_dim, num_heads, layer_id=layer_id, use_attn=(layer_id != 7))
                                     for layer_id in range(num_layers)])
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual learning
        # U-net structure on token value embeddings by @leloykun
        self.value_embeds = ValueEmbedding(vocab_size, model_dim)
        self.lm_head = CastedLinear(model_dim, vocab_size)
        self.lm_head.weight.data.zero_() # @Grad62304977
        # U-net design by @brendanh0gan
        self.num_encoder_layers = num_layers // 2 # Half of the layers for encoder
        self.num_decoder_layers = num_layers - self.num_encoder_layers # Remaining for decoder
        # Add learnable skip connection weights for decoder layers
        self.skip_weights = nn.Parameter(torch.ones(self.num_decoder_layers))

    def forward(self, inputs, targets, sliding_window_num_blocks):
        BLOCK_SIZE = 128
        seq_len = len(inputs)
        assert seq_len % BLOCK_SIZE == 0
        total_num_blocks = seq_len // BLOCK_SIZE
        assert inputs.ndim == 1
        docs = (inputs == 50256).cumsum(0)
        docs_low = docs.view(-1, BLOCK_SIZE)[:, 0].contiguous()
        docs_high = docs.view(-1, BLOCK_SIZE)[:, -1].contiguous()

        def document_causal(b, h, q_idx, kv_idx):
            causal_mask = q_idx >= kv_idx
            document_mask = docs[q_idx] == docs[kv_idx]
            return causal_mask & document_mask

        def dense_to_ordered(dense_mask):
            num_blocks = dense_mask.sum(dim=-1, dtype=torch.int32)
            indices = dense_mask.argsort(dim=-1, descending=True, stable=True).to(torch.int32)
            return num_blocks[None, None].contiguous(), indices[None, None].contiguous()

        def create_doc_swc_block_mask(sliding_window_num_blocks):
            kv_idx = block_idx = torch.arange(total_num_blocks, dtype=torch.int32, device='cuda')
            q_idx = block_idx[:, None]
            causal_bm = q_idx >= kv_idx
            causal_full_bm = q_idx > kv_idx
            window_bm = q_idx - kv_idx < sliding_window_num_blocks
            window_full_bm = window_bm # block-wise sliding window by @YouJiacheng
            # document_bm = (docs_low[q_idx] <= docs_high[kv_idx]) & (docs_low[kv_idx] <= docs_high[q_idx])
            document_bm = (docs_low[:, None] <= docs_high) & (docs_low <= docs_high[:, None])
            document_full_bm = (docs_low[:, None] == docs_high) & (docs_low == docs_high[:, None])
            nonzero_bm = causal_bm & window_bm & document_bm
            full_bm  = causal_full_bm & window_full_bm & document_full_bm
            kv_num_blocks, kv_indices = dense_to_ordered(nonzero_bm & ~full_bm)
            full_kv_num_blocks, full_kv_indices = dense_to_ordered(full_bm)
            return BlockMask.from_kv_blocks(
                kv_num_blocks,
                kv_indices,
                full_kv_num_blocks,
                full_kv_indices,
                BLOCK_SIZE=BLOCK_SIZE,
                mask_mod=document_causal,
            )

        block_mask = create_doc_swc_block_mask(sliding_window_num_blocks)

        x0 = norm(self.embed(inputs[None]).bfloat16()) # use of norm here by @Grad62304977
        x = x0
        ve = self.value_embeds(inputs)
        assert len(ve) == len(self.blocks)
        ve_enc, ve_dec = ve[:self.num_encoder_layers], ve[self.num_encoder_layers:]

        # Store outputs for U-Net skip connections
        skip_connections = []
        # Encoder pass - process only the first half of the blocks
        for i in range(self.num_encoder_layers):
            x = self.blocks[i](x, ve_enc[i], x0, block_mask)
            skip_connections.append(x)
        # Decoder pass - process the remaining blocks with weighted skip connections
        for i in range(self.num_decoder_layers):
            x = x + self.skip_weights[i] * skip_connections.pop()
            # U-net structure on token value embeddings by @leloykun
            x = self.blocks[self.num_encoder_layers + i](x, ve_dec[i], x0, block_mask)

        x = norm(x)
        logits = self.lm_head(x)
        logits = 15 * torch.tanh(logits / 15) # @Grad62304977 added tanh softcapping, @KoszarskyB reduced it from 30 to 15
        logits = logits.float()
        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets)
        return loss

# -----------------------------------------------------------------------------
# Our own simple Distributed Data Loader

def _load_data_shard(path):
    # only reads the header, returns header data
    # header is 256 int32
    header = torch.from_file(path, False, 256, dtype=torch.int32)
    assert header[0] == 20240520, 'magic number mismatch in the data .bin file'
    assert header[1] == 1, 'unsupported version'
    num_tokens = int(header[2]) # number of tokens (claimed)
    with open(path, 'rb', buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, 'number of tokens read does not match header'
    return tokens

class DistributedDataLoader:

    def __init__(self, filename_pattern):
        self.rank = int(os.environ['RANK'])
        self.world_size = int(os.environ['WORLD_SIZE'])
        self.files = sorted(glob.glob(filename_pattern))
        self.reset()

    def reset(self):
        self.current_shard = -1
        self.advance()

    def advance(self):
        self.current_shard = (self.current_shard + 1) % len(self.files)
        self.current_position = 0
        self.tokens = _load_data_shard(self.files[self.current_shard])

    def next_batch(self, batch_size):
        assert batch_size % self.world_size == 0
        device_batch_size = batch_size // self.world_size
        # load next shard if necessary
        if self.current_position + batch_size + 1 >= len(self.tokens):
            self.advance()
        pos = self.current_position + self.rank * device_batch_size
        device_batch_tokens = self.tokens[pos:pos+device_batch_size+1]
        # advance current position
        self.current_position += batch_size
        inputs = device_batch_tokens[:-1].to(device='cuda', dtype=torch.int32, non_blocking=True)
        targets = device_batch_tokens[1:].to(device='cuda', dtype=torch.int64, non_blocking=True)
        return inputs, targets

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files = 'data/fineweb10B/fineweb_train_*.bin' # input .bin to train on
    val_files = 'data/fineweb10B/fineweb_val_*.bin' # input .bin to eval validation loss on
    val_tokens = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # optimization
    batch_size = 8*64*1024 # batch size in tokens
    num_iterations = 1375 # number of iterations to run
    cooldown_frac = 0.4 # fraction of training spent cooling down the learning rate
    bf16_embeds = True
    # evaluation and logging
    val_loss_every = 125 # every how many steps to evaluate val loss? 0 for only at the end
    # implementation
    max_device_batch_size = 64*1024 # batch size per device in tokens
    save_checkpoint = False
args = Hyperparameters()

micro_bs = args.max_device_batch_size

# set up DDP (distributed data parallel). torchrun sets this env variable
rank = int(os.environ['RANK'])
local_rank = int(os.environ['LOCAL_RANK'])
world_size = int(os.environ['WORLD_SIZE'])
assert torch.cuda.is_available()
torch.cuda.set_device(local_rank)
dist.init_process_group(backend='nccl', device_id=torch.device(local_rank))
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = uuid.uuid4()
    os.makedirs('logs', exist_ok=True)
    logfile = f'logs/{run_id}.txt'
    print(logfile)

def print0(s, console=False):
    if master_process:
        with open(logfile, 'a') as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0('='*100)
# log information about the hardware/software environment this is running on
print0(f'Running Python {sys.version}')
print0(f'Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}')
print0(subprocess.run(['nvidia-smi'], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout)
print0('='*100)

# load data
train_loader = DistributedDataLoader(args.train_files)
val_loader = DistributedDataLoader(args.val_files)
print0(f'Training dataloader files: {train_loader.files}')
print0(f'Validation dataloader files: {val_loader.files}')
print0('='*100)

# there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency. suggested to me by @Grad62304977.
# this originates from Karpathy's experiments.
model = GPT(vocab_size=50304, num_layers=12, num_heads=6, model_dim=768)
model = model.cuda()
if args.bf16_embeds:
    for m in model.modules():
        if isinstance(m, nn.Embedding):
            m.bfloat16()
model: GPT = torch.compile(model)
ddp_model = DDP(model, device_ids=[local_rank], broadcast_buffers=False, gradient_as_bucket_view=True)

# collect the parameters to optimize
hidden_matrix_params = [p for p in model.blocks.parameters() if p.ndim == 2]
embed_params = [model.embed.weight, *model.value_embeds.parameters()]
scalar_params = [p for n, p in model.named_parameters() if p.ndim < 2 and "attn_scale" not in n]
attn_scale_params = [p for n, p in model.named_parameters() if p.ndim < 2 and "attn_scale" in n]
head_params = [model.lm_head.weight]

# init the optimizer(s)
optimizer1 = torch.optim.Adam([dict(params=embed_params, lr=0.6),
                               dict(params=head_params, lr=0.008),
                               dict(params=scalar_params, lr=0.04),
                               dict(params=attn_scale_params, lr=0.01)],
                              betas=(0.8, 0.95), fused=True)
optimizer2 = Muon(hidden_matrix_params, lr=0.05, momentum=0.95)
optimizers = [optimizer1, optimizer2]

# learning rate schedule: stable then decay
def get_lr(it):
    t = 1 - it / args.num_iterations # time remaining in training
    assert 1 >= t > 0
    # 1) constant lr for first part of training
    if t >= args.cooldown_frac:
        return 1.0
    # 2) then linear cooldown
    else:
        return t / args.cooldown_frac
schedulers = [torch.optim.lr_scheduler.LambdaLR(opt, get_lr) for opt in optimizers]

# sliding window size schedule: linear increase over training in chunks of 128 from 128 -> 1792. By @fernbear.bsky.social
def get_sliding_window_blocks(it):
    x = it / args.num_iterations # training progress
    assert 0 <= x <= 1
    return int(((1 - x) * 128 + x * 1856) // 128)
sliding_window_num_blocks = torch.tensor(1, dtype=torch.int32, device='cuda')

# Start training loop
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    # This effectively ignores timing first 10 steps, which are slower for weird reasons.
    # Alternately, and slightly more correctly in terms of benchmarking, we could do 10
    # steps with dummy data first, and then re-initialize the model and reset the loader.
    if step == 10:
        training_time_ms = 0
        t0 = time.perf_counter()
    timed_steps = float('nan') if step <= 11 else (step - 10) + 1 # <= 11 to avoid bug in val

    sliding_window_num_blocks.copy_(get_sliding_window_blocks(step))

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        # run validation batches
        model.eval()
        val_loader.reset()
        val_loss = 0.0
        # calculate the number of steps to take in the val loop.
        val_batch_size = world_size * micro_bs
        assert args.val_tokens % val_batch_size == 0
        val_steps = args.val_tokens // val_batch_size
        for _ in range(val_steps):
            with torch.no_grad():
                inputs_val, targets_val = val_loader.next_batch(val_batch_size)
                val_loss += ddp_model(inputs_val, targets_val, sliding_window_num_blocks)
        # Print attention scales
        for n, p in model.named_parameters():
            if p.ndim < 2 and "attn_scale" in n:
                print0(f'{n}: {p.item()}', console=True)
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        val_loss /= val_steps
        # logging
        print0(f'step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/(timed_steps-1):.2f}ms', console=True)
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f'logs/{run_id}', exist_ok=True)
            torch.save(log, f'logs/{run_id}/state_step{step:06d}.pt')
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    model.train()
    batch_size = args.batch_size
    assert batch_size % world_size == 0
    inputs_train, targets_train = train_loader.next_batch(batch_size)
    assert len(inputs_train) <= micro_bs or len(inputs_train) % micro_bs == 0
    for micro_inputs_train, micro_targets_train in zip(inputs_train.split(micro_bs), targets_train.split(micro_bs)):
        ddp_model(micro_inputs_train, micro_targets_train, sliding_window_num_blocks).backward()
    # momentum warmup for Muon
    frac = min(step/300, 1)
    for group in optimizer2.param_groups:
        group['momentum'] = (1 - frac) * 0.85 + frac * 0.95
    # step the optimizers and schedulers
    for opt, sched in zip(optimizers, schedulers):
        opt.step()
        if step != train_steps-1:
            sched.step()
    # null the gradients
    model.zero_grad(set_to_none=True)
    # logging
    approx_time = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f'step:{step+1}/{train_steps} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms', console=True)

print0(f'peak memory consumption: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB')
dist.destroy_process_group()

====================================================================================================
Running Python 3.10.12 (main, Sep 11 2024, 15:47:36) [GCC 11.4.0]
Running PyTorch 2.6.0.dev20241231+cu126 compiled for CUDA 12.6
Tue Jan 14 16:39:36 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.183.06             Driver Version: 535.183.06   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:19:00.0 Off |                    0 |
| N/A   39C    P0             127W / 700W |   7713MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:3B:00.0 Off |                    0 |
| N/A   32C    P0             114W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:4C:00.0 Off |                    0 |
| N/A   29C    P0             117W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   38C    P0             130W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:9B:00.0 Off |                    0 |
| N/A   39C    P0             124W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:BB:00.0 Off |                    0 |
| N/A   30C    P0             115W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:CB:00.0 Off |                    0 |
| N/A   37C    P0             116W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:DB:00.0 Off |                    0 |
| N/A   30C    P0             117W / 700W |   3211MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
Training dataloader files: ['data/fineweb10B/fineweb_train_000001.bin', 'data/fineweb10B/fineweb_train_000002.bin', 'data/fineweb10B/fineweb_train_000003.bin', 'data/fineweb10B/fineweb_train_000004.bin', 'data/fineweb10B/fineweb_train_000005.bin', 'data/fineweb10B/fineweb_train_000006.bin', 'data/fineweb10B/fineweb_train_000007.bin', 'data/fineweb10B/fineweb_train_000008.bin', 'data/fineweb10B/fineweb_train_000009.bin']
Validation dataloader files: ['data/fineweb10B/fineweb_val_000000.bin']
====================================================================================================
step:0/1375 val_loss:10.8258 train_time:0ms step_avg:nanms
step:1/1375 train_time:27405ms step_avg:nanms
step:2/1375 train_time:27474ms step_avg:nanms
step:3/1375 train_time:27656ms step_avg:nanms
step:4/1375 train_time:27789ms step_avg:nanms
step:5/1375 train_time:27922ms step_avg:nanms
step:6/1375 train_time:28056ms step_avg:nanms
step:7/1375 train_time:28188ms step_avg:nanms
step:8/1375 train_time:28323ms step_avg:nanms
step:9/1375 train_time:28456ms step_avg:nanms
step:10/1375 train_time:28601ms step_avg:nanms
step:11/1375 train_time:133ms step_avg:nanms
step:12/1375 train_time:269ms step_avg:nanms
step:13/1375 train_time:403ms step_avg:134.41ms
step:14/1375 train_time:538ms step_avg:134.57ms
step:15/1375 train_time:671ms step_avg:134.15ms
step:16/1375 train_time:805ms step_avg:134.13ms
step:17/1375 train_time:942ms step_avg:134.54ms
step:18/1375 train_time:1079ms step_avg:134.91ms
step:19/1375 train_time:1213ms step_avg:134.81ms
step:20/1375 train_time:1349ms step_avg:134.85ms
step:21/1375 train_time:1484ms step_avg:134.91ms
step:22/1375 train_time:1621ms step_avg:135.09ms
step:23/1375 train_time:1754ms step_avg:134.89ms
step:24/1375 train_time:1890ms step_avg:134.99ms
step:25/1375 train_time:2025ms step_avg:135.00ms
step:26/1375 train_time:2162ms step_avg:135.14ms
step:27/1375 train_time:2298ms step_avg:135.19ms
step:28/1375 train_time:2433ms step_avg:135.16ms
step:29/1375 train_time:2567ms step_avg:135.11ms
step:30/1375 train_time:2704ms step_avg:135.19ms
step:31/1375 train_time:2839ms step_avg:135.19ms
step:32/1375 train_time:2973ms step_avg:135.15ms
step:33/1375 train_time:3109ms step_avg:135.17ms
step:34/1375 train_time:3244ms step_avg:135.17ms
step:35/1375 train_time:3379ms step_avg:135.16ms
step:36/1375 train_time:3514ms step_avg:135.15ms
step:37/1375 train_time:3649ms step_avg:135.14ms
step:38/1375 train_time:3785ms step_avg:135.18ms
step:39/1375 train_time:3920ms step_avg:135.16ms
step:40/1375 train_time:4056ms step_avg:135.22ms
step:41/1375 train_time:4191ms step_avg:135.19ms
step:42/1375 train_time:4326ms step_avg:135.18ms
step:43/1375 train_time:4461ms step_avg:135.20ms
step:44/1375 train_time:4600ms step_avg:135.30ms
step:45/1375 train_time:4733ms step_avg:135.24ms
step:46/1375 train_time:4868ms step_avg:135.23ms
step:47/1375 train_time:5004ms step_avg:135.25ms
step:48/1375 train_time:5141ms step_avg:135.28ms
step:49/1375 train_time:5276ms step_avg:135.27ms
step:50/1375 train_time:5412ms step_avg:135.30ms
step:51/1375 train_time:5547ms step_avg:135.30ms
step:52/1375 train_time:5683ms step_avg:135.31ms
step:53/1375 train_time:5818ms step_avg:135.31ms
step:54/1375 train_time:5955ms step_avg:135.33ms
step:55/1375 train_time:6089ms step_avg:135.31ms
step:56/1375 train_time:6224ms step_avg:135.30ms
step:57/1375 train_time:6359ms step_avg:135.30ms
step:58/1375 train_time:6496ms step_avg:135.33ms
step:59/1375 train_time:6629ms step_avg:135.29ms
step:60/1375 train_time:6764ms step_avg:135.28ms
step:61/1375 train_time:6901ms step_avg:135.32ms
step:62/1375 train_time:7037ms step_avg:135.32ms
step:63/1375 train_time:7171ms step_avg:135.30ms
step:64/1375 train_time:7307ms step_avg:135.32ms
step:65/1375 train_time:7442ms step_avg:135.32ms
step:66/1375 train_time:7579ms step_avg:135.34ms
step:67/1375 train_time:7713ms step_avg:135.32ms
step:68/1375 train_time:7849ms step_avg:135.32ms
step:69/1375 train_time:7985ms step_avg:135.34ms
step:70/1375 train_time:8121ms step_avg:135.34ms
step:71/1375 train_time:8256ms step_avg:135.35ms
step:72/1375 train_time:8391ms step_avg:135.34ms
step:73/1375 train_time:8526ms step_avg:135.34ms
step:74/1375 train_time:8661ms step_avg:135.34ms
step:75/1375 train_time:8798ms step_avg:135.35ms
step:76/1375 train_time:8932ms step_avg:135.34ms
step:77/1375 train_time:9068ms step_avg:135.34ms
step:78/1375 train_time:9205ms step_avg:135.37ms
step:79/1375 train_time:9341ms step_avg:135.38ms
step:80/1375 train_time:9478ms step_avg:135.40ms
step:81/1375 train_time:9612ms step_avg:135.37ms
step:82/1375 train_time:9746ms step_avg:135.36ms
step:83/1375 train_time:9882ms step_avg:135.38ms
step:84/1375 train_time:10018ms step_avg:135.38ms
step:85/1375 train_time:10152ms step_avg:135.36ms
step:86/1375 train_time:10287ms step_avg:135.36ms
step:87/1375 train_time:10424ms step_avg:135.37ms
step:88/1375 train_time:10559ms step_avg:135.37ms
step:89/1375 train_time:10695ms step_avg:135.38ms
step:90/1375 train_time:10828ms step_avg:135.35ms
step:91/1375 train_time:10965ms step_avg:135.37ms
step:92/1375 train_time:11102ms step_avg:135.39ms
step:93/1375 train_time:11237ms step_avg:135.38ms
step:94/1375 train_time:11373ms step_avg:135.39ms
step:95/1375 train_time:11508ms step_avg:135.39ms
step:96/1375 train_time:11644ms step_avg:135.39ms
step:97/1375 train_time:11780ms step_avg:135.40ms
step:98/1375 train_time:11916ms step_avg:135.40ms
step:99/1375 train_time:12051ms step_avg:135.40ms
step:100/1375 train_time:12186ms step_avg:135.40ms
step:101/1375 train_time:12324ms step_avg:135.42ms
step:102/1375 train_time:12459ms step_avg:135.42ms
step:103/1375 train_time:12597ms step_avg:135.45ms
step:104/1375 train_time:12735ms step_avg:135.48ms
step:105/1375 train_time:12872ms step_avg:135.49ms
step:106/1375 train_time:13012ms step_avg:135.54ms
step:107/1375 train_time:13151ms step_avg:135.58ms
step:108/1375 train_time:13292ms step_avg:135.63ms
step:109/1375 train_time:13431ms step_avg:135.66ms
step:110/1375 train_time:13570ms step_avg:135.70ms
step:111/1375 train_time:13708ms step_avg:135.72ms
step:112/1375 train_time:13846ms step_avg:135.75ms
step:113/1375 train_time:13986ms step_avg:135.79ms
step:114/1375 train_time:14126ms step_avg:135.83ms
step:115/1375 train_time:14265ms step_avg:135.86ms
step:116/1375 train_time:14405ms step_avg:135.89ms
step:117/1375 train_time:14544ms step_avg:135.93ms
step:118/1375 train_time:14684ms step_avg:135.96ms
step:119/1375 train_time:14823ms step_avg:135.99ms
step:120/1375 train_time:14961ms step_avg:136.01ms
step:121/1375 train_time:15101ms step_avg:136.05ms
step:122/1375 train_time:15241ms step_avg:136.08ms
step:123/1375 train_time:15380ms step_avg:136.11ms
step:124/1375 train_time:15520ms step_avg:136.14ms
step:125/1375 train_time:15659ms step_avg:136.16ms
step:125/1375 val_loss:4.3718 train_time:15727ms step_avg:136.76ms
step:126/1375 train_time:15800ms step_avg:136.21ms
step:127/1375 train_time:15945ms step_avg:136.28ms
step:128/1375 train_time:16084ms step_avg:136.31ms
step:129/1375 train_time:16223ms step_avg:136.33ms
step:130/1375 train_time:16363ms step_avg:136.35ms
step:131/1375 train_time:16502ms step_avg:136.38ms
step:132/1375 train_time:16641ms step_avg:136.40ms
step:133/1375 train_time:16781ms step_avg:136.43ms
step:134/1375 train_time:16921ms step_avg:136.46ms
step:135/1375 train_time:17060ms step_avg:136.48ms
step:136/1375 train_time:17199ms step_avg:136.50ms
step:137/1375 train_time:17338ms step_avg:136.52ms
step:138/1375 train_time:17478ms step_avg:136.55ms
step:139/1375 train_time:17618ms step_avg:136.57ms
step:140/1375 train_time:17757ms step_avg:136.60ms
step:141/1375 train_time:17897ms step_avg:136.62ms
step:142/1375 train_time:18038ms step_avg:136.65ms
step:143/1375 train_time:18178ms step_avg:136.68ms
step:144/1375 train_time:18316ms step_avg:136.69ms
step:145/1375 train_time:18455ms step_avg:136.70ms
step:146/1375 train_time:18593ms step_avg:136.71ms
step:147/1375 train_time:18734ms step_avg:136.75ms
step:148/1375 train_time:18874ms step_avg:136.77ms
step:149/1375 train_time:19013ms step_avg:136.78ms
step:150/1375 train_time:19154ms step_avg:136.81ms
step:151/1375 train_time:19293ms step_avg:136.83ms
step:152/1375 train_time:19433ms step_avg:136.86ms
step:153/1375 train_time:19570ms step_avg:136.85ms
step:154/1375 train_time:19711ms step_avg:136.88ms
step:155/1375 train_time:19853ms step_avg:136.92ms
step:156/1375 train_time:19994ms step_avg:136.95ms
step:157/1375 train_time:20135ms step_avg:136.97ms
step:158/1375 train_time:20273ms step_avg:136.98ms
step:159/1375 train_time:20413ms step_avg:137.00ms
step:160/1375 train_time:20553ms step_avg:137.02ms
step:161/1375 train_time:20693ms step_avg:137.04ms
step:162/1375 train_time:20833ms step_avg:137.06ms
step:163/1375 train_time:20973ms step_avg:137.08ms
step:164/1375 train_time:21113ms step_avg:137.10ms
step:165/1375 train_time:21252ms step_avg:137.11ms
step:166/1375 train_time:21393ms step_avg:137.13ms
step:167/1375 train_time:21532ms step_avg:137.15ms
step:168/1375 train_time:21670ms step_avg:137.15ms
step:169/1375 train_time:21810ms step_avg:137.17ms
step:170/1375 train_time:21951ms step_avg:137.19ms
step:171/1375 train_time:22092ms step_avg:137.22ms
step:172/1375 train_time:22232ms step_avg:137.23ms
step:173/1375 train_time:22370ms step_avg:137.24ms
step:174/1375 train_time:22512ms step_avg:137.27ms
step:175/1375 train_time:22653ms step_avg:137.29ms
step:176/1375 train_time:22793ms step_avg:137.31ms
step:177/1375 train_time:22934ms step_avg:137.33ms
step:178/1375 train_time:23074ms step_avg:137.34ms
step:179/1375 train_time:23214ms step_avg:137.36ms
step:180/1375 train_time:23353ms step_avg:137.37ms
step:181/1375 train_time:23492ms step_avg:137.38ms
step:182/1375 train_time:23632ms step_avg:137.40ms
step:183/1375 train_time:23771ms step_avg:137.40ms
step:184/1375 train_time:23910ms step_avg:137.41ms
step:185/1375 train_time:24049ms step_avg:137.42ms
step:186/1375 train_time:24187ms step_avg:137.43ms
step:187/1375 train_time:24326ms step_avg:137.44ms
step:188/1375 train_time:24466ms step_avg:137.45ms
step:189/1375 train_time:24605ms step_avg:137.46ms
step:190/1375 train_time:24745ms step_avg:137.47ms
step:191/1375 train_time:24920ms step_avg:137.68ms
step:192/1375 train_time:25058ms step_avg:137.68ms
step:193/1375 train_time:25194ms step_avg:137.67ms
step:194/1375 train_time:25333ms step_avg:137.68ms
step:195/1375 train_time:25469ms step_avg:137.67ms
step:196/1375 train_time:25607ms step_avg:137.67ms
step:197/1375 train_time:25746ms step_avg:137.68ms
step:198/1375 train_time:25892ms step_avg:137.72ms
step:199/1375 train_time:26034ms step_avg:137.75ms
step:200/1375 train_time:26172ms step_avg:137.75ms
step:201/1375 train_time:26310ms step_avg:137.75ms
step:202/1375 train_time:26448ms step_avg:137.75ms
step:203/1375 train_time:26586ms step_avg:137.75ms
step:204/1375 train_time:26725ms step_avg:137.76ms
step:205/1375 train_time:26866ms step_avg:137.77ms
step:206/1375 train_time:27010ms step_avg:137.81ms
step:207/1375 train_time:27152ms step_avg:137.83ms
step:208/1375 train_time:27294ms step_avg:137.85ms
step:209/1375 train_time:27438ms step_avg:137.88ms
step:210/1375 train_time:27578ms step_avg:137.89ms
step:211/1375 train_time:27718ms step_avg:137.90ms
step:212/1375 train_time:27861ms step_avg:137.93ms
step:213/1375 train_time:28002ms step_avg:137.94ms
step:214/1375 train_time:28145ms step_avg:137.97ms
step:215/1375 train_time:28288ms step_avg:137.99ms
step:216/1375 train_time:28431ms step_avg:138.02ms
step:217/1375 train_time:28571ms step_avg:138.03ms
step:218/1375 train_time:28713ms step_avg:138.04ms
step:219/1375 train_time:28855ms step_avg:138.06ms
step:220/1375 train_time:28998ms step_avg:138.09ms
step:221/1375 train_time:29142ms step_avg:138.12ms
step:222/1375 train_time:29284ms step_avg:138.13ms
step:223/1375 train_time:29426ms step_avg:138.15ms
step:224/1375 train_time:29567ms step_avg:138.16ms
step:225/1375 train_time:29710ms step_avg:138.18ms
step:226/1375 train_time:29852ms step_avg:138.20ms
step:227/1375 train_time:29993ms step_avg:138.22ms
step:228/1375 train_time:30134ms step_avg:138.23ms
step:229/1375 train_time:30276ms step_avg:138.25ms
step:230/1375 train_time:30418ms step_avg:138.26ms
step:231/1375 train_time:30560ms step_avg:138.28ms
step:232/1375 train_time:30700ms step_avg:138.29ms
step:233/1375 train_time:30843ms step_avg:138.31ms
step:234/1375 train_time:30984ms step_avg:138.32ms
step:235/1375 train_time:31126ms step_avg:138.34ms
step:236/1375 train_time:31267ms step_avg:138.35ms
step:237/1375 train_time:31409ms step_avg:138.37ms
step:238/1375 train_time:31552ms step_avg:138.39ms
step:239/1375 train_time:31694ms step_avg:138.40ms
step:240/1375 train_time:31836ms step_avg:138.42ms
step:241/1375 train_time:31978ms step_avg:138.43ms
step:242/1375 train_time:32119ms step_avg:138.44ms
step:243/1375 train_time:32259ms step_avg:138.45ms
step:244/1375 train_time:32401ms step_avg:138.47ms
step:245/1375 train_time:32543ms step_avg:138.48ms
step:246/1375 train_time:32684ms step_avg:138.49ms
step:247/1375 train_time:32826ms step_avg:138.51ms
step:248/1375 train_time:32969ms step_avg:138.53ms
step:249/1375 train_time:33112ms step_avg:138.54ms
step:250/1375 train_time:33253ms step_avg:138.55ms
step:250/1375 val_loss:3.9528 train_time:33322ms step_avg:138.84ms
step:251/1375 train_time:33396ms step_avg:138.57ms
step:252/1375 train_time:33542ms step_avg:138.60ms
step:253/1375 train_time:33684ms step_avg:138.62ms
step:254/1375 train_time:33824ms step_avg:138.62ms
step:255/1375 train_time:33966ms step_avg:138.64ms
step:256/1375 train_time:34107ms step_avg:138.65ms
step:257/1375 train_time:34248ms step_avg:138.66ms
step:258/1375 train_time:34392ms step_avg:138.68ms
step:259/1375 train_time:34536ms step_avg:138.70ms
step:260/1375 train_time:34678ms step_avg:138.71ms
step:261/1375 train_time:34819ms step_avg:138.72ms
step:262/1375 train_time:34962ms step_avg:138.74ms
step:263/1375 train_time:35102ms step_avg:138.74ms
step:264/1375 train_time:35244ms step_avg:138.76ms
step:265/1375 train_time:35386ms step_avg:138.77ms
step:266/1375 train_time:35529ms step_avg:138.79ms
step:267/1375 train_time:35672ms step_avg:138.80ms
step:268/1375 train_time:35815ms step_avg:138.82ms
step:269/1375 train_time:35957ms step_avg:138.83ms
step:270/1375 train_time:36098ms step_avg:138.84ms
step:271/1375 train_time:36240ms step_avg:138.85ms
step:272/1375 train_time:36382ms step_avg:138.86ms
step:273/1375 train_time:36523ms step_avg:138.87ms
step:274/1375 train_time:36665ms step_avg:138.88ms
step:275/1375 train_time:36807ms step_avg:138.89ms
step:276/1375 train_time:36950ms step_avg:138.91ms
step:277/1375 train_time:37091ms step_avg:138.92ms
step:278/1375 train_time:37233ms step_avg:138.93ms
step:279/1375 train_time:37375ms step_avg:138.94ms
step:280/1375 train_time:37516ms step_avg:138.95ms
step:281/1375 train_time:37658ms step_avg:138.96ms
step:282/1375 train_time:37800ms step_avg:138.97ms
step:283/1375 train_time:37941ms step_avg:138.98ms
step:284/1375 train_time:38083ms step_avg:138.99ms
step:285/1375 train_time:38226ms step_avg:139.00ms
step:286/1375 train_time:38369ms step_avg:139.02ms
step:287/1375 train_time:38510ms step_avg:139.03ms
step:288/1375 train_time:38653ms step_avg:139.04ms
step:289/1375 train_time:38794ms step_avg:139.05ms
step:290/1375 train_time:38935ms step_avg:139.05ms
step:291/1375 train_time:39076ms step_avg:139.06ms
step:292/1375 train_time:39217ms step_avg:139.07ms
step:293/1375 train_time:39361ms step_avg:139.08ms
step:294/1375 train_time:39502ms step_avg:139.09ms
step:295/1375 train_time:39643ms step_avg:139.10ms
step:296/1375 train_time:39784ms step_avg:139.11ms
step:297/1375 train_time:39927ms step_avg:139.12ms
step:298/1375 train_time:40069ms step_avg:139.13ms
step:299/1375 train_time:40211ms step_avg:139.14ms
step:300/1375 train_time:40353ms step_avg:139.15ms
step:301/1375 train_time:40495ms step_avg:139.16ms
step:302/1375 train_time:40637ms step_avg:139.17ms
step:303/1375 train_time:40777ms step_avg:139.17ms
step:304/1375 train_time:40918ms step_avg:139.18ms
step:305/1375 train_time:41060ms step_avg:139.19ms
step:306/1375 train_time:41201ms step_avg:139.19ms
step:307/1375 train_time:41344ms step_avg:139.20ms
step:308/1375 train_time:41487ms step_avg:139.22ms
step:309/1375 train_time:41631ms step_avg:139.23ms
step:310/1375 train_time:41776ms step_avg:139.25ms
step:311/1375 train_time:41918ms step_avg:139.26ms
step:312/1375 train_time:42062ms step_avg:139.28ms
step:313/1375 train_time:42206ms step_avg:139.29ms
step:314/1375 train_time:42350ms step_avg:139.31ms
step:315/1375 train_time:42496ms step_avg:139.33ms
step:316/1375 train_time:42640ms step_avg:139.35ms
step:317/1375 train_time:42783ms step_avg:139.36ms
step:318/1375 train_time:42927ms step_avg:139.37ms
step:319/1375 train_time:43072ms step_avg:139.39ms
step:320/1375 train_time:43217ms step_avg:139.41ms
step:321/1375 train_time:43363ms step_avg:139.43ms
step:322/1375 train_time:43506ms step_avg:139.44ms
step:323/1375 train_time:43652ms step_avg:139.46ms
step:324/1375 train_time:43797ms step_avg:139.48ms
step:325/1375 train_time:43940ms step_avg:139.49ms
step:326/1375 train_time:44083ms step_avg:139.50ms
step:327/1375 train_time:44227ms step_avg:139.52ms
step:328/1375 train_time:44373ms step_avg:139.54ms
step:329/1375 train_time:44517ms step_avg:139.55ms
step:330/1375 train_time:44662ms step_avg:139.57ms
step:331/1375 train_time:44805ms step_avg:139.58ms
step:332/1375 train_time:44951ms step_avg:139.60ms
step:333/1375 train_time:45094ms step_avg:139.61ms
step:334/1375 train_time:45240ms step_avg:139.63ms
step:335/1375 train_time:45383ms step_avg:139.64ms
step:336/1375 train_time:45526ms step_avg:139.65ms
step:337/1375 train_time:45673ms step_avg:139.67ms
step:338/1375 train_time:45816ms step_avg:139.68ms
step:339/1375 train_time:45959ms step_avg:139.69ms
step:340/1375 train_time:46102ms step_avg:139.70ms
step:341/1375 train_time:46246ms step_avg:139.72ms
step:342/1375 train_time:46391ms step_avg:139.73ms
step:343/1375 train_time:46536ms step_avg:139.75ms
step:344/1375 train_time:46678ms step_avg:139.76ms
step:345/1375 train_time:46822ms step_avg:139.77ms
step:346/1375 train_time:46967ms step_avg:139.78ms
step:347/1375 train_time:47109ms step_avg:139.79ms
step:348/1375 train_time:47255ms step_avg:139.81ms
step:349/1375 train_time:47399ms step_avg:139.82ms
step:350/1375 train_time:47543ms step_avg:139.83ms
step:351/1375 train_time:47687ms step_avg:139.85ms
step:352/1375 train_time:47832ms step_avg:139.86ms
step:353/1375 train_time:47977ms step_avg:139.87ms
step:354/1375 train_time:48118ms step_avg:139.88ms
step:355/1375 train_time:48263ms step_avg:139.89ms
step:356/1375 train_time:48405ms step_avg:139.90ms
step:357/1375 train_time:48551ms step_avg:139.92ms
step:358/1375 train_time:48697ms step_avg:139.93ms
step:359/1375 train_time:48839ms step_avg:139.94ms
step:360/1375 train_time:48984ms step_avg:139.96ms
step:361/1375 train_time:49128ms step_avg:139.96ms
step:362/1375 train_time:49273ms step_avg:139.98ms
step:363/1375 train_time:49416ms step_avg:139.99ms
step:364/1375 train_time:49562ms step_avg:140.01ms
step:365/1375 train_time:49705ms step_avg:140.01ms
step:366/1375 train_time:49851ms step_avg:140.03ms
step:367/1375 train_time:49993ms step_avg:140.04ms
step:368/1375 train_time:50137ms step_avg:140.05ms
step:369/1375 train_time:50282ms step_avg:140.06ms
step:370/1375 train_time:50424ms step_avg:140.07ms
step:371/1375 train_time:50569ms step_avg:140.08ms
step:372/1375 train_time:50713ms step_avg:140.09ms
step:373/1375 train_time:50857ms step_avg:140.10ms
step:374/1375 train_time:51001ms step_avg:140.11ms
step:375/1375 train_time:51144ms step_avg:140.12ms
step:375/1375 val_loss:3.7697 train_time:51214ms step_avg:140.31ms
step:376/1375 train_time:51289ms step_avg:140.13ms
step:377/1375 train_time:51435ms step_avg:140.15ms
step:378/1375 train_time:51581ms step_avg:140.17ms
step:379/1375 train_time:51724ms step_avg:140.17ms
step:380/1375 train_time:51867ms step_avg:140.18ms
step:381/1375 train_time:52048ms step_avg:140.29ms
step:382/1375 train_time:52190ms step_avg:140.30ms
step:383/1375 train_time:52333ms step_avg:140.30ms
step:384/1375 train_time:52479ms step_avg:140.32ms
step:385/1375 train_time:52622ms step_avg:140.32ms
step:386/1375 train_time:52764ms step_avg:140.33ms
step:387/1375 train_time:52909ms step_avg:140.34ms
step:388/1375 train_time:53053ms step_avg:140.35ms
step:389/1375 train_time:53197ms step_avg:140.36ms
step:390/1375 train_time:53342ms step_avg:140.37ms
step:391/1375 train_time:53486ms step_avg:140.38ms
step:392/1375 train_time:53629ms step_avg:140.39ms
step:393/1375 train_time:53773ms step_avg:140.40ms
step:394/1375 train_time:53917ms step_avg:140.41ms
step:395/1375 train_time:54062ms step_avg:140.42ms
step:396/1375 train_time:54206ms step_avg:140.43ms
step:397/1375 train_time:54350ms step_avg:140.44ms
step:398/1375 train_time:54495ms step_avg:140.45ms
step:399/1375 train_time:54639ms step_avg:140.46ms
step:400/1375 train_time:54786ms step_avg:140.48ms
step:401/1375 train_time:54928ms step_avg:140.48ms
step:402/1375 train_time:55072ms step_avg:140.49ms
step:403/1375 train_time:55215ms step_avg:140.50ms
step:404/1375 train_time:55362ms step_avg:140.51ms
step:405/1375 train_time:55505ms step_avg:140.52ms
step:406/1375 train_time:55648ms step_avg:140.53ms
step:407/1375 train_time:55793ms step_avg:140.54ms
step:408/1375 train_time:55938ms step_avg:140.55ms
step:409/1375 train_time:56083ms step_avg:140.56ms
step:410/1375 train_time:56228ms step_avg:140.57ms
step:411/1375 train_time:56375ms step_avg:140.58ms
step:412/1375 train_time:56520ms step_avg:140.60ms
step:413/1375 train_time:56666ms step_avg:140.61ms
step:414/1375 train_time:56810ms step_avg:140.62ms
step:415/1375 train_time:56956ms step_avg:140.63ms
step:416/1375 train_time:57102ms step_avg:140.64ms
step:417/1375 train_time:57248ms step_avg:140.66ms
step:418/1375 train_time:57393ms step_avg:140.67ms
step:419/1375 train_time:57538ms step_avg:140.68ms
step:420/1375 train_time:57685ms step_avg:140.70ms
step:421/1375 train_time:57829ms step_avg:140.70ms
step:422/1375 train_time:57975ms step_avg:140.72ms
step:423/1375 train_time:58121ms step_avg:140.73ms
step:424/1375 train_time:58268ms step_avg:140.74ms
step:425/1375 train_time:58412ms step_avg:140.75ms
step:426/1375 train_time:58559ms step_avg:140.77ms
step:427/1375 train_time:58704ms step_avg:140.78ms
step:428/1375 train_time:58851ms step_avg:140.79ms
step:429/1375 train_time:58997ms step_avg:140.80ms
step:430/1375 train_time:59143ms step_avg:140.82ms
step:431/1375 train_time:59289ms step_avg:140.83ms
step:432/1375 train_time:59433ms step_avg:140.84ms
step:433/1375 train_time:59580ms step_avg:140.85ms
step:434/1375 train_time:59724ms step_avg:140.86ms
step:435/1375 train_time:59873ms step_avg:140.88ms
step:436/1375 train_time:60018ms step_avg:140.89ms
step:437/1375 train_time:60166ms step_avg:140.90ms
step:438/1375 train_time:60309ms step_avg:140.91ms
step:439/1375 train_time:60455ms step_avg:140.92ms
step:440/1375 train_time:60602ms step_avg:140.93ms
step:441/1375 train_time:60747ms step_avg:140.95ms
step:442/1375 train_time:60891ms step_avg:140.95ms
step:443/1375 train_time:61036ms step_avg:140.96ms
step:444/1375 train_time:61182ms step_avg:140.97ms
step:445/1375 train_time:61327ms step_avg:140.98ms
step:446/1375 train_time:61473ms step_avg:140.99ms
step:447/1375 train_time:61618ms step_avg:141.00ms
step:448/1375 train_time:61766ms step_avg:141.02ms
step:449/1375 train_time:61910ms step_avg:141.02ms
step:450/1375 train_time:62056ms step_avg:141.04ms
step:451/1375 train_time:62204ms step_avg:141.05ms
step:452/1375 train_time:62349ms step_avg:141.06ms
step:453/1375 train_time:62494ms step_avg:141.07ms
step:454/1375 train_time:62642ms step_avg:141.09ms
step:455/1375 train_time:62788ms step_avg:141.10ms
step:456/1375 train_time:62931ms step_avg:141.10ms
step:457/1375 train_time:63079ms step_avg:141.12ms
step:458/1375 train_time:63225ms step_avg:141.13ms
step:459/1375 train_time:63372ms step_avg:141.14ms
step:460/1375 train_time:63517ms step_avg:141.15ms
step:461/1375 train_time:63665ms step_avg:141.16ms
step:462/1375 train_time:63811ms step_avg:141.17ms
step:463/1375 train_time:63957ms step_avg:141.19ms
step:464/1375 train_time:64104ms step_avg:141.20ms
step:465/1375 train_time:64249ms step_avg:141.21ms
step:466/1375 train_time:64396ms step_avg:141.22ms
step:467/1375 train_time:64543ms step_avg:141.23ms
step:468/1375 train_time:64688ms step_avg:141.24ms
step:469/1375 train_time:64831ms step_avg:141.24ms
step:470/1375 train_time:64980ms step_avg:141.26ms
step:471/1375 train_time:65125ms step_avg:141.27ms
step:472/1375 train_time:65271ms step_avg:141.28ms
step:473/1375 train_time:65416ms step_avg:141.29ms
step:474/1375 train_time:65564ms step_avg:141.30ms
step:475/1375 train_time:65709ms step_avg:141.31ms
step:476/1375 train_time:65855ms step_avg:141.32ms
step:477/1375 train_time:66003ms step_avg:141.33ms
step:478/1375 train_time:66148ms step_avg:141.34ms
step:479/1375 train_time:66292ms step_avg:141.35ms
step:480/1375 train_time:66439ms step_avg:141.36ms
step:481/1375 train_time:66586ms step_avg:141.37ms
step:482/1375 train_time:66729ms step_avg:141.38ms
step:483/1375 train_time:66875ms step_avg:141.38ms
step:484/1375 train_time:67020ms step_avg:141.39ms
step:485/1375 train_time:67167ms step_avg:141.40ms
step:486/1375 train_time:67312ms step_avg:141.41ms
step:487/1375 train_time:67459ms step_avg:141.42ms
step:488/1375 train_time:67606ms step_avg:141.43ms
step:489/1375 train_time:67752ms step_avg:141.44ms
step:490/1375 train_time:67899ms step_avg:141.46ms
step:491/1375 train_time:68045ms step_avg:141.47ms
step:492/1375 train_time:68192ms step_avg:141.48ms
step:493/1375 train_time:68337ms step_avg:141.49ms
step:494/1375 train_time:68484ms step_avg:141.50ms
step:495/1375 train_time:68629ms step_avg:141.50ms
step:496/1375 train_time:68774ms step_avg:141.51ms
step:497/1375 train_time:68918ms step_avg:141.51ms
step:498/1375 train_time:69064ms step_avg:141.52ms
step:499/1375 train_time:69210ms step_avg:141.53ms
step:500/1375 train_time:69355ms step_avg:141.54ms
step:500/1375 val_loss:3.6545 train_time:69427ms step_avg:141.69ms
step:501/1375 train_time:69502ms step_avg:141.55ms
step:502/1375 train_time:69653ms step_avg:141.57ms
step:503/1375 train_time:69797ms step_avg:141.58ms
step:504/1375 train_time:69943ms step_avg:141.58ms
step:505/1375 train_time:70087ms step_avg:141.59ms
step:506/1375 train_time:70233ms step_avg:141.60ms
step:507/1375 train_time:70379ms step_avg:141.61ms
step:508/1375 train_time:70525ms step_avg:141.62ms
step:509/1375 train_time:70673ms step_avg:141.63ms
step:510/1375 train_time:70817ms step_avg:141.63ms
step:511/1375 train_time:70963ms step_avg:141.64ms
step:512/1375 train_time:71112ms step_avg:141.66ms
step:513/1375 train_time:71261ms step_avg:141.67ms
step:514/1375 train_time:71409ms step_avg:141.68ms
step:515/1375 train_time:71559ms step_avg:141.70ms
step:516/1375 train_time:71705ms step_avg:141.71ms
step:517/1375 train_time:71855ms step_avg:141.73ms
step:518/1375 train_time:72001ms step_avg:141.73ms
step:519/1375 train_time:72148ms step_avg:141.74ms
step:520/1375 train_time:72295ms step_avg:141.76ms
step:521/1375 train_time:72442ms step_avg:141.77ms
step:522/1375 train_time:72590ms step_avg:141.78ms
step:523/1375 train_time:72738ms step_avg:141.79ms
step:524/1375 train_time:72886ms step_avg:141.80ms
step:525/1375 train_time:73034ms step_avg:141.81ms
step:526/1375 train_time:73182ms step_avg:141.83ms
step:527/1375 train_time:73329ms step_avg:141.84ms
step:528/1375 train_time:73477ms step_avg:141.85ms
step:529/1375 train_time:73622ms step_avg:141.85ms
step:530/1375 train_time:73773ms step_avg:141.87ms
step:531/1375 train_time:73919ms step_avg:141.88ms
step:532/1375 train_time:74066ms step_avg:141.89ms
step:533/1375 train_time:74214ms step_avg:141.90ms
step:534/1375 train_time:74361ms step_avg:141.91ms
step:535/1375 train_time:74509ms step_avg:141.92ms
step:536/1375 train_time:74658ms step_avg:141.94ms
step:537/1375 train_time:74805ms step_avg:141.95ms
step:538/1375 train_time:74954ms step_avg:141.96ms
step:539/1375 train_time:75101ms step_avg:141.97ms
step:540/1375 train_time:75248ms step_avg:141.98ms
step:541/1375 train_time:75396ms step_avg:141.99ms
step:542/1375 train_time:75543ms step_avg:142.00ms
step:543/1375 train_time:75691ms step_avg:142.01ms
step:544/1375 train_time:75838ms step_avg:142.02ms
step:545/1375 train_time:75984ms step_avg:142.03ms
step:546/1375 train_time:76133ms step_avg:142.04ms
step:547/1375 train_time:76281ms step_avg:142.05ms
step:548/1375 train_time:76427ms step_avg:142.06ms
step:549/1375 train_time:76577ms step_avg:142.07ms
step:550/1375 train_time:76724ms step_avg:142.08ms
step:551/1375 train_time:76874ms step_avg:142.10ms
step:552/1375 train_time:77022ms step_avg:142.11ms
step:553/1375 train_time:77171ms step_avg:142.12ms
step:554/1375 train_time:77318ms step_avg:142.13ms
step:555/1375 train_time:77465ms step_avg:142.14ms
step:556/1375 train_time:77612ms step_avg:142.15ms
step:557/1375 train_time:77759ms step_avg:142.16ms
step:558/1375 train_time:77904ms step_avg:142.16ms
step:559/1375 train_time:78052ms step_avg:142.17ms
step:560/1375 train_time:78200ms step_avg:142.18ms
step:561/1375 train_time:78345ms step_avg:142.19ms
step:562/1375 train_time:78493ms step_avg:142.20ms
step:563/1375 train_time:78640ms step_avg:142.21ms
step:564/1375 train_time:78786ms step_avg:142.21ms
step:565/1375 train_time:78935ms step_avg:142.23ms
step:566/1375 train_time:79082ms step_avg:142.23ms
step:567/1375 train_time:79232ms step_avg:142.25ms
step:568/1375 train_time:79379ms step_avg:142.26ms
step:569/1375 train_time:79526ms step_avg:142.26ms
step:570/1375 train_time:79673ms step_avg:142.27ms
step:571/1375 train_time:79860ms step_avg:142.35ms
step:572/1375 train_time:80004ms step_avg:142.36ms
step:573/1375 train_time:80152ms step_avg:142.37ms
step:574/1375 train_time:80301ms step_avg:142.38ms
step:575/1375 train_time:80447ms step_avg:142.38ms
step:576/1375 train_time:80593ms step_avg:142.39ms
step:577/1375 train_time:80741ms step_avg:142.40ms
step:578/1375 train_time:80890ms step_avg:142.41ms
step:579/1375 train_time:81039ms step_avg:142.42ms
step:580/1375 train_time:81186ms step_avg:142.43ms
step:581/1375 train_time:81334ms step_avg:142.44ms
step:582/1375 train_time:81480ms step_avg:142.45ms
step:583/1375 train_time:81625ms step_avg:142.45ms
step:584/1375 train_time:81777ms step_avg:142.47ms
step:585/1375 train_time:81922ms step_avg:142.47ms
step:586/1375 train_time:82073ms step_avg:142.49ms
step:587/1375 train_time:82220ms step_avg:142.50ms
step:588/1375 train_time:82368ms step_avg:142.50ms
step:589/1375 train_time:82515ms step_avg:142.51ms
step:590/1375 train_time:82661ms step_avg:142.52ms
step:591/1375 train_time:82808ms step_avg:142.53ms
step:592/1375 train_time:82957ms step_avg:142.54ms
step:593/1375 train_time:83104ms step_avg:142.54ms
step:594/1375 train_time:83251ms step_avg:142.55ms
step:595/1375 train_time:83399ms step_avg:142.56ms
step:596/1375 train_time:83546ms step_avg:142.57ms
step:597/1375 train_time:83693ms step_avg:142.58ms
step:598/1375 train_time:83840ms step_avg:142.58ms
step:599/1375 train_time:83986ms step_avg:142.59ms
step:600/1375 train_time:84135ms step_avg:142.60ms
step:601/1375 train_time:84282ms step_avg:142.61ms
step:602/1375 train_time:84427ms step_avg:142.61ms
step:603/1375 train_time:84578ms step_avg:142.63ms
step:604/1375 train_time:84724ms step_avg:142.63ms
step:605/1375 train_time:84873ms step_avg:142.64ms
step:606/1375 train_time:85020ms step_avg:142.65ms
step:607/1375 train_time:85169ms step_avg:142.66ms
step:608/1375 train_time:85317ms step_avg:142.67ms
step:609/1375 train_time:85463ms step_avg:142.68ms
step:610/1375 train_time:85610ms step_avg:142.68ms
step:611/1375 train_time:85758ms step_avg:142.69ms
step:612/1375 train_time:85904ms step_avg:142.70ms
step:613/1375 train_time:86055ms step_avg:142.71ms
step:614/1375 train_time:86202ms step_avg:142.72ms
step:615/1375 train_time:86352ms step_avg:142.73ms
step:616/1375 train_time:86500ms step_avg:142.74ms
step:617/1375 train_time:86648ms step_avg:142.75ms
step:618/1375 train_time:86798ms step_avg:142.76ms
step:619/1375 train_time:86947ms step_avg:142.77ms
step:620/1375 train_time:87098ms step_avg:142.78ms
step:621/1375 train_time:87245ms step_avg:142.79ms
step:622/1375 train_time:87396ms step_avg:142.80ms
step:623/1375 train_time:87544ms step_avg:142.81ms
step:624/1375 train_time:87694ms step_avg:142.82ms
step:625/1375 train_time:87841ms step_avg:142.83ms
step:625/1375 val_loss:3.5725 train_time:87918ms step_avg:142.96ms
step:626/1375 train_time:87994ms step_avg:142.85ms
step:627/1375 train_time:88147ms step_avg:142.86ms
step:628/1375 train_time:88294ms step_avg:142.87ms
step:629/1375 train_time:88442ms step_avg:142.88ms
step:630/1375 train_time:88589ms step_avg:142.89ms
step:631/1375 train_time:88736ms step_avg:142.89ms
step:632/1375 train_time:88886ms step_avg:142.90ms
step:633/1375 train_time:89036ms step_avg:142.91ms
step:634/1375 train_time:89185ms step_avg:142.93ms
step:635/1375 train_time:89334ms step_avg:142.93ms
step:636/1375 train_time:89482ms step_avg:142.94ms
step:637/1375 train_time:89631ms step_avg:142.95ms
step:638/1375 train_time:89777ms step_avg:142.96ms
step:639/1375 train_time:89928ms step_avg:142.97ms
step:640/1375 train_time:90076ms step_avg:142.98ms
step:641/1375 train_time:90226ms step_avg:142.99ms
step:642/1375 train_time:90373ms step_avg:143.00ms
step:643/1375 train_time:90525ms step_avg:143.01ms
step:644/1375 train_time:90673ms step_avg:143.02ms
step:645/1375 train_time:90820ms step_avg:143.02ms
step:646/1375 train_time:90971ms step_avg:143.04ms
step:647/1375 train_time:91120ms step_avg:143.05ms
step:648/1375 train_time:91273ms step_avg:143.06ms
step:649/1375 train_time:91423ms step_avg:143.07ms
step:650/1375 train_time:91574ms step_avg:143.08ms
step:651/1375 train_time:91723ms step_avg:143.09ms
step:652/1375 train_time:91871ms step_avg:143.10ms
step:653/1375 train_time:92017ms step_avg:143.11ms
step:654/1375 train_time:92170ms step_avg:143.12ms
step:655/1375 train_time:92318ms step_avg:143.13ms
step:656/1375 train_time:92469ms step_avg:143.14ms
step:657/1375 train_time:92616ms step_avg:143.15ms
step:658/1375 train_time:92767ms step_avg:143.16ms
step:659/1375 train_time:92914ms step_avg:143.17ms
step:660/1375 train_time:93064ms step_avg:143.18ms
step:661/1375 train_time:93213ms step_avg:143.18ms
step:662/1375 train_time:93361ms step_avg:143.19ms
step:663/1375 train_time:93511ms step_avg:143.20ms
step:664/1375 train_time:93662ms step_avg:143.21ms
step:665/1375 train_time:93813ms step_avg:143.23ms
step:666/1375 train_time:93959ms step_avg:143.23ms
step:667/1375 train_time:94106ms step_avg:143.24ms
step:668/1375 train_time:94256ms step_avg:143.25ms
step:669/1375 train_time:94406ms step_avg:143.26ms
step:670/1375 train_time:94555ms step_avg:143.26ms
step:671/1375 train_time:94705ms step_avg:143.27ms
step:672/1375 train_time:94853ms step_avg:143.28ms
step:673/1375 train_time:95000ms step_avg:143.29ms
step:674/1375 train_time:95151ms step_avg:143.30ms
step:675/1375 train_time:95299ms step_avg:143.31ms
step:676/1375 train_time:95450ms step_avg:143.32ms
step:677/1375 train_time:95597ms step_avg:143.32ms
step:678/1375 train_time:95748ms step_avg:143.34ms
step:679/1375 train_time:95897ms step_avg:143.34ms
step:680/1375 train_time:96047ms step_avg:143.35ms
step:681/1375 train_time:96194ms step_avg:143.36ms
step:682/1375 train_time:96344ms step_avg:143.37ms
step:683/1375 train_time:96494ms step_avg:143.38ms
step:684/1375 train_time:96644ms step_avg:143.39ms
step:685/1375 train_time:96792ms step_avg:143.40ms
step:686/1375 train_time:96939ms step_avg:143.40ms
step:687/1375 train_time:97088ms step_avg:143.41ms
step:688/1375 train_time:97239ms step_avg:143.42ms
step:689/1375 train_time:97390ms step_avg:143.43ms
step:690/1375 train_time:97540ms step_avg:143.44ms
step:691/1375 train_time:97688ms step_avg:143.45ms
step:692/1375 train_time:97836ms step_avg:143.45ms
step:693/1375 train_time:97985ms step_avg:143.46ms
step:694/1375 train_time:98135ms step_avg:143.47ms
step:695/1375 train_time:98282ms step_avg:143.48ms
step:696/1375 train_time:98432ms step_avg:143.49ms
step:697/1375 train_time:98579ms step_avg:143.49ms
step:698/1375 train_time:98729ms step_avg:143.50ms
step:699/1375 train_time:98876ms step_avg:143.51ms
step:700/1375 train_time:99026ms step_avg:143.52ms
step:701/1375 train_time:99174ms step_avg:143.52ms
step:702/1375 train_time:99325ms step_avg:143.53ms
step:703/1375 train_time:99473ms step_avg:143.54ms
step:704/1375 train_time:99621ms step_avg:143.55ms
step:705/1375 train_time:99771ms step_avg:143.56ms
step:706/1375 train_time:99923ms step_avg:143.57ms
step:707/1375 train_time:100072ms step_avg:143.57ms
step:708/1375 train_time:100219ms step_avg:143.58ms
step:709/1375 train_time:100369ms step_avg:143.59ms
step:710/1375 train_time:100517ms step_avg:143.60ms
step:711/1375 train_time:100670ms step_avg:143.61ms
step:712/1375 train_time:100817ms step_avg:143.61ms
step:713/1375 train_time:100970ms step_avg:143.63ms
step:714/1375 train_time:101118ms step_avg:143.63ms
step:715/1375 train_time:101269ms step_avg:143.64ms
step:716/1375 train_time:101417ms step_avg:143.65ms
step:717/1375 train_time:101570ms step_avg:143.66ms
step:718/1375 train_time:101718ms step_avg:143.67ms
step:719/1375 train_time:101869ms step_avg:143.68ms
step:720/1375 train_time:102018ms step_avg:143.69ms
step:721/1375 train_time:102171ms step_avg:143.70ms
step:722/1375 train_time:102321ms step_avg:143.71ms
step:723/1375 train_time:102472ms step_avg:143.72ms
step:724/1375 train_time:102621ms step_avg:143.73ms
step:725/1375 train_time:102772ms step_avg:143.74ms
step:726/1375 train_time:102922ms step_avg:143.75ms
step:727/1375 train_time:103075ms step_avg:143.76ms
step:728/1375 train_time:103225ms step_avg:143.77ms
step:729/1375 train_time:103374ms step_avg:143.77ms
step:730/1375 train_time:103527ms step_avg:143.79ms
step:731/1375 train_time:103675ms step_avg:143.79ms
step:732/1375 train_time:103825ms step_avg:143.80ms
step:733/1375 train_time:103974ms step_avg:143.81ms
step:734/1375 train_time:104125ms step_avg:143.82ms
step:735/1375 train_time:104276ms step_avg:143.83ms
step:736/1375 train_time:104428ms step_avg:143.84ms
step:737/1375 train_time:104578ms step_avg:143.85ms
step:738/1375 train_time:104731ms step_avg:143.86ms
step:739/1375 train_time:104879ms step_avg:143.87ms
step:740/1375 train_time:105032ms step_avg:143.88ms
step:741/1375 train_time:105182ms step_avg:143.89ms
step:742/1375 train_time:105332ms step_avg:143.90ms
step:743/1375 train_time:105480ms step_avg:143.90ms
step:744/1375 train_time:105633ms step_avg:143.91ms
step:745/1375 train_time:105784ms step_avg:143.92ms
step:746/1375 train_time:105934ms step_avg:143.93ms
step:747/1375 train_time:106082ms step_avg:143.94ms
step:748/1375 train_time:106233ms step_avg:143.95ms
step:749/1375 train_time:106382ms step_avg:143.95ms
step:750/1375 train_time:106534ms step_avg:143.96ms
step:750/1375 val_loss:3.5189 train_time:106609ms step_avg:144.07ms
step:751/1375 train_time:106685ms step_avg:143.97ms
step:752/1375 train_time:106840ms step_avg:143.99ms
step:753/1375 train_time:106989ms step_avg:144.00ms
step:754/1375 train_time:107139ms step_avg:144.00ms
step:755/1375 train_time:107288ms step_avg:144.01ms
step:756/1375 train_time:107438ms step_avg:144.02ms
step:757/1375 train_time:107588ms step_avg:144.03ms
step:758/1375 train_time:107741ms step_avg:144.04ms
step:759/1375 train_time:107892ms step_avg:144.05ms
step:760/1375 train_time:108042ms step_avg:144.06ms
step:761/1375 train_time:108235ms step_avg:144.12ms
step:762/1375 train_time:108382ms step_avg:144.12ms
step:763/1375 train_time:108530ms step_avg:144.13ms
step:764/1375 train_time:108681ms step_avg:144.14ms
step:765/1375 train_time:108830ms step_avg:144.15ms
step:766/1375 train_time:108982ms step_avg:144.16ms
step:767/1375 train_time:109133ms step_avg:144.17ms
step:768/1375 train_time:109284ms step_avg:144.17ms
step:769/1375 train_time:109437ms step_avg:144.19ms
step:770/1375 train_time:109587ms step_avg:144.19ms
step:771/1375 train_time:109739ms step_avg:144.20ms
step:772/1375 train_time:109886ms step_avg:144.21ms
step:773/1375 train_time:110040ms step_avg:144.22ms
step:774/1375 train_time:110190ms step_avg:144.23ms
step:775/1375 train_time:110343ms step_avg:144.24ms
step:776/1375 train_time:110496ms step_avg:144.25ms
step:777/1375 train_time:110646ms step_avg:144.26ms
step:778/1375 train_time:110795ms step_avg:144.26ms
step:779/1375 train_time:110944ms step_avg:144.27ms
step:780/1375 train_time:111097ms step_avg:144.28ms
step:781/1375 train_time:111246ms step_avg:144.29ms
step:782/1375 train_time:111399ms step_avg:144.30ms
step:783/1375 train_time:111547ms step_avg:144.30ms
step:784/1375 train_time:111699ms step_avg:144.31ms
step:785/1375 train_time:111847ms step_avg:144.32ms
step:786/1375 train_time:111999ms step_avg:144.33ms
step:787/1375 train_time:112147ms step_avg:144.33ms
step:788/1375 train_time:112299ms step_avg:144.34ms
step:789/1375 train_time:112448ms step_avg:144.35ms
step:790/1375 train_time:112600ms step_avg:144.36ms
step:791/1375 train_time:112749ms step_avg:144.36ms
step:792/1375 train_time:112900ms step_avg:144.37ms
step:793/1375 train_time:113048ms step_avg:144.38ms
step:794/1375 train_time:113201ms step_avg:144.39ms
step:795/1375 train_time:113349ms step_avg:144.39ms
step:796/1375 train_time:113502ms step_avg:144.40ms
step:797/1375 train_time:113651ms step_avg:144.41ms
step:798/1375 train_time:113803ms step_avg:144.42ms
step:799/1375 train_time:113957ms step_avg:144.43ms
step:800/1375 train_time:114106ms step_avg:144.44ms
step:801/1375 train_time:114257ms step_avg:144.45ms
step:802/1375 train_time:114407ms step_avg:144.45ms
step:803/1375 train_time:114557ms step_avg:144.46ms
step:804/1375 train_time:114705ms step_avg:144.47ms
step:805/1375 train_time:114859ms step_avg:144.48ms
step:806/1375 train_time:115008ms step_avg:144.48ms
step:807/1375 train_time:115158ms step_avg:144.49ms
step:808/1375 train_time:115308ms step_avg:144.50ms
step:809/1375 train_time:115458ms step_avg:144.50ms
step:810/1375 train_time:115607ms step_avg:144.51ms
step:811/1375 train_time:115757ms step_avg:144.52ms
step:812/1375 train_time:115906ms step_avg:144.52ms
step:813/1375 train_time:116057ms step_avg:144.53ms
step:814/1375 train_time:116207ms step_avg:144.54ms
step:815/1375 train_time:116357ms step_avg:144.54ms
step:816/1375 train_time:116510ms step_avg:144.55ms
step:817/1375 train_time:116661ms step_avg:144.56ms
step:818/1375 train_time:116811ms step_avg:144.57ms
step:819/1375 train_time:116962ms step_avg:144.58ms
step:820/1375 train_time:117117ms step_avg:144.59ms
step:821/1375 train_time:117267ms step_avg:144.60ms
step:822/1375 train_time:117421ms step_avg:144.61ms
step:823/1375 train_time:117572ms step_avg:144.61ms
step:824/1375 train_time:117723ms step_avg:144.62ms
step:825/1375 train_time:117878ms step_avg:144.64ms
step:826/1375 train_time:118029ms step_avg:144.64ms
step:827/1375 train_time:118181ms step_avg:144.65ms
step:828/1375 train_time:118331ms step_avg:144.66ms
step:829/1375 train_time:118483ms step_avg:144.67ms
step:830/1375 train_time:118635ms step_avg:144.68ms
step:831/1375 train_time:118786ms step_avg:144.68ms
step:832/1375 train_time:118940ms step_avg:144.70ms
step:833/1375 train_time:119091ms step_avg:144.70ms
step:834/1375 train_time:119244ms step_avg:144.71ms
step:835/1375 train_time:119398ms step_avg:144.72ms
step:836/1375 train_time:119551ms step_avg:144.73ms
step:837/1375 train_time:119703ms step_avg:144.74ms
step:838/1375 train_time:119853ms step_avg:144.75ms
step:839/1375 train_time:120004ms step_avg:144.76ms
step:840/1375 train_time:120154ms step_avg:144.76ms
step:841/1375 train_time:120306ms step_avg:144.77ms
step:842/1375 train_time:120459ms step_avg:144.78ms
step:843/1375 train_time:120610ms step_avg:144.79ms
step:844/1375 train_time:120762ms step_avg:144.80ms
step:845/1375 train_time:120912ms step_avg:144.80ms
step:846/1375 train_time:121063ms step_avg:144.81ms
step:847/1375 train_time:121220ms step_avg:144.83ms
step:848/1375 train_time:121369ms step_avg:144.83ms
step:849/1375 train_time:121523ms step_avg:144.84ms
step:850/1375 train_time:121678ms step_avg:144.85ms
step:851/1375 train_time:121829ms step_avg:144.86ms
step:852/1375 train_time:121981ms step_avg:144.87ms
step:853/1375 train_time:122129ms step_avg:144.87ms
step:854/1375 train_time:122281ms step_avg:144.88ms
step:855/1375 train_time:122434ms step_avg:144.89ms
step:856/1375 train_time:122583ms step_avg:144.90ms
step:857/1375 train_time:122739ms step_avg:144.91ms
step:858/1375 train_time:122894ms step_avg:144.92ms
step:859/1375 train_time:123044ms step_avg:144.93ms
step:860/1375 train_time:123197ms step_avg:144.94ms
step:861/1375 train_time:123348ms step_avg:144.94ms
step:862/1375 train_time:123501ms step_avg:144.95ms
step:863/1375 train_time:123652ms step_avg:144.96ms
step:864/1375 train_time:123804ms step_avg:144.97ms
step:865/1375 train_time:123954ms step_avg:144.98ms
step:866/1375 train_time:124111ms step_avg:144.99ms
step:867/1375 train_time:124263ms step_avg:145.00ms
step:868/1375 train_time:124415ms step_avg:145.01ms
step:869/1375 train_time:124566ms step_avg:145.01ms
step:870/1375 train_time:124723ms step_avg:145.03ms
step:871/1375 train_time:124874ms step_avg:145.03ms
step:872/1375 train_time:125024ms step_avg:145.04ms
step:873/1375 train_time:125174ms step_avg:145.04ms
step:874/1375 train_time:125325ms step_avg:145.05ms
step:875/1375 train_time:125476ms step_avg:145.06ms
step:875/1375 val_loss:3.4667 train_time:125551ms step_avg:145.15ms
step:876/1375 train_time:125627ms step_avg:145.07ms
step:877/1375 train_time:125781ms step_avg:145.08ms
step:878/1375 train_time:125933ms step_avg:145.08ms
step:879/1375 train_time:126083ms step_avg:145.09ms
step:880/1375 train_time:126235ms step_avg:145.10ms
step:881/1375 train_time:126384ms step_avg:145.10ms
step:882/1375 train_time:126540ms step_avg:145.11ms
step:883/1375 train_time:126692ms step_avg:145.12ms
step:884/1375 train_time:126842ms step_avg:145.13ms
step:885/1375 train_time:126995ms step_avg:145.14ms
step:886/1375 train_time:127145ms step_avg:145.14ms
step:887/1375 train_time:127298ms step_avg:145.15ms
step:888/1375 train_time:127452ms step_avg:145.16ms
step:889/1375 train_time:127604ms step_avg:145.17ms
step:890/1375 train_time:127757ms step_avg:145.18ms
step:891/1375 train_time:127907ms step_avg:145.18ms
step:892/1375 train_time:128060ms step_avg:145.19ms
step:893/1375 train_time:128211ms step_avg:145.20ms
step:894/1375 train_time:128364ms step_avg:145.21ms
step:895/1375 train_time:128521ms step_avg:145.22ms
step:896/1375 train_time:128675ms step_avg:145.23ms
step:897/1375 train_time:128824ms step_avg:145.24ms
step:898/1375 train_time:128979ms step_avg:145.25ms
step:899/1375 train_time:129129ms step_avg:145.25ms
step:900/1375 train_time:129280ms step_avg:145.26ms
step:901/1375 train_time:129434ms step_avg:145.27ms
step:902/1375 train_time:129582ms step_avg:145.27ms
step:903/1375 train_time:129736ms step_avg:145.28ms
step:904/1375 train_time:129887ms step_avg:145.29ms
step:905/1375 train_time:130040ms step_avg:145.30ms
step:906/1375 train_time:130191ms step_avg:145.30ms
step:907/1375 train_time:130345ms step_avg:145.31ms
step:908/1375 train_time:130497ms step_avg:145.32ms
step:909/1375 train_time:130650ms step_avg:145.33ms
step:910/1375 train_time:130809ms step_avg:145.34ms
step:911/1375 train_time:130960ms step_avg:145.35ms
step:912/1375 train_time:131111ms step_avg:145.36ms
step:913/1375 train_time:131264ms step_avg:145.36ms
step:914/1375 train_time:131417ms step_avg:145.37ms
step:915/1375 train_time:131570ms step_avg:145.38ms
step:916/1375 train_time:131720ms step_avg:145.39ms
step:917/1375 train_time:131873ms step_avg:145.39ms
step:918/1375 train_time:132025ms step_avg:145.40ms
step:919/1375 train_time:132182ms step_avg:145.41ms
step:920/1375 train_time:132334ms step_avg:145.42ms
step:921/1375 train_time:132487ms step_avg:145.43ms
step:922/1375 train_time:132644ms step_avg:145.44ms
step:923/1375 train_time:132797ms step_avg:145.45ms
step:924/1375 train_time:132948ms step_avg:145.46ms
step:925/1375 train_time:133102ms step_avg:145.47ms
step:926/1375 train_time:133260ms step_avg:145.48ms
step:927/1375 train_time:133412ms step_avg:145.49ms
step:928/1375 train_time:133564ms step_avg:145.49ms
step:929/1375 train_time:133719ms step_avg:145.51ms
step:930/1375 train_time:133874ms step_avg:145.52ms
step:931/1375 train_time:134025ms step_avg:145.52ms
step:932/1375 train_time:134177ms step_avg:145.53ms
step:933/1375 train_time:134332ms step_avg:145.54ms
step:934/1375 train_time:134483ms step_avg:145.54ms
step:935/1375 train_time:134638ms step_avg:145.55ms
step:936/1375 train_time:134790ms step_avg:145.56ms
step:937/1375 train_time:134946ms step_avg:145.57ms
step:938/1375 train_time:135099ms step_avg:145.58ms
step:939/1375 train_time:135254ms step_avg:145.59ms
step:940/1375 train_time:135406ms step_avg:145.60ms
step:941/1375 train_time:135560ms step_avg:145.61ms
step:942/1375 train_time:135712ms step_avg:145.61ms
step:943/1375 train_time:135866ms step_avg:145.62ms
step:944/1375 train_time:136025ms step_avg:145.64ms
step:945/1375 train_time:136180ms step_avg:145.65ms
step:946/1375 train_time:136337ms step_avg:145.66ms
step:947/1375 train_time:136488ms step_avg:145.67ms
step:948/1375 train_time:136641ms step_avg:145.67ms
step:949/1375 train_time:136798ms step_avg:145.69ms
step:950/1375 train_time:136952ms step_avg:145.69ms
step:951/1375 train_time:137144ms step_avg:145.74ms
step:952/1375 train_time:137295ms step_avg:145.75ms
step:953/1375 train_time:137448ms step_avg:145.76ms
step:954/1375 train_time:137599ms step_avg:145.76ms
step:955/1375 train_time:137751ms step_avg:145.77ms
step:956/1375 train_time:137904ms step_avg:145.78ms
step:957/1375 train_time:138059ms step_avg:145.79ms
step:958/1375 train_time:138215ms step_avg:145.80ms
step:959/1375 train_time:138372ms step_avg:145.81ms
step:960/1375 train_time:138526ms step_avg:145.82ms
step:961/1375 train_time:138680ms step_avg:145.83ms
step:962/1375 train_time:138832ms step_avg:145.83ms
step:963/1375 train_time:138989ms step_avg:145.84ms
step:964/1375 train_time:139141ms step_avg:145.85ms
step:965/1375 train_time:139293ms step_avg:145.86ms
step:966/1375 train_time:139446ms step_avg:145.86ms
step:967/1375 train_time:139598ms step_avg:145.87ms
step:968/1375 train_time:139749ms step_avg:145.88ms
step:969/1375 train_time:139902ms step_avg:145.88ms
step:970/1375 train_time:140056ms step_avg:145.89ms
step:971/1375 train_time:140206ms step_avg:145.90ms
step:972/1375 train_time:140359ms step_avg:145.90ms
step:973/1375 train_time:140510ms step_avg:145.91ms
step:974/1375 train_time:140662ms step_avg:145.91ms
step:975/1375 train_time:140815ms step_avg:145.92ms
step:976/1375 train_time:140968ms step_avg:145.93ms
step:977/1375 train_time:141119ms step_avg:145.93ms
step:978/1375 train_time:141273ms step_avg:145.94ms
step:979/1375 train_time:141424ms step_avg:145.95ms
step:980/1375 train_time:141577ms step_avg:145.96ms
step:981/1375 train_time:141727ms step_avg:145.96ms
step:982/1375 train_time:141880ms step_avg:145.97ms
step:983/1375 train_time:142033ms step_avg:145.97ms
step:984/1375 train_time:142184ms step_avg:145.98ms
step:985/1375 train_time:142337ms step_avg:145.99ms
step:986/1375 train_time:142494ms step_avg:146.00ms
step:987/1375 train_time:142645ms step_avg:146.00ms
step:988/1375 train_time:142797ms step_avg:146.01ms
step:989/1375 train_time:142949ms step_avg:146.02ms
step:990/1375 train_time:143101ms step_avg:146.02ms
step:991/1375 train_time:143255ms step_avg:146.03ms
step:992/1375 train_time:143410ms step_avg:146.04ms
step:993/1375 train_time:143573ms step_avg:146.06ms
step:994/1375 train_time:143724ms step_avg:146.06ms
step:995/1375 train_time:143877ms step_avg:146.07ms
step:996/1375 train_time:144027ms step_avg:146.07ms
step:997/1375 train_time:144180ms step_avg:146.08ms
step:998/1375 train_time:144334ms step_avg:146.09ms
step:999/1375 train_time:144486ms step_avg:146.09ms
step:1000/1375 train_time:144640ms step_avg:146.10ms
step:1000/1375 val_loss:3.4007 train_time:144716ms step_avg:146.18ms
step:1001/1375 train_time:144792ms step_avg:146.11ms
step:1002/1375 train_time:144949ms step_avg:146.12ms
step:1003/1375 train_time:145103ms step_avg:146.13ms
step:1004/1375 train_time:145256ms step_avg:146.13ms
step:1005/1375 train_time:145409ms step_avg:146.14ms
step:1006/1375 train_time:145560ms step_avg:146.14ms
step:1007/1375 train_time:145714ms step_avg:146.15ms
step:1008/1375 train_time:145868ms step_avg:146.16ms
step:1009/1375 train_time:146028ms step_avg:146.17ms
step:1010/1375 train_time:146178ms step_avg:146.18ms
step:1011/1375 train_time:146330ms step_avg:146.18ms
step:1012/1375 train_time:146482ms step_avg:146.19ms
step:1013/1375 train_time:146634ms step_avg:146.20ms
step:1014/1375 train_time:146788ms step_avg:146.20ms
step:1015/1375 train_time:146941ms step_avg:146.21ms
step:1016/1375 train_time:147094ms step_avg:146.22ms
step:1017/1375 train_time:147248ms step_avg:146.22ms
step:1018/1375 train_time:147399ms step_avg:146.23ms
step:1019/1375 train_time:147554ms step_avg:146.24ms
step:1020/1375 train_time:147709ms step_avg:146.25ms
step:1021/1375 train_time:147862ms step_avg:146.25ms
step:1022/1375 train_time:148017ms step_avg:146.26ms
step:1023/1375 train_time:148171ms step_avg:146.27ms
step:1024/1375 train_time:148326ms step_avg:146.28ms
step:1025/1375 train_time:148478ms step_avg:146.28ms
step:1026/1375 train_time:148630ms step_avg:146.29ms
step:1027/1375 train_time:148785ms step_avg:146.30ms
step:1028/1375 train_time:148939ms step_avg:146.31ms
step:1029/1375 train_time:149095ms step_avg:146.31ms
step:1030/1375 train_time:149250ms step_avg:146.32ms
step:1031/1375 train_time:149402ms step_avg:146.33ms
step:1032/1375 train_time:149555ms step_avg:146.34ms
step:1033/1375 train_time:149709ms step_avg:146.34ms
step:1034/1375 train_time:149862ms step_avg:146.35ms
step:1035/1375 train_time:150018ms step_avg:146.36ms
step:1036/1375 train_time:150175ms step_avg:146.37ms
step:1037/1375 train_time:150332ms step_avg:146.38ms
step:1038/1375 train_time:150487ms step_avg:146.39ms
step:1039/1375 train_time:150638ms step_avg:146.39ms
step:1040/1375 train_time:150792ms step_avg:146.40ms
step:1041/1375 train_time:150947ms step_avg:146.41ms
step:1042/1375 train_time:151098ms step_avg:146.41ms
step:1043/1375 train_time:151250ms step_avg:146.42ms
step:1044/1375 train_time:151403ms step_avg:146.42ms
step:1045/1375 train_time:151560ms step_avg:146.44ms
step:1046/1375 train_time:151713ms step_avg:146.44ms
step:1047/1375 train_time:151867ms step_avg:146.45ms
step:1048/1375 train_time:152022ms step_avg:146.46ms
step:1049/1375 train_time:152174ms step_avg:146.46ms
step:1050/1375 train_time:152333ms step_avg:146.47ms
step:1051/1375 train_time:152492ms step_avg:146.49ms
step:1052/1375 train_time:152648ms step_avg:146.50ms
step:1053/1375 train_time:152800ms step_avg:146.50ms
step:1054/1375 train_time:152955ms step_avg:146.51ms
step:1055/1375 train_time:153109ms step_avg:146.52ms
step:1056/1375 train_time:153265ms step_avg:146.52ms
step:1057/1375 train_time:153420ms step_avg:146.53ms
step:1058/1375 train_time:153577ms step_avg:146.54ms
step:1059/1375 train_time:153734ms step_avg:146.55ms
step:1060/1375 train_time:153889ms step_avg:146.56ms
step:1061/1375 train_time:154041ms step_avg:146.57ms
step:1062/1375 train_time:154195ms step_avg:146.57ms
step:1063/1375 train_time:154350ms step_avg:146.58ms
step:1064/1375 train_time:154503ms step_avg:146.59ms
step:1065/1375 train_time:154655ms step_avg:146.59ms
step:1066/1375 train_time:154814ms step_avg:146.60ms
step:1067/1375 train_time:154971ms step_avg:146.61ms
step:1068/1375 train_time:155125ms step_avg:146.62ms
step:1069/1375 train_time:155282ms step_avg:146.63ms
step:1070/1375 train_time:155433ms step_avg:146.63ms
step:1071/1375 train_time:155589ms step_avg:146.64ms
step:1072/1375 train_time:155740ms step_avg:146.65ms
step:1073/1375 train_time:155894ms step_avg:146.65ms
step:1074/1375 train_time:156048ms step_avg:146.66ms
step:1075/1375 train_time:156201ms step_avg:146.67ms
step:1076/1375 train_time:156352ms step_avg:146.67ms
step:1077/1375 train_time:156507ms step_avg:146.68ms
step:1078/1375 train_time:156664ms step_avg:146.69ms
step:1079/1375 train_time:156822ms step_avg:146.70ms
step:1080/1375 train_time:156975ms step_avg:146.71ms
step:1081/1375 train_time:157129ms step_avg:146.71ms
step:1082/1375 train_time:157282ms step_avg:146.72ms
step:1083/1375 train_time:157434ms step_avg:146.72ms
step:1084/1375 train_time:157593ms step_avg:146.73ms
step:1085/1375 train_time:157747ms step_avg:146.74ms
step:1086/1375 train_time:157901ms step_avg:146.75ms
step:1087/1375 train_time:158054ms step_avg:146.75ms
step:1088/1375 train_time:158210ms step_avg:146.76ms
step:1089/1375 train_time:158368ms step_avg:146.77ms
step:1090/1375 train_time:158528ms step_avg:146.79ms
step:1091/1375 train_time:158684ms step_avg:146.79ms
step:1092/1375 train_time:158835ms step_avg:146.80ms
step:1093/1375 train_time:158991ms step_avg:146.81ms
step:1094/1375 train_time:159145ms step_avg:146.81ms
step:1095/1375 train_time:159295ms step_avg:146.82ms
step:1096/1375 train_time:159454ms step_avg:146.83ms
step:1097/1375 train_time:159609ms step_avg:146.83ms
step:1098/1375 train_time:159762ms step_avg:146.84ms
step:1099/1375 train_time:159913ms step_avg:146.84ms
step:1100/1375 train_time:160067ms step_avg:146.85ms
step:1101/1375 train_time:160220ms step_avg:146.86ms
step:1102/1375 train_time:160376ms step_avg:146.86ms
step:1103/1375 train_time:160530ms step_avg:146.87ms
step:1104/1375 train_time:160685ms step_avg:146.88ms
step:1105/1375 train_time:160837ms step_avg:146.88ms
step:1106/1375 train_time:160990ms step_avg:146.89ms
step:1107/1375 train_time:161143ms step_avg:146.89ms
step:1108/1375 train_time:161301ms step_avg:146.90ms
step:1109/1375 train_time:161453ms step_avg:146.91ms
step:1110/1375 train_time:161609ms step_avg:146.92ms
step:1111/1375 train_time:161765ms step_avg:146.93ms
step:1112/1375 train_time:161919ms step_avg:146.93ms
step:1113/1375 train_time:162071ms step_avg:146.94ms
step:1114/1375 train_time:162229ms step_avg:146.95ms
step:1115/1375 train_time:162384ms step_avg:146.95ms
step:1116/1375 train_time:162535ms step_avg:146.96ms
step:1117/1375 train_time:162693ms step_avg:146.97ms
step:1118/1375 train_time:162851ms step_avg:146.98ms
step:1119/1375 train_time:163006ms step_avg:146.99ms
step:1120/1375 train_time:163160ms step_avg:146.99ms
step:1121/1375 train_time:163313ms step_avg:147.00ms
step:1122/1375 train_time:163466ms step_avg:147.00ms
step:1123/1375 train_time:163619ms step_avg:147.01ms
step:1124/1375 train_time:163778ms step_avg:147.02ms
step:1125/1375 train_time:163935ms step_avg:147.03ms
step:1125/1375 val_loss:3.3478 train_time:164015ms step_avg:147.10ms
step:1126/1375 train_time:164092ms step_avg:147.04ms
step:1127/1375 train_time:164250ms step_avg:147.05ms
step:1128/1375 train_time:164405ms step_avg:147.05ms
step:1129/1375 train_time:164563ms step_avg:147.06ms
step:1130/1375 train_time:164715ms step_avg:147.07ms
step:1131/1375 train_time:164873ms step_avg:147.08ms
step:1132/1375 train_time:165025ms step_avg:147.08ms
step:1133/1375 train_time:165183ms step_avg:147.09ms
step:1134/1375 train_time:165341ms step_avg:147.10ms
step:1135/1375 train_time:165495ms step_avg:147.11ms
step:1136/1375 train_time:165655ms step_avg:147.12ms
step:1137/1375 train_time:165808ms step_avg:147.12ms
step:1138/1375 train_time:165963ms step_avg:147.13ms
step:1139/1375 train_time:166117ms step_avg:147.14ms
step:1140/1375 train_time:166274ms step_avg:147.14ms
step:1141/1375 train_time:166471ms step_avg:147.19ms
step:1142/1375 train_time:166621ms step_avg:147.19ms
step:1143/1375 train_time:166780ms step_avg:147.20ms
step:1144/1375 train_time:166934ms step_avg:147.21ms
step:1145/1375 train_time:167085ms step_avg:147.21ms
step:1146/1375 train_time:167242ms step_avg:147.22ms
step:1147/1375 train_time:167400ms step_avg:147.23ms
step:1148/1375 train_time:167555ms step_avg:147.24ms
step:1149/1375 train_time:167710ms step_avg:147.24ms
step:1150/1375 train_time:167862ms step_avg:147.25ms
step:1151/1375 train_time:168018ms step_avg:147.25ms
step:1152/1375 train_time:168174ms step_avg:147.26ms
step:1153/1375 train_time:168331ms step_avg:147.27ms
step:1154/1375 train_time:168487ms step_avg:147.28ms
step:1155/1375 train_time:168641ms step_avg:147.28ms
step:1156/1375 train_time:168801ms step_avg:147.30ms
step:1157/1375 train_time:168958ms step_avg:147.30ms
step:1158/1375 train_time:169112ms step_avg:147.31ms
step:1159/1375 train_time:169269ms step_avg:147.32ms
step:1160/1375 train_time:169422ms step_avg:147.32ms
step:1161/1375 train_time:169577ms step_avg:147.33ms
step:1162/1375 train_time:169733ms step_avg:147.34ms
step:1163/1375 train_time:169888ms step_avg:147.34ms
step:1164/1375 train_time:170042ms step_avg:147.35ms
step:1165/1375 train_time:170195ms step_avg:147.36ms
step:1166/1375 train_time:170354ms step_avg:147.36ms
step:1167/1375 train_time:170506ms step_avg:147.37ms
step:1168/1375 train_time:170663ms step_avg:147.38ms
step:1169/1375 train_time:170817ms step_avg:147.38ms
step:1170/1375 train_time:170973ms step_avg:147.39ms
step:1171/1375 train_time:171129ms step_avg:147.40ms
step:1172/1375 train_time:171283ms step_avg:147.40ms
step:1173/1375 train_time:171438ms step_avg:147.41ms
step:1174/1375 train_time:171601ms step_avg:147.42ms
step:1175/1375 train_time:171759ms step_avg:147.43ms
step:1176/1375 train_time:171916ms step_avg:147.44ms
step:1177/1375 train_time:172081ms step_avg:147.46ms
step:1178/1375 train_time:172235ms step_avg:147.46ms
step:1179/1375 train_time:172392ms step_avg:147.47ms
step:1180/1375 train_time:172555ms step_avg:147.48ms
step:1181/1375 train_time:172710ms step_avg:147.49ms
step:1182/1375 train_time:172864ms step_avg:147.50ms
step:1183/1375 train_time:173017ms step_avg:147.50ms
step:1184/1375 train_time:173173ms step_avg:147.51ms
step:1185/1375 train_time:173332ms step_avg:147.52ms
step:1186/1375 train_time:173488ms step_avg:147.52ms
step:1187/1375 train_time:173650ms step_avg:147.54ms
step:1188/1375 train_time:173802ms step_avg:147.54ms
step:1189/1375 train_time:173959ms step_avg:147.55ms
step:1190/1375 train_time:174115ms step_avg:147.56ms
step:1191/1375 train_time:174274ms step_avg:147.56ms
step:1192/1375 train_time:174428ms step_avg:147.57ms
step:1193/1375 train_time:174581ms step_avg:147.57ms
step:1194/1375 train_time:174735ms step_avg:147.58ms
step:1195/1375 train_time:174890ms step_avg:147.59ms
step:1196/1375 train_time:175046ms step_avg:147.59ms
step:1197/1375 train_time:175203ms step_avg:147.60ms
step:1198/1375 train_time:175362ms step_avg:147.61ms
step:1199/1375 train_time:175516ms step_avg:147.62ms
step:1200/1375 train_time:175671ms step_avg:147.62ms
step:1201/1375 train_time:175827ms step_avg:147.63ms
step:1202/1375 train_time:175993ms step_avg:147.65ms
step:1203/1375 train_time:176151ms step_avg:147.65ms
step:1204/1375 train_time:176307ms step_avg:147.66ms
step:1205/1375 train_time:176461ms step_avg:147.67ms
step:1206/1375 train_time:176619ms step_avg:147.67ms
step:1207/1375 train_time:176774ms step_avg:147.68ms
step:1208/1375 train_time:176930ms step_avg:147.69ms
step:1209/1375 train_time:177086ms step_avg:147.69ms
step:1210/1375 train_time:177243ms step_avg:147.70ms
step:1211/1375 train_time:177399ms step_avg:147.71ms
step:1212/1375 train_time:177553ms step_avg:147.71ms
step:1213/1375 train_time:177707ms step_avg:147.72ms
step:1214/1375 train_time:177863ms step_avg:147.73ms
step:1215/1375 train_time:178018ms step_avg:147.73ms
step:1216/1375 train_time:178172ms step_avg:147.74ms
step:1217/1375 train_time:178329ms step_avg:147.75ms
step:1218/1375 train_time:178481ms step_avg:147.75ms
step:1219/1375 train_time:178633ms step_avg:147.75ms
step:1220/1375 train_time:178787ms step_avg:147.76ms
step:1221/1375 train_time:178942ms step_avg:147.76ms
step:1222/1375 train_time:179096ms step_avg:147.77ms
step:1223/1375 train_time:179255ms step_avg:147.78ms
step:1224/1375 train_time:179413ms step_avg:147.79ms
step:1225/1375 train_time:179570ms step_avg:147.79ms
step:1226/1375 train_time:179725ms step_avg:147.80ms
step:1227/1375 train_time:179885ms step_avg:147.81ms
step:1228/1375 train_time:180040ms step_avg:147.82ms
step:1229/1375 train_time:180194ms step_avg:147.82ms
step:1230/1375 train_time:180355ms step_avg:147.83ms
step:1231/1375 train_time:180512ms step_avg:147.84ms
step:1232/1375 train_time:180673ms step_avg:147.85ms
step:1233/1375 train_time:180829ms step_avg:147.86ms
step:1234/1375 train_time:180983ms step_avg:147.86ms
step:1235/1375 train_time:181137ms step_avg:147.87ms
step:1236/1375 train_time:181293ms step_avg:147.87ms
step:1237/1375 train_time:181448ms step_avg:147.88ms
step:1238/1375 train_time:181611ms step_avg:147.89ms
step:1239/1375 train_time:181767ms step_avg:147.90ms
step:1240/1375 train_time:181928ms step_avg:147.91ms
step:1241/1375 train_time:182086ms step_avg:147.92ms
step:1242/1375 train_time:182240ms step_avg:147.92ms
step:1243/1375 train_time:182398ms step_avg:147.93ms
step:1244/1375 train_time:182554ms step_avg:147.94ms
step:1245/1375 train_time:182710ms step_avg:147.94ms
step:1246/1375 train_time:182869ms step_avg:147.95ms
step:1247/1375 train_time:183029ms step_avg:147.96ms
step:1248/1375 train_time:183183ms step_avg:147.97ms
step:1249/1375 train_time:183335ms step_avg:147.97ms
step:1250/1375 train_time:183491ms step_avg:147.98ms
step:1250/1375 val_loss:3.3024 train_time:183571ms step_avg:148.04ms
step:1251/1375 train_time:183650ms step_avg:147.99ms
step:1252/1375 train_time:183805ms step_avg:147.99ms
step:1253/1375 train_time:183958ms step_avg:148.00ms
step:1254/1375 train_time:184111ms step_avg:148.00ms
step:1255/1375 train_time:184278ms step_avg:148.01ms
step:1256/1375 train_time:184433ms step_avg:148.02ms
step:1257/1375 train_time:184590ms step_avg:148.03ms
step:1258/1375 train_time:184749ms step_avg:148.04ms
step:1259/1375 train_time:184909ms step_avg:148.05ms
step:1260/1375 train_time:185061ms step_avg:148.05ms
step:1261/1375 train_time:185217ms step_avg:148.06ms
step:1262/1375 train_time:185377ms step_avg:148.06ms
step:1263/1375 train_time:185534ms step_avg:148.07ms
step:1264/1375 train_time:185690ms step_avg:148.08ms
step:1265/1375 train_time:185847ms step_avg:148.09ms
step:1266/1375 train_time:186006ms step_avg:148.09ms
step:1267/1375 train_time:186160ms step_avg:148.10ms
step:1268/1375 train_time:186316ms step_avg:148.10ms
step:1269/1375 train_time:186480ms step_avg:148.12ms
step:1270/1375 train_time:186635ms step_avg:148.12ms
step:1271/1375 train_time:186796ms step_avg:148.13ms
step:1272/1375 train_time:186951ms step_avg:148.14ms
step:1273/1375 train_time:187107ms step_avg:148.15ms
step:1274/1375 train_time:187261ms step_avg:148.15ms
step:1275/1375 train_time:187415ms step_avg:148.15ms
step:1276/1375 train_time:187568ms step_avg:148.16ms
step:1277/1375 train_time:187725ms step_avg:148.16ms
step:1278/1375 train_time:187879ms step_avg:148.17ms
step:1279/1375 train_time:188034ms step_avg:148.17ms
step:1280/1375 train_time:188197ms step_avg:148.19ms
step:1281/1375 train_time:188353ms step_avg:148.19ms
step:1282/1375 train_time:188508ms step_avg:148.20ms
step:1283/1375 train_time:188664ms step_avg:148.20ms
step:1284/1375 train_time:188823ms step_avg:148.21ms
step:1285/1375 train_time:188976ms step_avg:148.22ms
step:1286/1375 train_time:189131ms step_avg:148.22ms
step:1287/1375 train_time:189287ms step_avg:148.23ms
step:1288/1375 train_time:189441ms step_avg:148.23ms
step:1289/1375 train_time:189601ms step_avg:148.24ms
step:1290/1375 train_time:189760ms step_avg:148.25ms
step:1291/1375 train_time:189922ms step_avg:148.26ms
step:1292/1375 train_time:190077ms step_avg:148.27ms
step:1293/1375 train_time:190237ms step_avg:148.27ms
step:1294/1375 train_time:190393ms step_avg:148.28ms
step:1295/1375 train_time:190548ms step_avg:148.29ms
step:1296/1375 train_time:190708ms step_avg:148.30ms
step:1297/1375 train_time:190868ms step_avg:148.30ms
step:1298/1375 train_time:191023ms step_avg:148.31ms
step:1299/1375 train_time:191177ms step_avg:148.31ms
step:1300/1375 train_time:191332ms step_avg:148.32ms
step:1301/1375 train_time:191487ms step_avg:148.32ms
step:1302/1375 train_time:191644ms step_avg:148.33ms
step:1303/1375 train_time:191803ms step_avg:148.34ms
step:1304/1375 train_time:191962ms step_avg:148.35ms
step:1305/1375 train_time:192114ms step_avg:148.35ms
step:1306/1375 train_time:192272ms step_avg:148.36ms
step:1307/1375 train_time:192428ms step_avg:148.36ms
step:1308/1375 train_time:192585ms step_avg:148.37ms
step:1309/1375 train_time:192739ms step_avg:148.38ms
step:1310/1375 train_time:192894ms step_avg:148.38ms
step:1311/1375 train_time:193049ms step_avg:148.38ms
step:1312/1375 train_time:193203ms step_avg:148.39ms
step:1313/1375 train_time:193356ms step_avg:148.39ms
step:1314/1375 train_time:193513ms step_avg:148.40ms
step:1315/1375 train_time:193668ms step_avg:148.40ms
step:1316/1375 train_time:193821ms step_avg:148.41ms
step:1317/1375 train_time:193974ms step_avg:148.41ms
step:1318/1375 train_time:194136ms step_avg:148.42ms
step:1319/1375 train_time:194291ms step_avg:148.43ms
step:1320/1375 train_time:194446ms step_avg:148.43ms
step:1321/1375 train_time:194604ms step_avg:148.44ms
step:1322/1375 train_time:194763ms step_avg:148.45ms
step:1323/1375 train_time:194916ms step_avg:148.45ms
step:1324/1375 train_time:195071ms step_avg:148.46ms
step:1325/1375 train_time:195229ms step_avg:148.46ms
step:1326/1375 train_time:195388ms step_avg:148.47ms
step:1327/1375 train_time:195544ms step_avg:148.48ms
step:1328/1375 train_time:195698ms step_avg:148.48ms
step:1329/1375 train_time:195873ms step_avg:148.50ms
step:1330/1375 train_time:196032ms step_avg:148.51ms
step:1331/1375 train_time:196230ms step_avg:148.55ms
step:1332/1375 train_time:196387ms step_avg:148.55ms
step:1333/1375 train_time:196544ms step_avg:148.56ms
step:1334/1375 train_time:196699ms step_avg:148.56ms
step:1335/1375 train_time:196851ms step_avg:148.57ms
step:1336/1375 train_time:197017ms step_avg:148.58ms
step:1337/1375 train_time:197176ms step_avg:148.59ms
step:1338/1375 train_time:197334ms step_avg:148.59ms
step:1339/1375 train_time:197495ms step_avg:148.60ms
step:1340/1375 train_time:197654ms step_avg:148.61ms
step:1341/1375 train_time:197809ms step_avg:148.62ms
step:1342/1375 train_time:197968ms step_avg:148.62ms
step:1343/1375 train_time:198124ms step_avg:148.63ms
step:1344/1375 train_time:198279ms step_avg:148.64ms
step:1345/1375 train_time:198436ms step_avg:148.64ms
step:1346/1375 train_time:198594ms step_avg:148.65ms
step:1347/1375 train_time:198752ms step_avg:148.66ms
step:1348/1375 train_time:198910ms step_avg:148.66ms
step:1349/1375 train_time:199069ms step_avg:148.67ms
step:1350/1375 train_time:199223ms step_avg:148.67ms
step:1351/1375 train_time:199377ms step_avg:148.68ms
step:1352/1375 train_time:199542ms step_avg:148.69ms
step:1353/1375 train_time:199705ms step_avg:148.70ms
step:1354/1375 train_time:199863ms step_avg:148.71ms
step:1355/1375 train_time:200021ms step_avg:148.71ms
step:1356/1375 train_time:200175ms step_avg:148.72ms
step:1357/1375 train_time:200333ms step_avg:148.73ms
step:1358/1375 train_time:200494ms step_avg:148.73ms
step:1359/1375 train_time:200650ms step_avg:148.74ms
step:1360/1375 train_time:200812ms step_avg:148.75ms
step:1361/1375 train_time:200972ms step_avg:148.76ms
step:1362/1375 train_time:201131ms step_avg:148.77ms
step:1363/1375 train_time:201292ms step_avg:148.77ms
step:1364/1375 train_time:201447ms step_avg:148.78ms
step:1365/1375 train_time:201600ms step_avg:148.78ms
step:1366/1375 train_time:201757ms step_avg:148.79ms
step:1367/1375 train_time:201915ms step_avg:148.80ms
step:1368/1375 train_time:202075ms step_avg:148.80ms
step:1369/1375 train_time:202242ms step_avg:148.82ms
step:1370/1375 train_time:202401ms step_avg:148.82ms
step:1371/1375 train_time:202557ms step_avg:148.83ms
step:1372/1375 train_time:202717ms step_avg:148.84ms
step:1373/1375 train_time:202871ms step_avg:148.84ms
step:1374/1375 train_time:203030ms step_avg:148.85ms
step:1375/1375 train_time:203188ms step_avg:148.86ms
step:1375/1375 val_loss:3.2771 train_time:203263ms step_avg:148.91ms
peak memory consumption: 31565 MiB

import os
import sys
with open(sys.argv[0]) as f:
    code = f.read() # read the code of this file ASAP, for logging
import uuid
import time
import contextlib
from dataclasses import dataclass
from pathlib import Path

import torch
from torch import nn
import torch.nn.functional as F
import torch.distributed as dist
import torch._inductor.config as config
from torch.nn.parallel import DistributedDataParallel as DDP
from torch.nn.attention.flex_attention import BlockMask, flex_attention #KoszarskyB

# -----------------------------------------------------------------------------
# Muon optimizer

@torch.compile
def zeropower_via_newtonschulz5(G, steps=10, eps=1e-7):
    """
    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a
    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose
    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at
    zero even beyond the point where the iteration no longer converges all the way to one everywhere
    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T
    where S' is diagonal with S_{ii}' ~ Uniform(0.5, 1.5), which turns out not to hurt model
    performance at all relative to UV^T, where USV^T = G is the SVD.
    """
    assert len(G.shape) == 2
    a, b, c = (3.4445, -4.7750,  2.0315)
    X = G.bfloat16()
    X /= (X.norm() + eps) # ensure top singular value <= 1
    if G.size(0) > G.size(1):
        X = X.T
    for _ in range(steps):
        A = X @ X.T
        B = b * A + c * A @ A # adapted from suggestion by @jxbz, @leloykun, and @YouJiacheng
        X = a * X + B @ X
    if G.size(0) > G.size(1):
        X = X.T
    return X

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Some warnings:
    - This optimizer assumes that all parameters passed in are 2D.
    - It should not be used for the embedding layer, the final fully connected layer, or any {0,1}-D
    parameters; those should all be optimized by a standard method (e.g., AdamW).
    - To use it with 4D convolutional filters, it works well to just flatten their last 3 dimensions.
    - We believe it is unlikely to work well for training with small batch size.
    - We believe it may not work well for finetuning pretrained models, but we haven't tested this.
    - We have not yet tried this optimizer for training scenarios larger than NanoGPT (124M).

    Arguments:
        lr: The learning rate used by the internal SGD.
        momentum: The momentum used by the internal SGD.
        nesterov: Whether to use Nesterov-style momentum in the internal SGD. (recommended)
        ns_steps: The number of Newton-Schulz iteration steps to use.
    """
    def __init__(self, params, lr=0.02, momentum=0.95, nesterov=True, ns_steps=5):
        self.world_size = int(os.environ['WORLD_SIZE'])
        self.rank = int(os.environ['RANK'])
        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, ns_steps=ns_steps)
        params = list(params)
        assert all(isinstance(p, torch.Tensor) for p in params)
        sizes = {p.numel() for p in params}
        param_groups = [
            {
                'params': [p for p in params if p.numel() == size],
                'update_buffer': [
                    torch.empty(size, device='cuda', dtype=torch.bfloat16)
                    for _ in range(self.world_size)
                ],
            }
            for size in sizes
        ]
        super().__init__(param_groups, defaults)

    def step(self):

        for group in self.param_groups:

            lr = group['lr']
            momentum = group['momentum']
            nesterov = group['nesterov']
            ns_steps = group['ns_steps']
            update_buffers = group['update_buffer']
            # generate weight updates in distributed fashion
            params = group['params']
            assert len(params) % self.world_size == 0
            handle = None
            params_world = None
            def update_prev():
                if params_world is None:
                    return
                assert handle is not None
                handle.wait()
                for p_world, g_world in zip(params_world, update_buffers):
                    p_world.data.add_(
                        g_world.view_as(p_world),
                        alpha=-lr * max(1, p_world.size(0) / p_world.size(1)) ** 0.5,
                    )
            for base_i in range(len(params))[::self.world_size]:
                p = params[base_i + self.rank]
                g = p.grad
                assert g is not None
                state = self.state[p]
                if 'momentum_buffer' not in state:
                    state['momentum_buffer'] = torch.zeros_like(g)
                buf = state['momentum_buffer']
                buf.lerp_(g, 1 - momentum)
                g = g.lerp_(buf, momentum) if nesterov else buf
                g = zeropower_via_newtonschulz5(g, steps=ns_steps).flatten()
                update_prev()
                handle = dist.all_gather(update_buffers, g, async_op=True)
                params_world = params[base_i : base_i + self.world_size]
            update_prev()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the GPT-2 model

def norm(x):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):

    def __init__(self, in_features, out_features):
        super().__init__(in_features, out_features, bias=False)

    def forward(self, x):
        return F.linear(x, self.weight.to(x.dtype))

class Rotary(torch.nn.Module):

    def __init__(self, dim, base=10000):
        super().__init__()
        self.register_buffer('inv_freq', (1 / base) ** (torch.arange(0, dim, 2) / dim))
        self.seq_len_cached = None
        self.cos_cached = None
        self.sin_cached = None

    def forward(self, x):
        seq_len = x.shape[1]
        if seq_len != self.seq_len_cached:
            t = torch.arange(seq_len, device=x.device)
            freqs = torch.outer(t, self.inv_freq)
            self.seq_len_cached = seq_len
            self.cos_cached = freqs.cos()
            self.sin_cached = freqs.sin()
        cos, sin = self.cos_cached[None, :, None, :], self.sin_cached[None, :, None, :]
        # apply_rotary_emb(x, cos, sin)
        x1, x2 = x.chunk(2, dim=3)
        y1 = x1 * cos + x2 * sin
        y2 = x1 * (-sin) + x2 * cos
        return torch.cat((y1, y2), 3).type_as(x)

class CausalSelfAttention(nn.Module):

    def __init__(self, dim, num_heads):
        super().__init__()
        assert dim % num_heads == 0
        self.num_heads = num_heads
        self.c_q = CastedLinear(dim, dim)
        self.c_k = CastedLinear(dim, dim)
        self.c_v = CastedLinear(dim, dim)
        self.lambdas = nn.Parameter(torch.tensor([0.5, 0.5]))
        self.rotary = Rotary(dim // num_heads) # dim // num_heads = head_dim
        self.c_proj = CastedLinear(dim, dim)
        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977

    def forward(self, x, vi, block_mask):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "Must use batch size = 1 for FlexAttention"
        q = self.c_q(x).view(B, T, self.num_heads, -1)
        k = self.c_k(x).view(B, T, self.num_heads, -1)
        v = self.c_v(x).view(B, T, self.num_heads, -1)
        v = self.lambdas[0] * v + self.lambdas[1] * vi.view_as(v) # @KoszarskyB & @Grad62304977
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = self.rotary(q), self.rotary(k)
        y = flex_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), block_mask=block_mask, enable_gqa=True)
        y = y.transpose(1, 2).contiguous().view_as(x) # re-assemble all head outputs side by side
        y = self.c_proj(y)
        return y

class MLP(nn.Module):

    def __init__(self, dim):
        super().__init__()
        self.c_fc   = CastedLinear(dim, 4 * dim)
        self.c_proj = CastedLinear(4 * dim, dim)
        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977

    def forward(self, x):
        x = self.c_fc(x)
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = self.c_proj(x)
        return x

class Block(nn.Module):

    def __init__(self, config):
        super().__init__()
        self.attn = CausalSelfAttention(config.model_dim, config.num_heads)
        self.mlp = MLP(config.model_dim)
        self.lambdas = nn.Parameter(torch.tensor([1., 0.]))

    def forward(self, x, vi, x0, block_mask):
        x = self.lambdas[0] * x + self.lambdas[1] * x0
        x = x + self.attn(norm(x), vi, block_mask)
        x = x + self.mlp(norm(x))
        return x

class ValueEmbedding(nn.Module):
    def __init__(self, config: "GPTConfig"):
        super().__init__()
        self.__setattr__
        self.embed = nn.ModuleList([
            nn.Embedding(config.vocab_size, config.model_dim)
            for _ in range(6)
        ])

    def forward(self, inputs) -> "list[torch.Tensor]":
        ve = [emb(inputs) for emb in self.embed]
        ve += reversed(ve)
        return ve


# -----------------------------------------------------------------------------
# The main GPT-2 model

@dataclass
class GPTConfig:
    vocab_size : int = 50304
    num_layers : int = 12
    num_heads : int = 6 # head dim 128 suggested by @Grad62304977
    model_dim : int = 768

class GPT(nn.Module):

    def __init__(self, config: GPTConfig):
        super().__init__()
        self.num_layers = config.num_layers

        # U-net design by @brendanh0gan
        self.num_encoder_layers = config.num_layers // 2 # Half of the layers for encoder
        self.num_decoder_layers = config.num_layers - self.num_encoder_layers # Remaining for decoder
        # Add learnable skip connection weights for decoder layers
        self.skip_weights = nn.Parameter(torch.ones(self.num_decoder_layers))

        self.embed = nn.Embedding(config.vocab_size, config.model_dim)
        self.blocks = nn.ModuleList([Block(config) for _ in range(config.num_layers)])
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual learning
        # U-net structure on token value embeddings by @leloykun
        self.value_embeds = ValueEmbedding(config)
        self.lm_head = CastedLinear(config.model_dim, config.vocab_size)
        self.lm_head.weight.data.zero_() # @Grad62304977

    def forward(
        self,
        inputs: torch.Tensor,
        targets: torch.Tensor,
        sliding_window_num_blocks: torch.Tensor,
    ):
        BLOCK_SIZE = 128
        assert inputs.ndim == 1
        docs = (inputs == 50256).cumsum(0)
        docs_low = docs.view(-1, BLOCK_SIZE)[:, 0].contiguous()
        docs_high = docs.view(-1, BLOCK_SIZE)[:, -1].contiguous()

        def document_causal(b, h, q_idx, kv_idx):
            causal_mask = q_idx >= kv_idx
            document_mask = docs[q_idx] == docs[kv_idx]
            return causal_mask & document_mask

        def dense_to_ordered(dense_mask: torch.Tensor):
            num_blocks = dense_mask.sum(dim=-1, dtype=torch.int32)
            indices = dense_mask.argsort(dim=-1, descending=True, stable=True).to(torch.int32)
            return num_blocks[None, None].contiguous(), indices[None, None].contiguous()

        def create_doc_swc_block_mask(sliding_window_num_blocks: torch.Tensor):
            kv_idx = block_idx = torch.arange(512, dtype=torch.int32, device="cuda")
            q_idx = block_idx[:, None]
            causal_bm = q_idx >= kv_idx
            causal_full_bm = q_idx > kv_idx
            window_bm = q_idx - kv_idx < sliding_window_num_blocks
            window_full_bm = window_bm
            # document_bm = (docs_low[q_idx] <= docs_high[kv_idx]) & (docs_low[kv_idx] <= docs_high[q_idx])
            document_bm = (docs_low[:, None] <= docs_high) & (docs_low <= docs_high[:, None])
            document_full_bm = (docs_low[:, None] == docs_high) & (docs_low == docs_high[:, None])
            nonzero_bm = causal_bm & window_bm & document_bm
            full_bm  = causal_full_bm & window_full_bm & document_full_bm
            kv_num_blocks, kv_indices = dense_to_ordered(nonzero_bm ^ full_bm)
            full_kv_num_blocks, full_kv_indices = dense_to_ordered(full_bm)
            return BlockMask.from_kv_blocks(
                kv_num_blocks,
                kv_indices,
                full_kv_num_blocks,
                full_kv_indices,
                BLOCK_SIZE=BLOCK_SIZE,
                mask_mod=document_causal,
            )

        block_mask = create_doc_swc_block_mask(sliding_window_num_blocks)

        # forward the GPT model itself
        x = self.embed(inputs[None]) # token embeddings of shape (b, t, model_dim)
        x = norm(x) # @Grad62304977
        x0 = x
        ve = self.value_embeds(inputs)
        ve_enc, ve_dec = ve[:self.num_encoder_layers], ve[self.num_encoder_layers:]

        # Store outputs for U-Net skip connections
        skip_connections = []
        # Encoder pass - process only the first half of the blocks
        for i in range(self.num_encoder_layers):
            x = self.blocks[i](x, ve_enc[i], x0, block_mask)
            skip_connections.append(x)
        # Decoder pass - process the remaining blocks with weighted skip connections
        for i in range(self.num_decoder_layers):
            x = x + self.skip_weights[i] * skip_connections.pop()
            # U-net structure on token value embeddings by @leloykun
            x = self.blocks[self.num_encoder_layers + i](x, ve_dec[i], x0, block_mask)

        x = norm(x)
        logits = self.lm_head(x)
        logits = 30 * torch.tanh(logits / 30) # @Grad62304977
        logits = logits.float()
        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))
        return loss

# -----------------------------------------------------------------------------
# Our own simple Distributed Data Loader

def _peek_data_shard(file: Path):
    # only reads the header, returns header data
    # header is 256 int32
    header = torch.from_file(f"{file}", False, 256, dtype=torch.int32)
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    return int(header[2]) # number of tokens (claimed)

def _load_data_shard(path: Path, num_tokens):
    with path.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True)
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy())
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header?"
    return tokens

class DistributedDataLoader:
    def __init__(self, filename_pattern, seq_len, process_rank, num_processes):
        self.process_rank = process_rank
        self.num_processes = num_processes
        self.seq_len = seq_len

        # glob files that match the pattern
        self.files = sorted(Path.cwd().glob(filename_pattern))
        assert len(self.files) > 0, f"did not find any files that match the pattern {filename_pattern}"

        # load and validate all data shards, count number of tokens in total
        self.files_num_tokens = [_peek_data_shard(file) for file in self.files]
        assert min(self.files_num_tokens) >= num_processes * seq_len + 1
        self.total_num_tokens = sum(self.files_num_tokens)

        self.reset()

    def reset(self):
        self.current_shard = -1
        self.advance()

    def advance(self): # advance to next data shard
        self.current_shard = (self.current_shard + 1) % len(self.files)
        self.current_position = self.process_rank * self.seq_len
        self.tokens = _load_data_shard(self.files[self.current_shard], self.files_num_tokens[self.current_shard])

    def next_batch(self):
        batch_size = self.seq_len * self.num_processes
        buf = self.tokens[self.current_position:self.current_position+self.seq_len+1]
        # host side async is sufficient;
        # no performance improvement was observed when introducing a separate stream.
        inputs = buf[:-1].to(device="cuda", dtype=torch.int32, non_blocking=True) # inputs
        targets = buf[1:].to(device="cuda", dtype=torch.int64, non_blocking=True) # targets
        # advance current position and load next shard if necessary
        self.current_position += batch_size
        if self.current_position + batch_size + 1 >= len(self.tokens):
            self.advance()
        return inputs, targets

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data hyperparams
    input_bin : str = 'data/fineweb10B/fineweb_train_*.bin' # input .bin to train on
    input_val_bin : str = 'data/fineweb10B/fineweb_val_*.bin' # input .bin to eval validation loss on
    # optimization hyperparams
    batch_size : int = 8 # batch size, in sequences, across all devices
    sequence_length : int = 64*1024 # sequence length, in tokens
    num_iterations : int = 1480 # number of iterations to run
    warmup_iters : int = 0
    cooldown_iters : int = 600 # number of iterations of linear warmup/cooldown for triangular or trapezoidal schedule
    weight_decay : float = 0
    # evaluation and logging hyperparams
    val_loss_every : int = 125 # every how many steps to evaluate val loss? 0 for only at the end
    val_tokens : int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    save_every : int = 0 # every how many steps to save the checkpoint? 0 for only at the end
args = Hyperparameters()

# set up DDP (distributed data parallel). torchrun sets this env variable
ddp_rank = int(os.environ['RANK'])
ddp_local_rank = int(os.environ['LOCAL_RANK'])
ddp_world_size = int(os.environ['WORLD_SIZE'])
assert torch.cuda.is_available()
device = torch.device(f"cuda:{ddp_local_rank}")
torch.cuda.set_device(device)
print(f"using device: {device}")
dist.init_process_group(backend='nccl', device_id=device)
dist.barrier()
master_process = (ddp_rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = uuid.uuid4()
    logdir = Path("logs") / f"{run_id}"
    logdir.mkdir(exist_ok=True)
    logfile = Path("logs") / f"{run_id}.txt"
    print(logfile.stem)
    # create the log file
    with logfile.open("w") as f:
        # begin the log by printing this file (the Python code)
        print(code, file=f)
        print("=" * 100, file=f)
def print0(s, logonly=False):
    if master_process:
        with logfile.open("a") as f:
            if not logonly:
                print(s)
            print(s, file=f)
# log information about the hardware/software environment this is running on
# and print the full `nvidia-smi` to file
print0(f"Running python {sys.version}")
print0(f"Running pytorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}\nnvidia-smi:")
import subprocess
result = subprocess.run(['nvidia-smi'], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)
print0(f'{result.stdout}', logonly=True)
print0('='*100, logonly=True)

# calculate the number of steps to take in the val loop.
assert args.val_tokens % (args.sequence_length * ddp_world_size) == 0
val_steps = args.val_tokens // (args.sequence_length * ddp_world_size)
# calculate the steps of gradient accumulation required to attain the desired global batch size.
assert args.batch_size % (ddp_world_size) == 0
train_accumulation_steps = args.batch_size // ddp_world_size

# load tokens
train_loader = DistributedDataLoader(args.input_bin, args.sequence_length, ddp_rank, ddp_world_size)
val_loader = DistributedDataLoader(args.input_val_bin, args.sequence_length, ddp_rank, ddp_world_size)
print0(f"Training DataLoader: total number of tokens: {train_loader.total_num_tokens} across {len(train_loader.files)} files")
print0(f"Validation DataLoader: total number of tokens: {val_loader.total_num_tokens} across {len(val_loader.files)} files")
print0('='*100, logonly=True)
inputs_train, targets_train = train_loader.next_batch()

# there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency. suggested to me by @Grad62304977.
# this originates from Karpathy's experiments.
num_vocab = 50304
model = GPT(GPTConfig(vocab_size=num_vocab, num_layers=12, num_heads=6, model_dim=768))
model = model.cuda().bfloat16()
for m in model.modules():
    if isinstance(m, CastedLinear):
        m.float()
config.coordinate_descent_tuning = True # suggested by @Chillee
model = torch.compile(model)
# here we wrap model into DDP container
model = DDP(model, device_ids=[ddp_local_rank], broadcast_buffers=False, gradient_as_bucket_view=True)
raw_model = model.module # always contains the "raw" unwrapped model

# init the optimizer(s)
embed_params = [*raw_model.embed.parameters(), *raw_model.value_embeds.parameters()]
optimizer1 = torch.optim.Adam(embed_params, lr=0.6, betas=(0.8, 0.95), fused=True)
optimizer2 = torch.optim.Adam([raw_model.lm_head.weight], lr=0.008, betas=(0.8, 0.95), fused=True)
params = list(raw_model.blocks.parameters())
matrix_params = [p for p in params if p.ndim == 2]
scalar_params = [p for p in params if p.ndim < 2] + [raw_model.skip_weights]
optimizer3 = Muon(matrix_params, lr=0.05, momentum=0.95)
optimizer4 = torch.optim.Adam(scalar_params, lr=0.04, betas=(0.8, 0.95), fused=True)
optimizers = [optimizer1, optimizer2, optimizer3, optimizer4]
# learning rate decay scheduler (linear warmup and cooldown)
def get_lr(it):
    assert it <= args.num_iterations
    # 1) linear warmup for warmup_iters steps
    if it < args.warmup_iters:
        return (it+1) / args.warmup_iters
    # 2) constant lr for a while
    elif it < args.num_iterations - args.cooldown_iters:
        return 1.0
    # 3) linear cooldown
    else:
        decay_ratio = (args.num_iterations - it) / args.cooldown_iters
        return decay_ratio
schedulers = [torch.optim.lr_scheduler.LambdaLR(opt, get_lr) for opt in optimizers]

sliding_window_num_blocks = torch.tensor(1, dtype=torch.int32, device="cuda")
sw_num_blocks_prev = 1
# Start training loop
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
for step in range(args.num_iterations + 1):
    last_step = (step == args.num_iterations)
    # This effectively ignores timing first 10 steps, which are slower for weird reasons.
    # Alternately, and slightly more correctly in terms of benchmarking, we could do 10
    # steps with dummy data first, and then re-initialize the model and reset the loader.
    if step == 10:
        training_time_ms = 0
        t0 = time.perf_counter()
    timed_steps = float('nan') if step <= 11 else (step - 10) + 1 # <= 11 to avoid bug in val

    # Linearly increase the sliding window size over training in chunks of 64 from 64 -> 1792. By @fernbear.bsky.social
    frac_done = step / args.num_iterations # training progress
    sw_num_blocks = int(((1 - frac_done) * 64 + frac_done * 1792 + 64) // 128)
    if sw_num_blocks != sw_num_blocks_prev:
        sliding_window_num_blocks.copy_(sw_num_blocks, non_blocking=True)
        sw_num_blocks_prev = sw_num_blocks

    # once in a while evaluate the validation dataset
    if (last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0)):
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        # run validation batches
        model.eval()
        val_loader.reset()
        val_loss = 0.0
        for _ in range(val_steps):
            with torch.no_grad():
                inputs_val, targets_val = val_loader.next_batch()
                val_loss += model(inputs_val, targets_val, sliding_window_num_blocks)
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        val_loss /= val_steps
        # log val loss to console and to logfile
        print0(f'step:{step}/{args.num_iterations} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/(timed_steps-1):.2f}ms')
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if master_process and (last_step or (args.save_every > 0 and step % args.save_every == 0)):
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        # save the state of the training process
        log = dict(step=step, code=code, model=raw_model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
        torch.save(log, 'logs/%s/state_step%06d.pt' % (run_id, step))
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    # bit confusing: we want to make sure to eval on 0th iteration
    # but also after the very last iteration. so we loop for step <= num_iterations
    # instead of just < num_iterations (one extra due to <=), only to do
    # the validation/sampling one last time, and then we break right here as we're done.
    if last_step:
        break

    # --------------- TRAINING SECTION BEGIN -----------------
    model.train()
    for i in range(1, train_accumulation_steps + 1):
        with contextlib.ExitStack() as stack:
            if i < train_accumulation_steps: # there's no need to sync gradients every accumulation step
                stack.enter_context(model.no_sync())
            if step >= 5:
                stack.enter_context(torch.compiler.set_stance(skip_guard_eval_unsafe=True))
            model(inputs_train, targets_train, sliding_window_num_blocks).backward()
            inputs_train, targets_train = train_loader.next_batch()
    if train_accumulation_steps != 1:
        for p in model.parameters():
            p.grad /= train_accumulation_steps
    # momentum warmup for Muon
    frac = min(step/300, 1)
    for group in optimizer3.param_groups:
        group['momentum'] = (1 - frac) * 0.85 + frac * 0.95
    # step the optimizers and schedulers
    for opt, sched in zip(optimizers, schedulers):
        opt.step()
        sched.step()
    # null the gradients
    model.zero_grad(set_to_none=True)
    # --------------- TRAINING SECTION END -------------------
    # everything that follows now is just diagnostics, prints, logging, etc.
    approx_time = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{args.num_iterations} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms")

print0(f"peak memory consumption: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB")

# -------------------------------------------------------------------------
# clean up nice
dist.destroy_process_group()

====================================================================================================
Running python 3.10.12 (main, Sep 11 2024, 15:47:36) [GCC 11.4.0]
Running pytorch 2.6.0.dev20241203+cu124 compiled for CUDA 12.4
nvidia-smi:
Wed Dec 11 08:22:20 2024       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.183.06             Driver Version: 535.183.06   CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:19:00.0 Off |                    0 |
| N/A   38C    P0             126W / 700W |   7084MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:3B:00.0 Off |                    0 |
| N/A   30C    P0             116W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:4C:00.0 Off |                    0 |
| N/A   29C    P0             112W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   36C    P0             114W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:9B:00.0 Off |                    0 |
| N/A   38C    P0             120W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:BB:00.0 Off |                    0 |
| N/A   30C    P0             118W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:CB:00.0 Off |                    0 |
| N/A   36C    P0             119W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:DB:00.0 Off |                    0 |
| N/A   30C    P0             118W / 700W |   3211MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
Training DataLoader: total number of tokens: 1000000000 across 10 files
Validation DataLoader: total number of tokens: 100000000 across 1 files
====================================================================================================
step:0/1480 val_loss:10.8258 train_time:0ms step_avg:nanms
step:1/1480 train_time:29267ms step_avg:nanms
step:2/1480 train_time:29763ms step_avg:nanms
step:3/1480 train_time:29886ms step_avg:nanms
step:4/1480 train_time:30023ms step_avg:nanms
step:5/1480 train_time:30164ms step_avg:nanms
step:6/1480 train_time:30306ms step_avg:nanms
step:7/1480 train_time:30448ms step_avg:nanms
step:8/1480 train_time:30589ms step_avg:nanms
step:9/1480 train_time:30734ms step_avg:nanms
step:10/1480 train_time:30878ms step_avg:nanms
step:11/1480 train_time:144ms step_avg:nanms
step:12/1480 train_time:282ms step_avg:nanms
step:13/1480 train_time:424ms step_avg:141.27ms
step:14/1480 train_time:565ms step_avg:141.33ms
step:15/1480 train_time:707ms step_avg:141.39ms
step:16/1480 train_time:849ms step_avg:141.53ms
step:17/1480 train_time:993ms step_avg:141.84ms
step:18/1480 train_time:1137ms step_avg:142.11ms
step:19/1480 train_time:1279ms step_avg:142.06ms
step:20/1480 train_time:1422ms step_avg:142.17ms
step:21/1480 train_time:1564ms step_avg:142.17ms
step:22/1480 train_time:1705ms step_avg:142.07ms
step:23/1480 train_time:1847ms step_avg:142.10ms
step:24/1480 train_time:1992ms step_avg:142.26ms
step:25/1480 train_time:2136ms step_avg:142.38ms
step:26/1480 train_time:2279ms step_avg:142.47ms
step:27/1480 train_time:2421ms step_avg:142.44ms
step:28/1480 train_time:2564ms step_avg:142.43ms
step:29/1480 train_time:2704ms step_avg:142.33ms
step:30/1480 train_time:3226ms step_avg:161.31ms
step:31/1480 train_time:3331ms step_avg:158.62ms
step:32/1480 train_time:3474ms step_avg:157.91ms
step:33/1480 train_time:3617ms step_avg:157.24ms
step:34/1480 train_time:3759ms step_avg:156.61ms
step:35/1480 train_time:3900ms step_avg:156.02ms
step:36/1480 train_time:4043ms step_avg:155.50ms
step:37/1480 train_time:4187ms step_avg:155.07ms
step:38/1480 train_time:4330ms step_avg:154.66ms
step:39/1480 train_time:4473ms step_avg:154.24ms
step:40/1480 train_time:4615ms step_avg:153.83ms
step:41/1480 train_time:4759ms step_avg:153.51ms
step:42/1480 train_time:4900ms step_avg:153.14ms
step:43/1480 train_time:5042ms step_avg:152.78ms
step:44/1480 train_time:5184ms step_avg:152.48ms
step:45/1480 train_time:5327ms step_avg:152.20ms
step:46/1480 train_time:5470ms step_avg:151.94ms
step:47/1480 train_time:5613ms step_avg:151.70ms
step:48/1480 train_time:5757ms step_avg:151.50ms
step:49/1480 train_time:5898ms step_avg:151.24ms
step:50/1480 train_time:6041ms step_avg:151.03ms
step:51/1480 train_time:6183ms step_avg:150.81ms
step:52/1480 train_time:6326ms step_avg:150.62ms
step:53/1480 train_time:6469ms step_avg:150.44ms
step:54/1480 train_time:6611ms step_avg:150.26ms
step:55/1480 train_time:6757ms step_avg:150.15ms
step:56/1480 train_time:6899ms step_avg:149.98ms
step:57/1480 train_time:7042ms step_avg:149.84ms
step:58/1480 train_time:7184ms step_avg:149.67ms
step:59/1480 train_time:7326ms step_avg:149.50ms
step:60/1480 train_time:7466ms step_avg:149.33ms
step:61/1480 train_time:7610ms step_avg:149.22ms
step:62/1480 train_time:7755ms step_avg:149.14ms
step:63/1480 train_time:7899ms step_avg:149.04ms
step:64/1480 train_time:8041ms step_avg:148.92ms
step:65/1480 train_time:8184ms step_avg:148.80ms
step:66/1480 train_time:8325ms step_avg:148.67ms
step:67/1480 train_time:8466ms step_avg:148.53ms
step:68/1480 train_time:8610ms step_avg:148.45ms
step:69/1480 train_time:8754ms step_avg:148.37ms
step:70/1480 train_time:8896ms step_avg:148.27ms
step:71/1480 train_time:9039ms step_avg:148.18ms
step:72/1480 train_time:9183ms step_avg:148.12ms
step:73/1480 train_time:9326ms step_avg:148.03ms
step:74/1480 train_time:9468ms step_avg:147.94ms
step:75/1480 train_time:9610ms step_avg:147.85ms
step:76/1480 train_time:9757ms step_avg:147.83ms
step:77/1480 train_time:9900ms step_avg:147.76ms
step:78/1480 train_time:10043ms step_avg:147.70ms
step:79/1480 train_time:10185ms step_avg:147.61ms
step:80/1480 train_time:10326ms step_avg:147.51ms
step:81/1480 train_time:10469ms step_avg:147.46ms
step:82/1480 train_time:10613ms step_avg:147.40ms
step:83/1480 train_time:10757ms step_avg:147.35ms
step:84/1480 train_time:10899ms step_avg:147.29ms
step:85/1480 train_time:11043ms step_avg:147.24ms
step:86/1480 train_time:11185ms step_avg:147.17ms
step:87/1480 train_time:11327ms step_avg:147.10ms
step:88/1480 train_time:11469ms step_avg:147.04ms
step:89/1480 train_time:11612ms step_avg:146.98ms
step:90/1480 train_time:11756ms step_avg:146.95ms
step:91/1480 train_time:11899ms step_avg:146.90ms
step:92/1480 train_time:12044ms step_avg:146.88ms
step:93/1480 train_time:12186ms step_avg:146.82ms
step:94/1480 train_time:12329ms step_avg:146.77ms
step:95/1480 train_time:12472ms step_avg:146.73ms
step:96/1480 train_time:12989ms step_avg:151.04ms
step:97/1480 train_time:13086ms step_avg:150.41ms
step:98/1480 train_time:13593ms step_avg:154.47ms
step:99/1480 train_time:13694ms step_avg:153.87ms
step:100/1480 train_time:13837ms step_avg:153.75ms
step:101/1480 train_time:13983ms step_avg:153.65ms
step:102/1480 train_time:14123ms step_avg:153.51ms
step:103/1480 train_time:14263ms step_avg:153.37ms
step:104/1480 train_time:14405ms step_avg:153.24ms
step:105/1480 train_time:14547ms step_avg:153.12ms
step:106/1480 train_time:14691ms step_avg:153.03ms
step:107/1480 train_time:14834ms step_avg:152.93ms
step:108/1480 train_time:14977ms step_avg:152.83ms
step:109/1480 train_time:15120ms step_avg:152.73ms
step:110/1480 train_time:15262ms step_avg:152.62ms
step:111/1480 train_time:15405ms step_avg:152.52ms
step:112/1480 train_time:15551ms step_avg:152.46ms
step:113/1480 train_time:15697ms step_avg:152.40ms
step:114/1480 train_time:15843ms step_avg:152.34ms
step:115/1480 train_time:15988ms step_avg:152.26ms
step:116/1480 train_time:16134ms step_avg:152.21ms
step:117/1480 train_time:16280ms step_avg:152.15ms
step:118/1480 train_time:16425ms step_avg:152.08ms
step:119/1480 train_time:16569ms step_avg:152.01ms
step:120/1480 train_time:16716ms step_avg:151.97ms
step:121/1480 train_time:16862ms step_avg:151.91ms
step:122/1480 train_time:17006ms step_avg:151.84ms
step:123/1480 train_time:17152ms step_avg:151.79ms
step:124/1480 train_time:17299ms step_avg:151.74ms
step:125/1480 train_time:17444ms step_avg:151.69ms
step:125/1480 val_loss:4.4153 train_time:17508ms step_avg:152.24ms
step:126/1480 train_time:17602ms step_avg:151.75ms
step:127/1480 train_time:17745ms step_avg:151.66ms
step:128/1480 train_time:17891ms step_avg:151.62ms
step:129/1480 train_time:18037ms step_avg:151.57ms
step:130/1480 train_time:18182ms step_avg:151.52ms
step:131/1480 train_time:18327ms step_avg:151.46ms
step:132/1480 train_time:18472ms step_avg:151.41ms
step:133/1480 train_time:18619ms step_avg:151.37ms
step:134/1480 train_time:18764ms step_avg:151.32ms
step:135/1480 train_time:18911ms step_avg:151.29ms
step:136/1480 train_time:19058ms step_avg:151.25ms
step:137/1480 train_time:19202ms step_avg:151.20ms
step:138/1480 train_time:19347ms step_avg:151.15ms
step:139/1480 train_time:19494ms step_avg:151.11ms
step:140/1480 train_time:19640ms step_avg:151.08ms
step:141/1480 train_time:19786ms step_avg:151.03ms
step:142/1480 train_time:19932ms step_avg:151.00ms
step:143/1480 train_time:20079ms step_avg:150.97ms
step:144/1480 train_time:20223ms step_avg:150.92ms
step:145/1480 train_time:20369ms step_avg:150.88ms
step:146/1480 train_time:20516ms step_avg:150.85ms
step:147/1480 train_time:20660ms step_avg:150.80ms
step:148/1480 train_time:20804ms step_avg:150.76ms
step:149/1480 train_time:20951ms step_avg:150.73ms
step:150/1480 train_time:21098ms step_avg:150.70ms
step:151/1480 train_time:21242ms step_avg:150.65ms
step:152/1480 train_time:21388ms step_avg:150.62ms
step:153/1480 train_time:21534ms step_avg:150.59ms
step:154/1480 train_time:21680ms step_avg:150.55ms
step:155/1480 train_time:21824ms step_avg:150.51ms
step:156/1480 train_time:21970ms step_avg:150.48ms
step:157/1480 train_time:22117ms step_avg:150.45ms
step:158/1480 train_time:22262ms step_avg:150.42ms
step:159/1480 train_time:22406ms step_avg:150.38ms
step:160/1480 train_time:22552ms step_avg:150.35ms
step:161/1480 train_time:22698ms step_avg:150.32ms
step:162/1480 train_time:22843ms step_avg:150.28ms
step:163/1480 train_time:22988ms step_avg:150.25ms
step:164/1480 train_time:23134ms step_avg:150.22ms
step:165/1480 train_time:23279ms step_avg:150.19ms
step:166/1480 train_time:23424ms step_avg:150.15ms
step:167/1480 train_time:23569ms step_avg:150.12ms
step:168/1480 train_time:23716ms step_avg:150.10ms
step:169/1480 train_time:23862ms step_avg:150.07ms
step:170/1480 train_time:24007ms step_avg:150.04ms
step:171/1480 train_time:24154ms step_avg:150.02ms
step:172/1480 train_time:24299ms step_avg:150.00ms
step:173/1480 train_time:24443ms step_avg:149.96ms
step:174/1480 train_time:24589ms step_avg:149.93ms
step:175/1480 train_time:24735ms step_avg:149.91ms
step:176/1480 train_time:24881ms step_avg:149.88ms
step:177/1480 train_time:25025ms step_avg:149.85ms
step:178/1480 train_time:25172ms step_avg:149.83ms
step:179/1480 train_time:25318ms step_avg:149.81ms
step:180/1480 train_time:25462ms step_avg:149.78ms
step:181/1480 train_time:25607ms step_avg:149.75ms
step:182/1480 train_time:25754ms step_avg:149.73ms
step:183/1480 train_time:25900ms step_avg:149.71ms
step:184/1480 train_time:26043ms step_avg:149.67ms
step:185/1480 train_time:26189ms step_avg:149.65ms
step:186/1480 train_time:26336ms step_avg:149.63ms
step:187/1480 train_time:26481ms step_avg:149.61ms
step:188/1480 train_time:26625ms step_avg:149.58ms
step:189/1480 train_time:26802ms step_avg:149.73ms
step:190/1480 train_time:26919ms step_avg:149.55ms
step:191/1480 train_time:27063ms step_avg:149.52ms
step:192/1480 train_time:27207ms step_avg:149.49ms
step:193/1480 train_time:27355ms step_avg:149.48ms
step:194/1480 train_time:27500ms step_avg:149.46ms
step:195/1480 train_time:27645ms step_avg:149.43ms
step:196/1480 train_time:27792ms step_avg:149.42ms
step:197/1480 train_time:27937ms step_avg:149.39ms
step:198/1480 train_time:28082ms step_avg:149.37ms
step:199/1480 train_time:28228ms step_avg:149.35ms
step:200/1480 train_time:28375ms step_avg:149.34ms
step:201/1480 train_time:28522ms step_avg:149.33ms
step:202/1480 train_time:28665ms step_avg:149.29ms
step:203/1480 train_time:28812ms step_avg:149.28ms
step:204/1480 train_time:28958ms step_avg:149.27ms
step:205/1480 train_time:29102ms step_avg:149.24ms
step:206/1480 train_time:29247ms step_avg:149.22ms
step:207/1480 train_time:29394ms step_avg:149.21ms
step:208/1480 train_time:29539ms step_avg:149.19ms
step:209/1480 train_time:29684ms step_avg:149.17ms
step:210/1480 train_time:29831ms step_avg:149.15ms
step:211/1480 train_time:29977ms step_avg:149.14ms
step:212/1480 train_time:30122ms step_avg:149.12ms
step:213/1480 train_time:30267ms step_avg:149.10ms
step:214/1480 train_time:30414ms step_avg:149.09ms
step:215/1480 train_time:30559ms step_avg:149.07ms
step:216/1480 train_time:30703ms step_avg:149.05ms
step:217/1480 train_time:30849ms step_avg:149.03ms
step:218/1480 train_time:30996ms step_avg:149.02ms
step:219/1480 train_time:31141ms step_avg:149.00ms
step:220/1480 train_time:31287ms step_avg:148.99ms
step:221/1480 train_time:31947ms step_avg:151.41ms
step:222/1480 train_time:32058ms step_avg:151.22ms
step:223/1480 train_time:32205ms step_avg:151.20ms
step:224/1480 train_time:32353ms step_avg:151.18ms
step:225/1480 train_time:32500ms step_avg:151.16ms
step:226/1480 train_time:32647ms step_avg:151.15ms
step:227/1480 train_time:32796ms step_avg:151.13ms
step:228/1480 train_time:32944ms step_avg:151.12ms
step:229/1480 train_time:33094ms step_avg:151.12ms
step:230/1480 train_time:33242ms step_avg:151.10ms
step:231/1480 train_time:33389ms step_avg:151.08ms
step:232/1480 train_time:33537ms step_avg:151.07ms
step:233/1480 train_time:33685ms step_avg:151.06ms
step:234/1480 train_time:33835ms step_avg:151.05ms
step:235/1480 train_time:33982ms step_avg:151.03ms
step:236/1480 train_time:34131ms step_avg:151.02ms
step:237/1480 train_time:34280ms step_avg:151.01ms
step:238/1480 train_time:34427ms step_avg:151.00ms
step:239/1480 train_time:34576ms step_avg:150.99ms
step:240/1480 train_time:34723ms step_avg:150.97ms
step:241/1480 train_time:34872ms step_avg:150.96ms
step:242/1480 train_time:35021ms step_avg:150.95ms
step:243/1480 train_time:35169ms step_avg:150.94ms
step:244/1480 train_time:35319ms step_avg:150.93ms
step:245/1480 train_time:35466ms step_avg:150.92ms
step:246/1480 train_time:35616ms step_avg:150.91ms
step:247/1480 train_time:35763ms step_avg:150.90ms
step:248/1480 train_time:35912ms step_avg:150.89ms
step:249/1480 train_time:36060ms step_avg:150.88ms
step:250/1480 train_time:36208ms step_avg:150.87ms
step:250/1480 val_loss:3.9977 train_time:36275ms step_avg:151.15ms
step:251/1480 train_time:36371ms step_avg:150.92ms
step:252/1480 train_time:36516ms step_avg:150.89ms
step:253/1480 train_time:36663ms step_avg:150.88ms
step:254/1480 train_time:36811ms step_avg:150.86ms
step:255/1480 train_time:36959ms step_avg:150.85ms
step:256/1480 train_time:37107ms step_avg:150.84ms
step:257/1480 train_time:37255ms step_avg:150.83ms
step:258/1480 train_time:37403ms step_avg:150.82ms
step:259/1480 train_time:37553ms step_avg:150.82ms
step:260/1480 train_time:37701ms step_avg:150.80ms
step:261/1480 train_time:37849ms step_avg:150.79ms
step:262/1480 train_time:37997ms step_avg:150.78ms
step:263/1480 train_time:38145ms step_avg:150.77ms
step:264/1480 train_time:38294ms step_avg:150.76ms
step:265/1480 train_time:38441ms step_avg:150.75ms
step:266/1480 train_time:38590ms step_avg:150.74ms
step:267/1480 train_time:38739ms step_avg:150.73ms
step:268/1480 train_time:38886ms step_avg:150.72ms
step:269/1480 train_time:39036ms step_avg:150.72ms
step:270/1480 train_time:39184ms step_avg:150.71ms
step:271/1480 train_time:39333ms step_avg:150.70ms
step:272/1480 train_time:39481ms step_avg:150.69ms
step:273/1480 train_time:39630ms step_avg:150.68ms
step:274/1480 train_time:39779ms step_avg:150.68ms
step:275/1480 train_time:39926ms step_avg:150.66ms
step:276/1480 train_time:40075ms step_avg:150.66ms
step:277/1480 train_time:40223ms step_avg:150.65ms
step:278/1480 train_time:40371ms step_avg:150.64ms
step:279/1480 train_time:40520ms step_avg:150.63ms
step:280/1480 train_time:40667ms step_avg:150.62ms
step:281/1480 train_time:40817ms step_avg:150.62ms
step:282/1480 train_time:40966ms step_avg:150.61ms
step:283/1480 train_time:41115ms step_avg:150.60ms
step:284/1480 train_time:41262ms step_avg:150.59ms
step:285/1480 train_time:41412ms step_avg:150.59ms
step:286/1480 train_time:41560ms step_avg:150.58ms
step:287/1480 train_time:41708ms step_avg:150.57ms
step:288/1480 train_time:41857ms step_avg:150.56ms
step:289/1480 train_time:42008ms step_avg:150.57ms
step:290/1480 train_time:42157ms step_avg:150.56ms
step:291/1480 train_time:42306ms step_avg:150.56ms
step:292/1480 train_time:42455ms step_avg:150.55ms
step:293/1480 train_time:42604ms step_avg:150.54ms
step:294/1480 train_time:42753ms step_avg:150.54ms
step:295/1480 train_time:42901ms step_avg:150.53ms
step:296/1480 train_time:43049ms step_avg:150.52ms
step:297/1480 train_time:43199ms step_avg:150.52ms
step:298/1480 train_time:43348ms step_avg:150.51ms
step:299/1480 train_time:43497ms step_avg:150.51ms
step:300/1480 train_time:43646ms step_avg:150.50ms
step:301/1480 train_time:43797ms step_avg:150.51ms
step:302/1480 train_time:43942ms step_avg:150.49ms
step:303/1480 train_time:44092ms step_avg:150.49ms
step:304/1480 train_time:44241ms step_avg:150.48ms
step:305/1480 train_time:44390ms step_avg:150.47ms
step:306/1480 train_time:44539ms step_avg:150.47ms
step:307/1480 train_time:44686ms step_avg:150.46ms
step:308/1480 train_time:44835ms step_avg:150.45ms
step:309/1480 train_time:44984ms step_avg:150.45ms
step:310/1480 train_time:45134ms step_avg:150.45ms
step:311/1480 train_time:45281ms step_avg:150.44ms
step:312/1480 train_time:45430ms step_avg:150.43ms
step:313/1480 train_time:45580ms step_avg:150.43ms
step:314/1480 train_time:45727ms step_avg:150.42ms
step:315/1480 train_time:45876ms step_avg:150.41ms
step:316/1480 train_time:46024ms step_avg:150.40ms
step:317/1480 train_time:46174ms step_avg:150.40ms
step:318/1480 train_time:46321ms step_avg:150.39ms
step:319/1480 train_time:46471ms step_avg:150.39ms
step:320/1480 train_time:46620ms step_avg:150.39ms
step:321/1480 train_time:46768ms step_avg:150.38ms
step:322/1480 train_time:46917ms step_avg:150.38ms
step:323/1480 train_time:47065ms step_avg:150.37ms
step:324/1480 train_time:47214ms step_avg:150.36ms
step:325/1480 train_time:47362ms step_avg:150.36ms
step:326/1480 train_time:47511ms step_avg:150.35ms
step:327/1480 train_time:47660ms step_avg:150.35ms
step:328/1480 train_time:47810ms step_avg:150.34ms
step:329/1480 train_time:47958ms step_avg:150.34ms
step:330/1480 train_time:48108ms step_avg:150.34ms
step:331/1480 train_time:48258ms step_avg:150.34ms
step:332/1480 train_time:48410ms step_avg:150.34ms
step:333/1480 train_time:48562ms step_avg:150.35ms
step:334/1480 train_time:48712ms step_avg:150.35ms
step:335/1480 train_time:48863ms step_avg:150.35ms
step:336/1480 train_time:49015ms step_avg:150.35ms
step:337/1480 train_time:49165ms step_avg:150.35ms
step:338/1480 train_time:49317ms step_avg:150.36ms
step:339/1480 train_time:49467ms step_avg:150.35ms
step:340/1480 train_time:49618ms step_avg:150.36ms
step:341/1480 train_time:49768ms step_avg:150.36ms
step:342/1480 train_time:49919ms step_avg:150.36ms
step:343/1480 train_time:50070ms step_avg:150.36ms
step:344/1480 train_time:50221ms step_avg:150.36ms
step:345/1480 train_time:50372ms step_avg:150.37ms
step:346/1480 train_time:50523ms step_avg:150.37ms
step:347/1480 train_time:50674ms step_avg:150.37ms
step:348/1480 train_time:50825ms step_avg:150.37ms
step:349/1480 train_time:50977ms step_avg:150.37ms
step:350/1480 train_time:51126ms step_avg:150.37ms
step:351/1480 train_time:51278ms step_avg:150.38ms
step:352/1480 train_time:51427ms step_avg:150.37ms
step:353/1480 train_time:51578ms step_avg:150.37ms
step:354/1480 train_time:51728ms step_avg:150.37ms
step:355/1480 train_time:51880ms step_avg:150.38ms
step:356/1480 train_time:52029ms step_avg:150.37ms
step:357/1480 train_time:52180ms step_avg:150.38ms
step:358/1480 train_time:52330ms step_avg:150.37ms
step:359/1480 train_time:52482ms step_avg:150.38ms
step:360/1480 train_time:52633ms step_avg:150.38ms
step:361/1480 train_time:52785ms step_avg:150.38ms
step:362/1480 train_time:52937ms step_avg:150.39ms
step:363/1480 train_time:53088ms step_avg:150.39ms
step:364/1480 train_time:53239ms step_avg:150.39ms
step:365/1480 train_time:53391ms step_avg:150.40ms
step:366/1480 train_time:53542ms step_avg:150.40ms
step:367/1480 train_time:53693ms step_avg:150.40ms
step:368/1480 train_time:53843ms step_avg:150.40ms
step:369/1480 train_time:53995ms step_avg:150.40ms
step:370/1480 train_time:54145ms step_avg:150.40ms
step:371/1480 train_time:54297ms step_avg:150.41ms
step:372/1480 train_time:54447ms step_avg:150.41ms
step:373/1480 train_time:54599ms step_avg:150.41ms
step:374/1480 train_time:54748ms step_avg:150.41ms
step:375/1480 train_time:54900ms step_avg:150.41ms
step:375/1480 val_loss:3.8157 train_time:54967ms step_avg:150.60ms
step:376/1480 train_time:55068ms step_avg:150.46ms
step:377/1480 train_time:55209ms step_avg:150.43ms
step:378/1480 train_time:55361ms step_avg:150.44ms
step:379/1480 train_time:55531ms step_avg:150.49ms
step:380/1480 train_time:55662ms step_avg:150.44ms
step:381/1480 train_time:55811ms step_avg:150.43ms
step:382/1480 train_time:55962ms step_avg:150.44ms
step:383/1480 train_time:56114ms step_avg:150.44ms
step:384/1480 train_time:56266ms step_avg:150.44ms
step:385/1480 train_time:56418ms step_avg:150.45ms
step:386/1480 train_time:56567ms step_avg:150.45ms
step:387/1480 train_time:56719ms step_avg:150.45ms
step:388/1480 train_time:56869ms step_avg:150.45ms
step:389/1480 train_time:57020ms step_avg:150.45ms
step:390/1480 train_time:57170ms step_avg:150.45ms
step:391/1480 train_time:57322ms step_avg:150.45ms
step:392/1480 train_time:57474ms step_avg:150.46ms
step:393/1480 train_time:57625ms step_avg:150.46ms
step:394/1480 train_time:57777ms step_avg:150.46ms
step:395/1480 train_time:57926ms step_avg:150.46ms
step:396/1480 train_time:58078ms step_avg:150.46ms
step:397/1480 train_time:58228ms step_avg:150.46ms
step:398/1480 train_time:58381ms step_avg:150.47ms
step:399/1480 train_time:58530ms step_avg:150.46ms
step:400/1480 train_time:58683ms step_avg:150.47ms
step:401/1480 train_time:58833ms step_avg:150.47ms
step:402/1480 train_time:58984ms step_avg:150.47ms
step:403/1480 train_time:59134ms step_avg:150.47ms
step:404/1480 train_time:59285ms step_avg:150.47ms
step:405/1480 train_time:59436ms step_avg:150.47ms
step:406/1480 train_time:59587ms step_avg:150.47ms
step:407/1480 train_time:59738ms step_avg:150.47ms
step:408/1480 train_time:59888ms step_avg:150.47ms
step:409/1480 train_time:60039ms step_avg:150.47ms
step:410/1480 train_time:60189ms step_avg:150.47ms
step:411/1480 train_time:60341ms step_avg:150.48ms
step:412/1480 train_time:60491ms step_avg:150.47ms
step:413/1480 train_time:60642ms step_avg:150.48ms
step:414/1480 train_time:60793ms step_avg:150.48ms
step:415/1480 train_time:60945ms step_avg:150.48ms
step:416/1480 train_time:61096ms step_avg:150.48ms
step:417/1480 train_time:61246ms step_avg:150.48ms
step:418/1480 train_time:61398ms step_avg:150.48ms
step:419/1480 train_time:61548ms step_avg:150.48ms
step:420/1480 train_time:61700ms step_avg:150.49ms
step:421/1480 train_time:61849ms step_avg:150.49ms
step:422/1480 train_time:62000ms step_avg:150.49ms
step:423/1480 train_time:62150ms step_avg:150.48ms
step:424/1480 train_time:62302ms step_avg:150.49ms
step:425/1480 train_time:62452ms step_avg:150.49ms
step:426/1480 train_time:62603ms step_avg:150.49ms
step:427/1480 train_time:62753ms step_avg:150.49ms
step:428/1480 train_time:62905ms step_avg:150.49ms
step:429/1480 train_time:63055ms step_avg:150.49ms
step:430/1480 train_time:63206ms step_avg:150.49ms
step:431/1480 train_time:63358ms step_avg:150.49ms
step:432/1480 train_time:63508ms step_avg:150.49ms
step:433/1480 train_time:63659ms step_avg:150.50ms
step:434/1480 train_time:63810ms step_avg:150.50ms
step:435/1480 train_time:63961ms step_avg:150.50ms
step:436/1480 train_time:64112ms step_avg:150.50ms
step:437/1480 train_time:64263ms step_avg:150.50ms
step:438/1480 train_time:64413ms step_avg:150.50ms
step:439/1480 train_time:64565ms step_avg:150.50ms
step:440/1480 train_time:64718ms step_avg:150.51ms
step:441/1480 train_time:64870ms step_avg:150.51ms
step:442/1480 train_time:65023ms step_avg:150.52ms
step:443/1480 train_time:65176ms step_avg:150.52ms
step:444/1480 train_time:65328ms step_avg:150.53ms
step:445/1480 train_time:65481ms step_avg:150.53ms
step:446/1480 train_time:65633ms step_avg:150.54ms
step:447/1480 train_time:65786ms step_avg:150.54ms
step:448/1480 train_time:65939ms step_avg:150.55ms
step:449/1480 train_time:66091ms step_avg:150.55ms
step:450/1480 train_time:66245ms step_avg:150.56ms
step:451/1480 train_time:66398ms step_avg:150.56ms
step:452/1480 train_time:66550ms step_avg:150.57ms
step:453/1480 train_time:66704ms step_avg:150.57ms
step:454/1480 train_time:66856ms step_avg:150.58ms
step:455/1480 train_time:67008ms step_avg:150.58ms
step:456/1480 train_time:67161ms step_avg:150.58ms
step:457/1480 train_time:67315ms step_avg:150.59ms
step:458/1480 train_time:67468ms step_avg:150.60ms
step:459/1480 train_time:67621ms step_avg:150.60ms
step:460/1480 train_time:67774ms step_avg:150.61ms
step:461/1480 train_time:67926ms step_avg:150.61ms
step:462/1480 train_time:68079ms step_avg:150.62ms
step:463/1480 train_time:68231ms step_avg:150.62ms
step:464/1480 train_time:68384ms step_avg:150.63ms
step:465/1480 train_time:68537ms step_avg:150.63ms
step:466/1480 train_time:68689ms step_avg:150.63ms
step:467/1480 train_time:68843ms step_avg:150.64ms
step:468/1480 train_time:68996ms step_avg:150.65ms
step:469/1480 train_time:69149ms step_avg:150.65ms
step:470/1480 train_time:69303ms step_avg:150.66ms
step:471/1480 train_time:69455ms step_avg:150.66ms
step:472/1480 train_time:69607ms step_avg:150.66ms
step:473/1480 train_time:69759ms step_avg:150.67ms
step:474/1480 train_time:69911ms step_avg:150.67ms
step:475/1480 train_time:70064ms step_avg:150.68ms
step:476/1480 train_time:70218ms step_avg:150.68ms
step:477/1480 train_time:70371ms step_avg:150.69ms
step:478/1480 train_time:70524ms step_avg:150.69ms
step:479/1480 train_time:70676ms step_avg:150.70ms
step:480/1480 train_time:70828ms step_avg:150.70ms
step:481/1480 train_time:70982ms step_avg:150.70ms
step:482/1480 train_time:71134ms step_avg:150.71ms
step:483/1480 train_time:71285ms step_avg:150.71ms
step:484/1480 train_time:71439ms step_avg:150.72ms
step:485/1480 train_time:71592ms step_avg:150.72ms
step:486/1480 train_time:71745ms step_avg:150.73ms
step:487/1480 train_time:71899ms step_avg:150.73ms
step:488/1480 train_time:72051ms step_avg:150.73ms
step:489/1480 train_time:72205ms step_avg:150.74ms
step:490/1480 train_time:72357ms step_avg:150.74ms
step:491/1480 train_time:72509ms step_avg:150.75ms
step:492/1480 train_time:72663ms step_avg:150.75ms
step:493/1480 train_time:72817ms step_avg:150.76ms
step:494/1480 train_time:72970ms step_avg:150.76ms
step:495/1480 train_time:73124ms step_avg:150.77ms
step:496/1480 train_time:73277ms step_avg:150.78ms
step:497/1480 train_time:73429ms step_avg:150.78ms
step:498/1480 train_time:73582ms step_avg:150.78ms
step:499/1480 train_time:73735ms step_avg:150.79ms
step:500/1480 train_time:73888ms step_avg:150.79ms
step:500/1480 val_loss:3.6921 train_time:73957ms step_avg:150.93ms
step:501/1480 train_time:74052ms step_avg:150.82ms
step:502/1480 train_time:74201ms step_avg:150.82ms
step:503/1480 train_time:74354ms step_avg:150.82ms
step:504/1480 train_time:74505ms step_avg:150.82ms
step:505/1480 train_time:74658ms step_avg:150.82ms
step:506/1480 train_time:74810ms step_avg:150.83ms
step:507/1480 train_time:74963ms step_avg:150.83ms
step:508/1480 train_time:75117ms step_avg:150.84ms
step:509/1480 train_time:75271ms step_avg:150.84ms
step:510/1480 train_time:75424ms step_avg:150.85ms
step:511/1480 train_time:75578ms step_avg:150.85ms
step:512/1480 train_time:75731ms step_avg:150.86ms
step:513/1480 train_time:75883ms step_avg:150.86ms
step:514/1480 train_time:76037ms step_avg:150.87ms
step:515/1480 train_time:76189ms step_avg:150.87ms
step:516/1480 train_time:76342ms step_avg:150.87ms
step:517/1480 train_time:76496ms step_avg:150.88ms
step:518/1480 train_time:76650ms step_avg:150.88ms
step:519/1480 train_time:76802ms step_avg:150.89ms
step:520/1480 train_time:76955ms step_avg:150.89ms
step:521/1480 train_time:77108ms step_avg:150.90ms
step:522/1480 train_time:77261ms step_avg:150.90ms
step:523/1480 train_time:77415ms step_avg:150.91ms
step:524/1480 train_time:77568ms step_avg:150.91ms
step:525/1480 train_time:77720ms step_avg:150.91ms
step:526/1480 train_time:77874ms step_avg:150.92ms
step:527/1480 train_time:78025ms step_avg:150.92ms
step:528/1480 train_time:78178ms step_avg:150.92ms
step:529/1480 train_time:78331ms step_avg:150.93ms
step:530/1480 train_time:78483ms step_avg:150.93ms
step:531/1480 train_time:78636ms step_avg:150.93ms
step:532/1480 train_time:78790ms step_avg:150.94ms
step:533/1480 train_time:78943ms step_avg:150.94ms
step:534/1480 train_time:79096ms step_avg:150.95ms
step:535/1480 train_time:79248ms step_avg:150.95ms
step:536/1480 train_time:79402ms step_avg:150.95ms
step:537/1480 train_time:79554ms step_avg:150.96ms
step:538/1480 train_time:79709ms step_avg:150.96ms
step:539/1480 train_time:79864ms step_avg:150.97ms
step:540/1480 train_time:80018ms step_avg:150.98ms
step:541/1480 train_time:80170ms step_avg:150.98ms
step:542/1480 train_time:80322ms step_avg:150.98ms
step:543/1480 train_time:80476ms step_avg:150.99ms
step:544/1480 train_time:80628ms step_avg:150.99ms
step:545/1480 train_time:80781ms step_avg:150.99ms
step:546/1480 train_time:80934ms step_avg:151.00ms
step:547/1480 train_time:81088ms step_avg:151.00ms
step:548/1480 train_time:81242ms step_avg:151.01ms
step:549/1480 train_time:81395ms step_avg:151.01ms
step:550/1480 train_time:81548ms step_avg:151.02ms
step:551/1480 train_time:81703ms step_avg:151.02ms
step:552/1480 train_time:81857ms step_avg:151.03ms
step:553/1480 train_time:82013ms step_avg:151.04ms
step:554/1480 train_time:82167ms step_avg:151.04ms
step:555/1480 train_time:82322ms step_avg:151.05ms
step:556/1480 train_time:82476ms step_avg:151.05ms
step:557/1480 train_time:82630ms step_avg:151.06ms
step:558/1480 train_time:82785ms step_avg:151.07ms
step:559/1480 train_time:82940ms step_avg:151.07ms
step:560/1480 train_time:83094ms step_avg:151.08ms
step:561/1480 train_time:83249ms step_avg:151.09ms
step:562/1480 train_time:83402ms step_avg:151.09ms
step:563/1480 train_time:83557ms step_avg:151.10ms
step:564/1480 train_time:83713ms step_avg:151.11ms
step:565/1480 train_time:83868ms step_avg:151.11ms
step:566/1480 train_time:84023ms step_avg:151.12ms
step:567/1480 train_time:84177ms step_avg:151.13ms
step:568/1480 train_time:84331ms step_avg:151.13ms
step:569/1480 train_time:84503ms step_avg:151.17ms
step:570/1480 train_time:84640ms step_avg:151.14ms
step:571/1480 train_time:84795ms step_avg:151.15ms
step:572/1480 train_time:84949ms step_avg:151.16ms
step:573/1480 train_time:85104ms step_avg:151.16ms
step:574/1480 train_time:85261ms step_avg:151.17ms
step:575/1480 train_time:85416ms step_avg:151.18ms
step:576/1480 train_time:85570ms step_avg:151.18ms
step:577/1480 train_time:85724ms step_avg:151.19ms
step:578/1480 train_time:85879ms step_avg:151.19ms
step:579/1480 train_time:86033ms step_avg:151.20ms
step:580/1480 train_time:86188ms step_avg:151.21ms
step:581/1480 train_time:86342ms step_avg:151.21ms
step:582/1480 train_time:86498ms step_avg:151.22ms
step:583/1480 train_time:86651ms step_avg:151.22ms
step:584/1480 train_time:86806ms step_avg:151.23ms
step:585/1480 train_time:86961ms step_avg:151.24ms
step:586/1480 train_time:87116ms step_avg:151.24ms
step:587/1480 train_time:87271ms step_avg:151.25ms
step:588/1480 train_time:87425ms step_avg:151.25ms
step:589/1480 train_time:87580ms step_avg:151.26ms
step:590/1480 train_time:87734ms step_avg:151.27ms
step:591/1480 train_time:87889ms step_avg:151.27ms
step:592/1480 train_time:88044ms step_avg:151.28ms
step:593/1480 train_time:88199ms step_avg:151.29ms
step:594/1480 train_time:88353ms step_avg:151.29ms
step:595/1480 train_time:88510ms step_avg:151.30ms
step:596/1480 train_time:88668ms step_avg:151.31ms
step:597/1480 train_time:88822ms step_avg:151.32ms
step:598/1480 train_time:88977ms step_avg:151.32ms
step:599/1480 train_time:89131ms step_avg:151.33ms
step:600/1480 train_time:89286ms step_avg:151.33ms
step:601/1480 train_time:89442ms step_avg:151.34ms
step:602/1480 train_time:89597ms step_avg:151.35ms
step:603/1480 train_time:89751ms step_avg:151.35ms
step:604/1480 train_time:89904ms step_avg:151.35ms
step:605/1480 train_time:90059ms step_avg:151.36ms
step:606/1480 train_time:90215ms step_avg:151.37ms
step:607/1480 train_time:90371ms step_avg:151.38ms
step:608/1480 train_time:90525ms step_avg:151.38ms
step:609/1480 train_time:90680ms step_avg:151.39ms
step:610/1480 train_time:90835ms step_avg:151.39ms
step:611/1480 train_time:90990ms step_avg:151.40ms
step:612/1480 train_time:91144ms step_avg:151.40ms
step:613/1480 train_time:91299ms step_avg:151.41ms
step:614/1480 train_time:91453ms step_avg:151.41ms
step:615/1480 train_time:91608ms step_avg:151.42ms
step:616/1480 train_time:91763ms step_avg:151.42ms
step:617/1480 train_time:91919ms step_avg:151.43ms
step:618/1480 train_time:92072ms step_avg:151.43ms
step:619/1480 train_time:92228ms step_avg:151.44ms
step:620/1480 train_time:92383ms step_avg:151.45ms
step:621/1480 train_time:92539ms step_avg:151.45ms
step:622/1480 train_time:92694ms step_avg:151.46ms
step:623/1480 train_time:92850ms step_avg:151.47ms
step:624/1480 train_time:93005ms step_avg:151.47ms
step:625/1480 train_time:93159ms step_avg:151.48ms
step:625/1480 val_loss:3.6105 train_time:93230ms step_avg:151.59ms
step:626/1480 train_time:93324ms step_avg:151.50ms
step:627/1480 train_time:93474ms step_avg:151.50ms
step:628/1480 train_time:93629ms step_avg:151.50ms
step:629/1480 train_time:93783ms step_avg:151.51ms
step:630/1480 train_time:93936ms step_avg:151.51ms
step:631/1480 train_time:94090ms step_avg:151.51ms
step:632/1480 train_time:94245ms step_avg:151.52ms
step:633/1480 train_time:94400ms step_avg:151.52ms
step:634/1480 train_time:94555ms step_avg:151.53ms
step:635/1480 train_time:94708ms step_avg:151.53ms
step:636/1480 train_time:94863ms step_avg:151.54ms
step:637/1480 train_time:95017ms step_avg:151.54ms
step:638/1480 train_time:95171ms step_avg:151.55ms
step:639/1480 train_time:95326ms step_avg:151.55ms
step:640/1480 train_time:95480ms step_avg:151.55ms
step:641/1480 train_time:95634ms step_avg:151.56ms
step:642/1480 train_time:95789ms step_avg:151.56ms
step:643/1480 train_time:95943ms step_avg:151.57ms
step:644/1480 train_time:96098ms step_avg:151.57ms
step:645/1480 train_time:96253ms step_avg:151.58ms
step:646/1480 train_time:96408ms step_avg:151.59ms
step:647/1480 train_time:96563ms step_avg:151.59ms
step:648/1480 train_time:96719ms step_avg:151.60ms
step:649/1480 train_time:96874ms step_avg:151.60ms
step:650/1480 train_time:97029ms step_avg:151.61ms
step:651/1480 train_time:97184ms step_avg:151.61ms
step:652/1480 train_time:97338ms step_avg:151.62ms
step:653/1480 train_time:97493ms step_avg:151.62ms
step:654/1480 train_time:97648ms step_avg:151.63ms
step:655/1480 train_time:97802ms step_avg:151.63ms
step:656/1480 train_time:97957ms step_avg:151.64ms
step:657/1480 train_time:98111ms step_avg:151.64ms
step:658/1480 train_time:98266ms step_avg:151.65ms
step:659/1480 train_time:98422ms step_avg:151.65ms
step:660/1480 train_time:98578ms step_avg:151.66ms
step:661/1480 train_time:98734ms step_avg:151.67ms
step:662/1480 train_time:98891ms step_avg:151.67ms
step:663/1480 train_time:99046ms step_avg:151.68ms
step:664/1480 train_time:99202ms step_avg:151.68ms
step:665/1480 train_time:99358ms step_avg:151.69ms
step:666/1480 train_time:99514ms step_avg:151.70ms
step:667/1480 train_time:99671ms step_avg:151.71ms
step:668/1480 train_time:99827ms step_avg:151.71ms
step:669/1480 train_time:99985ms step_avg:151.72ms
step:670/1480 train_time:100142ms step_avg:151.73ms
step:671/1480 train_time:100298ms step_avg:151.74ms
step:672/1480 train_time:100454ms step_avg:151.74ms
step:673/1480 train_time:100610ms step_avg:151.75ms
step:674/1480 train_time:100766ms step_avg:151.76ms
step:675/1480 train_time:100924ms step_avg:151.76ms
step:676/1480 train_time:101079ms step_avg:151.77ms
step:677/1480 train_time:101236ms step_avg:151.78ms
step:678/1480 train_time:101393ms step_avg:151.79ms
step:679/1480 train_time:101548ms step_avg:151.79ms
step:680/1480 train_time:101706ms step_avg:151.80ms
step:681/1480 train_time:101862ms step_avg:151.81ms
step:682/1480 train_time:102020ms step_avg:151.81ms
step:683/1480 train_time:102176ms step_avg:151.82ms
step:684/1480 train_time:102332ms step_avg:151.83ms
step:685/1480 train_time:102490ms step_avg:151.84ms
step:686/1480 train_time:102648ms step_avg:151.85ms
step:687/1480 train_time:102803ms step_avg:151.85ms
step:688/1480 train_time:102960ms step_avg:151.86ms
step:689/1480 train_time:103119ms step_avg:151.87ms
step:690/1480 train_time:103276ms step_avg:151.88ms
step:691/1480 train_time:103431ms step_avg:151.88ms
step:692/1480 train_time:103587ms step_avg:151.89ms
step:693/1480 train_time:103745ms step_avg:151.90ms
step:694/1480 train_time:103900ms step_avg:151.90ms
step:695/1480 train_time:104056ms step_avg:151.91ms
step:696/1480 train_time:104211ms step_avg:151.91ms
step:697/1480 train_time:104368ms step_avg:151.92ms
step:698/1480 train_time:104524ms step_avg:151.92ms
step:699/1480 train_time:104680ms step_avg:151.93ms
step:700/1480 train_time:104836ms step_avg:151.94ms
step:701/1480 train_time:104993ms step_avg:151.94ms
step:702/1480 train_time:105149ms step_avg:151.95ms
step:703/1480 train_time:105305ms step_avg:151.95ms
step:704/1480 train_time:105460ms step_avg:151.96ms
step:705/1480 train_time:105617ms step_avg:151.97ms
step:706/1480 train_time:105774ms step_avg:151.97ms
step:707/1480 train_time:105930ms step_avg:151.98ms
step:708/1480 train_time:106087ms step_avg:151.99ms
step:709/1480 train_time:106242ms step_avg:151.99ms
step:710/1480 train_time:106398ms step_avg:152.00ms
step:711/1480 train_time:106555ms step_avg:152.00ms
step:712/1480 train_time:106713ms step_avg:152.01ms
step:713/1480 train_time:106870ms step_avg:152.02ms
step:714/1480 train_time:107027ms step_avg:152.03ms
step:715/1480 train_time:107183ms step_avg:152.03ms
step:716/1480 train_time:107339ms step_avg:152.04ms
step:717/1480 train_time:107497ms step_avg:152.05ms
step:718/1480 train_time:107652ms step_avg:152.05ms
step:719/1480 train_time:107807ms step_avg:152.05ms
step:720/1480 train_time:107965ms step_avg:152.06ms
step:721/1480 train_time:108123ms step_avg:152.07ms
step:722/1480 train_time:108280ms step_avg:152.08ms
step:723/1480 train_time:108436ms step_avg:152.08ms
step:724/1480 train_time:108592ms step_avg:152.09ms
step:725/1480 train_time:108748ms step_avg:152.10ms
step:726/1480 train_time:108904ms step_avg:152.10ms
step:727/1480 train_time:109061ms step_avg:152.11ms
step:728/1480 train_time:109218ms step_avg:152.11ms
step:729/1480 train_time:109373ms step_avg:152.12ms
step:730/1480 train_time:109531ms step_avg:152.13ms
step:731/1480 train_time:109688ms step_avg:152.13ms
step:732/1480 train_time:109844ms step_avg:152.14ms
step:733/1480 train_time:110000ms step_avg:152.14ms
step:734/1480 train_time:110157ms step_avg:152.15ms
step:735/1480 train_time:110313ms step_avg:152.16ms
step:736/1480 train_time:110469ms step_avg:152.16ms
step:737/1480 train_time:110625ms step_avg:152.17ms
step:738/1480 train_time:110782ms step_avg:152.17ms
step:739/1480 train_time:110937ms step_avg:152.18ms
step:740/1480 train_time:111097ms step_avg:152.19ms
step:741/1480 train_time:111255ms step_avg:152.20ms
step:742/1480 train_time:111411ms step_avg:152.20ms
step:743/1480 train_time:111567ms step_avg:152.21ms
step:744/1480 train_time:111723ms step_avg:152.21ms
step:745/1480 train_time:111881ms step_avg:152.22ms
step:746/1480 train_time:112037ms step_avg:152.22ms
step:747/1480 train_time:112194ms step_avg:152.23ms
step:748/1480 train_time:112354ms step_avg:152.24ms
step:749/1480 train_time:112510ms step_avg:152.25ms
step:750/1480 train_time:112665ms step_avg:152.25ms
step:750/1480 val_loss:3.5553 train_time:112738ms step_avg:152.35ms
step:751/1480 train_time:112830ms step_avg:152.27ms
step:752/1480 train_time:112985ms step_avg:152.27ms
step:753/1480 train_time:113140ms step_avg:152.28ms
step:754/1480 train_time:113297ms step_avg:152.28ms
step:755/1480 train_time:113452ms step_avg:152.28ms
step:756/1480 train_time:113607ms step_avg:152.29ms
step:757/1480 train_time:113766ms step_avg:152.30ms
step:758/1480 train_time:113922ms step_avg:152.30ms
step:759/1480 train_time:114094ms step_avg:152.33ms
step:760/1480 train_time:114236ms step_avg:152.31ms
step:761/1480 train_time:114392ms step_avg:152.32ms
step:762/1480 train_time:114549ms step_avg:152.33ms
step:763/1480 train_time:114705ms step_avg:152.33ms
step:764/1480 train_time:114862ms step_avg:152.34ms
step:765/1480 train_time:115020ms step_avg:152.34ms
step:766/1480 train_time:115177ms step_avg:152.35ms
step:767/1480 train_time:115334ms step_avg:152.36ms
step:768/1480 train_time:115490ms step_avg:152.36ms
step:769/1480 train_time:115648ms step_avg:152.37ms
step:770/1480 train_time:115804ms step_avg:152.37ms
step:771/1480 train_time:115963ms step_avg:152.38ms
step:772/1480 train_time:116120ms step_avg:152.39ms
step:773/1480 train_time:116278ms step_avg:152.40ms
step:774/1480 train_time:116434ms step_avg:152.40ms
step:775/1480 train_time:116591ms step_avg:152.41ms
step:776/1480 train_time:116750ms step_avg:152.42ms
step:777/1480 train_time:116909ms step_avg:152.42ms
step:778/1480 train_time:117067ms step_avg:152.43ms
step:779/1480 train_time:117224ms step_avg:152.44ms
step:780/1480 train_time:117382ms step_avg:152.44ms
step:781/1480 train_time:117540ms step_avg:152.45ms
step:782/1480 train_time:117699ms step_avg:152.46ms
step:783/1480 train_time:117857ms step_avg:152.47ms
step:784/1480 train_time:118015ms step_avg:152.47ms
step:785/1480 train_time:118173ms step_avg:152.48ms
step:786/1480 train_time:118330ms step_avg:152.49ms
step:787/1480 train_time:118488ms step_avg:152.49ms
step:788/1480 train_time:118647ms step_avg:152.50ms
step:789/1480 train_time:118803ms step_avg:152.51ms
step:790/1480 train_time:118962ms step_avg:152.51ms
step:791/1480 train_time:119122ms step_avg:152.52ms
step:792/1480 train_time:119280ms step_avg:152.53ms
step:793/1480 train_time:119438ms step_avg:152.54ms
step:794/1480 train_time:119598ms step_avg:152.55ms
step:795/1480 train_time:119758ms step_avg:152.56ms
step:796/1480 train_time:119917ms step_avg:152.57ms
step:797/1480 train_time:120075ms step_avg:152.57ms
step:798/1480 train_time:120234ms step_avg:152.58ms
step:799/1480 train_time:120396ms step_avg:152.59ms
step:800/1480 train_time:120554ms step_avg:152.60ms
step:801/1480 train_time:120710ms step_avg:152.60ms
step:802/1480 train_time:120869ms step_avg:152.61ms
step:803/1480 train_time:121025ms step_avg:152.62ms
step:804/1480 train_time:121182ms step_avg:152.62ms
step:805/1480 train_time:121342ms step_avg:152.63ms
step:806/1480 train_time:121499ms step_avg:152.64ms
step:807/1480 train_time:121655ms step_avg:152.64ms
step:808/1480 train_time:121811ms step_avg:152.65ms
step:809/1480 train_time:121968ms step_avg:152.65ms
step:810/1480 train_time:122125ms step_avg:152.66ms
step:811/1480 train_time:122283ms step_avg:152.66ms
step:812/1480 train_time:122440ms step_avg:152.67ms
step:813/1480 train_time:122598ms step_avg:152.68ms
step:814/1480 train_time:122755ms step_avg:152.68ms
step:815/1480 train_time:122912ms step_avg:152.69ms
step:816/1480 train_time:123070ms step_avg:152.69ms
step:817/1480 train_time:123228ms step_avg:152.70ms
step:818/1480 train_time:123384ms step_avg:152.70ms
step:819/1480 train_time:123542ms step_avg:152.71ms
step:820/1480 train_time:123700ms step_avg:152.72ms
step:821/1480 train_time:123856ms step_avg:152.72ms
step:822/1480 train_time:124014ms step_avg:152.73ms
step:823/1480 train_time:124171ms step_avg:152.73ms
step:824/1480 train_time:124328ms step_avg:152.74ms
step:825/1480 train_time:124487ms step_avg:152.74ms
step:826/1480 train_time:124647ms step_avg:152.75ms
step:827/1480 train_time:124804ms step_avg:152.76ms
step:828/1480 train_time:124964ms step_avg:152.77ms
step:829/1480 train_time:125123ms step_avg:152.77ms
step:830/1480 train_time:125282ms step_avg:152.78ms
step:831/1480 train_time:125440ms step_avg:152.79ms
step:832/1480 train_time:125599ms step_avg:152.80ms
step:833/1480 train_time:125758ms step_avg:152.80ms
step:834/1480 train_time:125917ms step_avg:152.81ms
step:835/1480 train_time:126073ms step_avg:152.82ms
step:836/1480 train_time:126232ms step_avg:152.82ms
step:837/1480 train_time:126389ms step_avg:152.83ms
step:838/1480 train_time:126546ms step_avg:152.83ms
step:839/1480 train_time:126703ms step_avg:152.84ms
step:840/1480 train_time:126862ms step_avg:152.85ms
step:841/1480 train_time:127019ms step_avg:152.85ms
step:842/1480 train_time:127176ms step_avg:152.86ms
step:843/1480 train_time:127333ms step_avg:152.86ms
step:844/1480 train_time:127489ms step_avg:152.86ms
step:845/1480 train_time:127647ms step_avg:152.87ms
step:846/1480 train_time:127805ms step_avg:152.88ms
step:847/1480 train_time:127965ms step_avg:152.88ms
step:848/1480 train_time:128122ms step_avg:152.89ms
step:849/1480 train_time:128281ms step_avg:152.90ms
step:850/1480 train_time:128439ms step_avg:152.90ms
step:851/1480 train_time:128601ms step_avg:152.91ms
step:852/1480 train_time:128759ms step_avg:152.92ms
step:853/1480 train_time:128917ms step_avg:152.93ms
step:854/1480 train_time:129074ms step_avg:152.93ms
step:855/1480 train_time:129232ms step_avg:152.94ms
step:856/1480 train_time:129389ms step_avg:152.94ms
step:857/1480 train_time:129548ms step_avg:152.95ms
step:858/1480 train_time:129707ms step_avg:152.96ms
step:859/1480 train_time:129866ms step_avg:152.96ms
step:860/1480 train_time:130023ms step_avg:152.97ms
step:861/1480 train_time:130182ms step_avg:152.98ms
step:862/1480 train_time:130346ms step_avg:152.99ms
step:863/1480 train_time:130505ms step_avg:152.99ms
step:864/1480 train_time:130664ms step_avg:153.00ms
step:865/1480 train_time:130822ms step_avg:153.01ms
step:866/1480 train_time:130983ms step_avg:153.02ms
step:867/1480 train_time:131142ms step_avg:153.03ms
step:868/1480 train_time:131300ms step_avg:153.03ms
step:869/1480 train_time:131457ms step_avg:153.04ms
step:870/1480 train_time:131616ms step_avg:153.04ms
step:871/1480 train_time:131772ms step_avg:153.05ms
step:872/1480 train_time:131931ms step_avg:153.05ms
step:873/1480 train_time:132088ms step_avg:153.06ms
step:874/1480 train_time:132247ms step_avg:153.06ms
step:875/1480 train_time:132406ms step_avg:153.07ms
step:875/1480 val_loss:3.5104 train_time:132481ms step_avg:153.16ms
step:876/1480 train_time:132578ms step_avg:153.09ms
step:877/1480 train_time:132728ms step_avg:153.09ms
step:878/1480 train_time:132885ms step_avg:153.09ms
step:879/1480 train_time:133045ms step_avg:153.10ms
step:880/1480 train_time:133203ms step_avg:153.11ms
step:881/1480 train_time:133360ms step_avg:153.11ms
step:882/1480 train_time:133519ms step_avg:153.12ms
step:883/1480 train_time:133680ms step_avg:153.13ms
step:884/1480 train_time:133840ms step_avg:153.14ms
step:885/1480 train_time:134002ms step_avg:153.14ms
step:886/1480 train_time:134161ms step_avg:153.15ms
step:887/1480 train_time:134321ms step_avg:153.16ms
step:888/1480 train_time:134486ms step_avg:153.17ms
step:889/1480 train_time:134648ms step_avg:153.18ms
step:890/1480 train_time:134806ms step_avg:153.19ms
step:891/1480 train_time:134964ms step_avg:153.19ms
step:892/1480 train_time:135124ms step_avg:153.20ms
step:893/1480 train_time:135282ms step_avg:153.21ms
step:894/1480 train_time:135442ms step_avg:153.21ms
step:895/1480 train_time:135603ms step_avg:153.22ms
step:896/1480 train_time:135761ms step_avg:153.23ms
step:897/1480 train_time:135922ms step_avg:153.24ms
step:898/1480 train_time:136084ms step_avg:153.25ms
step:899/1480 train_time:136244ms step_avg:153.26ms
step:900/1480 train_time:136402ms step_avg:153.26ms
step:901/1480 train_time:136561ms step_avg:153.27ms
step:902/1480 train_time:136719ms step_avg:153.27ms
step:903/1480 train_time:136881ms step_avg:153.28ms
step:904/1480 train_time:137041ms step_avg:153.29ms
step:905/1480 train_time:137199ms step_avg:153.29ms
step:906/1480 train_time:137360ms step_avg:153.30ms
step:907/1480 train_time:137523ms step_avg:153.31ms
step:908/1480 train_time:137680ms step_avg:153.32ms
step:909/1480 train_time:137838ms step_avg:153.32ms
step:910/1480 train_time:138003ms step_avg:153.34ms
step:911/1480 train_time:138163ms step_avg:153.34ms
step:912/1480 train_time:138324ms step_avg:153.35ms
step:913/1480 train_time:138484ms step_avg:153.36ms
step:914/1480 train_time:138644ms step_avg:153.37ms
step:915/1480 train_time:138808ms step_avg:153.38ms
step:916/1480 train_time:138966ms step_avg:153.38ms
step:917/1480 train_time:139125ms step_avg:153.39ms
step:918/1480 train_time:139287ms step_avg:153.40ms
step:919/1480 train_time:139448ms step_avg:153.41ms
step:920/1480 train_time:139607ms step_avg:153.41ms
step:921/1480 train_time:139765ms step_avg:153.42ms
step:922/1480 train_time:139927ms step_avg:153.43ms
step:923/1480 train_time:140084ms step_avg:153.43ms
step:924/1480 train_time:140244ms step_avg:153.44ms
step:925/1480 train_time:140403ms step_avg:153.45ms
step:926/1480 train_time:140561ms step_avg:153.45ms
step:927/1480 train_time:140720ms step_avg:153.46ms
step:928/1480 train_time:140879ms step_avg:153.46ms
step:929/1480 train_time:141037ms step_avg:153.47ms
step:930/1480 train_time:141197ms step_avg:153.48ms
step:931/1480 train_time:141357ms step_avg:153.48ms
step:932/1480 train_time:141516ms step_avg:153.49ms
step:933/1480 train_time:141678ms step_avg:153.50ms
step:934/1480 train_time:141836ms step_avg:153.50ms
step:935/1480 train_time:142001ms step_avg:153.51ms
step:936/1480 train_time:142159ms step_avg:153.52ms
step:937/1480 train_time:142319ms step_avg:153.53ms
step:938/1480 train_time:142477ms step_avg:153.53ms
step:939/1480 train_time:142640ms step_avg:153.54ms
step:940/1480 train_time:142803ms step_avg:153.55ms
step:941/1480 train_time:142961ms step_avg:153.56ms
step:942/1480 train_time:143120ms step_avg:153.56ms
step:943/1480 train_time:143281ms step_avg:153.57ms
step:944/1480 train_time:143443ms step_avg:153.58ms
step:945/1480 train_time:143602ms step_avg:153.58ms
step:946/1480 train_time:143764ms step_avg:153.59ms
step:947/1480 train_time:143925ms step_avg:153.60ms
step:948/1480 train_time:144084ms step_avg:153.61ms
step:949/1480 train_time:144258ms step_avg:153.63ms
step:950/1480 train_time:144403ms step_avg:153.62ms
step:951/1480 train_time:144564ms step_avg:153.63ms
step:952/1480 train_time:144723ms step_avg:153.63ms
step:953/1480 train_time:144883ms step_avg:153.64ms
step:954/1480 train_time:145043ms step_avg:153.65ms
step:955/1480 train_time:145202ms step_avg:153.65ms
step:956/1480 train_time:145361ms step_avg:153.66ms
step:957/1480 train_time:145522ms step_avg:153.67ms
step:958/1480 train_time:145686ms step_avg:153.68ms
step:959/1480 train_time:145844ms step_avg:153.68ms
step:960/1480 train_time:146004ms step_avg:153.69ms
step:961/1480 train_time:146163ms step_avg:153.69ms
step:962/1480 train_time:146323ms step_avg:153.70ms
step:963/1480 train_time:146485ms step_avg:153.71ms
step:964/1480 train_time:146646ms step_avg:153.72ms
step:965/1480 train_time:146805ms step_avg:153.72ms
step:966/1480 train_time:146963ms step_avg:153.73ms
step:967/1480 train_time:147122ms step_avg:153.73ms
step:968/1480 train_time:147281ms step_avg:153.74ms
step:969/1480 train_time:147442ms step_avg:153.75ms
step:970/1480 train_time:147601ms step_avg:153.75ms
step:971/1480 train_time:147759ms step_avg:153.76ms
step:972/1480 train_time:147917ms step_avg:153.76ms
step:973/1480 train_time:148074ms step_avg:153.76ms
step:974/1480 train_time:148234ms step_avg:153.77ms
step:975/1480 train_time:148394ms step_avg:153.78ms
step:976/1480 train_time:148554ms step_avg:153.78ms
step:977/1480 train_time:148712ms step_avg:153.79ms
step:978/1480 train_time:148871ms step_avg:153.79ms
step:979/1480 train_time:149030ms step_avg:153.80ms
step:980/1480 train_time:149189ms step_avg:153.80ms
step:981/1480 train_time:149350ms step_avg:153.81ms
step:982/1480 train_time:149508ms step_avg:153.82ms
step:983/1480 train_time:149666ms step_avg:153.82ms
step:984/1480 train_time:149826ms step_avg:153.83ms
step:985/1480 train_time:149988ms step_avg:153.83ms
step:986/1480 train_time:150148ms step_avg:153.84ms
step:987/1480 train_time:150306ms step_avg:153.84ms
step:988/1480 train_time:150464ms step_avg:153.85ms
step:989/1480 train_time:150624ms step_avg:153.85ms
step:990/1480 train_time:150786ms step_avg:153.86ms
step:991/1480 train_time:150947ms step_avg:153.87ms
step:992/1480 train_time:151112ms step_avg:153.88ms
step:993/1480 train_time:151281ms step_avg:153.90ms
step:994/1480 train_time:151440ms step_avg:153.90ms
step:995/1480 train_time:151600ms step_avg:153.91ms
step:996/1480 train_time:151758ms step_avg:153.91ms
step:997/1480 train_time:151917ms step_avg:153.92ms
step:998/1480 train_time:152076ms step_avg:153.92ms
step:999/1480 train_time:152236ms step_avg:153.93ms
step:1000/1480 train_time:152397ms step_avg:153.94ms
step:1000/1480 val_loss:3.4460 train_time:152470ms step_avg:154.01ms
step:1001/1480 train_time:152561ms step_avg:153.95ms
step:1002/1480 train_time:152721ms step_avg:153.95ms
step:1003/1480 train_time:152885ms step_avg:153.96ms
step:1004/1480 train_time:153048ms step_avg:153.97ms
step:1005/1480 train_time:153208ms step_avg:153.98ms
step:1006/1480 train_time:153370ms step_avg:153.99ms
step:1007/1480 train_time:153530ms step_avg:153.99ms
step:1008/1480 train_time:153691ms step_avg:154.00ms
step:1009/1480 train_time:153856ms step_avg:154.01ms
step:1010/1480 train_time:154015ms step_avg:154.02ms
step:1011/1480 train_time:154175ms step_avg:154.02ms
step:1012/1480 train_time:154334ms step_avg:154.03ms
step:1013/1480 train_time:154495ms step_avg:154.03ms
step:1014/1480 train_time:154656ms step_avg:154.04ms
step:1015/1480 train_time:154818ms step_avg:154.05ms
step:1016/1480 train_time:154977ms step_avg:154.05ms
step:1017/1480 train_time:155138ms step_avg:154.06ms
step:1018/1480 train_time:155298ms step_avg:154.07ms
step:1019/1480 train_time:155459ms step_avg:154.07ms
step:1020/1480 train_time:155618ms step_avg:154.08ms
step:1021/1480 train_time:155776ms step_avg:154.08ms
step:1022/1480 train_time:155936ms step_avg:154.09ms
step:1023/1480 train_time:156096ms step_avg:154.09ms
step:1024/1480 train_time:156257ms step_avg:154.10ms
step:1025/1480 train_time:156417ms step_avg:154.11ms
step:1026/1480 train_time:156576ms step_avg:154.11ms
step:1027/1480 train_time:156734ms step_avg:154.11ms
step:1028/1480 train_time:156896ms step_avg:154.12ms
step:1029/1480 train_time:157060ms step_avg:154.13ms
step:1030/1480 train_time:157220ms step_avg:154.14ms
step:1031/1480 train_time:157378ms step_avg:154.14ms
step:1032/1480 train_time:157543ms step_avg:154.15ms
step:1033/1480 train_time:157702ms step_avg:154.16ms
step:1034/1480 train_time:157865ms step_avg:154.16ms
step:1035/1480 train_time:158024ms step_avg:154.17ms
step:1036/1480 train_time:158186ms step_avg:154.18ms
step:1037/1480 train_time:158348ms step_avg:154.19ms
step:1038/1480 train_time:158508ms step_avg:154.19ms
step:1039/1480 train_time:158673ms step_avg:154.20ms
step:1040/1480 train_time:158832ms step_avg:154.21ms
step:1041/1480 train_time:158994ms step_avg:154.21ms
step:1042/1480 train_time:159152ms step_avg:154.22ms
step:1043/1480 train_time:159310ms step_avg:154.22ms
step:1044/1480 train_time:159471ms step_avg:154.23ms
step:1045/1480 train_time:159632ms step_avg:154.23ms
step:1046/1480 train_time:159791ms step_avg:154.24ms
step:1047/1480 train_time:159953ms step_avg:154.25ms
step:1048/1480 train_time:160113ms step_avg:154.25ms
step:1049/1480 train_time:160273ms step_avg:154.26ms
step:1050/1480 train_time:160436ms step_avg:154.27ms
step:1051/1480 train_time:160598ms step_avg:154.27ms
step:1052/1480 train_time:160758ms step_avg:154.28ms
step:1053/1480 train_time:160917ms step_avg:154.28ms
step:1054/1480 train_time:161078ms step_avg:154.29ms
step:1055/1480 train_time:161238ms step_avg:154.29ms
step:1056/1480 train_time:161396ms step_avg:154.30ms
step:1057/1480 train_time:161558ms step_avg:154.31ms
step:1058/1480 train_time:161719ms step_avg:154.31ms
step:1059/1480 train_time:161880ms step_avg:154.32ms
step:1060/1480 train_time:162041ms step_avg:154.32ms
step:1061/1480 train_time:162198ms step_avg:154.33ms
step:1062/1480 train_time:162358ms step_avg:154.33ms
step:1063/1480 train_time:162518ms step_avg:154.34ms
step:1064/1480 train_time:162676ms step_avg:154.34ms
step:1065/1480 train_time:162836ms step_avg:154.35ms
step:1066/1480 train_time:162997ms step_avg:154.35ms
step:1067/1480 train_time:163158ms step_avg:154.36ms
step:1068/1480 train_time:163317ms step_avg:154.36ms
step:1069/1480 train_time:163481ms step_avg:154.37ms
step:1070/1480 train_time:163640ms step_avg:154.38ms
step:1071/1480 train_time:163804ms step_avg:154.39ms
step:1072/1480 train_time:163964ms step_avg:154.39ms
step:1073/1480 train_time:164121ms step_avg:154.39ms
step:1074/1480 train_time:164280ms step_avg:154.40ms
step:1075/1480 train_time:164443ms step_avg:154.41ms
step:1076/1480 train_time:164603ms step_avg:154.41ms
step:1077/1480 train_time:164763ms step_avg:154.42ms
step:1078/1480 train_time:164930ms step_avg:154.43ms
step:1079/1480 train_time:165095ms step_avg:154.44ms
step:1080/1480 train_time:165255ms step_avg:154.44ms
step:1081/1480 train_time:165414ms step_avg:154.45ms
step:1082/1480 train_time:165575ms step_avg:154.45ms
step:1083/1480 train_time:165735ms step_avg:154.46ms
step:1084/1480 train_time:165896ms step_avg:154.47ms
step:1085/1480 train_time:166057ms step_avg:154.47ms
step:1086/1480 train_time:166217ms step_avg:154.48ms
step:1087/1480 train_time:166377ms step_avg:154.48ms
step:1088/1480 train_time:166537ms step_avg:154.49ms
step:1089/1480 train_time:166700ms step_avg:154.49ms
step:1090/1480 train_time:166862ms step_avg:154.50ms
step:1091/1480 train_time:167023ms step_avg:154.51ms
step:1092/1480 train_time:167184ms step_avg:154.51ms
step:1093/1480 train_time:167344ms step_avg:154.52ms
step:1094/1480 train_time:167504ms step_avg:154.52ms
step:1095/1480 train_time:167666ms step_avg:154.53ms
step:1096/1480 train_time:167830ms step_avg:154.54ms
step:1097/1480 train_time:167992ms step_avg:154.55ms
step:1098/1480 train_time:168155ms step_avg:154.55ms
step:1099/1480 train_time:168315ms step_avg:154.56ms
step:1100/1480 train_time:168479ms step_avg:154.57ms
step:1101/1480 train_time:168643ms step_avg:154.58ms
step:1102/1480 train_time:168805ms step_avg:154.58ms
step:1103/1480 train_time:168972ms step_avg:154.59ms
step:1104/1480 train_time:169133ms step_avg:154.60ms
step:1105/1480 train_time:169295ms step_avg:154.61ms
step:1106/1480 train_time:169456ms step_avg:154.61ms
step:1107/1480 train_time:169617ms step_avg:154.62ms
step:1108/1480 train_time:169776ms step_avg:154.62ms
step:1109/1480 train_time:169937ms step_avg:154.63ms
step:1110/1480 train_time:170096ms step_avg:154.63ms
step:1111/1480 train_time:170257ms step_avg:154.64ms
step:1112/1480 train_time:170418ms step_avg:154.64ms
step:1113/1480 train_time:170586ms step_avg:154.66ms
step:1114/1480 train_time:170750ms step_avg:154.66ms
step:1115/1480 train_time:170912ms step_avg:154.67ms
step:1116/1480 train_time:171073ms step_avg:154.68ms
step:1117/1480 train_time:171236ms step_avg:154.68ms
step:1118/1480 train_time:171399ms step_avg:154.69ms
step:1119/1480 train_time:171560ms step_avg:154.70ms
step:1120/1480 train_time:171721ms step_avg:154.70ms
step:1121/1480 train_time:171883ms step_avg:154.71ms
step:1122/1480 train_time:172043ms step_avg:154.71ms
step:1123/1480 train_time:172202ms step_avg:154.72ms
step:1124/1480 train_time:172369ms step_avg:154.73ms
step:1125/1480 train_time:172531ms step_avg:154.74ms
step:1125/1480 val_loss:3.3903 train_time:172606ms step_avg:154.80ms
step:1126/1480 train_time:172701ms step_avg:154.75ms
step:1127/1480 train_time:172857ms step_avg:154.75ms
step:1128/1480 train_time:173017ms step_avg:154.76ms
step:1129/1480 train_time:173179ms step_avg:154.76ms
step:1130/1480 train_time:173339ms step_avg:154.77ms
step:1131/1480 train_time:173507ms step_avg:154.78ms
step:1132/1480 train_time:173670ms step_avg:154.79ms
step:1133/1480 train_time:173834ms step_avg:154.79ms
step:1134/1480 train_time:173997ms step_avg:154.80ms
step:1135/1480 train_time:174158ms step_avg:154.81ms
step:1136/1480 train_time:174320ms step_avg:154.81ms
step:1137/1480 train_time:174479ms step_avg:154.82ms
step:1138/1480 train_time:174645ms step_avg:154.83ms
step:1139/1480 train_time:174821ms step_avg:154.85ms
step:1140/1480 train_time:174968ms step_avg:154.84ms
step:1141/1480 train_time:175132ms step_avg:154.85ms
step:1142/1480 train_time:175293ms step_avg:154.85ms
step:1143/1480 train_time:175456ms step_avg:154.86ms
step:1144/1480 train_time:175618ms step_avg:154.87ms
step:1145/1480 train_time:175777ms step_avg:154.87ms
step:1146/1480 train_time:175940ms step_avg:154.88ms
step:1147/1480 train_time:176101ms step_avg:154.88ms
step:1148/1480 train_time:176261ms step_avg:154.89ms
step:1149/1480 train_time:176427ms step_avg:154.90ms
step:1150/1480 train_time:176589ms step_avg:154.90ms
step:1151/1480 train_time:176754ms step_avg:154.91ms
step:1152/1480 train_time:176918ms step_avg:154.92ms
step:1153/1480 train_time:177081ms step_avg:154.93ms
step:1154/1480 train_time:177241ms step_avg:154.93ms
step:1155/1480 train_time:177402ms step_avg:154.94ms
step:1156/1480 train_time:177572ms step_avg:154.95ms
step:1157/1480 train_time:177734ms step_avg:154.96ms
step:1158/1480 train_time:177895ms step_avg:154.96ms
step:1159/1480 train_time:178056ms step_avg:154.97ms
step:1160/1480 train_time:178215ms step_avg:154.97ms
step:1161/1480 train_time:178377ms step_avg:154.98ms
step:1162/1480 train_time:178539ms step_avg:154.98ms
step:1163/1480 train_time:178701ms step_avg:154.99ms
step:1164/1480 train_time:178861ms step_avg:154.99ms
step:1165/1480 train_time:179020ms step_avg:155.00ms
step:1166/1480 train_time:179181ms step_avg:155.00ms
step:1167/1480 train_time:179340ms step_avg:155.00ms
step:1168/1480 train_time:179503ms step_avg:155.01ms
step:1169/1480 train_time:179666ms step_avg:155.02ms
step:1170/1480 train_time:179828ms step_avg:155.02ms
step:1171/1480 train_time:179990ms step_avg:155.03ms
step:1172/1480 train_time:180150ms step_avg:155.03ms
step:1173/1480 train_time:180314ms step_avg:155.04ms
step:1174/1480 train_time:180482ms step_avg:155.05ms
step:1175/1480 train_time:180645ms step_avg:155.06ms
step:1176/1480 train_time:180809ms step_avg:155.07ms
step:1177/1480 train_time:180978ms step_avg:155.08ms
step:1178/1480 train_time:181137ms step_avg:155.08ms
step:1179/1480 train_time:181297ms step_avg:155.09ms
step:1180/1480 train_time:181466ms step_avg:155.10ms
step:1181/1480 train_time:181630ms step_avg:155.11ms
step:1182/1480 train_time:181791ms step_avg:155.11ms
step:1183/1480 train_time:181952ms step_avg:155.12ms
step:1184/1480 train_time:182113ms step_avg:155.12ms
step:1185/1480 train_time:182279ms step_avg:155.13ms
step:1186/1480 train_time:182441ms step_avg:155.14ms
step:1187/1480 train_time:182614ms step_avg:155.15ms
step:1188/1480 train_time:182774ms step_avg:155.16ms
step:1189/1480 train_time:182936ms step_avg:155.16ms
step:1190/1480 train_time:183097ms step_avg:155.17ms
step:1191/1480 train_time:183260ms step_avg:155.17ms
step:1192/1480 train_time:183419ms step_avg:155.18ms
step:1193/1480 train_time:183579ms step_avg:155.18ms
step:1194/1480 train_time:183741ms step_avg:155.19ms
step:1195/1480 train_time:183905ms step_avg:155.19ms
step:1196/1480 train_time:184076ms step_avg:155.21ms
step:1197/1480 train_time:184238ms step_avg:155.21ms
step:1198/1480 train_time:184406ms step_avg:155.22ms
step:1199/1480 train_time:184570ms step_avg:155.23ms
step:1200/1480 train_time:184731ms step_avg:155.24ms
step:1201/1480 train_time:184891ms step_avg:155.24ms
step:1202/1480 train_time:185061ms step_avg:155.25ms
step:1203/1480 train_time:185227ms step_avg:155.26ms
step:1204/1480 train_time:185393ms step_avg:155.27ms
step:1205/1480 train_time:185554ms step_avg:155.28ms
step:1206/1480 train_time:185716ms step_avg:155.28ms
step:1207/1480 train_time:185877ms step_avg:155.29ms
step:1208/1480 train_time:186037ms step_avg:155.29ms
step:1209/1480 train_time:186199ms step_avg:155.30ms
step:1210/1480 train_time:186362ms step_avg:155.30ms
step:1211/1480 train_time:186526ms step_avg:155.31ms
step:1212/1480 train_time:186690ms step_avg:155.32ms
step:1213/1480 train_time:186856ms step_avg:155.33ms
step:1214/1480 train_time:187021ms step_avg:155.33ms
step:1215/1480 train_time:187187ms step_avg:155.34ms
step:1216/1480 train_time:187349ms step_avg:155.35ms
step:1217/1480 train_time:187513ms step_avg:155.35ms
step:1218/1480 train_time:187674ms step_avg:155.36ms
step:1219/1480 train_time:187842ms step_avg:155.37ms
step:1220/1480 train_time:188005ms step_avg:155.38ms
step:1221/1480 train_time:188166ms step_avg:155.38ms
step:1222/1480 train_time:188326ms step_avg:155.38ms
step:1223/1480 train_time:188491ms step_avg:155.39ms
step:1224/1480 train_time:188656ms step_avg:155.40ms
step:1225/1480 train_time:188820ms step_avg:155.41ms
step:1226/1480 train_time:188985ms step_avg:155.41ms
step:1227/1480 train_time:189150ms step_avg:155.42ms
step:1228/1480 train_time:189313ms step_avg:155.43ms
step:1229/1480 train_time:189477ms step_avg:155.44ms
step:1230/1480 train_time:189645ms step_avg:155.45ms
step:1231/1480 train_time:189812ms step_avg:155.46ms
step:1232/1480 train_time:189977ms step_avg:155.46ms
step:1233/1480 train_time:190138ms step_avg:155.47ms
step:1234/1480 train_time:190299ms step_avg:155.47ms
step:1235/1480 train_time:190463ms step_avg:155.48ms
step:1236/1480 train_time:190625ms step_avg:155.49ms
step:1237/1480 train_time:190787ms step_avg:155.49ms
step:1238/1480 train_time:190959ms step_avg:155.50ms
step:1239/1480 train_time:191121ms step_avg:155.51ms
step:1240/1480 train_time:191286ms step_avg:155.52ms
step:1241/1480 train_time:191452ms step_avg:155.53ms
step:1242/1480 train_time:191614ms step_avg:155.53ms
step:1243/1480 train_time:191778ms step_avg:155.54ms
step:1244/1480 train_time:191937ms step_avg:155.54ms
step:1245/1480 train_time:192099ms step_avg:155.55ms
step:1246/1480 train_time:192261ms step_avg:155.55ms
step:1247/1480 train_time:192423ms step_avg:155.56ms
step:1248/1480 train_time:192586ms step_avg:155.56ms
step:1249/1480 train_time:192748ms step_avg:155.57ms
step:1250/1480 train_time:192912ms step_avg:155.57ms
step:1250/1480 val_loss:3.3404 train_time:192987ms step_avg:155.63ms
step:1251/1480 train_time:193081ms step_avg:155.59ms
step:1252/1480 train_time:193245ms step_avg:155.59ms
step:1253/1480 train_time:193405ms step_avg:155.60ms
step:1254/1480 train_time:193567ms step_avg:155.60ms
step:1255/1480 train_time:193737ms step_avg:155.61ms
step:1256/1480 train_time:193903ms step_avg:155.62ms
step:1257/1480 train_time:194065ms step_avg:155.63ms
step:1258/1480 train_time:194232ms step_avg:155.63ms
step:1259/1480 train_time:194394ms step_avg:155.64ms
step:1260/1480 train_time:194552ms step_avg:155.64ms
step:1261/1480 train_time:194715ms step_avg:155.65ms
step:1262/1480 train_time:194881ms step_avg:155.66ms
step:1263/1480 train_time:195047ms step_avg:155.66ms
step:1264/1480 train_time:195207ms step_avg:155.67ms
step:1265/1480 train_time:195368ms step_avg:155.67ms
step:1266/1480 train_time:195530ms step_avg:155.68ms
step:1267/1480 train_time:195690ms step_avg:155.68ms
step:1268/1480 train_time:195852ms step_avg:155.69ms
step:1269/1480 train_time:196017ms step_avg:155.69ms
step:1270/1480 train_time:196181ms step_avg:155.70ms
step:1271/1480 train_time:196344ms step_avg:155.70ms
step:1272/1480 train_time:196504ms step_avg:155.71ms
step:1273/1480 train_time:196666ms step_avg:155.71ms
step:1274/1480 train_time:196831ms step_avg:155.72ms
step:1275/1480 train_time:196991ms step_avg:155.72ms
step:1276/1480 train_time:197151ms step_avg:155.73ms
step:1277/1480 train_time:197312ms step_avg:155.73ms
step:1278/1480 train_time:197471ms step_avg:155.73ms
step:1279/1480 train_time:197634ms step_avg:155.74ms
step:1280/1480 train_time:197801ms step_avg:155.75ms
step:1281/1480 train_time:197964ms step_avg:155.75ms
step:1282/1480 train_time:198124ms step_avg:155.76ms
step:1283/1480 train_time:198287ms step_avg:155.76ms
step:1284/1480 train_time:198450ms step_avg:155.77ms
step:1285/1480 train_time:198612ms step_avg:155.77ms
step:1286/1480 train_time:198773ms step_avg:155.78ms
step:1287/1480 train_time:198936ms step_avg:155.78ms
step:1288/1480 train_time:199098ms step_avg:155.79ms
step:1289/1480 train_time:199267ms step_avg:155.80ms
step:1290/1480 train_time:199434ms step_avg:155.81ms
step:1291/1480 train_time:199599ms step_avg:155.82ms
step:1292/1480 train_time:199763ms step_avg:155.82ms
step:1293/1480 train_time:199930ms step_avg:155.83ms
step:1294/1480 train_time:200092ms step_avg:155.84ms
step:1295/1480 train_time:200254ms step_avg:155.84ms
step:1296/1480 train_time:200417ms step_avg:155.85ms
step:1297/1480 train_time:200581ms step_avg:155.85ms
step:1298/1480 train_time:200743ms step_avg:155.86ms
step:1299/1480 train_time:200907ms step_avg:155.86ms
step:1300/1480 train_time:201067ms step_avg:155.87ms
step:1301/1480 train_time:201228ms step_avg:155.87ms
step:1302/1480 train_time:201392ms step_avg:155.88ms
step:1303/1480 train_time:201558ms step_avg:155.88ms
step:1304/1480 train_time:201725ms step_avg:155.89ms
step:1305/1480 train_time:201886ms step_avg:155.90ms
step:1306/1480 train_time:202050ms step_avg:155.90ms
step:1307/1480 train_time:202211ms step_avg:155.91ms
step:1308/1480 train_time:202373ms step_avg:155.91ms
step:1309/1480 train_time:202538ms step_avg:155.92ms
step:1310/1480 train_time:202701ms step_avg:155.92ms
step:1311/1480 train_time:202863ms step_avg:155.93ms
step:1312/1480 train_time:203030ms step_avg:155.94ms
step:1313/1480 train_time:203190ms step_avg:155.94ms
step:1314/1480 train_time:203353ms step_avg:155.95ms
step:1315/1480 train_time:203517ms step_avg:155.95ms
step:1316/1480 train_time:203678ms step_avg:155.96ms
step:1317/1480 train_time:203841ms step_avg:155.96ms
step:1318/1480 train_time:204009ms step_avg:155.97ms
step:1319/1480 train_time:204173ms step_avg:155.98ms
step:1320/1480 train_time:204343ms step_avg:155.99ms
step:1321/1480 train_time:204507ms step_avg:155.99ms
step:1322/1480 train_time:204677ms step_avg:156.00ms
step:1323/1480 train_time:204842ms step_avg:156.01ms
step:1324/1480 train_time:205006ms step_avg:156.02ms
step:1325/1480 train_time:205178ms step_avg:156.03ms
step:1326/1480 train_time:205345ms step_avg:156.04ms
step:1327/1480 train_time:205507ms step_avg:156.04ms
step:1328/1480 train_time:205669ms step_avg:156.05ms
step:1329/1480 train_time:205860ms step_avg:156.07ms
step:1330/1480 train_time:206019ms step_avg:156.07ms
step:1331/1480 train_time:206182ms step_avg:156.08ms
step:1332/1480 train_time:206346ms step_avg:156.09ms
step:1333/1480 train_time:206512ms step_avg:156.09ms
step:1334/1480 train_time:206675ms step_avg:156.10ms
step:1335/1480 train_time:206835ms step_avg:156.10ms
step:1336/1480 train_time:207005ms step_avg:156.11ms
step:1337/1480 train_time:207171ms step_avg:156.12ms
step:1338/1480 train_time:207334ms step_avg:156.13ms
step:1339/1480 train_time:207499ms step_avg:156.13ms
step:1340/1480 train_time:207661ms step_avg:156.14ms
step:1341/1480 train_time:207826ms step_avg:156.14ms
step:1342/1480 train_time:207991ms step_avg:156.15ms
step:1343/1480 train_time:208152ms step_avg:156.15ms
step:1344/1480 train_time:208313ms step_avg:156.16ms
step:1345/1480 train_time:208483ms step_avg:156.17ms
step:1346/1480 train_time:208645ms step_avg:156.17ms
step:1347/1480 train_time:208808ms step_avg:156.18ms
step:1348/1480 train_time:208971ms step_avg:156.18ms
step:1349/1480 train_time:209133ms step_avg:156.19ms
step:1350/1480 train_time:209298ms step_avg:156.19ms
step:1351/1480 train_time:209459ms step_avg:156.20ms
step:1352/1480 train_time:209625ms step_avg:156.20ms
step:1353/1480 train_time:209791ms step_avg:156.21ms
step:1354/1480 train_time:209954ms step_avg:156.22ms
step:1355/1480 train_time:210116ms step_avg:156.22ms
step:1356/1480 train_time:210280ms step_avg:156.23ms
step:1357/1480 train_time:210445ms step_avg:156.23ms
step:1358/1480 train_time:210609ms step_avg:156.24ms
step:1359/1480 train_time:210772ms step_avg:156.24ms
step:1360/1480 train_time:210938ms step_avg:156.25ms
step:1361/1480 train_time:211106ms step_avg:156.26ms
step:1362/1480 train_time:211270ms step_avg:156.26ms
step:1363/1480 train_time:211438ms step_avg:156.27ms
step:1364/1480 train_time:211600ms step_avg:156.28ms
step:1365/1480 train_time:211759ms step_avg:156.28ms
step:1366/1480 train_time:211923ms step_avg:156.29ms
step:1367/1480 train_time:212085ms step_avg:156.29ms
step:1368/1480 train_time:212250ms step_avg:156.30ms
step:1369/1480 train_time:212420ms step_avg:156.31ms
step:1370/1480 train_time:212586ms step_avg:156.31ms
step:1371/1480 train_time:212750ms step_avg:156.32ms
step:1372/1480 train_time:212917ms step_avg:156.33ms
step:1373/1480 train_time:213078ms step_avg:156.33ms
step:1374/1480 train_time:213246ms step_avg:156.34ms
step:1375/1480 train_time:213408ms step_avg:156.34ms
step:1375/1480 val_loss:3.3013 train_time:213482ms step_avg:156.40ms
step:1376/1480 train_time:213575ms step_avg:156.35ms
step:1377/1480 train_time:213738ms step_avg:156.36ms
step:1378/1480 train_time:213899ms step_avg:156.36ms
step:1379/1480 train_time:214064ms step_avg:156.37ms
step:1380/1480 train_time:214228ms step_avg:156.37ms
step:1381/1480 train_time:214396ms step_avg:156.38ms
step:1382/1480 train_time:214560ms step_avg:156.38ms
step:1383/1480 train_time:214721ms step_avg:156.39ms
step:1384/1480 train_time:214887ms step_avg:156.40ms
step:1385/1480 train_time:215048ms step_avg:156.40ms
step:1386/1480 train_time:215212ms step_avg:156.40ms
step:1387/1480 train_time:215376ms step_avg:156.41ms
step:1388/1480 train_time:215537ms step_avg:156.41ms
step:1389/1480 train_time:215702ms step_avg:156.42ms
step:1390/1480 train_time:215863ms step_avg:156.42ms
step:1391/1480 train_time:216024ms step_avg:156.43ms
step:1392/1480 train_time:216188ms step_avg:156.43ms
step:1393/1480 train_time:216352ms step_avg:156.44ms
step:1394/1480 train_time:216516ms step_avg:156.44ms
step:1395/1480 train_time:216678ms step_avg:156.45ms
step:1396/1480 train_time:216842ms step_avg:156.45ms
step:1397/1480 train_time:217001ms step_avg:156.45ms
step:1398/1480 train_time:217161ms step_avg:156.46ms
step:1399/1480 train_time:217322ms step_avg:156.46ms
step:1400/1480 train_time:217493ms step_avg:156.47ms
step:1401/1480 train_time:217653ms step_avg:156.47ms
step:1402/1480 train_time:217815ms step_avg:156.48ms
step:1403/1480 train_time:217982ms step_avg:156.48ms
step:1404/1480 train_time:218144ms step_avg:156.49ms
step:1405/1480 train_time:218310ms step_avg:156.49ms
step:1406/1480 train_time:218477ms step_avg:156.50ms
step:1407/1480 train_time:218639ms step_avg:156.51ms
step:1408/1480 train_time:218800ms step_avg:156.51ms
step:1409/1480 train_time:218972ms step_avg:156.52ms
step:1410/1480 train_time:219135ms step_avg:156.53ms
step:1411/1480 train_time:219294ms step_avg:156.53ms
step:1412/1480 train_time:219456ms step_avg:156.53ms
step:1413/1480 train_time:219618ms step_avg:156.53ms
step:1414/1480 train_time:219783ms step_avg:156.54ms
step:1415/1480 train_time:219949ms step_avg:156.55ms
step:1416/1480 train_time:220123ms step_avg:156.56ms
step:1417/1480 train_time:220287ms step_avg:156.57ms
step:1418/1480 train_time:220452ms step_avg:156.57ms
step:1419/1480 train_time:220616ms step_avg:156.58ms
step:1420/1480 train_time:220781ms step_avg:156.58ms
step:1421/1480 train_time:220945ms step_avg:156.59ms
step:1422/1480 train_time:221112ms step_avg:156.59ms
step:1423/1480 train_time:221275ms step_avg:156.60ms
step:1424/1480 train_time:221442ms step_avg:156.61ms
step:1425/1480 train_time:221613ms step_avg:156.62ms
step:1426/1480 train_time:221778ms step_avg:156.62ms
step:1427/1480 train_time:221943ms step_avg:156.63ms
step:1428/1480 train_time:222104ms step_avg:156.63ms
step:1429/1480 train_time:222265ms step_avg:156.63ms
step:1430/1480 train_time:222428ms step_avg:156.64ms
step:1431/1480 train_time:222595ms step_avg:156.65ms
step:1432/1480 train_time:222762ms step_avg:156.65ms
step:1433/1480 train_time:222932ms step_avg:156.66ms
step:1434/1480 train_time:223101ms step_avg:156.67ms
step:1435/1480 train_time:223266ms step_avg:156.68ms
step:1436/1480 train_time:223430ms step_avg:156.68ms
step:1437/1480 train_time:223592ms step_avg:156.69ms
step:1438/1480 train_time:223755ms step_avg:156.69ms
step:1439/1480 train_time:223922ms step_avg:156.70ms
step:1440/1480 train_time:224085ms step_avg:156.70ms
step:1441/1480 train_time:224251ms step_avg:156.71ms
step:1442/1480 train_time:224416ms step_avg:156.72ms
step:1443/1480 train_time:224587ms step_avg:156.73ms
step:1444/1480 train_time:224753ms step_avg:156.73ms
step:1445/1480 train_time:224914ms step_avg:156.73ms
step:1446/1480 train_time:225080ms step_avg:156.74ms
step:1447/1480 train_time:225247ms step_avg:156.75ms
step:1448/1480 train_time:225411ms step_avg:156.75ms
step:1449/1480 train_time:225576ms step_avg:156.76ms
step:1450/1480 train_time:225741ms step_avg:156.76ms
step:1451/1480 train_time:225904ms step_avg:156.77ms
step:1452/1480 train_time:226069ms step_avg:156.77ms
step:1453/1480 train_time:226232ms step_avg:156.78ms
step:1454/1480 train_time:226395ms step_avg:156.78ms
step:1455/1480 train_time:226562ms step_avg:156.79ms
step:1456/1480 train_time:226726ms step_avg:156.80ms
step:1457/1480 train_time:226887ms step_avg:156.80ms
step:1458/1480 train_time:227053ms step_avg:156.80ms
step:1459/1480 train_time:227218ms step_avg:156.81ms
step:1460/1480 train_time:227382ms step_avg:156.82ms
step:1461/1480 train_time:227546ms step_avg:156.82ms
step:1462/1480 train_time:227711ms step_avg:156.83ms
step:1463/1480 train_time:227877ms step_avg:156.83ms
step:1464/1480 train_time:228043ms step_avg:156.84ms
step:1465/1480 train_time:228207ms step_avg:156.84ms
step:1466/1480 train_time:228369ms step_avg:156.85ms
step:1467/1480 train_time:228535ms step_avg:156.85ms
step:1468/1480 train_time:228698ms step_avg:156.86ms
step:1469/1480 train_time:228860ms step_avg:156.86ms
step:1470/1480 train_time:229028ms step_avg:156.87ms
step:1471/1480 train_time:229200ms step_avg:156.88ms
step:1472/1480 train_time:229370ms step_avg:156.89ms
step:1473/1480 train_time:229534ms step_avg:156.89ms
step:1474/1480 train_time:229701ms step_avg:156.90ms
step:1475/1480 train_time:229872ms step_avg:156.91ms
step:1476/1480 train_time:230036ms step_avg:156.91ms
step:1477/1480 train_time:230203ms step_avg:156.92ms
step:1478/1480 train_time:230374ms step_avg:156.93ms
step:1479/1480 train_time:230541ms step_avg:156.94ms
step:1480/1480 train_time:230702ms step_avg:156.94ms
step:1480/1480 val_loss:3.2824 train_time:230778ms step_avg:156.99ms
peak memory consumption: 34239 MiB

The `train_gpt.py` in this folder attains ~2.95 loss in ~26min on 8xH100

It can therefore form the first record for the task of matching the performance attained by Karpathy using llm.c to train a [350M parameter model on 30B tokens](https://github.com/karpathy/llm.c/discussions/481).

I'd be happy to see / help boost competition on this task as well


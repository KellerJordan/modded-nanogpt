import os
import sys
with open(sys.argv[0]) as f:
    code = f.read() # read the code of this file ASAP, for logging
import uuid
import time
import copy
import glob
from dataclasses import dataclass
from functools import lru_cache, partial # Added partial for hook registration
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch
torch.empty(1, device="cuda", requires_grad=True).backward() # prevents a bug on some systems
from torch import Tensor, nn
import torch.nn.functional as F
import torch.distributed as dist
# use of FlexAttention contributed by @KoszarskyB
from torch.nn.attention.flex_attention import BlockMask, flex_attention
#torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng

@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Muon optimizer

@torch.compile
def zeropower_via_newtonschulz5(G: Tensor, steps: int) -> Tensor:
    """
    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a
    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose
    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at
    zero even beyond the point where the iteration no longer converges all the way to one everywhere
    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T
    where S' is diagonal with S_{ii}' ~ Uniform(0.5, 1.5), which turns out not to hurt model
    performance at all relative to UV^T, where USV^T = G is the SVD.
    """
    assert G.ndim >= 2 # batched Muon implementation by @scottjmaddox, and put into practice in the record by @YouJiacheng
    a, b, c = (3.4445, -4.7750,  2.0315)
    X = G
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) + 1e-7)
    # Perform the NS iterations
    for _ in range(steps):
        A = X @ X.mT
        B = b * A + c * A @ A # quintic computation strategy adapted from suggestion by @jxbz, @leloykun, and @YouJiacheng
        X = a * X + B @ X

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

class NorMuon(torch.optim.Optimizer):

    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, beta2=0.95):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, beta2=beta2)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_reduce_futures: list[torch.Future] = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            grad = torch.empty_like(params[-1])
            grad_pad = [param.grad for param in params] + [torch.zeros_like(params[-1])] * world_size
            for base_i in range(0, len(params), world_size):
                if base_i + rank < len(params):
                    grad = params[base_i + rank].grad
                else:
                    grad = torch.zeros_like(grad)
                # This gives strange dynamo warnings
                reduce_scatter_futures.append(dist.reduce_scatter(grad, grad_pad[base_i:base_i + world_size], op=dist.ReduceOp.AVG, async_op=True).get_future())

        idx = 0
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            params_pad = params + [torch.empty_like(params[-1])] * world_size
            momentum = group["momentum"]
            beta2 = group["beta2"]
            for base_i in range(0, len(params), world_size):
                reduce_scatter_futures[idx].wait()
                if base_i + rank < len(params):
                    p = params[base_i + rank]
                    grad = p.grad
                    eff_lr = group["lr"] * max(1, p.size(-2) / p.size(-1)) ** 0.5 * getattr(p, "lr_mul", 1.0)
                    eff_weight_decay = group["lr"] * group["weight_decay"] * getattr(p, "wd_mul", 1.0)
                    state = self.state[p]
                    if len(state) == 0:
                        state["momentum_buffer"] = torch.zeros_like(grad)
                        state["second_momentum_buffer"] = torch.zeros_like(grad[..., 0:1]) if p.size(-2) >= p.size(-1) else torch.zeros_like(grad[0:1, ...])
                    momentum_buffer = state["momentum_buffer"]
                    second_momentum_buffer = state["second_momentum_buffer"]
                    p.mul_(1 - eff_weight_decay)
                    momentum_buffer.lerp_(grad, 1 - momentum)
                    grad = grad.lerp_(momentum_buffer, momentum)
                    v = zeropower_via_newtonschulz5(grad.bfloat16(), 5).float()
                    ###################################
                    vnorm = v.norm(dim=(-2,-1), keepdim=True)
                    v_mean = torch.mean(v * v, dim=-1, keepdim=True) if p.size(-2) >= p.size(-1) else torch.mean(v * v, dim=-2, keepdim=True)
                    second_momentum_buffer.lerp_(v_mean, 1 - beta2)
                    step_size = 1 / second_momentum_buffer.sqrt().add_(1e-10)
                    v.mul_(step_size)
                    vnorm_new = v.norm(dim=(-2,-1), keepdim=True)
                    v.mul_(vnorm / (vnorm_new + 1e-10))
                    ####################################
                    p.add_(other=v, alpha=-eff_lr)
                idx += 1
                all_reduce_futures.append(dist.all_gather(params_pad[base_i:base_i + world_size], params_pad[base_i + rank], async_op=True).get_future())
        torch.futures.collect_all(all_reduce_futures).wait()

class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_reduce_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            grad = torch.empty_like(params[-1])
            for base_i in range(len(params)):
                grad = params[base_i].grad
                rank_size = grad.shape[0] // world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for base in range(len(params)):
                reduce_scatter_futures[idx].wait()
                p = params[base]
                rank_size = p.shape[0] // world_size
                p_slice = p[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(p, "lr_mul", 1.0)
                state = self.state[p]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state['step'] = torch.tensor(0, dtype=torch.int64, device=p.device)
                    state['exp_avg'] = torch.zeros_like(p_slice)
                    state['exp_avg_sq'] = torch.zeros_like(p_slice)
                exp_avg = state['exp_avg']
                exp_avg_sq = state['exp_avg_sq']
                state['step'] += 1
                t = state['step']
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(p, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_reduce_futures.append(dist.all_gather_into_tensor(p, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_reduce_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

class Rotary(nn.Module):
    def __init__(self, dim: int, max_seq_len: int):
        super().__init__()
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=dim//4, dtype=torch.float32)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(dim//4)])
        t = torch.arange(max_seq_len, dtype=torch.float32)
        theta = torch.einsum("i,j -> ij", t, angular_freq)
        self.cos = nn.Buffer(theta.cos(), persistent=False)
        self.sin = nn.Buffer(theta.sin(), persistent=False)

    def forward(self, x_BTHD: Tensor):
        assert self.cos.size(0) >= x_BTHD.size(-3)
        cos, sin = self.cos[None, :x_BTHD.size(-3), None, :], self.sin[None, :x_BTHD.size(-3), None, :]
        x1, x2 = x_BTHD.to(dtype=torch.float32).chunk(2, dim=-1)
        y1 = x1 * cos + x2 * sin
        y2 = x1 * (-sin) + x2 * cos
        return torch.cat((y1, y2), 3).type_as(x_BTHD)

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, num_heads: int, max_seq_len: int, head_dim=128):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        hdim = num_heads * head_dim
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        self.qkv_w = nn.Parameter(torch.empty(3, hdim, dim).uniform_(-bound, bound))
        self.rotary = Rotary(head_dim, max_seq_len)
        self.c_proj = CastedLinear(hdim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977
        # scale the attention logits by given constant, instead of the default head_dim**-0.5, by @leloykun
        # inspired by learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.12

    def forward(self, x: Tensor, ve: Tensor | None, lambdas: Tensor, block_mask: BlockMask):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "Must use batch size = 1 for FlexAttention"
        q, k, v = F.linear(x, self.qkv_w.flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = self.rotary(q), self.rotary(k)
        if ve is not None:
            v = lambdas[0] * v + lambdas[1] * ve.view_as(v) # @KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = lambdas[0] * v
        y = flex_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), block_mask=block_mask, scale=self.attn_scale).transpose(1, 2)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = self.c_proj(y)
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        self.c_fc = CastedLinear(dim, hdim)
        self.c_proj = CastedLinear(hdim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = self.c_fc(x)
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = self.c_proj(x)
        return x

class Block(nn.Module):
    def __init__(self, dim: int, num_heads: int, max_seq_len: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, num_heads, max_seq_len) if layer_idx != 7 else None
        self.mlp = MLP(dim)

    def forward(self, x: Tensor, ve: Tensor | None, x0: Tensor, lambdas: Tensor, sa_lambdas: Tensor, block_mask: BlockMask):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), ve, sa_lambdas, block_mask)
        x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, num_heads, max_seq_len, i) for i in range(num_layers)])
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=True, x_s=(model_dim**0.5)/448, w_s=24/448, grad_s=1/448)
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5) % dist.get_world_size()
        self.scalars = nn.Parameter(torch.cat([
            torch.ones(num_layers), # skip_weights
            *[torch.tensor([1.0, 0.0]) for _ in range(num_layers)], # block lambdas
            *[torch.tensor([0.5, 0.5]) for _ in range(num_layers)], # SA lambdas
            torch.ones(pad),
        ]))
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 27.5
        self.scalars.lr_mul = 5.0

    def create_blockmasks(self, input_seq: Tensor, sliding_window_num_blocks: Tensor):
        BLOCK_SIZE = 128
        docs = (input_seq == 50256).cumsum(0)

        def document_causal(b, h, q_idx, kv_idx):
            causal_mask = q_idx >= kv_idx
            document_mask = docs[q_idx] == docs[kv_idx]
            return causal_mask & document_mask

        def dense_to_ordered(dense_blockmask: Tensor):
            num_blocks = dense_blockmask.sum(dim=-1, dtype=torch.int32)
            indices = dense_blockmask.argsort(dim=-1, descending=False, stable=True).flip(-1).to(torch.int32)
            return num_blocks[None, None].contiguous(), indices[None, None].contiguous()

        # manual block mask creation by @YouJiacheng
        assert len(input_seq) % BLOCK_SIZE == 0
        NUM_BLOCKS = len(input_seq) // BLOCK_SIZE
        block_idx = torch.arange(NUM_BLOCKS, dtype=torch.int32, device="cuda")
        causal_blockmask_any = block_idx[:, None] >= block_idx
        causal_blockmask_all = block_idx[:, None] > block_idx
        docs_low = docs.view(-1, BLOCK_SIZE)[:, 0].contiguous()
        docs_high = docs.view(-1, BLOCK_SIZE)[:, -1].contiguous()
        document_blockmask_any = (docs_low[:, None] <= docs_high) & (docs_high[:, None] >= docs_low)
        document_blockmask_all = (docs_low[:, None] == docs_high) & (docs_high[:, None] == docs_low)
        blockmask_any = causal_blockmask_any & document_blockmask_any
        blockmask_all = causal_blockmask_all & document_blockmask_all
        partial_kv_num_blocks, partial_kv_indices = dense_to_ordered(blockmask_any & ~blockmask_all)
        full_kv_num_blocks, full_kv_indices = dense_to_ordered(blockmask_all)
        def build_bm(window_size_blocks: Tensor) -> BlockMask:
            return BlockMask.from_kv_blocks(
                torch.clamp_max(partial_kv_num_blocks, torch.clamp_min(window_size_blocks - full_kv_num_blocks, 1)),
                partial_kv_indices,
                torch.clamp_max(full_kv_num_blocks, window_size_blocks - 1),
                full_kv_indices,
                BLOCK_SIZE=BLOCK_SIZE,
                mask_mod=document_causal,
            )
        # Long-short SWA block masks by @leloykun & @YouJiacheng, adapated from suggestion by @Grad62304977, following Gemma 2 paper
        return build_bm(sliding_window_num_blocks), build_bm(sliding_window_num_blocks // 2)

    def forward(self, input_seq: Tensor, target_seq: Tensor, sliding_window_num_blocks: Tensor):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        ve = [ve[0], ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        long_bm, short_bm = self.create_blockmasks(input_seq, sliding_window_num_blocks)
        block_masks = [long_bm, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, long_bm, short_bm, short_bm, short_bm, long_bm]
        assert len(block_masks) == len(self.blocks)

        x = x0 = norm(self.embed(input_seq)[None]) # use of norm here by @Grad62304977

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)

        n = len(self.blocks) // 2

        for i in range(len(self.blocks)):
            if i >= n:
                x = x + skip_weights[i - n] * skip_connections.pop()
            x = self.blocks[i](x, ve[i], x0, lambdas[i], sa_lambdas[i], block_masks[i])
            if i < n:
                skip_connections.append(x)

        x = norm(x)
        logits = self.lm_head(x).float()
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / (7.5 * x.size(-1)**0.5))
        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), target_seq, reduction="sum" if self.training else "mean")
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

# find world_size starting indicies, such that each begins with token 50256 and local_batches don't overlap
def find_batch_starts(tokens: Tensor, pos: int, local_batch_size: int, max_batch_span: int):
    boundary_mask = tokens[pos : pos + max_batch_span] == 50256
    boundary_positions = torch.nonzero(boundary_mask, as_tuple=False).squeeze(-1) + pos
    start = boundary_positions[0].item()
    starts = []
    for i in range(1, len(boundary_positions)):
        end = boundary_positions[i].item() 
        if end - start >= local_batch_size:
            starts.append(start) # append start once end pos is confirmed
            if len(starts) == dist.get_world_size():
                return starts, end - pos
            start = end
    assert False # increase max_batch_span if necessary

def distributed_data_generator(filename_pattern: str, batch_size: int, align_to_bos: bool):
    rank = dist.get_rank()
    world_size = dist.get_world_size()
    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    assert batch_size % world_size == 0
    local_batch_size = batch_size // world_size
    file_iter = iter(files) # use itertools.cycle(files) instead if you want to do multi-epoch training
    tokens, pos = _load_data_shard(next(file_iter)), 0
    max_batch_span = 2 * batch_size if align_to_bos else batch_size # provide buffer to handle samples up to length local_batch_size
    while True:
        if pos + max_batch_span + 1 >= len(tokens):
            tokens, pos = _load_data_shard(next(file_iter)), 0
        if align_to_bos:
            batch_starts, batch_span = find_batch_starts(tokens, pos, local_batch_size, max_batch_span)
            start_idx = batch_starts[rank]
        else:
            batch_span = batch_size
            start_idx = pos + rank * local_batch_size
        buf = tokens[start_idx:][:local_batch_size + 1]
        inputs = buf[:-1].to(device="cuda", dtype=torch.int32, non_blocking=True) # no sync on host side;
        targets = buf[1:].to(device="cuda", dtype=torch.int64, non_blocking=True) # H2D in another stream isn't helpful.
        pos += batch_span
        yield inputs, targets

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_seq_len = 48*1024 # FlexAttention sequence length
    val_seq_len = 4*64*1024 # FlexAttention sequence length for validation
    # optimization
    num_iterations = 1700 # number of iterations to run
    cooldown_frac = 0.45 # fraction of training spent cooling down the learning rate
    # evaluation and logging
    val_loss_every = 125 # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint = False
args = Hyperparameters()

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert world_size == 8 # this code is designed for 8xH100
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = uuid.uuid4()
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(vocab_size=50257, num_layers=12, num_heads=6, model_dim=768, max_seq_len=max(args.train_seq_len, args.val_seq_len)).cuda()
for m in model.modules():
    if isinstance(m, nn.Embedding):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(scalar_params + head_params + embed_params, lr=0.008, betas=(0.8, 0.95), eps=1e-10, weight_decay=0.0)
optimizer2 = NorMuon(hidden_matrix_params, lr=0.05, momentum=0.95, weight_decay=0.0, beta2=0.95)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: stable then decay
def get_lr(step: int):
    x = step / args.num_iterations # progress in training
    assert 0 <= x < 1
    if x < 1 - args.cooldown_frac:
        return 1.0
    else:
        w = (1 - x) / args.cooldown_frac
        return w * 1.0 + (1 - w) * 0.1

# attention window size schedule: linearly increase
@lru_cache(1)
def get_window_size_blocks_helper(window_size: int):
    return torch.tensor(window_size // 128, dtype=torch.int32, pin_memory=True).cuda(non_blocking=True)
def get_window_size_blocks(step: int):
    x = step / args.num_iterations # progress in training
    assert 0 <= x <= 1
    # Linearly increase the block-wise sliding window size over training 128 -> 1792
    # increase by @fernbear.bsky.social; block-wise by @YouJiacheng
    window_size = next_multiple_of_n(1728 * x, n=128)
    return get_window_size_blocks_helper(window_size)

model: nn.Module = torch.compile(model, dynamic=False)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 10
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, world_size * args.train_seq_len, align_to_bos=True)
for _ in range(warmup_steps):
    inputs, targets = next(train_loader)
    model(inputs, targets, get_window_size_blocks(1)).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, world_size * args.train_seq_len, align_to_bos=True)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
for step in range(train_steps + 1):
    last_step = (step == train_steps)

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        val_batch_size = world_size * args.val_seq_len
        assert args.val_tokens % val_batch_size == 0
        val_steps = args.val_tokens // val_batch_size
        val_loader = distributed_data_generator(args.val_files, val_batch_size, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets = next(val_loader)
                val_loss += model(inputs, targets, get_window_size_blocks(step))
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    inputs, targets = next(train_loader)
    model(inputs, targets, get_window_size_blocks(step)).backward()
    # set optimization hyperparameters
    for opt in optimizers:
        for group in opt.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)
    for group in optimizer2.param_groups:
        frac = min(step / 300, 1) # momentum warmup for muon
        group["momentum"] = (1 - frac) * 0.85 + frac * 0.95
    # step the optimizers
    for opt in optimizers:
        opt.step()
    # null the gradients
    model.zero_grad(set_to_none=True)
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()
====================================================================================================
Running Python 3.12.7 (main, Oct 11 2025, 17:06:43) [GCC 13.2.0]
Running PyTorch 2.10.0.dev20251011+cu126 compiled for CUDA 12.6
Mon Oct 13 03:30:34 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 565.57.01              Driver Version: 565.57.01      CUDA Version: 12.7     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000001:00:00.0 Off |                    0 |
| N/A   32C    P0            113W /  700W |    5858MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000002:00:00.0 Off |                    0 |
| N/A   33C    P0            115W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000003:00:00.0 Off |                    0 |
| N/A   30C    P0            116W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000008:00:00.0 Off |                    0 |
| N/A   29C    P0            115W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000009:00:00.0 Off |                    0 |
| N/A   28C    P0            112W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   0000000A:00:00.0 Off |                    0 |
| N/A   33C    P0            121W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   0000000B:00:00.0 Off |                    0 |
| N/A   29C    P0            117W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   0000000C:00:00.0 Off |                    0 |
| N/A   31C    P0            115W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
+-----------------------------------------------------------------------------------------+

====================================================================================================
step:0/1700 val_loss:10.8258 train_time:0ms step_avg:0.16ms
step:1/1700 train_time:142ms step_avg:142.26ms
step:2/1700 train_time:167ms step_avg:83.61ms
step:3/1700 train_time:249ms step_avg:83.07ms
step:4/1700 train_time:341ms step_avg:85.15ms
step:5/1700 train_time:433ms step_avg:86.56ms
step:6/1700 train_time:525ms step_avg:87.44ms
step:7/1700 train_time:617ms step_avg:88.11ms
step:8/1700 train_time:709ms step_avg:88.59ms
step:9/1700 train_time:801ms step_avg:88.96ms
step:10/1700 train_time:894ms step_avg:89.36ms
step:11/1700 train_time:986ms step_avg:89.62ms
step:12/1700 train_time:1081ms step_avg:90.05ms
step:13/1700 train_time:1176ms step_avg:90.44ms
step:14/1700 train_time:1270ms step_avg:90.70ms
step:15/1700 train_time:1363ms step_avg:90.84ms
step:16/1700 train_time:1456ms step_avg:90.98ms
step:17/1700 train_time:1548ms step_avg:91.08ms
step:18/1700 train_time:1641ms step_avg:91.15ms
step:19/1700 train_time:1734ms step_avg:91.24ms
step:20/1700 train_time:1826ms step_avg:91.29ms
step:21/1700 train_time:1918ms step_avg:91.34ms
step:22/1700 train_time:2012ms step_avg:91.43ms
step:23/1700 train_time:2105ms step_avg:91.53ms
step:24/1700 train_time:2199ms step_avg:91.61ms
step:25/1700 train_time:2293ms step_avg:91.73ms
step:26/1700 train_time:2386ms step_avg:91.77ms
step:27/1700 train_time:2479ms step_avg:91.81ms
step:28/1700 train_time:2572ms step_avg:91.87ms
step:29/1700 train_time:2664ms step_avg:91.87ms
step:30/1700 train_time:2757ms step_avg:91.89ms
step:31/1700 train_time:2849ms step_avg:91.91ms
step:32/1700 train_time:2942ms step_avg:91.95ms
step:33/1700 train_time:3035ms step_avg:91.96ms
step:34/1700 train_time:3128ms step_avg:92.00ms
step:35/1700 train_time:3222ms step_avg:92.07ms
step:36/1700 train_time:3316ms step_avg:92.11ms
step:37/1700 train_time:3409ms step_avg:92.14ms
step:38/1700 train_time:3502ms step_avg:92.16ms
step:39/1700 train_time:3595ms step_avg:92.19ms
step:40/1700 train_time:3688ms step_avg:92.19ms
step:41/1700 train_time:3780ms step_avg:92.18ms
step:42/1700 train_time:3873ms step_avg:92.21ms
step:43/1700 train_time:3966ms step_avg:92.23ms
step:44/1700 train_time:4058ms step_avg:92.24ms
step:45/1700 train_time:4152ms step_avg:92.27ms
step:46/1700 train_time:4245ms step_avg:92.28ms
step:47/1700 train_time:4338ms step_avg:92.30ms
step:48/1700 train_time:4432ms step_avg:92.32ms
step:49/1700 train_time:4524ms step_avg:92.33ms
step:50/1700 train_time:4618ms step_avg:92.35ms
step:51/1700 train_time:4711ms step_avg:92.38ms
step:52/1700 train_time:4803ms step_avg:92.37ms
step:53/1700 train_time:4896ms step_avg:92.38ms
step:54/1700 train_time:4989ms step_avg:92.39ms
step:55/1700 train_time:5081ms step_avg:92.39ms
step:56/1700 train_time:5174ms step_avg:92.39ms
step:57/1700 train_time:5267ms step_avg:92.41ms
step:58/1700 train_time:5360ms step_avg:92.41ms
step:59/1700 train_time:5453ms step_avg:92.43ms
step:60/1700 train_time:5546ms step_avg:92.44ms
step:61/1700 train_time:5639ms step_avg:92.44ms
step:62/1700 train_time:5732ms step_avg:92.46ms
step:63/1700 train_time:5825ms step_avg:92.47ms
step:64/1700 train_time:5918ms step_avg:92.47ms
step:65/1700 train_time:6011ms step_avg:92.48ms
step:66/1700 train_time:6104ms step_avg:92.48ms
step:67/1700 train_time:6197ms step_avg:92.49ms
step:68/1700 train_time:6290ms step_avg:92.50ms
step:69/1700 train_time:6383ms step_avg:92.51ms
step:70/1700 train_time:6476ms step_avg:92.51ms
step:71/1700 train_time:6570ms step_avg:92.53ms
step:72/1700 train_time:6662ms step_avg:92.53ms
step:73/1700 train_time:6755ms step_avg:92.54ms
step:74/1700 train_time:6848ms step_avg:92.54ms
step:75/1700 train_time:6940ms step_avg:92.53ms
step:76/1700 train_time:7034ms step_avg:92.55ms
step:77/1700 train_time:7126ms step_avg:92.55ms
step:78/1700 train_time:7219ms step_avg:92.56ms
step:79/1700 train_time:7312ms step_avg:92.56ms
step:80/1700 train_time:7405ms step_avg:92.56ms
step:81/1700 train_time:7498ms step_avg:92.57ms
step:82/1700 train_time:7591ms step_avg:92.57ms
step:83/1700 train_time:7684ms step_avg:92.57ms
step:84/1700 train_time:7776ms step_avg:92.57ms
step:85/1700 train_time:7869ms step_avg:92.58ms
step:86/1700 train_time:7961ms step_avg:92.57ms
step:87/1700 train_time:8055ms step_avg:92.58ms
step:88/1700 train_time:8147ms step_avg:92.58ms
step:89/1700 train_time:8240ms step_avg:92.58ms
step:90/1700 train_time:8333ms step_avg:92.58ms
step:91/1700 train_time:8425ms step_avg:92.58ms
step:92/1700 train_time:8518ms step_avg:92.58ms
step:93/1700 train_time:8611ms step_avg:92.59ms
step:94/1700 train_time:8704ms step_avg:92.59ms
step:95/1700 train_time:8796ms step_avg:92.59ms
step:96/1700 train_time:8889ms step_avg:92.60ms
step:97/1700 train_time:8982ms step_avg:92.60ms
step:98/1700 train_time:9075ms step_avg:92.61ms
step:99/1700 train_time:9168ms step_avg:92.61ms
step:100/1700 train_time:9261ms step_avg:92.61ms
step:101/1700 train_time:9353ms step_avg:92.61ms
step:102/1700 train_time:9447ms step_avg:92.61ms
step:103/1700 train_time:9539ms step_avg:92.61ms
step:104/1700 train_time:9632ms step_avg:92.61ms
step:105/1700 train_time:9724ms step_avg:92.61ms
step:106/1700 train_time:9817ms step_avg:92.62ms
step:107/1700 train_time:9911ms step_avg:92.62ms
step:108/1700 train_time:10003ms step_avg:92.62ms
step:109/1700 train_time:10096ms step_avg:92.63ms
step:110/1700 train_time:10189ms step_avg:92.63ms
step:111/1700 train_time:10281ms step_avg:92.63ms
step:112/1700 train_time:10375ms step_avg:92.63ms
step:113/1700 train_time:10468ms step_avg:92.64ms
step:114/1700 train_time:10561ms step_avg:92.64ms
step:115/1700 train_time:10653ms step_avg:92.64ms
step:116/1700 train_time:10746ms step_avg:92.64ms
step:117/1700 train_time:10838ms step_avg:92.63ms
step:118/1700 train_time:10931ms step_avg:92.64ms
step:119/1700 train_time:11024ms step_avg:92.64ms
step:120/1700 train_time:11117ms step_avg:92.64ms
step:121/1700 train_time:11209ms step_avg:92.64ms
step:122/1700 train_time:11302ms step_avg:92.64ms
step:123/1700 train_time:11396ms step_avg:92.65ms
step:124/1700 train_time:11488ms step_avg:92.65ms
step:125/1700 train_time:11581ms step_avg:92.65ms
step:125/1700 val_loss:4.6120 train_time:11657ms step_avg:93.26ms
step:126/1700 train_time:11684ms step_avg:92.73ms
step:127/1700 train_time:11772ms step_avg:92.69ms
step:128/1700 train_time:11871ms step_avg:92.74ms
step:129/1700 train_time:11966ms step_avg:92.76ms
step:130/1700 train_time:12059ms step_avg:92.76ms
step:131/1700 train_time:12151ms step_avg:92.76ms
step:132/1700 train_time:12244ms step_avg:92.76ms
step:133/1700 train_time:12337ms step_avg:92.76ms
step:134/1700 train_time:12429ms step_avg:92.75ms
step:135/1700 train_time:12522ms step_avg:92.75ms
step:136/1700 train_time:12614ms step_avg:92.75ms
step:137/1700 train_time:12708ms step_avg:92.76ms
step:138/1700 train_time:12802ms step_avg:92.77ms
step:139/1700 train_time:12896ms step_avg:92.78ms
step:140/1700 train_time:12991ms step_avg:92.79ms
step:141/1700 train_time:13085ms step_avg:92.80ms
step:142/1700 train_time:13178ms step_avg:92.80ms
step:143/1700 train_time:13271ms step_avg:92.80ms
step:144/1700 train_time:13365ms step_avg:92.81ms
step:145/1700 train_time:13457ms step_avg:92.81ms
step:146/1700 train_time:13549ms step_avg:92.80ms
step:147/1700 train_time:13643ms step_avg:92.81ms
step:148/1700 train_time:13736ms step_avg:92.81ms
step:149/1700 train_time:13830ms step_avg:92.82ms
step:150/1700 train_time:13924ms step_avg:92.82ms
step:151/1700 train_time:14018ms step_avg:92.84ms
step:152/1700 train_time:14112ms step_avg:92.84ms
step:153/1700 train_time:14206ms step_avg:92.85ms
step:154/1700 train_time:14298ms step_avg:92.85ms
step:155/1700 train_time:14391ms step_avg:92.84ms
step:156/1700 train_time:14484ms step_avg:92.85ms
step:157/1700 train_time:14578ms step_avg:92.85ms
step:158/1700 train_time:14671ms step_avg:92.85ms
step:159/1700 train_time:14765ms step_avg:92.86ms
step:160/1700 train_time:14858ms step_avg:92.87ms
step:161/1700 train_time:14952ms step_avg:92.87ms
step:162/1700 train_time:15046ms step_avg:92.87ms
step:163/1700 train_time:15139ms step_avg:92.88ms
step:164/1700 train_time:15232ms step_avg:92.88ms
step:165/1700 train_time:15326ms step_avg:92.88ms
step:166/1700 train_time:15419ms step_avg:92.89ms
step:167/1700 train_time:15512ms step_avg:92.88ms
step:168/1700 train_time:15605ms step_avg:92.89ms
step:169/1700 train_time:15699ms step_avg:92.89ms
step:170/1700 train_time:15792ms step_avg:92.89ms
step:171/1700 train_time:15885ms step_avg:92.90ms
step:172/1700 train_time:15979ms step_avg:92.90ms
step:173/1700 train_time:16072ms step_avg:92.90ms
step:174/1700 train_time:16166ms step_avg:92.91ms
step:175/1700 train_time:16260ms step_avg:92.91ms
step:176/1700 train_time:16353ms step_avg:92.91ms
step:177/1700 train_time:16446ms step_avg:92.92ms
step:178/1700 train_time:16539ms step_avg:92.92ms
step:179/1700 train_time:16632ms step_avg:92.92ms
step:180/1700 train_time:16726ms step_avg:92.92ms
step:181/1700 train_time:16819ms step_avg:92.92ms
step:182/1700 train_time:16912ms step_avg:92.92ms
step:183/1700 train_time:17006ms step_avg:92.93ms
step:184/1700 train_time:17100ms step_avg:92.93ms
step:185/1700 train_time:17193ms step_avg:92.93ms
step:186/1700 train_time:17287ms step_avg:92.94ms
step:187/1700 train_time:17380ms step_avg:92.94ms
step:188/1700 train_time:17473ms step_avg:92.94ms
step:189/1700 train_time:17567ms step_avg:92.95ms
step:190/1700 train_time:17660ms step_avg:92.95ms
step:191/1700 train_time:17753ms step_avg:92.95ms
step:192/1700 train_time:17846ms step_avg:92.95ms
step:193/1700 train_time:17940ms step_avg:92.95ms
step:194/1700 train_time:18033ms step_avg:92.95ms
step:195/1700 train_time:18127ms step_avg:92.96ms
step:196/1700 train_time:18220ms step_avg:92.96ms
step:197/1700 train_time:18314ms step_avg:92.96ms
step:198/1700 train_time:18407ms step_avg:92.97ms
step:199/1700 train_time:18501ms step_avg:92.97ms
step:200/1700 train_time:18594ms step_avg:92.97ms
step:201/1700 train_time:18687ms step_avg:92.97ms
step:202/1700 train_time:18780ms step_avg:92.97ms
step:203/1700 train_time:18872ms step_avg:92.97ms
step:204/1700 train_time:18967ms step_avg:92.97ms
step:205/1700 train_time:19060ms step_avg:92.98ms
step:206/1700 train_time:19153ms step_avg:92.98ms
step:207/1700 train_time:19247ms step_avg:92.98ms
step:208/1700 train_time:19341ms step_avg:92.98ms
step:209/1700 train_time:19433ms step_avg:92.98ms
step:210/1700 train_time:19526ms step_avg:92.98ms
step:211/1700 train_time:19620ms step_avg:92.99ms
step:212/1700 train_time:19713ms step_avg:92.99ms
step:213/1700 train_time:19807ms step_avg:92.99ms
step:214/1700 train_time:19900ms step_avg:92.99ms
step:215/1700 train_time:19993ms step_avg:92.99ms
step:216/1700 train_time:20086ms step_avg:92.99ms
step:217/1700 train_time:20180ms step_avg:93.00ms
step:218/1700 train_time:20273ms step_avg:93.00ms
step:219/1700 train_time:20367ms step_avg:93.00ms
step:220/1700 train_time:20460ms step_avg:93.00ms
step:221/1700 train_time:20553ms step_avg:93.00ms
step:222/1700 train_time:20647ms step_avg:93.00ms
step:223/1700 train_time:20740ms step_avg:93.01ms
step:224/1700 train_time:20833ms step_avg:93.00ms
step:225/1700 train_time:20926ms step_avg:93.01ms
step:226/1700 train_time:21019ms step_avg:93.01ms
step:227/1700 train_time:21112ms step_avg:93.01ms
step:228/1700 train_time:21206ms step_avg:93.01ms
step:229/1700 train_time:21300ms step_avg:93.01ms
step:230/1700 train_time:21392ms step_avg:93.01ms
step:231/1700 train_time:21486ms step_avg:93.01ms
step:232/1700 train_time:21580ms step_avg:93.02ms
step:233/1700 train_time:21673ms step_avg:93.02ms
step:234/1700 train_time:21767ms step_avg:93.02ms
step:235/1700 train_time:21860ms step_avg:93.02ms
step:236/1700 train_time:21954ms step_avg:93.02ms
step:237/1700 train_time:22047ms step_avg:93.03ms
step:238/1700 train_time:22141ms step_avg:93.03ms
step:239/1700 train_time:22234ms step_avg:93.03ms
step:240/1700 train_time:22327ms step_avg:93.03ms
step:241/1700 train_time:22421ms step_avg:93.03ms
step:242/1700 train_time:22513ms step_avg:93.03ms
step:243/1700 train_time:22607ms step_avg:93.03ms
step:244/1700 train_time:22701ms step_avg:93.04ms
step:245/1700 train_time:22794ms step_avg:93.04ms
step:246/1700 train_time:22887ms step_avg:93.04ms
step:247/1700 train_time:22981ms step_avg:93.04ms
step:248/1700 train_time:23074ms step_avg:93.04ms
step:249/1700 train_time:23168ms step_avg:93.04ms
step:250/1700 train_time:23261ms step_avg:93.04ms
step:250/1700 val_loss:4.0731 train_time:23337ms step_avg:93.35ms
step:251/1700 train_time:23365ms step_avg:93.09ms
step:252/1700 train_time:23455ms step_avg:93.08ms
step:253/1700 train_time:23553ms step_avg:93.10ms
step:254/1700 train_time:23647ms step_avg:93.10ms
step:255/1700 train_time:23742ms step_avg:93.10ms
step:256/1700 train_time:23835ms step_avg:93.11ms
step:257/1700 train_time:23928ms step_avg:93.11ms
step:258/1700 train_time:24022ms step_avg:93.11ms
step:259/1700 train_time:24115ms step_avg:93.11ms
step:260/1700 train_time:24208ms step_avg:93.11ms
step:261/1700 train_time:24301ms step_avg:93.11ms
step:262/1700 train_time:24396ms step_avg:93.12ms
step:263/1700 train_time:24491ms step_avg:93.12ms
step:264/1700 train_time:24586ms step_avg:93.13ms
step:265/1700 train_time:24681ms step_avg:93.14ms
step:266/1700 train_time:24774ms step_avg:93.14ms
step:267/1700 train_time:24868ms step_avg:93.14ms
step:268/1700 train_time:24961ms step_avg:93.14ms
step:269/1700 train_time:25053ms step_avg:93.14ms
step:270/1700 train_time:25147ms step_avg:93.14ms
step:271/1700 train_time:25240ms step_avg:93.14ms
step:272/1700 train_time:25334ms step_avg:93.14ms
step:273/1700 train_time:25428ms step_avg:93.14ms
step:274/1700 train_time:25523ms step_avg:93.15ms
step:275/1700 train_time:25617ms step_avg:93.15ms
step:276/1700 train_time:25711ms step_avg:93.16ms
step:277/1700 train_time:25806ms step_avg:93.16ms
step:278/1700 train_time:25899ms step_avg:93.16ms
step:279/1700 train_time:25992ms step_avg:93.16ms
step:280/1700 train_time:26085ms step_avg:93.16ms
step:281/1700 train_time:26179ms step_avg:93.16ms
step:282/1700 train_time:26272ms step_avg:93.16ms
step:283/1700 train_time:26366ms step_avg:93.17ms
step:284/1700 train_time:26460ms step_avg:93.17ms
step:285/1700 train_time:26554ms step_avg:93.17ms
step:286/1700 train_time:26648ms step_avg:93.17ms
step:287/1700 train_time:26742ms step_avg:93.18ms
step:288/1700 train_time:26836ms step_avg:93.18ms
step:289/1700 train_time:26930ms step_avg:93.18ms
step:290/1700 train_time:27024ms step_avg:93.18ms
step:291/1700 train_time:27117ms step_avg:93.18ms
step:292/1700 train_time:27210ms step_avg:93.19ms
step:293/1700 train_time:27304ms step_avg:93.19ms
step:294/1700 train_time:27397ms step_avg:93.19ms
step:295/1700 train_time:27491ms step_avg:93.19ms
step:296/1700 train_time:27585ms step_avg:93.19ms
step:297/1700 train_time:27679ms step_avg:93.20ms
step:298/1700 train_time:27773ms step_avg:93.20ms
step:299/1700 train_time:27868ms step_avg:93.20ms
step:300/1700 train_time:27961ms step_avg:93.20ms
step:301/1700 train_time:28054ms step_avg:93.20ms
step:302/1700 train_time:28148ms step_avg:93.21ms
step:303/1700 train_time:28242ms step_avg:93.21ms
step:304/1700 train_time:28335ms step_avg:93.21ms
step:305/1700 train_time:28429ms step_avg:93.21ms
step:306/1700 train_time:28523ms step_avg:93.21ms
step:307/1700 train_time:28617ms step_avg:93.22ms
step:308/1700 train_time:28711ms step_avg:93.22ms
step:309/1700 train_time:28806ms step_avg:93.22ms
step:310/1700 train_time:28900ms step_avg:93.23ms
step:311/1700 train_time:28993ms step_avg:93.23ms
step:312/1700 train_time:29087ms step_avg:93.23ms
step:313/1700 train_time:29180ms step_avg:93.23ms
step:314/1700 train_time:29274ms step_avg:93.23ms
step:315/1700 train_time:29368ms step_avg:93.23ms
step:316/1700 train_time:29461ms step_avg:93.23ms
step:317/1700 train_time:29554ms step_avg:93.23ms
step:318/1700 train_time:29648ms step_avg:93.23ms
step:319/1700 train_time:29741ms step_avg:93.23ms
step:320/1700 train_time:29836ms step_avg:93.24ms
step:321/1700 train_time:29930ms step_avg:93.24ms
step:322/1700 train_time:30024ms step_avg:93.24ms
step:323/1700 train_time:30118ms step_avg:93.24ms
step:324/1700 train_time:30211ms step_avg:93.24ms
step:325/1700 train_time:30304ms step_avg:93.24ms
step:326/1700 train_time:30397ms step_avg:93.24ms
step:327/1700 train_time:30492ms step_avg:93.25ms
step:328/1700 train_time:30585ms step_avg:93.25ms
step:329/1700 train_time:30679ms step_avg:93.25ms
step:330/1700 train_time:30773ms step_avg:93.25ms
step:331/1700 train_time:30867ms step_avg:93.25ms
step:332/1700 train_time:30961ms step_avg:93.25ms
step:333/1700 train_time:31054ms step_avg:93.26ms
step:334/1700 train_time:31148ms step_avg:93.26ms
step:335/1700 train_time:31242ms step_avg:93.26ms
step:336/1700 train_time:31334ms step_avg:93.26ms
step:337/1700 train_time:31428ms step_avg:93.26ms
step:338/1700 train_time:31523ms step_avg:93.26ms
step:339/1700 train_time:31616ms step_avg:93.26ms
step:340/1700 train_time:31709ms step_avg:93.26ms
step:341/1700 train_time:31803ms step_avg:93.27ms
step:342/1700 train_time:31897ms step_avg:93.27ms
step:343/1700 train_time:31991ms step_avg:93.27ms
step:344/1700 train_time:32084ms step_avg:93.27ms
step:345/1700 train_time:32178ms step_avg:93.27ms
step:346/1700 train_time:32271ms step_avg:93.27ms
step:347/1700 train_time:32366ms step_avg:93.27ms
step:348/1700 train_time:32460ms step_avg:93.28ms
step:349/1700 train_time:32553ms step_avg:93.27ms
step:350/1700 train_time:32647ms step_avg:93.28ms
step:351/1700 train_time:32740ms step_avg:93.28ms
step:352/1700 train_time:32834ms step_avg:93.28ms
step:353/1700 train_time:32928ms step_avg:93.28ms
step:354/1700 train_time:33021ms step_avg:93.28ms
step:355/1700 train_time:33115ms step_avg:93.28ms
step:356/1700 train_time:33209ms step_avg:93.28ms
step:357/1700 train_time:33302ms step_avg:93.28ms
step:358/1700 train_time:33396ms step_avg:93.28ms
step:359/1700 train_time:33489ms step_avg:93.28ms
step:360/1700 train_time:33583ms step_avg:93.29ms
step:361/1700 train_time:33677ms step_avg:93.29ms
step:362/1700 train_time:33771ms step_avg:93.29ms
step:363/1700 train_time:33865ms step_avg:93.29ms
step:364/1700 train_time:33959ms step_avg:93.29ms
step:365/1700 train_time:34052ms step_avg:93.29ms
step:366/1700 train_time:34147ms step_avg:93.30ms
step:367/1700 train_time:34241ms step_avg:93.30ms
step:368/1700 train_time:34334ms step_avg:93.30ms
step:369/1700 train_time:34428ms step_avg:93.30ms
step:370/1700 train_time:34522ms step_avg:93.30ms
step:371/1700 train_time:34615ms step_avg:93.30ms
step:372/1700 train_time:34709ms step_avg:93.30ms
step:373/1700 train_time:34802ms step_avg:93.30ms
step:374/1700 train_time:34896ms step_avg:93.31ms
step:375/1700 train_time:34990ms step_avg:93.31ms
step:375/1700 val_loss:3.8728 train_time:35067ms step_avg:93.51ms
step:376/1700 train_time:35096ms step_avg:93.34ms
step:377/1700 train_time:35186ms step_avg:93.33ms
step:378/1700 train_time:35283ms step_avg:93.34ms
step:379/1700 train_time:35378ms step_avg:93.34ms
step:380/1700 train_time:35472ms step_avg:93.35ms
step:381/1700 train_time:35567ms step_avg:93.35ms
step:382/1700 train_time:35662ms step_avg:93.36ms
step:383/1700 train_time:35758ms step_avg:93.36ms
step:384/1700 train_time:35852ms step_avg:93.37ms
step:385/1700 train_time:35947ms step_avg:93.37ms
step:386/1700 train_time:36042ms step_avg:93.37ms
step:387/1700 train_time:36139ms step_avg:93.38ms
step:388/1700 train_time:36236ms step_avg:93.39ms
step:389/1700 train_time:36332ms step_avg:93.40ms
step:390/1700 train_time:36427ms step_avg:93.40ms
step:391/1700 train_time:36524ms step_avg:93.41ms
step:392/1700 train_time:36619ms step_avg:93.42ms
step:393/1700 train_time:36715ms step_avg:93.42ms
step:394/1700 train_time:36809ms step_avg:93.42ms
step:395/1700 train_time:36904ms step_avg:93.43ms
step:396/1700 train_time:37000ms step_avg:93.43ms
step:397/1700 train_time:37096ms step_avg:93.44ms
step:398/1700 train_time:37193ms step_avg:93.45ms
step:399/1700 train_time:37289ms step_avg:93.45ms
step:400/1700 train_time:37385ms step_avg:93.46ms
step:401/1700 train_time:37481ms step_avg:93.47ms
step:402/1700 train_time:37576ms step_avg:93.47ms
step:403/1700 train_time:37671ms step_avg:93.48ms
step:404/1700 train_time:37766ms step_avg:93.48ms
step:405/1700 train_time:37861ms step_avg:93.49ms
step:406/1700 train_time:37957ms step_avg:93.49ms
step:407/1700 train_time:38052ms step_avg:93.49ms
step:408/1700 train_time:38147ms step_avg:93.50ms
step:409/1700 train_time:38244ms step_avg:93.51ms
step:410/1700 train_time:38340ms step_avg:93.51ms
step:411/1700 train_time:38436ms step_avg:93.52ms
step:412/1700 train_time:38531ms step_avg:93.52ms
step:413/1700 train_time:38627ms step_avg:93.53ms
step:414/1700 train_time:38723ms step_avg:93.53ms
step:415/1700 train_time:38818ms step_avg:93.54ms
step:416/1700 train_time:38914ms step_avg:93.54ms
step:417/1700 train_time:39009ms step_avg:93.55ms
step:418/1700 train_time:39105ms step_avg:93.55ms
step:419/1700 train_time:39201ms step_avg:93.56ms
step:420/1700 train_time:39297ms step_avg:93.56ms
step:421/1700 train_time:39393ms step_avg:93.57ms
step:422/1700 train_time:39488ms step_avg:93.57ms
step:423/1700 train_time:39584ms step_avg:93.58ms
step:424/1700 train_time:39679ms step_avg:93.58ms
step:425/1700 train_time:39775ms step_avg:93.59ms
step:426/1700 train_time:39870ms step_avg:93.59ms
step:427/1700 train_time:39965ms step_avg:93.59ms
step:428/1700 train_time:40061ms step_avg:93.60ms
step:429/1700 train_time:40157ms step_avg:93.61ms
step:430/1700 train_time:40252ms step_avg:93.61ms
step:431/1700 train_time:40347ms step_avg:93.61ms
step:432/1700 train_time:40443ms step_avg:93.62ms
step:433/1700 train_time:40540ms step_avg:93.63ms
step:434/1700 train_time:40635ms step_avg:93.63ms
step:435/1700 train_time:40730ms step_avg:93.63ms
step:436/1700 train_time:40826ms step_avg:93.64ms
step:437/1700 train_time:40921ms step_avg:93.64ms
step:438/1700 train_time:41017ms step_avg:93.65ms
step:439/1700 train_time:41113ms step_avg:93.65ms
step:440/1700 train_time:41208ms step_avg:93.65ms
step:441/1700 train_time:41304ms step_avg:93.66ms
step:442/1700 train_time:41400ms step_avg:93.67ms
step:443/1700 train_time:41496ms step_avg:93.67ms
step:444/1700 train_time:41591ms step_avg:93.67ms
step:445/1700 train_time:41686ms step_avg:93.68ms
step:446/1700 train_time:41783ms step_avg:93.68ms
step:447/1700 train_time:41878ms step_avg:93.69ms
step:448/1700 train_time:41973ms step_avg:93.69ms
step:449/1700 train_time:42068ms step_avg:93.69ms
step:450/1700 train_time:42165ms step_avg:93.70ms
step:451/1700 train_time:42260ms step_avg:93.70ms
step:452/1700 train_time:42357ms step_avg:93.71ms
step:453/1700 train_time:42452ms step_avg:93.71ms
step:454/1700 train_time:42548ms step_avg:93.72ms
step:455/1700 train_time:42643ms step_avg:93.72ms
step:456/1700 train_time:42738ms step_avg:93.72ms
step:457/1700 train_time:42834ms step_avg:93.73ms
step:458/1700 train_time:42929ms step_avg:93.73ms
step:459/1700 train_time:43025ms step_avg:93.74ms
step:460/1700 train_time:43122ms step_avg:93.74ms
step:461/1700 train_time:43217ms step_avg:93.75ms
step:462/1700 train_time:43312ms step_avg:93.75ms
step:463/1700 train_time:43408ms step_avg:93.75ms
step:464/1700 train_time:43505ms step_avg:93.76ms
step:465/1700 train_time:43602ms step_avg:93.77ms
step:466/1700 train_time:43697ms step_avg:93.77ms
step:467/1700 train_time:43792ms step_avg:93.77ms
step:468/1700 train_time:43888ms step_avg:93.78ms
step:469/1700 train_time:43985ms step_avg:93.78ms
step:470/1700 train_time:44080ms step_avg:93.79ms
step:471/1700 train_time:44175ms step_avg:93.79ms
step:472/1700 train_time:44270ms step_avg:93.79ms
step:473/1700 train_time:44366ms step_avg:93.80ms
step:474/1700 train_time:44462ms step_avg:93.80ms
step:475/1700 train_time:44558ms step_avg:93.81ms
step:476/1700 train_time:44654ms step_avg:93.81ms
step:477/1700 train_time:44749ms step_avg:93.81ms
step:478/1700 train_time:44845ms step_avg:93.82ms
step:479/1700 train_time:44942ms step_avg:93.82ms
step:480/1700 train_time:45037ms step_avg:93.83ms
step:481/1700 train_time:45132ms step_avg:93.83ms
step:482/1700 train_time:45227ms step_avg:93.83ms
step:483/1700 train_time:45324ms step_avg:93.84ms
step:484/1700 train_time:45420ms step_avg:93.84ms
step:485/1700 train_time:45515ms step_avg:93.85ms
step:486/1700 train_time:45610ms step_avg:93.85ms
step:487/1700 train_time:45706ms step_avg:93.85ms
step:488/1700 train_time:45801ms step_avg:93.86ms
step:489/1700 train_time:45897ms step_avg:93.86ms
step:490/1700 train_time:45993ms step_avg:93.86ms
step:491/1700 train_time:46088ms step_avg:93.87ms
step:492/1700 train_time:46184ms step_avg:93.87ms
step:493/1700 train_time:46279ms step_avg:93.87ms
step:494/1700 train_time:46374ms step_avg:93.88ms
step:495/1700 train_time:46469ms step_avg:93.88ms
step:496/1700 train_time:46566ms step_avg:93.88ms
step:497/1700 train_time:46661ms step_avg:93.89ms
step:498/1700 train_time:46758ms step_avg:93.89ms
step:499/1700 train_time:46853ms step_avg:93.89ms
step:500/1700 train_time:46948ms step_avg:93.90ms
step:500/1700 val_loss:3.7311 train_time:47027ms step_avg:94.05ms
step:501/1700 train_time:47055ms step_avg:93.92ms
step:502/1700 train_time:47145ms step_avg:93.92ms
step:503/1700 train_time:47242ms step_avg:93.92ms
step:504/1700 train_time:47338ms step_avg:93.92ms
step:505/1700 train_time:47433ms step_avg:93.93ms
step:506/1700 train_time:47527ms step_avg:93.93ms
step:507/1700 train_time:47623ms step_avg:93.93ms
step:508/1700 train_time:47719ms step_avg:93.93ms
step:509/1700 train_time:47814ms step_avg:93.94ms
step:510/1700 train_time:47910ms step_avg:93.94ms
step:511/1700 train_time:48006ms step_avg:93.94ms
step:512/1700 train_time:48103ms step_avg:93.95ms
step:513/1700 train_time:48200ms step_avg:93.96ms
step:514/1700 train_time:48297ms step_avg:93.96ms
step:515/1700 train_time:48392ms step_avg:93.97ms
step:516/1700 train_time:48487ms step_avg:93.97ms
step:517/1700 train_time:48582ms step_avg:93.97ms
step:518/1700 train_time:48678ms step_avg:93.97ms
step:519/1700 train_time:48773ms step_avg:93.98ms
step:520/1700 train_time:48869ms step_avg:93.98ms
step:521/1700 train_time:48964ms step_avg:93.98ms
step:522/1700 train_time:49061ms step_avg:93.99ms
step:523/1700 train_time:49158ms step_avg:93.99ms
step:524/1700 train_time:49255ms step_avg:94.00ms
step:525/1700 train_time:49352ms step_avg:94.00ms
step:526/1700 train_time:49448ms step_avg:94.01ms
step:527/1700 train_time:49543ms step_avg:94.01ms
step:528/1700 train_time:49638ms step_avg:94.01ms
step:529/1700 train_time:49733ms step_avg:94.01ms
step:530/1700 train_time:49829ms step_avg:94.02ms
step:531/1700 train_time:49924ms step_avg:94.02ms
step:532/1700 train_time:50019ms step_avg:94.02ms
step:533/1700 train_time:50115ms step_avg:94.03ms
step:534/1700 train_time:50211ms step_avg:94.03ms
step:535/1700 train_time:50308ms step_avg:94.03ms
step:536/1700 train_time:50404ms step_avg:94.04ms
step:537/1700 train_time:50501ms step_avg:94.04ms
step:538/1700 train_time:50597ms step_avg:94.05ms
step:539/1700 train_time:50693ms step_avg:94.05ms
step:540/1700 train_time:50789ms step_avg:94.05ms
step:541/1700 train_time:50885ms step_avg:94.06ms
step:542/1700 train_time:50981ms step_avg:94.06ms
step:543/1700 train_time:51078ms step_avg:94.07ms
step:544/1700 train_time:51174ms step_avg:94.07ms
step:545/1700 train_time:51270ms step_avg:94.07ms
step:546/1700 train_time:51365ms step_avg:94.08ms
step:547/1700 train_time:51462ms step_avg:94.08ms
step:548/1700 train_time:51558ms step_avg:94.08ms
step:549/1700 train_time:51654ms step_avg:94.09ms
step:550/1700 train_time:51750ms step_avg:94.09ms
step:551/1700 train_time:51845ms step_avg:94.09ms
step:552/1700 train_time:51941ms step_avg:94.10ms
step:553/1700 train_time:52037ms step_avg:94.10ms
step:554/1700 train_time:52134ms step_avg:94.10ms
step:555/1700 train_time:52229ms step_avg:94.11ms
step:556/1700 train_time:52325ms step_avg:94.11ms
step:557/1700 train_time:52421ms step_avg:94.11ms
step:558/1700 train_time:52517ms step_avg:94.12ms
step:559/1700 train_time:52613ms step_avg:94.12ms
step:560/1700 train_time:52709ms step_avg:94.12ms
step:561/1700 train_time:52805ms step_avg:94.13ms
step:562/1700 train_time:52901ms step_avg:94.13ms
step:563/1700 train_time:52998ms step_avg:94.13ms
step:564/1700 train_time:53094ms step_avg:94.14ms
step:565/1700 train_time:53190ms step_avg:94.14ms
step:566/1700 train_time:53286ms step_avg:94.15ms
step:567/1700 train_time:53383ms step_avg:94.15ms
step:568/1700 train_time:53479ms step_avg:94.15ms
step:569/1700 train_time:53575ms step_avg:94.16ms
step:570/1700 train_time:53671ms step_avg:94.16ms
step:571/1700 train_time:53766ms step_avg:94.16ms
step:572/1700 train_time:53862ms step_avg:94.16ms
step:573/1700 train_time:53958ms step_avg:94.17ms
step:574/1700 train_time:54054ms step_avg:94.17ms
step:575/1700 train_time:54150ms step_avg:94.17ms
step:576/1700 train_time:54246ms step_avg:94.18ms
step:577/1700 train_time:54342ms step_avg:94.18ms
step:578/1700 train_time:54439ms step_avg:94.18ms
step:579/1700 train_time:54535ms step_avg:94.19ms
step:580/1700 train_time:54631ms step_avg:94.19ms
step:581/1700 train_time:54726ms step_avg:94.19ms
step:582/1700 train_time:54822ms step_avg:94.20ms
step:583/1700 train_time:54918ms step_avg:94.20ms
step:584/1700 train_time:55014ms step_avg:94.20ms
step:585/1700 train_time:55111ms step_avg:94.21ms
step:586/1700 train_time:55206ms step_avg:94.21ms
step:587/1700 train_time:55302ms step_avg:94.21ms
step:588/1700 train_time:55398ms step_avg:94.21ms
step:589/1700 train_time:55494ms step_avg:94.22ms
step:590/1700 train_time:55590ms step_avg:94.22ms
step:591/1700 train_time:55685ms step_avg:94.22ms
step:592/1700 train_time:55781ms step_avg:94.23ms
step:593/1700 train_time:55877ms step_avg:94.23ms
step:594/1700 train_time:55972ms step_avg:94.23ms
step:595/1700 train_time:56068ms step_avg:94.23ms
step:596/1700 train_time:56164ms step_avg:94.23ms
step:597/1700 train_time:56261ms step_avg:94.24ms
step:598/1700 train_time:56357ms step_avg:94.24ms
step:599/1700 train_time:56454ms step_avg:94.25ms
step:600/1700 train_time:56551ms step_avg:94.25ms
step:601/1700 train_time:56647ms step_avg:94.25ms
step:602/1700 train_time:56743ms step_avg:94.26ms
step:603/1700 train_time:56840ms step_avg:94.26ms
step:604/1700 train_time:56936ms step_avg:94.26ms
step:605/1700 train_time:57031ms step_avg:94.27ms
step:606/1700 train_time:57127ms step_avg:94.27ms
step:607/1700 train_time:57223ms step_avg:94.27ms
step:608/1700 train_time:57320ms step_avg:94.28ms
step:609/1700 train_time:57417ms step_avg:94.28ms
step:610/1700 train_time:57513ms step_avg:94.28ms
step:611/1700 train_time:57609ms step_avg:94.29ms
step:612/1700 train_time:57705ms step_avg:94.29ms
step:613/1700 train_time:57801ms step_avg:94.29ms
step:614/1700 train_time:57898ms step_avg:94.30ms
step:615/1700 train_time:57994ms step_avg:94.30ms
step:616/1700 train_time:58090ms step_avg:94.30ms
step:617/1700 train_time:58186ms step_avg:94.30ms
step:618/1700 train_time:58281ms step_avg:94.31ms
step:619/1700 train_time:58378ms step_avg:94.31ms
step:620/1700 train_time:58474ms step_avg:94.31ms
step:621/1700 train_time:58570ms step_avg:94.32ms
step:622/1700 train_time:58666ms step_avg:94.32ms
step:623/1700 train_time:58762ms step_avg:94.32ms
step:624/1700 train_time:58859ms step_avg:94.33ms
step:625/1700 train_time:58955ms step_avg:94.33ms
step:625/1700 val_loss:3.6435 train_time:59034ms step_avg:94.45ms
step:626/1700 train_time:59058ms step_avg:94.34ms
step:627/1700 train_time:59154ms step_avg:94.34ms
step:628/1700 train_time:59252ms step_avg:94.35ms
step:629/1700 train_time:59348ms step_avg:94.35ms
step:630/1700 train_time:59443ms step_avg:94.35ms
step:631/1700 train_time:59539ms step_avg:94.36ms
step:632/1700 train_time:59637ms step_avg:94.36ms
step:633/1700 train_time:59733ms step_avg:94.37ms
step:634/1700 train_time:59829ms step_avg:94.37ms
step:635/1700 train_time:59926ms step_avg:94.37ms
step:636/1700 train_time:60024ms step_avg:94.38ms
step:637/1700 train_time:60123ms step_avg:94.38ms
step:638/1700 train_time:60223ms step_avg:94.39ms
step:639/1700 train_time:60322ms step_avg:94.40ms
step:640/1700 train_time:60420ms step_avg:94.41ms
step:641/1700 train_time:60517ms step_avg:94.41ms
step:642/1700 train_time:60615ms step_avg:94.42ms
step:643/1700 train_time:60711ms step_avg:94.42ms
step:644/1700 train_time:60807ms step_avg:94.42ms
step:645/1700 train_time:60904ms step_avg:94.43ms
step:646/1700 train_time:61001ms step_avg:94.43ms
step:647/1700 train_time:61100ms step_avg:94.44ms
step:648/1700 train_time:61198ms step_avg:94.44ms
step:649/1700 train_time:61296ms step_avg:94.45ms
step:650/1700 train_time:61393ms step_avg:94.45ms
step:651/1700 train_time:61491ms step_avg:94.46ms
step:652/1700 train_time:61588ms step_avg:94.46ms
step:653/1700 train_time:61686ms step_avg:94.47ms
step:654/1700 train_time:61784ms step_avg:94.47ms
step:655/1700 train_time:61881ms step_avg:94.47ms
step:656/1700 train_time:61977ms step_avg:94.48ms
step:657/1700 train_time:62075ms step_avg:94.48ms
step:658/1700 train_time:62172ms step_avg:94.49ms
step:659/1700 train_time:62270ms step_avg:94.49ms
step:660/1700 train_time:62367ms step_avg:94.50ms
step:661/1700 train_time:62465ms step_avg:94.50ms
step:662/1700 train_time:62564ms step_avg:94.51ms
step:663/1700 train_time:62661ms step_avg:94.51ms
step:664/1700 train_time:62759ms step_avg:94.52ms
step:665/1700 train_time:62857ms step_avg:94.52ms
step:666/1700 train_time:62953ms step_avg:94.52ms
step:667/1700 train_time:63050ms step_avg:94.53ms
step:668/1700 train_time:63147ms step_avg:94.53ms
step:669/1700 train_time:63245ms step_avg:94.54ms
step:670/1700 train_time:63343ms step_avg:94.54ms
step:671/1700 train_time:63441ms step_avg:94.55ms
step:672/1700 train_time:63538ms step_avg:94.55ms
step:673/1700 train_time:63635ms step_avg:94.55ms
step:674/1700 train_time:63733ms step_avg:94.56ms
step:675/1700 train_time:63830ms step_avg:94.56ms
step:676/1700 train_time:63927ms step_avg:94.57ms
step:677/1700 train_time:64026ms step_avg:94.57ms
step:678/1700 train_time:64124ms step_avg:94.58ms
step:679/1700 train_time:64223ms step_avg:94.58ms
step:680/1700 train_time:64320ms step_avg:94.59ms
step:681/1700 train_time:64417ms step_avg:94.59ms
step:682/1700 train_time:64515ms step_avg:94.60ms
step:683/1700 train_time:64612ms step_avg:94.60ms
step:684/1700 train_time:64710ms step_avg:94.61ms
step:685/1700 train_time:64807ms step_avg:94.61ms
step:686/1700 train_time:64905ms step_avg:94.61ms
step:687/1700 train_time:65003ms step_avg:94.62ms
step:688/1700 train_time:65101ms step_avg:94.62ms
step:689/1700 train_time:65199ms step_avg:94.63ms
step:690/1700 train_time:65296ms step_avg:94.63ms
step:691/1700 train_time:65392ms step_avg:94.63ms
step:692/1700 train_time:65490ms step_avg:94.64ms
step:693/1700 train_time:65588ms step_avg:94.64ms
step:694/1700 train_time:65686ms step_avg:94.65ms
step:695/1700 train_time:65785ms step_avg:94.65ms
step:696/1700 train_time:65883ms step_avg:94.66ms
step:697/1700 train_time:65980ms step_avg:94.66ms
step:698/1700 train_time:66078ms step_avg:94.67ms
step:699/1700 train_time:66176ms step_avg:94.67ms
step:700/1700 train_time:66273ms step_avg:94.68ms
step:701/1700 train_time:66370ms step_avg:94.68ms
step:702/1700 train_time:66467ms step_avg:94.68ms
step:703/1700 train_time:66565ms step_avg:94.69ms
step:704/1700 train_time:66663ms step_avg:94.69ms
step:705/1700 train_time:66761ms step_avg:94.70ms
step:706/1700 train_time:66858ms step_avg:94.70ms
step:707/1700 train_time:66955ms step_avg:94.70ms
step:708/1700 train_time:67052ms step_avg:94.71ms
step:709/1700 train_time:67150ms step_avg:94.71ms
step:710/1700 train_time:67247ms step_avg:94.71ms
step:711/1700 train_time:67345ms step_avg:94.72ms
step:712/1700 train_time:67445ms step_avg:94.73ms
step:713/1700 train_time:67542ms step_avg:94.73ms
step:714/1700 train_time:67640ms step_avg:94.73ms
step:715/1700 train_time:67737ms step_avg:94.74ms
step:716/1700 train_time:67834ms step_avg:94.74ms
step:717/1700 train_time:67930ms step_avg:94.74ms
step:718/1700 train_time:68029ms step_avg:94.75ms
step:719/1700 train_time:68127ms step_avg:94.75ms
step:720/1700 train_time:68225ms step_avg:94.76ms
step:721/1700 train_time:68323ms step_avg:94.76ms
step:722/1700 train_time:68421ms step_avg:94.77ms
step:723/1700 train_time:68519ms step_avg:94.77ms
step:724/1700 train_time:68616ms step_avg:94.77ms
step:725/1700 train_time:68713ms step_avg:94.78ms
step:726/1700 train_time:68810ms step_avg:94.78ms
step:727/1700 train_time:68907ms step_avg:94.78ms
step:728/1700 train_time:69005ms step_avg:94.79ms
step:729/1700 train_time:69104ms step_avg:94.79ms
step:730/1700 train_time:69202ms step_avg:94.80ms
step:731/1700 train_time:69299ms step_avg:94.80ms
step:732/1700 train_time:69398ms step_avg:94.81ms
step:733/1700 train_time:69495ms step_avg:94.81ms
step:734/1700 train_time:69592ms step_avg:94.81ms
step:735/1700 train_time:69690ms step_avg:94.82ms
step:736/1700 train_time:69788ms step_avg:94.82ms
step:737/1700 train_time:69886ms step_avg:94.82ms
step:738/1700 train_time:69984ms step_avg:94.83ms
step:739/1700 train_time:70082ms step_avg:94.83ms
step:740/1700 train_time:70180ms step_avg:94.84ms
step:741/1700 train_time:70278ms step_avg:94.84ms
step:742/1700 train_time:70376ms step_avg:94.85ms
step:743/1700 train_time:70473ms step_avg:94.85ms
step:744/1700 train_time:70570ms step_avg:94.85ms
step:745/1700 train_time:70667ms step_avg:94.86ms
step:746/1700 train_time:70765ms step_avg:94.86ms
step:747/1700 train_time:70863ms step_avg:94.86ms
step:748/1700 train_time:70960ms step_avg:94.87ms
step:749/1700 train_time:71058ms step_avg:94.87ms
step:750/1700 train_time:71155ms step_avg:94.87ms
step:750/1700 val_loss:3.5831 train_time:71235ms step_avg:94.98ms
step:751/1700 train_time:71259ms step_avg:94.89ms
step:752/1700 train_time:71359ms step_avg:94.89ms
step:753/1700 train_time:71459ms step_avg:94.90ms
step:754/1700 train_time:71558ms step_avg:94.90ms
step:755/1700 train_time:71656ms step_avg:94.91ms
step:756/1700 train_time:71753ms step_avg:94.91ms
step:757/1700 train_time:71850ms step_avg:94.91ms
step:758/1700 train_time:71947ms step_avg:94.92ms
step:759/1700 train_time:72043ms step_avg:94.92ms
step:760/1700 train_time:72140ms step_avg:94.92ms
step:761/1700 train_time:72238ms step_avg:94.92ms
step:762/1700 train_time:72338ms step_avg:94.93ms
step:763/1700 train_time:72437ms step_avg:94.94ms
step:764/1700 train_time:72536ms step_avg:94.94ms
step:765/1700 train_time:72635ms step_avg:94.95ms
step:766/1700 train_time:72733ms step_avg:94.95ms
step:767/1700 train_time:72831ms step_avg:94.96ms
step:768/1700 train_time:72928ms step_avg:94.96ms
step:769/1700 train_time:73024ms step_avg:94.96ms
step:770/1700 train_time:73121ms step_avg:94.96ms
step:771/1700 train_time:73218ms step_avg:94.97ms
step:772/1700 train_time:73318ms step_avg:94.97ms
step:773/1700 train_time:73417ms step_avg:94.98ms
step:774/1700 train_time:73516ms step_avg:94.98ms
step:775/1700 train_time:73613ms step_avg:94.98ms
step:776/1700 train_time:73711ms step_avg:94.99ms
step:777/1700 train_time:73809ms step_avg:94.99ms
step:778/1700 train_time:73907ms step_avg:95.00ms
step:779/1700 train_time:74003ms step_avg:95.00ms
step:780/1700 train_time:74101ms step_avg:95.00ms
step:781/1700 train_time:74198ms step_avg:95.00ms
step:782/1700 train_time:74296ms step_avg:95.01ms
step:783/1700 train_time:74394ms step_avg:95.01ms
step:784/1700 train_time:74493ms step_avg:95.02ms
step:785/1700 train_time:74591ms step_avg:95.02ms
step:786/1700 train_time:74688ms step_avg:95.02ms
step:787/1700 train_time:74785ms step_avg:95.03ms
step:788/1700 train_time:74883ms step_avg:95.03ms
step:789/1700 train_time:74980ms step_avg:95.03ms
step:790/1700 train_time:75078ms step_avg:95.04ms
step:791/1700 train_time:75176ms step_avg:95.04ms
step:792/1700 train_time:75274ms step_avg:95.04ms
step:793/1700 train_time:75371ms step_avg:95.05ms
step:794/1700 train_time:75470ms step_avg:95.05ms
step:795/1700 train_time:75567ms step_avg:95.05ms
step:796/1700 train_time:75665ms step_avg:95.06ms
step:797/1700 train_time:75763ms step_avg:95.06ms
step:798/1700 train_time:75862ms step_avg:95.06ms
step:799/1700 train_time:75960ms step_avg:95.07ms
step:800/1700 train_time:76058ms step_avg:95.07ms
step:801/1700 train_time:76155ms step_avg:95.08ms
step:802/1700 train_time:76253ms step_avg:95.08ms
step:803/1700 train_time:76351ms step_avg:95.08ms
step:804/1700 train_time:76448ms step_avg:95.08ms
step:805/1700 train_time:76545ms step_avg:95.09ms
step:806/1700 train_time:76643ms step_avg:95.09ms
step:807/1700 train_time:76740ms step_avg:95.09ms
step:808/1700 train_time:76838ms step_avg:95.10ms
step:809/1700 train_time:76937ms step_avg:95.10ms
step:810/1700 train_time:77037ms step_avg:95.11ms
step:811/1700 train_time:77134ms step_avg:95.11ms
step:812/1700 train_time:77232ms step_avg:95.11ms
step:813/1700 train_time:77329ms step_avg:95.12ms
step:814/1700 train_time:77426ms step_avg:95.12ms
step:815/1700 train_time:77523ms step_avg:95.12ms
step:816/1700 train_time:77620ms step_avg:95.12ms
step:817/1700 train_time:77719ms step_avg:95.13ms
step:818/1700 train_time:77818ms step_avg:95.13ms
step:819/1700 train_time:77916ms step_avg:95.14ms
step:820/1700 train_time:78014ms step_avg:95.14ms
step:821/1700 train_time:78112ms step_avg:95.14ms
step:822/1700 train_time:78209ms step_avg:95.15ms
step:823/1700 train_time:78307ms step_avg:95.15ms
step:824/1700 train_time:78404ms step_avg:95.15ms
step:825/1700 train_time:78501ms step_avg:95.15ms
step:826/1700 train_time:78600ms step_avg:95.16ms
step:827/1700 train_time:78699ms step_avg:95.16ms
step:828/1700 train_time:78797ms step_avg:95.17ms
step:829/1700 train_time:78895ms step_avg:95.17ms
step:830/1700 train_time:78992ms step_avg:95.17ms
step:831/1700 train_time:79090ms step_avg:95.17ms
step:832/1700 train_time:79187ms step_avg:95.18ms
step:833/1700 train_time:79284ms step_avg:95.18ms
step:834/1700 train_time:79382ms step_avg:95.18ms
step:835/1700 train_time:79480ms step_avg:95.19ms
step:836/1700 train_time:79578ms step_avg:95.19ms
step:837/1700 train_time:79676ms step_avg:95.19ms
step:838/1700 train_time:79775ms step_avg:95.20ms
step:839/1700 train_time:79871ms step_avg:95.20ms
step:840/1700 train_time:79969ms step_avg:95.20ms
step:841/1700 train_time:80066ms step_avg:95.20ms
step:842/1700 train_time:80163ms step_avg:95.21ms
step:843/1700 train_time:80261ms step_avg:95.21ms
step:844/1700 train_time:80359ms step_avg:95.21ms
step:845/1700 train_time:80457ms step_avg:95.22ms
step:846/1700 train_time:80555ms step_avg:95.22ms
step:847/1700 train_time:80653ms step_avg:95.22ms
step:848/1700 train_time:80752ms step_avg:95.23ms
step:849/1700 train_time:80849ms step_avg:95.23ms
step:850/1700 train_time:80946ms step_avg:95.23ms
step:851/1700 train_time:81043ms step_avg:95.23ms
step:852/1700 train_time:81141ms step_avg:95.24ms
step:853/1700 train_time:81239ms step_avg:95.24ms
step:854/1700 train_time:81337ms step_avg:95.24ms
step:855/1700 train_time:81435ms step_avg:95.25ms
step:856/1700 train_time:81533ms step_avg:95.25ms
step:857/1700 train_time:81630ms step_avg:95.25ms
step:858/1700 train_time:81728ms step_avg:95.25ms
step:859/1700 train_time:81825ms step_avg:95.26ms
step:860/1700 train_time:81922ms step_avg:95.26ms
step:861/1700 train_time:82021ms step_avg:95.26ms
step:862/1700 train_time:82119ms step_avg:95.27ms
step:863/1700 train_time:82218ms step_avg:95.27ms
step:864/1700 train_time:82315ms step_avg:95.27ms
step:865/1700 train_time:82413ms step_avg:95.28ms
step:866/1700 train_time:82511ms step_avg:95.28ms
step:867/1700 train_time:82609ms step_avg:95.28ms
step:868/1700 train_time:82707ms step_avg:95.28ms
step:869/1700 train_time:82805ms step_avg:95.29ms
step:870/1700 train_time:82902ms step_avg:95.29ms
step:871/1700 train_time:82999ms step_avg:95.29ms
step:872/1700 train_time:83098ms step_avg:95.30ms
step:873/1700 train_time:83196ms step_avg:95.30ms
step:874/1700 train_time:83294ms step_avg:95.30ms
step:875/1700 train_time:83393ms step_avg:95.31ms
step:875/1700 val_loss:3.5354 train_time:83473ms step_avg:95.40ms
step:876/1700 train_time:83498ms step_avg:95.32ms
step:877/1700 train_time:83597ms step_avg:95.32ms
step:878/1700 train_time:83695ms step_avg:95.33ms
step:879/1700 train_time:83793ms step_avg:95.33ms
step:880/1700 train_time:83891ms step_avg:95.33ms
step:881/1700 train_time:83988ms step_avg:95.33ms
step:882/1700 train_time:84085ms step_avg:95.33ms
step:883/1700 train_time:84182ms step_avg:95.34ms
step:884/1700 train_time:84281ms step_avg:95.34ms
step:885/1700 train_time:84379ms step_avg:95.34ms
step:886/1700 train_time:84479ms step_avg:95.35ms
step:887/1700 train_time:84580ms step_avg:95.36ms
step:888/1700 train_time:84680ms step_avg:95.36ms
step:889/1700 train_time:84780ms step_avg:95.37ms
step:890/1700 train_time:84879ms step_avg:95.37ms
step:891/1700 train_time:84978ms step_avg:95.37ms
step:892/1700 train_time:85077ms step_avg:95.38ms
step:893/1700 train_time:85174ms step_avg:95.38ms
step:894/1700 train_time:85272ms step_avg:95.38ms
step:895/1700 train_time:85371ms step_avg:95.39ms
step:896/1700 train_time:85470ms step_avg:95.39ms
step:897/1700 train_time:85570ms step_avg:95.40ms
step:898/1700 train_time:85670ms step_avg:95.40ms
step:899/1700 train_time:85771ms step_avg:95.41ms
step:900/1700 train_time:85871ms step_avg:95.41ms
step:901/1700 train_time:85971ms step_avg:95.42ms
step:902/1700 train_time:86070ms step_avg:95.42ms
step:903/1700 train_time:86170ms step_avg:95.43ms
step:904/1700 train_time:86270ms step_avg:95.43ms
step:905/1700 train_time:86369ms step_avg:95.44ms
step:906/1700 train_time:86468ms step_avg:95.44ms
step:907/1700 train_time:86568ms step_avg:95.44ms
step:908/1700 train_time:86668ms step_avg:95.45ms
step:909/1700 train_time:86769ms step_avg:95.45ms
step:910/1700 train_time:86870ms step_avg:95.46ms
step:911/1700 train_time:86969ms step_avg:95.47ms
step:912/1700 train_time:87070ms step_avg:95.47ms
step:913/1700 train_time:87171ms step_avg:95.48ms
step:914/1700 train_time:87271ms step_avg:95.48ms
step:915/1700 train_time:87370ms step_avg:95.49ms
step:916/1700 train_time:87469ms step_avg:95.49ms
step:917/1700 train_time:87569ms step_avg:95.50ms
step:918/1700 train_time:87670ms step_avg:95.50ms
step:919/1700 train_time:87771ms step_avg:95.51ms
step:920/1700 train_time:87870ms step_avg:95.51ms
step:921/1700 train_time:87970ms step_avg:95.52ms
step:922/1700 train_time:88070ms step_avg:95.52ms
step:923/1700 train_time:88170ms step_avg:95.53ms
step:924/1700 train_time:88269ms step_avg:95.53ms
step:925/1700 train_time:88369ms step_avg:95.53ms
step:926/1700 train_time:88469ms step_avg:95.54ms
step:927/1700 train_time:88569ms step_avg:95.54ms
step:928/1700 train_time:88668ms step_avg:95.55ms
step:929/1700 train_time:88768ms step_avg:95.55ms
step:930/1700 train_time:88869ms step_avg:95.56ms
step:931/1700 train_time:88970ms step_avg:95.56ms
step:932/1700 train_time:89071ms step_avg:95.57ms
step:933/1700 train_time:89172ms step_avg:95.58ms
step:934/1700 train_time:89271ms step_avg:95.58ms
step:935/1700 train_time:89371ms step_avg:95.58ms
step:936/1700 train_time:89470ms step_avg:95.59ms
step:937/1700 train_time:89570ms step_avg:95.59ms
step:938/1700 train_time:89670ms step_avg:95.60ms
step:939/1700 train_time:89770ms step_avg:95.60ms
step:940/1700 train_time:89870ms step_avg:95.61ms
step:941/1700 train_time:89971ms step_avg:95.61ms
step:942/1700 train_time:90071ms step_avg:95.62ms
step:943/1700 train_time:90170ms step_avg:95.62ms
step:944/1700 train_time:90271ms step_avg:95.63ms
step:945/1700 train_time:90370ms step_avg:95.63ms
step:946/1700 train_time:90469ms step_avg:95.63ms
step:947/1700 train_time:90569ms step_avg:95.64ms
step:948/1700 train_time:90668ms step_avg:95.64ms
step:949/1700 train_time:90769ms step_avg:95.65ms
step:950/1700 train_time:90870ms step_avg:95.65ms
step:951/1700 train_time:90970ms step_avg:95.66ms
step:952/1700 train_time:91070ms step_avg:95.66ms
step:953/1700 train_time:91170ms step_avg:95.67ms
step:954/1700 train_time:91270ms step_avg:95.67ms
step:955/1700 train_time:91369ms step_avg:95.67ms
step:956/1700 train_time:91468ms step_avg:95.68ms
step:957/1700 train_time:91569ms step_avg:95.68ms
step:958/1700 train_time:91670ms step_avg:95.69ms
step:959/1700 train_time:91770ms step_avg:95.69ms
step:960/1700 train_time:91871ms step_avg:95.70ms
step:961/1700 train_time:91970ms step_avg:95.70ms
step:962/1700 train_time:92070ms step_avg:95.71ms
step:963/1700 train_time:92171ms step_avg:95.71ms
step:964/1700 train_time:92271ms step_avg:95.72ms
step:965/1700 train_time:92371ms step_avg:95.72ms
step:966/1700 train_time:92470ms step_avg:95.72ms
step:967/1700 train_time:92571ms step_avg:95.73ms
step:968/1700 train_time:92670ms step_avg:95.73ms
step:969/1700 train_time:92770ms step_avg:95.74ms
step:970/1700 train_time:92869ms step_avg:95.74ms
step:971/1700 train_time:92969ms step_avg:95.75ms
step:972/1700 train_time:93069ms step_avg:95.75ms
step:973/1700 train_time:93170ms step_avg:95.76ms
step:974/1700 train_time:93270ms step_avg:95.76ms
step:975/1700 train_time:93370ms step_avg:95.76ms
step:976/1700 train_time:93469ms step_avg:95.77ms
step:977/1700 train_time:93570ms step_avg:95.77ms
step:978/1700 train_time:93670ms step_avg:95.78ms
step:979/1700 train_time:93769ms step_avg:95.78ms
step:980/1700 train_time:93869ms step_avg:95.78ms
step:981/1700 train_time:93969ms step_avg:95.79ms
step:982/1700 train_time:94069ms step_avg:95.79ms
step:983/1700 train_time:94169ms step_avg:95.80ms
step:984/1700 train_time:94270ms step_avg:95.80ms
step:985/1700 train_time:94370ms step_avg:95.81ms
step:986/1700 train_time:94470ms step_avg:95.81ms
step:987/1700 train_time:94570ms step_avg:95.82ms
step:988/1700 train_time:94670ms step_avg:95.82ms
step:989/1700 train_time:94770ms step_avg:95.82ms
step:990/1700 train_time:94870ms step_avg:95.83ms
step:991/1700 train_time:94970ms step_avg:95.83ms
step:992/1700 train_time:95069ms step_avg:95.84ms
step:993/1700 train_time:95170ms step_avg:95.84ms
step:994/1700 train_time:95269ms step_avg:95.84ms
step:995/1700 train_time:95369ms step_avg:95.85ms
step:996/1700 train_time:95469ms step_avg:95.85ms
step:997/1700 train_time:95570ms step_avg:95.86ms
step:998/1700 train_time:95670ms step_avg:95.86ms
step:999/1700 train_time:95770ms step_avg:95.87ms
step:1000/1700 train_time:95870ms step_avg:95.87ms
step:1000/1700 val_loss:3.4911 train_time:95953ms step_avg:95.95ms
step:1001/1700 train_time:95978ms step_avg:95.88ms
step:1002/1700 train_time:96080ms step_avg:95.89ms
step:1003/1700 train_time:96179ms step_avg:95.89ms
step:1004/1700 train_time:96278ms step_avg:95.89ms
step:1005/1700 train_time:96377ms step_avg:95.90ms
step:1006/1700 train_time:96474ms step_avg:95.90ms
step:1007/1700 train_time:96573ms step_avg:95.90ms
step:1008/1700 train_time:96670ms step_avg:95.90ms
step:1009/1700 train_time:96769ms step_avg:95.91ms
step:1010/1700 train_time:96867ms step_avg:95.91ms
step:1011/1700 train_time:96969ms step_avg:95.91ms
step:1012/1700 train_time:97069ms step_avg:95.92ms
step:1013/1700 train_time:97170ms step_avg:95.92ms
step:1014/1700 train_time:97268ms step_avg:95.93ms
step:1015/1700 train_time:97369ms step_avg:95.93ms
step:1016/1700 train_time:97469ms step_avg:95.93ms
step:1017/1700 train_time:97569ms step_avg:95.94ms
step:1018/1700 train_time:97668ms step_avg:95.94ms
step:1019/1700 train_time:97766ms step_avg:95.94ms
step:1020/1700 train_time:97866ms step_avg:95.95ms
step:1021/1700 train_time:97965ms step_avg:95.95ms
step:1022/1700 train_time:98064ms step_avg:95.95ms
step:1023/1700 train_time:98163ms step_avg:95.96ms
step:1024/1700 train_time:98262ms step_avg:95.96ms
step:1025/1700 train_time:98362ms step_avg:95.96ms
step:1026/1700 train_time:98462ms step_avg:95.97ms
step:1027/1700 train_time:98561ms step_avg:95.97ms
step:1028/1700 train_time:98661ms step_avg:95.97ms
step:1029/1700 train_time:98761ms step_avg:95.98ms
step:1030/1700 train_time:98860ms step_avg:95.98ms
step:1031/1700 train_time:98958ms step_avg:95.98ms
step:1032/1700 train_time:99057ms step_avg:95.99ms
step:1033/1700 train_time:99157ms step_avg:95.99ms
step:1034/1700 train_time:99255ms step_avg:95.99ms
step:1035/1700 train_time:99355ms step_avg:96.00ms
step:1036/1700 train_time:99456ms step_avg:96.00ms
step:1037/1700 train_time:99555ms step_avg:96.00ms
step:1038/1700 train_time:99656ms step_avg:96.01ms
step:1039/1700 train_time:99755ms step_avg:96.01ms
step:1040/1700 train_time:99855ms step_avg:96.01ms
step:1041/1700 train_time:99954ms step_avg:96.02ms
step:1042/1700 train_time:100054ms step_avg:96.02ms
step:1043/1700 train_time:100154ms step_avg:96.02ms
step:1044/1700 train_time:100253ms step_avg:96.03ms
step:1045/1700 train_time:100354ms step_avg:96.03ms
step:1046/1700 train_time:100454ms step_avg:96.04ms
step:1047/1700 train_time:100553ms step_avg:96.04ms
step:1048/1700 train_time:100654ms step_avg:96.04ms
step:1049/1700 train_time:100755ms step_avg:96.05ms
step:1050/1700 train_time:100856ms step_avg:96.05ms
step:1051/1700 train_time:100955ms step_avg:96.06ms
step:1052/1700 train_time:101054ms step_avg:96.06ms
step:1053/1700 train_time:101154ms step_avg:96.06ms
step:1054/1700 train_time:101253ms step_avg:96.07ms
step:1055/1700 train_time:101353ms step_avg:96.07ms
step:1056/1700 train_time:101454ms step_avg:96.07ms
step:1057/1700 train_time:101554ms step_avg:96.08ms
step:1058/1700 train_time:101654ms step_avg:96.08ms
step:1059/1700 train_time:101754ms step_avg:96.09ms
step:1060/1700 train_time:101854ms step_avg:96.09ms
step:1061/1700 train_time:101954ms step_avg:96.09ms
step:1062/1700 train_time:102055ms step_avg:96.10ms
step:1063/1700 train_time:102155ms step_avg:96.10ms
step:1064/1700 train_time:102256ms step_avg:96.10ms
step:1065/1700 train_time:102356ms step_avg:96.11ms
step:1066/1700 train_time:102455ms step_avg:96.11ms
step:1067/1700 train_time:102556ms step_avg:96.12ms
step:1068/1700 train_time:102656ms step_avg:96.12ms
step:1069/1700 train_time:102755ms step_avg:96.12ms
step:1070/1700 train_time:102855ms step_avg:96.13ms
step:1071/1700 train_time:102956ms step_avg:96.13ms
step:1072/1700 train_time:103056ms step_avg:96.13ms
step:1073/1700 train_time:103155ms step_avg:96.14ms
step:1074/1700 train_time:103255ms step_avg:96.14ms
step:1075/1700 train_time:103355ms step_avg:96.14ms
step:1076/1700 train_time:103455ms step_avg:96.15ms
step:1077/1700 train_time:103555ms step_avg:96.15ms
step:1078/1700 train_time:103655ms step_avg:96.15ms
step:1079/1700 train_time:103754ms step_avg:96.16ms
step:1080/1700 train_time:103855ms step_avg:96.16ms
step:1081/1700 train_time:103955ms step_avg:96.17ms
step:1082/1700 train_time:104055ms step_avg:96.17ms
step:1083/1700 train_time:104155ms step_avg:96.17ms
step:1084/1700 train_time:104254ms step_avg:96.18ms
step:1085/1700 train_time:104356ms step_avg:96.18ms
step:1086/1700 train_time:104455ms step_avg:96.18ms
step:1087/1700 train_time:104555ms step_avg:96.19ms
step:1088/1700 train_time:104654ms step_avg:96.19ms
step:1089/1700 train_time:104753ms step_avg:96.19ms
step:1090/1700 train_time:104855ms step_avg:96.20ms
step:1091/1700 train_time:104956ms step_avg:96.20ms
step:1092/1700 train_time:105055ms step_avg:96.20ms
step:1093/1700 train_time:105155ms step_avg:96.21ms
step:1094/1700 train_time:105255ms step_avg:96.21ms
step:1095/1700 train_time:105356ms step_avg:96.22ms
step:1096/1700 train_time:105456ms step_avg:96.22ms
step:1097/1700 train_time:105555ms step_avg:96.22ms
step:1098/1700 train_time:105655ms step_avg:96.23ms
step:1099/1700 train_time:105756ms step_avg:96.23ms
step:1100/1700 train_time:105855ms step_avg:96.23ms
step:1101/1700 train_time:105956ms step_avg:96.24ms
step:1102/1700 train_time:106056ms step_avg:96.24ms
step:1103/1700 train_time:106156ms step_avg:96.24ms
step:1104/1700 train_time:106256ms step_avg:96.25ms
step:1105/1700 train_time:106358ms step_avg:96.25ms
step:1106/1700 train_time:106457ms step_avg:96.25ms
step:1107/1700 train_time:106557ms step_avg:96.26ms
step:1108/1700 train_time:106657ms step_avg:96.26ms
step:1109/1700 train_time:106756ms step_avg:96.26ms
step:1110/1700 train_time:106857ms step_avg:96.27ms
step:1111/1700 train_time:106958ms step_avg:96.27ms
step:1112/1700 train_time:107058ms step_avg:96.28ms
step:1113/1700 train_time:107159ms step_avg:96.28ms
step:1114/1700 train_time:107258ms step_avg:96.28ms
step:1115/1700 train_time:107357ms step_avg:96.28ms
step:1116/1700 train_time:107459ms step_avg:96.29ms
step:1117/1700 train_time:107558ms step_avg:96.29ms
step:1118/1700 train_time:107657ms step_avg:96.29ms
step:1119/1700 train_time:107756ms step_avg:96.30ms
step:1120/1700 train_time:107856ms step_avg:96.30ms
step:1121/1700 train_time:107956ms step_avg:96.30ms
step:1122/1700 train_time:108056ms step_avg:96.31ms
step:1123/1700 train_time:108156ms step_avg:96.31ms
step:1124/1700 train_time:108256ms step_avg:96.31ms
step:1125/1700 train_time:108356ms step_avg:96.32ms
step:1125/1700 val_loss:3.4410 train_time:108437ms step_avg:96.39ms
step:1126/1700 train_time:108463ms step_avg:96.33ms
step:1127/1700 train_time:108561ms step_avg:96.33ms
step:1128/1700 train_time:108661ms step_avg:96.33ms
step:1129/1700 train_time:108760ms step_avg:96.33ms
step:1130/1700 train_time:108859ms step_avg:96.34ms
step:1131/1700 train_time:108957ms step_avg:96.34ms
step:1132/1700 train_time:109056ms step_avg:96.34ms
step:1133/1700 train_time:109154ms step_avg:96.34ms
step:1134/1700 train_time:109252ms step_avg:96.34ms
step:1135/1700 train_time:109351ms step_avg:96.34ms
step:1136/1700 train_time:109452ms step_avg:96.35ms
step:1137/1700 train_time:109556ms step_avg:96.36ms
step:1138/1700 train_time:109658ms step_avg:96.36ms
step:1139/1700 train_time:109758ms step_avg:96.36ms
step:1140/1700 train_time:109859ms step_avg:96.37ms
step:1141/1700 train_time:109959ms step_avg:96.37ms
step:1142/1700 train_time:110058ms step_avg:96.37ms
step:1143/1700 train_time:110157ms step_avg:96.38ms
step:1144/1700 train_time:110257ms step_avg:96.38ms
step:1145/1700 train_time:110358ms step_avg:96.38ms
step:1146/1700 train_time:110459ms step_avg:96.39ms
step:1147/1700 train_time:110560ms step_avg:96.39ms
step:1148/1700 train_time:110660ms step_avg:96.39ms
step:1149/1700 train_time:110761ms step_avg:96.40ms
step:1150/1700 train_time:110861ms step_avg:96.40ms
step:1151/1700 train_time:110960ms step_avg:96.40ms
step:1152/1700 train_time:111059ms step_avg:96.41ms
step:1153/1700 train_time:111160ms step_avg:96.41ms
step:1154/1700 train_time:111259ms step_avg:96.41ms
step:1155/1700 train_time:111359ms step_avg:96.41ms
step:1156/1700 train_time:111460ms step_avg:96.42ms
step:1157/1700 train_time:111560ms step_avg:96.42ms
step:1158/1700 train_time:111661ms step_avg:96.43ms
step:1159/1700 train_time:111761ms step_avg:96.43ms
step:1160/1700 train_time:111861ms step_avg:96.43ms
step:1161/1700 train_time:111961ms step_avg:96.44ms
step:1162/1700 train_time:112061ms step_avg:96.44ms
step:1163/1700 train_time:112160ms step_avg:96.44ms
step:1164/1700 train_time:112261ms step_avg:96.44ms
step:1165/1700 train_time:112361ms step_avg:96.45ms
step:1166/1700 train_time:112462ms step_avg:96.45ms
step:1167/1700 train_time:112563ms step_avg:96.45ms
step:1168/1700 train_time:112663ms step_avg:96.46ms
step:1169/1700 train_time:112763ms step_avg:96.46ms
step:1170/1700 train_time:112863ms step_avg:96.46ms
step:1171/1700 train_time:112963ms step_avg:96.47ms
step:1172/1700 train_time:113064ms step_avg:96.47ms
step:1173/1700 train_time:113163ms step_avg:96.47ms
step:1174/1700 train_time:113263ms step_avg:96.48ms
step:1175/1700 train_time:113363ms step_avg:96.48ms
step:1176/1700 train_time:113463ms step_avg:96.48ms
step:1177/1700 train_time:113563ms step_avg:96.49ms
step:1178/1700 train_time:113664ms step_avg:96.49ms
step:1179/1700 train_time:113763ms step_avg:96.49ms
step:1180/1700 train_time:113864ms step_avg:96.50ms
step:1181/1700 train_time:113966ms step_avg:96.50ms
step:1182/1700 train_time:114068ms step_avg:96.50ms
step:1183/1700 train_time:114168ms step_avg:96.51ms
step:1184/1700 train_time:114268ms step_avg:96.51ms
step:1185/1700 train_time:114369ms step_avg:96.51ms
step:1186/1700 train_time:114470ms step_avg:96.52ms
step:1187/1700 train_time:114569ms step_avg:96.52ms
step:1188/1700 train_time:114670ms step_avg:96.52ms
step:1189/1700 train_time:114770ms step_avg:96.53ms
step:1190/1700 train_time:114870ms step_avg:96.53ms
step:1191/1700 train_time:114973ms step_avg:96.53ms
step:1192/1700 train_time:115073ms step_avg:96.54ms
step:1193/1700 train_time:115174ms step_avg:96.54ms
step:1194/1700 train_time:115274ms step_avg:96.54ms
step:1195/1700 train_time:115375ms step_avg:96.55ms
step:1196/1700 train_time:115476ms step_avg:96.55ms
step:1197/1700 train_time:115577ms step_avg:96.56ms
step:1198/1700 train_time:115678ms step_avg:96.56ms
step:1199/1700 train_time:115778ms step_avg:96.56ms
step:1200/1700 train_time:115878ms step_avg:96.56ms
step:1201/1700 train_time:115978ms step_avg:96.57ms
step:1202/1700 train_time:116079ms step_avg:96.57ms
step:1203/1700 train_time:116179ms step_avg:96.57ms
step:1204/1700 train_time:116278ms step_avg:96.58ms
step:1205/1700 train_time:116379ms step_avg:96.58ms
step:1206/1700 train_time:116479ms step_avg:96.58ms
step:1207/1700 train_time:116579ms step_avg:96.59ms
step:1208/1700 train_time:116679ms step_avg:96.59ms
step:1209/1700 train_time:116780ms step_avg:96.59ms
step:1210/1700 train_time:116880ms step_avg:96.60ms
step:1211/1700 train_time:116981ms step_avg:96.60ms
step:1212/1700 train_time:117081ms step_avg:96.60ms
step:1213/1700 train_time:117181ms step_avg:96.60ms
step:1214/1700 train_time:117281ms step_avg:96.61ms
step:1215/1700 train_time:117382ms step_avg:96.61ms
step:1216/1700 train_time:117482ms step_avg:96.61ms
step:1217/1700 train_time:117582ms step_avg:96.62ms
step:1218/1700 train_time:117684ms step_avg:96.62ms
step:1219/1700 train_time:117786ms step_avg:96.62ms
step:1220/1700 train_time:117886ms step_avg:96.63ms
step:1221/1700 train_time:117985ms step_avg:96.63ms
step:1222/1700 train_time:118085ms step_avg:96.63ms
step:1223/1700 train_time:118187ms step_avg:96.64ms
step:1224/1700 train_time:118287ms step_avg:96.64ms
step:1225/1700 train_time:118388ms step_avg:96.64ms
step:1226/1700 train_time:118489ms step_avg:96.65ms
step:1227/1700 train_time:118588ms step_avg:96.65ms
step:1228/1700 train_time:118689ms step_avg:96.65ms
step:1229/1700 train_time:118790ms step_avg:96.66ms
step:1230/1700 train_time:118890ms step_avg:96.66ms
step:1231/1700 train_time:118990ms step_avg:96.66ms
step:1232/1700 train_time:119089ms step_avg:96.66ms
step:1233/1700 train_time:119189ms step_avg:96.67ms
step:1234/1700 train_time:119289ms step_avg:96.67ms
step:1235/1700 train_time:119389ms step_avg:96.67ms
step:1236/1700 train_time:119490ms step_avg:96.67ms
step:1237/1700 train_time:119590ms step_avg:96.68ms
step:1238/1700 train_time:119690ms step_avg:96.68ms
step:1239/1700 train_time:119789ms step_avg:96.68ms
step:1240/1700 train_time:119890ms step_avg:96.69ms
step:1241/1700 train_time:119991ms step_avg:96.69ms
step:1242/1700 train_time:120092ms step_avg:96.69ms
step:1243/1700 train_time:120194ms step_avg:96.70ms
step:1244/1700 train_time:120294ms step_avg:96.70ms
step:1245/1700 train_time:120396ms step_avg:96.70ms
step:1246/1700 train_time:120496ms step_avg:96.71ms
step:1247/1700 train_time:120596ms step_avg:96.71ms
step:1248/1700 train_time:120697ms step_avg:96.71ms
step:1249/1700 train_time:120797ms step_avg:96.71ms
step:1250/1700 train_time:120897ms step_avg:96.72ms
step:1250/1700 val_loss:3.3950 train_time:120979ms step_avg:96.78ms
step:1251/1700 train_time:121004ms step_avg:96.73ms
step:1252/1700 train_time:121106ms step_avg:96.73ms
step:1253/1700 train_time:121208ms step_avg:96.73ms
step:1254/1700 train_time:121307ms step_avg:96.74ms
step:1255/1700 train_time:121407ms step_avg:96.74ms
step:1256/1700 train_time:121507ms step_avg:96.74ms
step:1257/1700 train_time:121607ms step_avg:96.74ms
step:1258/1700 train_time:121707ms step_avg:96.75ms
step:1259/1700 train_time:121806ms step_avg:96.75ms
step:1260/1700 train_time:121907ms step_avg:96.75ms
step:1261/1700 train_time:122009ms step_avg:96.76ms
step:1262/1700 train_time:122111ms step_avg:96.76ms
step:1263/1700 train_time:122211ms step_avg:96.76ms
step:1264/1700 train_time:122311ms step_avg:96.77ms
step:1265/1700 train_time:122410ms step_avg:96.77ms
step:1266/1700 train_time:122510ms step_avg:96.77ms
step:1267/1700 train_time:122610ms step_avg:96.77ms
step:1268/1700 train_time:122710ms step_avg:96.77ms
step:1269/1700 train_time:122810ms step_avg:96.78ms
step:1270/1700 train_time:122911ms step_avg:96.78ms
step:1271/1700 train_time:123015ms step_avg:96.79ms
step:1272/1700 train_time:123114ms step_avg:96.79ms
step:1273/1700 train_time:123214ms step_avg:96.79ms
step:1274/1700 train_time:123314ms step_avg:96.79ms
step:1275/1700 train_time:123413ms step_avg:96.79ms
step:1276/1700 train_time:123514ms step_avg:96.80ms
step:1277/1700 train_time:123614ms step_avg:96.80ms
step:1278/1700 train_time:123714ms step_avg:96.80ms
step:1279/1700 train_time:123814ms step_avg:96.81ms
step:1280/1700 train_time:123916ms step_avg:96.81ms
step:1281/1700 train_time:124017ms step_avg:96.81ms
step:1282/1700 train_time:124117ms step_avg:96.82ms
step:1283/1700 train_time:124218ms step_avg:96.82ms
step:1284/1700 train_time:124318ms step_avg:96.82ms
step:1285/1700 train_time:124417ms step_avg:96.82ms
step:1286/1700 train_time:124517ms step_avg:96.82ms
step:1287/1700 train_time:124618ms step_avg:96.83ms
step:1288/1700 train_time:124718ms step_avg:96.83ms
step:1289/1700 train_time:124819ms step_avg:96.83ms
step:1290/1700 train_time:124921ms step_avg:96.84ms
step:1291/1700 train_time:125021ms step_avg:96.84ms
step:1292/1700 train_time:125121ms step_avg:96.84ms
step:1293/1700 train_time:125222ms step_avg:96.85ms
step:1294/1700 train_time:125323ms step_avg:96.85ms
step:1295/1700 train_time:125423ms step_avg:96.85ms
step:1296/1700 train_time:125524ms step_avg:96.85ms
step:1297/1700 train_time:125624ms step_avg:96.86ms
step:1298/1700 train_time:125726ms step_avg:96.86ms
step:1299/1700 train_time:125826ms step_avg:96.86ms
step:1300/1700 train_time:125928ms step_avg:96.87ms
step:1301/1700 train_time:126028ms step_avg:96.87ms
step:1302/1700 train_time:126128ms step_avg:96.87ms
step:1303/1700 train_time:126229ms step_avg:96.88ms
step:1304/1700 train_time:126329ms step_avg:96.88ms
step:1305/1700 train_time:126429ms step_avg:96.88ms
step:1306/1700 train_time:126529ms step_avg:96.88ms
step:1307/1700 train_time:126630ms step_avg:96.89ms
step:1308/1700 train_time:126731ms step_avg:96.89ms
step:1309/1700 train_time:126832ms step_avg:96.89ms
step:1310/1700 train_time:126933ms step_avg:96.90ms
step:1311/1700 train_time:127033ms step_avg:96.90ms
step:1312/1700 train_time:127133ms step_avg:96.90ms
step:1313/1700 train_time:127233ms step_avg:96.90ms
step:1314/1700 train_time:127333ms step_avg:96.90ms
step:1315/1700 train_time:127433ms step_avg:96.91ms
step:1316/1700 train_time:127533ms step_avg:96.91ms
step:1317/1700 train_time:127633ms step_avg:96.91ms
step:1318/1700 train_time:127734ms step_avg:96.91ms
step:1319/1700 train_time:127834ms step_avg:96.92ms
step:1320/1700 train_time:127935ms step_avg:96.92ms
step:1321/1700 train_time:128036ms step_avg:96.92ms
step:1322/1700 train_time:128136ms step_avg:96.93ms
step:1323/1700 train_time:128237ms step_avg:96.93ms
step:1324/1700 train_time:128338ms step_avg:96.93ms
step:1325/1700 train_time:128439ms step_avg:96.93ms
step:1326/1700 train_time:128539ms step_avg:96.94ms
step:1327/1700 train_time:128639ms step_avg:96.94ms
step:1328/1700 train_time:128739ms step_avg:96.94ms
step:1329/1700 train_time:128839ms step_avg:96.94ms
step:1330/1700 train_time:128940ms step_avg:96.95ms
step:1331/1700 train_time:129040ms step_avg:96.95ms
step:1332/1700 train_time:129141ms step_avg:96.95ms
step:1333/1700 train_time:129242ms step_avg:96.96ms
step:1334/1700 train_time:129343ms step_avg:96.96ms
step:1335/1700 train_time:129444ms step_avg:96.96ms
step:1336/1700 train_time:129546ms step_avg:96.97ms
step:1337/1700 train_time:129646ms step_avg:96.97ms
step:1338/1700 train_time:129746ms step_avg:96.97ms
step:1339/1700 train_time:129847ms step_avg:96.97ms
step:1340/1700 train_time:129947ms step_avg:96.98ms
step:1341/1700 train_time:130048ms step_avg:96.98ms
step:1342/1700 train_time:130148ms step_avg:96.98ms
step:1343/1700 train_time:130248ms step_avg:96.98ms
step:1344/1700 train_time:130348ms step_avg:96.99ms
step:1345/1700 train_time:130449ms step_avg:96.99ms
step:1346/1700 train_time:130551ms step_avg:96.99ms
step:1347/1700 train_time:130652ms step_avg:96.99ms
step:1348/1700 train_time:130752ms step_avg:97.00ms
step:1349/1700 train_time:130852ms step_avg:97.00ms
step:1350/1700 train_time:130953ms step_avg:97.00ms
step:1351/1700 train_time:131053ms step_avg:97.00ms
step:1352/1700 train_time:131153ms step_avg:97.01ms
step:1353/1700 train_time:131253ms step_avg:97.01ms
step:1354/1700 train_time:131353ms step_avg:97.01ms
step:1355/1700 train_time:131453ms step_avg:97.01ms
step:1356/1700 train_time:131553ms step_avg:97.02ms
step:1357/1700 train_time:131653ms step_avg:97.02ms
step:1358/1700 train_time:131753ms step_avg:97.02ms
step:1359/1700 train_time:131854ms step_avg:97.02ms
step:1360/1700 train_time:131954ms step_avg:97.02ms
step:1361/1700 train_time:132054ms step_avg:97.03ms
step:1362/1700 train_time:132153ms step_avg:97.03ms
step:1363/1700 train_time:132254ms step_avg:97.03ms
step:1364/1700 train_time:132355ms step_avg:97.03ms
step:1365/1700 train_time:132457ms step_avg:97.04ms
step:1366/1700 train_time:132558ms step_avg:97.04ms
step:1367/1700 train_time:132659ms step_avg:97.04ms
step:1368/1700 train_time:132760ms step_avg:97.05ms
step:1369/1700 train_time:132860ms step_avg:97.05ms
step:1370/1700 train_time:132960ms step_avg:97.05ms
step:1371/1700 train_time:133061ms step_avg:97.05ms
step:1372/1700 train_time:133161ms step_avg:97.06ms
step:1373/1700 train_time:133262ms step_avg:97.06ms
step:1374/1700 train_time:133364ms step_avg:97.06ms
step:1375/1700 train_time:133466ms step_avg:97.07ms
step:1375/1700 val_loss:3.3555 train_time:133549ms step_avg:97.13ms
step:1376/1700 train_time:133573ms step_avg:97.07ms
step:1377/1700 train_time:133675ms step_avg:97.08ms
step:1378/1700 train_time:133778ms step_avg:97.08ms
step:1379/1700 train_time:133879ms step_avg:97.08ms
step:1380/1700 train_time:133979ms step_avg:97.09ms
step:1381/1700 train_time:134079ms step_avg:97.09ms
step:1382/1700 train_time:134179ms step_avg:97.09ms
step:1383/1700 train_time:134278ms step_avg:97.09ms
step:1384/1700 train_time:134377ms step_avg:97.09ms
step:1385/1700 train_time:134477ms step_avg:97.10ms
step:1386/1700 train_time:134580ms step_avg:97.10ms
step:1387/1700 train_time:134683ms step_avg:97.10ms
step:1388/1700 train_time:134785ms step_avg:97.11ms
step:1389/1700 train_time:134886ms step_avg:97.11ms
step:1390/1700 train_time:134987ms step_avg:97.11ms
step:1391/1700 train_time:135088ms step_avg:97.12ms
step:1392/1700 train_time:135190ms step_avg:97.12ms
step:1393/1700 train_time:135290ms step_avg:97.12ms
step:1394/1700 train_time:135390ms step_avg:97.12ms
step:1395/1700 train_time:135493ms step_avg:97.13ms
step:1396/1700 train_time:135595ms step_avg:97.13ms
step:1397/1700 train_time:135696ms step_avg:97.13ms
step:1398/1700 train_time:135796ms step_avg:97.14ms
step:1399/1700 train_time:135899ms step_avg:97.14ms
step:1400/1700 train_time:136001ms step_avg:97.14ms
step:1401/1700 train_time:136103ms step_avg:97.15ms
step:1402/1700 train_time:136204ms step_avg:97.15ms
step:1403/1700 train_time:136305ms step_avg:97.15ms
step:1404/1700 train_time:136407ms step_avg:97.16ms
step:1405/1700 train_time:136510ms step_avg:97.16ms
step:1406/1700 train_time:136611ms step_avg:97.16ms
step:1407/1700 train_time:136712ms step_avg:97.17ms
step:1408/1700 train_time:136813ms step_avg:97.17ms
step:1409/1700 train_time:136919ms step_avg:97.17ms
step:1410/1700 train_time:137020ms step_avg:97.18ms
step:1411/1700 train_time:137121ms step_avg:97.18ms
step:1412/1700 train_time:137222ms step_avg:97.18ms
step:1413/1700 train_time:137323ms step_avg:97.19ms
step:1414/1700 train_time:137424ms step_avg:97.19ms
step:1415/1700 train_time:137525ms step_avg:97.19ms
step:1416/1700 train_time:137627ms step_avg:97.19ms
step:1417/1700 train_time:137728ms step_avg:97.20ms
step:1418/1700 train_time:137829ms step_avg:97.20ms
step:1419/1700 train_time:137930ms step_avg:97.20ms
step:1420/1700 train_time:138032ms step_avg:97.21ms
step:1421/1700 train_time:138135ms step_avg:97.21ms
step:1422/1700 train_time:138234ms step_avg:97.21ms
step:1423/1700 train_time:138335ms step_avg:97.21ms
step:1424/1700 train_time:138438ms step_avg:97.22ms
step:1425/1700 train_time:138541ms step_avg:97.22ms
step:1426/1700 train_time:138642ms step_avg:97.22ms
step:1427/1700 train_time:138743ms step_avg:97.23ms
step:1428/1700 train_time:138845ms step_avg:97.23ms
step:1429/1700 train_time:138946ms step_avg:97.23ms
step:1430/1700 train_time:139048ms step_avg:97.24ms
step:1431/1700 train_time:139149ms step_avg:97.24ms
step:1432/1700 train_time:139249ms step_avg:97.24ms
step:1433/1700 train_time:139351ms step_avg:97.24ms
step:1434/1700 train_time:139452ms step_avg:97.25ms
step:1435/1700 train_time:139557ms step_avg:97.25ms
step:1436/1700 train_time:139659ms step_avg:97.26ms
step:1437/1700 train_time:139762ms step_avg:97.26ms
step:1438/1700 train_time:139862ms step_avg:97.26ms
step:1439/1700 train_time:139964ms step_avg:97.26ms
step:1440/1700 train_time:140067ms step_avg:97.27ms
step:1441/1700 train_time:140169ms step_avg:97.27ms
step:1442/1700 train_time:140269ms step_avg:97.27ms
step:1443/1700 train_time:140370ms step_avg:97.28ms
step:1444/1700 train_time:140471ms step_avg:97.28ms
step:1445/1700 train_time:140573ms step_avg:97.28ms
step:1446/1700 train_time:140674ms step_avg:97.28ms
step:1447/1700 train_time:140774ms step_avg:97.29ms
step:1448/1700 train_time:140876ms step_avg:97.29ms
step:1449/1700 train_time:140975ms step_avg:97.29ms
step:1450/1700 train_time:141078ms step_avg:97.29ms
step:1451/1700 train_time:141179ms step_avg:97.30ms
step:1452/1700 train_time:141282ms step_avg:97.30ms
step:1453/1700 train_time:141385ms step_avg:97.31ms
step:1454/1700 train_time:141488ms step_avg:97.31ms
step:1455/1700 train_time:141589ms step_avg:97.31ms
step:1456/1700 train_time:141689ms step_avg:97.31ms
step:1457/1700 train_time:141790ms step_avg:97.32ms
step:1458/1700 train_time:141891ms step_avg:97.32ms
step:1459/1700 train_time:141993ms step_avg:97.32ms
step:1460/1700 train_time:142093ms step_avg:97.32ms
step:1461/1700 train_time:142195ms step_avg:97.33ms
step:1462/1700 train_time:142296ms step_avg:97.33ms
step:1463/1700 train_time:142398ms step_avg:97.33ms
step:1464/1700 train_time:142501ms step_avg:97.34ms
step:1465/1700 train_time:142602ms step_avg:97.34ms
step:1466/1700 train_time:142704ms step_avg:97.34ms
step:1467/1700 train_time:142805ms step_avg:97.34ms
step:1468/1700 train_time:142907ms step_avg:97.35ms
step:1469/1700 train_time:143008ms step_avg:97.35ms
step:1470/1700 train_time:143109ms step_avg:97.35ms
step:1471/1700 train_time:143211ms step_avg:97.36ms
step:1472/1700 train_time:143313ms step_avg:97.36ms
step:1473/1700 train_time:143415ms step_avg:97.36ms
step:1474/1700 train_time:143516ms step_avg:97.37ms
step:1475/1700 train_time:143616ms step_avg:97.37ms
step:1476/1700 train_time:143719ms step_avg:97.37ms
step:1477/1700 train_time:143821ms step_avg:97.37ms
step:1478/1700 train_time:143923ms step_avg:97.38ms
step:1479/1700 train_time:144025ms step_avg:97.38ms
step:1480/1700 train_time:144127ms step_avg:97.38ms
step:1481/1700 train_time:144229ms step_avg:97.39ms
step:1482/1700 train_time:144333ms step_avg:97.39ms
step:1483/1700 train_time:144433ms step_avg:97.39ms
step:1484/1700 train_time:144535ms step_avg:97.40ms
step:1485/1700 train_time:144637ms step_avg:97.40ms
step:1486/1700 train_time:144740ms step_avg:97.40ms
step:1487/1700 train_time:144841ms step_avg:97.40ms
step:1488/1700 train_time:144943ms step_avg:97.41ms
step:1489/1700 train_time:145044ms step_avg:97.41ms
step:1490/1700 train_time:145145ms step_avg:97.41ms
step:1491/1700 train_time:145247ms step_avg:97.42ms
step:1492/1700 train_time:145350ms step_avg:97.42ms
step:1493/1700 train_time:145451ms step_avg:97.42ms
step:1494/1700 train_time:145552ms step_avg:97.42ms
step:1495/1700 train_time:145653ms step_avg:97.43ms
step:1496/1700 train_time:145756ms step_avg:97.43ms
step:1497/1700 train_time:145855ms step_avg:97.43ms
step:1498/1700 train_time:145957ms step_avg:97.43ms
step:1499/1700 train_time:146058ms step_avg:97.44ms
step:1500/1700 train_time:146160ms step_avg:97.44ms
step:1500/1700 val_loss:3.3210 train_time:146245ms step_avg:97.50ms
step:1501/1700 train_time:146270ms step_avg:97.45ms
step:1502/1700 train_time:146372ms step_avg:97.45ms
step:1503/1700 train_time:146474ms step_avg:97.45ms
step:1504/1700 train_time:146573ms step_avg:97.46ms
step:1505/1700 train_time:146674ms step_avg:97.46ms
step:1506/1700 train_time:146776ms step_avg:97.46ms
step:1507/1700 train_time:146876ms step_avg:97.46ms
step:1508/1700 train_time:146977ms step_avg:97.46ms
step:1509/1700 train_time:147078ms step_avg:97.47ms
step:1510/1700 train_time:147180ms step_avg:97.47ms
step:1511/1700 train_time:147282ms step_avg:97.47ms
step:1512/1700 train_time:147385ms step_avg:97.48ms
step:1513/1700 train_time:147488ms step_avg:97.48ms
step:1514/1700 train_time:147590ms step_avg:97.48ms
step:1515/1700 train_time:147694ms step_avg:97.49ms
step:1516/1700 train_time:147796ms step_avg:97.49ms
step:1517/1700 train_time:147897ms step_avg:97.49ms
step:1518/1700 train_time:147997ms step_avg:97.49ms
step:1519/1700 train_time:148100ms step_avg:97.50ms
step:1520/1700 train_time:148201ms step_avg:97.50ms
step:1521/1700 train_time:148302ms step_avg:97.50ms
step:1522/1700 train_time:148406ms step_avg:97.51ms
step:1523/1700 train_time:148507ms step_avg:97.51ms
step:1524/1700 train_time:148609ms step_avg:97.51ms
step:1525/1700 train_time:148712ms step_avg:97.52ms
step:1526/1700 train_time:148813ms step_avg:97.52ms
step:1527/1700 train_time:148914ms step_avg:97.52ms
step:1528/1700 train_time:149016ms step_avg:97.52ms
step:1529/1700 train_time:149117ms step_avg:97.53ms
step:1530/1700 train_time:149218ms step_avg:97.53ms
step:1531/1700 train_time:149320ms step_avg:97.53ms
step:1532/1700 train_time:149422ms step_avg:97.53ms
step:1533/1700 train_time:149524ms step_avg:97.54ms
step:1534/1700 train_time:149625ms step_avg:97.54ms
step:1535/1700 train_time:149729ms step_avg:97.54ms
step:1536/1700 train_time:149830ms step_avg:97.55ms
step:1537/1700 train_time:149931ms step_avg:97.55ms
step:1538/1700 train_time:150033ms step_avg:97.55ms
step:1539/1700 train_time:150135ms step_avg:97.55ms
step:1540/1700 train_time:150236ms step_avg:97.56ms
step:1541/1700 train_time:150339ms step_avg:97.56ms
step:1542/1700 train_time:150441ms step_avg:97.56ms
step:1543/1700 train_time:150542ms step_avg:97.56ms
step:1544/1700 train_time:150644ms step_avg:97.57ms
step:1545/1700 train_time:150746ms step_avg:97.57ms
step:1546/1700 train_time:150846ms step_avg:97.57ms
step:1547/1700 train_time:150950ms step_avg:97.58ms
step:1548/1700 train_time:151052ms step_avg:97.58ms
step:1549/1700 train_time:151154ms step_avg:97.58ms
step:1550/1700 train_time:151255ms step_avg:97.58ms
step:1551/1700 train_time:151357ms step_avg:97.59ms
step:1552/1700 train_time:151458ms step_avg:97.59ms
step:1553/1700 train_time:151560ms step_avg:97.59ms
step:1554/1700 train_time:151662ms step_avg:97.59ms
step:1555/1700 train_time:151763ms step_avg:97.60ms
step:1556/1700 train_time:151864ms step_avg:97.60ms
step:1557/1700 train_time:151968ms step_avg:97.60ms
step:1558/1700 train_time:152071ms step_avg:97.61ms
step:1559/1700 train_time:152173ms step_avg:97.61ms
step:1560/1700 train_time:152274ms step_avg:97.61ms
step:1561/1700 train_time:152375ms step_avg:97.61ms
step:1562/1700 train_time:152477ms step_avg:97.62ms
step:1563/1700 train_time:152580ms step_avg:97.62ms
step:1564/1700 train_time:152680ms step_avg:97.62ms
step:1565/1700 train_time:152780ms step_avg:97.62ms
step:1566/1700 train_time:152881ms step_avg:97.63ms
step:1567/1700 train_time:152984ms step_avg:97.63ms
step:1568/1700 train_time:153085ms step_avg:97.63ms
step:1569/1700 train_time:153188ms step_avg:97.63ms
step:1570/1700 train_time:153289ms step_avg:97.64ms
step:1571/1700 train_time:153390ms step_avg:97.64ms
step:1572/1700 train_time:153491ms step_avg:97.64ms
step:1573/1700 train_time:153593ms step_avg:97.64ms
step:1574/1700 train_time:153695ms step_avg:97.65ms
step:1575/1700 train_time:153796ms step_avg:97.65ms
step:1576/1700 train_time:153898ms step_avg:97.65ms
step:1577/1700 train_time:154000ms step_avg:97.65ms
step:1578/1700 train_time:154102ms step_avg:97.66ms
step:1579/1700 train_time:154203ms step_avg:97.66ms
step:1580/1700 train_time:154306ms step_avg:97.66ms
step:1581/1700 train_time:154408ms step_avg:97.66ms
step:1582/1700 train_time:154509ms step_avg:97.67ms
step:1583/1700 train_time:154612ms step_avg:97.67ms
step:1584/1700 train_time:154715ms step_avg:97.67ms
step:1585/1700 train_time:154815ms step_avg:97.68ms
step:1586/1700 train_time:154917ms step_avg:97.68ms
step:1587/1700 train_time:155018ms step_avg:97.68ms
step:1588/1700 train_time:155119ms step_avg:97.68ms
step:1589/1700 train_time:155221ms step_avg:97.68ms
step:1590/1700 train_time:155324ms step_avg:97.69ms
step:1591/1700 train_time:155427ms step_avg:97.69ms
step:1592/1700 train_time:155529ms step_avg:97.69ms
step:1593/1700 train_time:155630ms step_avg:97.70ms
step:1594/1700 train_time:155736ms step_avg:97.70ms
step:1595/1700 train_time:155837ms step_avg:97.70ms
step:1596/1700 train_time:155937ms step_avg:97.70ms
step:1597/1700 train_time:156039ms step_avg:97.71ms
step:1598/1700 train_time:156140ms step_avg:97.71ms
step:1599/1700 train_time:156241ms step_avg:97.71ms
step:1600/1700 train_time:156343ms step_avg:97.71ms
step:1601/1700 train_time:156445ms step_avg:97.72ms
step:1602/1700 train_time:156547ms step_avg:97.72ms
step:1603/1700 train_time:156650ms step_avg:97.72ms
step:1604/1700 train_time:156751ms step_avg:97.72ms
step:1605/1700 train_time:156853ms step_avg:97.73ms
step:1606/1700 train_time:156954ms step_avg:97.73ms
step:1607/1700 train_time:157055ms step_avg:97.73ms
step:1608/1700 train_time:157157ms step_avg:97.73ms
step:1609/1700 train_time:157258ms step_avg:97.74ms
step:1610/1700 train_time:157360ms step_avg:97.74ms
step:1611/1700 train_time:157462ms step_avg:97.74ms
step:1612/1700 train_time:157566ms step_avg:97.75ms
step:1613/1700 train_time:157668ms step_avg:97.75ms
step:1614/1700 train_time:157769ms step_avg:97.75ms
step:1615/1700 train_time:157870ms step_avg:97.75ms
step:1616/1700 train_time:157971ms step_avg:97.75ms
step:1617/1700 train_time:158072ms step_avg:97.76ms
step:1618/1700 train_time:158173ms step_avg:97.76ms
step:1619/1700 train_time:158275ms step_avg:97.76ms
step:1620/1700 train_time:158377ms step_avg:97.76ms
step:1621/1700 train_time:158478ms step_avg:97.77ms
step:1622/1700 train_time:158581ms step_avg:97.77ms
step:1623/1700 train_time:158683ms step_avg:97.77ms
step:1624/1700 train_time:158786ms step_avg:97.77ms
step:1625/1700 train_time:158889ms step_avg:97.78ms
step:1625/1700 val_loss:3.2924 train_time:158971ms step_avg:97.83ms
step:1626/1700 train_time:158995ms step_avg:97.78ms
step:1627/1700 train_time:159094ms step_avg:97.78ms
step:1628/1700 train_time:159197ms step_avg:97.79ms
step:1629/1700 train_time:159297ms step_avg:97.79ms
step:1630/1700 train_time:159397ms step_avg:97.79ms
step:1631/1700 train_time:159498ms step_avg:97.79ms
step:1632/1700 train_time:159599ms step_avg:97.79ms
step:1633/1700 train_time:159699ms step_avg:97.79ms
step:1634/1700 train_time:159802ms step_avg:97.80ms
step:1635/1700 train_time:159902ms step_avg:97.80ms
step:1636/1700 train_time:160005ms step_avg:97.80ms
step:1637/1700 train_time:160110ms step_avg:97.81ms
step:1638/1700 train_time:160210ms step_avg:97.81ms
step:1639/1700 train_time:160311ms step_avg:97.81ms
step:1640/1700 train_time:160412ms step_avg:97.81ms
step:1641/1700 train_time:160514ms step_avg:97.81ms
step:1642/1700 train_time:160615ms step_avg:97.82ms
step:1643/1700 train_time:160717ms step_avg:97.82ms
step:1644/1700 train_time:160819ms step_avg:97.82ms
step:1645/1700 train_time:160922ms step_avg:97.83ms
step:1646/1700 train_time:161024ms step_avg:97.83ms
step:1647/1700 train_time:161130ms step_avg:97.83ms
step:1648/1700 train_time:161233ms step_avg:97.84ms
step:1649/1700 train_time:161335ms step_avg:97.84ms
step:1650/1700 train_time:161436ms step_avg:97.84ms
step:1651/1700 train_time:161537ms step_avg:97.84ms
step:1652/1700 train_time:161638ms step_avg:97.84ms
step:1653/1700 train_time:161740ms step_avg:97.85ms
step:1654/1700 train_time:161841ms step_avg:97.85ms
step:1655/1700 train_time:161945ms step_avg:97.85ms
step:1656/1700 train_time:162048ms step_avg:97.86ms
step:1657/1700 train_time:162150ms step_avg:97.86ms
step:1658/1700 train_time:162254ms step_avg:97.86ms
step:1659/1700 train_time:162359ms step_avg:97.87ms
step:1660/1700 train_time:162461ms step_avg:97.87ms
step:1661/1700 train_time:162566ms step_avg:97.87ms
step:1662/1700 train_time:162668ms step_avg:97.88ms
step:1663/1700 train_time:162771ms step_avg:97.88ms
step:1664/1700 train_time:162874ms step_avg:97.88ms
step:1665/1700 train_time:162980ms step_avg:97.89ms
step:1666/1700 train_time:163082ms step_avg:97.89ms
step:1667/1700 train_time:163183ms step_avg:97.89ms
step:1668/1700 train_time:163287ms step_avg:97.89ms
step:1669/1700 train_time:163392ms step_avg:97.90ms
step:1670/1700 train_time:163494ms step_avg:97.90ms
step:1671/1700 train_time:163596ms step_avg:97.90ms
step:1672/1700 train_time:163698ms step_avg:97.91ms
step:1673/1700 train_time:163800ms step_avg:97.91ms
step:1674/1700 train_time:163901ms step_avg:97.91ms
step:1675/1700 train_time:164003ms step_avg:97.91ms
step:1676/1700 train_time:164106ms step_avg:97.92ms
step:1677/1700 train_time:164208ms step_avg:97.92ms
step:1678/1700 train_time:164311ms step_avg:97.92ms
step:1679/1700 train_time:164413ms step_avg:97.92ms
step:1680/1700 train_time:164515ms step_avg:97.93ms
step:1681/1700 train_time:164619ms step_avg:97.93ms
step:1682/1700 train_time:164722ms step_avg:97.93ms
step:1683/1700 train_time:164823ms step_avg:97.93ms
step:1684/1700 train_time:164928ms step_avg:97.94ms
step:1685/1700 train_time:165031ms step_avg:97.94ms
step:1686/1700 train_time:165132ms step_avg:97.94ms
step:1687/1700 train_time:165234ms step_avg:97.95ms
step:1688/1700 train_time:165336ms step_avg:97.95ms
step:1689/1700 train_time:165438ms step_avg:97.95ms
step:1690/1700 train_time:165539ms step_avg:97.95ms
step:1691/1700 train_time:165641ms step_avg:97.95ms
step:1692/1700 train_time:165744ms step_avg:97.96ms
step:1693/1700 train_time:165847ms step_avg:97.96ms
step:1694/1700 train_time:165949ms step_avg:97.96ms
step:1695/1700 train_time:166052ms step_avg:97.97ms
step:1696/1700 train_time:166155ms step_avg:97.97ms
step:1697/1700 train_time:166262ms step_avg:97.97ms
step:1698/1700 train_time:166363ms step_avg:97.98ms
step:1699/1700 train_time:166464ms step_avg:97.98ms
step:1700/1700 train_time:166566ms step_avg:97.98ms
step:1700/1700 val_loss:3.2784 train_time:166649ms step_avg:98.03ms
peak memory allocated: 33278 MiB reserved: 48132 MiB

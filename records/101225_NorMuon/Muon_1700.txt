import os
import sys
with open(sys.argv[0]) as f:
    code = f.read() # read the code of this file ASAP, for logging
import uuid
import time
import copy
import glob
from dataclasses import dataclass
from functools import lru_cache, partial # Added partial for hook registration
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch
torch.empty(1, device="cuda", requires_grad=True).backward() # prevents a bug on some systems
from torch import Tensor, nn
import torch.nn.functional as F
import torch.distributed as dist
# use of FlexAttention contributed by @KoszarskyB
from torch.nn.attention.flex_attention import BlockMask, flex_attention
#torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng

@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Muon optimizer

@torch.compile
def zeropower_via_newtonschulz5(G: Tensor, steps: int) -> Tensor:
    """
    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a
    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose
    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at
    zero even beyond the point where the iteration no longer converges all the way to one everywhere
    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T
    where S' is diagonal with S_{ii}' ~ Uniform(0.5, 1.5), which turns out not to hurt model
    performance at all relative to UV^T, where USV^T = G is the SVD.
    """
    assert G.ndim >= 2 # batched Muon implementation by @scottjmaddox, and put into practice in the record by @YouJiacheng
    a, b, c = (3.4445, -4.7750,  2.0315)
    X = G
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) + 1e-7)
    # Perform the NS iterations
    for _ in range(steps):
        A = X @ X.mT
        B = b * A + c * A @ A # quintic computation strategy adapted from suggestion by @jxbz, @leloykun, and @YouJiacheng
        X = a * X + B @ X

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, beta2=0.95):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_reduce_futures: list[torch.Future] = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            grad = torch.empty_like(params[-1])
            grad_pad = [param.grad for param in params] + [torch.zeros_like(params[-1])] * world_size
            for base_i in range(0, len(params), world_size):
                if base_i + rank < len(params):
                    grad = params[base_i + rank].grad
                else:
                    grad = torch.zeros_like(grad)
                # This gives strange dynamo warnings
                reduce_scatter_futures.append(dist.reduce_scatter(grad, grad_pad[base_i:base_i + world_size], op=dist.ReduceOp.AVG, async_op=True).get_future())

        idx = 0
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            params_pad = params + [torch.empty_like(params[-1])] * world_size
            momentum = group["momentum"]
            for base_i in range(0, len(params), world_size):
                reduce_scatter_futures[idx].wait()
                if base_i + rank < len(params):
                    p = params[base_i + rank]
                    grad = p.grad
                    eff_lr = group["lr"] * max(1, p.size(-2) / p.size(-1)) ** 0.5 * getattr(p, "lr_mul", 1.0)
                    eff_weight_decay = group["lr"] * group["weight_decay"] * getattr(p, "wd_mul", 1.0)
                    state = self.state[p]
                    if len(state) == 0:
                        state["momentum_buffer"] = torch.zeros_like(grad)
                    momentum_buffer = state["momentum_buffer"]
                    p.mul_(1 - eff_weight_decay)
                    momentum_buffer.lerp_(grad, 1 - momentum)
                    grad = grad.lerp_(momentum_buffer, momentum)
                    v = zeropower_via_newtonschulz5(grad.bfloat16(), 5)
                    p.add_(other=v, alpha=-eff_lr)
                idx += 1
                all_reduce_futures.append(dist.all_gather(params_pad[base_i:base_i + world_size], params_pad[base_i + rank], async_op=True).get_future())
        torch.futures.collect_all(all_reduce_futures).wait()


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_reduce_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            grad = torch.empty_like(params[-1])
            for base_i in range(len(params)):
                grad = params[base_i].grad
                rank_size = grad.shape[0] // world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for base in range(len(params)):
                reduce_scatter_futures[idx].wait()
                p = params[base]
                rank_size = p.shape[0] // world_size
                p_slice = p[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(p, "lr_mul", 1.0)
                state = self.state[p]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state['step'] = torch.tensor(0, dtype=torch.int64, device=p.device)
                    state['exp_avg'] = torch.zeros_like(p_slice)
                    state['exp_avg_sq'] = torch.zeros_like(p_slice)
                exp_avg = state['exp_avg']
                exp_avg_sq = state['exp_avg_sq']
                state['step'] += 1
                t = state['step']
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(p, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_reduce_futures.append(dist.all_gather_into_tensor(p, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_reduce_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

class Rotary(nn.Module):
    def __init__(self, dim: int, max_seq_len: int):
        super().__init__()
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=dim//4, dtype=torch.float32)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(dim//4)])
        t = torch.arange(max_seq_len, dtype=torch.float32)
        theta = torch.einsum("i,j -> ij", t, angular_freq)
        self.cos = nn.Buffer(theta.cos(), persistent=False)
        self.sin = nn.Buffer(theta.sin(), persistent=False)

    def forward(self, x_BTHD: Tensor):
        assert self.cos.size(0) >= x_BTHD.size(-3)
        cos, sin = self.cos[None, :x_BTHD.size(-3), None, :], self.sin[None, :x_BTHD.size(-3), None, :]
        x1, x2 = x_BTHD.to(dtype=torch.float32).chunk(2, dim=-1)
        y1 = x1 * cos + x2 * sin
        y2 = x1 * (-sin) + x2 * cos
        return torch.cat((y1, y2), 3).type_as(x_BTHD)

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, num_heads: int, max_seq_len: int, head_dim=128):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        hdim = num_heads * head_dim
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        self.qkv_w = nn.Parameter(torch.empty(3, hdim, dim).uniform_(-bound, bound))
        self.rotary = Rotary(head_dim, max_seq_len)
        self.c_proj = CastedLinear(hdim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977
        # scale the attention logits by given constant, instead of the default head_dim**-0.5, by @leloykun
        # inspired by learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.12

    def forward(self, x: Tensor, ve: Tensor | None, lambdas: Tensor, block_mask: BlockMask):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "Must use batch size = 1 for FlexAttention"
        q, k, v = F.linear(x, self.qkv_w.flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = self.rotary(q), self.rotary(k)
        if ve is not None:
            v = lambdas[0] * v + lambdas[1] * ve.view_as(v) # @KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = lambdas[0] * v
        y = flex_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), block_mask=block_mask, scale=self.attn_scale).transpose(1, 2)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = self.c_proj(y)
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        self.c_fc = CastedLinear(dim, hdim)
        self.c_proj = CastedLinear(hdim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = self.c_fc(x)
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = self.c_proj(x)
        return x

class Block(nn.Module):
    def __init__(self, dim: int, num_heads: int, max_seq_len: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, num_heads, max_seq_len) if layer_idx != 7 else None
        self.mlp = MLP(dim)

    def forward(self, x: Tensor, ve: Tensor | None, x0: Tensor, lambdas: Tensor, sa_lambdas: Tensor, block_mask: BlockMask):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), ve, sa_lambdas, block_mask)
        x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, num_heads, max_seq_len, i) for i in range(num_layers)])
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=True, x_s=(model_dim**0.5)/448, w_s=24/448, grad_s=1/448)
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5) % dist.get_world_size()
        self.scalars = nn.Parameter(torch.cat([
            torch.ones(num_layers), # skip_weights
            *[torch.tensor([1.0, 0.0]) for _ in range(num_layers)], # block lambdas
            *[torch.tensor([0.5, 0.5]) for _ in range(num_layers)], # SA lambdas
            torch.ones(pad),
        ]))
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 27.5
        self.scalars.lr_mul = 5.0

    def create_blockmasks(self, input_seq: Tensor, sliding_window_num_blocks: Tensor):
        BLOCK_SIZE = 128
        docs = (input_seq == 50256).cumsum(0)

        def document_causal(b, h, q_idx, kv_idx):
            causal_mask = q_idx >= kv_idx
            document_mask = docs[q_idx] == docs[kv_idx]
            return causal_mask & document_mask

        def dense_to_ordered(dense_blockmask: Tensor):
            num_blocks = dense_blockmask.sum(dim=-1, dtype=torch.int32)
            indices = dense_blockmask.argsort(dim=-1, descending=False, stable=True).flip(-1).to(torch.int32)
            return num_blocks[None, None].contiguous(), indices[None, None].contiguous()

        # manual block mask creation by @YouJiacheng
        assert len(input_seq) % BLOCK_SIZE == 0
        NUM_BLOCKS = len(input_seq) // BLOCK_SIZE
        block_idx = torch.arange(NUM_BLOCKS, dtype=torch.int32, device="cuda")
        causal_blockmask_any = block_idx[:, None] >= block_idx
        causal_blockmask_all = block_idx[:, None] > block_idx
        docs_low = docs.view(-1, BLOCK_SIZE)[:, 0].contiguous()
        docs_high = docs.view(-1, BLOCK_SIZE)[:, -1].contiguous()
        document_blockmask_any = (docs_low[:, None] <= docs_high) & (docs_high[:, None] >= docs_low)
        document_blockmask_all = (docs_low[:, None] == docs_high) & (docs_high[:, None] == docs_low)
        blockmask_any = causal_blockmask_any & document_blockmask_any
        blockmask_all = causal_blockmask_all & document_blockmask_all
        partial_kv_num_blocks, partial_kv_indices = dense_to_ordered(blockmask_any & ~blockmask_all)
        full_kv_num_blocks, full_kv_indices = dense_to_ordered(blockmask_all)
        def build_bm(window_size_blocks: Tensor) -> BlockMask:
            return BlockMask.from_kv_blocks(
                torch.clamp_max(partial_kv_num_blocks, torch.clamp_min(window_size_blocks - full_kv_num_blocks, 1)),
                partial_kv_indices,
                torch.clamp_max(full_kv_num_blocks, window_size_blocks - 1),
                full_kv_indices,
                BLOCK_SIZE=BLOCK_SIZE,
                mask_mod=document_causal,
            )
        # Long-short SWA block masks by @leloykun & @YouJiacheng, adapated from suggestion by @Grad62304977, following Gemma 2 paper
        return build_bm(sliding_window_num_blocks), build_bm(sliding_window_num_blocks // 2)

    def forward(self, input_seq: Tensor, target_seq: Tensor, sliding_window_num_blocks: Tensor):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        ve = [ve[0], ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        long_bm, short_bm = self.create_blockmasks(input_seq, sliding_window_num_blocks)
        block_masks = [long_bm, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, long_bm, short_bm, short_bm, short_bm, long_bm]
        assert len(block_masks) == len(self.blocks)

        x = x0 = norm(self.embed(input_seq)[None]) # use of norm here by @Grad62304977

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)

        n = len(self.blocks) // 2

        for i in range(len(self.blocks)):
            if i >= n:
                x = x + skip_weights[i - n] * skip_connections.pop()
            x = self.blocks[i](x, ve[i], x0, lambdas[i], sa_lambdas[i], block_masks[i])
            if i < n:
                skip_connections.append(x)

        x = norm(x)
        logits = self.lm_head(x).float()
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / (7.5 * x.size(-1)**0.5))
        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), target_seq, reduction="sum" if self.training else "mean")
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

# find world_size starting indicies, such that each begins with token 50256 and local_batches don't overlap
def find_batch_starts(tokens: Tensor, pos: int, local_batch_size: int, max_batch_span: int):
    boundary_mask = tokens[pos : pos + max_batch_span] == 50256
    boundary_positions = torch.nonzero(boundary_mask, as_tuple=False).squeeze(-1) + pos
    start = boundary_positions[0].item()
    starts = []
    for i in range(1, len(boundary_positions)):
        end = boundary_positions[i].item() 
        if end - start >= local_batch_size:
            starts.append(start) # append start once end pos is confirmed
            if len(starts) == dist.get_world_size():
                return starts, end - pos
            start = end
    assert False # increase max_batch_span if necessary

def distributed_data_generator(filename_pattern: str, batch_size: int, align_to_bos: bool):
    rank = dist.get_rank()
    world_size = dist.get_world_size()
    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    assert batch_size % world_size == 0
    local_batch_size = batch_size // world_size
    file_iter = iter(files) # use itertools.cycle(files) instead if you want to do multi-epoch training
    tokens, pos = _load_data_shard(next(file_iter)), 0
    max_batch_span = 2 * batch_size if align_to_bos else batch_size # provide buffer to handle samples up to length local_batch_size
    while True:
        if pos + max_batch_span + 1 >= len(tokens):
            tokens, pos = _load_data_shard(next(file_iter)), 0
        if align_to_bos:
            batch_starts, batch_span = find_batch_starts(tokens, pos, local_batch_size, max_batch_span)
            start_idx = batch_starts[rank]
        else:
            batch_span = batch_size
            start_idx = pos + rank * local_batch_size
        buf = tokens[start_idx:][:local_batch_size + 1]
        inputs = buf[:-1].to(device="cuda", dtype=torch.int32, non_blocking=True) # no sync on host side;
        targets = buf[1:].to(device="cuda", dtype=torch.int64, non_blocking=True) # H2D in another stream isn't helpful.
        pos += batch_span
        yield inputs, targets

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_seq_len = 48*1024 # FlexAttention sequence length
    val_seq_len = 4*64*1024 # FlexAttention sequence length for validation
    # optimization
    num_iterations = 1700 # number of iterations to run
    cooldown_frac = 0.45 # fraction of training spent cooling down the learning rate
    # evaluation and logging
    val_loss_every = 125 # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint = False
args = Hyperparameters()


# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert world_size == 8 # this code is designed for 8xH100
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = uuid.uuid4()
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(vocab_size=50257, num_layers=12, num_heads=6, model_dim=768, max_seq_len=max(args.train_seq_len, args.val_seq_len)).cuda()
for m in model.modules():
    if isinstance(m, nn.Embedding):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(scalar_params + head_params + embed_params, lr=0.008, betas=(0.8, 0.95), eps=1e-10, weight_decay=0.0)
optimizer2 = Muon(hidden_matrix_params, lr=0.05, momentum=0.95, weight_decay=0.0, beta2=0.95)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: stable then decay
def get_lr(step: int):
    x = step / args.num_iterations # progress in training
    assert 0 <= x < 1
    if x < 1 - args.cooldown_frac:
        return 1.0
    else:
        w = (1 - x) / args.cooldown_frac
        return w * 1.0 + (1 - w) * 0.1

# attention window size schedule: linearly increase
@lru_cache(1)
def get_window_size_blocks_helper(window_size: int):
    return torch.tensor(window_size // 128, dtype=torch.int32, pin_memory=True).cuda(non_blocking=True)
def get_window_size_blocks(step: int):
    x = step / args.num_iterations # progress in training
    assert 0 <= x <= 1
    # Linearly increase the block-wise sliding window size over training 128 -> 1792
    # increase by @fernbear.bsky.social; block-wise by @YouJiacheng
    window_size = next_multiple_of_n(1728 * x, n=128)
    return get_window_size_blocks_helper(window_size)

model: nn.Module = torch.compile(model, dynamic=False)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 10
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
print([opt.state_dict().keys() for opt in optimizers])
train_loader = distributed_data_generator(args.train_files, world_size * args.train_seq_len, align_to_bos=True)
for _ in range(warmup_steps):
    inputs, targets = next(train_loader)
    model(inputs, targets, get_window_size_blocks(1)).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, world_size * args.train_seq_len, align_to_bos=True)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
for step in range(train_steps + 1):
    last_step = (step == train_steps)

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        val_batch_size = world_size * args.val_seq_len
        assert args.val_tokens % val_batch_size == 0
        val_steps = args.val_tokens // val_batch_size
        val_loader = distributed_data_generator(args.val_files, val_batch_size, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets = next(val_loader)
                val_loss += model(inputs, targets, get_window_size_blocks(step))
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    inputs, targets = next(train_loader)
    model(inputs, targets, get_window_size_blocks(step)).backward()
    # set optimization hyperparameters
    for opt in optimizers:
        for group in opt.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)
    for group in optimizer2.param_groups:
        frac = min(step / 300, 1) # momentum warmup for muon
        group["momentum"] = (1 - frac) * 0.85 + frac * 0.95
    # step the optimizers
    for opt in optimizers:
        opt.step()
    # null the gradients
    model.zero_grad(set_to_none=True)
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()
====================================================================================================
Running Python 3.12.7 (main, Oct 11 2025, 17:06:43) [GCC 13.2.0]
Running PyTorch 2.10.0.dev20251011+cu126 compiled for CUDA 12.6
Mon Oct 13 01:00:06 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 565.57.01              Driver Version: 565.57.01      CUDA Version: 12.7     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000001:00:00.0 Off |                    0 |
| N/A   28C    P0            111W /  700W |    5858MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000002:00:00.0 Off |                    0 |
| N/A   29C    P0            113W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000003:00:00.0 Off |                    0 |
| N/A   28C    P0            115W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000008:00:00.0 Off |                    0 |
| N/A   27C    P0            113W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000009:00:00.0 Off |                    0 |
| N/A   26C    P0            111W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   0000000A:00:00.0 Off |                    0 |
| N/A   29C    P0            118W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   0000000B:00:00.0 Off |                    0 |
| N/A   26C    P0            117W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   0000000C:00:00.0 Off |                    0 |
| N/A   27C    P0            113W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
+-----------------------------------------------------------------------------------------+

====================================================================================================
step:0/1700 val_loss:10.8258 train_time:0ms step_avg:0.02ms
step:1/1700 train_time:142ms step_avg:142.45ms
step:2/1700 train_time:163ms step_avg:81.70ms
step:3/1700 train_time:237ms step_avg:78.99ms
step:4/1700 train_time:328ms step_avg:82.06ms
step:5/1700 train_time:420ms step_avg:84.00ms
step:6/1700 train_time:512ms step_avg:85.41ms
step:7/1700 train_time:604ms step_avg:86.32ms
step:8/1700 train_time:696ms step_avg:87.01ms
step:9/1700 train_time:788ms step_avg:87.57ms
step:10/1700 train_time:880ms step_avg:87.97ms
step:11/1700 train_time:972ms step_avg:88.33ms
step:12/1700 train_time:1065ms step_avg:88.73ms
step:13/1700 train_time:1160ms step_avg:89.20ms
step:14/1700 train_time:1253ms step_avg:89.53ms
step:15/1700 train_time:1346ms step_avg:89.72ms
step:16/1700 train_time:1439ms step_avg:89.93ms
step:17/1700 train_time:1531ms step_avg:90.08ms
step:18/1700 train_time:1623ms step_avg:90.19ms
step:19/1700 train_time:1716ms step_avg:90.30ms
step:20/1700 train_time:1808ms step_avg:90.38ms
step:21/1700 train_time:1900ms step_avg:90.50ms
step:22/1700 train_time:1993ms step_avg:90.59ms
step:23/1700 train_time:2085ms step_avg:90.66ms
step:24/1700 train_time:2179ms step_avg:90.79ms
step:25/1700 train_time:2272ms step_avg:90.89ms
step:26/1700 train_time:2365ms step_avg:90.95ms
step:27/1700 train_time:2458ms step_avg:91.03ms
step:28/1700 train_time:2552ms step_avg:91.13ms
step:29/1700 train_time:2644ms step_avg:91.16ms
step:30/1700 train_time:2736ms step_avg:91.21ms
step:31/1700 train_time:2829ms step_avg:91.24ms
step:32/1700 train_time:2921ms step_avg:91.29ms
step:33/1700 train_time:3014ms step_avg:91.34ms
step:34/1700 train_time:3107ms step_avg:91.39ms
step:35/1700 train_time:3201ms step_avg:91.46ms
step:36/1700 train_time:3294ms step_avg:91.50ms
step:37/1700 train_time:3387ms step_avg:91.54ms
step:38/1700 train_time:3480ms step_avg:91.58ms
step:39/1700 train_time:3573ms step_avg:91.60ms
step:40/1700 train_time:3665ms step_avg:91.62ms
step:41/1700 train_time:3758ms step_avg:91.65ms
step:42/1700 train_time:3850ms step_avg:91.68ms
step:43/1700 train_time:3943ms step_avg:91.69ms
step:44/1700 train_time:4036ms step_avg:91.73ms
step:45/1700 train_time:4129ms step_avg:91.76ms
step:46/1700 train_time:4222ms step_avg:91.77ms
step:47/1700 train_time:4315ms step_avg:91.81ms
step:48/1700 train_time:4407ms step_avg:91.82ms
step:49/1700 train_time:4501ms step_avg:91.86ms
step:50/1700 train_time:4594ms step_avg:91.87ms
step:51/1700 train_time:4686ms step_avg:91.88ms
step:52/1700 train_time:4778ms step_avg:91.89ms
step:53/1700 train_time:4871ms step_avg:91.91ms
step:54/1700 train_time:4964ms step_avg:91.92ms
step:55/1700 train_time:5057ms step_avg:91.94ms
step:56/1700 train_time:5149ms step_avg:91.95ms
step:57/1700 train_time:5242ms step_avg:91.96ms
step:58/1700 train_time:5335ms step_avg:91.98ms
step:59/1700 train_time:5427ms step_avg:91.98ms
step:60/1700 train_time:5519ms step_avg:91.99ms
step:61/1700 train_time:5613ms step_avg:92.01ms
step:62/1700 train_time:5705ms step_avg:92.01ms
step:63/1700 train_time:5798ms step_avg:92.03ms
step:64/1700 train_time:5891ms step_avg:92.04ms
step:65/1700 train_time:5983ms step_avg:92.05ms
step:66/1700 train_time:6076ms step_avg:92.06ms
step:67/1700 train_time:6168ms step_avg:92.07ms
step:68/1700 train_time:6262ms step_avg:92.08ms
step:69/1700 train_time:6356ms step_avg:92.11ms
step:70/1700 train_time:6449ms step_avg:92.13ms
step:71/1700 train_time:6541ms step_avg:92.13ms
step:72/1700 train_time:6634ms step_avg:92.14ms
step:73/1700 train_time:6726ms step_avg:92.14ms
step:74/1700 train_time:6819ms step_avg:92.15ms
step:75/1700 train_time:6913ms step_avg:92.17ms
step:76/1700 train_time:7005ms step_avg:92.17ms
step:77/1700 train_time:7097ms step_avg:92.18ms
step:78/1700 train_time:7190ms step_avg:92.18ms
step:79/1700 train_time:7283ms step_avg:92.19ms
step:80/1700 train_time:7376ms step_avg:92.21ms
step:81/1700 train_time:7469ms step_avg:92.21ms
step:82/1700 train_time:7561ms step_avg:92.21ms
step:83/1700 train_time:7654ms step_avg:92.22ms
step:84/1700 train_time:7746ms step_avg:92.22ms
step:85/1700 train_time:7839ms step_avg:92.22ms
step:86/1700 train_time:7932ms step_avg:92.23ms
step:87/1700 train_time:8024ms step_avg:92.23ms
step:88/1700 train_time:8118ms step_avg:92.24ms
step:89/1700 train_time:8210ms step_avg:92.25ms
step:90/1700 train_time:8303ms step_avg:92.25ms
step:91/1700 train_time:8396ms step_avg:92.26ms
step:92/1700 train_time:8488ms step_avg:92.26ms
step:93/1700 train_time:8581ms step_avg:92.27ms
step:94/1700 train_time:8674ms step_avg:92.27ms
step:95/1700 train_time:8766ms step_avg:92.28ms
step:96/1700 train_time:8859ms step_avg:92.28ms
step:97/1700 train_time:8953ms step_avg:92.29ms
step:98/1700 train_time:9045ms step_avg:92.29ms
step:99/1700 train_time:9138ms step_avg:92.30ms
step:100/1700 train_time:9231ms step_avg:92.31ms
step:101/1700 train_time:9324ms step_avg:92.31ms
step:102/1700 train_time:9417ms step_avg:92.32ms
step:103/1700 train_time:9510ms step_avg:92.33ms
step:104/1700 train_time:9602ms step_avg:92.33ms
step:105/1700 train_time:9695ms step_avg:92.33ms
step:106/1700 train_time:9787ms step_avg:92.33ms
step:107/1700 train_time:9880ms step_avg:92.34ms
step:108/1700 train_time:9973ms step_avg:92.34ms
step:109/1700 train_time:10066ms step_avg:92.35ms
step:110/1700 train_time:10159ms step_avg:92.35ms
step:111/1700 train_time:10253ms step_avg:92.37ms
step:112/1700 train_time:10345ms step_avg:92.37ms
step:113/1700 train_time:10438ms step_avg:92.37ms
step:114/1700 train_time:10531ms step_avg:92.38ms
step:115/1700 train_time:10624ms step_avg:92.38ms
step:116/1700 train_time:10717ms step_avg:92.39ms
step:117/1700 train_time:10809ms step_avg:92.39ms
step:118/1700 train_time:10902ms step_avg:92.39ms
step:119/1700 train_time:10995ms step_avg:92.39ms
step:120/1700 train_time:11087ms step_avg:92.39ms
step:121/1700 train_time:11181ms step_avg:92.40ms
step:122/1700 train_time:11273ms step_avg:92.41ms
step:123/1700 train_time:11365ms step_avg:92.40ms
step:124/1700 train_time:11458ms step_avg:92.40ms
step:125/1700 train_time:11550ms step_avg:92.40ms
step:125/1700 val_loss:4.6426 train_time:11639ms step_avg:93.11ms
step:126/1700 train_time:11664ms step_avg:92.57ms
step:127/1700 train_time:11743ms step_avg:92.46ms
step:128/1700 train_time:11845ms step_avg:92.54ms
step:129/1700 train_time:11941ms step_avg:92.56ms
step:130/1700 train_time:12033ms step_avg:92.56ms
step:131/1700 train_time:12126ms step_avg:92.56ms
step:132/1700 train_time:12218ms step_avg:92.56ms
step:133/1700 train_time:12311ms step_avg:92.56ms
step:134/1700 train_time:12404ms step_avg:92.56ms
step:135/1700 train_time:12496ms step_avg:92.56ms
step:136/1700 train_time:12588ms step_avg:92.56ms
step:137/1700 train_time:12682ms step_avg:92.57ms
step:138/1700 train_time:12777ms step_avg:92.59ms
step:139/1700 train_time:12871ms step_avg:92.60ms
step:140/1700 train_time:12965ms step_avg:92.61ms
step:141/1700 train_time:13059ms step_avg:92.62ms
step:142/1700 train_time:13151ms step_avg:92.61ms
step:143/1700 train_time:13244ms step_avg:92.62ms
step:144/1700 train_time:13337ms step_avg:92.61ms
step:145/1700 train_time:13429ms step_avg:92.61ms
step:146/1700 train_time:13522ms step_avg:92.62ms
step:147/1700 train_time:13615ms step_avg:92.62ms
step:148/1700 train_time:13708ms step_avg:92.62ms
step:149/1700 train_time:13803ms step_avg:92.64ms
step:150/1700 train_time:13897ms step_avg:92.65ms
step:151/1700 train_time:13990ms step_avg:92.65ms
step:152/1700 train_time:14084ms step_avg:92.66ms
step:153/1700 train_time:14177ms step_avg:92.66ms
step:154/1700 train_time:14269ms step_avg:92.66ms
step:155/1700 train_time:14362ms step_avg:92.66ms
step:156/1700 train_time:14455ms step_avg:92.66ms
step:157/1700 train_time:14548ms step_avg:92.66ms
step:158/1700 train_time:14642ms step_avg:92.67ms
step:159/1700 train_time:14735ms step_avg:92.67ms
step:160/1700 train_time:14829ms step_avg:92.68ms
step:161/1700 train_time:14923ms step_avg:92.69ms
step:162/1700 train_time:15016ms step_avg:92.69ms
step:163/1700 train_time:15109ms step_avg:92.70ms
step:164/1700 train_time:15203ms step_avg:92.70ms
step:165/1700 train_time:15296ms step_avg:92.70ms
step:166/1700 train_time:15389ms step_avg:92.70ms
step:167/1700 train_time:15482ms step_avg:92.71ms
step:168/1700 train_time:15575ms step_avg:92.71ms
step:169/1700 train_time:15669ms step_avg:92.71ms
step:170/1700 train_time:15762ms step_avg:92.72ms
step:171/1700 train_time:15855ms step_avg:92.72ms
step:172/1700 train_time:15949ms step_avg:92.72ms
step:173/1700 train_time:16042ms step_avg:92.73ms
step:174/1700 train_time:16135ms step_avg:92.73ms
step:175/1700 train_time:16229ms step_avg:92.74ms
step:176/1700 train_time:16323ms step_avg:92.74ms
step:177/1700 train_time:16415ms step_avg:92.74ms
step:178/1700 train_time:16508ms step_avg:92.74ms
step:179/1700 train_time:16602ms step_avg:92.75ms
step:180/1700 train_time:16694ms step_avg:92.75ms
step:181/1700 train_time:16788ms step_avg:92.75ms
step:182/1700 train_time:16882ms step_avg:92.76ms
step:183/1700 train_time:16975ms step_avg:92.76ms
step:184/1700 train_time:17068ms step_avg:92.76ms
step:185/1700 train_time:17163ms step_avg:92.77ms
step:186/1700 train_time:17256ms step_avg:92.78ms
step:187/1700 train_time:17350ms step_avg:92.78ms
step:188/1700 train_time:17443ms step_avg:92.78ms
step:189/1700 train_time:17536ms step_avg:92.78ms
step:190/1700 train_time:17629ms step_avg:92.78ms
step:191/1700 train_time:17722ms step_avg:92.78ms
step:192/1700 train_time:17815ms step_avg:92.79ms
step:193/1700 train_time:17908ms step_avg:92.79ms
step:194/1700 train_time:18002ms step_avg:92.80ms
step:195/1700 train_time:18095ms step_avg:92.80ms
step:196/1700 train_time:18188ms step_avg:92.80ms
step:197/1700 train_time:18283ms step_avg:92.80ms
step:198/1700 train_time:18376ms step_avg:92.81ms
step:199/1700 train_time:18469ms step_avg:92.81ms
step:200/1700 train_time:18562ms step_avg:92.81ms
step:201/1700 train_time:18656ms step_avg:92.81ms
step:202/1700 train_time:18749ms step_avg:92.81ms
step:203/1700 train_time:18843ms step_avg:92.82ms
step:204/1700 train_time:18936ms step_avg:92.82ms
step:205/1700 train_time:19029ms step_avg:92.83ms
step:206/1700 train_time:19123ms step_avg:92.83ms
step:207/1700 train_time:19216ms step_avg:92.83ms
step:208/1700 train_time:19309ms step_avg:92.83ms
step:209/1700 train_time:19403ms step_avg:92.84ms
step:210/1700 train_time:19496ms step_avg:92.84ms
step:211/1700 train_time:19589ms step_avg:92.84ms
step:212/1700 train_time:19684ms step_avg:92.85ms
step:213/1700 train_time:19777ms step_avg:92.85ms
step:214/1700 train_time:19870ms step_avg:92.85ms
step:215/1700 train_time:19964ms step_avg:92.86ms
step:216/1700 train_time:20057ms step_avg:92.86ms
step:217/1700 train_time:20151ms step_avg:92.86ms
step:218/1700 train_time:20244ms step_avg:92.86ms
step:219/1700 train_time:20338ms step_avg:92.87ms
step:220/1700 train_time:20431ms step_avg:92.87ms
step:221/1700 train_time:20524ms step_avg:92.87ms
step:222/1700 train_time:20617ms step_avg:92.87ms
step:223/1700 train_time:20710ms step_avg:92.87ms
step:224/1700 train_time:20804ms step_avg:92.87ms
step:225/1700 train_time:20898ms step_avg:92.88ms
step:226/1700 train_time:20990ms step_avg:92.88ms
step:227/1700 train_time:21084ms step_avg:92.88ms
step:228/1700 train_time:21177ms step_avg:92.88ms
step:229/1700 train_time:21270ms step_avg:92.88ms
step:230/1700 train_time:21364ms step_avg:92.89ms
step:231/1700 train_time:21457ms step_avg:92.89ms
step:232/1700 train_time:21550ms step_avg:92.89ms
step:233/1700 train_time:21643ms step_avg:92.89ms
step:234/1700 train_time:21737ms step_avg:92.89ms
step:235/1700 train_time:21830ms step_avg:92.89ms
step:236/1700 train_time:21924ms step_avg:92.90ms
step:237/1700 train_time:22017ms step_avg:92.90ms
step:238/1700 train_time:22111ms step_avg:92.90ms
step:239/1700 train_time:22204ms step_avg:92.90ms
step:240/1700 train_time:22297ms step_avg:92.91ms
step:241/1700 train_time:22390ms step_avg:92.91ms
step:242/1700 train_time:22484ms step_avg:92.91ms
step:243/1700 train_time:22578ms step_avg:92.91ms
step:244/1700 train_time:22671ms step_avg:92.91ms
step:245/1700 train_time:22764ms step_avg:92.91ms
step:246/1700 train_time:22858ms step_avg:92.92ms
step:247/1700 train_time:22951ms step_avg:92.92ms
step:248/1700 train_time:23044ms step_avg:92.92ms
step:249/1700 train_time:23138ms step_avg:92.92ms
step:250/1700 train_time:23230ms step_avg:92.92ms
step:250/1700 val_loss:4.0903 train_time:23321ms step_avg:93.28ms
step:251/1700 train_time:23345ms step_avg:93.01ms
step:252/1700 train_time:23423ms step_avg:92.95ms
step:253/1700 train_time:23519ms step_avg:92.96ms
step:254/1700 train_time:23613ms step_avg:92.96ms
step:255/1700 train_time:23707ms step_avg:92.97ms
step:256/1700 train_time:23799ms step_avg:92.97ms
step:257/1700 train_time:23892ms step_avg:92.97ms
step:258/1700 train_time:23985ms step_avg:92.97ms
step:259/1700 train_time:24078ms step_avg:92.97ms
step:260/1700 train_time:24171ms step_avg:92.97ms
step:261/1700 train_time:24265ms step_avg:92.97ms
step:262/1700 train_time:24359ms step_avg:92.98ms
step:263/1700 train_time:24455ms step_avg:92.98ms
step:264/1700 train_time:24549ms step_avg:92.99ms
step:265/1700 train_time:24643ms step_avg:92.99ms
step:266/1700 train_time:24736ms step_avg:92.99ms
step:267/1700 train_time:24831ms step_avg:93.00ms
step:268/1700 train_time:24924ms step_avg:93.00ms
step:269/1700 train_time:25016ms step_avg:93.00ms
step:270/1700 train_time:25110ms step_avg:93.00ms
step:271/1700 train_time:25204ms step_avg:93.00ms
step:272/1700 train_time:25298ms step_avg:93.01ms
step:273/1700 train_time:25392ms step_avg:93.01ms
step:274/1700 train_time:25487ms step_avg:93.02ms
step:275/1700 train_time:25581ms step_avg:93.02ms
step:276/1700 train_time:25675ms step_avg:93.03ms
step:277/1700 train_time:25769ms step_avg:93.03ms
step:278/1700 train_time:25863ms step_avg:93.03ms
step:279/1700 train_time:25956ms step_avg:93.03ms
step:280/1700 train_time:26049ms step_avg:93.03ms
step:281/1700 train_time:26143ms step_avg:93.03ms
step:282/1700 train_time:26237ms step_avg:93.04ms
step:283/1700 train_time:26330ms step_avg:93.04ms
step:284/1700 train_time:26424ms step_avg:93.04ms
step:285/1700 train_time:26518ms step_avg:93.05ms
step:286/1700 train_time:26613ms step_avg:93.05ms
step:287/1700 train_time:26707ms step_avg:93.06ms
step:288/1700 train_time:26801ms step_avg:93.06ms
step:289/1700 train_time:26895ms step_avg:93.06ms
step:290/1700 train_time:26988ms step_avg:93.06ms
step:291/1700 train_time:27082ms step_avg:93.06ms
step:292/1700 train_time:27175ms step_avg:93.07ms
step:293/1700 train_time:27269ms step_avg:93.07ms
step:294/1700 train_time:27363ms step_avg:93.07ms
step:295/1700 train_time:27457ms step_avg:93.07ms
step:296/1700 train_time:27551ms step_avg:93.08ms
step:297/1700 train_time:27645ms step_avg:93.08ms
step:298/1700 train_time:27739ms step_avg:93.08ms
step:299/1700 train_time:27832ms step_avg:93.08ms
step:300/1700 train_time:27926ms step_avg:93.09ms
step:301/1700 train_time:28019ms step_avg:93.09ms
step:302/1700 train_time:28113ms step_avg:93.09ms
step:303/1700 train_time:28207ms step_avg:93.09ms
step:304/1700 train_time:28300ms step_avg:93.09ms
step:305/1700 train_time:28394ms step_avg:93.09ms
step:306/1700 train_time:28487ms step_avg:93.10ms
step:307/1700 train_time:28581ms step_avg:93.10ms
step:308/1700 train_time:28675ms step_avg:93.10ms
step:309/1700 train_time:28770ms step_avg:93.11ms
step:310/1700 train_time:28864ms step_avg:93.11ms
step:311/1700 train_time:28957ms step_avg:93.11ms
step:312/1700 train_time:29051ms step_avg:93.11ms
step:313/1700 train_time:29145ms step_avg:93.11ms
step:314/1700 train_time:29239ms step_avg:93.12ms
step:315/1700 train_time:29332ms step_avg:93.12ms
step:316/1700 train_time:29426ms step_avg:93.12ms
step:317/1700 train_time:29519ms step_avg:93.12ms
step:318/1700 train_time:29614ms step_avg:93.12ms
step:319/1700 train_time:29708ms step_avg:93.13ms
step:320/1700 train_time:29802ms step_avg:93.13ms
step:321/1700 train_time:29896ms step_avg:93.13ms
step:322/1700 train_time:29990ms step_avg:93.14ms
step:323/1700 train_time:30084ms step_avg:93.14ms
step:324/1700 train_time:30178ms step_avg:93.14ms
step:325/1700 train_time:30271ms step_avg:93.14ms
step:326/1700 train_time:30365ms step_avg:93.14ms
step:327/1700 train_time:30458ms step_avg:93.14ms
step:328/1700 train_time:30552ms step_avg:93.15ms
step:329/1700 train_time:30646ms step_avg:93.15ms
step:330/1700 train_time:30739ms step_avg:93.15ms
step:331/1700 train_time:30833ms step_avg:93.15ms
step:332/1700 train_time:30927ms step_avg:93.15ms
step:333/1700 train_time:31020ms step_avg:93.15ms
step:334/1700 train_time:31114ms step_avg:93.16ms
step:335/1700 train_time:31208ms step_avg:93.16ms
step:336/1700 train_time:31301ms step_avg:93.16ms
step:337/1700 train_time:31395ms step_avg:93.16ms
step:338/1700 train_time:31489ms step_avg:93.16ms
step:339/1700 train_time:31582ms step_avg:93.16ms
step:340/1700 train_time:31676ms step_avg:93.17ms
step:341/1700 train_time:31770ms step_avg:93.17ms
step:342/1700 train_time:31865ms step_avg:93.17ms
step:343/1700 train_time:31958ms step_avg:93.17ms
step:344/1700 train_time:32052ms step_avg:93.17ms
step:345/1700 train_time:32146ms step_avg:93.18ms
step:346/1700 train_time:32240ms step_avg:93.18ms
step:347/1700 train_time:32334ms step_avg:93.18ms
step:348/1700 train_time:32428ms step_avg:93.18ms
step:349/1700 train_time:32522ms step_avg:93.19ms
step:350/1700 train_time:32616ms step_avg:93.19ms
step:351/1700 train_time:32710ms step_avg:93.19ms
step:352/1700 train_time:32804ms step_avg:93.19ms
step:353/1700 train_time:32897ms step_avg:93.19ms
step:354/1700 train_time:32991ms step_avg:93.20ms
step:355/1700 train_time:33084ms step_avg:93.20ms
step:356/1700 train_time:33178ms step_avg:93.20ms
step:357/1700 train_time:33272ms step_avg:93.20ms
step:358/1700 train_time:33366ms step_avg:93.20ms
step:359/1700 train_time:33460ms step_avg:93.20ms
step:360/1700 train_time:33554ms step_avg:93.20ms
step:361/1700 train_time:33648ms step_avg:93.21ms
step:362/1700 train_time:33741ms step_avg:93.21ms
step:363/1700 train_time:33835ms step_avg:93.21ms
step:364/1700 train_time:33929ms step_avg:93.21ms
step:365/1700 train_time:34023ms step_avg:93.21ms
step:366/1700 train_time:34116ms step_avg:93.21ms
step:367/1700 train_time:34210ms step_avg:93.21ms
step:368/1700 train_time:34303ms step_avg:93.22ms
step:369/1700 train_time:34397ms step_avg:93.22ms
step:370/1700 train_time:34491ms step_avg:93.22ms
step:371/1700 train_time:34585ms step_avg:93.22ms
step:372/1700 train_time:34678ms step_avg:93.22ms
step:373/1700 train_time:34772ms step_avg:93.22ms
step:374/1700 train_time:34866ms step_avg:93.23ms
step:375/1700 train_time:34960ms step_avg:93.23ms
step:375/1700 val_loss:3.8937 train_time:35050ms step_avg:93.47ms
step:376/1700 train_time:35072ms step_avg:93.28ms
step:377/1700 train_time:35156ms step_avg:93.25ms
step:378/1700 train_time:35252ms step_avg:93.26ms
step:379/1700 train_time:35347ms step_avg:93.26ms
step:380/1700 train_time:35442ms step_avg:93.27ms
step:381/1700 train_time:35536ms step_avg:93.27ms
step:382/1700 train_time:35631ms step_avg:93.27ms
step:383/1700 train_time:35726ms step_avg:93.28ms
step:384/1700 train_time:35821ms step_avg:93.28ms
step:385/1700 train_time:35916ms step_avg:93.29ms
step:386/1700 train_time:36012ms step_avg:93.29ms
step:387/1700 train_time:36111ms step_avg:93.31ms
step:388/1700 train_time:36209ms step_avg:93.32ms
step:389/1700 train_time:36305ms step_avg:93.33ms
step:390/1700 train_time:36400ms step_avg:93.33ms
step:391/1700 train_time:36496ms step_avg:93.34ms
step:392/1700 train_time:36591ms step_avg:93.34ms
step:393/1700 train_time:36687ms step_avg:93.35ms
step:394/1700 train_time:36782ms step_avg:93.35ms
step:395/1700 train_time:36876ms step_avg:93.36ms
step:396/1700 train_time:36971ms step_avg:93.36ms
step:397/1700 train_time:37067ms step_avg:93.37ms
step:398/1700 train_time:37164ms step_avg:93.38ms
step:399/1700 train_time:37259ms step_avg:93.38ms
step:400/1700 train_time:37355ms step_avg:93.39ms
step:401/1700 train_time:37451ms step_avg:93.39ms
step:402/1700 train_time:37547ms step_avg:93.40ms
step:403/1700 train_time:37643ms step_avg:93.41ms
step:404/1700 train_time:37738ms step_avg:93.41ms
step:405/1700 train_time:37834ms step_avg:93.42ms
step:406/1700 train_time:37929ms step_avg:93.42ms
step:407/1700 train_time:38025ms step_avg:93.43ms
step:408/1700 train_time:38121ms step_avg:93.43ms
step:409/1700 train_time:38217ms step_avg:93.44ms
step:410/1700 train_time:38313ms step_avg:93.45ms
step:411/1700 train_time:38409ms step_avg:93.45ms
step:412/1700 train_time:38505ms step_avg:93.46ms
step:413/1700 train_time:38600ms step_avg:93.46ms
step:414/1700 train_time:38695ms step_avg:93.47ms
step:415/1700 train_time:38790ms step_avg:93.47ms
step:416/1700 train_time:38886ms step_avg:93.48ms
step:417/1700 train_time:38981ms step_avg:93.48ms
step:418/1700 train_time:39077ms step_avg:93.49ms
step:419/1700 train_time:39173ms step_avg:93.49ms
step:420/1700 train_time:39270ms step_avg:93.50ms
step:421/1700 train_time:39366ms step_avg:93.51ms
step:422/1700 train_time:39462ms step_avg:93.51ms
step:423/1700 train_time:39558ms step_avg:93.52ms
step:424/1700 train_time:39653ms step_avg:93.52ms
step:425/1700 train_time:39749ms step_avg:93.53ms
step:426/1700 train_time:39844ms step_avg:93.53ms
step:427/1700 train_time:39939ms step_avg:93.53ms
step:428/1700 train_time:40034ms step_avg:93.54ms
step:429/1700 train_time:40130ms step_avg:93.54ms
step:430/1700 train_time:40226ms step_avg:93.55ms
step:431/1700 train_time:40322ms step_avg:93.55ms
step:432/1700 train_time:40417ms step_avg:93.56ms
step:433/1700 train_time:40513ms step_avg:93.56ms
step:434/1700 train_time:40610ms step_avg:93.57ms
step:435/1700 train_time:40707ms step_avg:93.58ms
step:436/1700 train_time:40802ms step_avg:93.58ms
step:437/1700 train_time:40898ms step_avg:93.59ms
step:438/1700 train_time:40994ms step_avg:93.59ms
step:439/1700 train_time:41089ms step_avg:93.60ms
step:440/1700 train_time:41185ms step_avg:93.60ms
step:441/1700 train_time:41280ms step_avg:93.61ms
step:442/1700 train_time:41375ms step_avg:93.61ms
step:443/1700 train_time:41471ms step_avg:93.61ms
step:444/1700 train_time:41568ms step_avg:93.62ms
step:445/1700 train_time:41663ms step_avg:93.63ms
step:446/1700 train_time:41758ms step_avg:93.63ms
step:447/1700 train_time:41854ms step_avg:93.63ms
step:448/1700 train_time:41950ms step_avg:93.64ms
step:449/1700 train_time:42046ms step_avg:93.64ms
step:450/1700 train_time:42141ms step_avg:93.65ms
step:451/1700 train_time:42236ms step_avg:93.65ms
step:452/1700 train_time:42332ms step_avg:93.65ms
step:453/1700 train_time:42428ms step_avg:93.66ms
step:454/1700 train_time:42523ms step_avg:93.66ms
step:455/1700 train_time:42618ms step_avg:93.67ms
step:456/1700 train_time:42714ms step_avg:93.67ms
step:457/1700 train_time:42810ms step_avg:93.68ms
step:458/1700 train_time:42906ms step_avg:93.68ms
step:459/1700 train_time:43002ms step_avg:93.69ms
step:460/1700 train_time:43096ms step_avg:93.69ms
step:461/1700 train_time:43192ms step_avg:93.69ms
step:462/1700 train_time:43288ms step_avg:93.70ms
step:463/1700 train_time:43384ms step_avg:93.70ms
step:464/1700 train_time:43480ms step_avg:93.71ms
step:465/1700 train_time:43576ms step_avg:93.71ms
step:466/1700 train_time:43672ms step_avg:93.72ms
step:467/1700 train_time:43768ms step_avg:93.72ms
step:468/1700 train_time:43864ms step_avg:93.73ms
step:469/1700 train_time:43959ms step_avg:93.73ms
step:470/1700 train_time:44055ms step_avg:93.73ms
step:471/1700 train_time:44150ms step_avg:93.74ms
step:472/1700 train_time:44246ms step_avg:93.74ms
step:473/1700 train_time:44341ms step_avg:93.74ms
step:474/1700 train_time:44437ms step_avg:93.75ms
step:475/1700 train_time:44532ms step_avg:93.75ms
step:476/1700 train_time:44628ms step_avg:93.76ms
step:477/1700 train_time:44724ms step_avg:93.76ms
step:478/1700 train_time:44819ms step_avg:93.76ms
step:479/1700 train_time:44915ms step_avg:93.77ms
step:480/1700 train_time:45010ms step_avg:93.77ms
step:481/1700 train_time:45105ms step_avg:93.77ms
step:482/1700 train_time:45201ms step_avg:93.78ms
step:483/1700 train_time:45296ms step_avg:93.78ms
step:484/1700 train_time:45392ms step_avg:93.78ms
step:485/1700 train_time:45488ms step_avg:93.79ms
step:486/1700 train_time:45583ms step_avg:93.79ms
step:487/1700 train_time:45679ms step_avg:93.80ms
step:488/1700 train_time:45775ms step_avg:93.80ms
step:489/1700 train_time:45870ms step_avg:93.80ms
step:490/1700 train_time:45966ms step_avg:93.81ms
step:491/1700 train_time:46062ms step_avg:93.81ms
step:492/1700 train_time:46157ms step_avg:93.81ms
step:493/1700 train_time:46253ms step_avg:93.82ms
step:494/1700 train_time:46348ms step_avg:93.82ms
step:495/1700 train_time:46443ms step_avg:93.83ms
step:496/1700 train_time:46539ms step_avg:93.83ms
step:497/1700 train_time:46634ms step_avg:93.83ms
step:498/1700 train_time:46730ms step_avg:93.84ms
step:499/1700 train_time:46827ms step_avg:93.84ms
step:500/1700 train_time:46923ms step_avg:93.85ms
step:500/1700 val_loss:3.7468 train_time:47015ms step_avg:94.03ms
step:501/1700 train_time:47039ms step_avg:93.89ms
step:502/1700 train_time:47119ms step_avg:93.86ms
step:503/1700 train_time:47216ms step_avg:93.87ms
step:504/1700 train_time:47312ms step_avg:93.87ms
step:505/1700 train_time:47407ms step_avg:93.87ms
step:506/1700 train_time:47503ms step_avg:93.88ms
step:507/1700 train_time:47598ms step_avg:93.88ms
step:508/1700 train_time:47693ms step_avg:93.88ms
step:509/1700 train_time:47789ms step_avg:93.89ms
step:510/1700 train_time:47884ms step_avg:93.89ms
step:511/1700 train_time:47980ms step_avg:93.89ms
step:512/1700 train_time:48077ms step_avg:93.90ms
step:513/1700 train_time:48174ms step_avg:93.91ms
step:514/1700 train_time:48271ms step_avg:93.91ms
step:515/1700 train_time:48367ms step_avg:93.92ms
step:516/1700 train_time:48462ms step_avg:93.92ms
step:517/1700 train_time:48558ms step_avg:93.92ms
step:518/1700 train_time:48654ms step_avg:93.93ms
step:519/1700 train_time:48749ms step_avg:93.93ms
step:520/1700 train_time:48844ms step_avg:93.93ms
step:521/1700 train_time:48940ms step_avg:93.93ms
step:522/1700 train_time:49035ms step_avg:93.94ms
step:523/1700 train_time:49133ms step_avg:93.94ms
step:524/1700 train_time:49230ms step_avg:93.95ms
step:525/1700 train_time:49326ms step_avg:93.95ms
step:526/1700 train_time:49421ms step_avg:93.96ms
step:527/1700 train_time:49516ms step_avg:93.96ms
step:528/1700 train_time:49613ms step_avg:93.96ms
step:529/1700 train_time:49708ms step_avg:93.97ms
step:530/1700 train_time:49804ms step_avg:93.97ms
step:531/1700 train_time:49899ms step_avg:93.97ms
step:532/1700 train_time:49995ms step_avg:93.98ms
step:533/1700 train_time:50092ms step_avg:93.98ms
step:534/1700 train_time:50188ms step_avg:93.99ms
step:535/1700 train_time:50284ms step_avg:93.99ms
step:536/1700 train_time:50380ms step_avg:93.99ms
step:537/1700 train_time:50476ms step_avg:94.00ms
step:538/1700 train_time:50572ms step_avg:94.00ms
step:539/1700 train_time:50668ms step_avg:94.00ms
step:540/1700 train_time:50763ms step_avg:94.01ms
step:541/1700 train_time:50859ms step_avg:94.01ms
step:542/1700 train_time:50954ms step_avg:94.01ms
step:543/1700 train_time:51050ms step_avg:94.02ms
step:544/1700 train_time:51146ms step_avg:94.02ms
step:545/1700 train_time:51242ms step_avg:94.02ms
step:546/1700 train_time:51338ms step_avg:94.02ms
step:547/1700 train_time:51434ms step_avg:94.03ms
step:548/1700 train_time:51530ms step_avg:94.03ms
step:549/1700 train_time:51625ms step_avg:94.04ms
step:550/1700 train_time:51721ms step_avg:94.04ms
step:551/1700 train_time:51817ms step_avg:94.04ms
step:552/1700 train_time:51913ms step_avg:94.05ms
step:553/1700 train_time:52008ms step_avg:94.05ms
step:554/1700 train_time:52104ms step_avg:94.05ms
step:555/1700 train_time:52200ms step_avg:94.05ms
step:556/1700 train_time:52296ms step_avg:94.06ms
step:557/1700 train_time:52393ms step_avg:94.06ms
step:558/1700 train_time:52488ms step_avg:94.07ms
step:559/1700 train_time:52584ms step_avg:94.07ms
step:560/1700 train_time:52680ms step_avg:94.07ms
step:561/1700 train_time:52775ms step_avg:94.07ms
step:562/1700 train_time:52871ms step_avg:94.08ms
step:563/1700 train_time:52967ms step_avg:94.08ms
step:564/1700 train_time:53063ms step_avg:94.08ms
step:565/1700 train_time:53159ms step_avg:94.09ms
step:566/1700 train_time:53255ms step_avg:94.09ms
step:567/1700 train_time:53352ms step_avg:94.09ms
step:568/1700 train_time:53448ms step_avg:94.10ms
step:569/1700 train_time:53544ms step_avg:94.10ms
step:570/1700 train_time:53639ms step_avg:94.10ms
step:571/1700 train_time:53735ms step_avg:94.11ms
step:572/1700 train_time:53831ms step_avg:94.11ms
step:573/1700 train_time:53926ms step_avg:94.11ms
step:574/1700 train_time:54022ms step_avg:94.12ms
step:575/1700 train_time:54118ms step_avg:94.12ms
step:576/1700 train_time:54214ms step_avg:94.12ms
step:577/1700 train_time:54311ms step_avg:94.13ms
step:578/1700 train_time:54406ms step_avg:94.13ms
step:579/1700 train_time:54502ms step_avg:94.13ms
step:580/1700 train_time:54598ms step_avg:94.13ms
step:581/1700 train_time:54693ms step_avg:94.14ms
step:582/1700 train_time:54790ms step_avg:94.14ms
step:583/1700 train_time:54886ms step_avg:94.14ms
step:584/1700 train_time:54982ms step_avg:94.15ms
step:585/1700 train_time:55077ms step_avg:94.15ms
step:586/1700 train_time:55173ms step_avg:94.15ms
step:587/1700 train_time:55269ms step_avg:94.16ms
step:588/1700 train_time:55365ms step_avg:94.16ms
step:589/1700 train_time:55461ms step_avg:94.16ms
step:590/1700 train_time:55557ms step_avg:94.16ms
step:591/1700 train_time:55653ms step_avg:94.17ms
step:592/1700 train_time:55750ms step_avg:94.17ms
step:593/1700 train_time:55845ms step_avg:94.17ms
step:594/1700 train_time:55940ms step_avg:94.18ms
step:595/1700 train_time:56036ms step_avg:94.18ms
step:596/1700 train_time:56132ms step_avg:94.18ms
step:597/1700 train_time:56228ms step_avg:94.18ms
step:598/1700 train_time:56325ms step_avg:94.19ms
step:599/1700 train_time:56421ms step_avg:94.19ms
step:600/1700 train_time:56517ms step_avg:94.19ms
step:601/1700 train_time:56614ms step_avg:94.20ms
step:602/1700 train_time:56710ms step_avg:94.20ms
step:603/1700 train_time:56806ms step_avg:94.21ms
step:604/1700 train_time:56902ms step_avg:94.21ms
step:605/1700 train_time:56997ms step_avg:94.21ms
step:606/1700 train_time:57093ms step_avg:94.21ms
step:607/1700 train_time:57189ms step_avg:94.22ms
step:608/1700 train_time:57284ms step_avg:94.22ms
step:609/1700 train_time:57380ms step_avg:94.22ms
step:610/1700 train_time:57476ms step_avg:94.22ms
step:611/1700 train_time:57572ms step_avg:94.23ms
step:612/1700 train_time:57668ms step_avg:94.23ms
step:613/1700 train_time:57764ms step_avg:94.23ms
step:614/1700 train_time:57860ms step_avg:94.23ms
step:615/1700 train_time:57956ms step_avg:94.24ms
step:616/1700 train_time:58052ms step_avg:94.24ms
step:617/1700 train_time:58148ms step_avg:94.24ms
step:618/1700 train_time:58244ms step_avg:94.25ms
step:619/1700 train_time:58339ms step_avg:94.25ms
step:620/1700 train_time:58435ms step_avg:94.25ms
step:621/1700 train_time:58531ms step_avg:94.25ms
step:622/1700 train_time:58627ms step_avg:94.26ms
step:623/1700 train_time:58723ms step_avg:94.26ms
step:624/1700 train_time:58819ms step_avg:94.26ms
step:625/1700 train_time:58915ms step_avg:94.26ms
step:625/1700 val_loss:3.6578 train_time:59008ms step_avg:94.41ms
step:626/1700 train_time:59029ms step_avg:94.30ms
step:627/1700 train_time:59117ms step_avg:94.29ms
step:628/1700 train_time:59215ms step_avg:94.29ms
step:629/1700 train_time:59311ms step_avg:94.29ms
step:630/1700 train_time:59406ms step_avg:94.30ms
step:631/1700 train_time:59501ms step_avg:94.30ms
step:632/1700 train_time:59599ms step_avg:94.30ms
step:633/1700 train_time:59695ms step_avg:94.31ms
step:634/1700 train_time:59792ms step_avg:94.31ms
step:635/1700 train_time:59888ms step_avg:94.31ms
step:636/1700 train_time:59986ms step_avg:94.32ms
step:637/1700 train_time:60086ms step_avg:94.33ms
step:638/1700 train_time:60184ms step_avg:94.33ms
step:639/1700 train_time:60283ms step_avg:94.34ms
step:640/1700 train_time:60380ms step_avg:94.34ms
step:641/1700 train_time:60478ms step_avg:94.35ms
step:642/1700 train_time:60575ms step_avg:94.35ms
step:643/1700 train_time:60672ms step_avg:94.36ms
step:644/1700 train_time:60768ms step_avg:94.36ms
step:645/1700 train_time:60865ms step_avg:94.36ms
step:646/1700 train_time:60962ms step_avg:94.37ms
step:647/1700 train_time:61061ms step_avg:94.38ms
step:648/1700 train_time:61159ms step_avg:94.38ms
step:649/1700 train_time:61259ms step_avg:94.39ms
step:650/1700 train_time:61357ms step_avg:94.39ms
step:651/1700 train_time:61454ms step_avg:94.40ms
step:652/1700 train_time:61550ms step_avg:94.40ms
step:653/1700 train_time:61647ms step_avg:94.41ms
step:654/1700 train_time:61743ms step_avg:94.41ms
step:655/1700 train_time:61841ms step_avg:94.41ms
step:656/1700 train_time:61939ms step_avg:94.42ms
step:657/1700 train_time:62038ms step_avg:94.43ms
step:658/1700 train_time:62136ms step_avg:94.43ms
step:659/1700 train_time:62234ms step_avg:94.44ms
step:660/1700 train_time:62332ms step_avg:94.44ms
step:661/1700 train_time:62429ms step_avg:94.45ms
step:662/1700 train_time:62526ms step_avg:94.45ms
step:663/1700 train_time:62623ms step_avg:94.45ms
step:664/1700 train_time:62720ms step_avg:94.46ms
step:665/1700 train_time:62818ms step_avg:94.46ms
step:666/1700 train_time:62916ms step_avg:94.47ms
step:667/1700 train_time:63013ms step_avg:94.47ms
step:668/1700 train_time:63110ms step_avg:94.48ms
step:669/1700 train_time:63207ms step_avg:94.48ms
step:670/1700 train_time:63305ms step_avg:94.49ms
step:671/1700 train_time:63402ms step_avg:94.49ms
step:672/1700 train_time:63500ms step_avg:94.49ms
step:673/1700 train_time:63598ms step_avg:94.50ms
step:674/1700 train_time:63696ms step_avg:94.50ms
step:675/1700 train_time:63793ms step_avg:94.51ms
step:676/1700 train_time:63890ms step_avg:94.51ms
step:677/1700 train_time:63986ms step_avg:94.51ms
step:678/1700 train_time:64084ms step_avg:94.52ms
step:679/1700 train_time:64182ms step_avg:94.52ms
step:680/1700 train_time:64280ms step_avg:94.53ms
step:681/1700 train_time:64378ms step_avg:94.53ms
step:682/1700 train_time:64476ms step_avg:94.54ms
step:683/1700 train_time:64573ms step_avg:94.54ms
step:684/1700 train_time:64670ms step_avg:94.55ms
step:685/1700 train_time:64767ms step_avg:94.55ms
step:686/1700 train_time:64864ms step_avg:94.55ms
step:687/1700 train_time:64961ms step_avg:94.56ms
step:688/1700 train_time:65059ms step_avg:94.56ms
step:689/1700 train_time:65157ms step_avg:94.57ms
step:690/1700 train_time:65255ms step_avg:94.57ms
step:691/1700 train_time:65352ms step_avg:94.58ms
step:692/1700 train_time:65448ms step_avg:94.58ms
step:693/1700 train_time:65546ms step_avg:94.58ms
step:694/1700 train_time:65644ms step_avg:94.59ms
step:695/1700 train_time:65741ms step_avg:94.59ms
step:696/1700 train_time:65840ms step_avg:94.60ms
step:697/1700 train_time:65938ms step_avg:94.60ms
step:698/1700 train_time:66037ms step_avg:94.61ms
step:699/1700 train_time:66134ms step_avg:94.61ms
step:700/1700 train_time:66231ms step_avg:94.62ms
step:701/1700 train_time:66328ms step_avg:94.62ms
step:702/1700 train_time:66425ms step_avg:94.62ms
step:703/1700 train_time:66523ms step_avg:94.63ms
step:704/1700 train_time:66620ms step_avg:94.63ms
step:705/1700 train_time:66719ms step_avg:94.64ms
step:706/1700 train_time:66817ms step_avg:94.64ms
step:707/1700 train_time:66915ms step_avg:94.65ms
step:708/1700 train_time:67011ms step_avg:94.65ms
step:709/1700 train_time:67108ms step_avg:94.65ms
step:710/1700 train_time:67206ms step_avg:94.66ms
step:711/1700 train_time:67303ms step_avg:94.66ms
step:712/1700 train_time:67400ms step_avg:94.66ms
step:713/1700 train_time:67498ms step_avg:94.67ms
step:714/1700 train_time:67595ms step_avg:94.67ms
step:715/1700 train_time:67693ms step_avg:94.68ms
step:716/1700 train_time:67790ms step_avg:94.68ms
step:717/1700 train_time:67887ms step_avg:94.68ms
step:718/1700 train_time:67985ms step_avg:94.69ms
step:719/1700 train_time:68082ms step_avg:94.69ms
step:720/1700 train_time:68180ms step_avg:94.69ms
step:721/1700 train_time:68277ms step_avg:94.70ms
step:722/1700 train_time:68376ms step_avg:94.70ms
step:723/1700 train_time:68473ms step_avg:94.71ms
step:724/1700 train_time:68569ms step_avg:94.71ms
step:725/1700 train_time:68666ms step_avg:94.71ms
step:726/1700 train_time:68764ms step_avg:94.72ms
step:727/1700 train_time:68861ms step_avg:94.72ms
step:728/1700 train_time:68958ms step_avg:94.72ms
step:729/1700 train_time:69056ms step_avg:94.73ms
step:730/1700 train_time:69154ms step_avg:94.73ms
step:731/1700 train_time:69250ms step_avg:94.73ms
step:732/1700 train_time:69348ms step_avg:94.74ms
step:733/1700 train_time:69445ms step_avg:94.74ms
step:734/1700 train_time:69543ms step_avg:94.75ms
step:735/1700 train_time:69640ms step_avg:94.75ms
step:736/1700 train_time:69739ms step_avg:94.75ms
step:737/1700 train_time:69838ms step_avg:94.76ms
step:738/1700 train_time:69935ms step_avg:94.76ms
step:739/1700 train_time:70033ms step_avg:94.77ms
step:740/1700 train_time:70131ms step_avg:94.77ms
step:741/1700 train_time:70227ms step_avg:94.77ms
step:742/1700 train_time:70324ms step_avg:94.78ms
step:743/1700 train_time:70422ms step_avg:94.78ms
step:744/1700 train_time:70519ms step_avg:94.78ms
step:745/1700 train_time:70617ms step_avg:94.79ms
step:746/1700 train_time:70714ms step_avg:94.79ms
step:747/1700 train_time:70811ms step_avg:94.79ms
step:748/1700 train_time:70908ms step_avg:94.80ms
step:749/1700 train_time:71006ms step_avg:94.80ms
step:750/1700 train_time:71103ms step_avg:94.80ms
step:750/1700 val_loss:3.5945 train_time:71197ms step_avg:94.93ms
step:751/1700 train_time:71218ms step_avg:94.83ms
step:752/1700 train_time:71306ms step_avg:94.82ms
step:753/1700 train_time:71405ms step_avg:94.83ms
step:754/1700 train_time:71503ms step_avg:94.83ms
step:755/1700 train_time:71600ms step_avg:94.83ms
step:756/1700 train_time:71696ms step_avg:94.84ms
step:757/1700 train_time:71794ms step_avg:94.84ms
step:758/1700 train_time:71891ms step_avg:94.84ms
step:759/1700 train_time:71988ms step_avg:94.85ms
step:760/1700 train_time:72084ms step_avg:94.85ms
step:761/1700 train_time:72182ms step_avg:94.85ms
step:762/1700 train_time:72280ms step_avg:94.86ms
step:763/1700 train_time:72379ms step_avg:94.86ms
step:764/1700 train_time:72477ms step_avg:94.87ms
step:765/1700 train_time:72575ms step_avg:94.87ms
step:766/1700 train_time:72673ms step_avg:94.87ms
step:767/1700 train_time:72771ms step_avg:94.88ms
step:768/1700 train_time:72868ms step_avg:94.88ms
step:769/1700 train_time:72965ms step_avg:94.88ms
step:770/1700 train_time:73061ms step_avg:94.88ms
step:771/1700 train_time:73159ms step_avg:94.89ms
step:772/1700 train_time:73257ms step_avg:94.89ms
step:773/1700 train_time:73356ms step_avg:94.90ms
step:774/1700 train_time:73455ms step_avg:94.90ms
step:775/1700 train_time:73553ms step_avg:94.91ms
step:776/1700 train_time:73651ms step_avg:94.91ms
step:777/1700 train_time:73750ms step_avg:94.92ms
step:778/1700 train_time:73846ms step_avg:94.92ms
step:779/1700 train_time:73944ms step_avg:94.92ms
step:780/1700 train_time:74041ms step_avg:94.92ms
step:781/1700 train_time:74139ms step_avg:94.93ms
step:782/1700 train_time:74236ms step_avg:94.93ms
step:783/1700 train_time:74335ms step_avg:94.94ms
step:784/1700 train_time:74433ms step_avg:94.94ms
step:785/1700 train_time:74532ms step_avg:94.94ms
step:786/1700 train_time:74630ms step_avg:94.95ms
step:787/1700 train_time:74727ms step_avg:94.95ms
step:788/1700 train_time:74824ms step_avg:94.95ms
step:789/1700 train_time:74921ms step_avg:94.96ms
step:790/1700 train_time:75019ms step_avg:94.96ms
step:791/1700 train_time:75116ms step_avg:94.96ms
step:792/1700 train_time:75214ms step_avg:94.97ms
step:793/1700 train_time:75312ms step_avg:94.97ms
step:794/1700 train_time:75410ms step_avg:94.97ms
step:795/1700 train_time:75508ms step_avg:94.98ms
step:796/1700 train_time:75605ms step_avg:94.98ms
step:797/1700 train_time:75703ms step_avg:94.98ms
step:798/1700 train_time:75801ms step_avg:94.99ms
step:799/1700 train_time:75898ms step_avg:94.99ms
step:800/1700 train_time:75996ms step_avg:95.00ms
step:801/1700 train_time:76094ms step_avg:95.00ms
step:802/1700 train_time:76192ms step_avg:95.00ms
step:803/1700 train_time:76290ms step_avg:95.01ms
step:804/1700 train_time:76388ms step_avg:95.01ms
step:805/1700 train_time:76485ms step_avg:95.01ms
step:806/1700 train_time:76582ms step_avg:95.01ms
step:807/1700 train_time:76681ms step_avg:95.02ms
step:808/1700 train_time:76778ms step_avg:95.02ms
step:809/1700 train_time:76876ms step_avg:95.03ms
step:810/1700 train_time:76974ms step_avg:95.03ms
step:811/1700 train_time:77072ms step_avg:95.03ms
step:812/1700 train_time:77169ms step_avg:95.04ms
step:813/1700 train_time:77266ms step_avg:95.04ms
step:814/1700 train_time:77363ms step_avg:95.04ms
step:815/1700 train_time:77461ms step_avg:95.04ms
step:816/1700 train_time:77558ms step_avg:95.05ms
step:817/1700 train_time:77656ms step_avg:95.05ms
step:818/1700 train_time:77756ms step_avg:95.06ms
step:819/1700 train_time:77854ms step_avg:95.06ms
step:820/1700 train_time:77952ms step_avg:95.06ms
step:821/1700 train_time:78050ms step_avg:95.07ms
step:822/1700 train_time:78147ms step_avg:95.07ms
step:823/1700 train_time:78245ms step_avg:95.07ms
step:824/1700 train_time:78342ms step_avg:95.08ms
step:825/1700 train_time:78439ms step_avg:95.08ms
step:826/1700 train_time:78537ms step_avg:95.08ms
step:827/1700 train_time:78635ms step_avg:95.08ms
step:828/1700 train_time:78733ms step_avg:95.09ms
step:829/1700 train_time:78832ms step_avg:95.09ms
step:830/1700 train_time:78929ms step_avg:95.10ms
step:831/1700 train_time:79027ms step_avg:95.10ms
step:832/1700 train_time:79124ms step_avg:95.10ms
step:833/1700 train_time:79221ms step_avg:95.10ms
step:834/1700 train_time:79319ms step_avg:95.11ms
step:835/1700 train_time:79417ms step_avg:95.11ms
step:836/1700 train_time:79515ms step_avg:95.11ms
step:837/1700 train_time:79614ms step_avg:95.12ms
step:838/1700 train_time:79712ms step_avg:95.12ms
step:839/1700 train_time:79810ms step_avg:95.13ms
step:840/1700 train_time:79908ms step_avg:95.13ms
step:841/1700 train_time:80005ms step_avg:95.13ms
step:842/1700 train_time:80103ms step_avg:95.13ms
step:843/1700 train_time:80200ms step_avg:95.14ms
step:844/1700 train_time:80297ms step_avg:95.14ms
step:845/1700 train_time:80395ms step_avg:95.14ms
step:846/1700 train_time:80493ms step_avg:95.15ms
step:847/1700 train_time:80591ms step_avg:95.15ms
step:848/1700 train_time:80689ms step_avg:95.15ms
step:849/1700 train_time:80786ms step_avg:95.15ms
step:850/1700 train_time:80884ms step_avg:95.16ms
step:851/1700 train_time:80982ms step_avg:95.16ms
step:852/1700 train_time:81079ms step_avg:95.16ms
step:853/1700 train_time:81177ms step_avg:95.17ms
step:854/1700 train_time:81275ms step_avg:95.17ms
step:855/1700 train_time:81373ms step_avg:95.17ms
step:856/1700 train_time:81470ms step_avg:95.18ms
step:857/1700 train_time:81567ms step_avg:95.18ms
step:858/1700 train_time:81664ms step_avg:95.18ms
step:859/1700 train_time:81762ms step_avg:95.18ms
step:860/1700 train_time:81859ms step_avg:95.18ms
step:861/1700 train_time:81957ms step_avg:95.19ms
step:862/1700 train_time:82056ms step_avg:95.19ms
step:863/1700 train_time:82154ms step_avg:95.20ms
step:864/1700 train_time:82252ms step_avg:95.20ms
step:865/1700 train_time:82350ms step_avg:95.20ms
step:866/1700 train_time:82447ms step_avg:95.20ms
step:867/1700 train_time:82544ms step_avg:95.21ms
step:868/1700 train_time:82642ms step_avg:95.21ms
step:869/1700 train_time:82740ms step_avg:95.21ms
step:870/1700 train_time:82837ms step_avg:95.22ms
step:871/1700 train_time:82936ms step_avg:95.22ms
step:872/1700 train_time:83034ms step_avg:95.22ms
step:873/1700 train_time:83133ms step_avg:95.23ms
step:874/1700 train_time:83231ms step_avg:95.23ms
step:875/1700 train_time:83329ms step_avg:95.23ms
step:875/1700 val_loss:3.5474 train_time:83423ms step_avg:95.34ms
step:876/1700 train_time:83445ms step_avg:95.26ms
step:877/1700 train_time:83530ms step_avg:95.24ms
step:878/1700 train_time:83628ms step_avg:95.25ms
step:879/1700 train_time:83725ms step_avg:95.25ms
step:880/1700 train_time:83823ms step_avg:95.25ms
step:881/1700 train_time:83920ms step_avg:95.26ms
step:882/1700 train_time:84017ms step_avg:95.26ms
step:883/1700 train_time:84114ms step_avg:95.26ms
step:884/1700 train_time:84212ms step_avg:95.26ms
step:885/1700 train_time:84310ms step_avg:95.27ms
step:886/1700 train_time:84410ms step_avg:95.27ms
step:887/1700 train_time:84510ms step_avg:95.28ms
step:888/1700 train_time:84610ms step_avg:95.28ms
step:889/1700 train_time:84709ms step_avg:95.29ms
step:890/1700 train_time:84808ms step_avg:95.29ms
step:891/1700 train_time:84907ms step_avg:95.29ms
step:892/1700 train_time:85006ms step_avg:95.30ms
step:893/1700 train_time:85105ms step_avg:95.30ms
step:894/1700 train_time:85205ms step_avg:95.31ms
step:895/1700 train_time:85305ms step_avg:95.31ms
step:896/1700 train_time:85405ms step_avg:95.32ms
step:897/1700 train_time:85505ms step_avg:95.32ms
step:898/1700 train_time:85606ms step_avg:95.33ms
step:899/1700 train_time:85706ms step_avg:95.33ms
step:900/1700 train_time:85805ms step_avg:95.34ms
step:901/1700 train_time:85905ms step_avg:95.34ms
step:902/1700 train_time:86004ms step_avg:95.35ms
step:903/1700 train_time:86103ms step_avg:95.35ms
step:904/1700 train_time:86202ms step_avg:95.36ms
step:905/1700 train_time:86301ms step_avg:95.36ms
step:906/1700 train_time:86401ms step_avg:95.37ms
step:907/1700 train_time:86501ms step_avg:95.37ms
step:908/1700 train_time:86601ms step_avg:95.38ms
step:909/1700 train_time:86701ms step_avg:95.38ms
step:910/1700 train_time:86800ms step_avg:95.38ms
step:911/1700 train_time:86898ms step_avg:95.39ms
step:912/1700 train_time:86996ms step_avg:95.39ms
step:913/1700 train_time:87094ms step_avg:95.39ms
step:914/1700 train_time:87192ms step_avg:95.40ms
step:915/1700 train_time:87291ms step_avg:95.40ms
step:916/1700 train_time:87391ms step_avg:95.41ms
step:917/1700 train_time:87491ms step_avg:95.41ms
step:918/1700 train_time:87591ms step_avg:95.42ms
step:919/1700 train_time:87692ms step_avg:95.42ms
step:920/1700 train_time:87792ms step_avg:95.43ms
step:921/1700 train_time:87891ms step_avg:95.43ms
step:922/1700 train_time:87989ms step_avg:95.43ms
step:923/1700 train_time:88088ms step_avg:95.44ms
step:924/1700 train_time:88187ms step_avg:95.44ms
step:925/1700 train_time:88287ms step_avg:95.45ms
step:926/1700 train_time:88387ms step_avg:95.45ms
step:927/1700 train_time:88486ms step_avg:95.45ms
step:928/1700 train_time:88586ms step_avg:95.46ms
step:929/1700 train_time:88686ms step_avg:95.46ms
step:930/1700 train_time:88786ms step_avg:95.47ms
step:931/1700 train_time:88886ms step_avg:95.47ms
step:932/1700 train_time:88985ms step_avg:95.48ms
step:933/1700 train_time:89085ms step_avg:95.48ms
step:934/1700 train_time:89185ms step_avg:95.49ms
step:935/1700 train_time:89285ms step_avg:95.49ms
step:936/1700 train_time:89385ms step_avg:95.50ms
step:937/1700 train_time:89485ms step_avg:95.50ms
step:938/1700 train_time:89586ms step_avg:95.51ms
step:939/1700 train_time:89686ms step_avg:95.51ms
step:940/1700 train_time:89786ms step_avg:95.52ms
step:941/1700 train_time:89886ms step_avg:95.52ms
step:942/1700 train_time:89985ms step_avg:95.53ms
step:943/1700 train_time:90084ms step_avg:95.53ms
step:944/1700 train_time:90184ms step_avg:95.53ms
step:945/1700 train_time:90284ms step_avg:95.54ms
step:946/1700 train_time:90383ms step_avg:95.54ms
step:947/1700 train_time:90482ms step_avg:95.55ms
step:948/1700 train_time:90582ms step_avg:95.55ms
step:949/1700 train_time:90683ms step_avg:95.56ms
step:950/1700 train_time:90784ms step_avg:95.56ms
step:951/1700 train_time:90884ms step_avg:95.57ms
step:952/1700 train_time:90983ms step_avg:95.57ms
step:953/1700 train_time:91083ms step_avg:95.58ms
step:954/1700 train_time:91182ms step_avg:95.58ms
step:955/1700 train_time:91281ms step_avg:95.58ms
step:956/1700 train_time:91380ms step_avg:95.59ms
step:957/1700 train_time:91479ms step_avg:95.59ms
step:958/1700 train_time:91577ms step_avg:95.59ms
step:959/1700 train_time:91677ms step_avg:95.60ms
step:960/1700 train_time:91776ms step_avg:95.60ms
step:961/1700 train_time:91873ms step_avg:95.60ms
step:962/1700 train_time:91972ms step_avg:95.61ms
step:963/1700 train_time:92071ms step_avg:95.61ms
step:964/1700 train_time:92170ms step_avg:95.61ms
step:965/1700 train_time:92269ms step_avg:95.62ms
step:966/1700 train_time:92367ms step_avg:95.62ms
step:967/1700 train_time:92467ms step_avg:95.62ms
step:968/1700 train_time:92567ms step_avg:95.63ms
step:969/1700 train_time:92666ms step_avg:95.63ms
step:970/1700 train_time:92766ms step_avg:95.64ms
step:971/1700 train_time:92866ms step_avg:95.64ms
step:972/1700 train_time:92966ms step_avg:95.64ms
step:973/1700 train_time:93066ms step_avg:95.65ms
step:974/1700 train_time:93166ms step_avg:95.65ms
step:975/1700 train_time:93265ms step_avg:95.66ms
step:976/1700 train_time:93365ms step_avg:95.66ms
step:977/1700 train_time:93464ms step_avg:95.66ms
step:978/1700 train_time:93563ms step_avg:95.67ms
step:979/1700 train_time:93662ms step_avg:95.67ms
step:980/1700 train_time:93762ms step_avg:95.68ms
step:981/1700 train_time:93862ms step_avg:95.68ms
step:982/1700 train_time:93962ms step_avg:95.68ms
step:983/1700 train_time:94063ms step_avg:95.69ms
step:984/1700 train_time:94163ms step_avg:95.69ms
step:985/1700 train_time:94262ms step_avg:95.70ms
step:986/1700 train_time:94362ms step_avg:95.70ms
step:987/1700 train_time:94462ms step_avg:95.71ms
step:988/1700 train_time:94561ms step_avg:95.71ms
step:989/1700 train_time:94660ms step_avg:95.71ms
step:990/1700 train_time:94759ms step_avg:95.72ms
step:991/1700 train_time:94860ms step_avg:95.72ms
step:992/1700 train_time:94958ms step_avg:95.72ms
step:993/1700 train_time:95057ms step_avg:95.73ms
step:994/1700 train_time:95156ms step_avg:95.73ms
step:995/1700 train_time:95254ms step_avg:95.73ms
step:996/1700 train_time:95352ms step_avg:95.73ms
step:997/1700 train_time:95450ms step_avg:95.74ms
step:998/1700 train_time:95549ms step_avg:95.74ms
step:999/1700 train_time:95648ms step_avg:95.74ms
step:1000/1700 train_time:95748ms step_avg:95.75ms
step:1000/1700 val_loss:3.5029 train_time:95844ms step_avg:95.84ms
step:1001/1700 train_time:95867ms step_avg:95.77ms
step:1002/1700 train_time:95952ms step_avg:95.76ms
step:1003/1700 train_time:96051ms step_avg:95.76ms
step:1004/1700 train_time:96150ms step_avg:95.77ms
step:1005/1700 train_time:96249ms step_avg:95.77ms
step:1006/1700 train_time:96347ms step_avg:95.77ms
step:1007/1700 train_time:96445ms step_avg:95.78ms
step:1008/1700 train_time:96544ms step_avg:95.78ms
step:1009/1700 train_time:96644ms step_avg:95.78ms
step:1010/1700 train_time:96743ms step_avg:95.78ms
step:1011/1700 train_time:96845ms step_avg:95.79ms
step:1012/1700 train_time:96947ms step_avg:95.80ms
step:1013/1700 train_time:97047ms step_avg:95.80ms
step:1014/1700 train_time:97146ms step_avg:95.80ms
step:1015/1700 train_time:97246ms step_avg:95.81ms
step:1016/1700 train_time:97345ms step_avg:95.81ms
step:1017/1700 train_time:97445ms step_avg:95.82ms
step:1018/1700 train_time:97544ms step_avg:95.82ms
step:1019/1700 train_time:97643ms step_avg:95.82ms
step:1020/1700 train_time:97742ms step_avg:95.83ms
step:1021/1700 train_time:97843ms step_avg:95.83ms
step:1022/1700 train_time:97944ms step_avg:95.84ms
step:1023/1700 train_time:98045ms step_avg:95.84ms
step:1024/1700 train_time:98146ms step_avg:95.85ms
step:1025/1700 train_time:98246ms step_avg:95.85ms
step:1026/1700 train_time:98345ms step_avg:95.85ms
step:1027/1700 train_time:98446ms step_avg:95.86ms
step:1028/1700 train_time:98545ms step_avg:95.86ms
step:1029/1700 train_time:98644ms step_avg:95.86ms
step:1030/1700 train_time:98744ms step_avg:95.87ms
step:1031/1700 train_time:98844ms step_avg:95.87ms
step:1032/1700 train_time:98944ms step_avg:95.88ms
step:1033/1700 train_time:99044ms step_avg:95.88ms
step:1034/1700 train_time:99144ms step_avg:95.88ms
step:1035/1700 train_time:99244ms step_avg:95.89ms
step:1036/1700 train_time:99344ms step_avg:95.89ms
step:1037/1700 train_time:99444ms step_avg:95.90ms
step:1038/1700 train_time:99544ms step_avg:95.90ms
step:1039/1700 train_time:99642ms step_avg:95.90ms
step:1040/1700 train_time:99741ms step_avg:95.90ms
step:1041/1700 train_time:99840ms step_avg:95.91ms
step:1042/1700 train_time:99940ms step_avg:95.91ms
step:1043/1700 train_time:100041ms step_avg:95.92ms
step:1044/1700 train_time:100142ms step_avg:95.92ms
step:1045/1700 train_time:100243ms step_avg:95.93ms
step:1046/1700 train_time:100343ms step_avg:95.93ms
step:1047/1700 train_time:100442ms step_avg:95.93ms
step:1048/1700 train_time:100541ms step_avg:95.94ms
step:1049/1700 train_time:100641ms step_avg:95.94ms
step:1050/1700 train_time:100741ms step_avg:95.94ms
step:1051/1700 train_time:100840ms step_avg:95.95ms
step:1052/1700 train_time:100939ms step_avg:95.95ms
step:1053/1700 train_time:101038ms step_avg:95.95ms
step:1054/1700 train_time:101137ms step_avg:95.96ms
step:1055/1700 train_time:101236ms step_avg:95.96ms
step:1056/1700 train_time:101335ms step_avg:95.96ms
step:1057/1700 train_time:101433ms step_avg:95.96ms
step:1058/1700 train_time:101531ms step_avg:95.97ms
step:1059/1700 train_time:101631ms step_avg:95.97ms
step:1060/1700 train_time:101731ms step_avg:95.97ms
step:1061/1700 train_time:101832ms step_avg:95.98ms
step:1062/1700 train_time:101932ms step_avg:95.98ms
step:1063/1700 train_time:102034ms step_avg:95.99ms
step:1064/1700 train_time:102133ms step_avg:95.99ms
step:1065/1700 train_time:102233ms step_avg:95.99ms
step:1066/1700 train_time:102332ms step_avg:96.00ms
step:1067/1700 train_time:102430ms step_avg:96.00ms
step:1068/1700 train_time:102529ms step_avg:96.00ms
step:1069/1700 train_time:102627ms step_avg:96.00ms
step:1070/1700 train_time:102725ms step_avg:96.00ms
step:1071/1700 train_time:102825ms step_avg:96.01ms
step:1072/1700 train_time:102926ms step_avg:96.01ms
step:1073/1700 train_time:103026ms step_avg:96.02ms
step:1074/1700 train_time:103125ms step_avg:96.02ms
step:1075/1700 train_time:103226ms step_avg:96.02ms
step:1076/1700 train_time:103326ms step_avg:96.03ms
step:1077/1700 train_time:103426ms step_avg:96.03ms
step:1078/1700 train_time:103525ms step_avg:96.03ms
step:1079/1700 train_time:103624ms step_avg:96.04ms
step:1080/1700 train_time:103723ms step_avg:96.04ms
step:1081/1700 train_time:103824ms step_avg:96.04ms
step:1082/1700 train_time:103924ms step_avg:96.05ms
step:1083/1700 train_time:104024ms step_avg:96.05ms
step:1084/1700 train_time:104125ms step_avg:96.06ms
step:1085/1700 train_time:104225ms step_avg:96.06ms
step:1086/1700 train_time:104325ms step_avg:96.06ms
step:1087/1700 train_time:104425ms step_avg:96.07ms
step:1088/1700 train_time:104525ms step_avg:96.07ms
step:1089/1700 train_time:104624ms step_avg:96.07ms
step:1090/1700 train_time:104724ms step_avg:96.08ms
step:1091/1700 train_time:104824ms step_avg:96.08ms
step:1092/1700 train_time:104924ms step_avg:96.08ms
step:1093/1700 train_time:105024ms step_avg:96.09ms
step:1094/1700 train_time:105125ms step_avg:96.09ms
step:1095/1700 train_time:105225ms step_avg:96.10ms
step:1096/1700 train_time:105325ms step_avg:96.10ms
step:1097/1700 train_time:105426ms step_avg:96.10ms
step:1098/1700 train_time:105525ms step_avg:96.11ms
step:1099/1700 train_time:105625ms step_avg:96.11ms
step:1100/1700 train_time:105724ms step_avg:96.11ms
step:1101/1700 train_time:105824ms step_avg:96.12ms
step:1102/1700 train_time:105924ms step_avg:96.12ms
step:1103/1700 train_time:106024ms step_avg:96.12ms
step:1104/1700 train_time:106124ms step_avg:96.13ms
step:1105/1700 train_time:106224ms step_avg:96.13ms
step:1106/1700 train_time:106325ms step_avg:96.13ms
step:1107/1700 train_time:106426ms step_avg:96.14ms
step:1108/1700 train_time:106526ms step_avg:96.14ms
step:1109/1700 train_time:106626ms step_avg:96.15ms
step:1110/1700 train_time:106727ms step_avg:96.15ms
step:1111/1700 train_time:106827ms step_avg:96.15ms
step:1112/1700 train_time:106926ms step_avg:96.16ms
step:1113/1700 train_time:107026ms step_avg:96.16ms
step:1114/1700 train_time:107126ms step_avg:96.16ms
step:1115/1700 train_time:107226ms step_avg:96.17ms
step:1116/1700 train_time:107327ms step_avg:96.17ms
step:1117/1700 train_time:107427ms step_avg:96.17ms
step:1118/1700 train_time:107527ms step_avg:96.18ms
step:1119/1700 train_time:107626ms step_avg:96.18ms
step:1120/1700 train_time:107725ms step_avg:96.18ms
step:1121/1700 train_time:107825ms step_avg:96.19ms
step:1122/1700 train_time:107925ms step_avg:96.19ms
step:1123/1700 train_time:108026ms step_avg:96.19ms
step:1124/1700 train_time:108126ms step_avg:96.20ms
step:1125/1700 train_time:108226ms step_avg:96.20ms
step:1125/1700 val_loss:3.4494 train_time:108323ms step_avg:96.29ms
step:1126/1700 train_time:108344ms step_avg:96.22ms
step:1127/1700 train_time:108432ms step_avg:96.21ms
step:1128/1700 train_time:108531ms step_avg:96.22ms
step:1129/1700 train_time:108631ms step_avg:96.22ms
step:1130/1700 train_time:108730ms step_avg:96.22ms
step:1131/1700 train_time:108829ms step_avg:96.22ms
step:1132/1700 train_time:108927ms step_avg:96.23ms
step:1133/1700 train_time:109026ms step_avg:96.23ms
step:1134/1700 train_time:109124ms step_avg:96.23ms
step:1135/1700 train_time:109223ms step_avg:96.23ms
step:1136/1700 train_time:109325ms step_avg:96.24ms
step:1137/1700 train_time:109429ms step_avg:96.24ms
step:1138/1700 train_time:109530ms step_avg:96.25ms
step:1139/1700 train_time:109630ms step_avg:96.25ms
step:1140/1700 train_time:109731ms step_avg:96.25ms
step:1141/1700 train_time:109830ms step_avg:96.26ms
step:1142/1700 train_time:109930ms step_avg:96.26ms
step:1143/1700 train_time:110029ms step_avg:96.26ms
step:1144/1700 train_time:110128ms step_avg:96.27ms
step:1145/1700 train_time:110228ms step_avg:96.27ms
step:1146/1700 train_time:110328ms step_avg:96.27ms
step:1147/1700 train_time:110429ms step_avg:96.28ms
step:1148/1700 train_time:110531ms step_avg:96.28ms
step:1149/1700 train_time:110631ms step_avg:96.28ms
step:1150/1700 train_time:110731ms step_avg:96.29ms
step:1151/1700 train_time:110831ms step_avg:96.29ms
step:1152/1700 train_time:110931ms step_avg:96.29ms
step:1153/1700 train_time:111031ms step_avg:96.30ms
step:1154/1700 train_time:111130ms step_avg:96.30ms
step:1155/1700 train_time:111230ms step_avg:96.30ms
step:1156/1700 train_time:111330ms step_avg:96.31ms
step:1157/1700 train_time:111432ms step_avg:96.31ms
step:1158/1700 train_time:111532ms step_avg:96.31ms
step:1159/1700 train_time:111633ms step_avg:96.32ms
step:1160/1700 train_time:111733ms step_avg:96.32ms
step:1161/1700 train_time:111833ms step_avg:96.32ms
step:1162/1700 train_time:111932ms step_avg:96.33ms
step:1163/1700 train_time:112031ms step_avg:96.33ms
step:1164/1700 train_time:112131ms step_avg:96.33ms
step:1165/1700 train_time:112231ms step_avg:96.34ms
step:1166/1700 train_time:112331ms step_avg:96.34ms
step:1167/1700 train_time:112432ms step_avg:96.34ms
step:1168/1700 train_time:112533ms step_avg:96.35ms
step:1169/1700 train_time:112634ms step_avg:96.35ms
step:1170/1700 train_time:112734ms step_avg:96.35ms
step:1171/1700 train_time:112834ms step_avg:96.36ms
step:1172/1700 train_time:112935ms step_avg:96.36ms
step:1173/1700 train_time:113033ms step_avg:96.36ms
step:1174/1700 train_time:113134ms step_avg:96.37ms
step:1175/1700 train_time:113232ms step_avg:96.37ms
step:1176/1700 train_time:113333ms step_avg:96.37ms
step:1177/1700 train_time:113433ms step_avg:96.37ms
step:1178/1700 train_time:113534ms step_avg:96.38ms
step:1179/1700 train_time:113635ms step_avg:96.38ms
step:1180/1700 train_time:113735ms step_avg:96.39ms
step:1181/1700 train_time:113836ms step_avg:96.39ms
step:1182/1700 train_time:113936ms step_avg:96.39ms
step:1183/1700 train_time:114035ms step_avg:96.39ms
step:1184/1700 train_time:114136ms step_avg:96.40ms
step:1185/1700 train_time:114236ms step_avg:96.40ms
step:1186/1700 train_time:114337ms step_avg:96.41ms
step:1187/1700 train_time:114436ms step_avg:96.41ms
step:1188/1700 train_time:114536ms step_avg:96.41ms
step:1189/1700 train_time:114636ms step_avg:96.41ms
step:1190/1700 train_time:114735ms step_avg:96.42ms
step:1191/1700 train_time:114835ms step_avg:96.42ms
step:1192/1700 train_time:114935ms step_avg:96.42ms
step:1193/1700 train_time:115034ms step_avg:96.42ms
step:1194/1700 train_time:115135ms step_avg:96.43ms
step:1195/1700 train_time:115234ms step_avg:96.43ms
step:1196/1700 train_time:115333ms step_avg:96.43ms
step:1197/1700 train_time:115433ms step_avg:96.44ms
step:1198/1700 train_time:115533ms step_avg:96.44ms
step:1199/1700 train_time:115635ms step_avg:96.44ms
step:1200/1700 train_time:115735ms step_avg:96.45ms
step:1201/1700 train_time:115834ms step_avg:96.45ms
step:1202/1700 train_time:115934ms step_avg:96.45ms
step:1203/1700 train_time:116035ms step_avg:96.45ms
step:1204/1700 train_time:116135ms step_avg:96.46ms
step:1205/1700 train_time:116235ms step_avg:96.46ms
step:1206/1700 train_time:116335ms step_avg:96.46ms
step:1207/1700 train_time:116434ms step_avg:96.47ms
step:1208/1700 train_time:116534ms step_avg:96.47ms
step:1209/1700 train_time:116634ms step_avg:96.47ms
step:1210/1700 train_time:116734ms step_avg:96.47ms
step:1211/1700 train_time:116834ms step_avg:96.48ms
step:1212/1700 train_time:116934ms step_avg:96.48ms
step:1213/1700 train_time:117034ms step_avg:96.48ms
step:1214/1700 train_time:117134ms step_avg:96.49ms
step:1215/1700 train_time:117234ms step_avg:96.49ms
step:1216/1700 train_time:117333ms step_avg:96.49ms
step:1217/1700 train_time:117433ms step_avg:96.49ms
step:1218/1700 train_time:117533ms step_avg:96.50ms
step:1219/1700 train_time:117634ms step_avg:96.50ms
step:1220/1700 train_time:117735ms step_avg:96.50ms
step:1221/1700 train_time:117834ms step_avg:96.51ms
step:1222/1700 train_time:117934ms step_avg:96.51ms
step:1223/1700 train_time:118036ms step_avg:96.51ms
step:1224/1700 train_time:118135ms step_avg:96.52ms
step:1225/1700 train_time:118235ms step_avg:96.52ms
step:1226/1700 train_time:118335ms step_avg:96.52ms
step:1227/1700 train_time:118434ms step_avg:96.52ms
step:1228/1700 train_time:118534ms step_avg:96.53ms
step:1229/1700 train_time:118634ms step_avg:96.53ms
step:1230/1700 train_time:118734ms step_avg:96.53ms
step:1231/1700 train_time:118833ms step_avg:96.53ms
step:1232/1700 train_time:118935ms step_avg:96.54ms
step:1233/1700 train_time:119034ms step_avg:96.54ms
step:1234/1700 train_time:119134ms step_avg:96.54ms
step:1235/1700 train_time:119234ms step_avg:96.55ms
step:1236/1700 train_time:119334ms step_avg:96.55ms
step:1237/1700 train_time:119434ms step_avg:96.55ms
step:1238/1700 train_time:119533ms step_avg:96.55ms
step:1239/1700 train_time:119633ms step_avg:96.56ms
step:1240/1700 train_time:119733ms step_avg:96.56ms
step:1241/1700 train_time:119834ms step_avg:96.56ms
step:1242/1700 train_time:119936ms step_avg:96.57ms
step:1243/1700 train_time:120036ms step_avg:96.57ms
step:1244/1700 train_time:120136ms step_avg:96.57ms
step:1245/1700 train_time:120235ms step_avg:96.57ms
step:1246/1700 train_time:120335ms step_avg:96.58ms
step:1247/1700 train_time:120435ms step_avg:96.58ms
step:1248/1700 train_time:120535ms step_avg:96.58ms
step:1249/1700 train_time:120635ms step_avg:96.58ms
step:1250/1700 train_time:120735ms step_avg:96.59ms
step:1250/1700 val_loss:3.4037 train_time:120830ms step_avg:96.66ms
step:1251/1700 train_time:120853ms step_avg:96.61ms
step:1252/1700 train_time:120940ms step_avg:96.60ms
step:1253/1700 train_time:121041ms step_avg:96.60ms
step:1254/1700 train_time:121141ms step_avg:96.60ms
step:1255/1700 train_time:121241ms step_avg:96.61ms
step:1256/1700 train_time:121341ms step_avg:96.61ms
step:1257/1700 train_time:121441ms step_avg:96.61ms
step:1258/1700 train_time:121541ms step_avg:96.61ms
step:1259/1700 train_time:121641ms step_avg:96.62ms
step:1260/1700 train_time:121741ms step_avg:96.62ms
step:1261/1700 train_time:121844ms step_avg:96.62ms
step:1262/1700 train_time:121945ms step_avg:96.63ms
step:1263/1700 train_time:122045ms step_avg:96.63ms
step:1264/1700 train_time:122144ms step_avg:96.63ms
step:1265/1700 train_time:122244ms step_avg:96.64ms
step:1266/1700 train_time:122343ms step_avg:96.64ms
step:1267/1700 train_time:122442ms step_avg:96.64ms
step:1268/1700 train_time:122541ms step_avg:96.64ms
step:1269/1700 train_time:122641ms step_avg:96.64ms
step:1270/1700 train_time:122741ms step_avg:96.65ms
step:1271/1700 train_time:122844ms step_avg:96.65ms
step:1272/1700 train_time:122944ms step_avg:96.65ms
step:1273/1700 train_time:123044ms step_avg:96.66ms
step:1274/1700 train_time:123144ms step_avg:96.66ms
step:1275/1700 train_time:123242ms step_avg:96.66ms
step:1276/1700 train_time:123342ms step_avg:96.66ms
step:1277/1700 train_time:123442ms step_avg:96.67ms
step:1278/1700 train_time:123541ms step_avg:96.67ms
step:1279/1700 train_time:123641ms step_avg:96.67ms
step:1280/1700 train_time:123741ms step_avg:96.67ms
step:1281/1700 train_time:123841ms step_avg:96.68ms
step:1282/1700 train_time:123942ms step_avg:96.68ms
step:1283/1700 train_time:124044ms step_avg:96.68ms
step:1284/1700 train_time:124144ms step_avg:96.69ms
step:1285/1700 train_time:124243ms step_avg:96.69ms
step:1286/1700 train_time:124342ms step_avg:96.69ms
step:1287/1700 train_time:124442ms step_avg:96.69ms
step:1288/1700 train_time:124542ms step_avg:96.69ms
step:1289/1700 train_time:124642ms step_avg:96.70ms
step:1290/1700 train_time:124742ms step_avg:96.70ms
step:1291/1700 train_time:124842ms step_avg:96.70ms
step:1292/1700 train_time:124943ms step_avg:96.71ms
step:1293/1700 train_time:125044ms step_avg:96.71ms
step:1294/1700 train_time:125145ms step_avg:96.71ms
step:1295/1700 train_time:125246ms step_avg:96.71ms
step:1296/1700 train_time:125345ms step_avg:96.72ms
step:1297/1700 train_time:125445ms step_avg:96.72ms
step:1298/1700 train_time:125545ms step_avg:96.72ms
step:1299/1700 train_time:125645ms step_avg:96.72ms
step:1300/1700 train_time:125745ms step_avg:96.73ms
step:1301/1700 train_time:125845ms step_avg:96.73ms
step:1302/1700 train_time:125944ms step_avg:96.73ms
step:1303/1700 train_time:126045ms step_avg:96.73ms
step:1304/1700 train_time:126145ms step_avg:96.74ms
step:1305/1700 train_time:126245ms step_avg:96.74ms
step:1306/1700 train_time:126345ms step_avg:96.74ms
step:1307/1700 train_time:126444ms step_avg:96.74ms
step:1308/1700 train_time:126544ms step_avg:96.75ms
step:1309/1700 train_time:126643ms step_avg:96.75ms
step:1310/1700 train_time:126743ms step_avg:96.75ms
step:1311/1700 train_time:126843ms step_avg:96.75ms
step:1312/1700 train_time:126943ms step_avg:96.76ms
step:1313/1700 train_time:127044ms step_avg:96.76ms
step:1314/1700 train_time:127144ms step_avg:96.76ms
step:1315/1700 train_time:127244ms step_avg:96.76ms
step:1316/1700 train_time:127343ms step_avg:96.77ms
step:1317/1700 train_time:127443ms step_avg:96.77ms
step:1318/1700 train_time:127542ms step_avg:96.77ms
step:1319/1700 train_time:127643ms step_avg:96.77ms
step:1320/1700 train_time:127744ms step_avg:96.78ms
step:1321/1700 train_time:127843ms step_avg:96.78ms
step:1322/1700 train_time:127942ms step_avg:96.78ms
step:1323/1700 train_time:128043ms step_avg:96.78ms
step:1324/1700 train_time:128144ms step_avg:96.79ms
step:1325/1700 train_time:128244ms step_avg:96.79ms
step:1326/1700 train_time:128345ms step_avg:96.79ms
step:1327/1700 train_time:128445ms step_avg:96.79ms
step:1328/1700 train_time:128544ms step_avg:96.80ms
step:1329/1700 train_time:128643ms step_avg:96.80ms
step:1330/1700 train_time:128743ms step_avg:96.80ms
step:1331/1700 train_time:128842ms step_avg:96.80ms
step:1332/1700 train_time:128942ms step_avg:96.80ms
step:1333/1700 train_time:129043ms step_avg:96.81ms
step:1334/1700 train_time:129142ms step_avg:96.81ms
step:1335/1700 train_time:129242ms step_avg:96.81ms
step:1336/1700 train_time:129344ms step_avg:96.81ms
step:1337/1700 train_time:129444ms step_avg:96.82ms
step:1338/1700 train_time:129544ms step_avg:96.82ms
step:1339/1700 train_time:129644ms step_avg:96.82ms
step:1340/1700 train_time:129744ms step_avg:96.82ms
step:1341/1700 train_time:129844ms step_avg:96.83ms
step:1342/1700 train_time:129943ms step_avg:96.83ms
step:1343/1700 train_time:130042ms step_avg:96.83ms
step:1344/1700 train_time:130143ms step_avg:96.83ms
step:1345/1700 train_time:130243ms step_avg:96.83ms
step:1346/1700 train_time:130344ms step_avg:96.84ms
step:1347/1700 train_time:130445ms step_avg:96.84ms
step:1348/1700 train_time:130544ms step_avg:96.84ms
step:1349/1700 train_time:130644ms step_avg:96.84ms
step:1350/1700 train_time:130744ms step_avg:96.85ms
step:1351/1700 train_time:130844ms step_avg:96.85ms
step:1352/1700 train_time:130943ms step_avg:96.85ms
step:1353/1700 train_time:131043ms step_avg:96.85ms
step:1354/1700 train_time:131143ms step_avg:96.86ms
step:1355/1700 train_time:131243ms step_avg:96.86ms
step:1356/1700 train_time:131343ms step_avg:96.86ms
step:1357/1700 train_time:131442ms step_avg:96.86ms
step:1358/1700 train_time:131544ms step_avg:96.87ms
step:1359/1700 train_time:131644ms step_avg:96.87ms
step:1360/1700 train_time:131745ms step_avg:96.87ms
step:1361/1700 train_time:131844ms step_avg:96.87ms
step:1362/1700 train_time:131944ms step_avg:96.87ms
step:1363/1700 train_time:132044ms step_avg:96.88ms
step:1364/1700 train_time:132143ms step_avg:96.88ms
step:1365/1700 train_time:132242ms step_avg:96.88ms
step:1366/1700 train_time:132342ms step_avg:96.88ms
step:1367/1700 train_time:132442ms step_avg:96.88ms
step:1368/1700 train_time:132542ms step_avg:96.89ms
step:1369/1700 train_time:132642ms step_avg:96.89ms
step:1370/1700 train_time:132742ms step_avg:96.89ms
step:1371/1700 train_time:132842ms step_avg:96.89ms
step:1372/1700 train_time:132942ms step_avg:96.90ms
step:1373/1700 train_time:133043ms step_avg:96.90ms
step:1374/1700 train_time:133142ms step_avg:96.90ms
step:1375/1700 train_time:133243ms step_avg:96.90ms
step:1375/1700 val_loss:3.3645 train_time:133339ms step_avg:96.97ms
step:1376/1700 train_time:133361ms step_avg:96.92ms
step:1377/1700 train_time:133452ms step_avg:96.92ms
step:1378/1700 train_time:133553ms step_avg:96.92ms
step:1379/1700 train_time:133652ms step_avg:96.92ms
step:1380/1700 train_time:133753ms step_avg:96.92ms
step:1381/1700 train_time:133852ms step_avg:96.92ms
step:1382/1700 train_time:133951ms step_avg:96.93ms
step:1383/1700 train_time:134051ms step_avg:96.93ms
step:1384/1700 train_time:134151ms step_avg:96.93ms
step:1385/1700 train_time:134250ms step_avg:96.93ms
step:1386/1700 train_time:134353ms step_avg:96.94ms
step:1387/1700 train_time:134454ms step_avg:96.94ms
step:1388/1700 train_time:134555ms step_avg:96.94ms
step:1389/1700 train_time:134657ms step_avg:96.95ms
step:1390/1700 train_time:134757ms step_avg:96.95ms
step:1391/1700 train_time:134859ms step_avg:96.95ms
step:1392/1700 train_time:134961ms step_avg:96.95ms
step:1393/1700 train_time:135063ms step_avg:96.96ms
step:1394/1700 train_time:135166ms step_avg:96.96ms
step:1395/1700 train_time:135268ms step_avg:96.97ms
step:1396/1700 train_time:135370ms step_avg:96.97ms
step:1397/1700 train_time:135471ms step_avg:96.97ms
step:1398/1700 train_time:135572ms step_avg:96.98ms
step:1399/1700 train_time:135673ms step_avg:96.98ms
step:1400/1700 train_time:135774ms step_avg:96.98ms
step:1401/1700 train_time:135875ms step_avg:96.98ms
step:1402/1700 train_time:135975ms step_avg:96.99ms
step:1403/1700 train_time:136077ms step_avg:96.99ms
step:1404/1700 train_time:136178ms step_avg:96.99ms
step:1405/1700 train_time:136280ms step_avg:97.00ms
step:1406/1700 train_time:136382ms step_avg:97.00ms
step:1407/1700 train_time:136485ms step_avg:97.00ms
step:1408/1700 train_time:136588ms step_avg:97.01ms
step:1409/1700 train_time:136691ms step_avg:97.01ms
step:1410/1700 train_time:136792ms step_avg:97.02ms
step:1411/1700 train_time:136892ms step_avg:97.02ms
step:1412/1700 train_time:136994ms step_avg:97.02ms
step:1413/1700 train_time:137094ms step_avg:97.02ms
step:1414/1700 train_time:137195ms step_avg:97.03ms
step:1415/1700 train_time:137296ms step_avg:97.03ms
step:1416/1700 train_time:137396ms step_avg:97.03ms
step:1417/1700 train_time:137497ms step_avg:97.03ms
step:1418/1700 train_time:137597ms step_avg:97.04ms
step:1419/1700 train_time:137699ms step_avg:97.04ms
step:1420/1700 train_time:137801ms step_avg:97.04ms
step:1421/1700 train_time:137902ms step_avg:97.05ms
step:1422/1700 train_time:138004ms step_avg:97.05ms
step:1423/1700 train_time:138106ms step_avg:97.05ms
step:1424/1700 train_time:138209ms step_avg:97.06ms
step:1425/1700 train_time:138311ms step_avg:97.06ms
step:1426/1700 train_time:138411ms step_avg:97.06ms
step:1427/1700 train_time:138511ms step_avg:97.06ms
step:1428/1700 train_time:138613ms step_avg:97.07ms
step:1429/1700 train_time:138713ms step_avg:97.07ms
step:1430/1700 train_time:138816ms step_avg:97.07ms
step:1431/1700 train_time:138918ms step_avg:97.08ms
step:1432/1700 train_time:139018ms step_avg:97.08ms
step:1433/1700 train_time:139118ms step_avg:97.08ms
step:1434/1700 train_time:139219ms step_avg:97.08ms
step:1435/1700 train_time:139321ms step_avg:97.09ms
step:1436/1700 train_time:139425ms step_avg:97.09ms
step:1437/1700 train_time:139529ms step_avg:97.10ms
step:1438/1700 train_time:139629ms step_avg:97.10ms
step:1439/1700 train_time:139731ms step_avg:97.10ms
step:1440/1700 train_time:139833ms step_avg:97.11ms
step:1441/1700 train_time:139935ms step_avg:97.11ms
step:1442/1700 train_time:140034ms step_avg:97.11ms
step:1443/1700 train_time:140135ms step_avg:97.11ms
step:1444/1700 train_time:140236ms step_avg:97.12ms
step:1445/1700 train_time:140337ms step_avg:97.12ms
step:1446/1700 train_time:140437ms step_avg:97.12ms
step:1447/1700 train_time:140538ms step_avg:97.12ms
step:1448/1700 train_time:140638ms step_avg:97.13ms
step:1449/1700 train_time:140739ms step_avg:97.13ms
step:1450/1700 train_time:140841ms step_avg:97.13ms
step:1451/1700 train_time:140943ms step_avg:97.13ms
step:1452/1700 train_time:141045ms step_avg:97.14ms
step:1453/1700 train_time:141147ms step_avg:97.14ms
step:1454/1700 train_time:141252ms step_avg:97.15ms
step:1455/1700 train_time:141353ms step_avg:97.15ms
step:1456/1700 train_time:141453ms step_avg:97.15ms
step:1457/1700 train_time:141554ms step_avg:97.15ms
step:1458/1700 train_time:141655ms step_avg:97.16ms
step:1459/1700 train_time:141755ms step_avg:97.16ms
step:1460/1700 train_time:141856ms step_avg:97.16ms
step:1461/1700 train_time:141956ms step_avg:97.16ms
step:1462/1700 train_time:142056ms step_avg:97.17ms
step:1463/1700 train_time:142159ms step_avg:97.17ms
step:1464/1700 train_time:142260ms step_avg:97.17ms
step:1465/1700 train_time:142362ms step_avg:97.18ms
step:1466/1700 train_time:142464ms step_avg:97.18ms
step:1467/1700 train_time:142567ms step_avg:97.18ms
step:1468/1700 train_time:142669ms step_avg:97.19ms
step:1469/1700 train_time:142771ms step_avg:97.19ms
step:1470/1700 train_time:142871ms step_avg:97.19ms
step:1471/1700 train_time:142972ms step_avg:97.19ms
step:1472/1700 train_time:143072ms step_avg:97.20ms
step:1473/1700 train_time:143173ms step_avg:97.20ms
step:1474/1700 train_time:143274ms step_avg:97.20ms
step:1475/1700 train_time:143374ms step_avg:97.20ms
step:1476/1700 train_time:143476ms step_avg:97.21ms
step:1477/1700 train_time:143578ms step_avg:97.21ms
step:1478/1700 train_time:143679ms step_avg:97.21ms
step:1479/1700 train_time:143783ms step_avg:97.22ms
step:1480/1700 train_time:143886ms step_avg:97.22ms
step:1481/1700 train_time:143988ms step_avg:97.22ms
step:1482/1700 train_time:144091ms step_avg:97.23ms
step:1483/1700 train_time:144191ms step_avg:97.23ms
step:1484/1700 train_time:144292ms step_avg:97.23ms
step:1485/1700 train_time:144393ms step_avg:97.23ms
step:1486/1700 train_time:144494ms step_avg:97.24ms
step:1487/1700 train_time:144596ms step_avg:97.24ms
step:1488/1700 train_time:144697ms step_avg:97.24ms
step:1489/1700 train_time:144798ms step_avg:97.25ms
step:1490/1700 train_time:144900ms step_avg:97.25ms
step:1491/1700 train_time:145003ms step_avg:97.25ms
step:1492/1700 train_time:145106ms step_avg:97.26ms
step:1493/1700 train_time:145208ms step_avg:97.26ms
step:1494/1700 train_time:145309ms step_avg:97.26ms
step:1495/1700 train_time:145409ms step_avg:97.26ms
step:1496/1700 train_time:145510ms step_avg:97.27ms
step:1497/1700 train_time:145610ms step_avg:97.27ms
step:1498/1700 train_time:145712ms step_avg:97.27ms
step:1499/1700 train_time:145812ms step_avg:97.27ms
step:1500/1700 train_time:145913ms step_avg:97.28ms
step:1500/1700 val_loss:3.3293 train_time:146012ms step_avg:97.34ms
step:1501/1700 train_time:146034ms step_avg:97.29ms
step:1502/1700 train_time:146124ms step_avg:97.29ms
step:1503/1700 train_time:146225ms step_avg:97.29ms
step:1504/1700 train_time:146327ms step_avg:97.29ms
step:1505/1700 train_time:146428ms step_avg:97.29ms
step:1506/1700 train_time:146529ms step_avg:97.30ms
step:1507/1700 train_time:146630ms step_avg:97.30ms
step:1508/1700 train_time:146730ms step_avg:97.30ms
step:1509/1700 train_time:146832ms step_avg:97.30ms
step:1510/1700 train_time:146934ms step_avg:97.31ms
step:1511/1700 train_time:147038ms step_avg:97.31ms
step:1512/1700 train_time:147140ms step_avg:97.32ms
step:1513/1700 train_time:147241ms step_avg:97.32ms
step:1514/1700 train_time:147343ms step_avg:97.32ms
step:1515/1700 train_time:147447ms step_avg:97.32ms
step:1516/1700 train_time:147550ms step_avg:97.33ms
step:1517/1700 train_time:147650ms step_avg:97.33ms
step:1518/1700 train_time:147751ms step_avg:97.33ms
step:1519/1700 train_time:147855ms step_avg:97.34ms
step:1520/1700 train_time:147956ms step_avg:97.34ms
step:1521/1700 train_time:148057ms step_avg:97.34ms
step:1522/1700 train_time:148159ms step_avg:97.34ms
step:1523/1700 train_time:148260ms step_avg:97.35ms
step:1524/1700 train_time:148361ms step_avg:97.35ms
step:1525/1700 train_time:148463ms step_avg:97.35ms
step:1526/1700 train_time:148563ms step_avg:97.35ms
step:1527/1700 train_time:148666ms step_avg:97.36ms
step:1528/1700 train_time:148768ms step_avg:97.36ms
step:1529/1700 train_time:148870ms step_avg:97.36ms
step:1530/1700 train_time:148972ms step_avg:97.37ms
step:1531/1700 train_time:149073ms step_avg:97.37ms
step:1532/1700 train_time:149174ms step_avg:97.37ms
step:1533/1700 train_time:149274ms step_avg:97.37ms
step:1534/1700 train_time:149376ms step_avg:97.38ms
step:1535/1700 train_time:149476ms step_avg:97.38ms
step:1536/1700 train_time:149577ms step_avg:97.38ms
step:1537/1700 train_time:149677ms step_avg:97.38ms
step:1538/1700 train_time:149777ms step_avg:97.38ms
step:1539/1700 train_time:149878ms step_avg:97.39ms
step:1540/1700 train_time:149979ms step_avg:97.39ms
step:1541/1700 train_time:150083ms step_avg:97.39ms
step:1542/1700 train_time:150188ms step_avg:97.40ms
step:1543/1700 train_time:150290ms step_avg:97.40ms
step:1544/1700 train_time:150392ms step_avg:97.40ms
step:1545/1700 train_time:150493ms step_avg:97.41ms
step:1546/1700 train_time:150593ms step_avg:97.41ms
step:1547/1700 train_time:150696ms step_avg:97.41ms
step:1548/1700 train_time:150797ms step_avg:97.41ms
step:1549/1700 train_time:150899ms step_avg:97.42ms
step:1550/1700 train_time:151001ms step_avg:97.42ms
step:1551/1700 train_time:151104ms step_avg:97.42ms
step:1552/1700 train_time:151203ms step_avg:97.42ms
step:1553/1700 train_time:151307ms step_avg:97.43ms
step:1554/1700 train_time:151409ms step_avg:97.43ms
step:1555/1700 train_time:151511ms step_avg:97.43ms
step:1556/1700 train_time:151613ms step_avg:97.44ms
step:1557/1700 train_time:151716ms step_avg:97.44ms
step:1558/1700 train_time:151819ms step_avg:97.44ms
step:1559/1700 train_time:151919ms step_avg:97.45ms
step:1560/1700 train_time:152021ms step_avg:97.45ms
step:1561/1700 train_time:152121ms step_avg:97.45ms
step:1562/1700 train_time:152222ms step_avg:97.45ms
step:1563/1700 train_time:152325ms step_avg:97.46ms
step:1564/1700 train_time:152426ms step_avg:97.46ms
step:1565/1700 train_time:152529ms step_avg:97.46ms
step:1566/1700 train_time:152632ms step_avg:97.47ms
step:1567/1700 train_time:152734ms step_avg:97.47ms
step:1568/1700 train_time:152835ms step_avg:97.47ms
step:1569/1700 train_time:152937ms step_avg:97.47ms
step:1570/1700 train_time:153039ms step_avg:97.48ms
step:1571/1700 train_time:153140ms step_avg:97.48ms
step:1572/1700 train_time:153241ms step_avg:97.48ms
step:1573/1700 train_time:153340ms step_avg:97.48ms
step:1574/1700 train_time:153442ms step_avg:97.49ms
step:1575/1700 train_time:153543ms step_avg:97.49ms
step:1576/1700 train_time:153646ms step_avg:97.49ms
step:1577/1700 train_time:153752ms step_avg:97.50ms
step:1578/1700 train_time:153854ms step_avg:97.50ms
step:1579/1700 train_time:153955ms step_avg:97.50ms
step:1580/1700 train_time:154056ms step_avg:97.50ms
step:1581/1700 train_time:154156ms step_avg:97.51ms
step:1582/1700 train_time:154256ms step_avg:97.51ms
step:1583/1700 train_time:154361ms step_avg:97.51ms
step:1584/1700 train_time:154462ms step_avg:97.51ms
step:1585/1700 train_time:154562ms step_avg:97.52ms
step:1586/1700 train_time:154666ms step_avg:97.52ms
step:1587/1700 train_time:154767ms step_avg:97.52ms
step:1588/1700 train_time:154870ms step_avg:97.53ms
step:1589/1700 train_time:154971ms step_avg:97.53ms
step:1590/1700 train_time:155074ms step_avg:97.53ms
step:1591/1700 train_time:155175ms step_avg:97.53ms
step:1592/1700 train_time:155276ms step_avg:97.54ms
step:1593/1700 train_time:155378ms step_avg:97.54ms
step:1594/1700 train_time:155481ms step_avg:97.54ms
step:1595/1700 train_time:155582ms step_avg:97.54ms
step:1596/1700 train_time:155683ms step_avg:97.55ms
step:1597/1700 train_time:155786ms step_avg:97.55ms
step:1598/1700 train_time:155889ms step_avg:97.55ms
step:1599/1700 train_time:155990ms step_avg:97.56ms
step:1600/1700 train_time:156093ms step_avg:97.56ms
step:1601/1700 train_time:156194ms step_avg:97.56ms
step:1602/1700 train_time:156295ms step_avg:97.56ms
step:1603/1700 train_time:156396ms step_avg:97.56ms
step:1604/1700 train_time:156497ms step_avg:97.57ms
step:1605/1700 train_time:156599ms step_avg:97.57ms
step:1606/1700 train_time:156700ms step_avg:97.57ms
step:1607/1700 train_time:156799ms step_avg:97.57ms
step:1608/1700 train_time:156901ms step_avg:97.58ms
step:1609/1700 train_time:157002ms step_avg:97.58ms
step:1610/1700 train_time:157105ms step_avg:97.58ms
step:1611/1700 train_time:157209ms step_avg:97.58ms
step:1612/1700 train_time:157313ms step_avg:97.59ms
step:1613/1700 train_time:157415ms step_avg:97.59ms
step:1614/1700 train_time:157515ms step_avg:97.59ms
step:1615/1700 train_time:157615ms step_avg:97.59ms
step:1616/1700 train_time:157717ms step_avg:97.60ms
step:1617/1700 train_time:157819ms step_avg:97.60ms
step:1618/1700 train_time:157919ms step_avg:97.60ms
step:1619/1700 train_time:158020ms step_avg:97.60ms
step:1620/1700 train_time:158121ms step_avg:97.61ms
step:1621/1700 train_time:158222ms step_avg:97.61ms
step:1622/1700 train_time:158324ms step_avg:97.61ms
step:1623/1700 train_time:158426ms step_avg:97.61ms
step:1624/1700 train_time:158530ms step_avg:97.62ms
step:1625/1700 train_time:158633ms step_avg:97.62ms
step:1625/1700 val_loss:3.2999 train_time:158730ms step_avg:97.68ms
step:1626/1700 train_time:158751ms step_avg:97.63ms
step:1627/1700 train_time:158843ms step_avg:97.63ms
step:1628/1700 train_time:158943ms step_avg:97.63ms
step:1629/1700 train_time:159044ms step_avg:97.63ms
step:1630/1700 train_time:159145ms step_avg:97.64ms
step:1631/1700 train_time:159245ms step_avg:97.64ms
step:1632/1700 train_time:159346ms step_avg:97.64ms
step:1633/1700 train_time:159446ms step_avg:97.64ms
step:1634/1700 train_time:159548ms step_avg:97.64ms
step:1635/1700 train_time:159647ms step_avg:97.64ms
step:1636/1700 train_time:159748ms step_avg:97.65ms
step:1637/1700 train_time:159851ms step_avg:97.65ms
step:1638/1700 train_time:159952ms step_avg:97.65ms
step:1639/1700 train_time:160055ms step_avg:97.65ms
step:1640/1700 train_time:160157ms step_avg:97.66ms
step:1641/1700 train_time:160260ms step_avg:97.66ms
step:1642/1700 train_time:160361ms step_avg:97.66ms
step:1643/1700 train_time:160462ms step_avg:97.66ms
step:1644/1700 train_time:160565ms step_avg:97.67ms
step:1645/1700 train_time:160666ms step_avg:97.67ms
step:1646/1700 train_time:160768ms step_avg:97.67ms
step:1647/1700 train_time:160872ms step_avg:97.68ms
step:1648/1700 train_time:160976ms step_avg:97.68ms
step:1649/1700 train_time:161079ms step_avg:97.68ms
step:1650/1700 train_time:161181ms step_avg:97.69ms
step:1651/1700 train_time:161283ms step_avg:97.69ms
step:1652/1700 train_time:161384ms step_avg:97.69ms
step:1653/1700 train_time:161488ms step_avg:97.69ms
step:1654/1700 train_time:161588ms step_avg:97.70ms
step:1655/1700 train_time:161691ms step_avg:97.70ms
step:1656/1700 train_time:161793ms step_avg:97.70ms
step:1657/1700 train_time:161894ms step_avg:97.70ms
step:1658/1700 train_time:161998ms step_avg:97.71ms
step:1659/1700 train_time:162103ms step_avg:97.71ms
step:1660/1700 train_time:162206ms step_avg:97.71ms
step:1661/1700 train_time:162310ms step_avg:97.72ms
step:1662/1700 train_time:162414ms step_avg:97.72ms
step:1663/1700 train_time:162517ms step_avg:97.73ms
step:1664/1700 train_time:162618ms step_avg:97.73ms
step:1665/1700 train_time:162722ms step_avg:97.73ms
step:1666/1700 train_time:162826ms step_avg:97.73ms
step:1667/1700 train_time:162928ms step_avg:97.74ms
step:1668/1700 train_time:163030ms step_avg:97.74ms
step:1669/1700 train_time:163134ms step_avg:97.74ms
step:1670/1700 train_time:163237ms step_avg:97.75ms
step:1671/1700 train_time:163338ms step_avg:97.75ms
step:1672/1700 train_time:163442ms step_avg:97.75ms
step:1673/1700 train_time:163543ms step_avg:97.75ms
step:1674/1700 train_time:163646ms step_avg:97.76ms
step:1675/1700 train_time:163747ms step_avg:97.76ms
step:1676/1700 train_time:163849ms step_avg:97.76ms
step:1677/1700 train_time:163949ms step_avg:97.76ms
step:1678/1700 train_time:164052ms step_avg:97.77ms
step:1679/1700 train_time:164155ms step_avg:97.77ms
step:1680/1700 train_time:164259ms step_avg:97.77ms
step:1681/1700 train_time:164363ms step_avg:97.78ms
step:1682/1700 train_time:164466ms step_avg:97.78ms
step:1683/1700 train_time:164566ms step_avg:97.78ms
step:1684/1700 train_time:164670ms step_avg:97.78ms
step:1685/1700 train_time:164772ms step_avg:97.79ms
step:1686/1700 train_time:164874ms step_avg:97.79ms
step:1687/1700 train_time:164977ms step_avg:97.79ms
step:1688/1700 train_time:165078ms step_avg:97.80ms
step:1689/1700 train_time:165180ms step_avg:97.80ms
step:1690/1700 train_time:165283ms step_avg:97.80ms
step:1691/1700 train_time:165384ms step_avg:97.80ms
step:1692/1700 train_time:165486ms step_avg:97.80ms
step:1693/1700 train_time:165588ms step_avg:97.81ms
step:1694/1700 train_time:165689ms step_avg:97.81ms
step:1695/1700 train_time:165792ms step_avg:97.81ms
step:1696/1700 train_time:165893ms step_avg:97.81ms
step:1697/1700 train_time:166001ms step_avg:97.82ms
step:1698/1700 train_time:166102ms step_avg:97.82ms
step:1699/1700 train_time:166204ms step_avg:97.82ms
step:1700/1700 train_time:166306ms step_avg:97.83ms
step:1700/1700 val_loss:3.2859 train_time:166404ms step_avg:97.88ms
peak memory allocated: 33583 MiB reserved: 49012 MiB

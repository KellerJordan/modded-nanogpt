import os
import sys
with open(sys.argv[0]) as f:
    code = f.read() # read the code of this file ASAP, for logging
import uuid
import time
import copy
import glob
from dataclasses import dataclass
from functools import lru_cache, partial # Added partial for hook registration
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch
torch.empty(1, device="cuda", requires_grad=True).backward() # prevents a bug on some systems
from torch import Tensor, nn
import torch.nn.functional as F
import torch.distributed as dist
# use of FlexAttention contributed by @KoszarskyB
from torch.nn.attention.flex_attention import BlockMask, flex_attention
#torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng

@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Muon optimizer

@torch.compile
def zeropower_via_newtonschulz5(G: Tensor, steps: int) -> Tensor:
    """
    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a
    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose
    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at
    zero even beyond the point where the iteration no longer converges all the way to one everywhere
    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T
    where S' is diagonal with S_{ii}' ~ Uniform(0.5, 1.5), which turns out not to hurt model
    performance at all relative to UV^T, where USV^T = G is the SVD.
    """
    assert G.ndim >= 2 # batched Muon implementation by @scottjmaddox, and put into practice in the record by @YouJiacheng
    a, b, c = (3.4445, -4.7750,  2.0315)
    X = G
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) + 1e-7)
    # Perform the NS iterations
    for _ in range(steps):
        A = X @ X.mT
        B = b * A + c * A @ A # quintic computation strategy adapted from suggestion by @jxbz, @leloykun, and @YouJiacheng
        X = a * X + B @ X

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

class NorMuon(torch.optim.Optimizer):

    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, beta2=0.95):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, beta2=beta2)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_reduce_futures: list[torch.Future] = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            grad = torch.empty_like(params[-1])
            grad_pad = [param.grad for param in params] + [torch.zeros_like(params[-1])] * world_size
            for base_i in range(0, len(params), world_size):
                if base_i + rank < len(params):
                    grad = params[base_i + rank].grad
                else:
                    grad = torch.zeros_like(grad)
                # This gives strange dynamo warnings
                reduce_scatter_futures.append(dist.reduce_scatter(grad, grad_pad[base_i:base_i + world_size], op=dist.ReduceOp.AVG, async_op=True).get_future())

        idx = 0
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            params_pad = params + [torch.empty_like(params[-1])] * world_size
            momentum = group["momentum"]
            beta2 = group["beta2"]
            for base_i in range(0, len(params), world_size):
                reduce_scatter_futures[idx].wait()
                if base_i + rank < len(params):
                    p = params[base_i + rank]
                    grad = p.grad
                    eff_lr = group["lr"] * max(1, p.size(-2) / p.size(-1)) ** 0.5 * getattr(p, "lr_mul", 1.0)
                    eff_weight_decay = group["lr"] * group["weight_decay"] * getattr(p, "wd_mul", 1.0)
                    state = self.state[p]
                    if len(state) == 0:
                        state["momentum_buffer"] = torch.zeros_like(grad)
                        state["second_momentum_buffer"] = torch.zeros_like(grad[..., 0:1]) if p.size(-2) >= p.size(-1) else torch.zeros_like(grad[0:1, ...])
                    momentum_buffer = state["momentum_buffer"]
                    second_momentum_buffer = state["second_momentum_buffer"]
                    p.mul_(1 - eff_weight_decay)
                    momentum_buffer.lerp_(grad, 1 - momentum)
                    grad = grad.lerp_(momentum_buffer, momentum)
                    v = zeropower_via_newtonschulz5(grad.bfloat16(), 5).float()
                    ###################################
                    vnorm = v.norm(dim=(-2,-1), keepdim=True)
                    v_mean = torch.mean(v * v, dim=-1, keepdim=True) if p.size(-2) >= p.size(-1) else torch.mean(v * v, dim=-2, keepdim=True)
                    second_momentum_buffer.lerp_(v_mean, 1 - beta2)
                    step_size = 1 / second_momentum_buffer.sqrt().add_(1e-10)
                    v.mul_(step_size)
                    vnorm_new = v.norm(dim=(-2,-1), keepdim=True)
                    v.mul_(vnorm / (vnorm_new + 1e-10))
                    ####################################
                    p.add_(other=v, alpha=-eff_lr)
                idx += 1
                all_reduce_futures.append(dist.all_gather(params_pad[base_i:base_i + world_size], params_pad[base_i + rank], async_op=True).get_future())
        torch.futures.collect_all(all_reduce_futures).wait()

class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_reduce_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            grad = torch.empty_like(params[-1])
            for base_i in range(len(params)):
                grad = params[base_i].grad
                rank_size = grad.shape[0] // world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for base in range(len(params)):
                reduce_scatter_futures[idx].wait()
                p = params[base]
                rank_size = p.shape[0] // world_size
                p_slice = p[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(p, "lr_mul", 1.0)
                state = self.state[p]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state['step'] = torch.tensor(0, dtype=torch.int64, device=p.device)
                    state['exp_avg'] = torch.zeros_like(p_slice)
                    state['exp_avg_sq'] = torch.zeros_like(p_slice)
                exp_avg = state['exp_avg']
                exp_avg_sq = state['exp_avg_sq']
                state['step'] += 1
                t = state['step']
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(p, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_reduce_futures.append(dist.all_gather_into_tensor(p, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_reduce_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

class Rotary(nn.Module):
    def __init__(self, dim: int, max_seq_len: int):
        super().__init__()
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=dim//4, dtype=torch.float32)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(dim//4)])
        t = torch.arange(max_seq_len, dtype=torch.float32)
        theta = torch.einsum("i,j -> ij", t, angular_freq)
        self.cos = nn.Buffer(theta.cos(), persistent=False)
        self.sin = nn.Buffer(theta.sin(), persistent=False)

    def forward(self, x_BTHD: Tensor):
        assert self.cos.size(0) >= x_BTHD.size(-3)
        cos, sin = self.cos[None, :x_BTHD.size(-3), None, :], self.sin[None, :x_BTHD.size(-3), None, :]
        x1, x2 = x_BTHD.to(dtype=torch.float32).chunk(2, dim=-1)
        y1 = x1 * cos + x2 * sin
        y2 = x1 * (-sin) + x2 * cos
        return torch.cat((y1, y2), 3).type_as(x_BTHD)

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, num_heads: int, max_seq_len: int, head_dim=128):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        hdim = num_heads * head_dim
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        self.qkv_w = nn.Parameter(torch.empty(3, hdim, dim).uniform_(-bound, bound))
        self.rotary = Rotary(head_dim, max_seq_len)
        self.c_proj = CastedLinear(hdim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977
        # scale the attention logits by given constant, instead of the default head_dim**-0.5, by @leloykun
        # inspired by learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.12

    def forward(self, x: Tensor, ve: Tensor | None, lambdas: Tensor, block_mask: BlockMask):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "Must use batch size = 1 for FlexAttention"
        q, k, v = F.linear(x, self.qkv_w.flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = self.rotary(q), self.rotary(k)
        if ve is not None:
            v = lambdas[0] * v + lambdas[1] * ve.view_as(v) # @KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = lambdas[0] * v
        y = flex_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), block_mask=block_mask, scale=self.attn_scale).transpose(1, 2)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = self.c_proj(y)
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        self.c_fc = CastedLinear(dim, hdim)
        self.c_proj = CastedLinear(hdim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = self.c_fc(x)
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = self.c_proj(x)
        return x

class Block(nn.Module):
    def __init__(self, dim: int, num_heads: int, max_seq_len: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, num_heads, max_seq_len) if layer_idx != 7 else None
        self.mlp = MLP(dim)

    def forward(self, x: Tensor, ve: Tensor | None, x0: Tensor, lambdas: Tensor, sa_lambdas: Tensor, block_mask: BlockMask):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), ve, sa_lambdas, block_mask)
        x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, num_heads, max_seq_len, i) for i in range(num_layers)])
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=True, x_s=(model_dim**0.5)/448, w_s=24/448, grad_s=1/448)
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5) % dist.get_world_size()
        self.scalars = nn.Parameter(torch.cat([
            torch.ones(num_layers), # skip_weights
            *[torch.tensor([1.0, 0.0]) for _ in range(num_layers)], # block lambdas
            *[torch.tensor([0.5, 0.5]) for _ in range(num_layers)], # SA lambdas
            torch.ones(pad),
        ]))
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 27.5
        self.scalars.lr_mul = 5.0

    def create_blockmasks(self, input_seq: Tensor, sliding_window_num_blocks: Tensor):
        BLOCK_SIZE = 128
        docs = (input_seq == 50256).cumsum(0)

        def document_causal(b, h, q_idx, kv_idx):
            causal_mask = q_idx >= kv_idx
            document_mask = docs[q_idx] == docs[kv_idx]
            return causal_mask & document_mask

        def dense_to_ordered(dense_blockmask: Tensor):
            num_blocks = dense_blockmask.sum(dim=-1, dtype=torch.int32)
            indices = dense_blockmask.argsort(dim=-1, descending=False, stable=True).flip(-1).to(torch.int32)
            return num_blocks[None, None].contiguous(), indices[None, None].contiguous()

        # manual block mask creation by @YouJiacheng
        assert len(input_seq) % BLOCK_SIZE == 0
        NUM_BLOCKS = len(input_seq) // BLOCK_SIZE
        block_idx = torch.arange(NUM_BLOCKS, dtype=torch.int32, device="cuda")
        causal_blockmask_any = block_idx[:, None] >= block_idx
        causal_blockmask_all = block_idx[:, None] > block_idx
        docs_low = docs.view(-1, BLOCK_SIZE)[:, 0].contiguous()
        docs_high = docs.view(-1, BLOCK_SIZE)[:, -1].contiguous()
        document_blockmask_any = (docs_low[:, None] <= docs_high) & (docs_high[:, None] >= docs_low)
        document_blockmask_all = (docs_low[:, None] == docs_high) & (docs_high[:, None] == docs_low)
        blockmask_any = causal_blockmask_any & document_blockmask_any
        blockmask_all = causal_blockmask_all & document_blockmask_all
        partial_kv_num_blocks, partial_kv_indices = dense_to_ordered(blockmask_any & ~blockmask_all)
        full_kv_num_blocks, full_kv_indices = dense_to_ordered(blockmask_all)
        def build_bm(window_size_blocks: Tensor) -> BlockMask:
            return BlockMask.from_kv_blocks(
                torch.clamp_max(partial_kv_num_blocks, torch.clamp_min(window_size_blocks - full_kv_num_blocks, 1)),
                partial_kv_indices,
                torch.clamp_max(full_kv_num_blocks, window_size_blocks - 1),
                full_kv_indices,
                BLOCK_SIZE=BLOCK_SIZE,
                mask_mod=document_causal,
            )
        # Long-short SWA block masks by @leloykun & @YouJiacheng, adapated from suggestion by @Grad62304977, following Gemma 2 paper
        return build_bm(sliding_window_num_blocks), build_bm(sliding_window_num_blocks // 2)

    def forward(self, input_seq: Tensor, target_seq: Tensor, sliding_window_num_blocks: Tensor):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        ve = [ve[0], ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        long_bm, short_bm = self.create_blockmasks(input_seq, sliding_window_num_blocks)
        block_masks = [long_bm, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, long_bm, short_bm, short_bm, short_bm, long_bm]
        assert len(block_masks) == len(self.blocks)

        x = x0 = norm(self.embed(input_seq)[None]) # use of norm here by @Grad62304977

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)

        n = len(self.blocks) // 2

        for i in range(len(self.blocks)):
            if i >= n:
                x = x + skip_weights[i - n] * skip_connections.pop()
            x = self.blocks[i](x, ve[i], x0, lambdas[i], sa_lambdas[i], block_masks[i])
            if i < n:
                skip_connections.append(x)

        x = norm(x)
        logits = self.lm_head(x).float()
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / (7.5 * x.size(-1)**0.5))
        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), target_seq, reduction="sum" if self.training else "mean")
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

# find world_size starting indicies, such that each begins with token 50256 and local_batches don't overlap
def find_batch_starts(tokens: Tensor, pos: int, local_batch_size: int, max_batch_span: int):
    boundary_mask = tokens[pos : pos + max_batch_span] == 50256
    boundary_positions = torch.nonzero(boundary_mask, as_tuple=False).squeeze(-1) + pos
    start = boundary_positions[0].item()
    starts = []
    for i in range(1, len(boundary_positions)):
        end = boundary_positions[i].item() 
        if end - start >= local_batch_size:
            starts.append(start) # append start once end pos is confirmed
            if len(starts) == dist.get_world_size():
                return starts, end - pos
            start = end
    assert False # increase max_batch_span if necessary

def distributed_data_generator(filename_pattern: str, batch_size: int, align_to_bos: bool):
    rank = dist.get_rank()
    world_size = dist.get_world_size()
    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    assert batch_size % world_size == 0
    local_batch_size = batch_size // world_size
    file_iter = iter(files) # use itertools.cycle(files) instead if you want to do multi-epoch training
    tokens, pos = _load_data_shard(next(file_iter)), 0
    max_batch_span = 2 * batch_size if align_to_bos else batch_size # provide buffer to handle samples up to length local_batch_size
    while True:
        if pos + max_batch_span + 1 >= len(tokens):
            tokens, pos = _load_data_shard(next(file_iter)), 0
        if align_to_bos:
            batch_starts, batch_span = find_batch_starts(tokens, pos, local_batch_size, max_batch_span)
            start_idx = batch_starts[rank]
        else:
            batch_span = batch_size
            start_idx = pos + rank * local_batch_size
        buf = tokens[start_idx:][:local_batch_size + 1]
        inputs = buf[:-1].to(device="cuda", dtype=torch.int32, non_blocking=True) # no sync on host side;
        targets = buf[1:].to(device="cuda", dtype=torch.int64, non_blocking=True) # H2D in another stream isn't helpful.
        pos += batch_span
        yield inputs, targets

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_seq_len = 48*1024 # FlexAttention sequence length
    val_seq_len = 4*64*1024 # FlexAttention sequence length for validation
    # optimization
    num_iterations = 1700 # number of iterations to run
    cooldown_frac = 0.45 # fraction of training spent cooling down the learning rate
    # evaluation and logging
    val_loss_every = 125 # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint = False
args = Hyperparameters()

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert world_size == 8 # this code is designed for 8xH100
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = uuid.uuid4()
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(vocab_size=50257, num_layers=12, num_heads=6, model_dim=768, max_seq_len=max(args.train_seq_len, args.val_seq_len)).cuda()
for m in model.modules():
    if isinstance(m, nn.Embedding):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(scalar_params + head_params + embed_params, lr=0.008, betas=(0.8, 0.95), eps=1e-10, weight_decay=0.0)
optimizer2 = NorMuon(hidden_matrix_params, lr=0.05, momentum=0.95, weight_decay=0.0, beta2=0.95)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: stable then decay
def get_lr(step: int):
    x = step / args.num_iterations # progress in training
    assert 0 <= x < 1
    if x < 1 - args.cooldown_frac:
        return 1.0
    else:
        w = (1 - x) / args.cooldown_frac
        return w * 1.0 + (1 - w) * 0.1

# attention window size schedule: linearly increase
@lru_cache(1)
def get_window_size_blocks_helper(window_size: int):
    return torch.tensor(window_size // 128, dtype=torch.int32, pin_memory=True).cuda(non_blocking=True)
def get_window_size_blocks(step: int):
    x = step / args.num_iterations # progress in training
    assert 0 <= x <= 1
    # Linearly increase the block-wise sliding window size over training 128 -> 1792
    # increase by @fernbear.bsky.social; block-wise by @YouJiacheng
    window_size = next_multiple_of_n(1728 * x, n=128)
    return get_window_size_blocks_helper(window_size)

model: nn.Module = torch.compile(model, dynamic=False)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 10
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, world_size * args.train_seq_len, align_to_bos=True)
for _ in range(warmup_steps):
    inputs, targets = next(train_loader)
    model(inputs, targets, get_window_size_blocks(1)).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, world_size * args.train_seq_len, align_to_bos=True)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
for step in range(train_steps + 1):
    last_step = (step == train_steps)

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        val_batch_size = world_size * args.val_seq_len
        assert args.val_tokens % val_batch_size == 0
        val_steps = args.val_tokens // val_batch_size
        val_loader = distributed_data_generator(args.val_files, val_batch_size, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets = next(val_loader)
                val_loss += model(inputs, targets, get_window_size_blocks(step))
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    inputs, targets = next(train_loader)
    model(inputs, targets, get_window_size_blocks(step)).backward()
    # set optimization hyperparameters
    for opt in optimizers:
        for group in opt.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)
    for group in optimizer2.param_groups:
        frac = min(step / 300, 1) # momentum warmup for muon
        group["momentum"] = (1 - frac) * 0.85 + frac * 0.95
    # step the optimizers
    for opt in optimizers:
        opt.step()
    # null the gradients
    model.zero_grad(set_to_none=True)
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()
====================================================================================================
Running Python 3.12.7 (main, Oct 11 2025, 17:06:43) [GCC 13.2.0]
Running PyTorch 2.10.0.dev20251011+cu126 compiled for CUDA 12.6
Mon Oct 13 03:36:21 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 565.57.01              Driver Version: 565.57.01      CUDA Version: 12.7     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000001:00:00.0 Off |                    0 |
| N/A   32C    P0            115W /  700W |    5858MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000002:00:00.0 Off |                    0 |
| N/A   33C    P0            115W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000003:00:00.0 Off |                    0 |
| N/A   30C    P0            116W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000008:00:00.0 Off |                    0 |
| N/A   29C    P0            116W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000009:00:00.0 Off |                    0 |
| N/A   28C    P0            112W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   0000000A:00:00.0 Off |                    0 |
| N/A   33C    P0            121W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   0000000B:00:00.0 Off |                    0 |
| N/A   29C    P0            117W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   0000000C:00:00.0 Off |                    0 |
| N/A   31C    P0            115W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
+-----------------------------------------------------------------------------------------+

====================================================================================================
step:0/1700 val_loss:10.8258 train_time:0ms step_avg:0.16ms
step:1/1700 train_time:142ms step_avg:142.47ms
step:2/1700 train_time:166ms step_avg:83.17ms
step:3/1700 train_time:251ms step_avg:83.56ms
step:4/1700 train_time:342ms step_avg:85.61ms
step:5/1700 train_time:435ms step_avg:86.98ms
step:6/1700 train_time:527ms step_avg:87.90ms
step:7/1700 train_time:620ms step_avg:88.51ms
step:8/1700 train_time:712ms step_avg:88.96ms
step:9/1700 train_time:804ms step_avg:89.32ms
step:10/1700 train_time:896ms step_avg:89.58ms
step:11/1700 train_time:988ms step_avg:89.80ms
step:12/1700 train_time:1080ms step_avg:90.04ms
step:13/1700 train_time:1175ms step_avg:90.41ms
step:14/1700 train_time:1270ms step_avg:90.68ms
step:15/1700 train_time:1363ms step_avg:90.85ms
step:16/1700 train_time:1456ms step_avg:90.97ms
step:17/1700 train_time:1548ms step_avg:91.07ms
step:18/1700 train_time:1640ms step_avg:91.12ms
step:19/1700 train_time:1733ms step_avg:91.19ms
step:20/1700 train_time:1826ms step_avg:91.29ms
step:21/1700 train_time:1918ms step_avg:91.31ms
step:22/1700 train_time:2011ms step_avg:91.39ms
step:23/1700 train_time:2104ms step_avg:91.46ms
step:24/1700 train_time:2197ms step_avg:91.55ms
step:25/1700 train_time:2292ms step_avg:91.69ms
step:26/1700 train_time:2385ms step_avg:91.74ms
step:27/1700 train_time:2478ms step_avg:91.77ms
step:28/1700 train_time:2571ms step_avg:91.82ms
step:29/1700 train_time:2664ms step_avg:91.86ms
step:30/1700 train_time:2757ms step_avg:91.89ms
step:31/1700 train_time:2849ms step_avg:91.91ms
step:32/1700 train_time:2942ms step_avg:91.94ms
step:33/1700 train_time:3035ms step_avg:91.97ms
step:34/1700 train_time:3129ms step_avg:92.02ms
step:35/1700 train_time:3222ms step_avg:92.07ms
step:36/1700 train_time:3315ms step_avg:92.09ms
step:37/1700 train_time:3408ms step_avg:92.11ms
step:38/1700 train_time:3500ms step_avg:92.11ms
step:39/1700 train_time:3593ms step_avg:92.12ms
step:40/1700 train_time:3686ms step_avg:92.14ms
step:41/1700 train_time:3778ms step_avg:92.14ms
step:42/1700 train_time:3871ms step_avg:92.17ms
step:43/1700 train_time:3964ms step_avg:92.18ms
step:44/1700 train_time:4057ms step_avg:92.21ms
step:45/1700 train_time:4151ms step_avg:92.24ms
step:46/1700 train_time:4244ms step_avg:92.27ms
step:47/1700 train_time:4338ms step_avg:92.29ms
step:48/1700 train_time:4431ms step_avg:92.31ms
step:49/1700 train_time:4524ms step_avg:92.32ms
step:50/1700 train_time:4616ms step_avg:92.33ms
step:51/1700 train_time:4709ms step_avg:92.34ms
step:52/1700 train_time:4802ms step_avg:92.35ms
step:53/1700 train_time:4895ms step_avg:92.35ms
step:54/1700 train_time:4988ms step_avg:92.36ms
step:55/1700 train_time:5081ms step_avg:92.37ms
step:56/1700 train_time:5174ms step_avg:92.39ms
step:57/1700 train_time:5267ms step_avg:92.40ms
step:58/1700 train_time:5361ms step_avg:92.43ms
step:59/1700 train_time:5454ms step_avg:92.43ms
step:60/1700 train_time:5546ms step_avg:92.43ms
step:61/1700 train_time:5638ms step_avg:92.42ms
step:62/1700 train_time:5730ms step_avg:92.42ms
step:63/1700 train_time:5823ms step_avg:92.43ms
step:64/1700 train_time:5916ms step_avg:92.43ms
step:65/1700 train_time:6009ms step_avg:92.44ms
step:66/1700 train_time:6101ms step_avg:92.45ms
step:67/1700 train_time:6194ms step_avg:92.46ms
step:68/1700 train_time:6288ms step_avg:92.47ms
step:69/1700 train_time:6381ms step_avg:92.47ms
step:70/1700 train_time:6474ms step_avg:92.48ms
step:71/1700 train_time:6566ms step_avg:92.48ms
step:72/1700 train_time:6658ms step_avg:92.48ms
step:73/1700 train_time:6751ms step_avg:92.49ms
step:74/1700 train_time:6843ms step_avg:92.48ms
step:75/1700 train_time:6936ms step_avg:92.49ms
step:76/1700 train_time:7029ms step_avg:92.49ms
step:77/1700 train_time:7122ms step_avg:92.49ms
step:78/1700 train_time:7214ms step_avg:92.49ms
step:79/1700 train_time:7307ms step_avg:92.49ms
step:80/1700 train_time:7400ms step_avg:92.50ms
step:81/1700 train_time:7492ms step_avg:92.50ms
step:82/1700 train_time:7586ms step_avg:92.51ms
step:83/1700 train_time:7678ms step_avg:92.50ms
step:84/1700 train_time:7770ms step_avg:92.51ms
step:85/1700 train_time:7864ms step_avg:92.51ms
step:86/1700 train_time:7956ms step_avg:92.52ms
step:87/1700 train_time:8049ms step_avg:92.51ms
step:88/1700 train_time:8141ms step_avg:92.51ms
step:89/1700 train_time:8233ms step_avg:92.51ms
step:90/1700 train_time:8327ms step_avg:92.52ms
step:91/1700 train_time:8419ms step_avg:92.52ms
step:92/1700 train_time:8512ms step_avg:92.52ms
step:93/1700 train_time:8605ms step_avg:92.53ms
step:94/1700 train_time:8697ms step_avg:92.52ms
step:95/1700 train_time:8790ms step_avg:92.53ms
step:96/1700 train_time:8883ms step_avg:92.53ms
step:97/1700 train_time:8976ms step_avg:92.53ms
step:98/1700 train_time:9069ms step_avg:92.54ms
step:99/1700 train_time:9161ms step_avg:92.54ms
step:100/1700 train_time:9254ms step_avg:92.54ms
step:101/1700 train_time:9347ms step_avg:92.55ms
step:102/1700 train_time:9440ms step_avg:92.55ms
step:103/1700 train_time:9533ms step_avg:92.55ms
step:104/1700 train_time:9625ms step_avg:92.55ms
step:105/1700 train_time:9717ms step_avg:92.55ms
step:106/1700 train_time:9810ms step_avg:92.54ms
step:107/1700 train_time:9903ms step_avg:92.55ms
step:108/1700 train_time:9996ms step_avg:92.55ms
step:109/1700 train_time:10088ms step_avg:92.55ms
step:110/1700 train_time:10181ms step_avg:92.55ms
step:111/1700 train_time:10273ms step_avg:92.55ms
step:112/1700 train_time:10366ms step_avg:92.55ms
step:113/1700 train_time:10458ms step_avg:92.55ms
step:114/1700 train_time:10551ms step_avg:92.55ms
step:115/1700 train_time:10644ms step_avg:92.56ms
step:116/1700 train_time:10736ms step_avg:92.56ms
step:117/1700 train_time:10829ms step_avg:92.56ms
step:118/1700 train_time:10922ms step_avg:92.56ms
step:119/1700 train_time:11015ms step_avg:92.56ms
step:120/1700 train_time:11107ms step_avg:92.56ms
step:121/1700 train_time:11200ms step_avg:92.56ms
step:122/1700 train_time:11293ms step_avg:92.56ms
step:123/1700 train_time:11385ms step_avg:92.56ms
step:124/1700 train_time:11477ms step_avg:92.56ms
step:125/1700 train_time:11571ms step_avg:92.56ms
step:125/1700 val_loss:4.6120 train_time:11647ms step_avg:93.18ms
step:126/1700 train_time:11670ms step_avg:92.62ms
step:127/1700 train_time:11763ms step_avg:92.62ms
step:128/1700 train_time:11863ms step_avg:92.68ms
step:129/1700 train_time:11957ms step_avg:92.69ms
step:130/1700 train_time:12052ms step_avg:92.70ms
step:131/1700 train_time:12144ms step_avg:92.70ms
step:132/1700 train_time:12237ms step_avg:92.70ms
step:133/1700 train_time:12329ms step_avg:92.70ms
step:134/1700 train_time:12422ms step_avg:92.70ms
step:135/1700 train_time:12515ms step_avg:92.70ms
step:136/1700 train_time:12607ms step_avg:92.70ms
step:137/1700 train_time:12700ms step_avg:92.70ms
step:138/1700 train_time:12795ms step_avg:92.72ms
step:139/1700 train_time:12889ms step_avg:92.73ms
step:140/1700 train_time:12983ms step_avg:92.74ms
step:141/1700 train_time:13077ms step_avg:92.74ms
step:142/1700 train_time:13170ms step_avg:92.75ms
step:143/1700 train_time:13263ms step_avg:92.75ms
step:144/1700 train_time:13356ms step_avg:92.75ms
step:145/1700 train_time:13448ms step_avg:92.75ms
step:146/1700 train_time:13540ms step_avg:92.74ms
step:147/1700 train_time:13634ms step_avg:92.75ms
step:148/1700 train_time:13728ms step_avg:92.76ms
step:149/1700 train_time:13822ms step_avg:92.77ms
step:150/1700 train_time:13916ms step_avg:92.78ms
step:151/1700 train_time:14010ms step_avg:92.78ms
step:152/1700 train_time:14103ms step_avg:92.78ms
step:153/1700 train_time:14197ms step_avg:92.79ms
step:154/1700 train_time:14290ms step_avg:92.79ms
step:155/1700 train_time:14383ms step_avg:92.79ms
step:156/1700 train_time:14476ms step_avg:92.79ms
step:157/1700 train_time:14568ms step_avg:92.79ms
step:158/1700 train_time:14661ms step_avg:92.79ms
step:159/1700 train_time:14755ms step_avg:92.80ms
step:160/1700 train_time:14849ms step_avg:92.81ms
step:161/1700 train_time:14942ms step_avg:92.81ms
step:162/1700 train_time:15036ms step_avg:92.82ms
step:163/1700 train_time:15130ms step_avg:92.82ms
step:164/1700 train_time:15223ms step_avg:92.82ms
step:165/1700 train_time:15317ms step_avg:92.83ms
step:166/1700 train_time:15410ms step_avg:92.83ms
step:167/1700 train_time:15502ms step_avg:92.83ms
step:168/1700 train_time:15596ms step_avg:92.83ms
step:169/1700 train_time:15689ms step_avg:92.83ms
step:170/1700 train_time:15783ms step_avg:92.84ms
step:171/1700 train_time:15876ms step_avg:92.84ms
step:172/1700 train_time:15969ms step_avg:92.84ms
step:173/1700 train_time:16063ms step_avg:92.85ms
step:174/1700 train_time:16156ms step_avg:92.85ms
step:175/1700 train_time:16250ms step_avg:92.86ms
step:176/1700 train_time:16343ms step_avg:92.86ms
step:177/1700 train_time:16436ms step_avg:92.86ms
step:178/1700 train_time:16529ms step_avg:92.86ms
step:179/1700 train_time:16622ms step_avg:92.86ms
step:180/1700 train_time:16715ms step_avg:92.86ms
step:181/1700 train_time:16808ms step_avg:92.86ms
step:182/1700 train_time:16901ms step_avg:92.86ms
step:183/1700 train_time:16995ms step_avg:92.87ms
step:184/1700 train_time:17088ms step_avg:92.87ms
step:185/1700 train_time:17182ms step_avg:92.88ms
step:186/1700 train_time:17276ms step_avg:92.88ms
step:187/1700 train_time:17369ms step_avg:92.88ms
step:188/1700 train_time:17462ms step_avg:92.88ms
step:189/1700 train_time:17555ms step_avg:92.88ms
step:190/1700 train_time:17648ms step_avg:92.89ms
step:191/1700 train_time:17741ms step_avg:92.89ms
step:192/1700 train_time:17835ms step_avg:92.89ms
step:193/1700 train_time:17928ms step_avg:92.89ms
step:194/1700 train_time:18021ms step_avg:92.89ms
step:195/1700 train_time:18115ms step_avg:92.90ms
step:196/1700 train_time:18208ms step_avg:92.90ms
step:197/1700 train_time:18301ms step_avg:92.90ms
step:198/1700 train_time:18395ms step_avg:92.90ms
step:199/1700 train_time:18488ms step_avg:92.91ms
step:200/1700 train_time:18581ms step_avg:92.91ms
step:201/1700 train_time:18675ms step_avg:92.91ms
step:202/1700 train_time:18768ms step_avg:92.91ms
step:203/1700 train_time:18861ms step_avg:92.91ms
step:204/1700 train_time:18955ms step_avg:92.92ms
step:205/1700 train_time:19049ms step_avg:92.92ms
step:206/1700 train_time:19142ms step_avg:92.92ms
step:207/1700 train_time:19235ms step_avg:92.92ms
step:208/1700 train_time:19329ms step_avg:92.93ms
step:209/1700 train_time:19422ms step_avg:92.93ms
step:210/1700 train_time:19515ms step_avg:92.93ms
step:211/1700 train_time:19609ms step_avg:92.93ms
step:212/1700 train_time:19702ms step_avg:92.93ms
step:213/1700 train_time:19795ms step_avg:92.93ms
step:214/1700 train_time:19889ms step_avg:92.94ms
step:215/1700 train_time:19981ms step_avg:92.94ms
step:216/1700 train_time:20075ms step_avg:92.94ms
step:217/1700 train_time:20169ms step_avg:92.94ms
step:218/1700 train_time:20263ms step_avg:92.95ms
step:219/1700 train_time:20357ms step_avg:92.95ms
step:220/1700 train_time:20450ms step_avg:92.95ms
step:221/1700 train_time:20542ms step_avg:92.95ms
step:222/1700 train_time:20637ms step_avg:92.96ms
step:223/1700 train_time:20730ms step_avg:92.96ms
step:224/1700 train_time:20823ms step_avg:92.96ms
step:225/1700 train_time:20917ms step_avg:92.96ms
step:226/1700 train_time:21010ms step_avg:92.96ms
step:227/1700 train_time:21103ms step_avg:92.97ms
step:228/1700 train_time:21197ms step_avg:92.97ms
step:229/1700 train_time:21290ms step_avg:92.97ms
step:230/1700 train_time:21383ms step_avg:92.97ms
step:231/1700 train_time:21477ms step_avg:92.97ms
step:232/1700 train_time:21570ms step_avg:92.98ms
step:233/1700 train_time:21664ms step_avg:92.98ms
step:234/1700 train_time:21757ms step_avg:92.98ms
step:235/1700 train_time:21850ms step_avg:92.98ms
step:236/1700 train_time:21944ms step_avg:92.98ms
step:237/1700 train_time:22037ms step_avg:92.98ms
step:238/1700 train_time:22130ms step_avg:92.99ms
step:239/1700 train_time:22223ms step_avg:92.98ms
step:240/1700 train_time:22317ms step_avg:92.99ms
step:241/1700 train_time:22410ms step_avg:92.99ms
step:242/1700 train_time:22503ms step_avg:92.99ms
step:243/1700 train_time:22597ms step_avg:92.99ms
step:244/1700 train_time:22690ms step_avg:92.99ms
step:245/1700 train_time:22784ms step_avg:92.99ms
step:246/1700 train_time:22877ms step_avg:93.00ms
step:247/1700 train_time:22971ms step_avg:93.00ms
step:248/1700 train_time:23064ms step_avg:93.00ms
step:249/1700 train_time:23157ms step_avg:93.00ms
step:250/1700 train_time:23251ms step_avg:93.00ms
step:250/1700 val_loss:4.0746 train_time:23327ms step_avg:93.31ms
step:251/1700 train_time:23351ms step_avg:93.03ms
step:252/1700 train_time:23443ms step_avg:93.03ms
step:253/1700 train_time:23538ms step_avg:93.04ms
step:254/1700 train_time:23633ms step_avg:93.04ms
step:255/1700 train_time:23726ms step_avg:93.04ms
step:256/1700 train_time:23818ms step_avg:93.04ms
step:257/1700 train_time:23911ms step_avg:93.04ms
step:258/1700 train_time:24004ms step_avg:93.04ms
step:259/1700 train_time:24098ms step_avg:93.04ms
step:260/1700 train_time:24190ms step_avg:93.04ms
step:261/1700 train_time:24284ms step_avg:93.04ms
step:262/1700 train_time:24378ms step_avg:93.05ms
step:263/1700 train_time:24473ms step_avg:93.05ms
step:264/1700 train_time:24568ms step_avg:93.06ms
step:265/1700 train_time:24662ms step_avg:93.06ms
step:266/1700 train_time:24756ms step_avg:93.07ms
step:267/1700 train_time:24849ms step_avg:93.07ms
step:268/1700 train_time:24943ms step_avg:93.07ms
step:269/1700 train_time:25037ms step_avg:93.07ms
step:270/1700 train_time:25130ms step_avg:93.07ms
step:271/1700 train_time:25223ms step_avg:93.07ms
step:272/1700 train_time:25317ms step_avg:93.08ms
step:273/1700 train_time:25411ms step_avg:93.08ms
step:274/1700 train_time:25505ms step_avg:93.08ms
step:275/1700 train_time:25600ms step_avg:93.09ms
step:276/1700 train_time:25694ms step_avg:93.09ms
step:277/1700 train_time:25787ms step_avg:93.09ms
step:278/1700 train_time:25881ms step_avg:93.10ms
step:279/1700 train_time:25974ms step_avg:93.10ms
step:280/1700 train_time:26067ms step_avg:93.10ms
step:281/1700 train_time:26161ms step_avg:93.10ms
step:282/1700 train_time:26255ms step_avg:93.10ms
step:283/1700 train_time:26348ms step_avg:93.10ms
step:284/1700 train_time:26442ms step_avg:93.11ms
step:285/1700 train_time:26536ms step_avg:93.11ms
step:286/1700 train_time:26631ms step_avg:93.11ms
step:287/1700 train_time:26724ms step_avg:93.12ms
step:288/1700 train_time:26819ms step_avg:93.12ms
step:289/1700 train_time:26912ms step_avg:93.12ms
step:290/1700 train_time:27005ms step_avg:93.12ms
step:291/1700 train_time:27099ms step_avg:93.12ms
step:292/1700 train_time:27192ms step_avg:93.12ms
step:293/1700 train_time:27286ms step_avg:93.13ms
step:294/1700 train_time:27380ms step_avg:93.13ms
step:295/1700 train_time:27473ms step_avg:93.13ms
step:296/1700 train_time:27567ms step_avg:93.13ms
step:297/1700 train_time:27660ms step_avg:93.13ms
step:298/1700 train_time:27754ms step_avg:93.14ms
step:299/1700 train_time:27848ms step_avg:93.14ms
step:300/1700 train_time:27942ms step_avg:93.14ms
step:301/1700 train_time:28037ms step_avg:93.14ms
step:302/1700 train_time:28130ms step_avg:93.15ms
step:303/1700 train_time:28224ms step_avg:93.15ms
step:304/1700 train_time:28318ms step_avg:93.15ms
step:305/1700 train_time:28412ms step_avg:93.15ms
step:306/1700 train_time:28505ms step_avg:93.15ms
step:307/1700 train_time:28599ms step_avg:93.16ms
step:308/1700 train_time:28693ms step_avg:93.16ms
step:309/1700 train_time:28787ms step_avg:93.16ms
step:310/1700 train_time:28880ms step_avg:93.16ms
step:311/1700 train_time:28974ms step_avg:93.16ms
step:312/1700 train_time:29067ms step_avg:93.16ms
step:313/1700 train_time:29161ms step_avg:93.17ms
step:314/1700 train_time:29254ms step_avg:93.17ms
step:315/1700 train_time:29347ms step_avg:93.17ms
step:316/1700 train_time:29440ms step_avg:93.17ms
step:317/1700 train_time:29534ms step_avg:93.17ms
step:318/1700 train_time:29628ms step_avg:93.17ms
step:319/1700 train_time:29722ms step_avg:93.17ms
step:320/1700 train_time:29816ms step_avg:93.17ms
step:321/1700 train_time:29910ms step_avg:93.18ms
step:322/1700 train_time:30004ms step_avg:93.18ms
step:323/1700 train_time:30097ms step_avg:93.18ms
step:324/1700 train_time:30191ms step_avg:93.18ms
step:325/1700 train_time:30284ms step_avg:93.18ms
step:326/1700 train_time:30378ms step_avg:93.18ms
step:327/1700 train_time:30472ms step_avg:93.19ms
step:328/1700 train_time:30565ms step_avg:93.19ms
step:329/1700 train_time:30659ms step_avg:93.19ms
step:330/1700 train_time:30752ms step_avg:93.19ms
step:331/1700 train_time:30846ms step_avg:93.19ms
step:332/1700 train_time:30940ms step_avg:93.19ms
step:333/1700 train_time:31034ms step_avg:93.20ms
step:334/1700 train_time:31127ms step_avg:93.20ms
step:335/1700 train_time:31221ms step_avg:93.20ms
step:336/1700 train_time:31315ms step_avg:93.20ms
step:337/1700 train_time:31409ms step_avg:93.20ms
step:338/1700 train_time:31502ms step_avg:93.20ms
step:339/1700 train_time:31597ms step_avg:93.21ms
step:340/1700 train_time:31691ms step_avg:93.21ms
step:341/1700 train_time:31784ms step_avg:93.21ms
step:342/1700 train_time:31878ms step_avg:93.21ms
step:343/1700 train_time:31972ms step_avg:93.21ms
step:344/1700 train_time:32065ms step_avg:93.21ms
step:345/1700 train_time:32159ms step_avg:93.21ms
step:346/1700 train_time:32252ms step_avg:93.21ms
step:347/1700 train_time:32346ms step_avg:93.22ms
step:348/1700 train_time:32440ms step_avg:93.22ms
step:349/1700 train_time:32534ms step_avg:93.22ms
step:350/1700 train_time:32627ms step_avg:93.22ms
step:351/1700 train_time:32720ms step_avg:93.22ms
step:352/1700 train_time:32814ms step_avg:93.22ms
step:353/1700 train_time:32908ms step_avg:93.22ms
step:354/1700 train_time:33002ms step_avg:93.23ms
step:355/1700 train_time:33096ms step_avg:93.23ms
step:356/1700 train_time:33189ms step_avg:93.23ms
step:357/1700 train_time:33282ms step_avg:93.23ms
step:358/1700 train_time:33376ms step_avg:93.23ms
step:359/1700 train_time:33470ms step_avg:93.23ms
step:360/1700 train_time:33563ms step_avg:93.23ms
step:361/1700 train_time:33658ms step_avg:93.24ms
step:362/1700 train_time:33752ms step_avg:93.24ms
step:363/1700 train_time:33845ms step_avg:93.24ms
step:364/1700 train_time:33939ms step_avg:93.24ms
step:365/1700 train_time:34033ms step_avg:93.24ms
step:366/1700 train_time:34127ms step_avg:93.24ms
step:367/1700 train_time:34221ms step_avg:93.24ms
step:368/1700 train_time:34314ms step_avg:93.25ms
step:369/1700 train_time:34408ms step_avg:93.25ms
step:370/1700 train_time:34502ms step_avg:93.25ms
step:371/1700 train_time:34596ms step_avg:93.25ms
step:372/1700 train_time:34689ms step_avg:93.25ms
step:373/1700 train_time:34783ms step_avg:93.25ms
step:374/1700 train_time:34877ms step_avg:93.25ms
step:375/1700 train_time:34970ms step_avg:93.25ms
step:375/1700 val_loss:3.8787 train_time:35047ms step_avg:93.46ms
step:376/1700 train_time:35070ms step_avg:93.27ms
step:377/1700 train_time:35163ms step_avg:93.27ms
step:378/1700 train_time:35258ms step_avg:93.28ms
step:379/1700 train_time:35354ms step_avg:93.28ms
step:380/1700 train_time:35449ms step_avg:93.29ms
step:381/1700 train_time:35544ms step_avg:93.29ms
step:382/1700 train_time:35638ms step_avg:93.29ms
step:383/1700 train_time:35733ms step_avg:93.30ms
step:384/1700 train_time:35828ms step_avg:93.30ms
step:385/1700 train_time:35923ms step_avg:93.31ms
step:386/1700 train_time:36019ms step_avg:93.31ms
step:387/1700 train_time:36116ms step_avg:93.32ms
step:388/1700 train_time:36212ms step_avg:93.33ms
step:389/1700 train_time:36308ms step_avg:93.34ms
step:390/1700 train_time:36404ms step_avg:93.34ms
step:391/1700 train_time:36499ms step_avg:93.35ms
step:392/1700 train_time:36595ms step_avg:93.35ms
step:393/1700 train_time:36690ms step_avg:93.36ms
step:394/1700 train_time:36784ms step_avg:93.36ms
step:395/1700 train_time:36879ms step_avg:93.37ms
step:396/1700 train_time:36975ms step_avg:93.37ms
step:397/1700 train_time:37072ms step_avg:93.38ms
step:398/1700 train_time:37168ms step_avg:93.39ms
step:399/1700 train_time:37265ms step_avg:93.40ms
step:400/1700 train_time:37360ms step_avg:93.40ms
step:401/1700 train_time:37456ms step_avg:93.41ms
step:402/1700 train_time:37551ms step_avg:93.41ms
step:403/1700 train_time:37646ms step_avg:93.42ms
step:404/1700 train_time:37741ms step_avg:93.42ms
step:405/1700 train_time:37837ms step_avg:93.42ms
step:406/1700 train_time:37932ms step_avg:93.43ms
step:407/1700 train_time:38027ms step_avg:93.43ms
step:408/1700 train_time:38123ms step_avg:93.44ms
step:409/1700 train_time:38220ms step_avg:93.45ms
step:410/1700 train_time:38316ms step_avg:93.45ms
step:411/1700 train_time:38411ms step_avg:93.46ms
step:412/1700 train_time:38506ms step_avg:93.46ms
step:413/1700 train_time:38601ms step_avg:93.47ms
step:414/1700 train_time:38696ms step_avg:93.47ms
step:415/1700 train_time:38792ms step_avg:93.47ms
step:416/1700 train_time:38887ms step_avg:93.48ms
step:417/1700 train_time:38982ms step_avg:93.48ms
step:418/1700 train_time:39078ms step_avg:93.49ms
step:419/1700 train_time:39173ms step_avg:93.49ms
step:420/1700 train_time:39270ms step_avg:93.50ms
step:421/1700 train_time:39365ms step_avg:93.50ms
step:422/1700 train_time:39460ms step_avg:93.51ms
step:423/1700 train_time:39556ms step_avg:93.51ms
step:424/1700 train_time:39652ms step_avg:93.52ms
step:425/1700 train_time:39747ms step_avg:93.52ms
step:426/1700 train_time:39842ms step_avg:93.53ms
step:427/1700 train_time:39938ms step_avg:93.53ms
step:428/1700 train_time:40033ms step_avg:93.54ms
step:429/1700 train_time:40130ms step_avg:93.54ms
step:430/1700 train_time:40225ms step_avg:93.55ms
step:431/1700 train_time:40321ms step_avg:93.55ms
step:432/1700 train_time:40417ms step_avg:93.56ms
step:433/1700 train_time:40513ms step_avg:93.56ms
step:434/1700 train_time:40609ms step_avg:93.57ms
step:435/1700 train_time:40704ms step_avg:93.57ms
step:436/1700 train_time:40799ms step_avg:93.58ms
step:437/1700 train_time:40895ms step_avg:93.58ms
step:438/1700 train_time:40991ms step_avg:93.59ms
step:439/1700 train_time:41086ms step_avg:93.59ms
step:440/1700 train_time:41181ms step_avg:93.59ms
step:441/1700 train_time:41277ms step_avg:93.60ms
step:442/1700 train_time:41373ms step_avg:93.60ms
step:443/1700 train_time:41468ms step_avg:93.61ms
step:444/1700 train_time:41564ms step_avg:93.61ms
step:445/1700 train_time:41659ms step_avg:93.62ms
step:446/1700 train_time:41755ms step_avg:93.62ms
step:447/1700 train_time:41851ms step_avg:93.63ms
step:448/1700 train_time:41946ms step_avg:93.63ms
step:449/1700 train_time:42041ms step_avg:93.63ms
step:450/1700 train_time:42137ms step_avg:93.64ms
step:451/1700 train_time:42233ms step_avg:93.64ms
step:452/1700 train_time:42330ms step_avg:93.65ms
step:453/1700 train_time:42425ms step_avg:93.65ms
step:454/1700 train_time:42521ms step_avg:93.66ms
step:455/1700 train_time:42617ms step_avg:93.66ms
step:456/1700 train_time:42713ms step_avg:93.67ms
step:457/1700 train_time:42808ms step_avg:93.67ms
step:458/1700 train_time:42903ms step_avg:93.68ms
step:459/1700 train_time:42999ms step_avg:93.68ms
step:460/1700 train_time:43095ms step_avg:93.68ms
step:461/1700 train_time:43191ms step_avg:93.69ms
step:462/1700 train_time:43286ms step_avg:93.69ms
step:463/1700 train_time:43382ms step_avg:93.70ms
step:464/1700 train_time:43477ms step_avg:93.70ms
step:465/1700 train_time:43572ms step_avg:93.70ms
step:466/1700 train_time:43667ms step_avg:93.71ms
step:467/1700 train_time:43762ms step_avg:93.71ms
step:468/1700 train_time:43859ms step_avg:93.71ms
step:469/1700 train_time:43955ms step_avg:93.72ms
step:470/1700 train_time:44051ms step_avg:93.72ms
step:471/1700 train_time:44145ms step_avg:93.73ms
step:472/1700 train_time:44241ms step_avg:93.73ms
step:473/1700 train_time:44337ms step_avg:93.74ms
step:474/1700 train_time:44433ms step_avg:93.74ms
step:475/1700 train_time:44529ms step_avg:93.75ms
step:476/1700 train_time:44624ms step_avg:93.75ms
step:477/1700 train_time:44719ms step_avg:93.75ms
step:478/1700 train_time:44815ms step_avg:93.75ms
step:479/1700 train_time:44911ms step_avg:93.76ms
step:480/1700 train_time:45006ms step_avg:93.76ms
step:481/1700 train_time:45102ms step_avg:93.77ms
step:482/1700 train_time:45198ms step_avg:93.77ms
step:483/1700 train_time:45294ms step_avg:93.78ms
step:484/1700 train_time:45389ms step_avg:93.78ms
step:485/1700 train_time:45485ms step_avg:93.78ms
step:486/1700 train_time:45580ms step_avg:93.79ms
step:487/1700 train_time:45675ms step_avg:93.79ms
step:488/1700 train_time:45771ms step_avg:93.79ms
step:489/1700 train_time:45866ms step_avg:93.80ms
step:490/1700 train_time:45961ms step_avg:93.80ms
step:491/1700 train_time:46057ms step_avg:93.80ms
step:492/1700 train_time:46152ms step_avg:93.81ms
step:493/1700 train_time:46248ms step_avg:93.81ms
step:494/1700 train_time:46344ms step_avg:93.81ms
step:495/1700 train_time:46439ms step_avg:93.82ms
step:496/1700 train_time:46535ms step_avg:93.82ms
step:497/1700 train_time:46631ms step_avg:93.83ms
step:498/1700 train_time:46727ms step_avg:93.83ms
step:499/1700 train_time:46822ms step_avg:93.83ms
step:500/1700 train_time:46918ms step_avg:93.84ms
step:500/1700 val_loss:3.7318 train_time:46996ms step_avg:93.99ms
step:501/1700 train_time:47021ms step_avg:93.85ms
step:502/1700 train_time:47115ms step_avg:93.85ms
step:503/1700 train_time:47211ms step_avg:93.86ms
step:504/1700 train_time:47308ms step_avg:93.86ms
step:505/1700 train_time:47403ms step_avg:93.87ms
step:506/1700 train_time:47499ms step_avg:93.87ms
step:507/1700 train_time:47594ms step_avg:93.87ms
step:508/1700 train_time:47689ms step_avg:93.88ms
step:509/1700 train_time:47784ms step_avg:93.88ms
step:510/1700 train_time:47879ms step_avg:93.88ms
step:511/1700 train_time:47976ms step_avg:93.89ms
step:512/1700 train_time:48072ms step_avg:93.89ms
step:513/1700 train_time:48169ms step_avg:93.90ms
step:514/1700 train_time:48265ms step_avg:93.90ms
step:515/1700 train_time:48361ms step_avg:93.90ms
step:516/1700 train_time:48457ms step_avg:93.91ms
step:517/1700 train_time:48553ms step_avg:93.91ms
step:518/1700 train_time:48648ms step_avg:93.92ms
step:519/1700 train_time:48744ms step_avg:93.92ms
step:520/1700 train_time:48839ms step_avg:93.92ms
step:521/1700 train_time:48935ms step_avg:93.93ms
step:522/1700 train_time:49031ms step_avg:93.93ms
step:523/1700 train_time:49128ms step_avg:93.93ms
step:524/1700 train_time:49224ms step_avg:93.94ms
step:525/1700 train_time:49321ms step_avg:93.94ms
step:526/1700 train_time:49417ms step_avg:93.95ms
step:527/1700 train_time:49512ms step_avg:93.95ms
step:528/1700 train_time:49607ms step_avg:93.95ms
step:529/1700 train_time:49703ms step_avg:93.96ms
step:530/1700 train_time:49798ms step_avg:93.96ms
step:531/1700 train_time:49894ms step_avg:93.96ms
step:532/1700 train_time:49990ms step_avg:93.97ms
step:533/1700 train_time:50086ms step_avg:93.97ms
step:534/1700 train_time:50182ms step_avg:93.97ms
step:535/1700 train_time:50278ms step_avg:93.98ms
step:536/1700 train_time:50374ms step_avg:93.98ms
step:537/1700 train_time:50469ms step_avg:93.98ms
step:538/1700 train_time:50566ms step_avg:93.99ms
step:539/1700 train_time:50661ms step_avg:93.99ms
step:540/1700 train_time:50757ms step_avg:93.99ms
step:541/1700 train_time:50852ms step_avg:94.00ms
step:542/1700 train_time:50948ms step_avg:94.00ms
step:543/1700 train_time:51043ms step_avg:94.00ms
step:544/1700 train_time:51139ms step_avg:94.01ms
step:545/1700 train_time:51235ms step_avg:94.01ms
step:546/1700 train_time:51331ms step_avg:94.01ms
step:547/1700 train_time:51427ms step_avg:94.02ms
step:548/1700 train_time:51523ms step_avg:94.02ms
step:549/1700 train_time:51618ms step_avg:94.02ms
step:550/1700 train_time:51713ms step_avg:94.02ms
step:551/1700 train_time:51809ms step_avg:94.03ms
step:552/1700 train_time:51905ms step_avg:94.03ms
step:553/1700 train_time:52000ms step_avg:94.03ms
step:554/1700 train_time:52096ms step_avg:94.04ms
step:555/1700 train_time:52192ms step_avg:94.04ms
step:556/1700 train_time:52288ms step_avg:94.04ms
step:557/1700 train_time:52384ms step_avg:94.05ms
step:558/1700 train_time:52480ms step_avg:94.05ms
step:559/1700 train_time:52575ms step_avg:94.05ms
step:560/1700 train_time:52670ms step_avg:94.05ms
step:561/1700 train_time:52766ms step_avg:94.06ms
step:562/1700 train_time:52861ms step_avg:94.06ms
step:563/1700 train_time:52957ms step_avg:94.06ms
step:564/1700 train_time:53053ms step_avg:94.07ms
step:565/1700 train_time:53148ms step_avg:94.07ms
step:566/1700 train_time:53245ms step_avg:94.07ms
step:567/1700 train_time:53340ms step_avg:94.07ms
step:568/1700 train_time:53437ms step_avg:94.08ms
step:569/1700 train_time:53533ms step_avg:94.08ms
step:570/1700 train_time:53629ms step_avg:94.09ms
step:571/1700 train_time:53725ms step_avg:94.09ms
step:572/1700 train_time:53821ms step_avg:94.09ms
step:573/1700 train_time:53917ms step_avg:94.10ms
step:574/1700 train_time:54012ms step_avg:94.10ms
step:575/1700 train_time:54107ms step_avg:94.10ms
step:576/1700 train_time:54204ms step_avg:94.10ms
step:577/1700 train_time:54300ms step_avg:94.11ms
step:578/1700 train_time:54396ms step_avg:94.11ms
step:579/1700 train_time:54492ms step_avg:94.11ms
step:580/1700 train_time:54587ms step_avg:94.12ms
step:581/1700 train_time:54683ms step_avg:94.12ms
step:582/1700 train_time:54779ms step_avg:94.12ms
step:583/1700 train_time:54874ms step_avg:94.12ms
step:584/1700 train_time:54969ms step_avg:94.13ms
step:585/1700 train_time:55065ms step_avg:94.13ms
step:586/1700 train_time:55160ms step_avg:94.13ms
step:587/1700 train_time:55257ms step_avg:94.13ms
step:588/1700 train_time:55352ms step_avg:94.14ms
step:589/1700 train_time:55448ms step_avg:94.14ms
step:590/1700 train_time:55543ms step_avg:94.14ms
step:591/1700 train_time:55639ms step_avg:94.14ms
step:592/1700 train_time:55736ms step_avg:94.15ms
step:593/1700 train_time:55831ms step_avg:94.15ms
step:594/1700 train_time:55927ms step_avg:94.15ms
step:595/1700 train_time:56023ms step_avg:94.16ms
step:596/1700 train_time:56119ms step_avg:94.16ms
step:597/1700 train_time:56214ms step_avg:94.16ms
step:598/1700 train_time:56310ms step_avg:94.16ms
step:599/1700 train_time:56406ms step_avg:94.17ms
step:600/1700 train_time:56502ms step_avg:94.17ms
step:601/1700 train_time:56598ms step_avg:94.17ms
step:602/1700 train_time:56693ms step_avg:94.18ms
step:603/1700 train_time:56789ms step_avg:94.18ms
step:604/1700 train_time:56885ms step_avg:94.18ms
step:605/1700 train_time:56981ms step_avg:94.18ms
step:606/1700 train_time:57077ms step_avg:94.19ms
step:607/1700 train_time:57173ms step_avg:94.19ms
step:608/1700 train_time:57268ms step_avg:94.19ms
step:609/1700 train_time:57364ms step_avg:94.19ms
step:610/1700 train_time:57460ms step_avg:94.20ms
step:611/1700 train_time:57555ms step_avg:94.20ms
step:612/1700 train_time:57651ms step_avg:94.20ms
step:613/1700 train_time:57746ms step_avg:94.20ms
step:614/1700 train_time:57842ms step_avg:94.20ms
step:615/1700 train_time:57938ms step_avg:94.21ms
step:616/1700 train_time:58034ms step_avg:94.21ms
step:617/1700 train_time:58130ms step_avg:94.21ms
step:618/1700 train_time:58226ms step_avg:94.22ms
step:619/1700 train_time:58322ms step_avg:94.22ms
step:620/1700 train_time:58418ms step_avg:94.22ms
step:621/1700 train_time:58514ms step_avg:94.22ms
step:622/1700 train_time:58610ms step_avg:94.23ms
step:623/1700 train_time:58705ms step_avg:94.23ms
step:624/1700 train_time:58801ms step_avg:94.23ms
step:625/1700 train_time:58896ms step_avg:94.23ms
step:625/1700 val_loss:3.6481 train_time:58975ms step_avg:94.36ms
step:626/1700 train_time:59000ms step_avg:94.25ms
step:627/1700 train_time:59094ms step_avg:94.25ms
step:628/1700 train_time:59192ms step_avg:94.26ms
step:629/1700 train_time:59288ms step_avg:94.26ms
step:630/1700 train_time:59384ms step_avg:94.26ms
step:631/1700 train_time:59480ms step_avg:94.26ms
step:632/1700 train_time:59576ms step_avg:94.27ms
step:633/1700 train_time:59673ms step_avg:94.27ms
step:634/1700 train_time:59770ms step_avg:94.28ms
step:635/1700 train_time:59868ms step_avg:94.28ms
step:636/1700 train_time:59965ms step_avg:94.29ms
step:637/1700 train_time:60064ms step_avg:94.29ms
step:638/1700 train_time:60163ms step_avg:94.30ms
step:639/1700 train_time:60262ms step_avg:94.31ms
step:640/1700 train_time:60359ms step_avg:94.31ms
step:641/1700 train_time:60455ms step_avg:94.31ms
step:642/1700 train_time:60552ms step_avg:94.32ms
step:643/1700 train_time:60649ms step_avg:94.32ms
step:644/1700 train_time:60746ms step_avg:94.33ms
step:645/1700 train_time:60842ms step_avg:94.33ms
step:646/1700 train_time:60939ms step_avg:94.33ms
step:647/1700 train_time:61036ms step_avg:94.34ms
step:648/1700 train_time:61134ms step_avg:94.34ms
step:649/1700 train_time:61232ms step_avg:94.35ms
step:650/1700 train_time:61329ms step_avg:94.35ms
step:651/1700 train_time:61428ms step_avg:94.36ms
step:652/1700 train_time:61524ms step_avg:94.36ms
step:653/1700 train_time:61622ms step_avg:94.37ms
step:654/1700 train_time:61719ms step_avg:94.37ms
step:655/1700 train_time:61815ms step_avg:94.37ms
step:656/1700 train_time:61912ms step_avg:94.38ms
step:657/1700 train_time:62009ms step_avg:94.38ms
step:658/1700 train_time:62108ms step_avg:94.39ms
step:659/1700 train_time:62206ms step_avg:94.39ms
step:660/1700 train_time:62303ms step_avg:94.40ms
step:661/1700 train_time:62402ms step_avg:94.41ms
step:662/1700 train_time:62499ms step_avg:94.41ms
step:663/1700 train_time:62596ms step_avg:94.41ms
step:664/1700 train_time:62693ms step_avg:94.42ms
step:665/1700 train_time:62790ms step_avg:94.42ms
step:666/1700 train_time:62888ms step_avg:94.43ms
step:667/1700 train_time:62984ms step_avg:94.43ms
step:668/1700 train_time:63081ms step_avg:94.43ms
step:669/1700 train_time:63179ms step_avg:94.44ms
step:670/1700 train_time:63276ms step_avg:94.44ms
step:671/1700 train_time:63373ms step_avg:94.45ms
step:672/1700 train_time:63471ms step_avg:94.45ms
step:673/1700 train_time:63569ms step_avg:94.46ms
step:674/1700 train_time:63667ms step_avg:94.46ms
step:675/1700 train_time:63764ms step_avg:94.47ms
step:676/1700 train_time:63862ms step_avg:94.47ms
step:677/1700 train_time:63959ms step_avg:94.47ms
step:678/1700 train_time:64056ms step_avg:94.48ms
step:679/1700 train_time:64154ms step_avg:94.48ms
step:680/1700 train_time:64251ms step_avg:94.49ms
step:681/1700 train_time:64348ms step_avg:94.49ms
step:682/1700 train_time:64446ms step_avg:94.50ms
step:683/1700 train_time:64544ms step_avg:94.50ms
step:684/1700 train_time:64641ms step_avg:94.50ms
step:685/1700 train_time:64740ms step_avg:94.51ms
step:686/1700 train_time:64836ms step_avg:94.51ms
step:687/1700 train_time:64933ms step_avg:94.52ms
step:688/1700 train_time:65031ms step_avg:94.52ms
step:689/1700 train_time:65128ms step_avg:94.53ms
step:690/1700 train_time:65225ms step_avg:94.53ms
step:691/1700 train_time:65323ms step_avg:94.53ms
step:692/1700 train_time:65422ms step_avg:94.54ms
step:693/1700 train_time:65519ms step_avg:94.54ms
step:694/1700 train_time:65616ms step_avg:94.55ms
step:695/1700 train_time:65713ms step_avg:94.55ms
step:696/1700 train_time:65810ms step_avg:94.55ms
step:697/1700 train_time:65906ms step_avg:94.56ms
step:698/1700 train_time:66003ms step_avg:94.56ms
step:699/1700 train_time:66102ms step_avg:94.57ms
step:700/1700 train_time:66199ms step_avg:94.57ms
step:701/1700 train_time:66295ms step_avg:94.57ms
step:702/1700 train_time:66392ms step_avg:94.58ms
step:703/1700 train_time:66490ms step_avg:94.58ms
step:704/1700 train_time:66588ms step_avg:94.59ms
step:705/1700 train_time:66686ms step_avg:94.59ms
step:706/1700 train_time:66784ms step_avg:94.60ms
step:707/1700 train_time:66881ms step_avg:94.60ms
step:708/1700 train_time:66978ms step_avg:94.60ms
step:709/1700 train_time:67075ms step_avg:94.60ms
step:710/1700 train_time:67172ms step_avg:94.61ms
step:711/1700 train_time:67270ms step_avg:94.61ms
step:712/1700 train_time:67368ms step_avg:94.62ms
step:713/1700 train_time:67465ms step_avg:94.62ms
step:714/1700 train_time:67563ms step_avg:94.63ms
step:715/1700 train_time:67660ms step_avg:94.63ms
step:716/1700 train_time:67757ms step_avg:94.63ms
step:717/1700 train_time:67854ms step_avg:94.64ms
step:718/1700 train_time:67951ms step_avg:94.64ms
step:719/1700 train_time:68049ms step_avg:94.64ms
step:720/1700 train_time:68146ms step_avg:94.65ms
step:721/1700 train_time:68244ms step_avg:94.65ms
step:722/1700 train_time:68342ms step_avg:94.66ms
step:723/1700 train_time:68439ms step_avg:94.66ms
step:724/1700 train_time:68536ms step_avg:94.66ms
step:725/1700 train_time:68633ms step_avg:94.67ms
step:726/1700 train_time:68731ms step_avg:94.67ms
step:727/1700 train_time:68828ms step_avg:94.67ms
step:728/1700 train_time:68925ms step_avg:94.68ms
step:729/1700 train_time:69023ms step_avg:94.68ms
step:730/1700 train_time:69120ms step_avg:94.69ms
step:731/1700 train_time:69217ms step_avg:94.69ms
step:732/1700 train_time:69315ms step_avg:94.69ms
step:733/1700 train_time:69412ms step_avg:94.70ms
step:734/1700 train_time:69510ms step_avg:94.70ms
step:735/1700 train_time:69607ms step_avg:94.70ms
step:736/1700 train_time:69705ms step_avg:94.71ms
step:737/1700 train_time:69802ms step_avg:94.71ms
step:738/1700 train_time:69900ms step_avg:94.71ms
step:739/1700 train_time:69997ms step_avg:94.72ms
step:740/1700 train_time:70095ms step_avg:94.72ms
step:741/1700 train_time:70192ms step_avg:94.73ms
step:742/1700 train_time:70289ms step_avg:94.73ms
step:743/1700 train_time:70388ms step_avg:94.73ms
step:744/1700 train_time:70485ms step_avg:94.74ms
step:745/1700 train_time:70582ms step_avg:94.74ms
step:746/1700 train_time:70678ms step_avg:94.74ms
step:747/1700 train_time:70775ms step_avg:94.75ms
step:748/1700 train_time:70872ms step_avg:94.75ms
step:749/1700 train_time:70970ms step_avg:94.75ms
step:750/1700 train_time:71069ms step_avg:94.76ms
step:750/1700 val_loss:3.5869 train_time:71149ms step_avg:94.86ms
step:751/1700 train_time:71173ms step_avg:94.77ms
step:752/1700 train_time:71270ms step_avg:94.77ms
step:753/1700 train_time:71370ms step_avg:94.78ms
step:754/1700 train_time:71467ms step_avg:94.78ms
step:755/1700 train_time:71563ms step_avg:94.79ms
step:756/1700 train_time:71660ms step_avg:94.79ms
step:757/1700 train_time:71756ms step_avg:94.79ms
step:758/1700 train_time:71853ms step_avg:94.79ms
step:759/1700 train_time:71950ms step_avg:94.80ms
step:760/1700 train_time:72047ms step_avg:94.80ms
step:761/1700 train_time:72146ms step_avg:94.80ms
step:762/1700 train_time:72245ms step_avg:94.81ms
step:763/1700 train_time:72345ms step_avg:94.82ms
step:764/1700 train_time:72444ms step_avg:94.82ms
step:765/1700 train_time:72541ms step_avg:94.83ms
step:766/1700 train_time:72639ms step_avg:94.83ms
step:767/1700 train_time:72736ms step_avg:94.83ms
step:768/1700 train_time:72832ms step_avg:94.83ms
step:769/1700 train_time:72929ms step_avg:94.84ms
step:770/1700 train_time:73026ms step_avg:94.84ms
step:771/1700 train_time:73125ms step_avg:94.84ms
step:772/1700 train_time:73224ms step_avg:94.85ms
step:773/1700 train_time:73324ms step_avg:94.86ms
step:774/1700 train_time:73423ms step_avg:94.86ms
step:775/1700 train_time:73520ms step_avg:94.87ms
step:776/1700 train_time:73617ms step_avg:94.87ms
step:777/1700 train_time:73715ms step_avg:94.87ms
step:778/1700 train_time:73812ms step_avg:94.87ms
step:779/1700 train_time:73909ms step_avg:94.88ms
step:780/1700 train_time:74006ms step_avg:94.88ms
step:781/1700 train_time:74105ms step_avg:94.88ms
step:782/1700 train_time:74203ms step_avg:94.89ms
step:783/1700 train_time:74301ms step_avg:94.89ms
step:784/1700 train_time:74399ms step_avg:94.90ms
step:785/1700 train_time:74497ms step_avg:94.90ms
step:786/1700 train_time:74595ms step_avg:94.90ms
step:787/1700 train_time:74691ms step_avg:94.91ms
step:788/1700 train_time:74788ms step_avg:94.91ms
step:789/1700 train_time:74885ms step_avg:94.91ms
step:790/1700 train_time:74983ms step_avg:94.92ms
step:791/1700 train_time:75081ms step_avg:94.92ms
step:792/1700 train_time:75179ms step_avg:94.92ms
step:793/1700 train_time:75277ms step_avg:94.93ms
step:794/1700 train_time:75375ms step_avg:94.93ms
step:795/1700 train_time:75474ms step_avg:94.94ms
step:796/1700 train_time:75571ms step_avg:94.94ms
step:797/1700 train_time:75668ms step_avg:94.94ms
step:798/1700 train_time:75766ms step_avg:94.94ms
step:799/1700 train_time:75863ms step_avg:94.95ms
step:800/1700 train_time:75960ms step_avg:94.95ms
step:801/1700 train_time:76058ms step_avg:94.95ms
step:802/1700 train_time:76156ms step_avg:94.96ms
step:803/1700 train_time:76253ms step_avg:94.96ms
step:804/1700 train_time:76351ms step_avg:94.96ms
step:805/1700 train_time:76449ms step_avg:94.97ms
step:806/1700 train_time:76546ms step_avg:94.97ms
step:807/1700 train_time:76644ms step_avg:94.97ms
step:808/1700 train_time:76742ms step_avg:94.98ms
step:809/1700 train_time:76840ms step_avg:94.98ms
step:810/1700 train_time:76938ms step_avg:94.98ms
step:811/1700 train_time:77035ms step_avg:94.99ms
step:812/1700 train_time:77133ms step_avg:94.99ms
step:813/1700 train_time:77230ms step_avg:94.99ms
step:814/1700 train_time:77328ms step_avg:95.00ms
step:815/1700 train_time:77426ms step_avg:95.00ms
step:816/1700 train_time:77524ms step_avg:95.00ms
step:817/1700 train_time:77621ms step_avg:95.01ms
step:818/1700 train_time:77719ms step_avg:95.01ms
step:819/1700 train_time:77816ms step_avg:95.01ms
step:820/1700 train_time:77913ms step_avg:95.02ms
step:821/1700 train_time:78011ms step_avg:95.02ms
step:822/1700 train_time:78108ms step_avg:95.02ms
step:823/1700 train_time:78206ms step_avg:95.03ms
step:824/1700 train_time:78304ms step_avg:95.03ms
step:825/1700 train_time:78402ms step_avg:95.03ms
step:826/1700 train_time:78500ms step_avg:95.04ms
step:827/1700 train_time:78598ms step_avg:95.04ms
step:828/1700 train_time:78695ms step_avg:95.04ms
step:829/1700 train_time:78793ms step_avg:95.05ms
step:830/1700 train_time:78890ms step_avg:95.05ms
step:831/1700 train_time:78988ms step_avg:95.05ms
step:832/1700 train_time:79085ms step_avg:95.05ms
step:833/1700 train_time:79182ms step_avg:95.06ms
step:834/1700 train_time:79279ms step_avg:95.06ms
step:835/1700 train_time:79376ms step_avg:95.06ms
step:836/1700 train_time:79475ms step_avg:95.07ms
step:837/1700 train_time:79573ms step_avg:95.07ms
step:838/1700 train_time:79671ms step_avg:95.07ms
step:839/1700 train_time:79769ms step_avg:95.08ms
step:840/1700 train_time:79867ms step_avg:95.08ms
step:841/1700 train_time:79965ms step_avg:95.08ms
step:842/1700 train_time:80063ms step_avg:95.09ms
step:843/1700 train_time:80160ms step_avg:95.09ms
step:844/1700 train_time:80257ms step_avg:95.09ms
step:845/1700 train_time:80354ms step_avg:95.09ms
step:846/1700 train_time:80451ms step_avg:95.10ms
step:847/1700 train_time:80549ms step_avg:95.10ms
step:848/1700 train_time:80647ms step_avg:95.10ms
step:849/1700 train_time:80745ms step_avg:95.11ms
step:850/1700 train_time:80843ms step_avg:95.11ms
step:851/1700 train_time:80940ms step_avg:95.11ms
step:852/1700 train_time:81039ms step_avg:95.12ms
step:853/1700 train_time:81137ms step_avg:95.12ms
step:854/1700 train_time:81234ms step_avg:95.12ms
step:855/1700 train_time:81331ms step_avg:95.12ms
step:856/1700 train_time:81429ms step_avg:95.13ms
step:857/1700 train_time:81526ms step_avg:95.13ms
step:858/1700 train_time:81624ms step_avg:95.13ms
step:859/1700 train_time:81723ms step_avg:95.14ms
step:860/1700 train_time:81821ms step_avg:95.14ms
step:861/1700 train_time:81918ms step_avg:95.14ms
step:862/1700 train_time:82016ms step_avg:95.15ms
step:863/1700 train_time:82112ms step_avg:95.15ms
step:864/1700 train_time:82210ms step_avg:95.15ms
step:865/1700 train_time:82306ms step_avg:95.15ms
step:866/1700 train_time:82405ms step_avg:95.16ms
step:867/1700 train_time:82503ms step_avg:95.16ms
step:868/1700 train_time:82600ms step_avg:95.16ms
step:869/1700 train_time:82698ms step_avg:95.16ms
step:870/1700 train_time:82795ms step_avg:95.17ms
step:871/1700 train_time:82893ms step_avg:95.17ms
step:872/1700 train_time:82990ms step_avg:95.17ms
step:873/1700 train_time:83088ms step_avg:95.18ms
step:874/1700 train_time:83185ms step_avg:95.18ms
step:875/1700 train_time:83283ms step_avg:95.18ms
step:875/1700 val_loss:3.5383 train_time:83363ms step_avg:95.27ms
step:876/1700 train_time:83386ms step_avg:95.19ms
step:877/1700 train_time:83490ms step_avg:95.20ms
step:878/1700 train_time:83587ms step_avg:95.20ms
step:879/1700 train_time:83685ms step_avg:95.20ms
step:880/1700 train_time:83783ms step_avg:95.21ms
step:881/1700 train_time:83880ms step_avg:95.21ms
step:882/1700 train_time:83976ms step_avg:95.21ms
step:883/1700 train_time:84074ms step_avg:95.21ms
step:884/1700 train_time:84173ms step_avg:95.22ms
step:885/1700 train_time:84271ms step_avg:95.22ms
step:886/1700 train_time:84372ms step_avg:95.23ms
step:887/1700 train_time:84473ms step_avg:95.23ms
step:888/1700 train_time:84573ms step_avg:95.24ms
step:889/1700 train_time:84673ms step_avg:95.25ms
step:890/1700 train_time:84773ms step_avg:95.25ms
step:891/1700 train_time:84873ms step_avg:95.26ms
step:892/1700 train_time:84972ms step_avg:95.26ms
step:893/1700 train_time:85070ms step_avg:95.26ms
step:894/1700 train_time:85169ms step_avg:95.27ms
step:895/1700 train_time:85267ms step_avg:95.27ms
step:896/1700 train_time:85366ms step_avg:95.28ms
step:897/1700 train_time:85466ms step_avg:95.28ms
step:898/1700 train_time:85566ms step_avg:95.29ms
step:899/1700 train_time:85666ms step_avg:95.29ms
step:900/1700 train_time:85764ms step_avg:95.29ms
step:901/1700 train_time:85864ms step_avg:95.30ms
step:902/1700 train_time:85964ms step_avg:95.30ms
step:903/1700 train_time:86063ms step_avg:95.31ms
step:904/1700 train_time:86161ms step_avg:95.31ms
step:905/1700 train_time:86259ms step_avg:95.31ms
step:906/1700 train_time:86357ms step_avg:95.32ms
step:907/1700 train_time:86455ms step_avg:95.32ms
step:908/1700 train_time:86555ms step_avg:95.33ms
step:909/1700 train_time:86655ms step_avg:95.33ms
step:910/1700 train_time:86755ms step_avg:95.34ms
step:911/1700 train_time:86854ms step_avg:95.34ms
step:912/1700 train_time:86954ms step_avg:95.34ms
step:913/1700 train_time:87054ms step_avg:95.35ms
step:914/1700 train_time:87154ms step_avg:95.35ms
step:915/1700 train_time:87254ms step_avg:95.36ms
step:916/1700 train_time:87353ms step_avg:95.36ms
step:917/1700 train_time:87453ms step_avg:95.37ms
step:918/1700 train_time:87552ms step_avg:95.37ms
step:919/1700 train_time:87651ms step_avg:95.38ms
step:920/1700 train_time:87751ms step_avg:95.38ms
step:921/1700 train_time:87849ms step_avg:95.38ms
step:922/1700 train_time:87948ms step_avg:95.39ms
step:923/1700 train_time:88047ms step_avg:95.39ms
step:924/1700 train_time:88147ms step_avg:95.40ms
step:925/1700 train_time:88247ms step_avg:95.40ms
step:926/1700 train_time:88346ms step_avg:95.41ms
step:927/1700 train_time:88446ms step_avg:95.41ms
step:928/1700 train_time:88546ms step_avg:95.42ms
step:929/1700 train_time:88645ms step_avg:95.42ms
step:930/1700 train_time:88745ms step_avg:95.42ms
step:931/1700 train_time:88844ms step_avg:95.43ms
step:932/1700 train_time:88943ms step_avg:95.43ms
step:933/1700 train_time:89042ms step_avg:95.44ms
step:934/1700 train_time:89141ms step_avg:95.44ms
step:935/1700 train_time:89241ms step_avg:95.44ms
step:936/1700 train_time:89340ms step_avg:95.45ms
step:937/1700 train_time:89439ms step_avg:95.45ms
step:938/1700 train_time:89537ms step_avg:95.46ms
step:939/1700 train_time:89636ms step_avg:95.46ms
step:940/1700 train_time:89735ms step_avg:95.46ms
step:941/1700 train_time:89835ms step_avg:95.47ms
step:942/1700 train_time:89934ms step_avg:95.47ms
step:943/1700 train_time:90034ms step_avg:95.48ms
step:944/1700 train_time:90134ms step_avg:95.48ms
step:945/1700 train_time:90233ms step_avg:95.48ms
step:946/1700 train_time:90333ms step_avg:95.49ms
step:947/1700 train_time:90432ms step_avg:95.49ms
step:948/1700 train_time:90531ms step_avg:95.50ms
step:949/1700 train_time:90629ms step_avg:95.50ms
step:950/1700 train_time:90727ms step_avg:95.50ms
step:951/1700 train_time:90825ms step_avg:95.50ms
step:952/1700 train_time:90923ms step_avg:95.51ms
step:953/1700 train_time:91024ms step_avg:95.51ms
step:954/1700 train_time:91122ms step_avg:95.52ms
step:955/1700 train_time:91221ms step_avg:95.52ms
step:956/1700 train_time:91321ms step_avg:95.52ms
step:957/1700 train_time:91420ms step_avg:95.53ms
step:958/1700 train_time:91519ms step_avg:95.53ms
step:959/1700 train_time:91617ms step_avg:95.53ms
step:960/1700 train_time:91717ms step_avg:95.54ms
step:961/1700 train_time:91815ms step_avg:95.54ms
step:962/1700 train_time:91914ms step_avg:95.54ms
step:963/1700 train_time:92013ms step_avg:95.55ms
step:964/1700 train_time:92114ms step_avg:95.55ms
step:965/1700 train_time:92215ms step_avg:95.56ms
step:966/1700 train_time:92314ms step_avg:95.56ms
step:967/1700 train_time:92414ms step_avg:95.57ms
step:968/1700 train_time:92514ms step_avg:95.57ms
step:969/1700 train_time:92614ms step_avg:95.58ms
step:970/1700 train_time:92713ms step_avg:95.58ms
step:971/1700 train_time:92811ms step_avg:95.58ms
step:972/1700 train_time:92910ms step_avg:95.59ms
step:973/1700 train_time:93009ms step_avg:95.59ms
step:974/1700 train_time:93107ms step_avg:95.59ms
step:975/1700 train_time:93207ms step_avg:95.60ms
step:976/1700 train_time:93305ms step_avg:95.60ms
step:977/1700 train_time:93404ms step_avg:95.60ms
step:978/1700 train_time:93503ms step_avg:95.61ms
step:979/1700 train_time:93603ms step_avg:95.61ms
step:980/1700 train_time:93702ms step_avg:95.61ms
step:981/1700 train_time:93800ms step_avg:95.62ms
step:982/1700 train_time:93899ms step_avg:95.62ms
step:983/1700 train_time:93999ms step_avg:95.62ms
step:984/1700 train_time:94097ms step_avg:95.63ms
step:985/1700 train_time:94197ms step_avg:95.63ms
step:986/1700 train_time:94297ms step_avg:95.64ms
step:987/1700 train_time:94397ms step_avg:95.64ms
step:988/1700 train_time:94496ms step_avg:95.64ms
step:989/1700 train_time:94596ms step_avg:95.65ms
step:990/1700 train_time:94696ms step_avg:95.65ms
step:991/1700 train_time:94797ms step_avg:95.66ms
step:992/1700 train_time:94896ms step_avg:95.66ms
step:993/1700 train_time:94995ms step_avg:95.66ms
step:994/1700 train_time:95094ms step_avg:95.67ms
step:995/1700 train_time:95194ms step_avg:95.67ms
step:996/1700 train_time:95293ms step_avg:95.68ms
step:997/1700 train_time:95393ms step_avg:95.68ms
step:998/1700 train_time:95492ms step_avg:95.68ms
step:999/1700 train_time:95592ms step_avg:95.69ms
step:1000/1700 train_time:95691ms step_avg:95.69ms
step:1000/1700 val_loss:3.4926 train_time:95774ms step_avg:95.77ms
step:1001/1700 train_time:95797ms step_avg:95.70ms
step:1002/1700 train_time:95899ms step_avg:95.71ms
step:1003/1700 train_time:96000ms step_avg:95.71ms
step:1004/1700 train_time:96098ms step_avg:95.72ms
step:1005/1700 train_time:96197ms step_avg:95.72ms
step:1006/1700 train_time:96296ms step_avg:95.72ms
step:1007/1700 train_time:96395ms step_avg:95.73ms
step:1008/1700 train_time:96493ms step_avg:95.73ms
step:1009/1700 train_time:96590ms step_avg:95.73ms
step:1010/1700 train_time:96689ms step_avg:95.73ms
step:1011/1700 train_time:96791ms step_avg:95.74ms
step:1012/1700 train_time:96891ms step_avg:95.74ms
step:1013/1700 train_time:96991ms step_avg:95.75ms
step:1014/1700 train_time:97090ms step_avg:95.75ms
step:1015/1700 train_time:97189ms step_avg:95.75ms
step:1016/1700 train_time:97290ms step_avg:95.76ms
step:1017/1700 train_time:97389ms step_avg:95.76ms
step:1018/1700 train_time:97488ms step_avg:95.76ms
step:1019/1700 train_time:97586ms step_avg:95.77ms
step:1020/1700 train_time:97684ms step_avg:95.77ms
step:1021/1700 train_time:97783ms step_avg:95.77ms
step:1022/1700 train_time:97883ms step_avg:95.78ms
step:1023/1700 train_time:97981ms step_avg:95.78ms
step:1024/1700 train_time:98081ms step_avg:95.78ms
step:1025/1700 train_time:98181ms step_avg:95.79ms
step:1026/1700 train_time:98280ms step_avg:95.79ms
step:1027/1700 train_time:98379ms step_avg:95.79ms
step:1028/1700 train_time:98479ms step_avg:95.80ms
step:1029/1700 train_time:98578ms step_avg:95.80ms
step:1030/1700 train_time:98677ms step_avg:95.80ms
step:1031/1700 train_time:98778ms step_avg:95.81ms
step:1032/1700 train_time:98877ms step_avg:95.81ms
step:1033/1700 train_time:98977ms step_avg:95.82ms
step:1034/1700 train_time:99077ms step_avg:95.82ms
step:1035/1700 train_time:99177ms step_avg:95.82ms
step:1036/1700 train_time:99277ms step_avg:95.83ms
step:1037/1700 train_time:99377ms step_avg:95.83ms
step:1038/1700 train_time:99476ms step_avg:95.83ms
step:1039/1700 train_time:99575ms step_avg:95.84ms
step:1040/1700 train_time:99675ms step_avg:95.84ms
step:1041/1700 train_time:99775ms step_avg:95.85ms
step:1042/1700 train_time:99875ms step_avg:95.85ms
step:1043/1700 train_time:99975ms step_avg:95.85ms
step:1044/1700 train_time:100075ms step_avg:95.86ms
step:1045/1700 train_time:100175ms step_avg:95.86ms
step:1046/1700 train_time:100274ms step_avg:95.86ms
step:1047/1700 train_time:100374ms step_avg:95.87ms
step:1048/1700 train_time:100474ms step_avg:95.87ms
step:1049/1700 train_time:100573ms step_avg:95.88ms
step:1050/1700 train_time:100672ms step_avg:95.88ms
step:1051/1700 train_time:100771ms step_avg:95.88ms
step:1052/1700 train_time:100869ms step_avg:95.88ms
step:1053/1700 train_time:100969ms step_avg:95.89ms
step:1054/1700 train_time:101069ms step_avg:95.89ms
step:1055/1700 train_time:101169ms step_avg:95.89ms
step:1056/1700 train_time:101268ms step_avg:95.90ms
step:1057/1700 train_time:101368ms step_avg:95.90ms
step:1058/1700 train_time:101467ms step_avg:95.90ms
step:1059/1700 train_time:101565ms step_avg:95.91ms
step:1060/1700 train_time:101665ms step_avg:95.91ms
step:1061/1700 train_time:101764ms step_avg:95.91ms
step:1062/1700 train_time:101862ms step_avg:95.92ms
step:1063/1700 train_time:101961ms step_avg:95.92ms
step:1064/1700 train_time:102060ms step_avg:95.92ms
step:1065/1700 train_time:102160ms step_avg:95.92ms
step:1066/1700 train_time:102258ms step_avg:95.93ms
step:1067/1700 train_time:102358ms step_avg:95.93ms
step:1068/1700 train_time:102457ms step_avg:95.93ms
step:1069/1700 train_time:102555ms step_avg:95.94ms
step:1070/1700 train_time:102655ms step_avg:95.94ms
step:1071/1700 train_time:102756ms step_avg:95.94ms
step:1072/1700 train_time:102856ms step_avg:95.95ms
step:1073/1700 train_time:102955ms step_avg:95.95ms
step:1074/1700 train_time:103055ms step_avg:95.95ms
step:1075/1700 train_time:103155ms step_avg:95.96ms
step:1076/1700 train_time:103255ms step_avg:95.96ms
step:1077/1700 train_time:103354ms step_avg:95.96ms
step:1078/1700 train_time:103452ms step_avg:95.97ms
step:1079/1700 train_time:103552ms step_avg:95.97ms
step:1080/1700 train_time:103651ms step_avg:95.97ms
step:1081/1700 train_time:103750ms step_avg:95.98ms
step:1082/1700 train_time:103849ms step_avg:95.98ms
step:1083/1700 train_time:103948ms step_avg:95.98ms
step:1084/1700 train_time:104048ms step_avg:95.99ms
step:1085/1700 train_time:104149ms step_avg:95.99ms
step:1086/1700 train_time:104248ms step_avg:95.99ms
step:1087/1700 train_time:104348ms step_avg:96.00ms
step:1088/1700 train_time:104448ms step_avg:96.00ms
step:1089/1700 train_time:104547ms step_avg:96.00ms
step:1090/1700 train_time:104646ms step_avg:96.01ms
step:1091/1700 train_time:104745ms step_avg:96.01ms
step:1092/1700 train_time:104843ms step_avg:96.01ms
step:1093/1700 train_time:104942ms step_avg:96.01ms
step:1094/1700 train_time:105040ms step_avg:96.01ms
step:1095/1700 train_time:105139ms step_avg:96.02ms
step:1096/1700 train_time:105239ms step_avg:96.02ms
step:1097/1700 train_time:105338ms step_avg:96.02ms
step:1098/1700 train_time:105438ms step_avg:96.03ms
step:1099/1700 train_time:105537ms step_avg:96.03ms
step:1100/1700 train_time:105636ms step_avg:96.03ms
step:1101/1700 train_time:105736ms step_avg:96.04ms
step:1102/1700 train_time:105836ms step_avg:96.04ms
step:1103/1700 train_time:105936ms step_avg:96.04ms
step:1104/1700 train_time:106035ms step_avg:96.05ms
step:1105/1700 train_time:106136ms step_avg:96.05ms
step:1106/1700 train_time:106236ms step_avg:96.05ms
step:1107/1700 train_time:106336ms step_avg:96.06ms
step:1108/1700 train_time:106435ms step_avg:96.06ms
step:1109/1700 train_time:106535ms step_avg:96.06ms
step:1110/1700 train_time:106635ms step_avg:96.07ms
step:1111/1700 train_time:106736ms step_avg:96.07ms
step:1112/1700 train_time:106836ms step_avg:96.08ms
step:1113/1700 train_time:106936ms step_avg:96.08ms
step:1114/1700 train_time:107036ms step_avg:96.08ms
step:1115/1700 train_time:107135ms step_avg:96.09ms
step:1116/1700 train_time:107235ms step_avg:96.09ms
step:1117/1700 train_time:107335ms step_avg:96.09ms
step:1118/1700 train_time:107435ms step_avg:96.10ms
step:1119/1700 train_time:107534ms step_avg:96.10ms
step:1120/1700 train_time:107634ms step_avg:96.10ms
step:1121/1700 train_time:107734ms step_avg:96.11ms
step:1122/1700 train_time:107833ms step_avg:96.11ms
step:1123/1700 train_time:107933ms step_avg:96.11ms
step:1124/1700 train_time:108032ms step_avg:96.11ms
step:1125/1700 train_time:108131ms step_avg:96.12ms
step:1125/1700 val_loss:3.4415 train_time:108213ms step_avg:96.19ms
step:1126/1700 train_time:108236ms step_avg:96.12ms
step:1127/1700 train_time:108339ms step_avg:96.13ms
step:1128/1700 train_time:108439ms step_avg:96.13ms
step:1129/1700 train_time:108539ms step_avg:96.14ms
step:1130/1700 train_time:108638ms step_avg:96.14ms
step:1131/1700 train_time:108737ms step_avg:96.14ms
step:1132/1700 train_time:108836ms step_avg:96.14ms
step:1133/1700 train_time:108934ms step_avg:96.15ms
step:1134/1700 train_time:109032ms step_avg:96.15ms
step:1135/1700 train_time:109131ms step_avg:96.15ms
step:1136/1700 train_time:109233ms step_avg:96.16ms
step:1137/1700 train_time:109334ms step_avg:96.16ms
step:1138/1700 train_time:109435ms step_avg:96.16ms
step:1139/1700 train_time:109536ms step_avg:96.17ms
step:1140/1700 train_time:109636ms step_avg:96.17ms
step:1141/1700 train_time:109736ms step_avg:96.18ms
step:1142/1700 train_time:109836ms step_avg:96.18ms
step:1143/1700 train_time:109935ms step_avg:96.18ms
step:1144/1700 train_time:110035ms step_avg:96.18ms
step:1145/1700 train_time:110135ms step_avg:96.19ms
step:1146/1700 train_time:110236ms step_avg:96.19ms
step:1147/1700 train_time:110337ms step_avg:96.20ms
step:1148/1700 train_time:110438ms step_avg:96.20ms
step:1149/1700 train_time:110540ms step_avg:96.21ms
step:1150/1700 train_time:110641ms step_avg:96.21ms
step:1151/1700 train_time:110741ms step_avg:96.21ms
step:1152/1700 train_time:110841ms step_avg:96.22ms
step:1153/1700 train_time:110941ms step_avg:96.22ms
step:1154/1700 train_time:111040ms step_avg:96.22ms
step:1155/1700 train_time:111140ms step_avg:96.23ms
step:1156/1700 train_time:111240ms step_avg:96.23ms
step:1157/1700 train_time:111342ms step_avg:96.23ms
step:1158/1700 train_time:111442ms step_avg:96.24ms
step:1159/1700 train_time:111544ms step_avg:96.24ms
step:1160/1700 train_time:111645ms step_avg:96.25ms
step:1161/1700 train_time:111745ms step_avg:96.25ms
step:1162/1700 train_time:111844ms step_avg:96.25ms
step:1163/1700 train_time:111945ms step_avg:96.26ms
step:1164/1700 train_time:112044ms step_avg:96.26ms
step:1165/1700 train_time:112144ms step_avg:96.26ms
step:1166/1700 train_time:112245ms step_avg:96.26ms
step:1167/1700 train_time:112345ms step_avg:96.27ms
step:1168/1700 train_time:112446ms step_avg:96.27ms
step:1169/1700 train_time:112546ms step_avg:96.28ms
step:1170/1700 train_time:112646ms step_avg:96.28ms
step:1171/1700 train_time:112746ms step_avg:96.28ms
step:1172/1700 train_time:112847ms step_avg:96.29ms
step:1173/1700 train_time:112947ms step_avg:96.29ms
step:1174/1700 train_time:113046ms step_avg:96.29ms
step:1175/1700 train_time:113147ms step_avg:96.30ms
step:1176/1700 train_time:113248ms step_avg:96.30ms
step:1177/1700 train_time:113347ms step_avg:96.30ms
step:1178/1700 train_time:113448ms step_avg:96.31ms
step:1179/1700 train_time:113547ms step_avg:96.31ms
step:1180/1700 train_time:113647ms step_avg:96.31ms
step:1181/1700 train_time:113747ms step_avg:96.31ms
step:1182/1700 train_time:113848ms step_avg:96.32ms
step:1183/1700 train_time:113949ms step_avg:96.32ms
step:1184/1700 train_time:114051ms step_avg:96.33ms
step:1185/1700 train_time:114152ms step_avg:96.33ms
step:1186/1700 train_time:114253ms step_avg:96.33ms
step:1187/1700 train_time:114353ms step_avg:96.34ms
step:1188/1700 train_time:114453ms step_avg:96.34ms
step:1189/1700 train_time:114552ms step_avg:96.34ms
step:1190/1700 train_time:114653ms step_avg:96.35ms
step:1191/1700 train_time:114753ms step_avg:96.35ms
step:1192/1700 train_time:114853ms step_avg:96.35ms
step:1193/1700 train_time:114954ms step_avg:96.36ms
step:1194/1700 train_time:115056ms step_avg:96.36ms
step:1195/1700 train_time:115156ms step_avg:96.36ms
step:1196/1700 train_time:115257ms step_avg:96.37ms
step:1197/1700 train_time:115359ms step_avg:96.37ms
step:1198/1700 train_time:115459ms step_avg:96.38ms
step:1199/1700 train_time:115560ms step_avg:96.38ms
step:1200/1700 train_time:115659ms step_avg:96.38ms
step:1201/1700 train_time:115760ms step_avg:96.39ms
step:1202/1700 train_time:115861ms step_avg:96.39ms
step:1203/1700 train_time:115962ms step_avg:96.39ms
step:1204/1700 train_time:116062ms step_avg:96.40ms
step:1205/1700 train_time:116162ms step_avg:96.40ms
step:1206/1700 train_time:116264ms step_avg:96.40ms
step:1207/1700 train_time:116364ms step_avg:96.41ms
step:1208/1700 train_time:116465ms step_avg:96.41ms
step:1209/1700 train_time:116566ms step_avg:96.42ms
step:1210/1700 train_time:116666ms step_avg:96.42ms
step:1211/1700 train_time:116766ms step_avg:96.42ms
step:1212/1700 train_time:116865ms step_avg:96.42ms
step:1213/1700 train_time:116965ms step_avg:96.43ms
step:1214/1700 train_time:117064ms step_avg:96.43ms
step:1215/1700 train_time:117165ms step_avg:96.43ms
step:1216/1700 train_time:117265ms step_avg:96.43ms
step:1217/1700 train_time:117365ms step_avg:96.44ms
step:1218/1700 train_time:117466ms step_avg:96.44ms
step:1219/1700 train_time:117567ms step_avg:96.45ms
step:1220/1700 train_time:117667ms step_avg:96.45ms
step:1221/1700 train_time:117767ms step_avg:96.45ms
step:1222/1700 train_time:117868ms step_avg:96.45ms
step:1223/1700 train_time:117969ms step_avg:96.46ms
step:1224/1700 train_time:118070ms step_avg:96.46ms
step:1225/1700 train_time:118171ms step_avg:96.47ms
step:1226/1700 train_time:118273ms step_avg:96.47ms
step:1227/1700 train_time:118372ms step_avg:96.47ms
step:1228/1700 train_time:118471ms step_avg:96.48ms
step:1229/1700 train_time:118571ms step_avg:96.48ms
step:1230/1700 train_time:118673ms step_avg:96.48ms
step:1231/1700 train_time:118773ms step_avg:96.49ms
step:1232/1700 train_time:118874ms step_avg:96.49ms
step:1233/1700 train_time:118973ms step_avg:96.49ms
step:1234/1700 train_time:119073ms step_avg:96.49ms
step:1235/1700 train_time:119173ms step_avg:96.50ms
step:1236/1700 train_time:119275ms step_avg:96.50ms
step:1237/1700 train_time:119376ms step_avg:96.50ms
step:1238/1700 train_time:119476ms step_avg:96.51ms
step:1239/1700 train_time:119575ms step_avg:96.51ms
step:1240/1700 train_time:119675ms step_avg:96.51ms
step:1241/1700 train_time:119776ms step_avg:96.52ms
step:1242/1700 train_time:119878ms step_avg:96.52ms
step:1243/1700 train_time:119978ms step_avg:96.52ms
step:1244/1700 train_time:120080ms step_avg:96.53ms
step:1245/1700 train_time:120180ms step_avg:96.53ms
step:1246/1700 train_time:120281ms step_avg:96.53ms
step:1247/1700 train_time:120381ms step_avg:96.54ms
step:1248/1700 train_time:120481ms step_avg:96.54ms
step:1249/1700 train_time:120581ms step_avg:96.54ms
step:1250/1700 train_time:120681ms step_avg:96.55ms
step:1250/1700 val_loss:3.3970 train_time:120764ms step_avg:96.61ms
step:1251/1700 train_time:120787ms step_avg:96.55ms
step:1252/1700 train_time:120888ms step_avg:96.56ms
step:1253/1700 train_time:120990ms step_avg:96.56ms
step:1254/1700 train_time:121090ms step_avg:96.56ms
step:1255/1700 train_time:121190ms step_avg:96.57ms
step:1256/1700 train_time:121290ms step_avg:96.57ms
step:1257/1700 train_time:121390ms step_avg:96.57ms
step:1258/1700 train_time:121489ms step_avg:96.57ms
step:1259/1700 train_time:121589ms step_avg:96.58ms
step:1260/1700 train_time:121689ms step_avg:96.58ms
step:1261/1700 train_time:121791ms step_avg:96.58ms
step:1262/1700 train_time:121892ms step_avg:96.59ms
step:1263/1700 train_time:121993ms step_avg:96.59ms
step:1264/1700 train_time:122094ms step_avg:96.59ms
step:1265/1700 train_time:122194ms step_avg:96.60ms
step:1266/1700 train_time:122293ms step_avg:96.60ms
step:1267/1700 train_time:122393ms step_avg:96.60ms
step:1268/1700 train_time:122492ms step_avg:96.60ms
step:1269/1700 train_time:122592ms step_avg:96.61ms
step:1270/1700 train_time:122692ms step_avg:96.61ms
step:1271/1700 train_time:122795ms step_avg:96.61ms
step:1272/1700 train_time:122895ms step_avg:96.62ms
step:1273/1700 train_time:122996ms step_avg:96.62ms
step:1274/1700 train_time:123096ms step_avg:96.62ms
step:1275/1700 train_time:123195ms step_avg:96.62ms
step:1276/1700 train_time:123296ms step_avg:96.63ms
step:1277/1700 train_time:123397ms step_avg:96.63ms
step:1278/1700 train_time:123497ms step_avg:96.63ms
step:1279/1700 train_time:123597ms step_avg:96.64ms
step:1280/1700 train_time:123698ms step_avg:96.64ms
step:1281/1700 train_time:123798ms step_avg:96.64ms
step:1282/1700 train_time:123899ms step_avg:96.64ms
step:1283/1700 train_time:123999ms step_avg:96.65ms
step:1284/1700 train_time:124099ms step_avg:96.65ms
step:1285/1700 train_time:124197ms step_avg:96.65ms
step:1286/1700 train_time:124297ms step_avg:96.65ms
step:1287/1700 train_time:124398ms step_avg:96.66ms
step:1288/1700 train_time:124498ms step_avg:96.66ms
step:1289/1700 train_time:124598ms step_avg:96.66ms
step:1290/1700 train_time:124698ms step_avg:96.67ms
step:1291/1700 train_time:124800ms step_avg:96.67ms
step:1292/1700 train_time:124900ms step_avg:96.67ms
step:1293/1700 train_time:125000ms step_avg:96.67ms
step:1294/1700 train_time:125102ms step_avg:96.68ms
step:1295/1700 train_time:125203ms step_avg:96.68ms
step:1296/1700 train_time:125304ms step_avg:96.69ms
step:1297/1700 train_time:125405ms step_avg:96.69ms
step:1298/1700 train_time:125504ms step_avg:96.69ms
step:1299/1700 train_time:125604ms step_avg:96.69ms
step:1300/1700 train_time:125705ms step_avg:96.70ms
step:1301/1700 train_time:125805ms step_avg:96.70ms
step:1302/1700 train_time:125905ms step_avg:96.70ms
step:1303/1700 train_time:126007ms step_avg:96.71ms
step:1304/1700 train_time:126107ms step_avg:96.71ms
step:1305/1700 train_time:126207ms step_avg:96.71ms
step:1306/1700 train_time:126308ms step_avg:96.71ms
step:1307/1700 train_time:126407ms step_avg:96.72ms
step:1308/1700 train_time:126509ms step_avg:96.72ms
step:1309/1700 train_time:126610ms step_avg:96.72ms
step:1310/1700 train_time:126711ms step_avg:96.73ms
step:1311/1700 train_time:126811ms step_avg:96.73ms
step:1312/1700 train_time:126913ms step_avg:96.73ms
step:1313/1700 train_time:127013ms step_avg:96.73ms
step:1314/1700 train_time:127113ms step_avg:96.74ms
step:1315/1700 train_time:127213ms step_avg:96.74ms
step:1316/1700 train_time:127312ms step_avg:96.74ms
step:1317/1700 train_time:127412ms step_avg:96.74ms
step:1318/1700 train_time:127511ms step_avg:96.75ms
step:1319/1700 train_time:127613ms step_avg:96.75ms
step:1320/1700 train_time:127714ms step_avg:96.75ms
step:1321/1700 train_time:127814ms step_avg:96.76ms
step:1322/1700 train_time:127914ms step_avg:96.76ms
step:1323/1700 train_time:128014ms step_avg:96.76ms
step:1324/1700 train_time:128115ms step_avg:96.76ms
step:1325/1700 train_time:128217ms step_avg:96.77ms
step:1326/1700 train_time:128317ms step_avg:96.77ms
step:1327/1700 train_time:128417ms step_avg:96.77ms
step:1328/1700 train_time:128517ms step_avg:96.78ms
step:1329/1700 train_time:128618ms step_avg:96.78ms
step:1330/1700 train_time:128718ms step_avg:96.78ms
step:1331/1700 train_time:128818ms step_avg:96.78ms
step:1332/1700 train_time:128919ms step_avg:96.79ms
step:1333/1700 train_time:129020ms step_avg:96.79ms
step:1334/1700 train_time:129121ms step_avg:96.79ms
step:1335/1700 train_time:129222ms step_avg:96.80ms
step:1336/1700 train_time:129323ms step_avg:96.80ms
step:1337/1700 train_time:129425ms step_avg:96.80ms
step:1338/1700 train_time:129524ms step_avg:96.80ms
step:1339/1700 train_time:129625ms step_avg:96.81ms
step:1340/1700 train_time:129724ms step_avg:96.81ms
step:1341/1700 train_time:129825ms step_avg:96.81ms
step:1342/1700 train_time:129924ms step_avg:96.81ms
step:1343/1700 train_time:130024ms step_avg:96.82ms
step:1344/1700 train_time:130125ms step_avg:96.82ms
step:1345/1700 train_time:130225ms step_avg:96.82ms
step:1346/1700 train_time:130326ms step_avg:96.82ms
step:1347/1700 train_time:130425ms step_avg:96.83ms
step:1348/1700 train_time:130525ms step_avg:96.83ms
step:1349/1700 train_time:130625ms step_avg:96.83ms
step:1350/1700 train_time:130726ms step_avg:96.83ms
step:1351/1700 train_time:130826ms step_avg:96.84ms
step:1352/1700 train_time:130926ms step_avg:96.84ms
step:1353/1700 train_time:131026ms step_avg:96.84ms
step:1354/1700 train_time:131126ms step_avg:96.84ms
step:1355/1700 train_time:131227ms step_avg:96.85ms
step:1356/1700 train_time:131327ms step_avg:96.85ms
step:1357/1700 train_time:131428ms step_avg:96.85ms
step:1358/1700 train_time:131528ms step_avg:96.85ms
step:1359/1700 train_time:131629ms step_avg:96.86ms
step:1360/1700 train_time:131731ms step_avg:96.86ms
step:1361/1700 train_time:131832ms step_avg:96.86ms
step:1362/1700 train_time:131932ms step_avg:96.87ms
step:1363/1700 train_time:132034ms step_avg:96.87ms
step:1364/1700 train_time:132134ms step_avg:96.87ms
step:1365/1700 train_time:132235ms step_avg:96.88ms
step:1366/1700 train_time:132335ms step_avg:96.88ms
step:1367/1700 train_time:132435ms step_avg:96.88ms
step:1368/1700 train_time:132536ms step_avg:96.88ms
step:1369/1700 train_time:132637ms step_avg:96.89ms
step:1370/1700 train_time:132737ms step_avg:96.89ms
step:1371/1700 train_time:132839ms step_avg:96.89ms
step:1372/1700 train_time:132939ms step_avg:96.89ms
step:1373/1700 train_time:133039ms step_avg:96.90ms
step:1374/1700 train_time:133139ms step_avg:96.90ms
step:1375/1700 train_time:133240ms step_avg:96.90ms
step:1375/1700 val_loss:3.3574 train_time:133322ms step_avg:96.96ms
step:1376/1700 train_time:133345ms step_avg:96.91ms
step:1377/1700 train_time:133447ms step_avg:96.91ms
step:1378/1700 train_time:133548ms step_avg:96.91ms
step:1379/1700 train_time:133648ms step_avg:96.92ms
step:1380/1700 train_time:133748ms step_avg:96.92ms
step:1381/1700 train_time:133846ms step_avg:96.92ms
step:1382/1700 train_time:133945ms step_avg:96.92ms
step:1383/1700 train_time:134044ms step_avg:96.92ms
step:1384/1700 train_time:134143ms step_avg:96.92ms
step:1385/1700 train_time:134243ms step_avg:96.93ms
step:1386/1700 train_time:134345ms step_avg:96.93ms
step:1387/1700 train_time:134448ms step_avg:96.93ms
step:1388/1700 train_time:134550ms step_avg:96.94ms
step:1389/1700 train_time:134653ms step_avg:96.94ms
step:1390/1700 train_time:134752ms step_avg:96.94ms
step:1391/1700 train_time:134853ms step_avg:96.95ms
step:1392/1700 train_time:134956ms step_avg:96.95ms
step:1393/1700 train_time:135058ms step_avg:96.95ms
step:1394/1700 train_time:135159ms step_avg:96.96ms
step:1395/1700 train_time:135260ms step_avg:96.96ms
step:1396/1700 train_time:135363ms step_avg:96.96ms
step:1397/1700 train_time:135464ms step_avg:96.97ms
step:1398/1700 train_time:135567ms step_avg:96.97ms
step:1399/1700 train_time:135668ms step_avg:96.97ms
step:1400/1700 train_time:135769ms step_avg:96.98ms
step:1401/1700 train_time:135871ms step_avg:96.98ms
step:1402/1700 train_time:135972ms step_avg:96.98ms
step:1403/1700 train_time:136074ms step_avg:96.99ms
step:1404/1700 train_time:136175ms step_avg:96.99ms
step:1405/1700 train_time:136278ms step_avg:96.99ms
step:1406/1700 train_time:136380ms step_avg:97.00ms
step:1407/1700 train_time:136482ms step_avg:97.00ms
step:1408/1700 train_time:136584ms step_avg:97.01ms
step:1409/1700 train_time:136686ms step_avg:97.01ms
step:1410/1700 train_time:136786ms step_avg:97.01ms
step:1411/1700 train_time:136887ms step_avg:97.01ms
step:1412/1700 train_time:136989ms step_avg:97.02ms
step:1413/1700 train_time:137092ms step_avg:97.02ms
step:1414/1700 train_time:137194ms step_avg:97.03ms
step:1415/1700 train_time:137295ms step_avg:97.03ms
step:1416/1700 train_time:137395ms step_avg:97.03ms
step:1417/1700 train_time:137497ms step_avg:97.03ms
step:1418/1700 train_time:137598ms step_avg:97.04ms
step:1419/1700 train_time:137701ms step_avg:97.04ms
step:1420/1700 train_time:137802ms step_avg:97.04ms
step:1421/1700 train_time:137904ms step_avg:97.05ms
step:1422/1700 train_time:138005ms step_avg:97.05ms
step:1423/1700 train_time:138106ms step_avg:97.05ms
step:1424/1700 train_time:138208ms step_avg:97.06ms
step:1425/1700 train_time:138309ms step_avg:97.06ms
step:1426/1700 train_time:138410ms step_avg:97.06ms
step:1427/1700 train_time:138513ms step_avg:97.07ms
step:1428/1700 train_time:138614ms step_avg:97.07ms
step:1429/1700 train_time:138714ms step_avg:97.07ms
step:1430/1700 train_time:138816ms step_avg:97.07ms
step:1431/1700 train_time:138918ms step_avg:97.08ms
step:1432/1700 train_time:139021ms step_avg:97.08ms
step:1433/1700 train_time:139121ms step_avg:97.08ms
step:1434/1700 train_time:139222ms step_avg:97.09ms
step:1435/1700 train_time:139323ms step_avg:97.09ms
step:1436/1700 train_time:139425ms step_avg:97.09ms
step:1437/1700 train_time:139527ms step_avg:97.10ms
step:1438/1700 train_time:139627ms step_avg:97.10ms
step:1439/1700 train_time:139729ms step_avg:97.10ms
step:1440/1700 train_time:139833ms step_avg:97.11ms
step:1441/1700 train_time:139935ms step_avg:97.11ms
step:1442/1700 train_time:140036ms step_avg:97.11ms
step:1443/1700 train_time:140137ms step_avg:97.11ms
step:1444/1700 train_time:140238ms step_avg:97.12ms
step:1445/1700 train_time:140340ms step_avg:97.12ms
step:1446/1700 train_time:140442ms step_avg:97.12ms
step:1447/1700 train_time:140543ms step_avg:97.13ms
step:1448/1700 train_time:140644ms step_avg:97.13ms
step:1449/1700 train_time:140744ms step_avg:97.13ms
step:1450/1700 train_time:140847ms step_avg:97.14ms
step:1451/1700 train_time:140950ms step_avg:97.14ms
step:1452/1700 train_time:141051ms step_avg:97.14ms
step:1453/1700 train_time:141153ms step_avg:97.15ms
step:1454/1700 train_time:141256ms step_avg:97.15ms
step:1455/1700 train_time:141357ms step_avg:97.15ms
step:1456/1700 train_time:141458ms step_avg:97.16ms
step:1457/1700 train_time:141560ms step_avg:97.16ms
step:1458/1700 train_time:141661ms step_avg:97.16ms
step:1459/1700 train_time:141762ms step_avg:97.16ms
step:1460/1700 train_time:141863ms step_avg:97.17ms
step:1461/1700 train_time:141964ms step_avg:97.17ms
step:1462/1700 train_time:142066ms step_avg:97.17ms
step:1463/1700 train_time:142168ms step_avg:97.18ms
step:1464/1700 train_time:142269ms step_avg:97.18ms
step:1465/1700 train_time:142372ms step_avg:97.18ms
step:1466/1700 train_time:142473ms step_avg:97.18ms
step:1467/1700 train_time:142573ms step_avg:97.19ms
step:1468/1700 train_time:142675ms step_avg:97.19ms
step:1469/1700 train_time:142777ms step_avg:97.19ms
step:1470/1700 train_time:142880ms step_avg:97.20ms
step:1471/1700 train_time:142981ms step_avg:97.20ms
step:1472/1700 train_time:143082ms step_avg:97.20ms
step:1473/1700 train_time:143183ms step_avg:97.20ms
step:1474/1700 train_time:143285ms step_avg:97.21ms
step:1475/1700 train_time:143386ms step_avg:97.21ms
step:1476/1700 train_time:143489ms step_avg:97.21ms
step:1477/1700 train_time:143591ms step_avg:97.22ms
step:1478/1700 train_time:143692ms step_avg:97.22ms
step:1479/1700 train_time:143793ms step_avg:97.22ms
step:1480/1700 train_time:143893ms step_avg:97.23ms
step:1481/1700 train_time:143995ms step_avg:97.23ms
step:1482/1700 train_time:144097ms step_avg:97.23ms
step:1483/1700 train_time:144200ms step_avg:97.24ms
step:1484/1700 train_time:144301ms step_avg:97.24ms
step:1485/1700 train_time:144403ms step_avg:97.24ms
step:1486/1700 train_time:144505ms step_avg:97.24ms
step:1487/1700 train_time:144606ms step_avg:97.25ms
step:1488/1700 train_time:144709ms step_avg:97.25ms
step:1489/1700 train_time:144810ms step_avg:97.25ms
step:1490/1700 train_time:144912ms step_avg:97.26ms
step:1491/1700 train_time:145013ms step_avg:97.26ms
step:1492/1700 train_time:145115ms step_avg:97.26ms
step:1493/1700 train_time:145216ms step_avg:97.26ms
step:1494/1700 train_time:145318ms step_avg:97.27ms
step:1495/1700 train_time:145419ms step_avg:97.27ms
step:1496/1700 train_time:145521ms step_avg:97.27ms
step:1497/1700 train_time:145622ms step_avg:97.28ms
step:1498/1700 train_time:145723ms step_avg:97.28ms
step:1499/1700 train_time:145824ms step_avg:97.28ms
step:1500/1700 train_time:145925ms step_avg:97.28ms
step:1500/1700 val_loss:3.3228 train_time:146008ms step_avg:97.34ms
step:1501/1700 train_time:146031ms step_avg:97.29ms
step:1502/1700 train_time:146134ms step_avg:97.29ms
step:1503/1700 train_time:146235ms step_avg:97.30ms
step:1504/1700 train_time:146336ms step_avg:97.30ms
step:1505/1700 train_time:146438ms step_avg:97.30ms
step:1506/1700 train_time:146538ms step_avg:97.30ms
step:1507/1700 train_time:146639ms step_avg:97.31ms
step:1508/1700 train_time:146739ms step_avg:97.31ms
step:1509/1700 train_time:146840ms step_avg:97.31ms
step:1510/1700 train_time:146942ms step_avg:97.31ms
step:1511/1700 train_time:147044ms step_avg:97.32ms
step:1512/1700 train_time:147146ms step_avg:97.32ms
step:1513/1700 train_time:147248ms step_avg:97.32ms
step:1514/1700 train_time:147350ms step_avg:97.32ms
step:1515/1700 train_time:147454ms step_avg:97.33ms
step:1516/1700 train_time:147554ms step_avg:97.33ms
step:1517/1700 train_time:147655ms step_avg:97.33ms
step:1518/1700 train_time:147755ms step_avg:97.34ms
step:1519/1700 train_time:147858ms step_avg:97.34ms
step:1520/1700 train_time:147960ms step_avg:97.34ms
step:1521/1700 train_time:148061ms step_avg:97.34ms
step:1522/1700 train_time:148163ms step_avg:97.35ms
step:1523/1700 train_time:148265ms step_avg:97.35ms
step:1524/1700 train_time:148368ms step_avg:97.35ms
step:1525/1700 train_time:148470ms step_avg:97.36ms
step:1526/1700 train_time:148571ms step_avg:97.36ms
step:1527/1700 train_time:148672ms step_avg:97.36ms
step:1528/1700 train_time:148775ms step_avg:97.37ms
step:1529/1700 train_time:148876ms step_avg:97.37ms
step:1530/1700 train_time:148977ms step_avg:97.37ms
step:1531/1700 train_time:149078ms step_avg:97.37ms
step:1532/1700 train_time:149181ms step_avg:97.38ms
step:1533/1700 train_time:149282ms step_avg:97.38ms
step:1534/1700 train_time:149384ms step_avg:97.38ms
step:1535/1700 train_time:149485ms step_avg:97.38ms
step:1536/1700 train_time:149587ms step_avg:97.39ms
step:1537/1700 train_time:149688ms step_avg:97.39ms
step:1538/1700 train_time:149789ms step_avg:97.39ms
step:1539/1700 train_time:149891ms step_avg:97.40ms
step:1540/1700 train_time:149992ms step_avg:97.40ms
step:1541/1700 train_time:150093ms step_avg:97.40ms
step:1542/1700 train_time:150196ms step_avg:97.40ms
step:1543/1700 train_time:150299ms step_avg:97.41ms
step:1544/1700 train_time:150401ms step_avg:97.41ms
step:1545/1700 train_time:150501ms step_avg:97.41ms
step:1546/1700 train_time:150603ms step_avg:97.41ms
step:1547/1700 train_time:150707ms step_avg:97.42ms
step:1548/1700 train_time:150809ms step_avg:97.42ms
step:1549/1700 train_time:150911ms step_avg:97.42ms
step:1550/1700 train_time:151012ms step_avg:97.43ms
step:1551/1700 train_time:151114ms step_avg:97.43ms
step:1552/1700 train_time:151214ms step_avg:97.43ms
step:1553/1700 train_time:151315ms step_avg:97.43ms
step:1554/1700 train_time:151417ms step_avg:97.44ms
step:1555/1700 train_time:151518ms step_avg:97.44ms
step:1556/1700 train_time:151622ms step_avg:97.44ms
step:1557/1700 train_time:151723ms step_avg:97.45ms
step:1558/1700 train_time:151826ms step_avg:97.45ms
step:1559/1700 train_time:151928ms step_avg:97.45ms
step:1560/1700 train_time:152029ms step_avg:97.45ms
step:1561/1700 train_time:152130ms step_avg:97.46ms
step:1562/1700 train_time:152232ms step_avg:97.46ms
step:1563/1700 train_time:152335ms step_avg:97.46ms
step:1564/1700 train_time:152436ms step_avg:97.47ms
step:1565/1700 train_time:152536ms step_avg:97.47ms
step:1566/1700 train_time:152639ms step_avg:97.47ms
step:1567/1700 train_time:152740ms step_avg:97.47ms
step:1568/1700 train_time:152841ms step_avg:97.47ms
step:1569/1700 train_time:152942ms step_avg:97.48ms
step:1570/1700 train_time:153045ms step_avg:97.48ms
step:1571/1700 train_time:153148ms step_avg:97.48ms
step:1572/1700 train_time:153250ms step_avg:97.49ms
step:1573/1700 train_time:153350ms step_avg:97.49ms
step:1574/1700 train_time:153452ms step_avg:97.49ms
step:1575/1700 train_time:153553ms step_avg:97.49ms
step:1576/1700 train_time:153656ms step_avg:97.50ms
step:1577/1700 train_time:153759ms step_avg:97.50ms
step:1578/1700 train_time:153860ms step_avg:97.50ms
step:1579/1700 train_time:153961ms step_avg:97.51ms
step:1580/1700 train_time:154061ms step_avg:97.51ms
step:1581/1700 train_time:154163ms step_avg:97.51ms
step:1582/1700 train_time:154265ms step_avg:97.51ms
step:1583/1700 train_time:154370ms step_avg:97.52ms
step:1584/1700 train_time:154472ms step_avg:97.52ms
step:1585/1700 train_time:154573ms step_avg:97.52ms
step:1586/1700 train_time:154676ms step_avg:97.53ms
step:1587/1700 train_time:154778ms step_avg:97.53ms
step:1588/1700 train_time:154878ms step_avg:97.53ms
step:1589/1700 train_time:154979ms step_avg:97.53ms
step:1590/1700 train_time:155080ms step_avg:97.53ms
step:1591/1700 train_time:155181ms step_avg:97.54ms
step:1592/1700 train_time:155282ms step_avg:97.54ms
step:1593/1700 train_time:155384ms step_avg:97.54ms
step:1594/1700 train_time:155488ms step_avg:97.55ms
step:1595/1700 train_time:155589ms step_avg:97.55ms
step:1596/1700 train_time:155690ms step_avg:97.55ms
step:1597/1700 train_time:155792ms step_avg:97.55ms
step:1598/1700 train_time:155894ms step_avg:97.56ms
step:1599/1700 train_time:155995ms step_avg:97.56ms
step:1600/1700 train_time:156097ms step_avg:97.56ms
step:1601/1700 train_time:156200ms step_avg:97.56ms
step:1602/1700 train_time:156301ms step_avg:97.57ms
step:1603/1700 train_time:156402ms step_avg:97.57ms
step:1604/1700 train_time:156503ms step_avg:97.57ms
step:1605/1700 train_time:156606ms step_avg:97.57ms
step:1606/1700 train_time:156708ms step_avg:97.58ms
step:1607/1700 train_time:156809ms step_avg:97.58ms
step:1608/1700 train_time:156911ms step_avg:97.58ms
step:1609/1700 train_time:157012ms step_avg:97.58ms
step:1610/1700 train_time:157115ms step_avg:97.59ms
step:1611/1700 train_time:157217ms step_avg:97.59ms
step:1612/1700 train_time:157318ms step_avg:97.59ms
step:1613/1700 train_time:157419ms step_avg:97.59ms
step:1614/1700 train_time:157520ms step_avg:97.60ms
step:1615/1700 train_time:157620ms step_avg:97.60ms
step:1616/1700 train_time:157722ms step_avg:97.60ms
step:1617/1700 train_time:157825ms step_avg:97.60ms
step:1618/1700 train_time:157928ms step_avg:97.61ms
step:1619/1700 train_time:158029ms step_avg:97.61ms
step:1620/1700 train_time:158131ms step_avg:97.61ms
step:1621/1700 train_time:158231ms step_avg:97.61ms
step:1622/1700 train_time:158332ms step_avg:97.62ms
step:1623/1700 train_time:158432ms step_avg:97.62ms
step:1624/1700 train_time:158536ms step_avg:97.62ms
step:1625/1700 train_time:158639ms step_avg:97.62ms
step:1625/1700 val_loss:3.2935 train_time:158723ms step_avg:97.68ms
step:1626/1700 train_time:158745ms step_avg:97.63ms
step:1627/1700 train_time:158853ms step_avg:97.64ms
step:1628/1700 train_time:158954ms step_avg:97.64ms
step:1629/1700 train_time:159054ms step_avg:97.64ms
step:1630/1700 train_time:159155ms step_avg:97.64ms
step:1631/1700 train_time:159255ms step_avg:97.64ms
step:1632/1700 train_time:159356ms step_avg:97.64ms
step:1633/1700 train_time:159456ms step_avg:97.65ms
step:1634/1700 train_time:159557ms step_avg:97.65ms
step:1635/1700 train_time:159657ms step_avg:97.65ms
step:1636/1700 train_time:159763ms step_avg:97.65ms
step:1637/1700 train_time:159865ms step_avg:97.66ms
step:1638/1700 train_time:159969ms step_avg:97.66ms
step:1639/1700 train_time:160070ms step_avg:97.66ms
step:1640/1700 train_time:160172ms step_avg:97.67ms
step:1641/1700 train_time:160274ms step_avg:97.67ms
step:1642/1700 train_time:160376ms step_avg:97.67ms
step:1643/1700 train_time:160478ms step_avg:97.67ms
step:1644/1700 train_time:160579ms step_avg:97.68ms
step:1645/1700 train_time:160681ms step_avg:97.68ms
step:1646/1700 train_time:160783ms step_avg:97.68ms
step:1647/1700 train_time:160888ms step_avg:97.69ms
step:1648/1700 train_time:160990ms step_avg:97.69ms
step:1649/1700 train_time:161092ms step_avg:97.69ms
step:1650/1700 train_time:161194ms step_avg:97.69ms
step:1651/1700 train_time:161296ms step_avg:97.70ms
step:1652/1700 train_time:161397ms step_avg:97.70ms
step:1653/1700 train_time:161499ms step_avg:97.70ms
step:1654/1700 train_time:161601ms step_avg:97.70ms
step:1655/1700 train_time:161704ms step_avg:97.71ms
step:1656/1700 train_time:161808ms step_avg:97.71ms
step:1657/1700 train_time:161908ms step_avg:97.71ms
step:1658/1700 train_time:162011ms step_avg:97.71ms
step:1659/1700 train_time:162118ms step_avg:97.72ms
step:1660/1700 train_time:162220ms step_avg:97.72ms
step:1661/1700 train_time:162324ms step_avg:97.73ms
step:1662/1700 train_time:162427ms step_avg:97.73ms
step:1663/1700 train_time:162529ms step_avg:97.73ms
step:1664/1700 train_time:162631ms step_avg:97.73ms
step:1665/1700 train_time:162734ms step_avg:97.74ms
step:1666/1700 train_time:162837ms step_avg:97.74ms
step:1667/1700 train_time:162939ms step_avg:97.74ms
step:1668/1700 train_time:163042ms step_avg:97.75ms
step:1669/1700 train_time:163146ms step_avg:97.75ms
step:1670/1700 train_time:163246ms step_avg:97.75ms
step:1671/1700 train_time:163347ms step_avg:97.75ms
step:1672/1700 train_time:163450ms step_avg:97.76ms
step:1673/1700 train_time:163551ms step_avg:97.76ms
step:1674/1700 train_time:163655ms step_avg:97.76ms
step:1675/1700 train_time:163756ms step_avg:97.76ms
step:1676/1700 train_time:163858ms step_avg:97.77ms
step:1677/1700 train_time:163959ms step_avg:97.77ms
step:1678/1700 train_time:164062ms step_avg:97.77ms
step:1679/1700 train_time:164166ms step_avg:97.78ms
step:1680/1700 train_time:164269ms step_avg:97.78ms
step:1681/1700 train_time:164371ms step_avg:97.78ms
step:1682/1700 train_time:164475ms step_avg:97.79ms
step:1683/1700 train_time:164576ms step_avg:97.79ms
step:1684/1700 train_time:164679ms step_avg:97.79ms
step:1685/1700 train_time:164781ms step_avg:97.79ms
step:1686/1700 train_time:164882ms step_avg:97.79ms
step:1687/1700 train_time:164985ms step_avg:97.80ms
step:1688/1700 train_time:165087ms step_avg:97.80ms
step:1689/1700 train_time:165189ms step_avg:97.80ms
step:1690/1700 train_time:165291ms step_avg:97.81ms
step:1691/1700 train_time:165393ms step_avg:97.81ms
step:1692/1700 train_time:165495ms step_avg:97.81ms
step:1693/1700 train_time:165596ms step_avg:97.81ms
step:1694/1700 train_time:165699ms step_avg:97.82ms
step:1695/1700 train_time:165800ms step_avg:97.82ms
step:1696/1700 train_time:165904ms step_avg:97.82ms
step:1697/1700 train_time:166008ms step_avg:97.82ms
step:1698/1700 train_time:166110ms step_avg:97.83ms
step:1699/1700 train_time:166212ms step_avg:97.83ms
step:1700/1700 train_time:166315ms step_avg:97.83ms
step:1700/1700 val_loss:3.2795 train_time:166397ms step_avg:97.88ms
peak memory allocated: 33278 MiB reserved: 48912 MiB

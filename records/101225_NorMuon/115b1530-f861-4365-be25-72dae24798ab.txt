import os
import sys
with open(sys.argv[0]) as f:
    code = f.read() # read the code of this file ASAP, for logging
import uuid
import time
import copy
import glob
from dataclasses import dataclass
from functools import lru_cache, partial # Added partial for hook registration
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch
torch.empty(1, device="cuda", requires_grad=True).backward() # prevents a bug on some systems
from torch import Tensor, nn
import torch.nn.functional as F
import torch.distributed as dist
# use of FlexAttention contributed by @KoszarskyB
from torch.nn.attention.flex_attention import BlockMask, flex_attention
#torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng

@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Muon optimizer

@torch.compile
def zeropower_via_newtonschulz5(G: Tensor, steps: int) -> Tensor:
    """
    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a
    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose
    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at
    zero even beyond the point where the iteration no longer converges all the way to one everywhere
    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T
    where S' is diagonal with S_{ii}' ~ Uniform(0.5, 1.5), which turns out not to hurt model
    performance at all relative to UV^T, where USV^T = G is the SVD.
    """
    assert G.ndim >= 2 # batched Muon implementation by @scottjmaddox, and put into practice in the record by @YouJiacheng
    a, b, c = (3.4445, -4.7750,  2.0315)
    X = G
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) + 1e-7)
    # Perform the NS iterations
    for _ in range(steps):
        A = X @ X.mT
        B = b * A + c * A @ A # quintic computation strategy adapted from suggestion by @jxbz, @leloykun, and @YouJiacheng
        X = a * X + B @ X

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

class NorMuon(torch.optim.Optimizer):

    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, beta2=0.95):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, beta2=beta2)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_reduce_futures: list[torch.Future] = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            grad = torch.empty_like(params[-1])
            grad_pad = [param.grad for param in params] + [torch.zeros_like(params[-1])] * world_size
            for base_i in range(0, len(params), world_size):
                if base_i + rank < len(params):
                    grad = params[base_i + rank].grad
                else:
                    grad = torch.zeros_like(grad)
                # This gives strange dynamo warnings
                reduce_scatter_futures.append(dist.reduce_scatter(grad, grad_pad[base_i:base_i + world_size], op=dist.ReduceOp.AVG, async_op=True).get_future())

        idx = 0
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            params_pad = params + [torch.empty_like(params[-1])] * world_size
            momentum = group["momentum"]
            beta2 = group["beta2"]
            for base_i in range(0, len(params), world_size):
                reduce_scatter_futures[idx].wait()
                if base_i + rank < len(params):
                    p = params[base_i + rank]
                    grad = p.grad
                    eff_lr = group["lr"] * max(1, p.size(-2) / p.size(-1)) ** 0.5 * getattr(p, "lr_mul", 1.0)
                    eff_weight_decay = group["lr"] * group["weight_decay"] * getattr(p, "wd_mul", 1.0)
                    state = self.state[p]
                    if len(state) == 0:
                        state["momentum_buffer"] = torch.zeros_like(grad)
                        state["second_momentum_buffer"] = torch.zeros_like(grad[..., 0:1]) if p.size(-2) >= p.size(-1) else torch.zeros_like(grad[0:1, ...])
                    momentum_buffer = state["momentum_buffer"]
                    second_momentum_buffer = state["second_momentum_buffer"]
                    p.mul_(1 - eff_weight_decay)
                    momentum_buffer.lerp_(grad, 1 - momentum)
                    grad = grad.lerp_(momentum_buffer, momentum)
                    v = zeropower_via_newtonschulz5(grad.bfloat16(), 5).float()
                    ###################################
                    vnorm = v.norm(dim=(-2,-1), keepdim=True)
                    v_mean = torch.mean(v * v, dim=-1, keepdim=True) if p.size(-2) >= p.size(-1) else torch.mean(v * v, dim=-2, keepdim=True)
                    second_momentum_buffer.lerp_(v_mean, 1 - beta2)
                    step_size = 1 / second_momentum_buffer.sqrt().add_(1e-10)
                    v.mul_(step_size)
                    vnorm_new = v.norm(dim=(-2,-1), keepdim=True)
                    v.mul_(vnorm / (vnorm_new + 1e-10))
                    ####################################
                    p.add_(other=v, alpha=-eff_lr)
                idx += 1
                all_reduce_futures.append(dist.all_gather(params_pad[base_i:base_i + world_size], params_pad[base_i + rank], async_op=True).get_future())
        torch.futures.collect_all(all_reduce_futures).wait()

class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_reduce_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            grad = torch.empty_like(params[-1])
            for base_i in range(len(params)):
                grad = params[base_i].grad
                rank_size = grad.shape[0] // world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for base in range(len(params)):
                reduce_scatter_futures[idx].wait()
                p = params[base]
                rank_size = p.shape[0] // world_size
                p_slice = p[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(p, "lr_mul", 1.0)
                state = self.state[p]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state['step'] = torch.tensor(0, dtype=torch.int64, device=p.device)
                    state['exp_avg'] = torch.zeros_like(p_slice)
                    state['exp_avg_sq'] = torch.zeros_like(p_slice)
                exp_avg = state['exp_avg']
                exp_avg_sq = state['exp_avg_sq']
                state['step'] += 1
                t = state['step']
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(p, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_reduce_futures.append(dist.all_gather_into_tensor(p, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_reduce_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

class Rotary(nn.Module):
    def __init__(self, dim: int, max_seq_len: int):
        super().__init__()
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=dim//4, dtype=torch.float32)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(dim//4)])
        t = torch.arange(max_seq_len, dtype=torch.float32)
        theta = torch.einsum("i,j -> ij", t, angular_freq)
        self.cos = nn.Buffer(theta.cos(), persistent=False)
        self.sin = nn.Buffer(theta.sin(), persistent=False)

    def forward(self, x_BTHD: Tensor):
        assert self.cos.size(0) >= x_BTHD.size(-3)
        cos, sin = self.cos[None, :x_BTHD.size(-3), None, :], self.sin[None, :x_BTHD.size(-3), None, :]
        x1, x2 = x_BTHD.to(dtype=torch.float32).chunk(2, dim=-1)
        y1 = x1 * cos + x2 * sin
        y2 = x1 * (-sin) + x2 * cos
        return torch.cat((y1, y2), 3).type_as(x_BTHD)

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, num_heads: int, max_seq_len: int, head_dim=128):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        hdim = num_heads * head_dim
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        self.qkv_w = nn.Parameter(torch.empty(3, hdim, dim).uniform_(-bound, bound))
        self.rotary = Rotary(head_dim, max_seq_len)
        self.c_proj = CastedLinear(hdim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977
        # scale the attention logits by given constant, instead of the default head_dim**-0.5, by @leloykun
        # inspired by learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.12

    def forward(self, x: Tensor, ve: Tensor | None, lambdas: Tensor, block_mask: BlockMask):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "Must use batch size = 1 for FlexAttention"
        q, k, v = F.linear(x, self.qkv_w.flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = self.rotary(q), self.rotary(k)
        if ve is not None:
            v = lambdas[0] * v + lambdas[1] * ve.view_as(v) # @KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = lambdas[0] * v
        y = flex_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), block_mask=block_mask, scale=self.attn_scale).transpose(1, 2)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = self.c_proj(y)
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        self.c_fc = CastedLinear(dim, hdim)
        self.c_proj = CastedLinear(hdim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = self.c_fc(x)
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = self.c_proj(x)
        return x

class Block(nn.Module):
    def __init__(self, dim: int, num_heads: int, max_seq_len: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, num_heads, max_seq_len) if layer_idx != 7 else None
        self.mlp = MLP(dim)

    def forward(self, x: Tensor, ve: Tensor | None, x0: Tensor, lambdas: Tensor, sa_lambdas: Tensor, block_mask: BlockMask):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), ve, sa_lambdas, block_mask)
        x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, num_heads, max_seq_len, i) for i in range(num_layers)])
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=True, x_s=(model_dim**0.5)/448, w_s=24/448, grad_s=1/448)
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5) % dist.get_world_size()
        self.scalars = nn.Parameter(torch.cat([
            torch.ones(num_layers), # skip_weights
            *[torch.tensor([1.0, 0.0]) for _ in range(num_layers)], # block lambdas
            *[torch.tensor([0.5, 0.5]) for _ in range(num_layers)], # SA lambdas
            torch.ones(pad),
        ]))
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 27.5
        self.scalars.lr_mul = 5.0

    def create_blockmasks(self, input_seq: Tensor, sliding_window_num_blocks: Tensor):
        BLOCK_SIZE = 128
        docs = (input_seq == 50256).cumsum(0)

        def document_causal(b, h, q_idx, kv_idx):
            causal_mask = q_idx >= kv_idx
            document_mask = docs[q_idx] == docs[kv_idx]
            return causal_mask & document_mask

        def dense_to_ordered(dense_blockmask: Tensor):
            num_blocks = dense_blockmask.sum(dim=-1, dtype=torch.int32)
            indices = dense_blockmask.argsort(dim=-1, descending=False, stable=True).flip(-1).to(torch.int32)
            return num_blocks[None, None].contiguous(), indices[None, None].contiguous()

        # manual block mask creation by @YouJiacheng
        assert len(input_seq) % BLOCK_SIZE == 0
        NUM_BLOCKS = len(input_seq) // BLOCK_SIZE
        block_idx = torch.arange(NUM_BLOCKS, dtype=torch.int32, device="cuda")
        causal_blockmask_any = block_idx[:, None] >= block_idx
        causal_blockmask_all = block_idx[:, None] > block_idx
        docs_low = docs.view(-1, BLOCK_SIZE)[:, 0].contiguous()
        docs_high = docs.view(-1, BLOCK_SIZE)[:, -1].contiguous()
        document_blockmask_any = (docs_low[:, None] <= docs_high) & (docs_high[:, None] >= docs_low)
        document_blockmask_all = (docs_low[:, None] == docs_high) & (docs_high[:, None] == docs_low)
        blockmask_any = causal_blockmask_any & document_blockmask_any
        blockmask_all = causal_blockmask_all & document_blockmask_all
        partial_kv_num_blocks, partial_kv_indices = dense_to_ordered(blockmask_any & ~blockmask_all)
        full_kv_num_blocks, full_kv_indices = dense_to_ordered(blockmask_all)
        def build_bm(window_size_blocks: Tensor) -> BlockMask:
            return BlockMask.from_kv_blocks(
                torch.clamp_max(partial_kv_num_blocks, torch.clamp_min(window_size_blocks - full_kv_num_blocks, 1)),
                partial_kv_indices,
                torch.clamp_max(full_kv_num_blocks, window_size_blocks - 1),
                full_kv_indices,
                BLOCK_SIZE=BLOCK_SIZE,
                mask_mod=document_causal,
            )
        # Long-short SWA block masks by @leloykun & @YouJiacheng, adapated from suggestion by @Grad62304977, following Gemma 2 paper
        return build_bm(sliding_window_num_blocks), build_bm(sliding_window_num_blocks // 2)

    def forward(self, input_seq: Tensor, target_seq: Tensor, sliding_window_num_blocks: Tensor):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        ve = [ve[0], ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        long_bm, short_bm = self.create_blockmasks(input_seq, sliding_window_num_blocks)
        block_masks = [long_bm, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, long_bm, short_bm, short_bm, short_bm, long_bm]
        assert len(block_masks) == len(self.blocks)

        x = x0 = norm(self.embed(input_seq)[None]) # use of norm here by @Grad62304977

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)

        n = len(self.blocks) // 2

        for i in range(len(self.blocks)):
            if i >= n:
                x = x + skip_weights[i - n] * skip_connections.pop()
            x = self.blocks[i](x, ve[i], x0, lambdas[i], sa_lambdas[i], block_masks[i])
            if i < n:
                skip_connections.append(x)

        x = norm(x)
        logits = self.lm_head(x).float()
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / (7.5 * x.size(-1)**0.5))
        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), target_seq, reduction="sum" if self.training else "mean")
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

# find world_size starting indicies, such that each begins with token 50256 and local_batches don't overlap
def find_batch_starts(tokens: Tensor, pos: int, local_batch_size: int, max_batch_span: int):
    boundary_mask = tokens[pos : pos + max_batch_span] == 50256
    boundary_positions = torch.nonzero(boundary_mask, as_tuple=False).squeeze(-1) + pos
    start = boundary_positions[0].item()
    starts = []
    for i in range(1, len(boundary_positions)):
        end = boundary_positions[i].item() 
        if end - start >= local_batch_size:
            starts.append(start) # append start once end pos is confirmed
            if len(starts) == dist.get_world_size():
                return starts, end - pos
            start = end
    assert False # increase max_batch_span if necessary

def distributed_data_generator(filename_pattern: str, batch_size: int, align_to_bos: bool):
    rank = dist.get_rank()
    world_size = dist.get_world_size()
    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    assert batch_size % world_size == 0
    local_batch_size = batch_size // world_size
    file_iter = iter(files) # use itertools.cycle(files) instead if you want to do multi-epoch training
    tokens, pos = _load_data_shard(next(file_iter)), 0
    max_batch_span = 2 * batch_size if align_to_bos else batch_size # provide buffer to handle samples up to length local_batch_size
    while True:
        if pos + max_batch_span + 1 >= len(tokens):
            tokens, pos = _load_data_shard(next(file_iter)), 0
        if align_to_bos:
            batch_starts, batch_span = find_batch_starts(tokens, pos, local_batch_size, max_batch_span)
            start_idx = batch_starts[rank]
        else:
            batch_span = batch_size
            start_idx = pos + rank * local_batch_size
        buf = tokens[start_idx:][:local_batch_size + 1]
        inputs = buf[:-1].to(device="cuda", dtype=torch.int32, non_blocking=True) # no sync on host side;
        targets = buf[1:].to(device="cuda", dtype=torch.int64, non_blocking=True) # H2D in another stream isn't helpful.
        pos += batch_span
        yield inputs, targets

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_seq_len = 48*1024 # FlexAttention sequence length
    val_seq_len = 4*64*1024 # FlexAttention sequence length for validation
    # optimization
    num_iterations = 1700 # number of iterations to run
    cooldown_frac = 0.45 # fraction of training spent cooling down the learning rate
    # evaluation and logging
    val_loss_every = 125 # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint = False
args = Hyperparameters()

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert world_size == 8 # this code is designed for 8xH100
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = uuid.uuid4()
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(vocab_size=50257, num_layers=12, num_heads=6, model_dim=768, max_seq_len=max(args.train_seq_len, args.val_seq_len)).cuda()
for m in model.modules():
    if isinstance(m, nn.Embedding):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(scalar_params + head_params + embed_params, lr=0.008, betas=(0.8, 0.95), eps=1e-10, weight_decay=0.0)
optimizer2 = NorMuon(hidden_matrix_params, lr=0.05, momentum=0.95, weight_decay=0.0, beta2=0.95)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: stable then decay
def get_lr(step: int):
    x = step / args.num_iterations # progress in training
    assert 0 <= x < 1
    if x < 1 - args.cooldown_frac:
        return 1.0
    else:
        w = (1 - x) / args.cooldown_frac
        return w * 1.0 + (1 - w) * 0.1

# attention window size schedule: linearly increase
@lru_cache(1)
def get_window_size_blocks_helper(window_size: int):
    return torch.tensor(window_size // 128, dtype=torch.int32, pin_memory=True).cuda(non_blocking=True)
def get_window_size_blocks(step: int):
    x = step / args.num_iterations # progress in training
    assert 0 <= x <= 1
    # Linearly increase the block-wise sliding window size over training 128 -> 1792
    # increase by @fernbear.bsky.social; block-wise by @YouJiacheng
    window_size = next_multiple_of_n(1728 * x, n=128)
    return get_window_size_blocks_helper(window_size)

model: nn.Module = torch.compile(model, dynamic=False)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 10
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, world_size * args.train_seq_len, align_to_bos=True)
for _ in range(warmup_steps):
    inputs, targets = next(train_loader)
    model(inputs, targets, get_window_size_blocks(1)).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, world_size * args.train_seq_len, align_to_bos=True)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
for step in range(train_steps + 1):
    last_step = (step == train_steps)

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        val_batch_size = world_size * args.val_seq_len
        assert args.val_tokens % val_batch_size == 0
        val_steps = args.val_tokens // val_batch_size
        val_loader = distributed_data_generator(args.val_files, val_batch_size, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets = next(val_loader)
                val_loss += model(inputs, targets, get_window_size_blocks(step))
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    inputs, targets = next(train_loader)
    model(inputs, targets, get_window_size_blocks(step)).backward()
    # set optimization hyperparameters
    for opt in optimizers:
        for group in opt.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)
    for group in optimizer2.param_groups:
        frac = min(step / 300, 1) # momentum warmup for muon
        group["momentum"] = (1 - frac) * 0.85 + frac * 0.95
    # step the optimizers
    for opt in optimizers:
        opt.step()
    # null the gradients
    model.zero_grad(set_to_none=True)
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()
====================================================================================================
Running Python 3.12.7 (main, Oct 11 2025, 17:06:43) [GCC 13.2.0]
Running PyTorch 2.10.0.dev20251011+cu126 compiled for CUDA 12.6
Mon Oct 13 04:14:33 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 565.57.01              Driver Version: 565.57.01      CUDA Version: 12.7     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000001:00:00.0 Off |                    0 |
| N/A   32C    P0            113W /  700W |    5858MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000002:00:00.0 Off |                    0 |
| N/A   32C    P0            114W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000003:00:00.0 Off |                    0 |
| N/A   29C    P0            118W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000008:00:00.0 Off |                    0 |
| N/A   29C    P0            116W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000009:00:00.0 Off |                    0 |
| N/A   27C    P0            112W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   0000000A:00:00.0 Off |                    0 |
| N/A   32C    P0            120W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   0000000B:00:00.0 Off |                    0 |
| N/A   28C    P0            116W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   0000000C:00:00.0 Off |                    0 |
| N/A   30C    P0            116W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
+-----------------------------------------------------------------------------------------+

====================================================================================================
step:0/1700 val_loss:10.8258 train_time:0ms step_avg:0.01ms
step:1/1700 train_time:137ms step_avg:137.40ms
step:2/1700 train_time:160ms step_avg:79.94ms
step:3/1700 train_time:246ms step_avg:82.01ms
step:4/1700 train_time:337ms step_avg:84.28ms
step:5/1700 train_time:429ms step_avg:85.89ms
step:6/1700 train_time:522ms step_avg:86.94ms
step:7/1700 train_time:614ms step_avg:87.77ms
step:8/1700 train_time:706ms step_avg:88.28ms
step:9/1700 train_time:798ms step_avg:88.71ms
step:10/1700 train_time:891ms step_avg:89.10ms
step:11/1700 train_time:983ms step_avg:89.36ms
step:12/1700 train_time:1077ms step_avg:89.74ms
step:13/1700 train_time:1173ms step_avg:90.27ms
step:14/1700 train_time:1268ms step_avg:90.60ms
step:15/1700 train_time:1362ms step_avg:90.80ms
step:16/1700 train_time:1455ms step_avg:90.92ms
step:17/1700 train_time:1547ms step_avg:91.03ms
step:18/1700 train_time:1639ms step_avg:91.07ms
step:19/1700 train_time:1732ms step_avg:91.15ms
step:20/1700 train_time:1824ms step_avg:91.20ms
step:21/1700 train_time:1917ms step_avg:91.28ms
step:22/1700 train_time:2009ms step_avg:91.33ms
step:23/1700 train_time:2103ms step_avg:91.43ms
step:24/1700 train_time:2196ms step_avg:91.52ms
step:25/1700 train_time:2291ms step_avg:91.63ms
step:26/1700 train_time:2384ms step_avg:91.71ms
step:27/1700 train_time:2478ms step_avg:91.77ms
step:28/1700 train_time:2571ms step_avg:91.83ms
step:29/1700 train_time:2663ms step_avg:91.83ms
step:30/1700 train_time:2755ms step_avg:91.84ms
step:31/1700 train_time:2847ms step_avg:91.85ms
step:32/1700 train_time:2940ms step_avg:91.87ms
step:33/1700 train_time:3033ms step_avg:91.91ms
step:34/1700 train_time:3126ms step_avg:91.93ms
step:35/1700 train_time:3219ms step_avg:91.97ms
step:36/1700 train_time:3313ms step_avg:92.02ms
step:37/1700 train_time:3406ms step_avg:92.06ms
step:38/1700 train_time:3500ms step_avg:92.12ms
step:39/1700 train_time:3593ms step_avg:92.14ms
step:40/1700 train_time:3686ms step_avg:92.16ms
step:41/1700 train_time:3779ms step_avg:92.16ms
step:42/1700 train_time:3872ms step_avg:92.19ms
step:43/1700 train_time:3964ms step_avg:92.18ms
step:44/1700 train_time:4057ms step_avg:92.19ms
step:45/1700 train_time:4150ms step_avg:92.22ms
step:46/1700 train_time:4243ms step_avg:92.23ms
step:47/1700 train_time:4336ms step_avg:92.25ms
step:48/1700 train_time:4429ms step_avg:92.27ms
step:49/1700 train_time:4523ms step_avg:92.30ms
step:50/1700 train_time:4616ms step_avg:92.32ms
step:51/1700 train_time:4709ms step_avg:92.33ms
step:52/1700 train_time:4802ms step_avg:92.34ms
step:53/1700 train_time:4895ms step_avg:92.36ms
step:54/1700 train_time:4988ms step_avg:92.37ms
step:55/1700 train_time:5080ms step_avg:92.37ms
step:56/1700 train_time:5173ms step_avg:92.38ms
step:57/1700 train_time:5266ms step_avg:92.39ms
step:58/1700 train_time:5359ms step_avg:92.39ms
step:59/1700 train_time:5452ms step_avg:92.40ms
step:60/1700 train_time:5544ms step_avg:92.41ms
step:61/1700 train_time:5637ms step_avg:92.41ms
step:62/1700 train_time:5730ms step_avg:92.42ms
step:63/1700 train_time:5823ms step_avg:92.42ms
step:64/1700 train_time:5915ms step_avg:92.42ms
step:65/1700 train_time:6007ms step_avg:92.42ms
step:66/1700 train_time:6100ms step_avg:92.43ms
step:67/1700 train_time:6193ms step_avg:92.44ms
step:68/1700 train_time:6286ms step_avg:92.45ms
step:69/1700 train_time:6378ms step_avg:92.44ms
step:70/1700 train_time:6472ms step_avg:92.45ms
step:71/1700 train_time:6564ms step_avg:92.45ms
step:72/1700 train_time:6656ms step_avg:92.45ms
step:73/1700 train_time:6749ms step_avg:92.46ms
step:74/1700 train_time:6842ms step_avg:92.46ms
step:75/1700 train_time:6935ms step_avg:92.46ms
step:76/1700 train_time:7027ms step_avg:92.46ms
step:77/1700 train_time:7120ms step_avg:92.47ms
step:78/1700 train_time:7213ms step_avg:92.48ms
step:79/1700 train_time:7306ms step_avg:92.48ms
step:80/1700 train_time:7398ms step_avg:92.48ms
step:81/1700 train_time:7491ms step_avg:92.48ms
step:82/1700 train_time:7584ms step_avg:92.49ms
step:83/1700 train_time:7676ms step_avg:92.49ms
step:84/1700 train_time:7770ms step_avg:92.50ms
step:85/1700 train_time:7863ms step_avg:92.50ms
step:86/1700 train_time:7956ms step_avg:92.51ms
step:87/1700 train_time:8049ms step_avg:92.51ms
step:88/1700 train_time:8141ms step_avg:92.51ms
step:89/1700 train_time:8234ms step_avg:92.52ms
step:90/1700 train_time:8326ms step_avg:92.51ms
step:91/1700 train_time:8419ms step_avg:92.51ms
step:92/1700 train_time:8512ms step_avg:92.52ms
step:93/1700 train_time:8605ms step_avg:92.53ms
step:94/1700 train_time:8698ms step_avg:92.53ms
step:95/1700 train_time:8791ms step_avg:92.53ms
step:96/1700 train_time:8883ms step_avg:92.54ms
step:97/1700 train_time:8976ms step_avg:92.54ms
step:98/1700 train_time:9069ms step_avg:92.54ms
step:99/1700 train_time:9162ms step_avg:92.54ms
step:100/1700 train_time:9254ms step_avg:92.54ms
step:101/1700 train_time:9347ms step_avg:92.55ms
step:102/1700 train_time:9440ms step_avg:92.55ms
step:103/1700 train_time:9533ms step_avg:92.55ms
step:104/1700 train_time:9625ms step_avg:92.55ms
step:105/1700 train_time:9717ms step_avg:92.55ms
step:106/1700 train_time:9810ms step_avg:92.55ms
step:107/1700 train_time:9903ms step_avg:92.55ms
step:108/1700 train_time:9996ms step_avg:92.56ms
step:109/1700 train_time:10089ms step_avg:92.56ms
step:110/1700 train_time:10182ms step_avg:92.56ms
step:111/1700 train_time:10275ms step_avg:92.57ms
step:112/1700 train_time:10368ms step_avg:92.57ms
step:113/1700 train_time:10461ms step_avg:92.58ms
step:114/1700 train_time:10554ms step_avg:92.58ms
step:115/1700 train_time:10646ms step_avg:92.58ms
step:116/1700 train_time:10739ms step_avg:92.58ms
step:117/1700 train_time:10832ms step_avg:92.58ms
step:118/1700 train_time:10925ms step_avg:92.58ms
step:119/1700 train_time:11017ms step_avg:92.58ms
step:120/1700 train_time:11111ms step_avg:92.59ms
step:121/1700 train_time:11203ms step_avg:92.59ms
step:122/1700 train_time:11296ms step_avg:92.59ms
step:123/1700 train_time:11389ms step_avg:92.59ms
step:124/1700 train_time:11482ms step_avg:92.60ms
step:125/1700 train_time:11576ms step_avg:92.60ms
step:125/1700 val_loss:4.6092 train_time:11652ms step_avg:93.22ms
step:126/1700 train_time:11676ms step_avg:92.67ms
step:127/1700 train_time:11768ms step_avg:92.66ms
step:128/1700 train_time:11869ms step_avg:92.73ms
step:129/1700 train_time:11964ms step_avg:92.75ms
step:130/1700 train_time:12057ms step_avg:92.75ms
step:131/1700 train_time:12150ms step_avg:92.74ms
step:132/1700 train_time:12243ms step_avg:92.75ms
step:133/1700 train_time:12335ms step_avg:92.74ms
step:134/1700 train_time:12427ms step_avg:92.74ms
step:135/1700 train_time:12520ms step_avg:92.74ms
step:136/1700 train_time:12613ms step_avg:92.74ms
step:137/1700 train_time:12706ms step_avg:92.75ms
step:138/1700 train_time:12801ms step_avg:92.76ms
step:139/1700 train_time:12896ms step_avg:92.78ms
step:140/1700 train_time:12990ms step_avg:92.78ms
step:141/1700 train_time:13083ms step_avg:92.79ms
step:142/1700 train_time:13176ms step_avg:92.79ms
step:143/1700 train_time:13269ms step_avg:92.79ms
step:144/1700 train_time:13363ms step_avg:92.80ms
step:145/1700 train_time:13455ms step_avg:92.80ms
step:146/1700 train_time:13548ms step_avg:92.79ms
step:147/1700 train_time:13641ms step_avg:92.80ms
step:148/1700 train_time:13734ms step_avg:92.80ms
step:149/1700 train_time:13828ms step_avg:92.81ms
step:150/1700 train_time:13923ms step_avg:92.82ms
step:151/1700 train_time:14018ms step_avg:92.83ms
step:152/1700 train_time:14111ms step_avg:92.83ms
step:153/1700 train_time:14205ms step_avg:92.84ms
step:154/1700 train_time:14298ms step_avg:92.84ms
step:155/1700 train_time:14390ms step_avg:92.84ms
step:156/1700 train_time:14484ms step_avg:92.84ms
step:157/1700 train_time:14576ms step_avg:92.84ms
step:158/1700 train_time:14669ms step_avg:92.84ms
step:159/1700 train_time:14763ms step_avg:92.85ms
step:160/1700 train_time:14857ms step_avg:92.85ms
step:161/1700 train_time:14950ms step_avg:92.86ms
step:162/1700 train_time:15045ms step_avg:92.87ms
step:163/1700 train_time:15138ms step_avg:92.87ms
step:164/1700 train_time:15231ms step_avg:92.87ms
step:165/1700 train_time:15324ms step_avg:92.87ms
step:166/1700 train_time:15417ms step_avg:92.87ms
step:167/1700 train_time:15510ms step_avg:92.87ms
step:168/1700 train_time:15604ms step_avg:92.88ms
step:169/1700 train_time:15696ms step_avg:92.88ms
step:170/1700 train_time:15790ms step_avg:92.88ms
step:171/1700 train_time:15883ms step_avg:92.89ms
step:172/1700 train_time:15977ms step_avg:92.89ms
step:173/1700 train_time:16070ms step_avg:92.89ms
step:174/1700 train_time:16164ms step_avg:92.90ms
step:175/1700 train_time:16257ms step_avg:92.90ms
step:176/1700 train_time:16351ms step_avg:92.90ms
step:177/1700 train_time:16444ms step_avg:92.90ms
step:178/1700 train_time:16537ms step_avg:92.90ms
step:179/1700 train_time:16630ms step_avg:92.90ms
step:180/1700 train_time:16724ms step_avg:92.91ms
step:181/1700 train_time:16817ms step_avg:92.91ms
step:182/1700 train_time:16911ms step_avg:92.91ms
step:183/1700 train_time:17005ms step_avg:92.92ms
step:184/1700 train_time:17098ms step_avg:92.93ms
step:185/1700 train_time:17193ms step_avg:92.93ms
step:186/1700 train_time:17286ms step_avg:92.93ms
step:187/1700 train_time:17379ms step_avg:92.94ms
step:188/1700 train_time:17472ms step_avg:92.94ms
step:189/1700 train_time:17565ms step_avg:92.94ms
step:190/1700 train_time:17658ms step_avg:92.94ms
step:191/1700 train_time:17752ms step_avg:92.94ms
step:192/1700 train_time:17846ms step_avg:92.95ms
step:193/1700 train_time:17939ms step_avg:92.95ms
step:194/1700 train_time:18033ms step_avg:92.95ms
step:195/1700 train_time:18126ms step_avg:92.96ms
step:196/1700 train_time:18219ms step_avg:92.96ms
step:197/1700 train_time:18312ms step_avg:92.96ms
step:198/1700 train_time:18406ms step_avg:92.96ms
step:199/1700 train_time:18498ms step_avg:92.96ms
step:200/1700 train_time:18591ms step_avg:92.96ms
step:201/1700 train_time:18685ms step_avg:92.96ms
step:202/1700 train_time:18777ms step_avg:92.96ms
step:203/1700 train_time:18870ms step_avg:92.96ms
step:204/1700 train_time:18964ms step_avg:92.96ms
step:205/1700 train_time:19058ms step_avg:92.96ms
step:206/1700 train_time:19151ms step_avg:92.97ms
step:207/1700 train_time:19245ms step_avg:92.97ms
step:208/1700 train_time:19338ms step_avg:92.97ms
step:209/1700 train_time:19431ms step_avg:92.97ms
step:210/1700 train_time:19524ms step_avg:92.97ms
step:211/1700 train_time:19617ms step_avg:92.97ms
step:212/1700 train_time:19710ms step_avg:92.97ms
step:213/1700 train_time:19804ms step_avg:92.98ms
step:214/1700 train_time:19897ms step_avg:92.98ms
step:215/1700 train_time:19991ms step_avg:92.98ms
step:216/1700 train_time:20084ms step_avg:92.98ms
step:217/1700 train_time:20177ms step_avg:92.98ms
step:218/1700 train_time:20270ms step_avg:92.98ms
step:219/1700 train_time:20364ms step_avg:92.98ms
step:220/1700 train_time:20457ms step_avg:92.99ms
step:221/1700 train_time:20549ms step_avg:92.98ms
step:222/1700 train_time:20644ms step_avg:92.99ms
step:223/1700 train_time:20737ms step_avg:92.99ms
step:224/1700 train_time:20830ms step_avg:92.99ms
step:225/1700 train_time:20924ms step_avg:92.99ms
step:226/1700 train_time:21017ms step_avg:92.99ms
step:227/1700 train_time:21110ms step_avg:93.00ms
step:228/1700 train_time:21204ms step_avg:93.00ms
step:229/1700 train_time:21298ms step_avg:93.00ms
step:230/1700 train_time:21391ms step_avg:93.01ms
step:231/1700 train_time:21485ms step_avg:93.01ms
step:232/1700 train_time:21578ms step_avg:93.01ms
step:233/1700 train_time:21671ms step_avg:93.01ms
step:234/1700 train_time:21765ms step_avg:93.01ms
step:235/1700 train_time:21858ms step_avg:93.01ms
step:236/1700 train_time:21951ms step_avg:93.01ms
step:237/1700 train_time:22045ms step_avg:93.02ms
step:238/1700 train_time:22138ms step_avg:93.02ms
step:239/1700 train_time:22231ms step_avg:93.02ms
step:240/1700 train_time:22326ms step_avg:93.02ms
step:241/1700 train_time:22420ms step_avg:93.03ms
step:242/1700 train_time:22513ms step_avg:93.03ms
step:243/1700 train_time:22606ms step_avg:93.03ms
step:244/1700 train_time:22700ms step_avg:93.03ms
step:245/1700 train_time:22793ms step_avg:93.03ms
step:246/1700 train_time:22886ms step_avg:93.03ms
step:247/1700 train_time:22980ms step_avg:93.03ms
step:248/1700 train_time:23073ms step_avg:93.04ms
step:249/1700 train_time:23166ms step_avg:93.04ms
step:250/1700 train_time:23260ms step_avg:93.04ms
step:250/1700 val_loss:4.0719 train_time:23337ms step_avg:93.35ms
step:251/1700 train_time:23361ms step_avg:93.07ms
step:252/1700 train_time:23455ms step_avg:93.08ms
step:253/1700 train_time:23554ms step_avg:93.10ms
step:254/1700 train_time:23647ms step_avg:93.10ms
step:255/1700 train_time:23740ms step_avg:93.10ms
step:256/1700 train_time:23833ms step_avg:93.10ms
step:257/1700 train_time:23927ms step_avg:93.10ms
step:258/1700 train_time:24020ms step_avg:93.10ms
step:259/1700 train_time:24113ms step_avg:93.10ms
step:260/1700 train_time:24205ms step_avg:93.10ms
step:261/1700 train_time:24300ms step_avg:93.10ms
step:262/1700 train_time:24394ms step_avg:93.11ms
step:263/1700 train_time:24489ms step_avg:93.12ms
step:264/1700 train_time:24584ms step_avg:93.12ms
step:265/1700 train_time:24678ms step_avg:93.12ms
step:266/1700 train_time:24772ms step_avg:93.13ms
step:267/1700 train_time:24865ms step_avg:93.13ms
step:268/1700 train_time:24959ms step_avg:93.13ms
step:269/1700 train_time:25053ms step_avg:93.13ms
step:270/1700 train_time:25145ms step_avg:93.13ms
step:271/1700 train_time:25239ms step_avg:93.13ms
step:272/1700 train_time:25333ms step_avg:93.14ms
step:273/1700 train_time:25427ms step_avg:93.14ms
step:274/1700 train_time:25522ms step_avg:93.14ms
step:275/1700 train_time:25616ms step_avg:93.15ms
step:276/1700 train_time:25710ms step_avg:93.15ms
step:277/1700 train_time:25804ms step_avg:93.15ms
step:278/1700 train_time:25898ms step_avg:93.16ms
step:279/1700 train_time:25991ms step_avg:93.16ms
step:280/1700 train_time:26084ms step_avg:93.16ms
step:281/1700 train_time:26178ms step_avg:93.16ms
step:282/1700 train_time:26271ms step_avg:93.16ms
step:283/1700 train_time:26364ms step_avg:93.16ms
step:284/1700 train_time:26458ms step_avg:93.16ms
step:285/1700 train_time:26553ms step_avg:93.17ms
step:286/1700 train_time:26647ms step_avg:93.17ms
step:287/1700 train_time:26741ms step_avg:93.17ms
step:288/1700 train_time:26835ms step_avg:93.18ms
step:289/1700 train_time:26928ms step_avg:93.18ms
step:290/1700 train_time:27022ms step_avg:93.18ms
step:291/1700 train_time:27116ms step_avg:93.18ms
step:292/1700 train_time:27209ms step_avg:93.18ms
step:293/1700 train_time:27302ms step_avg:93.18ms
step:294/1700 train_time:27396ms step_avg:93.18ms
step:295/1700 train_time:27490ms step_avg:93.19ms
step:296/1700 train_time:27584ms step_avg:93.19ms
step:297/1700 train_time:27678ms step_avg:93.19ms
step:298/1700 train_time:27772ms step_avg:93.19ms
step:299/1700 train_time:27866ms step_avg:93.20ms
step:300/1700 train_time:27960ms step_avg:93.20ms
step:301/1700 train_time:28054ms step_avg:93.20ms
step:302/1700 train_time:28147ms step_avg:93.20ms
step:303/1700 train_time:28240ms step_avg:93.20ms
step:304/1700 train_time:28334ms step_avg:93.20ms
step:305/1700 train_time:28427ms step_avg:93.20ms
step:306/1700 train_time:28521ms step_avg:93.21ms
step:307/1700 train_time:28615ms step_avg:93.21ms
step:308/1700 train_time:28709ms step_avg:93.21ms
step:309/1700 train_time:28803ms step_avg:93.21ms
step:310/1700 train_time:28897ms step_avg:93.21ms
step:311/1700 train_time:28991ms step_avg:93.22ms
step:312/1700 train_time:29084ms step_avg:93.22ms
step:313/1700 train_time:29178ms step_avg:93.22ms
step:314/1700 train_time:29272ms step_avg:93.22ms
step:315/1700 train_time:29364ms step_avg:93.22ms
step:316/1700 train_time:29459ms step_avg:93.22ms
step:317/1700 train_time:29553ms step_avg:93.23ms
step:318/1700 train_time:29647ms step_avg:93.23ms
step:319/1700 train_time:29740ms step_avg:93.23ms
step:320/1700 train_time:29834ms step_avg:93.23ms
step:321/1700 train_time:29928ms step_avg:93.23ms
step:322/1700 train_time:30022ms step_avg:93.24ms
step:323/1700 train_time:30116ms step_avg:93.24ms
step:324/1700 train_time:30209ms step_avg:93.24ms
step:325/1700 train_time:30303ms step_avg:93.24ms
step:326/1700 train_time:30397ms step_avg:93.24ms
step:327/1700 train_time:30490ms step_avg:93.24ms
step:328/1700 train_time:30584ms step_avg:93.24ms
step:329/1700 train_time:30678ms step_avg:93.25ms
step:330/1700 train_time:30772ms step_avg:93.25ms
step:331/1700 train_time:30867ms step_avg:93.25ms
step:332/1700 train_time:30960ms step_avg:93.25ms
step:333/1700 train_time:31054ms step_avg:93.25ms
step:334/1700 train_time:31148ms step_avg:93.26ms
step:335/1700 train_time:31242ms step_avg:93.26ms
step:336/1700 train_time:31335ms step_avg:93.26ms
step:337/1700 train_time:31429ms step_avg:93.26ms
step:338/1700 train_time:31523ms step_avg:93.26ms
step:339/1700 train_time:31617ms step_avg:93.26ms
step:340/1700 train_time:31710ms step_avg:93.27ms
step:341/1700 train_time:31804ms step_avg:93.27ms
step:342/1700 train_time:31898ms step_avg:93.27ms
step:343/1700 train_time:31992ms step_avg:93.27ms
step:344/1700 train_time:32086ms step_avg:93.27ms
step:345/1700 train_time:32179ms step_avg:93.27ms
step:346/1700 train_time:32273ms step_avg:93.28ms
step:347/1700 train_time:32367ms step_avg:93.28ms
step:348/1700 train_time:32461ms step_avg:93.28ms
step:349/1700 train_time:32555ms step_avg:93.28ms
step:350/1700 train_time:32650ms step_avg:93.28ms
step:351/1700 train_time:32744ms step_avg:93.29ms
step:352/1700 train_time:32838ms step_avg:93.29ms
step:353/1700 train_time:32932ms step_avg:93.29ms
step:354/1700 train_time:33026ms step_avg:93.29ms
step:355/1700 train_time:33120ms step_avg:93.29ms
step:356/1700 train_time:33213ms step_avg:93.30ms
step:357/1700 train_time:33307ms step_avg:93.30ms
step:358/1700 train_time:33401ms step_avg:93.30ms
step:359/1700 train_time:33494ms step_avg:93.30ms
step:360/1700 train_time:33588ms step_avg:93.30ms
step:361/1700 train_time:33682ms step_avg:93.30ms
step:362/1700 train_time:33775ms step_avg:93.30ms
step:363/1700 train_time:33869ms step_avg:93.30ms
step:364/1700 train_time:33963ms step_avg:93.31ms
step:365/1700 train_time:34057ms step_avg:93.31ms
step:366/1700 train_time:34150ms step_avg:93.31ms
step:367/1700 train_time:34244ms step_avg:93.31ms
step:368/1700 train_time:34338ms step_avg:93.31ms
step:369/1700 train_time:34432ms step_avg:93.31ms
step:370/1700 train_time:34525ms step_avg:93.31ms
step:371/1700 train_time:34618ms step_avg:93.31ms
step:372/1700 train_time:34712ms step_avg:93.31ms
step:373/1700 train_time:34806ms step_avg:93.31ms
step:374/1700 train_time:34900ms step_avg:93.32ms
step:375/1700 train_time:34994ms step_avg:93.32ms
step:375/1700 val_loss:3.8785 train_time:35071ms step_avg:93.52ms
step:376/1700 train_time:35094ms step_avg:93.34ms
step:377/1700 train_time:35187ms step_avg:93.33ms
step:378/1700 train_time:35283ms step_avg:93.34ms
step:379/1700 train_time:35377ms step_avg:93.34ms
step:380/1700 train_time:35472ms step_avg:93.35ms
step:381/1700 train_time:35568ms step_avg:93.36ms
step:382/1700 train_time:35663ms step_avg:93.36ms
step:383/1700 train_time:35758ms step_avg:93.36ms
step:384/1700 train_time:35853ms step_avg:93.37ms
step:385/1700 train_time:35947ms step_avg:93.37ms
step:386/1700 train_time:36043ms step_avg:93.38ms
step:387/1700 train_time:36140ms step_avg:93.39ms
step:388/1700 train_time:36237ms step_avg:93.39ms
step:389/1700 train_time:36333ms step_avg:93.40ms
step:390/1700 train_time:36429ms step_avg:93.41ms
step:391/1700 train_time:36524ms step_avg:93.41ms
step:392/1700 train_time:36620ms step_avg:93.42ms
step:393/1700 train_time:36715ms step_avg:93.42ms
step:394/1700 train_time:36809ms step_avg:93.42ms
step:395/1700 train_time:36903ms step_avg:93.43ms
step:396/1700 train_time:36999ms step_avg:93.43ms
step:397/1700 train_time:37096ms step_avg:93.44ms
step:398/1700 train_time:37192ms step_avg:93.45ms
step:399/1700 train_time:37288ms step_avg:93.45ms
step:400/1700 train_time:37384ms step_avg:93.46ms
step:401/1700 train_time:37479ms step_avg:93.46ms
step:402/1700 train_time:37575ms step_avg:93.47ms
step:403/1700 train_time:37672ms step_avg:93.48ms
step:404/1700 train_time:37767ms step_avg:93.48ms
step:405/1700 train_time:37862ms step_avg:93.49ms
step:406/1700 train_time:37957ms step_avg:93.49ms
step:407/1700 train_time:38054ms step_avg:93.50ms
step:408/1700 train_time:38151ms step_avg:93.51ms
step:409/1700 train_time:38247ms step_avg:93.51ms
step:410/1700 train_time:38342ms step_avg:93.52ms
step:411/1700 train_time:38438ms step_avg:93.52ms
step:412/1700 train_time:38535ms step_avg:93.53ms
step:413/1700 train_time:38630ms step_avg:93.54ms
step:414/1700 train_time:38725ms step_avg:93.54ms
step:415/1700 train_time:38821ms step_avg:93.54ms
step:416/1700 train_time:38916ms step_avg:93.55ms
step:417/1700 train_time:39011ms step_avg:93.55ms
step:418/1700 train_time:39106ms step_avg:93.56ms
step:419/1700 train_time:39202ms step_avg:93.56ms
step:420/1700 train_time:39298ms step_avg:93.57ms
step:421/1700 train_time:39394ms step_avg:93.57ms
step:422/1700 train_time:39490ms step_avg:93.58ms
step:423/1700 train_time:39585ms step_avg:93.58ms
step:424/1700 train_time:39681ms step_avg:93.59ms
step:425/1700 train_time:39777ms step_avg:93.59ms
step:426/1700 train_time:39872ms step_avg:93.60ms
step:427/1700 train_time:39967ms step_avg:93.60ms
step:428/1700 train_time:40063ms step_avg:93.60ms
step:429/1700 train_time:40158ms step_avg:93.61ms
step:430/1700 train_time:40254ms step_avg:93.61ms
step:431/1700 train_time:40350ms step_avg:93.62ms
step:432/1700 train_time:40446ms step_avg:93.63ms
step:433/1700 train_time:40541ms step_avg:93.63ms
step:434/1700 train_time:40637ms step_avg:93.63ms
step:435/1700 train_time:40733ms step_avg:93.64ms
step:436/1700 train_time:40828ms step_avg:93.64ms
step:437/1700 train_time:40924ms step_avg:93.65ms
step:438/1700 train_time:41019ms step_avg:93.65ms
step:439/1700 train_time:41116ms step_avg:93.66ms
step:440/1700 train_time:41212ms step_avg:93.66ms
step:441/1700 train_time:41308ms step_avg:93.67ms
step:442/1700 train_time:41404ms step_avg:93.67ms
step:443/1700 train_time:41500ms step_avg:93.68ms
step:444/1700 train_time:41596ms step_avg:93.68ms
step:445/1700 train_time:41692ms step_avg:93.69ms
step:446/1700 train_time:41788ms step_avg:93.69ms
step:447/1700 train_time:41883ms step_avg:93.70ms
step:448/1700 train_time:41979ms step_avg:93.70ms
step:449/1700 train_time:42075ms step_avg:93.71ms
step:450/1700 train_time:42171ms step_avg:93.71ms
step:451/1700 train_time:42267ms step_avg:93.72ms
step:452/1700 train_time:42362ms step_avg:93.72ms
step:453/1700 train_time:42458ms step_avg:93.73ms
step:454/1700 train_time:42554ms step_avg:93.73ms
step:455/1700 train_time:42649ms step_avg:93.73ms
step:456/1700 train_time:42745ms step_avg:93.74ms
step:457/1700 train_time:42841ms step_avg:93.74ms
step:458/1700 train_time:42937ms step_avg:93.75ms
step:459/1700 train_time:43034ms step_avg:93.75ms
step:460/1700 train_time:43128ms step_avg:93.76ms
step:461/1700 train_time:43224ms step_avg:93.76ms
step:462/1700 train_time:43320ms step_avg:93.77ms
step:463/1700 train_time:43416ms step_avg:93.77ms
step:464/1700 train_time:43513ms step_avg:93.78ms
step:465/1700 train_time:43607ms step_avg:93.78ms
step:466/1700 train_time:43703ms step_avg:93.78ms
step:467/1700 train_time:43798ms step_avg:93.79ms
step:468/1700 train_time:43894ms step_avg:93.79ms
step:469/1700 train_time:43991ms step_avg:93.80ms
step:470/1700 train_time:44086ms step_avg:93.80ms
step:471/1700 train_time:44181ms step_avg:93.80ms
step:472/1700 train_time:44278ms step_avg:93.81ms
step:473/1700 train_time:44372ms step_avg:93.81ms
step:474/1700 train_time:44468ms step_avg:93.81ms
step:475/1700 train_time:44563ms step_avg:93.82ms
step:476/1700 train_time:44658ms step_avg:93.82ms
step:477/1700 train_time:44754ms step_avg:93.82ms
step:478/1700 train_time:44850ms step_avg:93.83ms
step:479/1700 train_time:44946ms step_avg:93.83ms
step:480/1700 train_time:45042ms step_avg:93.84ms
step:481/1700 train_time:45137ms step_avg:93.84ms
step:482/1700 train_time:45234ms step_avg:93.85ms
step:483/1700 train_time:45329ms step_avg:93.85ms
step:484/1700 train_time:45424ms step_avg:93.85ms
step:485/1700 train_time:45520ms step_avg:93.85ms
step:486/1700 train_time:45615ms step_avg:93.86ms
step:487/1700 train_time:45711ms step_avg:93.86ms
step:488/1700 train_time:45806ms step_avg:93.87ms
step:489/1700 train_time:45902ms step_avg:93.87ms
step:490/1700 train_time:45997ms step_avg:93.87ms
step:491/1700 train_time:46093ms step_avg:93.88ms
step:492/1700 train_time:46188ms step_avg:93.88ms
step:493/1700 train_time:46283ms step_avg:93.88ms
step:494/1700 train_time:46379ms step_avg:93.88ms
step:495/1700 train_time:46476ms step_avg:93.89ms
step:496/1700 train_time:46573ms step_avg:93.90ms
step:497/1700 train_time:46669ms step_avg:93.90ms
step:498/1700 train_time:46764ms step_avg:93.90ms
step:499/1700 train_time:46859ms step_avg:93.91ms
step:500/1700 train_time:46954ms step_avg:93.91ms
step:500/1700 val_loss:3.7307 train_time:47032ms step_avg:94.06ms
step:501/1700 train_time:47056ms step_avg:93.92ms
step:502/1700 train_time:47153ms step_avg:93.93ms
step:503/1700 train_time:47251ms step_avg:93.94ms
step:504/1700 train_time:47346ms step_avg:93.94ms
step:505/1700 train_time:47442ms step_avg:93.94ms
step:506/1700 train_time:47537ms step_avg:93.95ms
step:507/1700 train_time:47632ms step_avg:93.95ms
step:508/1700 train_time:47726ms step_avg:93.95ms
step:509/1700 train_time:47821ms step_avg:93.95ms
step:510/1700 train_time:47917ms step_avg:93.95ms
step:511/1700 train_time:48014ms step_avg:93.96ms
step:512/1700 train_time:48111ms step_avg:93.97ms
step:513/1700 train_time:48208ms step_avg:93.97ms
step:514/1700 train_time:48304ms step_avg:93.98ms
step:515/1700 train_time:48400ms step_avg:93.98ms
step:516/1700 train_time:48497ms step_avg:93.99ms
step:517/1700 train_time:48593ms step_avg:93.99ms
step:518/1700 train_time:48689ms step_avg:93.99ms
step:519/1700 train_time:48785ms step_avg:94.00ms
step:520/1700 train_time:48880ms step_avg:94.00ms
step:521/1700 train_time:48975ms step_avg:94.00ms
step:522/1700 train_time:49072ms step_avg:94.01ms
step:523/1700 train_time:49168ms step_avg:94.01ms
step:524/1700 train_time:49264ms step_avg:94.02ms
step:525/1700 train_time:49361ms step_avg:94.02ms
step:526/1700 train_time:49457ms step_avg:94.03ms
step:527/1700 train_time:49554ms step_avg:94.03ms
step:528/1700 train_time:49649ms step_avg:94.03ms
step:529/1700 train_time:49744ms step_avg:94.03ms
step:530/1700 train_time:49840ms step_avg:94.04ms
step:531/1700 train_time:49936ms step_avg:94.04ms
step:532/1700 train_time:50032ms step_avg:94.04ms
step:533/1700 train_time:50128ms step_avg:94.05ms
step:534/1700 train_time:50224ms step_avg:94.05ms
step:535/1700 train_time:50320ms step_avg:94.06ms
step:536/1700 train_time:50417ms step_avg:94.06ms
step:537/1700 train_time:50514ms step_avg:94.07ms
step:538/1700 train_time:50609ms step_avg:94.07ms
step:539/1700 train_time:50704ms step_avg:94.07ms
step:540/1700 train_time:50800ms step_avg:94.07ms
step:541/1700 train_time:50897ms step_avg:94.08ms
step:542/1700 train_time:50992ms step_avg:94.08ms
step:543/1700 train_time:51088ms step_avg:94.08ms
step:544/1700 train_time:51184ms step_avg:94.09ms
step:545/1700 train_time:51280ms step_avg:94.09ms
step:546/1700 train_time:51377ms step_avg:94.10ms
step:547/1700 train_time:51473ms step_avg:94.10ms
step:548/1700 train_time:51569ms step_avg:94.10ms
step:549/1700 train_time:51665ms step_avg:94.11ms
step:550/1700 train_time:51761ms step_avg:94.11ms
step:551/1700 train_time:51857ms step_avg:94.11ms
step:552/1700 train_time:51953ms step_avg:94.12ms
step:553/1700 train_time:52049ms step_avg:94.12ms
step:554/1700 train_time:52145ms step_avg:94.12ms
step:555/1700 train_time:52241ms step_avg:94.13ms
step:556/1700 train_time:52337ms step_avg:94.13ms
step:557/1700 train_time:52435ms step_avg:94.14ms
step:558/1700 train_time:52531ms step_avg:94.14ms
step:559/1700 train_time:52627ms step_avg:94.14ms
step:560/1700 train_time:52723ms step_avg:94.15ms
step:561/1700 train_time:52819ms step_avg:94.15ms
step:562/1700 train_time:52916ms step_avg:94.16ms
step:563/1700 train_time:53012ms step_avg:94.16ms
step:564/1700 train_time:53107ms step_avg:94.16ms
step:565/1700 train_time:53203ms step_avg:94.16ms
step:566/1700 train_time:53299ms step_avg:94.17ms
step:567/1700 train_time:53395ms step_avg:94.17ms
step:568/1700 train_time:53491ms step_avg:94.17ms
step:569/1700 train_time:53587ms step_avg:94.18ms
step:570/1700 train_time:53683ms step_avg:94.18ms
step:571/1700 train_time:53778ms step_avg:94.18ms
step:572/1700 train_time:53874ms step_avg:94.19ms
step:573/1700 train_time:53970ms step_avg:94.19ms
step:574/1700 train_time:54066ms step_avg:94.19ms
step:575/1700 train_time:54162ms step_avg:94.19ms
step:576/1700 train_time:54257ms step_avg:94.20ms
step:577/1700 train_time:54354ms step_avg:94.20ms
step:578/1700 train_time:54449ms step_avg:94.20ms
step:579/1700 train_time:54545ms step_avg:94.21ms
step:580/1700 train_time:54641ms step_avg:94.21ms
step:581/1700 train_time:54736ms step_avg:94.21ms
step:582/1700 train_time:54832ms step_avg:94.21ms
step:583/1700 train_time:54928ms step_avg:94.22ms
step:584/1700 train_time:55024ms step_avg:94.22ms
step:585/1700 train_time:55119ms step_avg:94.22ms
step:586/1700 train_time:55215ms step_avg:94.22ms
step:587/1700 train_time:55311ms step_avg:94.23ms
step:588/1700 train_time:55407ms step_avg:94.23ms
step:589/1700 train_time:55503ms step_avg:94.23ms
step:590/1700 train_time:55599ms step_avg:94.24ms
step:591/1700 train_time:55696ms step_avg:94.24ms
step:592/1700 train_time:55792ms step_avg:94.24ms
step:593/1700 train_time:55888ms step_avg:94.25ms
step:594/1700 train_time:55984ms step_avg:94.25ms
step:595/1700 train_time:56079ms step_avg:94.25ms
step:596/1700 train_time:56175ms step_avg:94.25ms
step:597/1700 train_time:56271ms step_avg:94.26ms
step:598/1700 train_time:56366ms step_avg:94.26ms
step:599/1700 train_time:56463ms step_avg:94.26ms
step:600/1700 train_time:56559ms step_avg:94.26ms
step:601/1700 train_time:56655ms step_avg:94.27ms
step:602/1700 train_time:56750ms step_avg:94.27ms
step:603/1700 train_time:56846ms step_avg:94.27ms
step:604/1700 train_time:56943ms step_avg:94.28ms
step:605/1700 train_time:57038ms step_avg:94.28ms
step:606/1700 train_time:57135ms step_avg:94.28ms
step:607/1700 train_time:57231ms step_avg:94.28ms
step:608/1700 train_time:57327ms step_avg:94.29ms
step:609/1700 train_time:57423ms step_avg:94.29ms
step:610/1700 train_time:57518ms step_avg:94.29ms
step:611/1700 train_time:57615ms step_avg:94.30ms
step:612/1700 train_time:57711ms step_avg:94.30ms
step:613/1700 train_time:57807ms step_avg:94.30ms
step:614/1700 train_time:57902ms step_avg:94.30ms
step:615/1700 train_time:57998ms step_avg:94.31ms
step:616/1700 train_time:58093ms step_avg:94.31ms
step:617/1700 train_time:58189ms step_avg:94.31ms
step:618/1700 train_time:58285ms step_avg:94.31ms
step:619/1700 train_time:58381ms step_avg:94.32ms
step:620/1700 train_time:58477ms step_avg:94.32ms
step:621/1700 train_time:58573ms step_avg:94.32ms
step:622/1700 train_time:58669ms step_avg:94.32ms
step:623/1700 train_time:58764ms step_avg:94.32ms
step:624/1700 train_time:58860ms step_avg:94.33ms
step:625/1700 train_time:58956ms step_avg:94.33ms
step:625/1700 val_loss:3.6474 train_time:59035ms step_avg:94.46ms
step:626/1700 train_time:59058ms step_avg:94.34ms
step:627/1700 train_time:59154ms step_avg:94.35ms
step:628/1700 train_time:59252ms step_avg:94.35ms
step:629/1700 train_time:59348ms step_avg:94.35ms
step:630/1700 train_time:59444ms step_avg:94.36ms
step:631/1700 train_time:59540ms step_avg:94.36ms
step:632/1700 train_time:59636ms step_avg:94.36ms
step:633/1700 train_time:59733ms step_avg:94.36ms
step:634/1700 train_time:59829ms step_avg:94.37ms
step:635/1700 train_time:59927ms step_avg:94.37ms
step:636/1700 train_time:60024ms step_avg:94.38ms
step:637/1700 train_time:60123ms step_avg:94.38ms
step:638/1700 train_time:60222ms step_avg:94.39ms
step:639/1700 train_time:60320ms step_avg:94.40ms
step:640/1700 train_time:60418ms step_avg:94.40ms
step:641/1700 train_time:60515ms step_avg:94.41ms
step:642/1700 train_time:60613ms step_avg:94.41ms
step:643/1700 train_time:60709ms step_avg:94.42ms
step:644/1700 train_time:60805ms step_avg:94.42ms
step:645/1700 train_time:60902ms step_avg:94.42ms
step:646/1700 train_time:61000ms step_avg:94.43ms
step:647/1700 train_time:61098ms step_avg:94.43ms
step:648/1700 train_time:61196ms step_avg:94.44ms
step:649/1700 train_time:61295ms step_avg:94.44ms
step:650/1700 train_time:61393ms step_avg:94.45ms
step:651/1700 train_time:61490ms step_avg:94.46ms
step:652/1700 train_time:61587ms step_avg:94.46ms
step:653/1700 train_time:61685ms step_avg:94.46ms
step:654/1700 train_time:61782ms step_avg:94.47ms
step:655/1700 train_time:61879ms step_avg:94.47ms
step:656/1700 train_time:61976ms step_avg:94.48ms
step:657/1700 train_time:62073ms step_avg:94.48ms
step:658/1700 train_time:62170ms step_avg:94.48ms
step:659/1700 train_time:62268ms step_avg:94.49ms
step:660/1700 train_time:62367ms step_avg:94.50ms
step:661/1700 train_time:62464ms step_avg:94.50ms
step:662/1700 train_time:62562ms step_avg:94.50ms
step:663/1700 train_time:62659ms step_avg:94.51ms
step:664/1700 train_time:62755ms step_avg:94.51ms
step:665/1700 train_time:62853ms step_avg:94.52ms
step:666/1700 train_time:62950ms step_avg:94.52ms
step:667/1700 train_time:63048ms step_avg:94.52ms
step:668/1700 train_time:63145ms step_avg:94.53ms
step:669/1700 train_time:63243ms step_avg:94.53ms
step:670/1700 train_time:63342ms step_avg:94.54ms
step:671/1700 train_time:63440ms step_avg:94.55ms
step:672/1700 train_time:63538ms step_avg:94.55ms
step:673/1700 train_time:63635ms step_avg:94.55ms
step:674/1700 train_time:63732ms step_avg:94.56ms
step:675/1700 train_time:63829ms step_avg:94.56ms
step:676/1700 train_time:63926ms step_avg:94.56ms
step:677/1700 train_time:64024ms step_avg:94.57ms
step:678/1700 train_time:64121ms step_avg:94.57ms
step:679/1700 train_time:64218ms step_avg:94.58ms
step:680/1700 train_time:64316ms step_avg:94.58ms
step:681/1700 train_time:64414ms step_avg:94.59ms
step:682/1700 train_time:64512ms step_avg:94.59ms
step:683/1700 train_time:64609ms step_avg:94.60ms
step:684/1700 train_time:64707ms step_avg:94.60ms
step:685/1700 train_time:64804ms step_avg:94.60ms
step:686/1700 train_time:64901ms step_avg:94.61ms
step:687/1700 train_time:64999ms step_avg:94.61ms
step:688/1700 train_time:65096ms step_avg:94.62ms
step:689/1700 train_time:65193ms step_avg:94.62ms
step:690/1700 train_time:65291ms step_avg:94.62ms
step:691/1700 train_time:65388ms step_avg:94.63ms
step:692/1700 train_time:65485ms step_avg:94.63ms
step:693/1700 train_time:65583ms step_avg:94.64ms
step:694/1700 train_time:65681ms step_avg:94.64ms
step:695/1700 train_time:65779ms step_avg:94.65ms
step:696/1700 train_time:65877ms step_avg:94.65ms
step:697/1700 train_time:65975ms step_avg:94.66ms
step:698/1700 train_time:66072ms step_avg:94.66ms
step:699/1700 train_time:66169ms step_avg:94.66ms
step:700/1700 train_time:66266ms step_avg:94.67ms
step:701/1700 train_time:66363ms step_avg:94.67ms
step:702/1700 train_time:66462ms step_avg:94.68ms
step:703/1700 train_time:66559ms step_avg:94.68ms
step:704/1700 train_time:66656ms step_avg:94.68ms
step:705/1700 train_time:66754ms step_avg:94.69ms
step:706/1700 train_time:66852ms step_avg:94.69ms
step:707/1700 train_time:66949ms step_avg:94.69ms
step:708/1700 train_time:67046ms step_avg:94.70ms
step:709/1700 train_time:67144ms step_avg:94.70ms
step:710/1700 train_time:67242ms step_avg:94.71ms
step:711/1700 train_time:67340ms step_avg:94.71ms
step:712/1700 train_time:67437ms step_avg:94.72ms
step:713/1700 train_time:67535ms step_avg:94.72ms
step:714/1700 train_time:67633ms step_avg:94.72ms
step:715/1700 train_time:67731ms step_avg:94.73ms
step:716/1700 train_time:67828ms step_avg:94.73ms
step:717/1700 train_time:67925ms step_avg:94.73ms
step:718/1700 train_time:68022ms step_avg:94.74ms
step:719/1700 train_time:68120ms step_avg:94.74ms
step:720/1700 train_time:68217ms step_avg:94.75ms
step:721/1700 train_time:68314ms step_avg:94.75ms
step:722/1700 train_time:68411ms step_avg:94.75ms
step:723/1700 train_time:68509ms step_avg:94.76ms
step:724/1700 train_time:68607ms step_avg:94.76ms
step:725/1700 train_time:68704ms step_avg:94.76ms
step:726/1700 train_time:68801ms step_avg:94.77ms
step:727/1700 train_time:68899ms step_avg:94.77ms
step:728/1700 train_time:68996ms step_avg:94.78ms
step:729/1700 train_time:69093ms step_avg:94.78ms
step:730/1700 train_time:69190ms step_avg:94.78ms
step:731/1700 train_time:69288ms step_avg:94.79ms
step:732/1700 train_time:69386ms step_avg:94.79ms
step:733/1700 train_time:69484ms step_avg:94.79ms
step:734/1700 train_time:69582ms step_avg:94.80ms
step:735/1700 train_time:69679ms step_avg:94.80ms
step:736/1700 train_time:69776ms step_avg:94.80ms
step:737/1700 train_time:69874ms step_avg:94.81ms
step:738/1700 train_time:69971ms step_avg:94.81ms
step:739/1700 train_time:70068ms step_avg:94.81ms
step:740/1700 train_time:70166ms step_avg:94.82ms
step:741/1700 train_time:70264ms step_avg:94.82ms
step:742/1700 train_time:70362ms step_avg:94.83ms
step:743/1700 train_time:70459ms step_avg:94.83ms
step:744/1700 train_time:70557ms step_avg:94.83ms
step:745/1700 train_time:70654ms step_avg:94.84ms
step:746/1700 train_time:70751ms step_avg:94.84ms
step:747/1700 train_time:70848ms step_avg:94.84ms
step:748/1700 train_time:70947ms step_avg:94.85ms
step:749/1700 train_time:71044ms step_avg:94.85ms
step:750/1700 train_time:71142ms step_avg:94.86ms
step:750/1700 val_loss:3.5838 train_time:71222ms step_avg:94.96ms
step:751/1700 train_time:71245ms step_avg:94.87ms
step:752/1700 train_time:71344ms step_avg:94.87ms
step:753/1700 train_time:71443ms step_avg:94.88ms
step:754/1700 train_time:71541ms step_avg:94.88ms
step:755/1700 train_time:71638ms step_avg:94.88ms
step:756/1700 train_time:71735ms step_avg:94.89ms
step:757/1700 train_time:71831ms step_avg:94.89ms
step:758/1700 train_time:71928ms step_avg:94.89ms
step:759/1700 train_time:72025ms step_avg:94.89ms
step:760/1700 train_time:72122ms step_avg:94.90ms
step:761/1700 train_time:72221ms step_avg:94.90ms
step:762/1700 train_time:72321ms step_avg:94.91ms
step:763/1700 train_time:72418ms step_avg:94.91ms
step:764/1700 train_time:72517ms step_avg:94.92ms
step:765/1700 train_time:72615ms step_avg:94.92ms
step:766/1700 train_time:72712ms step_avg:94.92ms
step:767/1700 train_time:72809ms step_avg:94.93ms
step:768/1700 train_time:72906ms step_avg:94.93ms
step:769/1700 train_time:73002ms step_avg:94.93ms
step:770/1700 train_time:73099ms step_avg:94.93ms
step:771/1700 train_time:73198ms step_avg:94.94ms
step:772/1700 train_time:73297ms step_avg:94.94ms
step:773/1700 train_time:73395ms step_avg:94.95ms
step:774/1700 train_time:73493ms step_avg:94.95ms
step:775/1700 train_time:73590ms step_avg:94.95ms
step:776/1700 train_time:73687ms step_avg:94.96ms
step:777/1700 train_time:73785ms step_avg:94.96ms
step:778/1700 train_time:73883ms step_avg:94.96ms
step:779/1700 train_time:73981ms step_avg:94.97ms
step:780/1700 train_time:74079ms step_avg:94.97ms
step:781/1700 train_time:74176ms step_avg:94.98ms
step:782/1700 train_time:74274ms step_avg:94.98ms
step:783/1700 train_time:74371ms step_avg:94.98ms
step:784/1700 train_time:74469ms step_avg:94.99ms
step:785/1700 train_time:74567ms step_avg:94.99ms
step:786/1700 train_time:74663ms step_avg:94.99ms
step:787/1700 train_time:74762ms step_avg:95.00ms
step:788/1700 train_time:74860ms step_avg:95.00ms
step:789/1700 train_time:74958ms step_avg:95.00ms
step:790/1700 train_time:75055ms step_avg:95.01ms
step:791/1700 train_time:75152ms step_avg:95.01ms
step:792/1700 train_time:75250ms step_avg:95.01ms
step:793/1700 train_time:75348ms step_avg:95.02ms
step:794/1700 train_time:75446ms step_avg:95.02ms
step:795/1700 train_time:75543ms step_avg:95.02ms
step:796/1700 train_time:75640ms step_avg:95.03ms
step:797/1700 train_time:75737ms step_avg:95.03ms
step:798/1700 train_time:75836ms step_avg:95.03ms
step:799/1700 train_time:75934ms step_avg:95.04ms
step:800/1700 train_time:76031ms step_avg:95.04ms
step:801/1700 train_time:76128ms step_avg:95.04ms
step:802/1700 train_time:76226ms step_avg:95.05ms
step:803/1700 train_time:76324ms step_avg:95.05ms
step:804/1700 train_time:76421ms step_avg:95.05ms
step:805/1700 train_time:76519ms step_avg:95.05ms
step:806/1700 train_time:76618ms step_avg:95.06ms
step:807/1700 train_time:76716ms step_avg:95.06ms
step:808/1700 train_time:76814ms step_avg:95.07ms
step:809/1700 train_time:76912ms step_avg:95.07ms
step:810/1700 train_time:77009ms step_avg:95.07ms
step:811/1700 train_time:77107ms step_avg:95.08ms
step:812/1700 train_time:77205ms step_avg:95.08ms
step:813/1700 train_time:77302ms step_avg:95.08ms
step:814/1700 train_time:77400ms step_avg:95.09ms
step:815/1700 train_time:77498ms step_avg:95.09ms
step:816/1700 train_time:77595ms step_avg:95.09ms
step:817/1700 train_time:77693ms step_avg:95.09ms
step:818/1700 train_time:77790ms step_avg:95.10ms
step:819/1700 train_time:77888ms step_avg:95.10ms
step:820/1700 train_time:77985ms step_avg:95.10ms
step:821/1700 train_time:78082ms step_avg:95.11ms
step:822/1700 train_time:78180ms step_avg:95.11ms
step:823/1700 train_time:78278ms step_avg:95.11ms
step:824/1700 train_time:78376ms step_avg:95.12ms
step:825/1700 train_time:78474ms step_avg:95.12ms
step:826/1700 train_time:78571ms step_avg:95.12ms
step:827/1700 train_time:78669ms step_avg:95.13ms
step:828/1700 train_time:78767ms step_avg:95.13ms
step:829/1700 train_time:78864ms step_avg:95.13ms
step:830/1700 train_time:78962ms step_avg:95.14ms
step:831/1700 train_time:79060ms step_avg:95.14ms
step:832/1700 train_time:79157ms step_avg:95.14ms
step:833/1700 train_time:79256ms step_avg:95.14ms
step:834/1700 train_time:79353ms step_avg:95.15ms
step:835/1700 train_time:79451ms step_avg:95.15ms
step:836/1700 train_time:79548ms step_avg:95.15ms
step:837/1700 train_time:79647ms step_avg:95.16ms
step:838/1700 train_time:79744ms step_avg:95.16ms
step:839/1700 train_time:79843ms step_avg:95.16ms
step:840/1700 train_time:79940ms step_avg:95.17ms
step:841/1700 train_time:80038ms step_avg:95.17ms
step:842/1700 train_time:80135ms step_avg:95.17ms
step:843/1700 train_time:80232ms step_avg:95.17ms
step:844/1700 train_time:80330ms step_avg:95.18ms
step:845/1700 train_time:80428ms step_avg:95.18ms
step:846/1700 train_time:80525ms step_avg:95.18ms
step:847/1700 train_time:80623ms step_avg:95.19ms
step:848/1700 train_time:80720ms step_avg:95.19ms
step:849/1700 train_time:80819ms step_avg:95.19ms
step:850/1700 train_time:80917ms step_avg:95.20ms
step:851/1700 train_time:81014ms step_avg:95.20ms
step:852/1700 train_time:81112ms step_avg:95.20ms
step:853/1700 train_time:81208ms step_avg:95.20ms
step:854/1700 train_time:81306ms step_avg:95.21ms
step:855/1700 train_time:81403ms step_avg:95.21ms
step:856/1700 train_time:81501ms step_avg:95.21ms
step:857/1700 train_time:81599ms step_avg:95.21ms
step:858/1700 train_time:81696ms step_avg:95.22ms
step:859/1700 train_time:81795ms step_avg:95.22ms
step:860/1700 train_time:81892ms step_avg:95.22ms
step:861/1700 train_time:81990ms step_avg:95.23ms
step:862/1700 train_time:82088ms step_avg:95.23ms
step:863/1700 train_time:82186ms step_avg:95.23ms
step:864/1700 train_time:82282ms step_avg:95.23ms
step:865/1700 train_time:82380ms step_avg:95.24ms
step:866/1700 train_time:82478ms step_avg:95.24ms
step:867/1700 train_time:82577ms step_avg:95.24ms
step:868/1700 train_time:82675ms step_avg:95.25ms
step:869/1700 train_time:82773ms step_avg:95.25ms
step:870/1700 train_time:82870ms step_avg:95.25ms
step:871/1700 train_time:82968ms step_avg:95.26ms
step:872/1700 train_time:83065ms step_avg:95.26ms
step:873/1700 train_time:83162ms step_avg:95.26ms
step:874/1700 train_time:83260ms step_avg:95.26ms
step:875/1700 train_time:83357ms step_avg:95.27ms
step:875/1700 val_loss:3.5377 train_time:83437ms step_avg:95.36ms
step:876/1700 train_time:83461ms step_avg:95.27ms
step:877/1700 train_time:83559ms step_avg:95.28ms
step:878/1700 train_time:83658ms step_avg:95.28ms
step:879/1700 train_time:83754ms step_avg:95.28ms
step:880/1700 train_time:83851ms step_avg:95.29ms
step:881/1700 train_time:83948ms step_avg:95.29ms
step:882/1700 train_time:84045ms step_avg:95.29ms
step:883/1700 train_time:84142ms step_avg:95.29ms
step:884/1700 train_time:84241ms step_avg:95.30ms
step:885/1700 train_time:84340ms step_avg:95.30ms
step:886/1700 train_time:84439ms step_avg:95.30ms
step:887/1700 train_time:84540ms step_avg:95.31ms
step:888/1700 train_time:84641ms step_avg:95.32ms
step:889/1700 train_time:84741ms step_avg:95.32ms
step:890/1700 train_time:84840ms step_avg:95.33ms
step:891/1700 train_time:84939ms step_avg:95.33ms
step:892/1700 train_time:85038ms step_avg:95.33ms
step:893/1700 train_time:85135ms step_avg:95.34ms
step:894/1700 train_time:85234ms step_avg:95.34ms
step:895/1700 train_time:85332ms step_avg:95.34ms
step:896/1700 train_time:85431ms step_avg:95.35ms
step:897/1700 train_time:85530ms step_avg:95.35ms
step:898/1700 train_time:85630ms step_avg:95.36ms
step:899/1700 train_time:85731ms step_avg:95.36ms
step:900/1700 train_time:85830ms step_avg:95.37ms
step:901/1700 train_time:85930ms step_avg:95.37ms
step:902/1700 train_time:86029ms step_avg:95.38ms
step:903/1700 train_time:86128ms step_avg:95.38ms
step:904/1700 train_time:86227ms step_avg:95.38ms
step:905/1700 train_time:86326ms step_avg:95.39ms
step:906/1700 train_time:86424ms step_avg:95.39ms
step:907/1700 train_time:86524ms step_avg:95.40ms
step:908/1700 train_time:86624ms step_avg:95.40ms
step:909/1700 train_time:86723ms step_avg:95.41ms
step:910/1700 train_time:86823ms step_avg:95.41ms
step:911/1700 train_time:86922ms step_avg:95.41ms
step:912/1700 train_time:87021ms step_avg:95.42ms
step:913/1700 train_time:87122ms step_avg:95.42ms
step:914/1700 train_time:87221ms step_avg:95.43ms
step:915/1700 train_time:87320ms step_avg:95.43ms
step:916/1700 train_time:87419ms step_avg:95.44ms
step:917/1700 train_time:87518ms step_avg:95.44ms
step:918/1700 train_time:87618ms step_avg:95.44ms
step:919/1700 train_time:87717ms step_avg:95.45ms
step:920/1700 train_time:87816ms step_avg:95.45ms
step:921/1700 train_time:87915ms step_avg:95.46ms
step:922/1700 train_time:88013ms step_avg:95.46ms
step:923/1700 train_time:88113ms step_avg:95.46ms
step:924/1700 train_time:88213ms step_avg:95.47ms
step:925/1700 train_time:88312ms step_avg:95.47ms
step:926/1700 train_time:88412ms step_avg:95.48ms
step:927/1700 train_time:88512ms step_avg:95.48ms
step:928/1700 train_time:88611ms step_avg:95.49ms
step:929/1700 train_time:88710ms step_avg:95.49ms
step:930/1700 train_time:88809ms step_avg:95.49ms
step:931/1700 train_time:88907ms step_avg:95.50ms
step:932/1700 train_time:89005ms step_avg:95.50ms
step:933/1700 train_time:89105ms step_avg:95.50ms
step:934/1700 train_time:89205ms step_avg:95.51ms
step:935/1700 train_time:89305ms step_avg:95.51ms
step:936/1700 train_time:89404ms step_avg:95.52ms
step:937/1700 train_time:89504ms step_avg:95.52ms
step:938/1700 train_time:89603ms step_avg:95.53ms
step:939/1700 train_time:89702ms step_avg:95.53ms
step:940/1700 train_time:89801ms step_avg:95.53ms
step:941/1700 train_time:89901ms step_avg:95.54ms
step:942/1700 train_time:90000ms step_avg:95.54ms
step:943/1700 train_time:90098ms step_avg:95.54ms
step:944/1700 train_time:90198ms step_avg:95.55ms
step:945/1700 train_time:90296ms step_avg:95.55ms
step:946/1700 train_time:90395ms step_avg:95.56ms
step:947/1700 train_time:90494ms step_avg:95.56ms
step:948/1700 train_time:90594ms step_avg:95.56ms
step:949/1700 train_time:90693ms step_avg:95.57ms
step:950/1700 train_time:90794ms step_avg:95.57ms
step:951/1700 train_time:90893ms step_avg:95.58ms
step:952/1700 train_time:90993ms step_avg:95.58ms
step:953/1700 train_time:91093ms step_avg:95.59ms
step:954/1700 train_time:91192ms step_avg:95.59ms
step:955/1700 train_time:91292ms step_avg:95.59ms
step:956/1700 train_time:91390ms step_avg:95.60ms
step:957/1700 train_time:91489ms step_avg:95.60ms
step:958/1700 train_time:91588ms step_avg:95.60ms
step:959/1700 train_time:91687ms step_avg:95.61ms
step:960/1700 train_time:91786ms step_avg:95.61ms
step:961/1700 train_time:91885ms step_avg:95.61ms
step:962/1700 train_time:91985ms step_avg:95.62ms
step:963/1700 train_time:92084ms step_avg:95.62ms
step:964/1700 train_time:92183ms step_avg:95.63ms
step:965/1700 train_time:92282ms step_avg:95.63ms
step:966/1700 train_time:92382ms step_avg:95.63ms
step:967/1700 train_time:92482ms step_avg:95.64ms
step:968/1700 train_time:92582ms step_avg:95.64ms
step:969/1700 train_time:92681ms step_avg:95.65ms
step:970/1700 train_time:92781ms step_avg:95.65ms
step:971/1700 train_time:92880ms step_avg:95.65ms
step:972/1700 train_time:92979ms step_avg:95.66ms
step:973/1700 train_time:93078ms step_avg:95.66ms
step:974/1700 train_time:93178ms step_avg:95.66ms
step:975/1700 train_time:93276ms step_avg:95.67ms
step:976/1700 train_time:93376ms step_avg:95.67ms
step:977/1700 train_time:93475ms step_avg:95.68ms
step:978/1700 train_time:93574ms step_avg:95.68ms
step:979/1700 train_time:93674ms step_avg:95.68ms
step:980/1700 train_time:93773ms step_avg:95.69ms
step:981/1700 train_time:93874ms step_avg:95.69ms
step:982/1700 train_time:93974ms step_avg:95.70ms
step:983/1700 train_time:94074ms step_avg:95.70ms
step:984/1700 train_time:94173ms step_avg:95.70ms
step:985/1700 train_time:94271ms step_avg:95.71ms
step:986/1700 train_time:94372ms step_avg:95.71ms
step:987/1700 train_time:94471ms step_avg:95.71ms
step:988/1700 train_time:94570ms step_avg:95.72ms
step:989/1700 train_time:94669ms step_avg:95.72ms
step:990/1700 train_time:94768ms step_avg:95.72ms
step:991/1700 train_time:94867ms step_avg:95.73ms
step:992/1700 train_time:94966ms step_avg:95.73ms
step:993/1700 train_time:95064ms step_avg:95.73ms
step:994/1700 train_time:95164ms step_avg:95.74ms
step:995/1700 train_time:95264ms step_avg:95.74ms
step:996/1700 train_time:95363ms step_avg:95.75ms
step:997/1700 train_time:95463ms step_avg:95.75ms
step:998/1700 train_time:95562ms step_avg:95.75ms
step:999/1700 train_time:95661ms step_avg:95.76ms
step:1000/1700 train_time:95761ms step_avg:95.76ms
step:1000/1700 val_loss:3.4917 train_time:95843ms step_avg:95.84ms
step:1001/1700 train_time:95867ms step_avg:95.77ms
step:1002/1700 train_time:95970ms step_avg:95.78ms
step:1003/1700 train_time:96070ms step_avg:95.78ms
step:1004/1700 train_time:96169ms step_avg:95.79ms
step:1005/1700 train_time:96269ms step_avg:95.79ms
step:1006/1700 train_time:96368ms step_avg:95.79ms
step:1007/1700 train_time:96465ms step_avg:95.79ms
step:1008/1700 train_time:96564ms step_avg:95.80ms
step:1009/1700 train_time:96663ms step_avg:95.80ms
step:1010/1700 train_time:96762ms step_avg:95.80ms
step:1011/1700 train_time:96864ms step_avg:95.81ms
step:1012/1700 train_time:96964ms step_avg:95.81ms
step:1013/1700 train_time:97065ms step_avg:95.82ms
step:1014/1700 train_time:97164ms step_avg:95.82ms
step:1015/1700 train_time:97262ms step_avg:95.82ms
step:1016/1700 train_time:97361ms step_avg:95.83ms
step:1017/1700 train_time:97461ms step_avg:95.83ms
step:1018/1700 train_time:97561ms step_avg:95.84ms
step:1019/1700 train_time:97660ms step_avg:95.84ms
step:1020/1700 train_time:97759ms step_avg:95.84ms
step:1021/1700 train_time:97861ms step_avg:95.85ms
step:1022/1700 train_time:97961ms step_avg:95.85ms
step:1023/1700 train_time:98061ms step_avg:95.86ms
step:1024/1700 train_time:98161ms step_avg:95.86ms
step:1025/1700 train_time:98261ms step_avg:95.86ms
step:1026/1700 train_time:98361ms step_avg:95.87ms
step:1027/1700 train_time:98461ms step_avg:95.87ms
step:1028/1700 train_time:98559ms step_avg:95.87ms
step:1029/1700 train_time:98659ms step_avg:95.88ms
step:1030/1700 train_time:98758ms step_avg:95.88ms
step:1031/1700 train_time:98858ms step_avg:95.89ms
step:1032/1700 train_time:98959ms step_avg:95.89ms
step:1033/1700 train_time:99059ms step_avg:95.89ms
step:1034/1700 train_time:99159ms step_avg:95.90ms
step:1035/1700 train_time:99258ms step_avg:95.90ms
step:1036/1700 train_time:99358ms step_avg:95.91ms
step:1037/1700 train_time:99458ms step_avg:95.91ms
step:1038/1700 train_time:99557ms step_avg:95.91ms
step:1039/1700 train_time:99656ms step_avg:95.92ms
step:1040/1700 train_time:99754ms step_avg:95.92ms
step:1041/1700 train_time:99853ms step_avg:95.92ms
step:1042/1700 train_time:99952ms step_avg:95.92ms
step:1043/1700 train_time:100051ms step_avg:95.93ms
step:1044/1700 train_time:100150ms step_avg:95.93ms
step:1045/1700 train_time:100250ms step_avg:95.93ms
step:1046/1700 train_time:100350ms step_avg:95.94ms
step:1047/1700 train_time:100450ms step_avg:95.94ms
step:1048/1700 train_time:100551ms step_avg:95.95ms
step:1049/1700 train_time:100650ms step_avg:95.95ms
step:1050/1700 train_time:100750ms step_avg:95.95ms
step:1051/1700 train_time:100849ms step_avg:95.96ms
step:1052/1700 train_time:100949ms step_avg:95.96ms
step:1053/1700 train_time:101047ms step_avg:95.96ms
step:1054/1700 train_time:101146ms step_avg:95.96ms
step:1055/1700 train_time:101245ms step_avg:95.97ms
step:1056/1700 train_time:101346ms step_avg:95.97ms
step:1057/1700 train_time:101446ms step_avg:95.98ms
step:1058/1700 train_time:101546ms step_avg:95.98ms
step:1059/1700 train_time:101646ms step_avg:95.98ms
step:1060/1700 train_time:101746ms step_avg:95.99ms
step:1061/1700 train_time:101845ms step_avg:95.99ms
step:1062/1700 train_time:101945ms step_avg:95.99ms
step:1063/1700 train_time:102044ms step_avg:96.00ms
step:1064/1700 train_time:102144ms step_avg:96.00ms
step:1065/1700 train_time:102243ms step_avg:96.00ms
step:1066/1700 train_time:102342ms step_avg:96.01ms
step:1067/1700 train_time:102441ms step_avg:96.01ms
step:1068/1700 train_time:102542ms step_avg:96.01ms
step:1069/1700 train_time:102642ms step_avg:96.02ms
step:1070/1700 train_time:102741ms step_avg:96.02ms
step:1071/1700 train_time:102841ms step_avg:96.02ms
step:1072/1700 train_time:102941ms step_avg:96.03ms
step:1073/1700 train_time:103041ms step_avg:96.03ms
step:1074/1700 train_time:103141ms step_avg:96.03ms
step:1075/1700 train_time:103240ms step_avg:96.04ms
step:1076/1700 train_time:103338ms step_avg:96.04ms
step:1077/1700 train_time:103438ms step_avg:96.04ms
step:1078/1700 train_time:103537ms step_avg:96.05ms
step:1079/1700 train_time:103638ms step_avg:96.05ms
step:1080/1700 train_time:103737ms step_avg:96.05ms
step:1081/1700 train_time:103837ms step_avg:96.06ms
step:1082/1700 train_time:103937ms step_avg:96.06ms
step:1083/1700 train_time:104037ms step_avg:96.06ms
step:1084/1700 train_time:104137ms step_avg:96.07ms
step:1085/1700 train_time:104237ms step_avg:96.07ms
step:1086/1700 train_time:104336ms step_avg:96.07ms
step:1087/1700 train_time:104435ms step_avg:96.08ms
step:1088/1700 train_time:104534ms step_avg:96.08ms
step:1089/1700 train_time:104634ms step_avg:96.08ms
step:1090/1700 train_time:104733ms step_avg:96.09ms
step:1091/1700 train_time:104832ms step_avg:96.09ms
step:1092/1700 train_time:104932ms step_avg:96.09ms
step:1093/1700 train_time:105030ms step_avg:96.09ms
step:1094/1700 train_time:105130ms step_avg:96.10ms
step:1095/1700 train_time:105230ms step_avg:96.10ms
step:1096/1700 train_time:105330ms step_avg:96.10ms
step:1097/1700 train_time:105429ms step_avg:96.11ms
step:1098/1700 train_time:105529ms step_avg:96.11ms
step:1099/1700 train_time:105629ms step_avg:96.11ms
step:1100/1700 train_time:105729ms step_avg:96.12ms
step:1101/1700 train_time:105829ms step_avg:96.12ms
step:1102/1700 train_time:105928ms step_avg:96.12ms
step:1103/1700 train_time:106028ms step_avg:96.13ms
step:1104/1700 train_time:106128ms step_avg:96.13ms
step:1105/1700 train_time:106228ms step_avg:96.13ms
step:1106/1700 train_time:106329ms step_avg:96.14ms
step:1107/1700 train_time:106429ms step_avg:96.14ms
step:1108/1700 train_time:106529ms step_avg:96.15ms
step:1109/1700 train_time:106630ms step_avg:96.15ms
step:1110/1700 train_time:106730ms step_avg:96.15ms
step:1111/1700 train_time:106830ms step_avg:96.16ms
step:1112/1700 train_time:106929ms step_avg:96.16ms
step:1113/1700 train_time:107029ms step_avg:96.16ms
step:1114/1700 train_time:107128ms step_avg:96.17ms
step:1115/1700 train_time:107228ms step_avg:96.17ms
step:1116/1700 train_time:107328ms step_avg:96.17ms
step:1117/1700 train_time:107427ms step_avg:96.17ms
step:1118/1700 train_time:107526ms step_avg:96.18ms
step:1119/1700 train_time:107626ms step_avg:96.18ms
step:1120/1700 train_time:107725ms step_avg:96.18ms
step:1121/1700 train_time:107824ms step_avg:96.19ms
step:1122/1700 train_time:107923ms step_avg:96.19ms
step:1123/1700 train_time:108022ms step_avg:96.19ms
step:1124/1700 train_time:108120ms step_avg:96.19ms
step:1125/1700 train_time:108222ms step_avg:96.20ms
step:1125/1700 val_loss:3.4417 train_time:108302ms step_avg:96.27ms
step:1126/1700 train_time:108324ms step_avg:96.20ms
step:1127/1700 train_time:108425ms step_avg:96.21ms
step:1128/1700 train_time:108527ms step_avg:96.21ms
step:1129/1700 train_time:108625ms step_avg:96.21ms
step:1130/1700 train_time:108724ms step_avg:96.22ms
step:1131/1700 train_time:108823ms step_avg:96.22ms
step:1132/1700 train_time:108922ms step_avg:96.22ms
step:1133/1700 train_time:109020ms step_avg:96.22ms
step:1134/1700 train_time:109118ms step_avg:96.22ms
step:1135/1700 train_time:109217ms step_avg:96.23ms
step:1136/1700 train_time:109318ms step_avg:96.23ms
step:1137/1700 train_time:109420ms step_avg:96.24ms
step:1138/1700 train_time:109522ms step_avg:96.24ms
step:1139/1700 train_time:109622ms step_avg:96.24ms
step:1140/1700 train_time:109723ms step_avg:96.25ms
step:1141/1700 train_time:109823ms step_avg:96.25ms
step:1142/1700 train_time:109924ms step_avg:96.26ms
step:1143/1700 train_time:110023ms step_avg:96.26ms
step:1144/1700 train_time:110124ms step_avg:96.26ms
step:1145/1700 train_time:110225ms step_avg:96.27ms
step:1146/1700 train_time:110325ms step_avg:96.27ms
step:1147/1700 train_time:110426ms step_avg:96.27ms
step:1148/1700 train_time:110526ms step_avg:96.28ms
step:1149/1700 train_time:110627ms step_avg:96.28ms
step:1150/1700 train_time:110727ms step_avg:96.28ms
step:1151/1700 train_time:110827ms step_avg:96.29ms
step:1152/1700 train_time:110926ms step_avg:96.29ms
step:1153/1700 train_time:111026ms step_avg:96.29ms
step:1154/1700 train_time:111127ms step_avg:96.30ms
step:1155/1700 train_time:111228ms step_avg:96.30ms
step:1156/1700 train_time:111328ms step_avg:96.30ms
step:1157/1700 train_time:111430ms step_avg:96.31ms
step:1158/1700 train_time:111530ms step_avg:96.31ms
step:1159/1700 train_time:111631ms step_avg:96.32ms
step:1160/1700 train_time:111732ms step_avg:96.32ms
step:1161/1700 train_time:111832ms step_avg:96.32ms
step:1162/1700 train_time:111932ms step_avg:96.33ms
step:1163/1700 train_time:112033ms step_avg:96.33ms
step:1164/1700 train_time:112133ms step_avg:96.33ms
step:1165/1700 train_time:112233ms step_avg:96.34ms
step:1166/1700 train_time:112333ms step_avg:96.34ms
step:1167/1700 train_time:112433ms step_avg:96.34ms
step:1168/1700 train_time:112534ms step_avg:96.35ms
step:1169/1700 train_time:112635ms step_avg:96.35ms
step:1170/1700 train_time:112735ms step_avg:96.35ms
step:1171/1700 train_time:112835ms step_avg:96.36ms
step:1172/1700 train_time:112936ms step_avg:96.36ms
step:1173/1700 train_time:113035ms step_avg:96.36ms
step:1174/1700 train_time:113136ms step_avg:96.37ms
step:1175/1700 train_time:113236ms step_avg:96.37ms
step:1176/1700 train_time:113336ms step_avg:96.37ms
step:1177/1700 train_time:113437ms step_avg:96.38ms
step:1178/1700 train_time:113537ms step_avg:96.38ms
step:1179/1700 train_time:113638ms step_avg:96.38ms
step:1180/1700 train_time:113737ms step_avg:96.39ms
step:1181/1700 train_time:113837ms step_avg:96.39ms
step:1182/1700 train_time:113938ms step_avg:96.39ms
step:1183/1700 train_time:114038ms step_avg:96.40ms
step:1184/1700 train_time:114137ms step_avg:96.40ms
step:1185/1700 train_time:114237ms step_avg:96.40ms
step:1186/1700 train_time:114338ms step_avg:96.41ms
step:1187/1700 train_time:114437ms step_avg:96.41ms
step:1188/1700 train_time:114538ms step_avg:96.41ms
step:1189/1700 train_time:114639ms step_avg:96.42ms
step:1190/1700 train_time:114738ms step_avg:96.42ms
step:1191/1700 train_time:114838ms step_avg:96.42ms
step:1192/1700 train_time:114939ms step_avg:96.42ms
step:1193/1700 train_time:115038ms step_avg:96.43ms
step:1194/1700 train_time:115138ms step_avg:96.43ms
step:1195/1700 train_time:115238ms step_avg:96.43ms
step:1196/1700 train_time:115338ms step_avg:96.44ms
step:1197/1700 train_time:115439ms step_avg:96.44ms
step:1198/1700 train_time:115539ms step_avg:96.44ms
step:1199/1700 train_time:115640ms step_avg:96.45ms
step:1200/1700 train_time:115740ms step_avg:96.45ms
step:1201/1700 train_time:115841ms step_avg:96.45ms
step:1202/1700 train_time:115942ms step_avg:96.46ms
step:1203/1700 train_time:116043ms step_avg:96.46ms
step:1204/1700 train_time:116143ms step_avg:96.46ms
step:1205/1700 train_time:116244ms step_avg:96.47ms
step:1206/1700 train_time:116345ms step_avg:96.47ms
step:1207/1700 train_time:116446ms step_avg:96.48ms
step:1208/1700 train_time:116546ms step_avg:96.48ms
step:1209/1700 train_time:116647ms step_avg:96.48ms
step:1210/1700 train_time:116747ms step_avg:96.49ms
step:1211/1700 train_time:116848ms step_avg:96.49ms
step:1212/1700 train_time:116949ms step_avg:96.49ms
step:1213/1700 train_time:117049ms step_avg:96.50ms
step:1214/1700 train_time:117150ms step_avg:96.50ms
step:1215/1700 train_time:117251ms step_avg:96.50ms
step:1216/1700 train_time:117353ms step_avg:96.51ms
step:1217/1700 train_time:117454ms step_avg:96.51ms
step:1218/1700 train_time:117554ms step_avg:96.51ms
step:1219/1700 train_time:117655ms step_avg:96.52ms
step:1220/1700 train_time:117756ms step_avg:96.52ms
step:1221/1700 train_time:117856ms step_avg:96.52ms
step:1222/1700 train_time:117956ms step_avg:96.53ms
step:1223/1700 train_time:118056ms step_avg:96.53ms
step:1224/1700 train_time:118156ms step_avg:96.53ms
step:1225/1700 train_time:118256ms step_avg:96.54ms
step:1226/1700 train_time:118356ms step_avg:96.54ms
step:1227/1700 train_time:118457ms step_avg:96.54ms
step:1228/1700 train_time:118557ms step_avg:96.54ms
step:1229/1700 train_time:118657ms step_avg:96.55ms
step:1230/1700 train_time:118756ms step_avg:96.55ms
step:1231/1700 train_time:118856ms step_avg:96.55ms
step:1232/1700 train_time:118957ms step_avg:96.56ms
step:1233/1700 train_time:119058ms step_avg:96.56ms
step:1234/1700 train_time:119157ms step_avg:96.56ms
step:1235/1700 train_time:119258ms step_avg:96.56ms
step:1236/1700 train_time:119359ms step_avg:96.57ms
step:1237/1700 train_time:119459ms step_avg:96.57ms
step:1238/1700 train_time:119559ms step_avg:96.57ms
step:1239/1700 train_time:119658ms step_avg:96.58ms
step:1240/1700 train_time:119758ms step_avg:96.58ms
step:1241/1700 train_time:119860ms step_avg:96.58ms
step:1242/1700 train_time:119961ms step_avg:96.59ms
step:1243/1700 train_time:120061ms step_avg:96.59ms
step:1244/1700 train_time:120162ms step_avg:96.59ms
step:1245/1700 train_time:120263ms step_avg:96.60ms
step:1246/1700 train_time:120364ms step_avg:96.60ms
step:1247/1700 train_time:120463ms step_avg:96.60ms
step:1248/1700 train_time:120564ms step_avg:96.61ms
step:1249/1700 train_time:120665ms step_avg:96.61ms
step:1250/1700 train_time:120765ms step_avg:96.61ms
step:1250/1700 val_loss:3.3962 train_time:120848ms step_avg:96.68ms
step:1251/1700 train_time:120871ms step_avg:96.62ms
step:1252/1700 train_time:120973ms step_avg:96.62ms
step:1253/1700 train_time:121074ms step_avg:96.63ms
step:1254/1700 train_time:121174ms step_avg:96.63ms
step:1255/1700 train_time:121275ms step_avg:96.63ms
step:1256/1700 train_time:121375ms step_avg:96.64ms
step:1257/1700 train_time:121474ms step_avg:96.64ms
step:1258/1700 train_time:121573ms step_avg:96.64ms
step:1259/1700 train_time:121672ms step_avg:96.64ms
step:1260/1700 train_time:121773ms step_avg:96.65ms
step:1261/1700 train_time:121875ms step_avg:96.65ms
step:1262/1700 train_time:121978ms step_avg:96.65ms
step:1263/1700 train_time:122080ms step_avg:96.66ms
step:1264/1700 train_time:122181ms step_avg:96.66ms
step:1265/1700 train_time:122281ms step_avg:96.66ms
step:1266/1700 train_time:122381ms step_avg:96.67ms
step:1267/1700 train_time:122480ms step_avg:96.67ms
step:1268/1700 train_time:122580ms step_avg:96.67ms
step:1269/1700 train_time:122681ms step_avg:96.68ms
step:1270/1700 train_time:122781ms step_avg:96.68ms
step:1271/1700 train_time:122884ms step_avg:96.68ms
step:1272/1700 train_time:122986ms step_avg:96.69ms
step:1273/1700 train_time:123086ms step_avg:96.69ms
step:1274/1700 train_time:123187ms step_avg:96.69ms
step:1275/1700 train_time:123288ms step_avg:96.70ms
step:1276/1700 train_time:123388ms step_avg:96.70ms
step:1277/1700 train_time:123488ms step_avg:96.70ms
step:1278/1700 train_time:123589ms step_avg:96.70ms
step:1279/1700 train_time:123691ms step_avg:96.71ms
step:1280/1700 train_time:123791ms step_avg:96.71ms
step:1281/1700 train_time:123892ms step_avg:96.72ms
step:1282/1700 train_time:123992ms step_avg:96.72ms
step:1283/1700 train_time:124093ms step_avg:96.72ms
step:1284/1700 train_time:124193ms step_avg:96.72ms
step:1285/1700 train_time:124293ms step_avg:96.73ms
step:1286/1700 train_time:124393ms step_avg:96.73ms
step:1287/1700 train_time:124493ms step_avg:96.73ms
step:1288/1700 train_time:124593ms step_avg:96.73ms
step:1289/1700 train_time:124693ms step_avg:96.74ms
step:1290/1700 train_time:124793ms step_avg:96.74ms
step:1291/1700 train_time:124895ms step_avg:96.74ms
step:1292/1700 train_time:124996ms step_avg:96.75ms
step:1293/1700 train_time:125096ms step_avg:96.75ms
step:1294/1700 train_time:125199ms step_avg:96.75ms
step:1295/1700 train_time:125300ms step_avg:96.76ms
step:1296/1700 train_time:125401ms step_avg:96.76ms
step:1297/1700 train_time:125501ms step_avg:96.76ms
step:1298/1700 train_time:125601ms step_avg:96.77ms
step:1299/1700 train_time:125702ms step_avg:96.77ms
step:1300/1700 train_time:125802ms step_avg:96.77ms
step:1301/1700 train_time:125903ms step_avg:96.77ms
step:1302/1700 train_time:126004ms step_avg:96.78ms
step:1303/1700 train_time:126105ms step_avg:96.78ms
step:1304/1700 train_time:126206ms step_avg:96.78ms
step:1305/1700 train_time:126306ms step_avg:96.79ms
step:1306/1700 train_time:126408ms step_avg:96.79ms
step:1307/1700 train_time:126508ms step_avg:96.79ms
step:1308/1700 train_time:126609ms step_avg:96.80ms
step:1309/1700 train_time:126709ms step_avg:96.80ms
step:1310/1700 train_time:126809ms step_avg:96.80ms
step:1311/1700 train_time:126910ms step_avg:96.80ms
step:1312/1700 train_time:127011ms step_avg:96.81ms
step:1313/1700 train_time:127112ms step_avg:96.81ms
step:1314/1700 train_time:127212ms step_avg:96.81ms
step:1315/1700 train_time:127313ms step_avg:96.82ms
step:1316/1700 train_time:127413ms step_avg:96.82ms
step:1317/1700 train_time:127513ms step_avg:96.82ms
step:1318/1700 train_time:127612ms step_avg:96.82ms
step:1319/1700 train_time:127713ms step_avg:96.83ms
step:1320/1700 train_time:127814ms step_avg:96.83ms
step:1321/1700 train_time:127916ms step_avg:96.83ms
step:1322/1700 train_time:128016ms step_avg:96.84ms
step:1323/1700 train_time:128117ms step_avg:96.84ms
step:1324/1700 train_time:128218ms step_avg:96.84ms
step:1325/1700 train_time:128320ms step_avg:96.85ms
step:1326/1700 train_time:128420ms step_avg:96.85ms
step:1327/1700 train_time:128520ms step_avg:96.85ms
step:1328/1700 train_time:128620ms step_avg:96.85ms
step:1329/1700 train_time:128720ms step_avg:96.85ms
step:1330/1700 train_time:128821ms step_avg:96.86ms
step:1331/1700 train_time:128922ms step_avg:96.86ms
step:1332/1700 train_time:129023ms step_avg:96.86ms
step:1333/1700 train_time:129124ms step_avg:96.87ms
step:1334/1700 train_time:129225ms step_avg:96.87ms
step:1335/1700 train_time:129326ms step_avg:96.87ms
step:1336/1700 train_time:129425ms step_avg:96.88ms
step:1337/1700 train_time:129526ms step_avg:96.88ms
step:1338/1700 train_time:129626ms step_avg:96.88ms
step:1339/1700 train_time:129727ms step_avg:96.88ms
step:1340/1700 train_time:129828ms step_avg:96.89ms
step:1341/1700 train_time:129930ms step_avg:96.89ms
step:1342/1700 train_time:130030ms step_avg:96.89ms
step:1343/1700 train_time:130131ms step_avg:96.90ms
step:1344/1700 train_time:130231ms step_avg:96.90ms
step:1345/1700 train_time:130331ms step_avg:96.90ms
step:1346/1700 train_time:130433ms step_avg:96.90ms
step:1347/1700 train_time:130533ms step_avg:96.91ms
step:1348/1700 train_time:130634ms step_avg:96.91ms
step:1349/1700 train_time:130735ms step_avg:96.91ms
step:1350/1700 train_time:130836ms step_avg:96.92ms
step:1351/1700 train_time:130937ms step_avg:96.92ms
step:1352/1700 train_time:131039ms step_avg:96.92ms
step:1353/1700 train_time:131140ms step_avg:96.93ms
step:1354/1700 train_time:131239ms step_avg:96.93ms
step:1355/1700 train_time:131340ms step_avg:96.93ms
step:1356/1700 train_time:131441ms step_avg:96.93ms
step:1357/1700 train_time:131541ms step_avg:96.94ms
step:1358/1700 train_time:131641ms step_avg:96.94ms
step:1359/1700 train_time:131742ms step_avg:96.94ms
step:1360/1700 train_time:131842ms step_avg:96.94ms
step:1361/1700 train_time:131943ms step_avg:96.95ms
step:1362/1700 train_time:132043ms step_avg:96.95ms
step:1363/1700 train_time:132144ms step_avg:96.95ms
step:1364/1700 train_time:132245ms step_avg:96.95ms
step:1365/1700 train_time:132346ms step_avg:96.96ms
step:1366/1700 train_time:132446ms step_avg:96.96ms
step:1367/1700 train_time:132547ms step_avg:96.96ms
step:1368/1700 train_time:132648ms step_avg:96.96ms
step:1369/1700 train_time:132748ms step_avg:96.97ms
step:1370/1700 train_time:132848ms step_avg:96.97ms
step:1371/1700 train_time:132949ms step_avg:96.97ms
step:1372/1700 train_time:133049ms step_avg:96.97ms
step:1373/1700 train_time:133151ms step_avg:96.98ms
step:1374/1700 train_time:133252ms step_avg:96.98ms
step:1375/1700 train_time:133353ms step_avg:96.98ms
step:1375/1700 val_loss:3.3566 train_time:133435ms step_avg:97.04ms
step:1376/1700 train_time:133458ms step_avg:96.99ms
step:1377/1700 train_time:133563ms step_avg:97.00ms
step:1378/1700 train_time:133664ms step_avg:97.00ms
step:1379/1700 train_time:133763ms step_avg:97.00ms
step:1380/1700 train_time:133864ms step_avg:97.00ms
step:1381/1700 train_time:133964ms step_avg:97.00ms
step:1382/1700 train_time:134063ms step_avg:97.01ms
step:1383/1700 train_time:134163ms step_avg:97.01ms
step:1384/1700 train_time:134262ms step_avg:97.01ms
step:1385/1700 train_time:134362ms step_avg:97.01ms
step:1386/1700 train_time:134465ms step_avg:97.02ms
step:1387/1700 train_time:134566ms step_avg:97.02ms
step:1388/1700 train_time:134668ms step_avg:97.02ms
step:1389/1700 train_time:134770ms step_avg:97.03ms
step:1390/1700 train_time:134871ms step_avg:97.03ms
step:1391/1700 train_time:134973ms step_avg:97.03ms
step:1392/1700 train_time:135074ms step_avg:97.04ms
step:1393/1700 train_time:135174ms step_avg:97.04ms
step:1394/1700 train_time:135276ms step_avg:97.04ms
step:1395/1700 train_time:135377ms step_avg:97.04ms
step:1396/1700 train_time:135480ms step_avg:97.05ms
step:1397/1700 train_time:135582ms step_avg:97.05ms
step:1398/1700 train_time:135683ms step_avg:97.06ms
step:1399/1700 train_time:135786ms step_avg:97.06ms
step:1400/1700 train_time:135886ms step_avg:97.06ms
step:1401/1700 train_time:135989ms step_avg:97.07ms
step:1402/1700 train_time:136090ms step_avg:97.07ms
step:1403/1700 train_time:136191ms step_avg:97.07ms
step:1404/1700 train_time:136292ms step_avg:97.07ms
step:1405/1700 train_time:136394ms step_avg:97.08ms
step:1406/1700 train_time:136496ms step_avg:97.08ms
step:1407/1700 train_time:136599ms step_avg:97.09ms
step:1408/1700 train_time:136701ms step_avg:97.09ms
step:1409/1700 train_time:136804ms step_avg:97.09ms
step:1410/1700 train_time:136904ms step_avg:97.10ms
step:1411/1700 train_time:137004ms step_avg:97.10ms
step:1412/1700 train_time:137107ms step_avg:97.10ms
step:1413/1700 train_time:137208ms step_avg:97.10ms
step:1414/1700 train_time:137309ms step_avg:97.11ms
step:1415/1700 train_time:137410ms step_avg:97.11ms
step:1416/1700 train_time:137512ms step_avg:97.11ms
step:1417/1700 train_time:137614ms step_avg:97.12ms
step:1418/1700 train_time:137715ms step_avg:97.12ms
step:1419/1700 train_time:137817ms step_avg:97.12ms
step:1420/1700 train_time:137918ms step_avg:97.13ms
step:1421/1700 train_time:138020ms step_avg:97.13ms
step:1422/1700 train_time:138120ms step_avg:97.13ms
step:1423/1700 train_time:138221ms step_avg:97.13ms
step:1424/1700 train_time:138324ms step_avg:97.14ms
step:1425/1700 train_time:138425ms step_avg:97.14ms
step:1426/1700 train_time:138525ms step_avg:97.14ms
step:1427/1700 train_time:138626ms step_avg:97.15ms
step:1428/1700 train_time:138728ms step_avg:97.15ms
step:1429/1700 train_time:138830ms step_avg:97.15ms
step:1430/1700 train_time:138933ms step_avg:97.16ms
step:1431/1700 train_time:139034ms step_avg:97.16ms
step:1432/1700 train_time:139135ms step_avg:97.16ms
step:1433/1700 train_time:139237ms step_avg:97.16ms
step:1434/1700 train_time:139339ms step_avg:97.17ms
step:1435/1700 train_time:139440ms step_avg:97.17ms
step:1436/1700 train_time:139543ms step_avg:97.17ms
step:1437/1700 train_time:139646ms step_avg:97.18ms
step:1438/1700 train_time:139746ms step_avg:97.18ms
step:1439/1700 train_time:139848ms step_avg:97.18ms
step:1440/1700 train_time:139953ms step_avg:97.19ms
step:1441/1700 train_time:140056ms step_avg:97.19ms
step:1442/1700 train_time:140155ms step_avg:97.20ms
step:1443/1700 train_time:140256ms step_avg:97.20ms
step:1444/1700 train_time:140357ms step_avg:97.20ms
step:1445/1700 train_time:140459ms step_avg:97.20ms
step:1446/1700 train_time:140560ms step_avg:97.21ms
step:1447/1700 train_time:140662ms step_avg:97.21ms
step:1448/1700 train_time:140763ms step_avg:97.21ms
step:1449/1700 train_time:140865ms step_avg:97.22ms
step:1450/1700 train_time:140967ms step_avg:97.22ms
step:1451/1700 train_time:141067ms step_avg:97.22ms
step:1452/1700 train_time:141168ms step_avg:97.22ms
step:1453/1700 train_time:141271ms step_avg:97.23ms
step:1454/1700 train_time:141374ms step_avg:97.23ms
step:1455/1700 train_time:141475ms step_avg:97.23ms
step:1456/1700 train_time:141577ms step_avg:97.24ms
step:1457/1700 train_time:141679ms step_avg:97.24ms
step:1458/1700 train_time:141780ms step_avg:97.24ms
step:1459/1700 train_time:141882ms step_avg:97.25ms
step:1460/1700 train_time:141984ms step_avg:97.25ms
step:1461/1700 train_time:142084ms step_avg:97.25ms
step:1462/1700 train_time:142186ms step_avg:97.25ms
step:1463/1700 train_time:142288ms step_avg:97.26ms
step:1464/1700 train_time:142389ms step_avg:97.26ms
step:1465/1700 train_time:142491ms step_avg:97.26ms
step:1466/1700 train_time:142592ms step_avg:97.27ms
step:1467/1700 train_time:142693ms step_avg:97.27ms
step:1468/1700 train_time:142796ms step_avg:97.27ms
step:1469/1700 train_time:142898ms step_avg:97.28ms
step:1470/1700 train_time:143000ms step_avg:97.28ms
step:1471/1700 train_time:143101ms step_avg:97.28ms
step:1472/1700 train_time:143202ms step_avg:97.28ms
step:1473/1700 train_time:143303ms step_avg:97.29ms
step:1474/1700 train_time:143404ms step_avg:97.29ms
step:1475/1700 train_time:143505ms step_avg:97.29ms
step:1476/1700 train_time:143607ms step_avg:97.29ms
step:1477/1700 train_time:143708ms step_avg:97.30ms
step:1478/1700 train_time:143811ms step_avg:97.30ms
step:1479/1700 train_time:143913ms step_avg:97.30ms
step:1480/1700 train_time:144014ms step_avg:97.31ms
step:1481/1700 train_time:144116ms step_avg:97.31ms
step:1482/1700 train_time:144217ms step_avg:97.31ms
step:1483/1700 train_time:144320ms step_avg:97.32ms
step:1484/1700 train_time:144421ms step_avg:97.32ms
step:1485/1700 train_time:144523ms step_avg:97.32ms
step:1486/1700 train_time:144625ms step_avg:97.33ms
step:1487/1700 train_time:144727ms step_avg:97.33ms
step:1488/1700 train_time:144829ms step_avg:97.33ms
step:1489/1700 train_time:144930ms step_avg:97.33ms
step:1490/1700 train_time:145032ms step_avg:97.34ms
step:1491/1700 train_time:145133ms step_avg:97.34ms
step:1492/1700 train_time:145235ms step_avg:97.34ms
step:1493/1700 train_time:145337ms step_avg:97.35ms
step:1494/1700 train_time:145438ms step_avg:97.35ms
step:1495/1700 train_time:145540ms step_avg:97.35ms
step:1496/1700 train_time:145642ms step_avg:97.35ms
step:1497/1700 train_time:145742ms step_avg:97.36ms
step:1498/1700 train_time:145843ms step_avg:97.36ms
step:1499/1700 train_time:145944ms step_avg:97.36ms
step:1500/1700 train_time:146046ms step_avg:97.36ms
step:1500/1700 val_loss:3.3219 train_time:146130ms step_avg:97.42ms
step:1501/1700 train_time:146154ms step_avg:97.37ms
step:1502/1700 train_time:146260ms step_avg:97.38ms
step:1503/1700 train_time:146360ms step_avg:97.38ms
step:1504/1700 train_time:146460ms step_avg:97.38ms
step:1505/1700 train_time:146561ms step_avg:97.38ms
step:1506/1700 train_time:146662ms step_avg:97.38ms
step:1507/1700 train_time:146763ms step_avg:97.39ms
step:1508/1700 train_time:146863ms step_avg:97.39ms
step:1509/1700 train_time:146964ms step_avg:97.39ms
step:1510/1700 train_time:147067ms step_avg:97.40ms
step:1511/1700 train_time:147170ms step_avg:97.40ms
step:1512/1700 train_time:147271ms step_avg:97.40ms
step:1513/1700 train_time:147373ms step_avg:97.40ms
step:1514/1700 train_time:147474ms step_avg:97.41ms
step:1515/1700 train_time:147579ms step_avg:97.41ms
step:1516/1700 train_time:147680ms step_avg:97.41ms
step:1517/1700 train_time:147781ms step_avg:97.42ms
step:1518/1700 train_time:147883ms step_avg:97.42ms
step:1519/1700 train_time:147987ms step_avg:97.42ms
step:1520/1700 train_time:148090ms step_avg:97.43ms
step:1521/1700 train_time:148191ms step_avg:97.43ms
step:1522/1700 train_time:148294ms step_avg:97.43ms
step:1523/1700 train_time:148395ms step_avg:97.44ms
step:1524/1700 train_time:148496ms step_avg:97.44ms
step:1525/1700 train_time:148599ms step_avg:97.44ms
step:1526/1700 train_time:148700ms step_avg:97.44ms
step:1527/1700 train_time:148803ms step_avg:97.45ms
step:1528/1700 train_time:148907ms step_avg:97.45ms
step:1529/1700 train_time:149007ms step_avg:97.45ms
step:1530/1700 train_time:149109ms step_avg:97.46ms
step:1531/1700 train_time:149210ms step_avg:97.46ms
step:1532/1700 train_time:149310ms step_avg:97.46ms
step:1533/1700 train_time:149412ms step_avg:97.46ms
step:1534/1700 train_time:149513ms step_avg:97.47ms
step:1535/1700 train_time:149614ms step_avg:97.47ms
step:1536/1700 train_time:149715ms step_avg:97.47ms
step:1537/1700 train_time:149817ms step_avg:97.47ms
step:1538/1700 train_time:149920ms step_avg:97.48ms
step:1539/1700 train_time:150022ms step_avg:97.48ms
step:1540/1700 train_time:150124ms step_avg:97.48ms
step:1541/1700 train_time:150227ms step_avg:97.49ms
step:1542/1700 train_time:150330ms step_avg:97.49ms
step:1543/1700 train_time:150431ms step_avg:97.49ms
step:1544/1700 train_time:150533ms step_avg:97.50ms
step:1545/1700 train_time:150633ms step_avg:97.50ms
step:1546/1700 train_time:150734ms step_avg:97.50ms
step:1547/1700 train_time:150836ms step_avg:97.50ms
step:1548/1700 train_time:150939ms step_avg:97.51ms
step:1549/1700 train_time:151041ms step_avg:97.51ms
step:1550/1700 train_time:151143ms step_avg:97.51ms
step:1551/1700 train_time:151245ms step_avg:97.51ms
step:1552/1700 train_time:151346ms step_avg:97.52ms
step:1553/1700 train_time:151448ms step_avg:97.52ms
step:1554/1700 train_time:151550ms step_avg:97.52ms
step:1555/1700 train_time:151652ms step_avg:97.53ms
step:1556/1700 train_time:151752ms step_avg:97.53ms
step:1557/1700 train_time:151855ms step_avg:97.53ms
step:1558/1700 train_time:151957ms step_avg:97.53ms
step:1559/1700 train_time:152058ms step_avg:97.54ms
step:1560/1700 train_time:152159ms step_avg:97.54ms
step:1561/1700 train_time:152261ms step_avg:97.54ms
step:1562/1700 train_time:152363ms step_avg:97.54ms
step:1563/1700 train_time:152467ms step_avg:97.55ms
step:1564/1700 train_time:152568ms step_avg:97.55ms
step:1565/1700 train_time:152669ms step_avg:97.55ms
step:1566/1700 train_time:152770ms step_avg:97.55ms
step:1567/1700 train_time:152872ms step_avg:97.56ms
step:1568/1700 train_time:152975ms step_avg:97.56ms
step:1569/1700 train_time:153076ms step_avg:97.56ms
step:1570/1700 train_time:153178ms step_avg:97.57ms
step:1571/1700 train_time:153279ms step_avg:97.57ms
step:1572/1700 train_time:153380ms step_avg:97.57ms
step:1573/1700 train_time:153484ms step_avg:97.57ms
step:1574/1700 train_time:153585ms step_avg:97.58ms
step:1575/1700 train_time:153687ms step_avg:97.58ms
step:1576/1700 train_time:153790ms step_avg:97.58ms
step:1577/1700 train_time:153894ms step_avg:97.59ms
step:1578/1700 train_time:153996ms step_avg:97.59ms
step:1579/1700 train_time:154096ms step_avg:97.59ms
step:1580/1700 train_time:154197ms step_avg:97.59ms
step:1581/1700 train_time:154299ms step_avg:97.60ms
step:1582/1700 train_time:154400ms step_avg:97.60ms
step:1583/1700 train_time:154503ms step_avg:97.60ms
step:1584/1700 train_time:154606ms step_avg:97.61ms
step:1585/1700 train_time:154707ms step_avg:97.61ms
step:1586/1700 train_time:154811ms step_avg:97.61ms
step:1587/1700 train_time:154911ms step_avg:97.61ms
step:1588/1700 train_time:155013ms step_avg:97.62ms
step:1589/1700 train_time:155114ms step_avg:97.62ms
step:1590/1700 train_time:155215ms step_avg:97.62ms
step:1591/1700 train_time:155318ms step_avg:97.62ms
step:1592/1700 train_time:155419ms step_avg:97.63ms
step:1593/1700 train_time:155521ms step_avg:97.63ms
step:1594/1700 train_time:155628ms step_avg:97.63ms
step:1595/1700 train_time:155729ms step_avg:97.64ms
step:1596/1700 train_time:155830ms step_avg:97.64ms
step:1597/1700 train_time:155932ms step_avg:97.64ms
step:1598/1700 train_time:156033ms step_avg:97.64ms
step:1599/1700 train_time:156133ms step_avg:97.64ms
step:1600/1700 train_time:156234ms step_avg:97.65ms
step:1601/1700 train_time:156335ms step_avg:97.65ms
step:1602/1700 train_time:156437ms step_avg:97.65ms
step:1603/1700 train_time:156540ms step_avg:97.65ms
step:1604/1700 train_time:156641ms step_avg:97.66ms
step:1605/1700 train_time:156744ms step_avg:97.66ms
step:1606/1700 train_time:156845ms step_avg:97.66ms
step:1607/1700 train_time:156947ms step_avg:97.66ms
step:1608/1700 train_time:157049ms step_avg:97.67ms
step:1609/1700 train_time:157150ms step_avg:97.67ms
step:1610/1700 train_time:157254ms step_avg:97.67ms
step:1611/1700 train_time:157356ms step_avg:97.68ms
step:1612/1700 train_time:157459ms step_avg:97.68ms
step:1613/1700 train_time:157559ms step_avg:97.68ms
step:1614/1700 train_time:157660ms step_avg:97.68ms
step:1615/1700 train_time:157761ms step_avg:97.68ms
step:1616/1700 train_time:157862ms step_avg:97.69ms
step:1617/1700 train_time:157964ms step_avg:97.69ms
step:1618/1700 train_time:158065ms step_avg:97.69ms
step:1619/1700 train_time:158170ms step_avg:97.70ms
step:1620/1700 train_time:158272ms step_avg:97.70ms
step:1621/1700 train_time:158373ms step_avg:97.70ms
step:1622/1700 train_time:158473ms step_avg:97.70ms
step:1623/1700 train_time:158573ms step_avg:97.70ms
step:1624/1700 train_time:158675ms step_avg:97.71ms
step:1625/1700 train_time:158778ms step_avg:97.71ms
step:1625/1700 val_loss:3.2929 train_time:158862ms step_avg:97.76ms
step:1626/1700 train_time:158885ms step_avg:97.72ms
step:1627/1700 train_time:158993ms step_avg:97.72ms
step:1628/1700 train_time:159094ms step_avg:97.72ms
step:1629/1700 train_time:159195ms step_avg:97.73ms
step:1630/1700 train_time:159296ms step_avg:97.73ms
step:1631/1700 train_time:159397ms step_avg:97.73ms
step:1632/1700 train_time:159498ms step_avg:97.73ms
step:1633/1700 train_time:159599ms step_avg:97.73ms
step:1634/1700 train_time:159701ms step_avg:97.74ms
step:1635/1700 train_time:159801ms step_avg:97.74ms
step:1636/1700 train_time:159904ms step_avg:97.74ms
step:1637/1700 train_time:160007ms step_avg:97.74ms
step:1638/1700 train_time:160110ms step_avg:97.75ms
step:1639/1700 train_time:160211ms step_avg:97.75ms
step:1640/1700 train_time:160312ms step_avg:97.75ms
step:1641/1700 train_time:160416ms step_avg:97.75ms
step:1642/1700 train_time:160517ms step_avg:97.76ms
step:1643/1700 train_time:160619ms step_avg:97.76ms
step:1644/1700 train_time:160721ms step_avg:97.76ms
step:1645/1700 train_time:160824ms step_avg:97.77ms
step:1646/1700 train_time:160928ms step_avg:97.77ms
step:1647/1700 train_time:161032ms step_avg:97.77ms
step:1648/1700 train_time:161135ms step_avg:97.78ms
step:1649/1700 train_time:161239ms step_avg:97.78ms
step:1650/1700 train_time:161339ms step_avg:97.78ms
step:1651/1700 train_time:161442ms step_avg:97.78ms
step:1652/1700 train_time:161542ms step_avg:97.79ms
step:1653/1700 train_time:161645ms step_avg:97.79ms
step:1654/1700 train_time:161748ms step_avg:97.79ms
step:1655/1700 train_time:161850ms step_avg:97.79ms
step:1656/1700 train_time:161953ms step_avg:97.80ms
step:1657/1700 train_time:162054ms step_avg:97.80ms
step:1658/1700 train_time:162156ms step_avg:97.80ms
step:1659/1700 train_time:162262ms step_avg:97.81ms
step:1660/1700 train_time:162365ms step_avg:97.81ms
step:1661/1700 train_time:162469ms step_avg:97.81ms
step:1662/1700 train_time:162572ms step_avg:97.82ms
step:1663/1700 train_time:162674ms step_avg:97.82ms
step:1664/1700 train_time:162777ms step_avg:97.82ms
step:1665/1700 train_time:162880ms step_avg:97.83ms
step:1666/1700 train_time:162983ms step_avg:97.83ms
step:1667/1700 train_time:163084ms step_avg:97.83ms
step:1668/1700 train_time:163188ms step_avg:97.83ms
step:1669/1700 train_time:163292ms step_avg:97.84ms
step:1670/1700 train_time:163393ms step_avg:97.84ms
step:1671/1700 train_time:163495ms step_avg:97.84ms
step:1672/1700 train_time:163598ms step_avg:97.85ms
step:1673/1700 train_time:163699ms step_avg:97.85ms
step:1674/1700 train_time:163802ms step_avg:97.85ms
step:1675/1700 train_time:163903ms step_avg:97.85ms
step:1676/1700 train_time:164006ms step_avg:97.86ms
step:1677/1700 train_time:164108ms step_avg:97.86ms
step:1678/1700 train_time:164211ms step_avg:97.86ms
step:1679/1700 train_time:164314ms step_avg:97.86ms
step:1680/1700 train_time:164416ms step_avg:97.87ms
step:1681/1700 train_time:164521ms step_avg:97.87ms
step:1682/1700 train_time:164624ms step_avg:97.87ms
step:1683/1700 train_time:164725ms step_avg:97.88ms
step:1684/1700 train_time:164829ms step_avg:97.88ms
step:1685/1700 train_time:164932ms step_avg:97.88ms
step:1686/1700 train_time:165034ms step_avg:97.88ms
step:1687/1700 train_time:165136ms step_avg:97.89ms
step:1688/1700 train_time:165237ms step_avg:97.89ms
step:1689/1700 train_time:165340ms step_avg:97.89ms
step:1690/1700 train_time:165442ms step_avg:97.89ms
step:1691/1700 train_time:165543ms step_avg:97.90ms
step:1692/1700 train_time:165644ms step_avg:97.90ms
step:1693/1700 train_time:165746ms step_avg:97.90ms
step:1694/1700 train_time:165848ms step_avg:97.90ms
step:1695/1700 train_time:165951ms step_avg:97.91ms
step:1696/1700 train_time:166053ms step_avg:97.91ms
step:1697/1700 train_time:166159ms step_avg:97.91ms
step:1698/1700 train_time:166261ms step_avg:97.92ms
step:1699/1700 train_time:166362ms step_avg:97.92ms
step:1700/1700 train_time:166465ms step_avg:97.92ms
step:1700/1700 val_loss:3.2793 train_time:166548ms step_avg:97.97ms
peak memory allocated: 33278 MiB reserved: 49252 MiB

import os
import sys
with open(sys.argv[0]) as f:
    code = f.read() # read the code of this file ASAP, for logging

with open('optimizer.py', 'r', encoding='utf-8') as f:
    source_code = f.read()
    code += source_code

with open('model.py', 'r', encoding='utf-8') as f:
    source_code = f.read()
    code += source_code

with open('utils.py', 'r', encoding='utf-8') as f:
    source_code = f.read()
    code += source_code

with open('dataloading.py', 'r', encoding='utf-8') as f:
    source_code = f.read()
    code += source_code

import argparse
import uuid
import time
import contextlib
import math
import torch
import torch.distributed as dist
import torch._inductor.config as config
from torch.nn.parallel import DistributedDataParallel as DDP
from pathlib import Path

from optimizer import Muon
from model import ModelConfig, ESM, CastedLinear
from dataloading import DistributedDataLoader


def get_args():
    parser = argparse.ArgumentParser(description='ESM2 training arguments')
    
    # Model hyperparams
    parser.add_argument('--vocab_size', type=int, default=33, help='vocabulary size')
    parser.add_argument('--num_hidden_layers', type=int, default=24, help='number of transformer layers')
    parser.add_argument('--num_attention_heads', type=int, default=6, help='number of attention heads (head dim 128 suggested by @Grad62304977)')
    parser.add_argument('--hidden_size', type=int, default=768, help='model hidden dimension size')
    
    # Data hyperparams
    parser.add_argument('--input_bin', type=str, default='data/omgprot50/omgprot50_train_*.bin', help='input .bins to train on')
    parser.add_argument('--input_valid_bin', type=str, default='data/omgprot50/omgprot50_valid_*.bin', help='input .bins to eval validation loss on')
    parser.add_argument('--input_test_bin', type=str, default='data/omgprot50/omgprot50_test_*.bin', help='input .bins to eval test loss on')   
    
    # Optimization hyperparams
    parser.add_argument('--batch_size', type=int, default=8*64*1024, help='batch size, in tokens, across all devices')
    parser.add_argument('--grad_accum', type=int, default=1, help='manually set number of gradient accumulation steps, else, will be ddp_world_size')
    parser.add_argument('--num_steps', type=int, default=25000, help='number of iterations to run')
    parser.add_argument('--warmup_steps', type=int, default=1000, help='number of warmup steps')
    parser.add_argument('--cooldown_steps', type=int, default=1000, help='number of cooldown steps')
    
    # Evaluation and logging hyperparams
    parser.add_argument('--valid_loss_every', type=int, default=1000, help='every how many steps to evaluate val loss? 0 for only at the end')
    parser.add_argument('--hf_model_name', type=str, default='Synthyra/esm_speedrun', help='huggingface model name')
    parser.add_argument('--token', type=str, default=None, help='huggingface token')
    parser.add_argument('--save_every', type=int, default=None, help='save every how many steps? None for no saving')
    args = parser.parse_args()
    return args


def get_param_count(model):
    total_params = 0
    for _, param in model.named_parameters():
        total_params += param.numel()
    return total_params


if __name__ == "__main__":
    args = get_args()
    if args.token:
        from huggingface_hub import login
        login(args.token)
    model_config = ModelConfig(args)

    # set up DDP (distributed data parallel) if available, otherwise single GPU
    if 'RANK' in os.environ:
        ddp_rank = int(os.environ['RANK'])
        ddp_local_rank = int(os.environ['LOCAL_RANK'])
        ddp_world_size = int(os.environ['WORLD_SIZE'])
        device = torch.device(f'cuda:{ddp_local_rank}')
        torch.cuda.set_device(device)
        dist.init_process_group(backend='nccl', device_id=device)
        dist.barrier()
        master_process = (ddp_rank == 0)
    else:
        ddp_rank = 0
        ddp_local_rank = 0 
        ddp_world_size = 1
        device = torch.device('cuda:0')
        torch.cuda.set_device(device)
        master_process = True

    print(f'using device: {device}')

    # begin logging
    logfile = None
    if master_process:
        run_id = uuid.uuid4()
        Path('logs').mkdir(exist_ok=True)
        # logdir = Path('logs') / f'{run_id}'
        # logdir.mkdir()
        logfile = Path('logs') / f'{run_id}.txt'
        print(logfile.stem)
        # create the log file
        with logfile.open('w') as f:
            # begin the log by printing this file (the Python code)
            print(code, file=f)
            print('=' * 100, file=f)

    def print0(s, logonly=False):
        if master_process:
            with logfile.open('a') as f:
                if not logonly:
                    print(s)
                print(s, file=f)

    # log information about the hardware/software environment this is running on
    # and print the full `nvidia-smi` to file
    print0(f'Running python {sys.version}')
    print0(f'Running pytorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}\nnvidia-smi:')
    import subprocess
    result = subprocess.run(['nvidia-smi'], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)
    print0(f'{result.stdout}', logonly=True)
    print0('='*100, logonly=True)

    # calculate the steps of gradient accumulation required to attain the desired global batch size
    # args.batch_size should refer to the total amount of tokens per backward pass
    train_accumulation_steps = 1
    batch_size = args.batch_size

    assert ddp_world_size == 1 or args.grad_accum == 1, "Cannot currently use both DDP and gradient accumulation"
    if ddp_world_size > 1:
        train_accumulation_steps = ddp_world_size
        batch_size = args.batch_size // ddp_world_size 
    elif args.grad_accum > 1:
        train_accumulation_steps *= args.grad_accum
        batch_size = args.batch_size // args.grad_accum

    print0(f'Train accumulation steps: {train_accumulation_steps}')
    print0(f'Adjusted local batch size: {batch_size} tokens')
    print0(f'Across {ddp_world_size} GPUs')
    print0(f'Total batch size: {args.batch_size} tokens')

    # load tokens
    train_loader = DistributedDataLoader(args.input_bin, batch_size, ddp_rank, ddp_world_size)
    valid_loader = DistributedDataLoader(args.input_valid_bin, batch_size, ddp_rank, ddp_world_size)
    test_loader = DistributedDataLoader(args.input_test_bin, batch_size // 4, ddp_rank, ddp_world_size)
    print0(f"Training DataLoader: total number of tokens: {train_loader.total_num_tokens} across {len(train_loader.files)} files")
    print0(f"Validation DataLoader: total number of tokens: {valid_loader.total_num_tokens} across {len(valid_loader.files)} files")
    print0(f"Testing DataLoader: total number of tokens: {test_loader.total_num_tokens} across {len(test_loader.files)} files")
    print0('='*100, logonly=True)

    valid_steps = valid_loader.total_num_tokens // args.batch_size
    test_steps = test_loader.total_num_tokens // args.batch_size

    input_ids = train_loader.next_batch()

    model = ESM(model_config)
    model = model.cuda().bfloat16()
    for m in model.modules():
        if isinstance(m, CastedLinear):
            m.float()
    config.coordinate_descent_tuning = True # suggested by @Chillee
    model = torch.compile(model)

    # wrap model in DDP only if using distributed training
    if ddp_world_size > 1:
        model = DDP(model, device_ids=[ddp_local_rank], broadcast_buffers=False, gradient_as_bucket_view=True)
        raw_model = model.module
    else:
        raw_model = model

    # init the optimizers
    embed_params = [*raw_model.embed.parameters(), *raw_model.value_embeds.parameters()]
    params = list(raw_model.blocks.parameters())
    matrix_params = [p for p in params if p.ndim == 2]
    scalar_params = [p for p in params if p.ndim < 2] + [raw_model.skip_weights]
    optimizer1 = torch.optim.Adam(embed_params, lr=0.6, betas=(0.8, 0.95), fused=True)
    optimizer2 = torch.optim.Adam([raw_model.lm_head.weight], lr=0.008, betas=(0.8, 0.95), fused=True)
    optimizer3 = Muon(matrix_params, lr=0.05, momentum=0.95)
    optimizer4 = torch.optim.Adam(scalar_params, lr=0.04, betas=(0.8, 0.95), fused=True)
    optimizers = [optimizer1, optimizer2, optimizer3, optimizer4]

    # learning rate decay scheduler (linear warmup and cooldown)
    def get_lr(it):
        assert it <= args.num_steps
        # 1) linear warmup for warmup_steps steps
        if it < args.warmup_steps:
            return (it+1) / args.warmup_steps
        # 2) constant lr for a while
        elif it < args.num_steps - args.cooldown_steps:
            return 1.0
        # 3) linear cooldown
        else:
            decay_ratio = (args.num_steps - it) / args.cooldown_steps
            return decay_ratio

    schedulers = [torch.optim.lr_scheduler.LambdaLR(opt, get_lr) for opt in optimizers]

    sliding_window_size = torch.tensor(1024 - 128, dtype=torch.int32, device="cuda")
    sw_prev = 1024 - 128
    # Start training loop
    training_time_ms = 0
    # start the clock
    torch.cuda.synchronize()
    t0 = time.perf_counter()

    ### BEGIN TRAINING LOOP ###
    for step in range(args.num_steps + 1):
        last_step = (step == args.num_steps)
        # This effectively ignores timing first 10 steps, which are slower for weird reasons.
        # Alternately, and slightly more correctly in terms of benchmarking, we could do 10
        # steps with dummy data first, and then re-initialize the model and reset the loader.
        if step == 10:
            training_time_ms = 0
            t0 = time.perf_counter()
        timed_steps = float('nan') if step <= 11 else (step - 10) + 1 # <= 11 to avoid bug in val

        # Linearly increase the sliding window size over training in chunks of 128 from 1024 -> 2048. By @fernbear.bsky.social
        frac_done = step / args.num_steps # training progress
        sw_size = int(((1 - frac_done) * 1023 + frac_done * 2048) // 128) * 128
        if sw_size != sw_prev:
            sliding_window_size.copy_(sw_size, non_blocking=True)
            sw_prev = sw_size

        # once in a while evaluate the validation dataset
        if args.valid_loss_every > 0 and step % args.valid_loss_every == 0 or last_step:
            # stop the clock
            torch.cuda.synchronize()
            training_time_ms += 1000 * (time.perf_counter() - t0)
            # run validation batches
            model.eval()
            valid_loader.reset()
            val_loss = 0.0
            with torch.no_grad():
                for _ in range(valid_steps):
                    input_ids = valid_loader.next_batch()
                    val_loss += model(input_ids, sliding_window_size)
            if ddp_world_size > 1:
                dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
            val_loss /= valid_steps
            # log val loss to console and to logfile
            print0(f'step:{step}/{args.num_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/(timed_steps-1):.2f}ms perplexity:{(math.e**val_loss):.4f} param_count:{get_param_count(model):,}')
            # start the clock again
            torch.cuda.synchronize()
            t0 = time.perf_counter()

        # save checkpoint every `save_every` steps
        if master_process and args.save_every:
            if last_step or (step % args.save_every == 0):
                # stop the clock
                torch.cuda.synchronize()
                training_time_ms += 1000 * (time.perf_counter() - t0)
                # save the state of the training process
                log = dict(step=step, code=code, model=raw_model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
                torch.save(log, 'logs/%s/state_step%06d.pt' % (run_id, step))
                # start the clock again
                torch.cuda.synchronize()
                t0 = time.perf_counter()

        if last_step:
            break

        # --------------- FORWARD AND BACKWARD PASS -----------------
        model.train()
        for i in range(1, train_accumulation_steps + 1):
            with contextlib.ExitStack() as stack:
                if ddp_world_size > 1 and i < train_accumulation_steps: # there's no need to sync gradients every accumulation step
                    stack.enter_context(model.no_sync())
                #if step >= 5:
                #    stack.enter_context(torch.compiler.set_stance(skip_guard_eval_unsafe=True))
                model(input_ids, sliding_window_size).backward()
                input_ids = train_loader.next_batch()
        if train_accumulation_steps != 1:
            for p in model.parameters():
                p.grad /= train_accumulation_steps
        # momentum warmup for Muon
        frac = min(step/300, 1)
        for group in optimizer3.param_groups:
            group['momentum'] = (1 - frac) * 0.85 + frac * 0.95
        # step the optimizers and schedulers
        for opt, sched in zip(optimizers, schedulers):
            opt.step()
            sched.step()
        # null the gradients
        model.zero_grad(set_to_none=True)
        # --------------- FORWARD AND BACKWARD PASS END -------------------
        # everything that follows now is just eval, diagnostics, prints, logging, etc.
        if step % 100 == 0:
            approx_time = training_time_ms + 1000 * (time.perf_counter() - t0)
            print0(f"step:{step+1}/{args.num_steps} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms")

    print0(f"peak memory consumption training: {torch.cuda.max_memory_allocated() // 1024 // 1024 // 1024} GiB")

    # save the model to huggingface
    try:
        model.push_to_hub(args.hf_model_name)
    except Exception as e:
        print(e)

    torch.cuda.empty_cache()
    torch.cuda.synchronize()
    torch.manual_seed(42)
    import numpy as np
    from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, matthews_corrcoef
    model.eval()
    results, all_true, all_pred = [], [], []
    total_loss = 0.0

    from tqdm import tqdm
    with torch.no_grad():
        for _ in tqdm(range(test_steps), desc="Evaluating"):
            input_ids = test_loader.next_batch()
            logits, loss, labels = model.inference(input_ids, sliding_window_size)     
            """
            TODO
            Figure out why inference takes up so much more memory than training
            Probably returning the logits and labels?
            """  
            all_true.extend(labels.cpu().numpy().flatten())
            all_pred.extend(logits.argmax(dim=-1).cpu().numpy().flatten())
            total_loss += loss.detach().cpu().item()

    average_loss = total_loss / test_steps
    perplexity = torch.exp(torch.tensor(average_loss)).item()
    all_true = np.array(all_true)
    all_pred = np.array(all_pred)
    mask = (all_true != -100)
    all_true = all_true[mask]
    all_pred = all_pred[mask]

    precision = precision_score(all_true, all_pred, average='weighted')
    recall = recall_score(all_true, all_pred, average='weighted')
    f1 = f1_score(all_true, all_pred, average='weighted')
    accuracy = accuracy_score(all_true, all_pred)
    mcc = matthews_corrcoef(all_true, all_pred)

    print0("Final Results:")
    print0(f"  Loss:        {average_loss:.4f}")
    print0(f"  Perplexity:  {perplexity:.4f}")
    print0(f"  Precision:   {precision:.4f}")
    print0(f"  Recall:      {recall:.4f}")
    print0(f"  F1:          {f1:.4f}")
    print0(f"  Accuracy:    {accuracy:.4f}")
    print0(f"  MCC:         {mcc:.4f}")

    print0(f"peak memory consumption testing: {torch.cuda.max_memory_allocated() // 1024 // 1024 // 1024} GiB")
    # -------------------------------------------------------------------------
    # clean up nice
    if ddp_world_size > 1:
        dist.destroy_process_group()
import os
import torch
import torch.distributed as dist


### Muon optimizer
@torch.compile
def zeropower_via_newtonschulz5(G, steps):
    """
    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a
    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose
    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at
    zero even beyond the point where the iteration no longer converges all the way to one everywhere
    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T
    where S' is diagonal with S_{ii}' ~ Uniform(0.5, 1.5), which turns out not to hurt model
    performance at all relative to UV^T, where USV^T = G is the SVD.
    """
    assert len(G.shape) == 2
    a, b, c = (3.4445, -4.7750,  2.0315)
    X = G.bfloat16()
    if G.size(0) > G.size(1):
        X = X.T

    # Ensure spectral norm is at most 1
    X = X / (X.norm() + 1e-7)
    # Perform the NS iterations
    for _ in range(steps):
        A = X @ X.T
        B = b * A + c * A @ A # adapted from suggestion by @jxbz, @leloykun, and @YouJiacheng
        X = a * X + B @ X

    if G.size(0) > G.size(1):
        X = X.T
    return X


class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Some warnings:
    - This optimizer assumes that all parameters passed in are 2D.
    - It should not be used for the embedding layer, the final fully connected layer, or any {0,1}-D
    parameters; those should all be optimized by a standard method (e.g., AdamW).
    - To use it with 4D convolutional filters, it works well to just flatten their last 3 dimensions.
    - We believe it is unlikely to work well for training with small batch size.
    - We believe it may not work well for finetuning pretrained models, but we haven't tested this.
    - We have not yet tried this optimizer for training scenarios larger than NanoGPT (124M).

    Arguments:
        lr: The learning rate used by the internal SGD.
        momentum: The momentum used by the internal SGD.
        nesterov: Whether to use Nesterov-style momentum in the internal SGD. (recommended)
        ns_steps: The number of Newton-Schulz iteration steps to use.
    """
    def __init__(self, params, lr=0.02, momentum=0.95, nesterov=True, ns_steps=5):
        self.world_size = int(os.environ.get('WORLD_SIZE', '1'))
        self.rank = int(os.environ.get('RANK', '0'))
        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, ns_steps=ns_steps)
        params = list(params)
        assert all(isinstance(p, torch.Tensor) for p in params)
        sizes = {p.numel() for p in params}
        param_groups = [
            {
                'params': [p for p in params if p.numel() == size],
                'update_buffer': [
                    torch.empty(size, device='cuda', dtype=torch.bfloat16)
                    for _ in range(self.world_size)
                ],
            }
            for size in sizes
        ]
        super().__init__(param_groups, defaults)

    def step(self):
        for group in self.param_groups:
            lr = group['lr']
            momentum = group['momentum']
            nesterov = group['nesterov']
            ns_steps = group['ns_steps']
            update_buffers = group['update_buffer']
            # generate weight updates in distributed fashion
            params = group['params']
            assert len(params) % self.world_size == 0
            handle = None
            params_world = None
            def update_prev():
                if params_world is None:
                    return
                if handle is not None:
                    handle.wait()
                for p_world, g_world in zip(params_world, update_buffers):
                    p_world.data.add_(
                        g_world.view_as(p_world),
                        alpha=-lr * max(1, p_world.size(0) / p_world.size(1)) ** 0.5,
                    )
            for base_i in range(len(params))[::self.world_size]:
                p = params[base_i + self.rank]
                g = p.grad
                assert g is not None
                state = self.state[p]
                if 'momentum_buffer' not in state:
                    state['momentum_buffer'] = torch.zeros_like(g)
                buf = state['momentum_buffer']
                buf.lerp_(g, 1 - momentum)
                g = g.lerp_(buf, momentum) if nesterov else buf
                g = zeropower_via_newtonschulz5(g, steps=ns_steps).flatten()
                update_prev()
                if self.world_size > 1:
                    handle = dist.all_gather(update_buffers, g, async_op=True)
                else:
                    update_buffers[0].copy_(g)
                    handle = None
                params_world = params[base_i : base_i + self.world_size]
            update_prev()
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.nn.attention.flex_attention import flex_attention, create_block_mask
from transformers import EsmTokenizer, PretrainedConfig, PreTrainedModel
from utils import ProteinMasker
from typing import Optional, Tuple, List, Any


class ModelConfig(PretrainedConfig):
    def __init__(
        self,
        args,
        **kwargs
    ):
        super().__init__(**kwargs)
        # 33 tokens: https://huggingface.co/Synthyra/ESMplusplus_large/blob/main/modeling_esm_plusplus.py#L868-L874
        # Depth of the number of layers is typically more important than the depth of the hidden dimension for PLMs
        # ESM2-8M has 6 layers, 20 heads, 320 hidden dim: https://huggingface.co/facebook/esm2_t6_8M_UR50D/blob/main/config.json
        # ESM2-35M has 12 layers, 20 heads, 480 hidden dim: https://huggingface.co/facebook/esm2_t12_35M_UR50D/blob/main/config.json
        # ESM2-150M has 30 layers, 20 heads, 640 hidden dim: https://huggingface.co/facebook/esm2_t30_150M_UR50D/blob/main/config.json
        # ESM2-650M has 33 layers, 20 heads, 1280 hidden dim: https://huggingface.co/facebook/esm2_t33_650M_UR50D/blob/main/config.json
        self.vocab_size = args.vocab_size
        self.num_hidden_layers = args.num_hidden_layers
        self.num_attention_heads = args.num_attention_heads
        self.hidden_size = args.hidden_size


def norm(x: torch.Tensor) -> torch.Tensor:
    return F.rms_norm(x, (x.size(-1),))


class CastedLinear(nn.Linear):
    def __init__(self, in_features, out_features):
        super().__init__(in_features, out_features, bias=False)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        return F.linear(x, self.weight.to(x.dtype))


class Rotary(nn.Module):
    def __init__(self, dim, base=10000):
        super().__init__()
        self.register_buffer('inv_freq', (1 / base) ** (torch.arange(0, dim, 2) / dim))
        self.seq_len_cached = None
        self.cos_cached = None
        self.sin_cached = None

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        seq_len = x.shape[1]
        if seq_len != self.seq_len_cached:
            t = torch.arange(seq_len, device=x.device)
            freqs = torch.outer(t, self.inv_freq)
            self.seq_len_cached = seq_len
            self.cos_cached = freqs.cos()
            self.sin_cached = freqs.sin()
        cos, sin = self.cos_cached[None, :, None, :], self.sin_cached[None, :, None, :]
        # apply_rotary_emb(x, cos, sin)
        x1, x2 = x.chunk(2, dim=3)
        y1 = x1 * cos + x2 * sin
        y2 = x1 * (-sin) + x2 * cos
        return torch.cat((y1, y2), 3).type_as(x)


class SelfAttention(nn.Module):
    """
    TODO
    Add F.spda option
    Add causal option (flex and sdpa)
    """
    def __init__(self, dim, num_attention_heads):
        super().__init__()
        assert dim % num_attention_heads == 0
        self.num_attention_heads = num_attention_heads
        self.qkv = CastedLinear(dim, 3 * dim)
        self.lambdas = nn.Parameter(torch.tensor([0.5, 0.5]))
        self.rotary = Rotary(dim // num_attention_heads) # dim // num_attention_heads = head_dim
        self.o_proj = CastedLinear(dim, dim)
        self.o_proj.weight.data.zero_() # zero init suggested by @Grad62304977

    def forward_sdpa(self, x: torch.Tensor, vi: torch.Tensor, attention_mask: Optional[torch.Tensor] = None) -> torch.Tensor:
        """
        TODO
        Question? Is this output actually different than flex attention output?
        Likely yes because of scoremod and / or soft capping
        Would be good to be able to do inference this way for typical PLM inference pipelines
        https://pytorch.org/blog/flexattention/
        """
        B, T = x.size(0), x.size(1) # batch size, sequence length
        qkv = self.qkv(x)
        q, k, v = qkv.chunk(3, dim=-1)
        q = q.view(B, T, self.num_attention_heads, -1)
        k = k.view(B, T, self.num_attention_heads, -1)
        v = v.view(B, T, self.num_attention_heads, -1)
        v = self.lambdas[0] * v + self.lambdas[1] * vi.view_as(v) # @KoszarskyB & @Grad62304977
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = self.rotary(q), self.rotary(k)
        y = F.scaled_dot_product_attention(
            q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2),
            attn_mask=attention_mask,
            dropout_p=0.0,
            is_causal=False,
            enable_gqa=True
        )
        y = y.transpose(1, 2).contiguous().view_as(x) # re-assemble all head outputs side by side
        y = self.o_proj(y)
        return y

    def forward(self, x: torch.Tensor, vi: torch.Tensor, block_mask: torch.Tensor) -> torch.Tensor:
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "Must use batch size = 1 for FlexAttention"
        qkv = self.qkv(x)
        q, k, v = qkv.chunk(3, dim=-1)
        q = q.view(B, T, self.num_attention_heads, -1)
        k = k.view(B, T, self.num_attention_heads, -1)
        v = v.view(B, T, self.num_attention_heads, -1)
        v = self.lambdas[0] * v + self.lambdas[1] * vi.view_as(v) # @KoszarskyB & @Grad62304977
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = self.rotary(q), self.rotary(k)
        y = flex_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), block_mask=block_mask, enable_gqa=True)
        y = y.transpose(1, 2).contiguous().view_as(x) # re-assemble all head outputs side by side
        y = self.o_proj(y)
        return y


def correction_fn(expansion_ratio: float, d_model: int) -> int:
    return int(((expansion_ratio * d_model) + 255) // 256 * 256)


class MLP(nn.Module):
    def __init__(self, dim):
        super().__init__()
        self.up   = CastedLinear(dim, correction_fn(8/3, dim))
        self.down = CastedLinear(correction_fn(8/3, dim), dim)
        self.down.weight.data.zero_() # zero init suggested by @Grad62304977
        self.relu = nn.ReLU()

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        # https://arxiv.org/abs/2109.08668v2
        # ReLU squared ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        return self.down(self.relu(self.up(x)).square())


class Block(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.attn = SelfAttention(config.hidden_size, config.num_attention_heads)
        self.mlp = MLP(config.hidden_size)
        self.lambdas = nn.Parameter(torch.tensor([1., 0.]))

    def sdpa_forward(self, x: torch.Tensor, vi: torch.Tensor, x0: torch.Tensor, attention_mask: Optional[torch.Tensor] = None) -> torch.Tensor:
        x = self.lambdas[0] * x + self.lambdas[1] * x0
        x = x + self.attn.forward_sdpa(norm(x), vi, attention_mask)
        x = x + self.mlp(norm(x))
        return x

    def forward(self, x: torch.Tensor, vi: torch.Tensor, x0: torch.Tensor, block_mask: torch.Tensor) -> torch.Tensor:
        x = self.lambdas[0] * x + self.lambdas[1] * x0
        x = x + self.attn(norm(x), vi, block_mask)
        x = x + self.mlp(norm(x))
        return x


class ValueEmbedding(nn.Module):
    def __init__(self, config: "ModelConfig"):
        super().__init__()
        self.embed = nn.ModuleList([
            nn.Embedding(config.vocab_size, config.hidden_size)
            for _ in range(config.num_hidden_layers // 2)
        ])

    def forward(self, inputs: torch.Tensor) -> List[torch.Tensor]:
        ve = [emb(inputs) for emb in self.embed]
        ve += reversed(ve)
        return ve


class ESM(PreTrainedModel):
    """
    TODO
    Add causal option (flex and sdpa)
    """
    def __init__(self, config: ModelConfig):
        super().__init__(config)
        tokenizer = EsmTokenizer.from_pretrained('facebook/esm2_t6_8M_UR50D')
        self.masker = ProteinMasker(tokenizer, 0.20) # 20% masking rate https://arxiv.org/abs/2301.06568
        self.inference_masker = ProteinMasker(tokenizer, 0.15) # 15% masking rate for inference, ESM2
        self.cls_id = tokenizer.cls_token_id
        self.vocab_size = tokenizer.vocab_size
        self.num_hidden_layers = config.num_hidden_layers

        # U-net design by @brendanh0gan
        assert config.num_hidden_layers % 2 == 0, "Number of layers should be even for U-net design"
        self.num_encoder_layers = config.num_hidden_layers // 2 # Half of the layers for encoder
        self.num_decoder_layers = config.num_hidden_layers - self.num_encoder_layers # Remaining for decoder
        # Add learnable skip connection weights for decoder layers
        self.skip_weights = nn.Parameter(torch.ones(self.num_decoder_layers))

        self.embed = nn.Embedding(self.vocab_size, config.hidden_size)
        self.blocks = nn.ModuleList([Block(config) for _ in range(config.num_hidden_layers)])
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual learning
        # U-net structure on token value embeddings by @leloykun
        self.value_embeds = ValueEmbedding(config)
        self.lm_head = CastedLinear(config.hidden_size, self.vocab_size)
        self.lm_head.weight.data.zero_() # @Grad62304977
        self.cross_entropy = nn.CrossEntropyLoss()

    def embed_forward(self, input_ids: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, List[torch.Tensor]]:
        x = self.embed(input_ids[None])
        x = norm(x) # @Grad62304977
        x0 = x
        ve = self.value_embeds(input_ids)
        return x, x0, ve

    def get_logits(self, x: torch.Tensor) -> torch.Tensor:
        x = norm(x)
        logits = self.lm_head(x)
        logits = 30 * torch.tanh(logits / 30) # @Grad62304977
        logits = logits.float()
        return logits

    def sdpa_forward(self, input_ids: torch.Tensor, attention_mask: Optional[torch.Tensor] = None) -> torch.Tensor:
        if attention_mask is not None:
            attention_mask = attention_mask[:, None, None, :].bool()
        
        x, x0, ve = self.embed_forward(input_ids)
        ve_enc, ve_dec = ve[:self.num_encoder_layers], ve[self.num_encoder_layers:]

        skip_connections = []
        for i in range(self.num_encoder_layers):
            x = self.blocks[i].sdpa_forward(x, ve_enc[i], x0, attention_mask)
            skip_connections.append(x)

        for i in range(self.num_decoder_layers):
            x = x + self.skip_weights[i] * skip_connections.pop()
            x = self.blocks[self.num_encoder_layers + i].sdpa_forward(x, ve_dec[i], x0, attention_mask)

        return self.get_logits(x)

    def flex_forward(self, input_ids: torch.Tensor, sliding_window_size: torch.Tensor) -> torch.Tensor:
        input_ids = input_ids.flatten() # flex_attention needs batch 1
        docs = (input_ids == self.cls_id).cumsum(0)

        def doc_mask_mod(b, h, q_idx, kv_idx):
            bidirectional_sliding_window_mask = torch.abs(q_idx - kv_idx) < sliding_window_size
            doc_mask = docs[q_idx] == docs[kv_idx]
            return bidirectional_sliding_window_mask & doc_mask

        S = len(input_ids)
        block_mask = create_block_mask(doc_mask_mod, None, None, S, S)

        x, x0, ve = self.embed_forward(input_ids)
        ve_enc, ve_dec = ve[:self.num_encoder_layers], ve[self.num_encoder_layers:]

        # Store outputs for U-Net skip connections
        skip_connections = []
        # Encoder pass - process only the first half of the blocks
        for i in range(self.num_encoder_layers):
            x = self.blocks[i](x, ve_enc[i], x0, block_mask)
            skip_connections.append(x)
        # Decoder pass - process the remaining blocks with weighted skip connections
        for i in range(self.num_decoder_layers):
            x = x + self.skip_weights[i] * skip_connections.pop()
            # U-net structure on token value embeddings by @leloykun
            x = self.blocks[self.num_encoder_layers + i](x, ve_dec[i], x0, block_mask)

        return self.get_logits(x)

    def inference(self, input_ids: torch.Tensor, sliding_window_size: torch.Tensor = None) -> Tuple[torch.Tensor, Any, Any]:
        input_ids, labels = self.inference_masker(input_ids)
        logits = self.flex_forward(input_ids, sliding_window_size)
        loss = None
        if labels is not None:
            loss = self.cross_entropy(logits.view(-1, self.vocab_size), labels.view(-1).long())
        return logits, loss, labels

    def forward(self, input_ids: torch.Tensor, sliding_window_size: torch.Tensor) -> torch.Tensor:
        input_ids, labels = self.masker(input_ids)
        logits = self.flex_forward(input_ids, sliding_window_size)
        return self.cross_entropy(logits.view(-1, self.vocab_size), labels.view(-1).long())


if __name__ == '__main__':
    """
    TODO
    look at MSE between flex attention outputs and sdpa outputs
    """


import torch
from typing import Tuple

"""
Standardized MLM masking approach for consistency
"""

class ProteinMasker:
    def __init__(self, tokenizer, mlm_probability=0.15):
        """
        Initialize the ProteinMasker with the given tokenizer and masking parameters.
        Of the masked tokens, 80% are replaced with [MASK], 10% are replaced with a random amino acid token, and 10% are unchanged.
        """
        self.tokenizer = tokenizer
        self.mlm_probability = mlm_probability
        self.mask_token_id = tokenizer.mask_token_id
        self.special_tokens = torch.tensor(tokenizer.all_special_ids)
        canonical_amino_acids = 'ACDEFGHIKLMNPQRSTVWY'
        canonical_amino_acids_ids = tokenizer.convert_tokens_to_ids(list(canonical_amino_acids))
        self.low_range = min(canonical_amino_acids_ids)
        self.high_range = max(canonical_amino_acids_ids)

    def __call__(self, input_ids: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:
        labels = input_ids.clone()
        
        # Create special tokens mask using broadcasting
        special_tokens = self.special_tokens.to(input_ids.device)
        special_tokens_mask = (input_ids[..., None] == special_tokens).any(-1)
        
        # Create probability matrix and mask special tokens
        probability_matrix = torch.full_like(labels, self.mlm_probability, dtype=torch.float)
        probability_matrix.masked_fill_(special_tokens_mask, value=0.0)
        
        # Create masked indices
        masked_indices = torch.bernoulli(probability_matrix).bool()
        labels[~masked_indices] = -100  # We only compute loss on masked tokens
        
        # 80% of the time, we replace masked input tokens with tokenizer.mask_token ([MASK])
        indices_replaced = torch.bernoulli(torch.full_like(probability_matrix, 0.8)).bool() & masked_indices
        input_ids[indices_replaced] = self.mask_token_id
        
        # 10% of the time, we replace masked input tokens with random word
        indices_random = torch.bernoulli(torch.full_like(probability_matrix, 0.5)).bool() & masked_indices & ~indices_replaced
        random_words = torch.randint(low=self.low_range, high=self.high_range, size=labels.shape, dtype=input_ids.dtype, device=labels.device)
        input_ids[indices_random] = random_words[indices_random]
        
        # The rest of the time (10% of the time) we keep the masked input tokens unchanged
        return input_ids, labels


if __name__ == "__main__":
    from transformers import EsmTokenizer
    tokenizer = EsmTokenizer.from_pretrained("facebook/esm2_t6_8M_UR50D")
    test_seqs = [
        'MNFKYKLYSYITIFQIILILPTIVASNERCIALGGVCKDFSDCTGNYKPIDKHCDGSNNIKCCIRKIECPTSQNSNFTISGKNKEDEALPFIFKSEGGCQNDKNDNGNKINGKIGYTCAGITPMVGWKNKENYFSYAIKECTNDTNFTYCAYKLNENKFREGAKNIYIDKYAVAGKCNNLPQPAYYVCFDTSVNHGSGWSSKTITANPIGNMDGREYGLLLNKKSREKYINIVKNDSSQEKYLNGWLSRADDREKYCNNYCTSNCNCDNSASKASVSSNTNTTDIYNSVNTVDSDICNCDDNEPTDFLDDDYINNEEEIDEEIIDQEEY',
        'MYRTALYFTVCSIWLCQIITGVLSLKCKCDLCKDKNYTCITDGYCYTSATLKDGVILYNYRCLDLNFPMRNPMFCHKQIPIHHEFTLECCNDRDFCNIRLVPKLTPKDNATSDTSLGTIEIAVVIILPTLVICIIAMAIYLYYQNKRSTHHHLGLGDDSIEAPDHPILNGVSLKHMIEMTTSGSGSGLPLLVQRSIARQIQLVEIIGQGRYGEVWRGRWRGENVAVKIFSSREERSWFREAEIYQTVMLRHDNILGFIAADNKGVLSLKCKCDLCKDKNYTCITDGYCYTSATLKDGVILYNYRQLGASLNRFXVYALGLIFWEISRRCNVGGIYDEYQLPFYDAVPSDPTIEEMRRVVCVERQRPSIPNRWQSCEALHVMSKLMKECWYHNATARLTALRIKKTLANFRASEELKM'
    ]
    test_ids = tokenizer(test_seqs, return_tensors="pt", padding=True).input_ids
    masker = ProteinMasker(tokenizer, mlm_probability=0.5)
    print(masker.mask_token_id)
    print(masker.special_tokens)
    print(masker.low_range, masker.high_range)

    # First set of masking
    masked_ids1, labels1 = masker(test_ids.clone())
    masked_ids2, labels2 = masker(test_ids.clone())

    print("Before setting seed:")
    print("Original: ", test_ids[0][:20].tolist())
    print("Masking 1:", masked_ids1[0][:20].tolist()) 
    print("Masking 2:", masked_ids2[0][:20].tolist())
    print("Are they equal?", torch.equal(masked_ids1, masked_ids2))

    # Now with seed
    torch.manual_seed(42)
    masked_ids3, labels3 = masker(test_ids.clone())
    torch.manual_seed(42)
    masked_ids4, labels4 = masker(test_ids.clone())

    print("\nAfter setting seed:")
    print("Original: ", test_ids[0][:20].tolist())
    print("Masking 3:", masked_ids3[0][:20].tolist())
    print("Masking 4:", masked_ids4[0][:20].tolist()) 
    print("Are they equal?", torch.equal(masked_ids3, masked_ids4))
import torch
from pathlib import Path


def _peek_data_shard(file: Path):
    # only reads the header, returns header data
    # header is 256 int32
    header = torch.from_file(f"{file}", False, 256, dtype=torch.int32)
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    return int(header[2]) # number of tokens (claimed)


def _load_data_shard(path: Path, num_tokens):
    with path.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint8, pin_memory=True)
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy())
        assert nbytes == num_tokens, "number of tokens read does not match header?"
    return tokens


class DistributedDataLoader:
    def __init__(self, filename_pattern, batch_size, process_rank, num_processes):
        self.process_rank = process_rank
        self.num_processes = num_processes
        self.batch_size = batch_size

        # glob files that match the pattern
        self.files = sorted(Path.cwd().glob(filename_pattern))
        assert len(self.files) > 0, f"did not find any files that match the pattern {filename_pattern}"

        # load and validate all data shards, count number of tokens in total
        self.files_num_tokens = [_peek_data_shard(file) for file in self.files]
        self.total_num_tokens = sum(self.files_num_tokens)

        self.reset()

    def reset(self):
        self.current_shard = -1
        self.advance()

    def advance(self): # advance to next data shard
        self.current_shard = (self.current_shard + 1) % len(self.files)
        self.current_position = self.process_rank * self.batch_size
        self.tokens = _load_data_shard(self.files[self.current_shard], self.files_num_tokens[self.current_shard])

    def next_batch(self):
        batch_size = self.batch_size * self.num_processes
        buf = self.tokens[self.current_position:self.current_position+self.batch_size]
        # host side async is sufficient;
        # no performance improvement was observed when introducing a separate stream.
        input_ids = buf.to(device="cuda", dtype=torch.int32, non_blocking=True) # inputs
        # advance current position and load next shard if necessary
        self.current_position += batch_size
        if self.current_position + batch_size >= len(self.tokens):
            self.advance()
        return input_ids

====================================================================================================
Running python 3.10.12 (main, Nov  6 2024, 20:22:13) [GCC 11.4.0]
Running pytorch 2.6.0.dev20241203+cu124 compiled for CUDA 12.4
nvidia-smi:
Sat Dec 28 01:17:38 2024       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.127.05             Driver Version: 550.127.05     CUDA Version: 12.4     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA GH200 480GB             On  |   00000000:DD:00.0 Off |                    0 |
| N/A   37C    P0             83W /  700W |       4MiB /  97871MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+

====================================================================================================
Train accumulation steps: 8
Adjusted local batch size: 65536 tokens
Across 1 GPUs
Total batch size: 524288 tokens
Training DataLoader: total number of tokens: 10000000000 across 100 files
Validation DataLoader: total number of tokens: 2097660 across 1 files
Testing DataLoader: total number of tokens: 3686279 across 1 files
====================================================================================================
step:0/25000 val_loss:3.4965 train_time:0ms step_avg:nanms perplexity:33.0000 param_count:132,475,500
step:1/25000 train_time:51873ms step_avg:nanms
step:101/25000 train_time:146878ms step_avg:1614.04ms
step:201/25000 train_time:309510ms step_avg:1620.47ms
step:301/25000 train_time:472030ms step_avg:1622.10ms
step:401/25000 train_time:634208ms step_avg:1622.02ms
step:501/25000 train_time:796190ms step_avg:1621.57ms
step:601/25000 train_time:958241ms step_avg:1621.39ms
step:701/25000 train_time:1120182ms step_avg:1621.10ms
step:801/25000 train_time:1282168ms step_avg:1620.95ms
step:901/25000 train_time:1443945ms step_avg:1620.59ms
step:1000/25000 val_loss:2.4943 train_time:1604141ms step_avg:1620.34ms perplexity:12.1130 param_count:132,475,500
step:1001/25000 train_time:1605758ms step_avg:1620.34ms
step:1101/25000 train_time:1768695ms step_avg:1621.17ms
step:1201/25000 train_time:1930255ms step_avg:1620.70ms
step:1301/25000 train_time:2091358ms step_avg:1619.95ms
step:1401/25000 train_time:2252356ms step_avg:1619.24ms
step:1501/25000 train_time:2413177ms step_avg:1618.50ms
step:1601/25000 train_time:2574185ms step_avg:1617.97ms
step:1701/25000 train_time:2735159ms step_avg:1617.48ms
step:1801/25000 train_time:2896103ms step_avg:1617.03ms
step:1901/25000 train_time:3056936ms step_avg:1616.57ms
step:2000/25000 val_loss:2.4315 train_time:3216115ms step_avg:1616.14ms perplexity:11.3755 param_count:132,475,500
step:2001/25000 train_time:3217724ms step_avg:1616.13ms
step:2101/25000 train_time:3378657ms step_avg:1615.81ms
step:2201/25000 train_time:3539594ms step_avg:1615.52ms
step:2301/25000 train_time:3701294ms step_avg:1615.58ms
step:2401/25000 train_time:3862138ms step_avg:1615.28ms
step:2501/25000 train_time:4023001ms step_avg:1615.01ms
step:2601/25000 train_time:4183759ms step_avg:1614.73ms
step:2701/25000 train_time:4344569ms step_avg:1614.48ms
step:2801/25000 train_time:4505369ms step_avg:1614.25ms
step:2901/25000 train_time:4666237ms step_avg:1614.06ms
step:3000/25000 val_loss:2.3925 train_time:4825461ms step_avg:1613.87ms perplexity:10.9410 param_count:132,475,500
step:3001/25000 train_time:4827068ms step_avg:1613.86ms
step:3101/25000 train_time:4987972ms step_avg:1613.71ms
step:3201/25000 train_time:5149041ms step_avg:1613.61ms
step:3301/25000 train_time:5310170ms step_avg:1613.54ms
step:3401/25000 train_time:5472229ms step_avg:1613.75ms
step:3501/25000 train_time:5633132ms step_avg:1613.62ms
step:3601/25000 train_time:5794095ms step_avg:1613.50ms
step:3701/25000 train_time:5955008ms step_avg:1613.39ms
step:3801/25000 train_time:6115833ms step_avg:1613.25ms
step:3901/25000 train_time:6276741ms step_avg:1613.14ms
step:4000/25000 val_loss:2.3680 train_time:6436282ms step_avg:1613.10ms perplexity:10.6763 param_count:132,475,500
step:4001/25000 train_time:6437901ms step_avg:1613.10ms
step:4101/25000 train_time:6598774ms step_avg:1613.00ms
step:4201/25000 train_time:6759620ms step_avg:1612.89ms
step:4301/25000 train_time:6920501ms step_avg:1612.79ms
step:4401/25000 train_time:7081475ms step_avg:1612.72ms
step:4501/25000 train_time:7242194ms step_avg:1612.60ms
step:4601/25000 train_time:7403929ms step_avg:1612.71ms
step:4701/25000 train_time:7564884ms step_avg:1612.64ms
step:4801/25000 train_time:7725780ms step_avg:1612.56ms
step:4901/25000 train_time:7886926ms step_avg:1612.54ms
step:5000/25000 val_loss:2.3445 train_time:8046223ms step_avg:1612.47ms perplexity:10.4285 param_count:132,475,500
step:5001/25000 train_time:8047837ms step_avg:1612.47ms
step:5101/25000 train_time:8208731ms step_avg:1612.40ms
step:5201/25000 train_time:8369502ms step_avg:1612.31ms
step:5301/25000 train_time:8530364ms step_avg:1612.24ms
step:5401/25000 train_time:8691274ms step_avg:1612.18ms
step:5501/25000 train_time:8852106ms step_avg:1612.11ms
step:5601/25000 train_time:9013147ms step_avg:1612.08ms
step:5701/25000 train_time:9173935ms step_avg:1612.01ms
step:5801/25000 train_time:9335594ms step_avg:1612.09ms
step:5901/25000 train_time:9496429ms step_avg:1612.02ms
step:6000/25000 val_loss:2.3337 train_time:9655834ms step_avg:1611.99ms perplexity:10.3157 param_count:132,475,500
step:6001/25000 train_time:9657453ms step_avg:1611.99ms
step:6101/25000 train_time:9818329ms step_avg:1611.94ms
step:6201/25000 train_time:9979150ms step_avg:1611.88ms
step:6301/25000 train_time:10140048ms step_avg:1611.83ms
step:6401/25000 train_time:10301032ms step_avg:1611.80ms
step:6501/25000 train_time:10462122ms step_avg:1611.79ms
step:6601/25000 train_time:10623331ms step_avg:1611.79ms
step:6701/25000 train_time:10784356ms step_avg:1611.77ms
step:6801/25000 train_time:10945464ms step_avg:1611.76ms
step:6901/25000 train_time:11107668ms step_avg:1611.91ms
step:7000/25000 val_loss:2.3132 train_time:11267064ms step_avg:1611.88ms perplexity:10.1062 param_count:132,475,500
step:7001/25000 train_time:11268678ms step_avg:1611.88ms
step:7101/25000 train_time:11429770ms step_avg:1611.87ms
step:7201/25000 train_time:11590850ms step_avg:1611.86ms
step:7301/25000 train_time:11752067ms step_avg:1611.86ms
step:7401/25000 train_time:11913320ms step_avg:1611.87ms
step:7501/25000 train_time:12074389ms step_avg:1611.85ms
step:7601/25000 train_time:12235443ms step_avg:1611.84ms
step:7701/25000 train_time:12396342ms step_avg:1611.80ms
step:7801/25000 train_time:12557394ms step_avg:1611.78ms
step:7901/25000 train_time:12718519ms step_avg:1611.78ms
step:8000/25000 val_loss:2.3015 train_time:12877982ms step_avg:1611.76ms perplexity:9.9893 param_count:132,475,500
step:8001/25000 train_time:12879589ms step_avg:1611.76ms
step:8101/25000 train_time:13041665ms step_avg:1611.87ms
step:8201/25000 train_time:13202672ms step_avg:1611.85ms
step:8301/25000 train_time:13363838ms step_avg:1611.85ms
step:8401/25000 train_time:13524997ms step_avg:1611.85ms
step:8501/25000 train_time:13685917ms step_avg:1611.81ms
step:8601/25000 train_time:13846860ms step_avg:1611.79ms
step:8701/25000 train_time:14007983ms step_avg:1611.78ms
step:8801/25000 train_time:14169177ms step_avg:1611.78ms
step:8901/25000 train_time:14330150ms step_avg:1611.76ms
step:9000/25000 val_loss:2.2928 train_time:14489453ms step_avg:1611.73ms perplexity:9.9026 param_count:132,475,500
step:9001/25000 train_time:14491069ms step_avg:1611.73ms
step:9101/25000 train_time:14652243ms step_avg:1611.73ms
step:9201/25000 train_time:14814164ms step_avg:1611.81ms
step:9301/25000 train_time:14975100ms step_avg:1611.79ms
step:9401/25000 train_time:15136180ms step_avg:1611.78ms
step:9501/25000 train_time:15297366ms step_avg:1611.78ms
step:9601/25000 train_time:15458536ms step_avg:1611.78ms
step:9701/25000 train_time:15619810ms step_avg:1611.79ms
step:9801/25000 train_time:15780851ms step_avg:1611.77ms
step:9901/25000 train_time:15942116ms step_avg:1611.78ms
step:10000/25000 val_loss:2.2931 train_time:16101755ms step_avg:1611.79ms perplexity:9.9056 param_count:132,475,500
step:10001/25000 train_time:16103370ms step_avg:1611.79ms
step:10101/25000 train_time:16264749ms step_avg:1611.81ms
step:10201/25000 train_time:16425887ms step_avg:1611.80ms
step:10301/25000 train_time:16587102ms step_avg:1611.81ms
step:10401/25000 train_time:16749274ms step_avg:1611.90ms
step:10501/25000 train_time:16910496ms step_avg:1611.91ms
step:10601/25000 train_time:17071632ms step_avg:1611.90ms
step:10701/25000 train_time:17232824ms step_avg:1611.90ms
step:10801/25000 train_time:17394144ms step_avg:1611.91ms
step:10901/25000 train_time:17555268ms step_avg:1611.91ms
step:11000/25000 val_loss:2.2803 train_time:17714974ms step_avg:1611.92ms perplexity:9.7799 param_count:132,475,500
step:11001/25000 train_time:17716583ms step_avg:1611.92ms
step:11101/25000 train_time:17877693ms step_avg:1611.91ms
step:11201/25000 train_time:18038934ms step_avg:1611.91ms
step:11301/25000 train_time:18200228ms step_avg:1611.92ms
step:11401/25000 train_time:18361304ms step_avg:1611.91ms
step:11501/25000 train_time:18523379ms step_avg:1611.99ms
step:11601/25000 train_time:18684605ms step_avg:1611.99ms
step:11701/25000 train_time:18845786ms step_avg:1611.99ms
step:11801/25000 train_time:19006934ms step_avg:1611.99ms
step:11901/25000 train_time:19167971ms step_avg:1611.97ms
step:12000/25000 val_loss:2.2713 train_time:19327499ms step_avg:1611.97ms perplexity:9.6918 param_count:132,475,500
step:12001/25000 train_time:19329124ms step_avg:1611.97ms
step:12101/25000 train_time:19490339ms step_avg:1611.97ms
step:12201/25000 train_time:19651646ms step_avg:1611.98ms
step:12301/25000 train_time:19812826ms step_avg:1611.98ms
step:12401/25000 train_time:19973834ms step_avg:1611.96ms
step:12501/25000 train_time:20135008ms step_avg:1611.96ms
step:12601/25000 train_time:20296183ms step_avg:1611.96ms
step:12701/25000 train_time:20458473ms step_avg:1612.05ms
step:12801/25000 train_time:20619744ms step_avg:1612.05ms
step:12901/25000 train_time:20781043ms step_avg:1612.06ms
step:13000/25000 val_loss:2.2676 train_time:20940869ms step_avg:1612.08ms perplexity:9.6565 param_count:132,475,500
step:13001/25000 train_time:20942488ms step_avg:1612.08ms
step:13101/25000 train_time:21104060ms step_avg:1612.10ms
step:13201/25000 train_time:21265464ms step_avg:1612.12ms
step:13301/25000 train_time:21426728ms step_avg:1612.12ms
step:13401/25000 train_time:21588291ms step_avg:1612.15ms
step:13501/25000 train_time:21749430ms step_avg:1612.14ms
step:13601/25000 train_time:21910765ms step_avg:1612.15ms
step:13701/25000 train_time:22072082ms step_avg:1612.16ms
step:13801/25000 train_time:22234268ms step_avg:1612.23ms
step:13901/25000 train_time:22395592ms step_avg:1612.24ms
step:14000/25000 val_loss:2.2546 train_time:22555067ms step_avg:1612.23ms perplexity:9.5316 param_count:132,475,500
step:14001/25000 train_time:22556681ms step_avg:1612.23ms
step:14101/25000 train_time:22717795ms step_avg:1612.22ms
step:14201/25000 train_time:22878983ms step_avg:1612.22ms
step:14301/25000 train_time:23040124ms step_avg:1612.21ms
step:14401/25000 train_time:23201412ms step_avg:1612.22ms
step:14501/25000 train_time:23362648ms step_avg:1612.22ms
step:14601/25000 train_time:23523887ms step_avg:1612.22ms
step:14701/25000 train_time:23684987ms step_avg:1612.21ms
step:14801/25000 train_time:23846238ms step_avg:1612.21ms
step:14901/25000 train_time:24007532ms step_avg:1612.22ms
step:15000/25000 val_loss:2.2447 train_time:24168107ms step_avg:1612.28ms perplexity:9.4379 param_count:132,475,500
step:15001/25000 train_time:24169706ms step_avg:1612.28ms
step:15101/25000 train_time:24331020ms step_avg:1612.29ms
step:15201/25000 train_time:24492228ms step_avg:1612.29ms
step:15301/25000 train_time:24653330ms step_avg:1612.28ms
step:15401/25000 train_time:24814598ms step_avg:1612.28ms
step:15501/25000 train_time:24975675ms step_avg:1612.27ms
step:15601/25000 train_time:25136957ms step_avg:1612.27ms
step:15701/25000 train_time:25298406ms step_avg:1612.29ms
step:15801/25000 train_time:25459929ms step_avg:1612.31ms
step:15901/25000 train_time:25621531ms step_avg:1612.33ms
step:16000/25000 val_loss:2.2421 train_time:25781526ms step_avg:1612.35ms perplexity:9.4127 param_count:132,475,500
step:16001/25000 train_time:25783151ms step_avg:1612.35ms
step:16101/25000 train_time:25945771ms step_avg:1612.44ms
step:16201/25000 train_time:26107159ms step_avg:1612.45ms
step:16301/25000 train_time:26268987ms step_avg:1612.48ms
step:16401/25000 train_time:26430498ms step_avg:1612.50ms
step:16501/25000 train_time:26591887ms step_avg:1612.51ms
step:16601/25000 train_time:26753568ms step_avg:1612.53ms
step:16701/25000 train_time:26914985ms step_avg:1612.54ms
step:16801/25000 train_time:27076529ms step_avg:1612.56ms
step:16901/25000 train_time:27238105ms step_avg:1612.58ms
step:17000/25000 val_loss:2.2308 train_time:27397943ms step_avg:1612.59ms perplexity:9.3071 param_count:132,475,500
step:17001/25000 train_time:27399571ms step_avg:1612.59ms
step:17101/25000 train_time:27560982ms step_avg:1612.60ms
step:17201/25000 train_time:27722645ms step_avg:1612.63ms
step:17301/25000 train_time:27885295ms step_avg:1612.71ms
step:17401/25000 train_time:28046793ms step_avg:1612.72ms
step:17501/25000 train_time:28208532ms step_avg:1612.75ms
step:17601/25000 train_time:28370086ms step_avg:1612.76ms
step:17701/25000 train_time:28531568ms step_avg:1612.77ms
step:17801/25000 train_time:28693213ms step_avg:1612.79ms
step:17901/25000 train_time:28854832ms step_avg:1612.81ms
step:18000/25000 val_loss:2.2373 train_time:29014895ms step_avg:1612.83ms perplexity:9.3679 param_count:132,475,500
step:18001/25000 train_time:29016519ms step_avg:1612.84ms
step:18101/25000 train_time:29177965ms step_avg:1612.84ms
step:18201/25000 train_time:29339603ms step_avg:1612.86ms
step:18301/25000 train_time:29501135ms step_avg:1612.88ms
step:18401/25000 train_time:29662771ms step_avg:1612.90ms
step:18501/25000 train_time:29825396ms step_avg:1612.97ms
step:18601/25000 train_time:29986758ms step_avg:1612.97ms
step:18701/25000 train_time:30148296ms step_avg:1612.98ms
step:18801/25000 train_time:30310057ms step_avg:1613.01ms
step:18901/25000 train_time:30471739ms step_avg:1613.03ms
step:19000/25000 val_loss:2.2251 train_time:30631949ms step_avg:1613.06ms perplexity:9.2542 param_count:132,475,500
step:19001/25000 train_time:30633567ms step_avg:1613.06ms
step:19101/25000 train_time:30795338ms step_avg:1613.08ms
step:19201/25000 train_time:30957009ms step_avg:1613.10ms
step:19301/25000 train_time:31118637ms step_avg:1613.12ms
step:19401/25000 train_time:31280241ms step_avg:1613.13ms
step:19501/25000 train_time:31441991ms step_avg:1613.15ms
step:19601/25000 train_time:31604681ms step_avg:1613.22ms
step:19701/25000 train_time:31766338ms step_avg:1613.24ms
step:19801/25000 train_time:31928124ms step_avg:1613.26ms
step:19901/25000 train_time:32089502ms step_avg:1613.27ms
step:20000/25000 val_loss:2.2176 train_time:32249674ms step_avg:1613.29ms perplexity:9.1850 param_count:132,475,500
step:20001/25000 train_time:32251289ms step_avg:1613.29ms
step:20101/25000 train_time:32413051ms step_avg:1613.31ms
step:20201/25000 train_time:32574629ms step_avg:1613.32ms
step:20301/25000 train_time:32736295ms step_avg:1613.34ms
step:20401/25000 train_time:32898005ms step_avg:1613.36ms
step:20501/25000 train_time:33059592ms step_avg:1613.37ms
step:20601/25000 train_time:33221117ms step_avg:1613.38ms
step:20701/25000 train_time:33382802ms step_avg:1613.40ms
step:20801/25000 train_time:33545532ms step_avg:1613.46ms
step:20901/25000 train_time:33707281ms step_avg:1613.48ms
step:21000/25000 val_loss:2.2213 train_time:33867425ms step_avg:1613.50ms perplexity:9.2194 param_count:132,475,500
step:21001/25000 train_time:33869036ms step_avg:1613.50ms
step:21101/25000 train_time:34030615ms step_avg:1613.51ms
step:21201/25000 train_time:34192607ms step_avg:1613.54ms
step:21301/25000 train_time:34354336ms step_avg:1613.56ms
step:21401/25000 train_time:34516277ms step_avg:1613.59ms
step:21501/25000 train_time:34677989ms step_avg:1613.61ms
step:21601/25000 train_time:34839901ms step_avg:1613.63ms
step:21701/25000 train_time:35001708ms step_avg:1613.65ms
step:21801/25000 train_time:35163470ms step_avg:1613.67ms
step:21901/25000 train_time:35326414ms step_avg:1613.74ms
step:22000/25000 val_loss:2.2128 train_time:35486671ms step_avg:1613.76ms perplexity:9.1411 param_count:132,475,500
step:22001/25000 train_time:35488301ms step_avg:1613.76ms
step:22101/25000 train_time:35650330ms step_avg:1613.79ms
step:22201/25000 train_time:35812351ms step_avg:1613.82ms
step:22301/25000 train_time:35974477ms step_avg:1613.86ms
step:22401/25000 train_time:36136391ms step_avg:1613.88ms
step:22501/25000 train_time:36298142ms step_avg:1613.90ms
step:22601/25000 train_time:36460000ms step_avg:1613.92ms
step:22701/25000 train_time:36621990ms step_avg:1613.94ms
step:22801/25000 train_time:36783952ms step_avg:1613.97ms
step:22901/25000 train_time:36945792ms step_avg:1613.99ms
step:23000/25000 val_loss:2.2084 train_time:37106237ms step_avg:1614.02ms perplexity:9.1016 param_count:132,475,500
step:23001/25000 train_time:37107856ms step_avg:1614.02ms
step:23101/25000 train_time:37270967ms step_avg:1614.09ms
step:23201/25000 train_time:37432846ms step_avg:1614.11ms
step:23301/25000 train_time:37594711ms step_avg:1614.13ms
step:23401/25000 train_time:37756502ms step_avg:1614.15ms
step:23501/25000 train_time:37918206ms step_avg:1614.16ms
step:23601/25000 train_time:38080156ms step_avg:1614.18ms
step:23701/25000 train_time:38242180ms step_avg:1614.21ms
step:23801/25000 train_time:38403994ms step_avg:1614.22ms
step:23901/25000 train_time:38566018ms step_avg:1614.25ms
step:24000/25000 val_loss:2.2134 train_time:38726361ms step_avg:1614.27ms perplexity:9.1465 param_count:132,475,500
step:24001/25000 train_time:38727992ms step_avg:1614.27ms
step:24101/25000 train_time:38889841ms step_avg:1614.29ms
step:24201/25000 train_time:39052562ms step_avg:1614.34ms
step:24301/25000 train_time:39214368ms step_avg:1614.36ms
step:24401/25000 train_time:39376160ms step_avg:1614.37ms
step:24501/25000 train_time:39537852ms step_avg:1614.38ms
step:24601/25000 train_time:39699853ms step_avg:1614.41ms
step:24701/25000 train_time:39861647ms step_avg:1614.42ms
step:24801/25000 train_time:40023395ms step_avg:1614.43ms
step:24901/25000 train_time:40185257ms step_avg:1614.45ms
step:25000/25000 val_loss:2.1873 train_time:40345558ms step_avg:1614.47ms perplexity:8.9114 param_count:132,475,500
peak memory consumption training: 42 GiB
Final Results:
  Loss:        2.7136
  Perplexity:  15.0838
  Precision:   0.2432
  Recall:      0.1780
  F1:          0.1629
  Accuracy:    0.1780
  MCC:         0.1120
peak memory consumption testing: 42 GiB

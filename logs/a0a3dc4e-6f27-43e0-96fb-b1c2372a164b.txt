import os
import sys
with open(sys.argv[0]) as f:
    code = f.read() # read the code of this file ASAP, for logging

with open('optimizer.py', 'r', encoding='utf-8') as f:
    source_code = f.read()
    code += source_code

with open('model.py', 'r', encoding='utf-8') as f:
    source_code = f.read()
    code += source_code

with open('utils.py', 'r', encoding='utf-8') as f:
    source_code = f.read()
    code += source_code

with open('dataloading.py', 'r', encoding='utf-8') as f:
    source_code = f.read()
    code += source_code

import argparse
import uuid
import time
import contextlib
import math
import numpy as np
import torch
import torch.distributed as dist
import torch._inductor.config as config
from torch.nn.parallel import DistributedDataParallel as DDP
from pathlib import Path

from optimizer import Muon
from model import ModelConfig, ESM, CastedLinear
from dataloading import DistributedDataLoader


def get_args():
    parser = argparse.ArgumentParser(description='ESM2 training arguments')
    
    # Model hyperparams
    parser.add_argument('--vocab_size', type=int, default=33, help='vocabulary size')
    parser.add_argument('--num_hidden_layers', type=int, default=24, help='number of transformer layers')
    parser.add_argument('--num_attention_heads', type=int, default=6, help='number of attention heads (head dim 128 suggested by @Grad62304977)')
    parser.add_argument('--hidden_size', type=int, default=768, help='model hidden dimension size')
    
    # Data hyperparams
    parser.add_argument('--input_bin', type=str, default='data/omgprot50/omgprot50_train_*.bin', help='input .bins to train on')
    parser.add_argument('--input_valid_bin', type=str, default='data/omgprot50/omgprot50_valid_*.bin', help='input .bins to eval validation loss on')
    parser.add_argument('--input_test_bin', type=str, default='data/omgprot50/omgprot50_test_*.bin', help='input .bins to eval test loss on')   
    
    # Optimization hyperparams
    parser.add_argument('--batch_size', type=int, default=8*64*1024, help='batch size, in tokens, across all devices')
    parser.add_argument('--grad_accum', type=int, default=1, help='manually set number of gradient accumulation steps, else, will be ddp_world_size')
    parser.add_argument('--num_steps', type=int, default=25000, help='number of iterations to run')
    parser.add_argument('--warmup_steps', type=int, default=1000, help='number of warmup steps')
    parser.add_argument('--cooldown_steps', type=int, default=1000, help='number of cooldown steps')
    
    # Evaluation and logging hyperparams
    parser.add_argument('--valid_loss_every', type=int, default=1000, help='every how many steps to evaluate val loss? 0 for only at the end')
    parser.add_argument('--hf_model_name', type=str, default='Synthyra/esm_speedrun', help='huggingface model name')
    parser.add_argument('--token', type=str, default=None, help='huggingface token')
    parser.add_argument('--save_every', type=int, default=None, help='save every how many steps? None for no saving')
    args = parser.parse_args()
    return args


def get_param_count(model):
    total_params = 0
    for _, param in model.named_parameters():
        total_params += param.numel()
    return total_params


if __name__ == "__main__":
    args = get_args()
    if args.token:
        from huggingface_hub import login
        login(args.token)
        args.token = None
    model_config = ModelConfig(
        vocab_size=args.vocab_size,
        num_hidden_layers=args.num_hidden_layers,
        num_attention_heads=args.num_attention_heads,
        hidden_size=args.hidden_size,
    )

    # set up DDP (distributed data parallel) if available, otherwise single GPU
    if 'RANK' in os.environ:
        ddp_rank = int(os.environ['RANK'])
        ddp_local_rank = int(os.environ['LOCAL_RANK'])
        ddp_world_size = int(os.environ['WORLD_SIZE'])
        device = torch.device(f'cuda:{ddp_local_rank}')
        torch.cuda.set_device(device)
        dist.init_process_group(backend='nccl', device_id=device)
        dist.barrier()
        master_process = (ddp_rank == 0)
    else:
        ddp_rank = 0
        ddp_local_rank = 0 
        ddp_world_size = 1
        device = torch.device('cuda:0')
        torch.cuda.set_device(device)
        master_process = True

    print(f'using device: {device}')

    # begin logging
    logfile = None
    if master_process:
        run_id = uuid.uuid4()
        Path('logs').mkdir(exist_ok=True)
        # logdir = Path('logs') / f'{run_id}'
        # logdir.mkdir()
        logfile = Path('logs') / f'{run_id}.txt'
        print(logfile.stem)
        # create the log file
        with logfile.open('w') as f:
            # begin the log by printing this file (the Python code)
            print(code, file=f)
            print('=' * 100, file=f)

    def print0(s, logonly=False):
        if master_process:
            with logfile.open('a') as f:
                if not logonly:
                    print(s)
                print(s, file=f)

    # log information about the hardware/software environment this is running on
    # and print the full `nvidia-smi` to file
    print0(f'Running python {sys.version}')
    print0(f'Running pytorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}\nnvidia-smi:')
    import subprocess
    result = subprocess.run(['nvidia-smi'], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)
    print0(f'{result.stdout}', logonly=True)
    print0('='*100, logonly=True)

    print0(f'Model config: {model_config}')
    print0(f'Args: {args.__dict__}')

    # calculate the steps of gradient accumulation required to attain the desired global batch size
    # args.batch_size should refer to the total amount of tokens per backward pass
    train_accumulation_steps = 1
    batch_size = args.batch_size

    assert ddp_world_size == 1 or args.grad_accum == 1, "Cannot currently use both DDP and gradient accumulation"
    if ddp_world_size > 1:
        train_accumulation_steps = ddp_world_size
        batch_size = args.batch_size // ddp_world_size 
    elif args.grad_accum > 1:
        train_accumulation_steps *= args.grad_accum
        batch_size = args.batch_size // args.grad_accum

    print0(f'Train accumulation steps: {train_accumulation_steps}')
    print0(f'Adjusted local batch size: {batch_size} tokens')
    print0(f'Across {ddp_world_size} GPUs')
    print0(f'Total batch size: {args.batch_size} tokens')

    # load tokens
    train_loader = DistributedDataLoader(args.input_bin, batch_size, ddp_rank, ddp_world_size)
    valid_loader = DistributedDataLoader(args.input_valid_bin, batch_size, ddp_rank, ddp_world_size)
    test_loader = DistributedDataLoader(args.input_test_bin, batch_size, ddp_rank, ddp_world_size)
    print0(f"Training DataLoader: total number of tokens: {train_loader.total_num_tokens} across {len(train_loader.files)} files")
    print0(f"Validation DataLoader: total number of tokens: {valid_loader.total_num_tokens} across {len(valid_loader.files)} files")
    print0(f"Testing DataLoader: total number of tokens: {test_loader.total_num_tokens} across {len(test_loader.files)} files")
    print0('='*100, logonly=True)

    valid_steps = valid_loader.total_num_tokens // args.batch_size
    test_steps = test_loader.total_num_tokens // args.batch_size

    input_ids = train_loader.next_batch()

    model = ESM(model_config)
    model = model.cuda().bfloat16()
    for m in model.modules():
        if isinstance(m, CastedLinear):
            m.float()
    config.coordinate_descent_tuning = True # suggested by @Chillee
    model = torch.compile(model)

    # wrap model in DDP only if using distributed training
    if ddp_world_size > 1:
        model = DDP(model, device_ids=[ddp_local_rank], broadcast_buffers=False, gradient_as_bucket_view=True)
        raw_model = model.module
    else:
        raw_model = model

    # init the optimizers
    embed_params = [*raw_model.embed.parameters(), *raw_model.value_embeds.parameters()]
    params = list(raw_model.blocks.parameters())
    matrix_params = [p for p in params if p.ndim == 2]
    scalar_params = [p for p in params if p.ndim < 2] + [raw_model.skip_weights]
    optimizer1 = torch.optim.Adam(embed_params, lr=0.6, betas=(0.8, 0.95), fused=True)
    optimizer2 = torch.optim.Adam([raw_model.lm_head.weight], lr=0.008, betas=(0.8, 0.95), fused=True)
    optimizer3 = Muon(matrix_params, lr=0.05, momentum=0.95)
    optimizer4 = torch.optim.Adam(scalar_params, lr=0.04, betas=(0.8, 0.95), fused=True)
    optimizers = [optimizer1, optimizer2, optimizer3, optimizer4]

    # learning rate decay scheduler (linear warmup and cooldown)
    def get_lr(it):
        assert it <= args.num_steps
        # 1) linear warmup for warmup_steps steps
        if it < args.warmup_steps:
            return (it+1) / args.warmup_steps
        # 2) constant lr for a while
        elif it < args.num_steps - args.cooldown_steps:
            return 1.0
        # 3) linear cooldown
        else:
            decay_ratio = (args.num_steps - it) / args.cooldown_steps
            return decay_ratio

    schedulers = [torch.optim.lr_scheduler.LambdaLR(opt, get_lr) for opt in optimizers]

    sliding_window_size = torch.tensor(1024 - 128, dtype=torch.int32, device="cuda")
    sw_prev = 1024 - 128
    # Start training loop
    training_time_ms = 0
    # start the clock
    torch.cuda.synchronize()
    t0 = time.perf_counter()

    ### BEGIN TRAINING LOOP ###
    for step in range(args.num_steps + 1):
        last_step = (step == args.num_steps)
        # This effectively ignores timing first 10 steps, which are slower for weird reasons.
        # Alternately, and slightly more correctly in terms of benchmarking, we could do 10
        # steps with dummy data first, and then re-initialize the model and reset the loader.
        if step == 10:
            training_time_ms = 0
            t0 = time.perf_counter()
        timed_steps = float('nan') if step <= 11 else (step - 10) + 1 # <= 11 to avoid bug in val

        # Linearly increase the sliding window size over training in chunks of 128 from 1024 -> 2048. By @fernbear.bsky.social
        frac_done = step / args.num_steps # training progress
        sw_size = int(((1 - frac_done) * 1023 + frac_done * 2048) // 128) * 128
        if sw_size != sw_prev:
            sliding_window_size.copy_(sw_size, non_blocking=True)
            sw_prev = sw_size

        # once in a while evaluate the validation dataset
        if args.valid_loss_every > 0 and step % args.valid_loss_every == 0 or last_step:
            # stop the clock
            torch.cuda.synchronize()
            training_time_ms += 1000 * (time.perf_counter() - t0)
            # run validation batches
            model.eval()
            valid_loader.reset()
            val_loss = 0.0
            with torch.no_grad():
                for _ in range(valid_steps):
                    input_ids = valid_loader.next_batch()
                    val_loss += model(input_ids, sliding_window_size)
            if ddp_world_size > 1:
                dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
            val_loss /= valid_steps
            # log val loss to console and to logfile
            print0(f'step:{step}/{args.num_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/(timed_steps-1):.2f}ms perplexity:{(math.e**val_loss):.4f} param_count:{get_param_count(model):,}')
            # start the clock again
            torch.cuda.synchronize()
            t0 = time.perf_counter()

        # save checkpoint every `save_every` steps
        if master_process and args.save_every:
            if last_step or (step % args.save_every == 0):
                # stop the clock
                torch.cuda.synchronize()
                training_time_ms += 1000 * (time.perf_counter() - t0)
                # save the state of the training process
                log = dict(step=step, code=code, model=raw_model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
                torch.save(log, 'logs/state_step%06d.pt' % step)

                try:
                    if ddp_world_size > 1:
                        model.module.push_to_hub(args.hf_model_name, subfolder='step%06d' % step)
                    else:
                        model.push_to_hub(args.hf_model_name, subfolder='step%06d' % step)
                except Exception as e:
                    print(e)

                torch.cuda.synchronize()
                t0 = time.perf_counter()

        if last_step:
            break

        # --------------- FORWARD AND BACKWARD PASS -----------------
        model.train()
        for i in range(1, train_accumulation_steps + 1):
            with contextlib.ExitStack() as stack:
                if ddp_world_size > 1 and i < train_accumulation_steps: # there's no need to sync gradients every accumulation step
                    stack.enter_context(model.no_sync())
                #if step >= 5:
                #    stack.enter_context(torch.compiler.set_stance(skip_guard_eval_unsafe=True))
                model(input_ids, sliding_window_size).backward()
                input_ids = train_loader.next_batch()
        if train_accumulation_steps != 1:
            for p in model.parameters():
                p.grad /= train_accumulation_steps
        # momentum warmup for Muon
        frac = min(step/300, 1)
        for group in optimizer3.param_groups:
            group['momentum'] = (1 - frac) * 0.85 + frac * 0.95
        # step the optimizers and schedulers
        for opt, sched in zip(optimizers, schedulers):
            opt.step()
            sched.step()
        # null the gradients
        model.zero_grad(set_to_none=True)
        # --------------- FORWARD AND BACKWARD PASS END -------------------
        # everything that follows now is just eval, diagnostics, prints, logging, etc.
        if step % 100 == 0:
            approx_time = training_time_ms + 1000 * (time.perf_counter() - t0)
            print0(f"step:{step+1}/{args.num_steps} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms")

    print0(f"peak memory consumption training: {torch.cuda.max_memory_allocated() // 1024 // 1024 // 1024} GiB")

    # save the model to huggingface
    try:
        if ddp_world_size > 1:
            model.module.push_to_hub(args.hf_model_name)
        else:
            model.push_to_hub(args.hf_model_name)
    except Exception as e:
        print(e)

    torch.cuda.empty_cache()
    torch.cuda.synchronize()
    torch.manual_seed(42)
    model.eval()
    test_loader.reset()
    test_loss = 0.0
    with torch.no_grad():
        for _ in range(test_steps):
            input_ids = test_loader.next_batch()
            test_loss += model(input_ids, sliding_window_size)

    test_loss /= test_steps
    print0(f"Test results | Loss: {test_loss:.4f} | Perplexity: {math.e**test_loss:.4f}")
    print0(f"Total train time (min): {training_time_ms / 60000:.2f}")
    print0(f"Total train time (hours): {training_time_ms / 3600000:.2f}")

    print0(f"peak memory consumption testing: {torch.cuda.max_memory_allocated() // 1024 // 1024 // 1024} GiB")
    # -------------------------------------------------------------------------
    # clean up nice
    if ddp_world_size > 1:
        dist.destroy_process_group()
import os
import torch
import torch.distributed as dist


### Muon optimizer
@torch.compile
def zeropower_via_newtonschulz5(G, steps):
    """
    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a
    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose
    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at
    zero even beyond the point where the iteration no longer converges all the way to one everywhere
    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T
    where S' is diagonal with S_{ii}' ~ Uniform(0.5, 1.5), which turns out not to hurt model
    performance at all relative to UV^T, where USV^T = G is the SVD.
    """
    assert len(G.shape) == 2
    a, b, c = (3.4445, -4.7750,  2.0315)
    X = G.bfloat16()
    if G.size(0) > G.size(1):
        X = X.T

    # Ensure spectral norm is at most 1
    X = X / (X.norm() + 1e-7)
    # Perform the NS iterations
    for _ in range(steps):
        A = X @ X.T
        B = b * A + c * A @ A # adapted from suggestion by @jxbz, @leloykun, and @YouJiacheng
        X = a * X + B @ X

    if G.size(0) > G.size(1):
        X = X.T
    return X


class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Some warnings:
    - This optimizer assumes that all parameters passed in are 2D.
    - It should not be used for the embedding layer, the final fully connected layer, or any {0,1}-D
    parameters; those should all be optimized by a standard method (e.g., AdamW).
    - To use it with 4D convolutional filters, it works well to just flatten their last 3 dimensions.
    - We believe it is unlikely to work well for training with small batch size.
    - We believe it may not work well for finetuning pretrained models, but we haven't tested this.
    - We have not yet tried this optimizer for training scenarios larger than NanoGPT (124M).

    Arguments:
        lr: The learning rate used by the internal SGD.
        momentum: The momentum used by the internal SGD.
        nesterov: Whether to use Nesterov-style momentum in the internal SGD. (recommended)
        ns_steps: The number of Newton-Schulz iteration steps to use.
    """
    def __init__(self, params, lr=0.02, momentum=0.95, nesterov=True, ns_steps=5):
        self.world_size = int(os.environ.get('WORLD_SIZE', '1'))
        self.rank = int(os.environ.get('RANK', '0'))
        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, ns_steps=ns_steps)
        params = list(params)
        assert all(isinstance(p, torch.Tensor) for p in params)
        sizes = {p.numel() for p in params}
        param_groups = [
            {
                'params': [p for p in params if p.numel() == size],
                'update_buffer': [
                    torch.empty(size, device='cuda', dtype=torch.bfloat16)
                    for _ in range(self.world_size)
                ],
            }
            for size in sizes
        ]
        super().__init__(param_groups, defaults)

    def step(self):
        for group in self.param_groups:
            lr = group['lr']
            momentum = group['momentum']
            nesterov = group['nesterov']
            ns_steps = group['ns_steps']
            update_buffers = group['update_buffer']
            # generate weight updates in distributed fashion
            params = group['params']
            assert len(params) % self.world_size == 0
            handle = None
            params_world = None
            def update_prev():
                if params_world is None:
                    return
                if handle is not None:
                    handle.wait()
                for p_world, g_world in zip(params_world, update_buffers):
                    p_world.data.add_(
                        g_world.view_as(p_world),
                        alpha=-lr * max(1, p_world.size(0) / p_world.size(1)) ** 0.5,
                    )
            for base_i in range(len(params))[::self.world_size]:
                p = params[base_i + self.rank]
                g = p.grad
                assert g is not None
                state = self.state[p]
                if 'momentum_buffer' not in state:
                    state['momentum_buffer'] = torch.zeros_like(g)
                buf = state['momentum_buffer']
                buf.lerp_(g, 1 - momentum)
                g = g.lerp_(buf, momentum) if nesterov else buf
                g = zeropower_via_newtonschulz5(g, steps=ns_steps).flatten()
                update_prev()
                if self.world_size > 1:
                    handle = dist.all_gather(update_buffers, g, async_op=True)
                else:
                    update_buffers[0].copy_(g)
                    handle = None
                params_world = params[base_i : base_i + self.world_size]
            update_prev()
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.nn.attention.flex_attention import flex_attention, create_block_mask
from transformers import EsmTokenizer, PretrainedConfig, PreTrainedModel
from typing import Optional, Tuple, List, Any
try:
    from .utils import ProteinMasker
except ImportError:
    from utils import ProteinMasker


class ModelConfig(PretrainedConfig):
    """
    33 tokens: https://huggingface.co/Synthyra/ESMplusplus_large/blob/main/modeling_esm_plusplus.py#L868-L874
    ESM2-8M has 6 layers, 20 heads, 320 hidden dim: https://huggingface.co/facebook/esm2_t6_8M_UR50D/blob/main/config.json
    ESM2-35M has 12 layers, 20 heads, 480 hidden dim: https://huggingface.co/facebook/esm2_t12_35M_UR50D/blob/main/config.json
    ESM2-150M has 30 layers, 20 heads, 640 hidden dim: https://huggingface.co/facebook/esm2_t30_150M_UR50D/blob/main/config.json
    ESM2-650M has 33 layers, 20 heads, 1280 hidden dim: https://huggingface.co/facebook/esm2_t33_650M_UR50D/blob/main/config.json
    """
    def __init__(
        self,
        vocab_size=33,
        hidden_size=768,
        num_hidden_layers=12,
        num_attention_heads=12,
        expansion_ratio=8/3,
        **kwargs,
    ):
        super().__init__(**kwargs)
        self.vocab_size = vocab_size
        self.hidden_size = hidden_size
        self.num_hidden_layers = num_hidden_layers
        self.num_attention_heads = num_attention_heads
        self.expansion_ratio = expansion_ratio


def norm(x: torch.Tensor) -> torch.Tensor:
    return F.rms_norm(x, (x.size(-1),))


class CastedLinear(nn.Linear):
    def __init__(self, in_features, out_features):
        super().__init__(in_features, out_features, bias=False)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        return F.linear(x, self.weight.to(x.dtype))


class Rotary(nn.Module):
    def __init__(self, dim, base=10000):
        super().__init__()
        self.register_buffer('inv_freq', (1 / base) ** (torch.arange(0, dim, 2) / dim))
        self.seq_len_cached = None
        self.cos_cached = None
        self.sin_cached = None

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        seq_len = x.shape[1]
        if seq_len != self.seq_len_cached:
            t = torch.arange(seq_len, device=x.device)
            freqs = torch.outer(t, self.inv_freq)
            self.seq_len_cached = seq_len
            self.cos_cached = freqs.cos()
            self.sin_cached = freqs.sin()
        cos, sin = self.cos_cached[None, :, None, :], self.sin_cached[None, :, None, :]
        # apply_rotary_emb(x, cos, sin)
        x1, x2 = x.chunk(2, dim=3)
        y1 = x1 * cos + x2 * sin
        y2 = x1 * (-sin) + x2 * cos
        return torch.cat((y1, y2), 3).type_as(x)


class SelfAttention(nn.Module):
    """
    TODO
    Add F.spda option
    Add causal option (flex and sdpa)
    """
    def __init__(self, dim, num_attention_heads):
        super().__init__()
        assert dim % num_attention_heads == 0
        self.num_attention_heads = num_attention_heads
        self.qkv = CastedLinear(dim, 3 * dim)
        self.lambdas = nn.Parameter(torch.tensor([0.5, 0.5]))
        self.rotary = Rotary(dim // num_attention_heads) # dim // num_attention_heads = head_dim
        self.o_proj = CastedLinear(dim, dim)
        self.o_proj.weight.data.zero_() # zero init suggested by @Grad62304977

    def forward_sdpa(self, x: torch.Tensor, vi: torch.Tensor, attention_mask: Optional[torch.Tensor] = None) -> torch.Tensor:
        """
        TODO
        Question? Is this output actually different than flex attention output?
        Likely yes because of scoremod and / or soft capping
        Would be good to be able to do inference this way for typical PLM inference pipelines
        https://pytorch.org/blog/flexattention/
        """
        B, T = x.size(0), x.size(1) # batch size, sequence length
        qkv = self.qkv(x)
        q, k, v = qkv.chunk(3, dim=-1)
        q = q.view(B, T, self.num_attention_heads, -1)
        k = k.view(B, T, self.num_attention_heads, -1)
        v = v.view(B, T, self.num_attention_heads, -1)
        v = self.lambdas[0] * v + self.lambdas[1] * vi.view_as(v) # @KoszarskyB & @Grad62304977
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = self.rotary(q), self.rotary(k)
        y = F.scaled_dot_product_attention(
            q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2),
            attn_mask=attention_mask,
            dropout_p=0.0,
            is_causal=False,
            enable_gqa=True
        )
        y = y.transpose(1, 2).contiguous().view_as(x) # re-assemble all head outputs side by side
        y = self.o_proj(y)
        return y

    def forward(self, x: torch.Tensor, vi: torch.Tensor, block_mask: torch.Tensor) -> torch.Tensor:
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "Must use batch size = 1 for FlexAttention"
        qkv = self.qkv(x)
        q, k, v = qkv.chunk(3, dim=-1)
        q = q.view(B, T, self.num_attention_heads, -1)
        k = k.view(B, T, self.num_attention_heads, -1)
        v = v.view(B, T, self.num_attention_heads, -1)
        v = self.lambdas[0] * v + self.lambdas[1] * vi.view_as(v) # @KoszarskyB & @Grad62304977
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = self.rotary(q), self.rotary(k)
        y = flex_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), block_mask=block_mask, enable_gqa=True)
        y = y.transpose(1, 2).contiguous().view_as(x) # re-assemble all head outputs side by side
        y = self.o_proj(y)
        return y


def correction_fn(expansion_ratio: float, d_model: int) -> int:
    return int(((expansion_ratio * d_model) + 255) // 256 * 256)


class MLP(nn.Module):
    def __init__(self, dim, expansion_ratio):
        super().__init__()
        self.up   = CastedLinear(dim, correction_fn(expansion_ratio, dim))
        self.down = CastedLinear(correction_fn(expansion_ratio, dim), dim)
        self.down.weight.data.zero_() # zero init suggested by @Grad62304977
        self.relu = nn.ReLU()

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        # https://arxiv.org/abs/2109.08668v2
        # ReLU squared ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        return self.down(self.relu(self.up(x)).square())


class Block(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.attn = SelfAttention(config.hidden_size, config.num_attention_heads)
        self.mlp = MLP(config.hidden_size, config.expansion_ratio)
        self.lambdas = nn.Parameter(torch.tensor([1., 0.]))

    def sdpa_forward(self, x: torch.Tensor, vi: torch.Tensor, x0: torch.Tensor, attention_mask: Optional[torch.Tensor] = None) -> torch.Tensor:
        x = self.lambdas[0] * x + self.lambdas[1] * x0
        x = x + self.attn.forward_sdpa(norm(x), vi, attention_mask)
        x = x + self.mlp(norm(x))
        return x

    def forward(self, x: torch.Tensor, vi: torch.Tensor, x0: torch.Tensor, block_mask: torch.Tensor) -> torch.Tensor:
        x = self.lambdas[0] * x + self.lambdas[1] * x0
        x = x + self.attn(norm(x), vi, block_mask)
        x = x + self.mlp(norm(x))
        return x


class ValueEmbedding(nn.Module):
    def __init__(self, config: "ModelConfig"):
        super().__init__()
        self.embed = nn.ModuleList([
            nn.Embedding(config.vocab_size, config.hidden_size)
            for _ in range(config.num_hidden_layers // 2)
        ])

    def forward(self, inputs: torch.Tensor) -> List[torch.Tensor]:
        ve = [emb(inputs) for emb in self.embed]
        ve += reversed(ve)
        return ve


class ESM(PreTrainedModel):
    """
    TODO
    Add causal option (flex and sdpa)
    """
    config_class = ModelConfig
    def __init__(self, config: ModelConfig):
        super().__init__(config)
        self.config = config
        tokenizer = EsmTokenizer.from_pretrained('facebook/esm2_t6_8M_UR50D')
        self.masker = ProteinMasker(tokenizer, 0.20) # 20% masking rate https://arxiv.org/abs/2301.06568
        self.inference_masker = ProteinMasker(tokenizer, 0.15) # 15% masking rate for inference, ESM2
        self.cls_id = tokenizer.cls_token_id
        self.vocab_size = tokenizer.vocab_size
        self.num_hidden_layers = config.num_hidden_layers

        # U-net design by @brendanh0gan
        assert config.num_hidden_layers % 2 == 0, "Number of layers should be even for U-net design"
        self.num_encoder_layers = config.num_hidden_layers // 2 # Half of the layers for encoder
        self.num_decoder_layers = config.num_hidden_layers - self.num_encoder_layers # Remaining for decoder
        # Add learnable skip connection weights for decoder layers
        self.skip_weights = nn.Parameter(torch.ones(self.num_decoder_layers))

        self.embed = nn.Embedding(self.vocab_size, config.hidden_size)
        self.blocks = nn.ModuleList([Block(config) for _ in range(config.num_hidden_layers)])
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual learning
        # U-net structure on token value embeddings by @leloykun
        self.value_embeds = ValueEmbedding(config)
        self.lm_head = CastedLinear(config.hidden_size, self.vocab_size)
        self.lm_head.weight.data.zero_() # @Grad62304977
        self.cross_entropy = nn.CrossEntropyLoss()

    def embed_forward(self, input_ids: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, List[torch.Tensor]]:
        x = self.embed(input_ids[None])
        x = norm(x) # @Grad62304977
        x0 = x
        ve = self.value_embeds(input_ids)
        return x, x0, ve

    def get_logits(self, x: torch.Tensor) -> torch.Tensor:
        x = norm(x)
        logits = self.lm_head(x)
        logits = 30 * torch.tanh(logits / 30) # @Grad62304977
        logits = logits.float()
        return logits

    def sdpa_forward(self, input_ids: torch.Tensor, attention_mask: Optional[torch.Tensor] = None) -> torch.Tensor:
        if attention_mask is not None:
            attention_mask = attention_mask[:, None, None, :].bool()
        
        x, x0, ve = self.embed_forward(input_ids)
        ve_enc, ve_dec = ve[:self.num_encoder_layers], ve[self.num_encoder_layers:]

        skip_connections = []
        for i in range(self.num_encoder_layers):
            x = self.blocks[i].sdpa_forward(x, ve_enc[i], x0, attention_mask)
            skip_connections.append(x)

        for i in range(self.num_decoder_layers):
            x = x + self.skip_weights[i] * skip_connections.pop()
            x = self.blocks[self.num_encoder_layers + i].sdpa_forward(x, ve_dec[i], x0, attention_mask)

        return self.get_logits(x)

    def flex_forward(self, input_ids: torch.Tensor, sliding_window_size: torch.Tensor) -> torch.Tensor:
        input_ids = input_ids.flatten() # flex_attention needs batch 1
        docs = (input_ids == self.cls_id).cumsum(0)

        def doc_mask_mod(b, h, q_idx, kv_idx):
            bidirectional_sliding_window_mask = torch.abs(q_idx - kv_idx) < sliding_window_size
            doc_mask = docs[q_idx] == docs[kv_idx]
            return bidirectional_sliding_window_mask & doc_mask

        S = len(input_ids)
        block_mask = create_block_mask(doc_mask_mod, None, None, S, S)

        x, x0, ve = self.embed_forward(input_ids)
        ve_enc, ve_dec = ve[:self.num_encoder_layers], ve[self.num_encoder_layers:]

        # Store outputs for U-Net skip connections
        skip_connections = []
        # Encoder pass - process only the first half of the blocks
        for i in range(self.num_encoder_layers):
            x = self.blocks[i](x, ve_enc[i], x0, block_mask)
            skip_connections.append(x)
        # Decoder pass - process the remaining blocks with weighted skip connections
        for i in range(self.num_decoder_layers):
            x = x + self.skip_weights[i] * skip_connections.pop()
            # U-net structure on token value embeddings by @leloykun
            x = self.blocks[self.num_encoder_layers + i](x, ve_dec[i], x0, block_mask)

        return self.get_logits(x)

    def get_vector_embeddings(self, input_ids: torch.Tensor, sliding_window_size: torch.Tensor) -> torch.Tensor:
        input_ids = input_ids.flatten()
        docs = (input_ids == self.cls_id).cumsum(dim=0)  # shape: [S]
        
        def doc_mask_mod(b, h, q_idx, kv_idx):
            bidirectional_sliding_window_mask = torch.abs(q_idx - kv_idx) < sliding_window_size
            doc_mask = docs[q_idx] == docs[kv_idx]
            return bidirectional_sliding_window_mask & doc_mask
        
        S = len(input_ids)
        block_mask = create_block_mask(doc_mask_mod, None, None, S, S)
        x, x0, ve = self.embed_forward(input_ids)  # x shape: [S, hidden_size]
        ve_enc, ve_dec = ve[:self.num_encoder_layers], ve[self.num_encoder_layers:]
        skip_connections = []
        for i in range(self.num_encoder_layers):
            x = self.blocks[i](x, ve_enc[i], x0, block_mask)
            skip_connections.append(x)

        for i in range(self.num_decoder_layers):
            x = x + self.skip_weights[i] * skip_connections.pop()
            x = self.blocks[self.num_encoder_layers + i](x, ve_dec[i], x0, block_mask)
        
        x = x.view(-1, self.config.hidden_size)
        # At this point, x is shape [S, hidden_size]
        # We want to mean-pool across each document index.
        # Convert docs to 0-based so we can do nice indexing
        num_docs = docs.max().item()
        doc_ids = docs - 1  # Now documents are labeled [0, 1, 2, ...]
        # Mean-pool across tokens belonging to each doc
        doc_embeds = []
        for doc_idx in range(num_docs):
            mask = (doc_ids == doc_idx)
            # Collect all token embeddings for this doc and average
            doc_embeds.append(x[mask].mean(dim=0))
        # Stack into [num_documents, hidden_size]
        return torch.stack(doc_embeds, dim=0)

    def inference(self, input_ids: torch.Tensor, sliding_window_size: torch.Tensor = None) -> Tuple[torch.Tensor, Any, Any]:
        input_ids, labels = self.inference_masker(input_ids)
        logits = self.flex_forward(input_ids, sliding_window_size)
        loss = None
        if labels is not None:
            loss = self.cross_entropy(logits.view(-1, self.vocab_size), labels.view(-1).long())
        return logits, loss, labels

    def forward(self, input_ids: torch.Tensor, sliding_window_size: torch.Tensor) -> torch.Tensor:
        input_ids, labels = self.masker(input_ids)
        logits = self.flex_forward(input_ids, sliding_window_size)
        return self.cross_entropy(logits.view(-1, self.vocab_size), labels.view(-1).long())


if __name__ == '__main__':
    """
    TODO
    look at MSE between flex attention outputs and sdpa outputs
    """import torch
from typing import Tuple

"""
Standardized MLM masking approach for consistency
"""

class ProteinMasker:
    def __init__(self, tokenizer, mlm_probability=0.15):
        """
        Initialize the ProteinMasker with the given tokenizer and masking parameters.
        Of the masked tokens, 80% are replaced with [MASK], 10% are replaced with a random amino acid token, and 10% are unchanged.
        """
        self.tokenizer = tokenizer
        self.mlm_probability = mlm_probability
        self.mask_token_id = tokenizer.mask_token_id
        self.special_tokens = torch.tensor(tokenizer.all_special_ids)
        canonical_amino_acids = 'ACDEFGHIKLMNPQRSTVWY'
        canonical_amino_acids_ids = tokenizer.convert_tokens_to_ids(list(canonical_amino_acids))
        self.low_range = min(canonical_amino_acids_ids)
        self.high_range = max(canonical_amino_acids_ids)

    def __call__(self, input_ids: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:
        labels = input_ids.clone()
        
        # Create special tokens mask using broadcasting
        special_tokens = self.special_tokens.to(input_ids.device)
        special_tokens_mask = (input_ids[..., None] == special_tokens).any(-1)
        
        # Create probability matrix and mask special tokens
        probability_matrix = torch.full_like(labels, self.mlm_probability, dtype=torch.float)
        probability_matrix.masked_fill_(special_tokens_mask, value=0.0)
        
        # Create masked indices
        masked_indices = torch.bernoulli(probability_matrix).bool()
        labels[~masked_indices] = -100  # We only compute loss on masked tokens
        
        # 80% of the time, we replace masked input tokens with tokenizer.mask_token ([MASK])
        indices_replaced = torch.bernoulli(torch.full_like(probability_matrix, 0.8)).bool() & masked_indices
        input_ids[indices_replaced] = self.mask_token_id
        
        # 10% of the time, we replace masked input tokens with random word
        indices_random = torch.bernoulli(torch.full_like(probability_matrix, 0.5)).bool() & masked_indices & ~indices_replaced
        random_words = torch.randint(low=self.low_range, high=self.high_range, size=labels.shape, dtype=input_ids.dtype, device=labels.device)
        input_ids[indices_random] = random_words[indices_random]
        
        # The rest of the time (10% of the time) we keep the masked input tokens unchanged
        return input_ids, labels


if __name__ == "__main__":
    from transformers import EsmTokenizer
    tokenizer = EsmTokenizer.from_pretrained("facebook/esm2_t6_8M_UR50D")
    test_seqs = [
        'MNFKYKLYSYITIFQIILILPTIVASNERCIALGGVCKDFSDCTGNYKPIDKHCDGSNNIKCCIRKIECPTSQNSNFTISGKNKEDEALPFIFKSEGGCQNDKNDNGNKINGKIGYTCAGITPMVGWKNKENYFSYAIKECTNDTNFTYCAYKLNENKFREGAKNIYIDKYAVAGKCNNLPQPAYYVCFDTSVNHGSGWSSKTITANPIGNMDGREYGLLLNKKSREKYINIVKNDSSQEKYLNGWLSRADDREKYCNNYCTSNCNCDNSASKASVSSNTNTTDIYNSVNTVDSDICNCDDNEPTDFLDDDYINNEEEIDEEIIDQEEY',
        'MYRTALYFTVCSIWLCQIITGVLSLKCKCDLCKDKNYTCITDGYCYTSATLKDGVILYNYRCLDLNFPMRNPMFCHKQIPIHHEFTLECCNDRDFCNIRLVPKLTPKDNATSDTSLGTIEIAVVIILPTLVICIIAMAIYLYYQNKRSTHHHLGLGDDSIEAPDHPILNGVSLKHMIEMTTSGSGSGLPLLVQRSIARQIQLVEIIGQGRYGEVWRGRWRGENVAVKIFSSREERSWFREAEIYQTVMLRHDNILGFIAADNKGVLSLKCKCDLCKDKNYTCITDGYCYTSATLKDGVILYNYRQLGASLNRFXVYALGLIFWEISRRCNVGGIYDEYQLPFYDAVPSDPTIEEMRRVVCVERQRPSIPNRWQSCEALHVMSKLMKECWYHNATARLTALRIKKTLANFRASEELKM'
    ]
    test_ids = tokenizer(test_seqs, return_tensors="pt", padding=True).input_ids
    masker = ProteinMasker(tokenizer, mlm_probability=0.5)
    print(masker.mask_token_id)
    print(masker.special_tokens)
    print(masker.low_range, masker.high_range)

    # First set of masking
    masked_ids1, labels1 = masker(test_ids.clone())
    masked_ids2, labels2 = masker(test_ids.clone())

    print("Before setting seed:")
    print("Original: ", test_ids[0][:20].tolist())
    print("Masking 1:", masked_ids1[0][:20].tolist()) 
    print("Masking 2:", masked_ids2[0][:20].tolist())
    print("Are they equal?", torch.equal(masked_ids1, masked_ids2))

    # Now with seed
    torch.manual_seed(42)
    masked_ids3, labels3 = masker(test_ids.clone())
    torch.manual_seed(42)
    masked_ids4, labels4 = masker(test_ids.clone())

    print("\nAfter setting seed:")
    print("Original: ", test_ids[0][:20].tolist())
    print("Masking 3:", masked_ids3[0][:20].tolist())
    print("Masking 4:", masked_ids4[0][:20].tolist()) 
    print("Are they equal?", torch.equal(masked_ids3, masked_ids4))
import torch
from pathlib import Path


def _peek_data_shard(file: Path):
    # only reads the header, returns header data
    # header is 256 int32
    header = torch.from_file(f"{file}", False, 256, dtype=torch.int32)
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    return int(header[2]) # number of tokens (claimed)


def _load_data_shard(path: Path, num_tokens):
    with path.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint8, pin_memory=True)
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy())
        assert nbytes == num_tokens, "number of tokens read does not match header?"
    return tokens


class DistributedDataLoader:
    def __init__(self, filename_pattern, batch_size, process_rank, num_processes):
        self.process_rank = process_rank
        self.num_processes = num_processes
        self.batch_size = batch_size

        # glob files that match the pattern
        self.files = sorted(Path.cwd().glob(filename_pattern))
        assert len(self.files) > 0, f"did not find any files that match the pattern {filename_pattern}"

        # load and validate all data shards, count number of tokens in total
        self.files_num_tokens = [_peek_data_shard(file) for file in self.files]
        self.total_num_tokens = sum(self.files_num_tokens)

        self.reset()

    def reset(self):
        self.current_shard = -1
        self.advance()

    def advance(self): # advance to next data shard
        self.current_shard = (self.current_shard + 1) % len(self.files)
        self.current_position = self.process_rank * self.batch_size
        self.tokens = _load_data_shard(self.files[self.current_shard], self.files_num_tokens[self.current_shard])

    def next_batch(self):
        batch_size = self.batch_size * self.num_processes
        buf = self.tokens[self.current_position:self.current_position+self.batch_size]
        # host side async is sufficient;
        # no performance improvement was observed when introducing a separate stream.
        input_ids = buf.to(device="cuda", dtype=torch.int32, non_blocking=True) # inputs
        # advance current position and load next shard if necessary
        self.current_position += batch_size
        if self.current_position + batch_size >= len(self.tokens):
            self.advance()
        return input_ids

====================================================================================================
Running python 3.10.12 (main, Nov  6 2024, 20:22:13) [GCC 11.4.0]
Running pytorch 2.6.0.dev20241203+cu124 compiled for CUDA 12.4
nvidia-smi:
Sun Dec 29 23:25:58 2024       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.127.05             Driver Version: 550.127.05     CUDA Version: 12.4     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA GH200 480GB             On  |   00000000:DD:00.0 Off |                    0 |
| N/A   38C    P0             83W /  700W |       9MiB /  97871MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+

====================================================================================================
Model config: ModelConfig {
  "expansion_ratio": 2.6666666666666665,
  "hidden_size": 768,
  "num_attention_heads": 6,
  "num_hidden_layers": 24,
  "transformers_version": "4.47.1",
  "vocab_size": 33
}

Args: {'vocab_size': 33, 'num_hidden_layers': 24, 'num_attention_heads': 6, 'hidden_size': 768, 'input_bin': 'data/omgprot50/omgprot50_train_*.bin', 'input_valid_bin': 'data/omgprot50/omgprot50_valid_*.bin', 'input_test_bin': 'data/omgprot50/omgprot50_test_*.bin', 'batch_size': 524288, 'grad_accum': 8, 'num_steps': 100000, 'warmup_steps': 1000, 'cooldown_steps': 75000, 'valid_loss_every': 1000, 'hf_model_name': 'Synthyra/esm_speedrun_100000', 'token': None, 'save_every': 1000}
Train accumulation steps: 8
Adjusted local batch size: 65536 tokens
Across 1 GPUs
Total batch size: 524288 tokens
Training DataLoader: total number of tokens: 43956331596 across 440 files
Validation DataLoader: total number of tokens: 2097660 across 1 files
Testing DataLoader: total number of tokens: 3686279 across 1 files
====================================================================================================
step:0/100000 val_loss:3.4965 train_time:0ms step_avg:nanms perplexity:33.0000 param_count:132,475,500
step:1/100000 train_time:35192ms step_avg:nanms
step:101/100000 train_time:147108ms step_avg:1616.57ms
step:201/100000 train_time:310490ms step_avg:1625.60ms
step:301/100000 train_time:473876ms step_avg:1628.44ms
step:401/100000 train_time:637004ms step_avg:1629.17ms
step:501/100000 train_time:799867ms step_avg:1629.06ms
step:601/100000 train_time:962948ms step_avg:1629.35ms
step:701/100000 train_time:1126347ms step_avg:1630.03ms
step:801/100000 train_time:1289141ms step_avg:1629.76ms
step:901/100000 train_time:1451950ms step_avg:1629.57ms
step:1000/100000 val_loss:2.4980 train_time:1613141ms step_avg:1629.44ms perplexity:12.1585 param_count:132,475,500
step:1001/100000 train_time:1614756ms step_avg:1629.42ms
step:1101/100000 train_time:1776673ms step_avg:1628.48ms
step:1201/100000 train_time:1938385ms step_avg:1627.53ms
step:1301/100000 train_time:2100181ms step_avg:1626.79ms
step:1401/100000 train_time:2262024ms step_avg:1626.19ms
step:1501/100000 train_time:2424131ms step_avg:1625.84ms
step:1601/100000 train_time:2585815ms step_avg:1625.28ms
step:1701/100000 train_time:2747416ms step_avg:1624.73ms
step:1801/100000 train_time:2909062ms step_avg:1624.27ms
step:1901/100000 train_time:3070777ms step_avg:1623.89ms
step:2000/100000 val_loss:2.4467 train_time:3230778ms step_avg:1623.51ms perplexity:11.5501 param_count:132,475,500
step:2001/100000 train_time:3232384ms step_avg:1623.50ms
step:2101/100000 train_time:3393865ms step_avg:1623.08ms
step:2201/100000 train_time:3555658ms step_avg:1622.85ms
step:2301/100000 train_time:3717860ms step_avg:1622.81ms
step:2401/100000 train_time:3879538ms step_avg:1622.56ms
step:2501/100000 train_time:4041266ms step_avg:1622.35ms
step:2601/100000 train_time:4203045ms step_avg:1622.17ms
step:2701/100000 train_time:4364835ms step_avg:1622.01ms
step:2801/100000 train_time:4526584ms step_avg:1621.85ms
step:2901/100000 train_time:4688313ms step_avg:1621.69ms
step:3000/100000 val_loss:2.4030 train_time:4848338ms step_avg:1621.52ms perplexity:11.0562 param_count:132,475,500
step:3001/100000 train_time:4850466ms step_avg:1621.69ms
step:3101/100000 train_time:5011911ms step_avg:1621.45ms
step:3201/100000 train_time:5173658ms step_avg:1621.33ms
step:3301/100000 train_time:5335346ms step_avg:1621.19ms
step:3401/100000 train_time:5496896ms step_avg:1621.03ms
step:3501/100000 train_time:5658529ms step_avg:1620.89ms
step:3601/100000 train_time:5820208ms step_avg:1620.78ms
step:3701/100000 train_time:5981881ms step_avg:1620.67ms
step:3801/100000 train_time:6143980ms step_avg:1620.68ms
step:3901/100000 train_time:6305613ms step_avg:1620.56ms
step:4000/100000 val_loss:2.3713 train_time:6465808ms step_avg:1620.50ms perplexity:10.7113 param_count:132,475,500
step:4001/100000 train_time:6467424ms step_avg:1620.50ms
step:4101/100000 train_time:6628753ms step_avg:1620.33ms
step:4201/100000 train_time:6790200ms step_avg:1620.19ms
step:4301/100000 train_time:6951699ms step_avg:1620.07ms
step:4401/100000 train_time:7113197ms step_avg:1619.95ms
step:4501/100000 train_time:7274611ms step_avg:1619.82ms
step:4601/100000 train_time:7436669ms step_avg:1619.84ms
step:4701/100000 train_time:7598270ms step_avg:1619.75ms
step:4801/100000 train_time:7759778ms step_avg:1619.66ms
step:4901/100000 train_time:7921408ms step_avg:1619.59ms
step:5000/100000 val_loss:2.3548 train_time:8081242ms step_avg:1619.49ms perplexity:10.5357 param_count:132,475,500
step:5001/100000 train_time:8082850ms step_avg:1619.49ms
step:5101/100000 train_time:8244005ms step_avg:1619.33ms
step:5201/100000 train_time:8405313ms step_avg:1619.21ms
step:5301/100000 train_time:8566819ms step_avg:1619.13ms
step:5401/100000 train_time:8728743ms step_avg:1619.13ms
step:5501/100000 train_time:8890291ms step_avg:1619.07ms
step:5601/100000 train_time:9051940ms step_avg:1619.02ms
step:5701/100000 train_time:9213407ms step_avg:1618.94ms
step:5801/100000 train_time:9374849ms step_avg:1618.87ms
step:5901/100000 train_time:9536335ms step_avg:1618.80ms
step:6000/100000 val_loss:2.3378 train_time:9696367ms step_avg:1618.76ms perplexity:10.3585 param_count:132,475,500
step:6001/100000 train_time:9697982ms step_avg:1618.76ms
step:6101/100000 train_time:9859776ms step_avg:1618.75ms
step:6201/100000 train_time:10021221ms step_avg:1618.68ms
step:6301/100000 train_time:10182644ms step_avg:1618.61ms
step:6401/100000 train_time:10344056ms step_avg:1618.53ms
step:6501/100000 train_time:10505407ms step_avg:1618.46ms
step:6601/100000 train_time:10666940ms step_avg:1618.41ms
step:6701/100000 train_time:10828425ms step_avg:1618.36ms
step:6801/100000 train_time:10989782ms step_avg:1618.29ms
step:6901/100000 train_time:11151802ms step_avg:1618.31ms
step:7000/100000 val_loss:2.3209 train_time:11311569ms step_avg:1618.25ms perplexity:10.1850 param_count:132,475,500
step:7001/100000 train_time:11313172ms step_avg:1618.25ms
step:7101/100000 train_time:11474259ms step_avg:1618.14ms
step:7201/100000 train_time:11635628ms step_avg:1618.08ms
step:7301/100000 train_time:11797186ms step_avg:1618.05ms
step:7401/100000 train_time:11958840ms step_avg:1618.03ms
step:7501/100000 train_time:12120296ms step_avg:1617.98ms
step:7601/100000 train_time:12281716ms step_avg:1617.93ms
step:7701/100000 train_time:12443479ms step_avg:1617.93ms
step:7801/100000 train_time:12604872ms step_avg:1617.88ms
step:7901/100000 train_time:12766381ms step_avg:1617.84ms
step:8000/100000 val_loss:2.3087 train_time:12926187ms step_avg:1617.80ms perplexity:10.0610 param_count:132,475,500
step:8001/100000 train_time:12927797ms step_avg:1617.79ms
step:8101/100000 train_time:13089036ms step_avg:1617.73ms
step:8201/100000 train_time:13250313ms step_avg:1617.67ms
step:8301/100000 train_time:13411713ms step_avg:1617.62ms
step:8401/100000 train_time:13573193ms step_avg:1617.59ms
step:8501/100000 train_time:13735066ms step_avg:1617.60ms
step:8601/100000 train_time:13896399ms step_avg:1617.55ms
step:8701/100000 train_time:14057807ms step_avg:1617.51ms
step:8801/100000 train_time:14219272ms step_avg:1617.48ms
step:8901/100000 train_time:14380615ms step_avg:1617.44ms
step:9000/100000 val_loss:2.3021 train_time:14540235ms step_avg:1617.38ms perplexity:9.9947 param_count:132,475,500
step:9001/100000 train_time:14541844ms step_avg:1617.38ms
step:9101/100000 train_time:14703049ms step_avg:1617.32ms
step:9201/100000 train_time:14864853ms step_avg:1617.33ms
step:9301/100000 train_time:15026184ms step_avg:1617.28ms
step:9401/100000 train_time:15187575ms step_avg:1617.25ms
step:9501/100000 train_time:15348914ms step_avg:1617.21ms
step:9601/100000 train_time:15510294ms step_avg:1617.17ms
step:9701/100000 train_time:15671700ms step_avg:1617.14ms
step:9801/100000 train_time:15832957ms step_avg:1617.09ms
step:9901/100000 train_time:15994370ms step_avg:1617.06ms
step:10000/100000 val_loss:2.2837 train_time:16154760ms step_avg:1617.09ms perplexity:9.8128 param_count:132,475,500
step:10001/100000 train_time:16156373ms step_avg:1617.09ms
step:10101/100000 train_time:16317601ms step_avg:1617.04ms
step:10201/100000 train_time:16478889ms step_avg:1617.00ms
step:10301/100000 train_time:16640312ms step_avg:1616.98ms
step:10401/100000 train_time:16801681ms step_avg:1616.95ms
step:10501/100000 train_time:16963010ms step_avg:1616.91ms
step:10601/100000 train_time:17124233ms step_avg:1616.87ms
step:10701/100000 train_time:17285507ms step_avg:1616.83ms
step:10801/100000 train_time:17447477ms step_avg:1616.85ms
step:10901/100000 train_time:17608795ms step_avg:1616.82ms
step:11000/100000 val_loss:2.2666 train_time:17768628ms step_avg:1616.80ms perplexity:9.6463 param_count:132,475,500
step:11001/100000 train_time:17770240ms step_avg:1616.80ms
step:11101/100000 train_time:17931233ms step_avg:1616.74ms
step:11201/100000 train_time:18092656ms step_avg:1616.71ms
step:11301/100000 train_time:18254126ms step_avg:1616.70ms
step:11401/100000 train_time:18415433ms step_avg:1616.67ms
step:11501/100000 train_time:18576719ms step_avg:1616.63ms
step:11601/100000 train_time:18738647ms step_avg:1616.65ms
step:11701/100000 train_time:18900034ms step_avg:1616.63ms
step:11801/100000 train_time:19061281ms step_avg:1616.60ms
step:11901/100000 train_time:19222379ms step_avg:1616.55ms
step:12000/100000 val_loss:2.2698 train_time:19382063ms step_avg:1616.52ms perplexity:9.6779 param_count:132,475,500
step:12001/100000 train_time:19383675ms step_avg:1616.52ms
step:12101/100000 train_time:19544704ms step_avg:1616.47ms
step:12201/100000 train_time:19705885ms step_avg:1616.43ms
step:12301/100000 train_time:19867456ms step_avg:1616.42ms
step:12401/100000 train_time:20028562ms step_avg:1616.38ms
step:12501/100000 train_time:20189808ms step_avg:1616.35ms
step:12601/100000 train_time:20351039ms step_avg:1616.32ms
step:12701/100000 train_time:20512544ms step_avg:1616.31ms
step:12801/100000 train_time:20674088ms step_avg:1616.30ms
step:12901/100000 train_time:20835679ms step_avg:1616.30ms
step:13000/100000 val_loss:2.2689 train_time:20995831ms step_avg:1616.31ms perplexity:9.6690 param_count:132,475,500
step:13001/100000 train_time:20997447ms step_avg:1616.31ms
step:13101/100000 train_time:21159356ms step_avg:1616.33ms
step:13201/100000 train_time:21321086ms step_avg:1616.34ms
step:13301/100000 train_time:21482724ms step_avg:1616.34ms
step:13401/100000 train_time:21644561ms step_avg:1616.35ms
step:13501/100000 train_time:21806150ms step_avg:1616.35ms
step:13601/100000 train_time:21967812ms step_avg:1616.35ms
step:13701/100000 train_time:22129535ms step_avg:1616.36ms
step:13801/100000 train_time:22291333ms step_avg:1616.37ms
step:13901/100000 train_time:22453612ms step_avg:1616.41ms
step:14000/100000 val_loss:2.2550 train_time:22613662ms step_avg:1616.42ms perplexity:9.5349 param_count:132,475,500
step:14001/100000 train_time:22615268ms step_avg:1616.42ms
step:14101/100000 train_time:22776743ms step_avg:1616.40ms
step:14201/100000 train_time:22938490ms step_avg:1616.41ms
step:14301/100000 train_time:23100142ms step_avg:1616.41ms
step:14401/100000 train_time:23261839ms step_avg:1616.42ms
step:14501/100000 train_time:23423509ms step_avg:1616.42ms
step:14601/100000 train_time:23585245ms step_avg:1616.42ms
step:14701/100000 train_time:23747384ms step_avg:1616.46ms
step:14801/100000 train_time:23909099ms step_avg:1616.46ms
step:14901/100000 train_time:24070788ms step_avg:1616.47ms
step:15000/100000 val_loss:2.2583 train_time:24230899ms step_avg:1616.47ms perplexity:9.5665 param_count:132,475,500
step:15001/100000 train_time:24232502ms step_avg:1616.47ms
step:15101/100000 train_time:24394003ms step_avg:1616.46ms
step:15201/100000 train_time:24555665ms step_avg:1616.46ms
step:15301/100000 train_time:24717301ms step_avg:1616.46ms
step:15401/100000 train_time:24879678ms step_avg:1616.51ms
step:15501/100000 train_time:25041336ms step_avg:1616.51ms
step:15601/100000 train_time:25203023ms step_avg:1616.51ms
step:15701/100000 train_time:25364684ms step_avg:1616.51ms
step:15801/100000 train_time:25526401ms step_avg:1616.52ms
step:15901/100000 train_time:25688161ms step_avg:1616.52ms
step:16000/100000 val_loss:2.2495 train_time:25848298ms step_avg:1616.53ms perplexity:9.4829 param_count:132,475,500
step:16001/100000 train_time:25849913ms step_avg:1616.53ms
step:16101/100000 train_time:26011537ms step_avg:1616.53ms
step:16201/100000 train_time:26173821ms step_avg:1616.57ms
step:16301/100000 train_time:26335791ms step_avg:1616.59ms
step:16401/100000 train_time:26497501ms step_avg:1616.59ms
step:16501/100000 train_time:26659135ms step_avg:1616.59ms
step:16601/100000 train_time:26821005ms step_avg:1616.60ms
step:16701/100000 train_time:26982649ms step_avg:1616.60ms
step:16801/100000 train_time:27144364ms step_avg:1616.60ms
step:16901/100000 train_time:27306089ms step_avg:1616.61ms
step:17000/100000 val_loss:2.2392 train_time:27466590ms step_avg:1616.63ms perplexity:9.3861 param_count:132,475,500
step:17001/100000 train_time:27468204ms step_avg:1616.63ms
step:17101/100000 train_time:27629599ms step_avg:1616.62ms
step:17201/100000 train_time:27791447ms step_avg:1616.63ms
step:17301/100000 train_time:27953310ms step_avg:1616.64ms
step:17401/100000 train_time:28115148ms step_avg:1616.65ms
step:17501/100000 train_time:28277105ms step_avg:1616.67ms
step:17601/100000 train_time:28438913ms step_avg:1616.67ms
step:17701/100000 train_time:28600605ms step_avg:1616.68ms
step:17801/100000 train_time:28762871ms step_avg:1616.71ms
step:17901/100000 train_time:28924746ms step_avg:1616.72ms
step:18000/100000 val_loss:2.2357 train_time:29085063ms step_avg:1616.73ms perplexity:9.3533 param_count:132,475,500
step:18001/100000 train_time:29086684ms step_avg:1616.74ms
step:18101/100000 train_time:29248048ms step_avg:1616.72ms
step:18201/100000 train_time:29409824ms step_avg:1616.72ms
step:18301/100000 train_time:29571565ms step_avg:1616.73ms
step:18401/100000 train_time:29733321ms step_avg:1616.73ms
step:18501/100000 train_time:29895601ms step_avg:1616.76ms
step:18601/100000 train_time:30057328ms step_avg:1616.77ms
step:18701/100000 train_time:30219148ms step_avg:1616.78ms
step:18801/100000 train_time:30381085ms step_avg:1616.79ms
step:18901/100000 train_time:30543003ms step_avg:1616.80ms
step:19000/100000 val_loss:2.2397 train_time:30703241ms step_avg:1616.81ms perplexity:9.3909 param_count:132,475,500
step:19001/100000 train_time:30704851ms step_avg:1616.81ms
step:19101/100000 train_time:30866424ms step_avg:1616.81ms
step:19201/100000 train_time:31028332ms step_avg:1616.82ms
step:19301/100000 train_time:31190823ms step_avg:1616.86ms
step:19401/100000 train_time:31352775ms step_avg:1616.87ms
step:19501/100000 train_time:31514811ms step_avg:1616.89ms
step:19601/100000 train_time:31676637ms step_avg:1616.90ms
step:19701/100000 train_time:31838467ms step_avg:1616.90ms
step:19801/100000 train_time:32000332ms step_avg:1616.91ms
step:19901/100000 train_time:32162246ms step_avg:1616.92ms
step:20000/100000 val_loss:2.2281 train_time:32322512ms step_avg:1616.93ms perplexity:9.2825 param_count:132,475,500
step:20001/100000 train_time:32324121ms step_avg:1616.93ms
step:20101/100000 train_time:32486055ms step_avg:1616.95ms
step:20201/100000 train_time:32647819ms step_avg:1616.95ms
step:20301/100000 train_time:32809630ms step_avg:1616.95ms
step:20401/100000 train_time:32971550ms step_avg:1616.97ms
step:20501/100000 train_time:33133369ms step_avg:1616.97ms
step:20601/100000 train_time:33295114ms step_avg:1616.97ms
step:20701/100000 train_time:33456995ms step_avg:1616.98ms
step:20801/100000 train_time:33618796ms step_avg:1616.99ms
step:20901/100000 train_time:33781259ms step_avg:1617.02ms
step:21000/100000 val_loss:2.2203 train_time:33941464ms step_avg:1617.03ms perplexity:9.2096 param_count:132,475,500
step:21001/100000 train_time:33943072ms step_avg:1617.03ms
step:21101/100000 train_time:34104792ms step_avg:1617.03ms
step:21201/100000 train_time:34266529ms step_avg:1617.03ms
step:21301/100000 train_time:34428390ms step_avg:1617.04ms
step:21401/100000 train_time:34590137ms step_avg:1617.04ms
step:21501/100000 train_time:34752068ms step_avg:1617.05ms
step:21601/100000 train_time:34914406ms step_avg:1617.08ms
step:21701/100000 train_time:35076201ms step_avg:1617.09ms
step:21801/100000 train_time:35238081ms step_avg:1617.09ms
step:21901/100000 train_time:35399957ms step_avg:1617.10ms
step:22000/100000 val_loss:2.2258 train_time:35560187ms step_avg:1617.11ms perplexity:9.2605 param_count:132,475,500
step:22001/100000 train_time:35561786ms step_avg:1617.11ms
step:22101/100000 train_time:35723305ms step_avg:1617.10ms
step:22201/100000 train_time:35885165ms step_avg:1617.10ms
step:22301/100000 train_time:36047003ms step_avg:1617.11ms
step:22401/100000 train_time:36209423ms step_avg:1617.14ms
step:22501/100000 train_time:36371256ms step_avg:1617.15ms
step:22601/100000 train_time:36533104ms step_avg:1617.15ms
step:22701/100000 train_time:36695037ms step_avg:1617.16ms
step:22801/100000 train_time:36856937ms step_avg:1617.17ms
step:22901/100000 train_time:37018966ms step_avg:1617.18ms
step:23000/100000 val_loss:2.2201 train_time:37179219ms step_avg:1617.19ms perplexity:9.2084 param_count:132,475,500
step:23001/100000 train_time:37180829ms step_avg:1617.19ms
step:23101/100000 train_time:37342468ms step_avg:1617.19ms
step:23201/100000 train_time:37504865ms step_avg:1617.22ms
step:23301/100000 train_time:37666752ms step_avg:1617.22ms
step:23401/100000 train_time:37828656ms step_avg:1617.23ms
step:23501/100000 train_time:37990459ms step_avg:1617.23ms
step:23601/100000 train_time:38152417ms step_avg:1617.24ms
step:23701/100000 train_time:38314283ms step_avg:1617.25ms
step:23801/100000 train_time:38476169ms step_avg:1617.26ms
step:23901/100000 train_time:38638028ms step_avg:1617.26ms
step:24000/100000 val_loss:2.2138 train_time:38798947ms step_avg:1617.30ms perplexity:9.1504 param_count:132,475,500
step:24001/100000 train_time:38800551ms step_avg:1617.30ms
step:24101/100000 train_time:38962310ms step_avg:1617.30ms
step:24201/100000 train_time:39124208ms step_avg:1617.30ms
step:24301/100000 train_time:39286142ms step_avg:1617.31ms
step:24401/100000 train_time:39448025ms step_avg:1617.32ms
step:24501/100000 train_time:39610093ms step_avg:1617.33ms
step:24601/100000 train_time:39771996ms step_avg:1617.34ms
step:24701/100000 train_time:39934359ms step_avg:1617.36ms
step:24801/100000 train_time:40096200ms step_avg:1617.37ms
step:24901/100000 train_time:40258191ms step_avg:1617.38ms
step:25000/100000 val_loss:2.2100 train_time:40418436ms step_avg:1617.38ms perplexity:9.1160 param_count:132,475,500
step:25001/100000 train_time:40420045ms step_avg:1617.38ms
step:25101/100000 train_time:40581807ms step_avg:1617.39ms
step:25201/100000 train_time:40744032ms step_avg:1617.40ms
step:25301/100000 train_time:40906344ms step_avg:1617.43ms
step:25401/100000 train_time:41068445ms step_avg:1617.44ms
step:25501/100000 train_time:41231224ms step_avg:1617.48ms
step:25601/100000 train_time:41393457ms step_avg:1617.50ms
step:25701/100000 train_time:41555575ms step_avg:1617.51ms
step:25801/100000 train_time:41717754ms step_avg:1617.53ms
step:25901/100000 train_time:41880030ms step_avg:1617.55ms
step:26000/100000 val_loss:2.2077 train_time:42040609ms step_avg:1617.57ms perplexity:9.0947 param_count:132,475,500
step:26001/100000 train_time:42042218ms step_avg:1617.57ms
step:26101/100000 train_time:42204138ms step_avg:1617.57ms
step:26201/100000 train_time:42366244ms step_avg:1617.59ms
step:26301/100000 train_time:42528948ms step_avg:1617.62ms
step:26401/100000 train_time:42691138ms step_avg:1617.64ms
step:26501/100000 train_time:42853335ms step_avg:1617.66ms
step:26601/100000 train_time:43015655ms step_avg:1617.68ms
step:26701/100000 train_time:43177806ms step_avg:1617.69ms
step:26801/100000 train_time:43339994ms step_avg:1617.71ms
step:26901/100000 train_time:43502137ms step_avg:1617.72ms
step:27000/100000 val_loss:2.2100 train_time:43662706ms step_avg:1617.74ms perplexity:9.1160 param_count:132,475,500
step:27001/100000 train_time:43664319ms step_avg:1617.74ms
step:27101/100000 train_time:43826800ms step_avg:1617.76ms
step:27201/100000 train_time:43988978ms step_avg:1617.78ms
step:27301/100000 train_time:44151187ms step_avg:1617.79ms
step:27401/100000 train_time:44313352ms step_avg:1617.81ms
step:27501/100000 train_time:44475556ms step_avg:1617.82ms
step:27601/100000 train_time:44637702ms step_avg:1617.84ms
step:27701/100000 train_time:44799771ms step_avg:1617.85ms
step:27801/100000 train_time:44962577ms step_avg:1617.88ms
step:27901/100000 train_time:45124782ms step_avg:1617.90ms
step:28000/100000 val_loss:2.2029 train_time:45285429ms step_avg:1617.91ms perplexity:9.0514 param_count:132,475,500
step:28001/100000 train_time:45287051ms step_avg:1617.91ms
step:28101/100000 train_time:45448867ms step_avg:1617.92ms
step:28201/100000 train_time:45611000ms step_avg:1617.93ms
step:28301/100000 train_time:45773140ms step_avg:1617.94ms
step:28401/100000 train_time:45935339ms step_avg:1617.95ms
step:28501/100000 train_time:46097414ms step_avg:1617.96ms
step:28601/100000 train_time:46260061ms step_avg:1617.99ms
step:28701/100000 train_time:46422230ms step_avg:1618.01ms
step:28801/100000 train_time:46584539ms step_avg:1618.02ms
step:28901/100000 train_time:46747410ms step_avg:1618.06ms
step:29000/100000 val_loss:2.1940 train_time:46908819ms step_avg:1618.10ms perplexity:8.9714 param_count:132,475,500
step:29001/100000 train_time:46910442ms step_avg:1618.10ms
step:29101/100000 train_time:47073026ms step_avg:1618.13ms
step:29201/100000 train_time:47235827ms step_avg:1618.16ms
step:29301/100000 train_time:47398972ms step_avg:1618.21ms
step:29401/100000 train_time:47562495ms step_avg:1618.27ms
step:29501/100000 train_time:47725499ms step_avg:1618.31ms
step:29601/100000 train_time:47888633ms step_avg:1618.35ms
step:29701/100000 train_time:48051778ms step_avg:1618.40ms
step:29801/100000 train_time:48214976ms step_avg:1618.44ms
step:29901/100000 train_time:48378170ms step_avg:1618.49ms
step:30000/100000 val_loss:2.1990 train_time:48539756ms step_avg:1618.53ms perplexity:9.0160 param_count:132,475,500
step:30001/100000 train_time:48541381ms step_avg:1618.53ms
step:30101/100000 train_time:48703589ms step_avg:1618.54ms
step:30201/100000 train_time:48866280ms step_avg:1618.57ms
step:30301/100000 train_time:49028632ms step_avg:1618.59ms
step:30401/100000 train_time:49190672ms step_avg:1618.59ms
step:30501/100000 train_time:49352695ms step_avg:1618.60ms
step:30601/100000 train_time:49515007ms step_avg:1618.61ms
step:30701/100000 train_time:49677093ms step_avg:1618.62ms
step:30801/100000 train_time:49839339ms step_avg:1618.63ms
step:30901/100000 train_time:50002134ms step_avg:1618.66ms
step:31000/100000 val_loss:2.1988 train_time:50162738ms step_avg:1618.67ms perplexity:9.0141 param_count:132,475,500
step:31001/100000 train_time:50164353ms step_avg:1618.67ms
step:31101/100000 train_time:50326226ms step_avg:1618.68ms
step:31201/100000 train_time:50488279ms step_avg:1618.68ms
step:31301/100000 train_time:50650437ms step_avg:1618.69ms
step:31401/100000 train_time:50812718ms step_avg:1618.70ms
step:31501/100000 train_time:50974954ms step_avg:1618.72ms
step:31601/100000 train_time:51137146ms step_avg:1618.73ms
step:31701/100000 train_time:51299858ms step_avg:1618.75ms
step:31801/100000 train_time:51462182ms step_avg:1618.77ms
step:31901/100000 train_time:51624650ms step_avg:1618.78ms
step:32000/100000 val_loss:2.1962 train_time:51785294ms step_avg:1618.80ms perplexity:8.9912 param_count:132,475,500
step:32001/100000 train_time:51786904ms step_avg:1618.80ms
step:32101/100000 train_time:51948916ms step_avg:1618.80ms
step:32201/100000 train_time:52111034ms step_avg:1618.81ms
step:32301/100000 train_time:52273277ms step_avg:1618.82ms
step:32401/100000 train_time:52435613ms step_avg:1618.83ms
step:32501/100000 train_time:52598540ms step_avg:1618.86ms
step:32601/100000 train_time:52760905ms step_avg:1618.88ms
step:32701/100000 train_time:52922980ms step_avg:1618.89ms
step:32801/100000 train_time:53085144ms step_avg:1618.89ms
step:32901/100000 train_time:53247391ms step_avg:1618.90ms
step:33000/100000 val_loss:2.1804 train_time:53408001ms step_avg:1618.91ms perplexity:8.8498 param_count:132,475,500
step:33001/100000 train_time:53409603ms step_avg:1618.91ms
step:33101/100000 train_time:53571709ms step_avg:1618.92ms
step:33201/100000 train_time:53733897ms step_avg:1618.93ms
step:33301/100000 train_time:53896547ms step_avg:1618.95ms
step:33401/100000 train_time:54058888ms step_avg:1618.97ms
step:33501/100000 train_time:54221109ms step_avg:1618.98ms
step:33601/100000 train_time:54383543ms step_avg:1618.99ms
step:33701/100000 train_time:54545847ms step_avg:1619.00ms
step:33801/100000 train_time:54708113ms step_avg:1619.01ms
step:33901/100000 train_time:54870378ms step_avg:1619.03ms
step:34000/100000 val_loss:2.1847 train_time:55031596ms step_avg:1619.05ms perplexity:8.8882 param_count:132,475,500
step:34001/100000 train_time:55033214ms step_avg:1619.05ms
step:34101/100000 train_time:55195314ms step_avg:1619.06ms
step:34201/100000 train_time:55357502ms step_avg:1619.07ms
step:34301/100000 train_time:55519809ms step_avg:1619.08ms
step:34401/100000 train_time:55682016ms step_avg:1619.09ms
step:34501/100000 train_time:55844236ms step_avg:1619.10ms
step:34601/100000 train_time:56006385ms step_avg:1619.10ms
step:34701/100000 train_time:56168700ms step_avg:1619.11ms
step:34801/100000 train_time:56331492ms step_avg:1619.14ms
step:34901/100000 train_time:56493897ms step_avg:1619.15ms
step:35000/100000 val_loss:2.1827 train_time:56654637ms step_avg:1619.17ms perplexity:8.8699 param_count:132,475,500
step:35001/100000 train_time:56656252ms step_avg:1619.17ms
step:35101/100000 train_time:56818284ms step_avg:1619.17ms
step:35201/100000 train_time:56980543ms step_avg:1619.18ms
step:35301/100000 train_time:57142839ms step_avg:1619.19ms
step:35401/100000 train_time:57305301ms step_avg:1619.21ms
step:35501/100000 train_time:57467490ms step_avg:1619.21ms
step:35601/100000 train_time:57630197ms step_avg:1619.24ms
step:35701/100000 train_time:57792696ms step_avg:1619.25ms
step:35801/100000 train_time:57955023ms step_avg:1619.26ms
step:35901/100000 train_time:58117287ms step_avg:1619.27ms
step:36000/100000 val_loss:2.1829 train_time:58277840ms step_avg:1619.28ms perplexity:8.8718 param_count:132,475,500
step:36001/100000 train_time:58279454ms step_avg:1619.28ms
step:36101/100000 train_time:58441327ms step_avg:1619.28ms
step:36201/100000 train_time:58603720ms step_avg:1619.29ms
step:36301/100000 train_time:58766044ms step_avg:1619.30ms
step:36401/100000 train_time:58928889ms step_avg:1619.33ms
step:36501/100000 train_time:59091288ms step_avg:1619.34ms
step:36601/100000 train_time:59253775ms step_avg:1619.35ms
step:36701/100000 train_time:59416228ms step_avg:1619.37ms
step:36801/100000 train_time:59578650ms step_avg:1619.38ms
step:36901/100000 train_time:59741166ms step_avg:1619.40ms
step:37000/100000 val_loss:2.1802 train_time:59901971ms step_avg:1619.41ms perplexity:8.8483 param_count:132,475,500
step:37001/100000 train_time:59903591ms step_avg:1619.41ms
step:37101/100000 train_time:60066273ms step_avg:1619.43ms
step:37201/100000 train_time:60228602ms step_avg:1619.44ms
step:37301/100000 train_time:60390938ms step_avg:1619.45ms
step:37401/100000 train_time:60553524ms step_avg:1619.47ms
step:37501/100000 train_time:60715979ms step_avg:1619.48ms
step:37601/100000 train_time:60878414ms step_avg:1619.49ms
step:37701/100000 train_time:61041043ms step_avg:1619.51ms
step:37801/100000 train_time:61203563ms step_avg:1619.53ms
step:37901/100000 train_time:61366648ms step_avg:1619.56ms
step:38000/100000 val_loss:2.1769 train_time:61527713ms step_avg:1619.58ms perplexity:8.8188 param_count:132,475,500
step:38001/100000 train_time:61529344ms step_avg:1619.58ms
step:38101/100000 train_time:61691550ms step_avg:1619.58ms
step:38201/100000 train_time:61854080ms step_avg:1619.60ms
step:38301/100000 train_time:62016456ms step_avg:1619.61ms
step:38401/100000 train_time:62178900ms step_avg:1619.62ms
step:38501/100000 train_time:62341285ms step_avg:1619.63ms
step:38601/100000 train_time:62503887ms step_avg:1619.65ms
step:38701/100000 train_time:62667000ms step_avg:1619.68ms
step:38801/100000 train_time:62829428ms step_avg:1619.69ms
step:38901/100000 train_time:62991856ms step_avg:1619.70ms
step:39000/100000 val_loss:2.1798 train_time:63152816ms step_avg:1619.72ms perplexity:8.8448 param_count:132,475,500
step:39001/100000 train_time:63154433ms step_avg:1619.72ms
step:39101/100000 train_time:63316667ms step_avg:1619.72ms
step:39201/100000 train_time:63479140ms step_avg:1619.74ms
step:39301/100000 train_time:63641643ms step_avg:1619.75ms
step:39401/100000 train_time:63804120ms step_avg:1619.76ms
step:39501/100000 train_time:63966904ms step_avg:1619.78ms
step:39601/100000 train_time:64129169ms step_avg:1619.79ms
step:39701/100000 train_time:64291573ms step_avg:1619.80ms
step:39801/100000 train_time:64454236ms step_avg:1619.82ms
step:39901/100000 train_time:64616754ms step_avg:1619.83ms
step:40000/100000 val_loss:2.1775 train_time:64777666ms step_avg:1619.85ms perplexity:8.8247 param_count:132,475,500
step:40001/100000 train_time:64779301ms step_avg:1619.85ms
step:40101/100000 train_time:64941709ms step_avg:1619.86ms
step:40201/100000 train_time:65104759ms step_avg:1619.88ms
step:40301/100000 train_time:65267356ms step_avg:1619.90ms
step:40401/100000 train_time:65430000ms step_avg:1619.92ms
step:40501/100000 train_time:65592682ms step_avg:1619.93ms
step:40601/100000 train_time:65755480ms step_avg:1619.95ms
step:40701/100000 train_time:65918068ms step_avg:1619.97ms
step:40801/100000 train_time:66080729ms step_avg:1619.98ms
step:40901/100000 train_time:66243337ms step_avg:1620.00ms
step:41000/100000 val_loss:2.1718 train_time:66404703ms step_avg:1620.02ms perplexity:8.7743 param_count:132,475,500
step:41001/100000 train_time:66406310ms step_avg:1620.02ms
step:41101/100000 train_time:66568636ms step_avg:1620.03ms
step:41201/100000 train_time:66731199ms step_avg:1620.04ms
step:41301/100000 train_time:66893799ms step_avg:1620.06ms
step:41401/100000 train_time:67056487ms step_avg:1620.07ms
step:41501/100000 train_time:67219121ms step_avg:1620.09ms
step:41601/100000 train_time:67381563ms step_avg:1620.10ms
step:41701/100000 train_time:67544260ms step_avg:1620.12ms
step:41801/100000 train_time:67707421ms step_avg:1620.14ms
step:41901/100000 train_time:67869939ms step_avg:1620.16ms
step:42000/100000 val_loss:2.1570 train_time:68030886ms step_avg:1620.17ms perplexity:8.6454 param_count:132,475,500
step:42001/100000 train_time:68032521ms step_avg:1620.17ms
step:42101/100000 train_time:68194928ms step_avg:1620.18ms
step:42201/100000 train_time:68357515ms step_avg:1620.19ms
step:42301/100000 train_time:68520168ms step_avg:1620.21ms
step:42401/100000 train_time:68682813ms step_avg:1620.22ms
step:42501/100000 train_time:68845825ms step_avg:1620.24ms
step:42601/100000 train_time:69008416ms step_avg:1620.26ms
step:42701/100000 train_time:69170935ms step_avg:1620.27ms
step:42801/100000 train_time:69333505ms step_avg:1620.28ms
step:42901/100000 train_time:69496180ms step_avg:1620.30ms
step:43000/100000 val_loss:2.1671 train_time:69657234ms step_avg:1620.31ms perplexity:8.7327 param_count:132,475,500
step:43001/100000 train_time:69658864ms step_avg:1620.31ms
step:43101/100000 train_time:69821345ms step_avg:1620.32ms
step:43201/100000 train_time:69984017ms step_avg:1620.34ms
step:43301/100000 train_time:70147047ms step_avg:1620.36ms
step:43401/100000 train_time:70309690ms step_avg:1620.37ms
step:43501/100000 train_time:70472162ms step_avg:1620.38ms
step:43601/100000 train_time:70634791ms step_avg:1620.40ms
step:43701/100000 train_time:70797356ms step_avg:1620.41ms
step:43801/100000 train_time:70960058ms step_avg:1620.43ms
step:43901/100000 train_time:71122750ms step_avg:1620.44ms
step:44000/100000 val_loss:2.1619 train_time:71283745ms step_avg:1620.45ms perplexity:8.6878 param_count:132,475,500
step:44001/100000 train_time:71285381ms step_avg:1620.45ms
step:44101/100000 train_time:71448218ms step_avg:1620.47ms
step:44201/100000 train_time:71610881ms step_avg:1620.49ms
step:44301/100000 train_time:71773596ms step_avg:1620.50ms
step:44401/100000 train_time:71936305ms step_avg:1620.52ms
step:44501/100000 train_time:72099004ms step_avg:1620.53ms
step:44601/100000 train_time:72261645ms step_avg:1620.54ms
step:44701/100000 train_time:72424263ms step_avg:1620.56ms
step:44801/100000 train_time:72586744ms step_avg:1620.57ms
step:44901/100000 train_time:72749895ms step_avg:1620.59ms
step:45000/100000 val_loss:2.1600 train_time:72910818ms step_avg:1620.60ms perplexity:8.6712 param_count:132,475,500
step:45001/100000 train_time:72912440ms step_avg:1620.60ms
step:45101/100000 train_time:73074851ms step_avg:1620.61ms
step:45201/100000 train_time:73237456ms step_avg:1620.62ms
step:45301/100000 train_time:73400118ms step_avg:1620.63ms
step:45401/100000 train_time:73562721ms step_avg:1620.65ms
step:45501/100000 train_time:73725428ms step_avg:1620.66ms
step:45601/100000 train_time:73888564ms step_avg:1620.68ms
step:45701/100000 train_time:74051238ms step_avg:1620.70ms
step:45801/100000 train_time:74213905ms step_avg:1620.71ms
step:45901/100000 train_time:74376526ms step_avg:1620.72ms
step:46000/100000 val_loss:2.1661 train_time:74537680ms step_avg:1620.74ms perplexity:8.7241 param_count:132,475,500
step:46001/100000 train_time:74539291ms step_avg:1620.74ms
step:46101/100000 train_time:74701711ms step_avg:1620.74ms
step:46201/100000 train_time:74864399ms step_avg:1620.76ms
step:46301/100000 train_time:75027024ms step_avg:1620.77ms
step:46401/100000 train_time:75190270ms step_avg:1620.79ms
step:46501/100000 train_time:75352827ms step_avg:1620.80ms
step:46601/100000 train_time:75515389ms step_avg:1620.81ms
step:46701/100000 train_time:75677950ms step_avg:1620.83ms
step:46801/100000 train_time:75840505ms step_avg:1620.84ms
step:46901/100000 train_time:76003069ms step_avg:1620.85ms
step:47000/100000 val_loss:2.1577 train_time:76163950ms step_avg:1620.85ms perplexity:8.6508 param_count:132,475,500
step:47001/100000 train_time:76165575ms step_avg:1620.85ms
step:47101/100000 train_time:76327841ms step_avg:1620.86ms
step:47201/100000 train_time:76490814ms step_avg:1620.88ms
step:47301/100000 train_time:76653315ms step_avg:1620.89ms
step:47401/100000 train_time:76815840ms step_avg:1620.90ms
step:47501/100000 train_time:76978480ms step_avg:1620.91ms
step:47601/100000 train_time:77141152ms step_avg:1620.92ms
step:47701/100000 train_time:77303880ms step_avg:1620.93ms
step:47801/100000 train_time:77466595ms step_avg:1620.95ms
step:47901/100000 train_time:77629387ms step_avg:1620.96ms
step:48000/100000 val_loss:2.1662 train_time:77790946ms step_avg:1620.98ms perplexity:8.7251 param_count:132,475,500
step:48001/100000 train_time:77792568ms step_avg:1620.98ms
step:48101/100000 train_time:77955042ms step_avg:1620.99ms
step:48201/100000 train_time:78117748ms step_avg:1621.00ms
step:48301/100000 train_time:78280622ms step_avg:1621.02ms
step:48401/100000 train_time:78443388ms step_avg:1621.03ms
step:48501/100000 train_time:78606041ms step_avg:1621.04ms
step:48601/100000 train_time:78768729ms step_avg:1621.06ms
step:48701/100000 train_time:78931821ms step_avg:1621.08ms
step:48801/100000 train_time:79094394ms step_avg:1621.09ms
step:48901/100000 train_time:79256994ms step_avg:1621.10ms
step:49000/100000 val_loss:2.1556 train_time:79418079ms step_avg:1621.11ms perplexity:8.6328 param_count:132,475,500
step:49001/100000 train_time:79419692ms step_avg:1621.11ms
step:49101/100000 train_time:79582035ms step_avg:1621.11ms
step:49201/100000 train_time:79744616ms step_avg:1621.12ms
step:49301/100000 train_time:79907229ms step_avg:1621.13ms
step:49401/100000 train_time:80069948ms step_avg:1621.14ms
step:49501/100000 train_time:80233285ms step_avg:1621.17ms
step:49601/100000 train_time:80396003ms step_avg:1621.18ms
step:49701/100000 train_time:80558747ms step_avg:1621.19ms
step:49801/100000 train_time:80721486ms step_avg:1621.21ms
step:49901/100000 train_time:80884135ms step_avg:1621.22ms
step:50000/100000 val_loss:2.1683 train_time:81045194ms step_avg:1621.23ms perplexity:8.7435 param_count:132,475,500
step:50001/100000 train_time:81046803ms step_avg:1621.23ms
step:50101/100000 train_time:81209335ms step_avg:1621.24ms
step:50201/100000 train_time:81372116ms step_avg:1621.25ms
step:50301/100000 train_time:81535501ms step_avg:1621.27ms
step:50401/100000 train_time:81698384ms step_avg:1621.29ms
step:50501/100000 train_time:81861192ms step_avg:1621.30ms
step:50601/100000 train_time:82023889ms step_avg:1621.31ms
step:50701/100000 train_time:82186730ms step_avg:1621.33ms
step:50801/100000 train_time:82349464ms step_avg:1621.34ms
step:50901/100000 train_time:82512271ms step_avg:1621.35ms
step:51000/100000 val_loss:2.1638 train_time:82673437ms step_avg:1621.37ms perplexity:8.7044 param_count:132,475,500
step:51001/100000 train_time:82675057ms step_avg:1621.37ms
step:51101/100000 train_time:82837936ms step_avg:1621.38ms
step:51201/100000 train_time:83000816ms step_avg:1621.39ms
step:51301/100000 train_time:83163410ms step_avg:1621.40ms
step:51401/100000 train_time:83326252ms step_avg:1621.42ms
step:51501/100000 train_time:83489071ms step_avg:1621.43ms
step:51601/100000 train_time:83651855ms step_avg:1621.44ms
step:51701/100000 train_time:83814624ms step_avg:1621.45ms
step:51801/100000 train_time:83978109ms step_avg:1621.48ms
step:51901/100000 train_time:84140827ms step_avg:1621.49ms
step:52000/100000 val_loss:2.1525 train_time:84301941ms step_avg:1621.50ms perplexity:8.6065 param_count:132,475,500
step:52001/100000 train_time:84303550ms step_avg:1621.50ms
step:52101/100000 train_time:84466088ms step_avg:1621.51ms
step:52201/100000 train_time:84628791ms step_avg:1621.52ms
step:52301/100000 train_time:84791475ms step_avg:1621.53ms
step:52401/100000 train_time:84954225ms step_avg:1621.54ms
step:52501/100000 train_time:85116755ms step_avg:1621.55ms
step:52601/100000 train_time:85279875ms step_avg:1621.57ms
step:52701/100000 train_time:85442307ms step_avg:1621.57ms
step:52801/100000 train_time:85604769ms step_avg:1621.58ms
step:52901/100000 train_time:85767232ms step_avg:1621.58ms
step:53000/100000 val_loss:2.1543 train_time:85928166ms step_avg:1621.59ms perplexity:8.6214 param_count:132,475,500
step:53001/100000 train_time:85929784ms step_avg:1621.59ms
step:53101/100000 train_time:86092055ms step_avg:1621.59ms
step:53201/100000 train_time:86254707ms step_avg:1621.60ms
step:53301/100000 train_time:86417383ms step_avg:1621.61ms
step:53401/100000 train_time:86580498ms step_avg:1621.63ms
step:53501/100000 train_time:86743118ms step_avg:1621.64ms
step:53601/100000 train_time:86905844ms step_avg:1621.65ms
step:53701/100000 train_time:87068485ms step_avg:1621.66ms
step:53801/100000 train_time:87231203ms step_avg:1621.67ms
step:53901/100000 train_time:87393919ms step_avg:1621.68ms
step:54000/100000 val_loss:2.1576 train_time:87554910ms step_avg:1621.69ms perplexity:8.6508 param_count:132,475,500
step:54001/100000 train_time:87556548ms step_avg:1621.69ms
step:54101/100000 train_time:87719031ms step_avg:1621.69ms
step:54201/100000 train_time:87882245ms step_avg:1621.71ms
step:54301/100000 train_time:88044909ms step_avg:1621.72ms
step:54401/100000 train_time:88207639ms step_avg:1621.73ms
step:54501/100000 train_time:88370327ms step_avg:1621.74ms
step:54601/100000 train_time:88533009ms step_avg:1621.75ms
step:54701/100000 train_time:88695759ms step_avg:1621.76ms
step:54801/100000 train_time:88858459ms step_avg:1621.77ms
step:54901/100000 train_time:89021764ms step_avg:1621.79ms
step:55000/100000 val_loss:2.1477 train_time:89182749ms step_avg:1621.80ms perplexity:8.5648 param_count:132,475,500
step:55001/100000 train_time:89184358ms step_avg:1621.80ms
step:55101/100000 train_time:89346802ms step_avg:1621.80ms
step:55201/100000 train_time:89509534ms step_avg:1621.81ms
step:55301/100000 train_time:89672192ms step_avg:1621.82ms
step:55401/100000 train_time:89834886ms step_avg:1621.83ms
step:55501/100000 train_time:89997713ms step_avg:1621.84ms
step:55601/100000 train_time:90160476ms step_avg:1621.85ms
step:55701/100000 train_time:90323865ms step_avg:1621.88ms
step:55801/100000 train_time:90486556ms step_avg:1621.88ms
step:55901/100000 train_time:90649384ms step_avg:1621.90ms
step:56000/100000 val_loss:2.1482 train_time:90810438ms step_avg:1621.90ms perplexity:8.5694 param_count:132,475,500
step:56001/100000 train_time:90812064ms step_avg:1621.90ms
step:56101/100000 train_time:90974516ms step_avg:1621.91ms
step:56201/100000 train_time:91137354ms step_avg:1621.92ms
step:56301/100000 train_time:91300035ms step_avg:1621.93ms
step:56401/100000 train_time:91462852ms step_avg:1621.94ms
step:56501/100000 train_time:91626133ms step_avg:1621.96ms
step:56601/100000 train_time:91788872ms step_avg:1621.97ms
step:56701/100000 train_time:91951768ms step_avg:1621.98ms
step:56801/100000 train_time:92114409ms step_avg:1621.99ms
step:56901/100000 train_time:92277241ms step_avg:1622.00ms
step:57000/100000 val_loss:2.1525 train_time:92438282ms step_avg:1622.01ms perplexity:8.6060 param_count:132,475,500
step:57001/100000 train_time:92439899ms step_avg:1622.01ms
step:57101/100000 train_time:92602257ms step_avg:1622.01ms
step:57201/100000 train_time:92764978ms step_avg:1622.02ms
step:57301/100000 train_time:92928226ms step_avg:1622.04ms
step:57401/100000 train_time:93090880ms step_avg:1622.05ms
step:57501/100000 train_time:93253683ms step_avg:1622.06ms
step:57601/100000 train_time:93416462ms step_avg:1622.07ms
step:57701/100000 train_time:93579231ms step_avg:1622.08ms
step:57801/100000 train_time:93742027ms step_avg:1622.09ms
step:57901/100000 train_time:93904736ms step_avg:1622.10ms
step:58000/100000 val_loss:2.1508 train_time:94066416ms step_avg:1622.11ms perplexity:8.5920 param_count:132,475,500
step:58001/100000 train_time:94068035ms step_avg:1622.11ms
step:58101/100000 train_time:94230587ms step_avg:1622.12ms
step:58201/100000 train_time:94393389ms step_avg:1622.13ms
step:58301/100000 train_time:94556268ms step_avg:1622.14ms
step:58401/100000 train_time:94719067ms step_avg:1622.15ms
step:58501/100000 train_time:94882018ms step_avg:1622.16ms
step:58601/100000 train_time:95044666ms step_avg:1622.17ms
step:58701/100000 train_time:95207572ms step_avg:1622.18ms
step:58801/100000 train_time:95370852ms step_avg:1622.20ms
step:58901/100000 train_time:95533790ms step_avg:1622.21ms
step:59000/100000 val_loss:2.1457 train_time:95695013ms step_avg:1622.22ms perplexity:8.5483 param_count:132,475,500
step:59001/100000 train_time:95696631ms step_avg:1622.22ms
step:59101/100000 train_time:95859135ms step_avg:1622.23ms
step:59201/100000 train_time:96021906ms step_avg:1622.24ms
step:59301/100000 train_time:96184679ms step_avg:1622.25ms
step:59401/100000 train_time:96347618ms step_avg:1622.26ms
step:59501/100000 train_time:96510363ms step_avg:1622.27ms
step:59601/100000 train_time:96673765ms step_avg:1622.29ms
step:59701/100000 train_time:96836527ms step_avg:1622.30ms
step:59801/100000 train_time:96999470ms step_avg:1622.31ms
step:59901/100000 train_time:97162182ms step_avg:1622.32ms
step:60000/100000 val_loss:2.1404 train_time:97323290ms step_avg:1622.33ms perplexity:8.5024 param_count:132,475,500
step:60001/100000 train_time:97324901ms step_avg:1622.33ms
step:60101/100000 train_time:97487385ms step_avg:1622.33ms
step:60201/100000 train_time:97650056ms step_avg:1622.34ms
step:60301/100000 train_time:97812909ms step_avg:1622.35ms
step:60401/100000 train_time:97976151ms step_avg:1622.36ms
step:60501/100000 train_time:98139008ms step_avg:1622.37ms
step:60601/100000 train_time:98301765ms step_avg:1622.38ms
step:60701/100000 train_time:98464566ms step_avg:1622.39ms
step:60801/100000 train_time:98627301ms step_avg:1622.40ms
step:60901/100000 train_time:98790154ms step_avg:1622.41ms
step:61000/100000 val_loss:2.1329 train_time:98951432ms step_avg:1622.42ms perplexity:8.4396 param_count:132,475,500
step:61001/100000 train_time:98953054ms step_avg:1622.42ms
step:61101/100000 train_time:99116229ms step_avg:1622.44ms
step:61201/100000 train_time:99279180ms step_avg:1622.45ms
step:61301/100000 train_time:99442153ms step_avg:1622.46ms
step:61401/100000 train_time:99605147ms step_avg:1622.47ms
step:61501/100000 train_time:99768131ms step_avg:1622.48ms
step:61601/100000 train_time:99930953ms step_avg:1622.49ms
step:61701/100000 train_time:100093925ms step_avg:1622.50ms
step:61801/100000 train_time:100256760ms step_avg:1622.51ms
step:61901/100000 train_time:100420179ms step_avg:1622.53ms
step:62000/100000 val_loss:2.1390 train_time:100581380ms step_avg:1622.54ms perplexity:8.4913 param_count:132,475,500
step:62001/100000 train_time:100583004ms step_avg:1622.54ms
step:62101/100000 train_time:100745526ms step_avg:1622.55ms
step:62201/100000 train_time:100908309ms step_avg:1622.55ms
step:62301/100000 train_time:101071205ms step_avg:1622.57ms
step:62401/100000 train_time:101233987ms step_avg:1622.57ms
step:62501/100000 train_time:101396870ms step_avg:1622.58ms
step:62601/100000 train_time:101559786ms step_avg:1622.59ms
step:62701/100000 train_time:101723376ms step_avg:1622.62ms
step:62801/100000 train_time:101886403ms step_avg:1622.63ms
step:62901/100000 train_time:102049498ms step_avg:1622.64ms
step:63000/100000 val_loss:2.1466 train_time:102210791ms step_avg:1622.65ms perplexity:8.5558 param_count:132,475,500
step:63001/100000 train_time:102212417ms step_avg:1622.65ms
step:63101/100000 train_time:102375109ms step_avg:1622.66ms
step:63201/100000 train_time:102538074ms step_avg:1622.67ms
step:63301/100000 train_time:102701039ms step_avg:1622.68ms
step:63401/100000 train_time:102864164ms step_avg:1622.69ms
step:63501/100000 train_time:103027579ms step_avg:1622.71ms
step:63601/100000 train_time:103190544ms step_avg:1622.72ms
step:63701/100000 train_time:103353447ms step_avg:1622.73ms
step:63801/100000 train_time:103516373ms step_avg:1622.74ms
step:63901/100000 train_time:103679340ms step_avg:1622.75ms
step:64000/100000 val_loss:2.1398 train_time:103840558ms step_avg:1622.76ms perplexity:8.4976 param_count:132,475,500
step:64001/100000 train_time:103842175ms step_avg:1622.76ms
step:64101/100000 train_time:104004854ms step_avg:1622.77ms
step:64201/100000 train_time:104168191ms step_avg:1622.78ms
step:64301/100000 train_time:104331131ms step_avg:1622.80ms
step:64401/100000 train_time:104493900ms step_avg:1622.80ms
step:64501/100000 train_time:104656915ms step_avg:1622.81ms
step:64601/100000 train_time:104819692ms step_avg:1622.82ms
step:64701/100000 train_time:104982622ms step_avg:1622.83ms
step:64801/100000 train_time:105145752ms step_avg:1622.85ms
step:64901/100000 train_time:105308633ms step_avg:1622.85ms
step:65000/100000 val_loss:2.1308 train_time:105470519ms step_avg:1622.87ms perplexity:8.4213 param_count:132,475,500
step:65001/100000 train_time:105472144ms step_avg:1622.87ms
step:65101/100000 train_time:105634797ms step_avg:1622.88ms
step:65201/100000 train_time:105797884ms step_avg:1622.89ms
step:65301/100000 train_time:105960817ms step_avg:1622.90ms
step:65401/100000 train_time:106123985ms step_avg:1622.91ms
step:65501/100000 train_time:106286930ms step_avg:1622.92ms
step:65601/100000 train_time:106450165ms step_avg:1622.94ms
step:65701/100000 train_time:106613208ms step_avg:1622.95ms
step:65801/100000 train_time:106776695ms step_avg:1622.97ms
step:65901/100000 train_time:106939696ms step_avg:1622.98ms
step:66000/100000 val_loss:2.1346 train_time:107101052ms step_avg:1622.99ms perplexity:8.4537 param_count:132,475,500
step:66001/100000 train_time:107102667ms step_avg:1622.99ms
step:66101/100000 train_time:107265359ms step_avg:1622.99ms
step:66201/100000 train_time:107428352ms step_avg:1623.01ms
step:66301/100000 train_time:107591270ms step_avg:1623.01ms
step:66401/100000 train_time:107754254ms step_avg:1623.03ms
step:66501/100000 train_time:107917185ms step_avg:1623.03ms
step:66601/100000 train_time:108080788ms step_avg:1623.05ms
step:66701/100000 train_time:108243800ms step_avg:1623.06ms
step:66801/100000 train_time:108406800ms step_avg:1623.07ms
step:66901/100000 train_time:108569771ms step_avg:1623.08ms
step:67000/100000 val_loss:2.1420 train_time:108731264ms step_avg:1623.10ms perplexity:8.5161 param_count:132,475,500
step:67001/100000 train_time:108732877ms step_avg:1623.10ms
step:67101/100000 train_time:108895486ms step_avg:1623.10ms
step:67201/100000 train_time:109058466ms step_avg:1623.11ms
step:67301/100000 train_time:109221897ms step_avg:1623.13ms
step:67401/100000 train_time:109385098ms step_avg:1623.14ms
step:67501/100000 train_time:109547964ms step_avg:1623.15ms
step:67601/100000 train_time:109710936ms step_avg:1623.16ms
step:67701/100000 train_time:109873840ms step_avg:1623.17ms
step:67801/100000 train_time:110036823ms step_avg:1623.18ms
step:67901/100000 train_time:110199660ms step_avg:1623.19ms
step:68000/100000 val_loss:2.1319 train_time:110361012ms step_avg:1623.19ms perplexity:8.4305 param_count:132,475,500
step:68001/100000 train_time:110362641ms step_avg:1623.19ms
step:68101/100000 train_time:110525724ms step_avg:1623.21ms
step:68201/100000 train_time:110688766ms step_avg:1623.22ms
step:68301/100000 train_time:110851653ms step_avg:1623.22ms
step:68401/100000 train_time:111014630ms step_avg:1623.23ms
step:68501/100000 train_time:111177579ms step_avg:1623.24ms
step:68601/100000 train_time:111340577ms step_avg:1623.25ms
step:68701/100000 train_time:111503619ms step_avg:1623.26ms
step:68801/100000 train_time:111666548ms step_avg:1623.27ms
step:68901/100000 train_time:111830071ms step_avg:1623.29ms
step:69000/100000 val_loss:2.1378 train_time:111991447ms step_avg:1623.30ms perplexity:8.4806 param_count:132,475,500
step:69001/100000 train_time:111993076ms step_avg:1623.30ms
step:69101/100000 train_time:112155851ms step_avg:1623.31ms
step:69201/100000 train_time:112318824ms step_avg:1623.32ms
step:69301/100000 train_time:112481884ms step_avg:1623.33ms
step:69401/100000 train_time:112644901ms step_avg:1623.34ms
step:69501/100000 train_time:112807919ms step_avg:1623.35ms
step:69601/100000 train_time:112970934ms step_avg:1623.36ms
step:69701/100000 train_time:113134437ms step_avg:1623.37ms
step:69801/100000 train_time:113297344ms step_avg:1623.38ms
step:69901/100000 train_time:113460336ms step_avg:1623.39ms
step:70000/100000 val_loss:2.1372 train_time:113621648ms step_avg:1623.40ms perplexity:8.4759 param_count:132,475,500
step:70001/100000 train_time:113623255ms step_avg:1623.40ms
step:70101/100000 train_time:113785938ms step_avg:1623.40ms
step:70201/100000 train_time:113948811ms step_avg:1623.41ms
step:70301/100000 train_time:114111769ms step_avg:1623.42ms
step:70401/100000 train_time:114275322ms step_avg:1623.44ms
step:70501/100000 train_time:114438272ms step_avg:1623.45ms
step:70601/100000 train_time:114601295ms step_avg:1623.45ms
step:70701/100000 train_time:114764204ms step_avg:1623.46ms
step:70801/100000 train_time:114927184ms step_avg:1623.47ms
step:70901/100000 train_time:115090077ms step_avg:1623.48ms
step:71000/100000 val_loss:2.1394 train_time:115251492ms step_avg:1623.49ms perplexity:8.4945 param_count:132,475,500
step:71001/100000 train_time:115253116ms step_avg:1623.49ms
step:71101/100000 train_time:115415671ms step_avg:1623.49ms
step:71201/100000 train_time:115579204ms step_avg:1623.51ms
step:71301/100000 train_time:115742154ms step_avg:1623.52ms
step:71401/100000 train_time:115904957ms step_avg:1623.52ms
step:71501/100000 train_time:116068056ms step_avg:1623.53ms
step:71601/100000 train_time:116231001ms step_avg:1623.54ms
step:71701/100000 train_time:116394103ms step_avg:1623.55ms
step:71801/100000 train_time:116557046ms step_avg:1623.56ms
step:71901/100000 train_time:116719987ms step_avg:1623.57ms
step:72000/100000 val_loss:2.1248 train_time:116881882ms step_avg:1623.58ms perplexity:8.3708 param_count:132,475,500
step:72001/100000 train_time:116883515ms step_avg:1623.59ms
step:72101/100000 train_time:117046210ms step_avg:1623.59ms
step:72201/100000 train_time:117209234ms step_avg:1623.60ms
step:72301/100000 train_time:117372248ms step_avg:1623.61ms
step:72401/100000 train_time:117535331ms step_avg:1623.62ms
step:72501/100000 train_time:117698371ms step_avg:1623.63ms
step:72601/100000 train_time:117861448ms step_avg:1623.64ms
step:72701/100000 train_time:118024519ms step_avg:1623.65ms
step:72801/100000 train_time:118188077ms step_avg:1623.66ms
step:72901/100000 train_time:118351228ms step_avg:1623.67ms
step:73000/100000 val_loss:2.1343 train_time:118512611ms step_avg:1623.68ms perplexity:8.4512 param_count:132,475,500
step:73001/100000 train_time:118514242ms step_avg:1623.68ms
step:73101/100000 train_time:118677103ms step_avg:1623.69ms
step:73201/100000 train_time:118840044ms step_avg:1623.70ms
step:73301/100000 train_time:119003128ms step_avg:1623.71ms
step:73401/100000 train_time:119166026ms step_avg:1623.71ms
step:73501/100000 train_time:119329632ms step_avg:1623.73ms
step:73601/100000 train_time:119492569ms step_avg:1623.74ms
step:73701/100000 train_time:119655610ms step_avg:1623.75ms
step:73801/100000 train_time:119818603ms step_avg:1623.76ms
step:73901/100000 train_time:119981825ms step_avg:1623.77ms
step:74000/100000 val_loss:2.1279 train_time:120143137ms step_avg:1623.78ms perplexity:8.3969 param_count:132,475,500
step:74001/100000 train_time:120144744ms step_avg:1623.78ms
step:74101/100000 train_time:120307594ms step_avg:1623.78ms
step:74201/100000 train_time:120470510ms step_avg:1623.79ms
step:74301/100000 train_time:120633991ms step_avg:1623.80ms
step:74401/100000 train_time:120797109ms step_avg:1623.81ms
step:74501/100000 train_time:120960063ms step_avg:1623.82ms
step:74601/100000 train_time:121123156ms step_avg:1623.83ms
step:74701/100000 train_time:121286128ms step_avg:1623.84ms
step:74801/100000 train_time:121449339ms step_avg:1623.85ms
step:74901/100000 train_time:121612257ms step_avg:1623.86ms
step:75000/100000 val_loss:2.1240 train_time:121773751ms step_avg:1623.87ms perplexity:8.3642 param_count:132,475,500
step:75001/100000 train_time:121775366ms step_avg:1623.87ms
step:75101/100000 train_time:121938694ms step_avg:1623.88ms
step:75201/100000 train_time:122101767ms step_avg:1623.89ms
step:75301/100000 train_time:122264881ms step_avg:1623.90ms
step:75401/100000 train_time:122427887ms step_avg:1623.91ms
step:75501/100000 train_time:122590983ms step_avg:1623.92ms
step:75601/100000 train_time:122754186ms step_avg:1623.93ms
step:75701/100000 train_time:122917275ms step_avg:1623.94ms
step:75801/100000 train_time:123080473ms step_avg:1623.95ms
step:75901/100000 train_time:123244089ms step_avg:1623.96ms
step:76000/100000 val_loss:2.1273 train_time:123405618ms step_avg:1623.97ms perplexity:8.3922 param_count:132,475,500
step:76001/100000 train_time:123407236ms step_avg:1623.97ms
step:76101/100000 train_time:123570030ms step_avg:1623.98ms
step:76201/100000 train_time:123733266ms step_avg:1623.99ms
step:76301/100000 train_time:123896547ms step_avg:1624.00ms
step:76401/100000 train_time:124059672ms step_avg:1624.01ms
step:76501/100000 train_time:124223031ms step_avg:1624.02ms
step:76601/100000 train_time:124386761ms step_avg:1624.04ms
step:76701/100000 train_time:124550037ms step_avg:1624.05ms
step:76801/100000 train_time:124713290ms step_avg:1624.06ms
step:76901/100000 train_time:124876545ms step_avg:1624.07ms
step:77000/100000 val_loss:2.1223 train_time:125038159ms step_avg:1624.08ms perplexity:8.3506 param_count:132,475,500
step:77001/100000 train_time:125039763ms step_avg:1624.08ms
step:77101/100000 train_time:125202682ms step_avg:1624.09ms
step:77201/100000 train_time:125365679ms step_avg:1624.10ms
step:77301/100000 train_time:125528829ms step_avg:1624.11ms
step:77401/100000 train_time:125692495ms step_avg:1624.12ms
step:77501/100000 train_time:125855655ms step_avg:1624.13ms
step:77601/100000 train_time:126018834ms step_avg:1624.14ms
step:77701/100000 train_time:126181947ms step_avg:1624.15ms
step:77801/100000 train_time:126345048ms step_avg:1624.16ms
step:77901/100000 train_time:126508068ms step_avg:1624.17ms
step:78000/100000 val_loss:2.1139 train_time:126669484ms step_avg:1624.18ms perplexity:8.2806 param_count:132,475,500
step:78001/100000 train_time:126671115ms step_avg:1624.18ms
step:78101/100000 train_time:126833982ms step_avg:1624.18ms
step:78201/100000 train_time:126997552ms step_avg:1624.20ms
step:78301/100000 train_time:127160632ms step_avg:1624.20ms
step:78401/100000 train_time:127323614ms step_avg:1624.21ms
step:78501/100000 train_time:127486671ms step_avg:1624.22ms
step:78601/100000 train_time:127649847ms step_avg:1624.23ms
step:78701/100000 train_time:127812879ms step_avg:1624.24ms
step:78801/100000 train_time:127976102ms step_avg:1624.25ms
step:78901/100000 train_time:128139216ms step_avg:1624.26ms
step:79000/100000 val_loss:2.1104 train_time:128301160ms step_avg:1624.27ms perplexity:8.2513 param_count:132,475,500
step:79001/100000 train_time:128302786ms step_avg:1624.27ms
step:79101/100000 train_time:128465544ms step_avg:1624.28ms
step:79201/100000 train_time:128628594ms step_avg:1624.28ms
step:79301/100000 train_time:128791775ms step_avg:1624.29ms
step:79401/100000 train_time:128954786ms step_avg:1624.30ms
step:79501/100000 train_time:129117924ms step_avg:1624.31ms
step:79601/100000 train_time:129280859ms step_avg:1624.32ms
step:79701/100000 train_time:129444484ms step_avg:1624.33ms
step:79801/100000 train_time:129607565ms step_avg:1624.34ms
step:79901/100000 train_time:129770702ms step_avg:1624.35ms
step:80000/100000 val_loss:2.1201 train_time:129932247ms step_avg:1624.36ms perplexity:8.3322 param_count:132,475,500
step:80001/100000 train_time:129933893ms step_avg:1624.36ms
step:80101/100000 train_time:130096976ms step_avg:1624.36ms
step:80201/100000 train_time:130260022ms step_avg:1624.37ms
step:80301/100000 train_time:130423292ms step_avg:1624.38ms
step:80401/100000 train_time:130586474ms step_avg:1624.39ms
step:80501/100000 train_time:130750152ms step_avg:1624.41ms
step:80601/100000 train_time:130913453ms step_avg:1624.42ms
step:80701/100000 train_time:131076680ms step_avg:1624.43ms
step:80801/100000 train_time:131239935ms step_avg:1624.44ms
step:80901/100000 train_time:131403185ms step_avg:1624.45ms
step:81000/100000 val_loss:2.1226 train_time:131564774ms step_avg:1624.46ms perplexity:8.3527 param_count:132,475,500
step:81001/100000 train_time:131566400ms step_avg:1624.46ms
step:81101/100000 train_time:131729136ms step_avg:1624.46ms
step:81201/100000 train_time:131892354ms step_avg:1624.47ms
step:81301/100000 train_time:132055967ms step_avg:1624.48ms
step:81401/100000 train_time:132219141ms step_avg:1624.49ms
step:81501/100000 train_time:132382215ms step_avg:1624.50ms
step:81601/100000 train_time:132545428ms step_avg:1624.51ms
step:81701/100000 train_time:132708518ms step_avg:1624.52ms
step:81801/100000 train_time:132871681ms step_avg:1624.53ms
step:81901/100000 train_time:133034860ms step_avg:1624.54ms
step:82000/100000 val_loss:2.1142 train_time:133196207ms step_avg:1624.54ms perplexity:8.2829 param_count:132,475,500
step:82001/100000 train_time:133197841ms step_avg:1624.54ms
step:82101/100000 train_time:133361210ms step_avg:1624.55ms
step:82201/100000 train_time:133524225ms step_avg:1624.56ms
step:82301/100000 train_time:133687191ms step_avg:1624.57ms
step:82401/100000 train_time:133850305ms step_avg:1624.57ms
step:82501/100000 train_time:134013404ms step_avg:1624.58ms
step:82601/100000 train_time:134176498ms step_avg:1624.59ms
step:82701/100000 train_time:134339638ms step_avg:1624.60ms
step:82801/100000 train_time:134503213ms step_avg:1624.61ms
step:82901/100000 train_time:134666478ms step_avg:1624.62ms
step:83000/100000 val_loss:2.1205 train_time:134827847ms step_avg:1624.63ms perplexity:8.3349 param_count:132,475,500
step:83001/100000 train_time:134829473ms step_avg:1624.63ms
step:83101/100000 train_time:134992422ms step_avg:1624.63ms
step:83201/100000 train_time:135155421ms step_avg:1624.64ms
step:83301/100000 train_time:135318605ms step_avg:1624.65ms
step:83401/100000 train_time:135481667ms step_avg:1624.66ms
step:83501/100000 train_time:135644711ms step_avg:1624.66ms
step:83601/100000 train_time:135808326ms step_avg:1624.68ms
step:83701/100000 train_time:135971262ms step_avg:1624.68ms
step:83801/100000 train_time:136134520ms step_avg:1624.69ms
step:83901/100000 train_time:136297679ms step_avg:1624.70ms
step:84000/100000 val_loss:2.1143 train_time:136459129ms step_avg:1624.71ms perplexity:8.2836 param_count:132,475,500
step:84001/100000 train_time:136460753ms step_avg:1624.71ms
step:84101/100000 train_time:136623667ms step_avg:1624.71ms
step:84201/100000 train_time:136786804ms step_avg:1624.72ms
step:84301/100000 train_time:136949859ms step_avg:1624.73ms
step:84401/100000 train_time:137113731ms step_avg:1624.74ms
step:84501/100000 train_time:137276870ms step_avg:1624.75ms
step:84601/100000 train_time:137439962ms step_avg:1624.76ms
step:84701/100000 train_time:137603049ms step_avg:1624.77ms
step:84801/100000 train_time:137766264ms step_avg:1624.77ms
step:84901/100000 train_time:137929503ms step_avg:1624.78ms
step:85000/100000 val_loss:2.1169 train_time:138090939ms step_avg:1624.79ms perplexity:8.3057 param_count:132,475,500
step:85001/100000 train_time:138092575ms step_avg:1624.79ms
step:85101/100000 train_time:138255349ms step_avg:1624.79ms
step:85201/100000 train_time:138418986ms step_avg:1624.81ms
step:85301/100000 train_time:138581974ms step_avg:1624.81ms
step:85401/100000 train_time:138745187ms step_avg:1624.82ms
step:85501/100000 train_time:138908338ms step_avg:1624.83ms
step:85601/100000 train_time:139071618ms step_avg:1624.84ms
step:85701/100000 train_time:139234699ms step_avg:1624.85ms
step:85801/100000 train_time:139397733ms step_avg:1624.85ms
step:85901/100000 train_time:139561671ms step_avg:1624.87ms
step:86000/100000 val_loss:2.1067 train_time:139723230ms step_avg:1624.88ms perplexity:8.2208 param_count:132,475,500
step:86001/100000 train_time:139724857ms step_avg:1624.88ms
step:86101/100000 train_time:139887702ms step_avg:1624.88ms
step:86201/100000 train_time:140050852ms step_avg:1624.89ms
step:86301/100000 train_time:140214120ms step_avg:1624.90ms
step:86401/100000 train_time:140377313ms step_avg:1624.91ms
step:86501/100000 train_time:140540545ms step_avg:1624.92ms
step:86601/100000 train_time:140703820ms step_avg:1624.92ms
step:86701/100000 train_time:140867582ms step_avg:1624.94ms
step:86801/100000 train_time:141030843ms step_avg:1624.95ms
step:86901/100000 train_time:141194211ms step_avg:1624.96ms
step:87000/100000 val_loss:2.1094 train_time:141355949ms step_avg:1624.97ms perplexity:8.2429 param_count:132,475,500
step:87001/100000 train_time:141357582ms step_avg:1624.97ms
step:87101/100000 train_time:141520587ms step_avg:1624.97ms
step:87201/100000 train_time:141683605ms step_avg:1624.98ms
step:87301/100000 train_time:141846784ms step_avg:1624.99ms
step:87401/100000 train_time:142009941ms step_avg:1625.00ms
step:87501/100000 train_time:142173654ms step_avg:1625.01ms
step:87601/100000 train_time:142336793ms step_avg:1625.02ms
step:87701/100000 train_time:142500130ms step_avg:1625.03ms
step:87801/100000 train_time:142663644ms step_avg:1625.04ms
step:87901/100000 train_time:142826912ms step_avg:1625.05ms
step:88000/100000 val_loss:2.1126 train_time:142988408ms step_avg:1625.05ms perplexity:8.2697 param_count:132,475,500
step:88001/100000 train_time:142990040ms step_avg:1625.05ms
step:88101/100000 train_time:143152954ms step_avg:1625.06ms
step:88201/100000 train_time:143316157ms step_avg:1625.07ms
step:88301/100000 train_time:143480049ms step_avg:1625.08ms
step:88401/100000 train_time:143643298ms step_avg:1625.09ms
step:88501/100000 train_time:143806571ms step_avg:1625.10ms
step:88601/100000 train_time:143969865ms step_avg:1625.11ms
step:88701/100000 train_time:144133260ms step_avg:1625.12ms
step:88801/100000 train_time:144296529ms step_avg:1625.13ms
step:88901/100000 train_time:144459660ms step_avg:1625.13ms
step:89000/100000 val_loss:2.1047 train_time:144621731ms step_avg:1625.15ms perplexity:8.2047 param_count:132,475,500
step:89001/100000 train_time:144623371ms step_avg:1625.15ms
step:89101/100000 train_time:144786297ms step_avg:1625.15ms
step:89201/100000 train_time:144949377ms step_avg:1625.16ms
step:89301/100000 train_time:145112590ms step_avg:1625.16ms
step:89401/100000 train_time:145275859ms step_avg:1625.17ms
step:89501/100000 train_time:145439036ms step_avg:1625.18ms
step:89601/100000 train_time:145602164ms step_avg:1625.19ms
step:89701/100000 train_time:145765458ms step_avg:1625.20ms
step:89801/100000 train_time:145929254ms step_avg:1625.21ms
step:89901/100000 train_time:146092564ms step_avg:1625.22ms
step:90000/100000 val_loss:2.1122 train_time:146254119ms step_avg:1625.23ms perplexity:8.2663 param_count:132,475,500
step:90001/100000 train_time:146255754ms step_avg:1625.23ms
step:90101/100000 train_time:146418772ms step_avg:1625.23ms
step:90201/100000 train_time:146581797ms step_avg:1625.24ms
step:90301/100000 train_time:146745063ms step_avg:1625.25ms
step:90401/100000 train_time:146908450ms step_avg:1625.26ms
step:90501/100000 train_time:147071581ms step_avg:1625.26ms
step:90601/100000 train_time:147235341ms step_avg:1625.28ms
step:90701/100000 train_time:147398720ms step_avg:1625.28ms
step:90801/100000 train_time:147561947ms step_avg:1625.29ms
step:90901/100000 train_time:147725199ms step_avg:1625.30ms
step:91000/100000 val_loss:2.1086 train_time:147886765ms step_avg:1625.31ms perplexity:8.2366 param_count:132,475,500
step:91001/100000 train_time:147888401ms step_avg:1625.31ms
step:91101/100000 train_time:148051601ms step_avg:1625.32ms
step:91201/100000 train_time:148215045ms step_avg:1625.33ms
step:91301/100000 train_time:148378297ms step_avg:1625.33ms
step:91401/100000 train_time:148541990ms step_avg:1625.35ms
step:91501/100000 train_time:148705160ms step_avg:1625.35ms
step:91601/100000 train_time:148868459ms step_avg:1625.36ms
step:91701/100000 train_time:149031849ms step_avg:1625.37ms
step:91801/100000 train_time:149195081ms step_avg:1625.38ms
step:91901/100000 train_time:149358410ms step_avg:1625.39ms
step:92000/100000 val_loss:2.1074 train_time:149520121ms step_avg:1625.40ms perplexity:8.2267 param_count:132,475,500
step:92001/100000 train_time:149521733ms step_avg:1625.40ms
step:92101/100000 train_time:149685221ms step_avg:1625.41ms
step:92201/100000 train_time:149848527ms step_avg:1625.41ms
step:92301/100000 train_time:150011696ms step_avg:1625.42ms
step:92401/100000 train_time:150174884ms step_avg:1625.43ms
step:92501/100000 train_time:150338179ms step_avg:1625.44ms
step:92601/100000 train_time:150501564ms step_avg:1625.44ms
step:92701/100000 train_time:150664677ms step_avg:1625.45ms
step:92801/100000 train_time:150827856ms step_avg:1625.46ms
step:92901/100000 train_time:150991732ms step_avg:1625.47ms
step:93000/100000 val_loss:2.1073 train_time:151153327ms step_avg:1625.48ms perplexity:8.2256 param_count:132,475,500
step:93001/100000 train_time:151154946ms step_avg:1625.48ms
step:93101/100000 train_time:151317927ms step_avg:1625.48ms
step:93201/100000 train_time:151481169ms step_avg:1625.49ms
step:93301/100000 train_time:151644499ms step_avg:1625.50ms
step:93401/100000 train_time:151807733ms step_avg:1625.51ms
step:93501/100000 train_time:151971086ms step_avg:1625.52ms
step:93601/100000 train_time:152134349ms step_avg:1625.52ms
step:93701/100000 train_time:152298349ms step_avg:1625.54ms
step:93801/100000 train_time:152461628ms step_avg:1625.55ms
step:93901/100000 train_time:152625054ms step_avg:1625.56ms
step:94000/100000 val_loss:2.1048 train_time:152786643ms step_avg:1625.56ms perplexity:8.2052 param_count:132,475,500
step:94001/100000 train_time:152788287ms step_avg:1625.56ms
step:94101/100000 train_time:152951350ms step_avg:1625.57ms
step:94201/100000 train_time:153114433ms step_avg:1625.57ms
step:94301/100000 train_time:153277636ms step_avg:1625.58ms
step:94401/100000 train_time:153441364ms step_avg:1625.59ms
step:94501/100000 train_time:153604501ms step_avg:1625.60ms
step:94601/100000 train_time:153767797ms step_avg:1625.61ms
step:94701/100000 train_time:153931175ms step_avg:1625.62ms
step:94801/100000 train_time:154094580ms step_avg:1625.62ms
step:94901/100000 train_time:154257704ms step_avg:1625.63ms
step:95000/100000 val_loss:2.1060 train_time:154419439ms step_avg:1625.64ms perplexity:8.2150 param_count:132,475,500
step:95001/100000 train_time:154421068ms step_avg:1625.64ms
step:95101/100000 train_time:154584106ms step_avg:1625.64ms
step:95201/100000 train_time:154747809ms step_avg:1625.66ms
step:95301/100000 train_time:154910937ms step_avg:1625.66ms
step:95401/100000 train_time:155074247ms step_avg:1625.67ms
step:95501/100000 train_time:155237531ms step_avg:1625.68ms
step:95601/100000 train_time:155400578ms step_avg:1625.68ms
step:95701/100000 train_time:155563142ms step_avg:1625.68ms
step:95801/100000 train_time:155725145ms step_avg:1625.68ms
step:95901/100000 train_time:155888574ms step_avg:1625.69ms
step:96000/100000 val_loss:2.1140 train_time:156050805ms step_avg:1625.70ms perplexity:8.2815 param_count:132,475,500
step:96001/100000 train_time:156052440ms step_avg:1625.70ms
step:96101/100000 train_time:156215261ms step_avg:1625.70ms
step:96201/100000 train_time:156378278ms step_avg:1625.71ms
step:96301/100000 train_time:156541420ms step_avg:1625.71ms
step:96401/100000 train_time:156704342ms step_avg:1625.72ms
step:96501/100000 train_time:156867390ms step_avg:1625.72ms
step:96601/100000 train_time:157030521ms step_avg:1625.73ms
step:96701/100000 train_time:157193560ms step_avg:1625.73ms
step:96801/100000 train_time:157357222ms step_avg:1625.74ms
step:96901/100000 train_time:157520501ms step_avg:1625.75ms
step:97000/100000 val_loss:2.0956 train_time:157682070ms step_avg:1625.76ms perplexity:8.1302 param_count:132,475,500
step:97001/100000 train_time:157683684ms step_avg:1625.76ms
step:97101/100000 train_time:157846607ms step_avg:1625.76ms
step:97201/100000 train_time:158009855ms step_avg:1625.77ms
step:97301/100000 train_time:158172678ms step_avg:1625.77ms
step:97401/100000 train_time:158335670ms step_avg:1625.77ms
step:97501/100000 train_time:158499276ms step_avg:1625.78ms
step:97601/100000 train_time:158662403ms step_avg:1625.79ms
step:97701/100000 train_time:158825649ms step_avg:1625.80ms
step:97801/100000 train_time:158988821ms step_avg:1625.80ms
step:97901/100000 train_time:159152061ms step_avg:1625.81ms
step:98000/100000 val_loss:2.0964 train_time:159313654ms step_avg:1625.82ms perplexity:8.1364 param_count:132,475,500
step:98001/100000 train_time:159315266ms step_avg:1625.82ms
step:98101/100000 train_time:159478152ms step_avg:1625.82ms
step:98201/100000 train_time:159641374ms step_avg:1625.82ms
step:98301/100000 train_time:159805132ms step_avg:1625.84ms
step:98401/100000 train_time:159968432ms step_avg:1625.84ms
step:98501/100000 train_time:160131450ms step_avg:1625.85ms
step:98601/100000 train_time:160294588ms step_avg:1625.85ms
step:98701/100000 train_time:160457786ms step_avg:1625.86ms
step:98801/100000 train_time:160620983ms step_avg:1625.87ms
step:98901/100000 train_time:160784166ms step_avg:1625.87ms
step:99000/100000 val_loss:2.1039 train_time:160945651ms step_avg:1625.88ms perplexity:8.1981 param_count:132,475,500
step:99001/100000 train_time:160947297ms step_avg:1625.88ms
step:99101/100000 train_time:161110517ms step_avg:1625.88ms
step:99201/100000 train_time:161273753ms step_avg:1625.89ms
step:99301/100000 train_time:161436872ms step_avg:1625.90ms
step:99401/100000 train_time:161600053ms step_avg:1625.90ms
step:99501/100000 train_time:161763325ms step_avg:1625.91ms
step:99601/100000 train_time:161926448ms step_avg:1625.91ms
step:99701/100000 train_time:162089640ms step_avg:1625.92ms
step:99801/100000 train_time:162253133ms step_avg:1625.93ms
step:99901/100000 train_time:162416936ms step_avg:1625.94ms
step:100000/100000 val_loss:2.1044 train_time:162578444ms step_avg:1625.95ms perplexity:8.2019 param_count:132,475,500
peak memory consumption training: 42 GiB
Test results | Loss: 2.1058 | Perplexity: 8.2136
Total train time (min): 2709.64
Total train time (hours): 45.16
peak memory consumption testing: 42 GiB
